2006.jeptalnrecital-invite.2,J97-4005,0,0.277657,"Missing"
2006.jeptalnrecital-invite.2,bouma-kloosterman-2002-querying,0,0.129211,"Missing"
2006.jeptalnrecital-invite.2,W96-0209,0,0.0666284,"Missing"
2006.jeptalnrecital-invite.2,W00-1505,0,0.182322,"Missing"
2006.jeptalnrecital-invite.2,P02-1036,0,0.0610153,"Missing"
2006.jeptalnrecital-invite.2,C04-1188,0,0.0511045,"Missing"
2006.jeptalnrecital-invite.2,P99-1061,0,0.00881843,"Missing"
2006.jeptalnrecital-invite.2,J03-1006,0,0.0298997,"Missing"
2006.jeptalnrecital-invite.2,oostdijk-2000-spoken,0,0.0152675,"Missing"
2006.jeptalnrecital-invite.2,C00-1085,0,0.136387,"Missing"
2006.jeptalnrecital-invite.2,P02-1035,0,0.0843875,"Missing"
2006.jeptalnrecital-invite.2,I05-7011,0,0.074401,"Missing"
2006.jeptalnrecital-invite.2,J97-3004,1,0.782832,"Missing"
2006.jeptalnrecital-invite.2,P04-1057,1,0.827228,"Missing"
2006.jeptalnrecital-invite.2,W98-1303,0,0.0605621,"Missing"
2006.jeptalnrecital-invite.2,1995.iwpt-1.31,0,0.0812491,"Missing"
2018.gwc-1.30,E09-1005,0,0.135498,"Missing"
2018.gwc-1.30,C14-1151,0,0.119067,"ised WSD Rothe and Sch¨utze (2015; Jauhar et al. (2015; Taghipour and Ng (2015). Our system makes use of a combination of sense embeddings, context embeddings, and gloss embeddings. Similar approaches have been proposed by Chen et al. (2014) and Pelevina et al. (2016). The main difference to our approach is that they automatically induce sense embeddings and find the best sense by comparing them to context embeddings, while we add gloss embeddings for better performance. Inkpen and Hirst (2003) apply gloss- and context vectors to the disambiguation of near-synonyms in dictionary entries. Also Basile et al. (2014) use a distributional approach to representing definitions and the context of the target word. They create semantic vectors for glosses and contexts to compute similarity of the gloss and the context of a target word, while we also compute the similarity of a sense and its context directly using sense embeddings. 3 Lesk++ Our WSD algorithm takes sentences as input and outputs a preferred sense for each polysemous word. Given a sentence w1 . . . wi of i words, we retrieve a set of word senses from the sense inventory for each word w. Then, for each sense s of each word w, we consider the simila"
2018.gwc-1.30,D14-1110,0,0.0620143,"Missing"
2018.gwc-1.30,W00-1322,0,0.275767,"Missing"
2018.gwc-1.30,P12-1092,0,0.0609768,"mbeddings is that they exhibit certain algebraic relations and can, therefore, be used for meaningful semantic operations such as computing word similarity (Turney, 2006), and capturing lexical relationships (Mikolov et al., 2013b). A disadvantage of word embeddings is that they assign a single embedding to each word, thus ignoring the possibility that words may have more than one meaning. This problem can be addressed by associating each word with a number of sensespecific embeddings. For this, several methods have been proposed in recent work. For example, in Reisinger and Mooney (2010) and Huang et al. (2012), a fixed number of senses is learned for each word that has multiple meanings by first clustering the contexts of each token, and subsequently relabeling each word token with the clustered sense before learning embeddings. Although such sense embedding methods have demonstrated good performance, they use automatically induced senses. They are, therefore, not readily applicable for applications that rely on WordNet-based senses, such as machine translation and information retrieval and extraction systems (see Morato et al. (2004) for examples of such systems). Recently, features based on sense"
2018.gwc-1.30,N15-1070,0,0.01786,"tered sense before learning embeddings. Although such sense embedding methods have demonstrated good performance, they use automatically induced senses. They are, therefore, not readily applicable for applications that rely on WordNet-based senses, such as machine translation and information retrieval and extraction systems (see Morato et al. (2004) for examples of such systems). Recently, features based on sensespecific embeddings learned using a combination of large corpora and a sense inventory have been shown to achieve state-of-the-art results for supervised WSD Rothe and Sch¨utze (2015; Jauhar et al. (2015; Taghipour and Ng (2015). Our system makes use of a combination of sense embeddings, context embeddings, and gloss embeddings. Similar approaches have been proposed by Chen et al. (2014) and Pelevina et al. (2016). The main difference to our approach is that they automatically induce sense embeddings and find the best sense by comparing them to context embeddings, while we add gloss embeddings for better performance. Inkpen and Hirst (2003) apply gloss- and context vectors to the disambiguation of near-synonyms in dictionary entries. Also Basile et al. (2014) use a distributional approach to"
2018.gwc-1.30,kilgarriff-rosenzweig-2000-english,0,0.0487387,"not require manually tagged data and have proven to be applicable to new domains (Agirre et al., 2009). An example of such a system is the Lesk algorithm (Lesk, 1986) that exploits the idea that the overlap between the definition of a word and the definitions of the words in its context can provide information about its meaning. It only requires two types of information: a set of dictionary entries with definitions (hereafter referred to as glosses) for each possible word meaning, and the context in which the word occurs. A popular variant of the algorithm is the “simplified” Lesk algorithm (Kilgarriff and Rosenzweig, 2000), which disambiguates one word at a time by comparing each of its glosses to the context in which the word is found. This variant avoids the combinatorial explosion of word sense combinations the original version suffers from when trying to disambiguate multiple words in a text. A problem with the aforementioned method, however, is that, when a gloss is matched against the context of a word, in most cases the lexical overlap is very small. As a solution to this problem, we use a WSD-method that, instead of counting the number of words that overlap, takes embeddings as input to compute the simi"
2018.gwc-1.30,N13-1090,0,0.309939,"et word improves Simplified Lesk. In this paper we describe experiments where both methods are used in combination with out method that is based on word- and sense embeddings. 2 Related work In the past few years, much progress has been made on learning word embeddings from unlabeled data that represent the meanings of words as contextual feature vectors. A major advantage of these word embeddings is that they exhibit certain algebraic relations and can, therefore, be used for meaningful semantic operations such as computing word similarity (Turney, 2006), and capturing lexical relationships (Mikolov et al., 2013b). A disadvantage of word embeddings is that they assign a single embedding to each word, thus ignoring the possibility that words may have more than one meaning. This problem can be addressed by associating each word with a number of sensespecific embeddings. For this, several methods have been proposed in recent work. For example, in Reisinger and Mooney (2010) and Huang et al. (2012), a fixed number of senses is learned for each word that has multiple meanings by first clustering the contexts of each token, and subsequently relabeling each word token with the clustered sense before learnin"
2018.gwc-1.30,C12-1109,0,0.149721,"method works well on its own, its simplicity allows us to explore whether other extensions to the Lesk algorithm that have proven to be successful can improve it further. As both the Lesk algorithm and our extension rely on the definition of the words and the words that surround it, it is interesting to see whether adapting both sources of information would improve either of them. In this light, there are two possibilities: expansion or reduction. For the first option, the existing words of the context and glosses can be expanded with additional words that have similar meanings. For example, Miller et al. (2012) use a distributional thesaurus, that is computed from a large parsed corpus to lexically expand the context and sense information. They show that, using these expanded context and glosses, improves two variants of Lesk. When reducing the amount of words in either the context or the target words’ sense, methods are required that prohibit the loss of informative words. Vasilescu et al. (2004) shows that a pre-selection of words in the context of the target word improves Simplified Lesk. In this paper we describe experiments where both methods are used in combination with out method that is base"
2018.gwc-1.30,S01-1005,0,0.112053,"Missing"
2018.gwc-1.30,W16-1620,0,0.0188117,"ns that rely on WordNet-based senses, such as machine translation and information retrieval and extraction systems (see Morato et al. (2004) for examples of such systems). Recently, features based on sensespecific embeddings learned using a combination of large corpora and a sense inventory have been shown to achieve state-of-the-art results for supervised WSD Rothe and Sch¨utze (2015; Jauhar et al. (2015; Taghipour and Ng (2015). Our system makes use of a combination of sense embeddings, context embeddings, and gloss embeddings. Similar approaches have been proposed by Chen et al. (2014) and Pelevina et al. (2016). The main difference to our approach is that they automatically induce sense embeddings and find the best sense by comparing them to context embeddings, while we add gloss embeddings for better performance. Inkpen and Hirst (2003) apply gloss- and context vectors to the disambiguation of near-synonyms in dictionary entries. Also Basile et al. (2014) use a distributional approach to representing definitions and the context of the target word. They create semantic vectors for glosses and contexts to compute similarity of the gloss and the context of a target word, while we also compute the simi"
2018.gwc-1.30,S07-1016,0,0.0293435,"context, although it works well for other implementations of Lesk, harms our method. Using a lexical selection method on the context words, on the other hand, improves it. The combination of our method with lexical selection enables our method to outperform state-of the art knowledgebased systems. 1 Introduction The quest of automatically finding the correct meaning of a word in context, also known as Word Sense Disambiguation (WSD), is an important topic in Natural Language Processing (NLP). WSD systems that are based on supervised learning methods gain best results (Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli and Lapata, 2007; Navigli, 2009; Zhong and Ng, 2010). However, they require a large amount of manually annotated data for training. Also, even if such a supervised system obtains good results in a certain domain, it is not readily portable to other domains (Escudero et al., 2000). As an alternative to supervised systems, Gertjan van Noord Rijksuniversiteit Groningen, Groningen, g.j.m.van.noord@rug.nl knowledge-based systems do not require manually tagged data and have proven to be applicable to new domains (Agirre et al., 2009). An example of such a system is the Lesk algorithm (Lesk"
2018.gwc-1.30,E17-1010,0,0.0111383,"o a state-of-the-art knowledge-based WSD system, UKB (Agirre and Soroa, 2009), that, similar to our method, does not require any manually tagged data. UKB can be used for graph-based WSD using a pre-existing knowledge base. It applies random walks, e.g. Personalized PageRank, on the Knowledge Base graph to rank the vertices according to the context. We use UKBs Personalized PageRank method word-by-word with WordNet 1.7 and eXtended WordNet for English, as this setup yielded the best results in Agirre and Soroa (2009). For Senseval-2 (SE-2) and Senseval-3 we use the WSD evaluation framework of Raganato et al. (2017), which provides evaluation datasets and output of other knowledge-based WSD systems. From those systems we report on the Extended Lesk version of Basile et al. (2014), (DSM)4 which is most similar to our approach. The manually annotated part of DutchSemCor is balanced per sense which means that an equal number of examples for each sense is annotated. It is therefore not a reliable source for computing the most frequent sense. Alternatively, similar to Vossen et al. (2013a), we derive sense frequencies by using the automatically annotated counts in DutchSemCor5 . The most frequent sense baseli"
2018.gwc-1.30,N10-1013,0,0.0402931,"major advantage of these word embeddings is that they exhibit certain algebraic relations and can, therefore, be used for meaningful semantic operations such as computing word similarity (Turney, 2006), and capturing lexical relationships (Mikolov et al., 2013b). A disadvantage of word embeddings is that they assign a single embedding to each word, thus ignoring the possibility that words may have more than one meaning. This problem can be addressed by associating each word with a number of sensespecific embeddings. For this, several methods have been proposed in recent work. For example, in Reisinger and Mooney (2010) and Huang et al. (2012), a fixed number of senses is learned for each word that has multiple meanings by first clustering the contexts of each token, and subsequently relabeling each word token with the clustered sense before learning embeddings. Although such sense embedding methods have demonstrated good performance, they use automatically induced senses. They are, therefore, not readily applicable for applications that rely on WordNet-based senses, such as machine translation and information retrieval and extraction systems (see Morato et al. (2004) for examples of such systems). Recently,"
2018.gwc-1.30,P15-1173,0,0.042786,"Missing"
2018.gwc-1.30,W04-0811,0,0.0605537,"f words in the gloss and context, although it works well for other implementations of Lesk, harms our method. Using a lexical selection method on the context words, on the other hand, improves it. The combination of our method with lexical selection enables our method to outperform state-of the art knowledgebased systems. 1 Introduction The quest of automatically finding the correct meaning of a word in context, also known as Word Sense Disambiguation (WSD), is an important topic in Natural Language Processing (NLP). WSD systems that are based on supervised learning methods gain best results (Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli and Lapata, 2007; Navigli, 2009; Zhong and Ng, 2010). However, they require a large amount of manually annotated data for training. Also, even if such a supervised system obtains good results in a certain domain, it is not readily portable to other domains (Escudero et al., 2000). As an alternative to supervised systems, Gertjan van Noord Rijksuniversiteit Groningen, Groningen, g.j.m.van.noord@rug.nl knowledge-based systems do not require manually tagged data and have proven to be applicable to new domains (Agirre et al., 2009). An example of such a system is th"
2018.gwc-1.30,N15-1035,0,0.0178355,"arning embeddings. Although such sense embedding methods have demonstrated good performance, they use automatically induced senses. They are, therefore, not readily applicable for applications that rely on WordNet-based senses, such as machine translation and information retrieval and extraction systems (see Morato et al. (2004) for examples of such systems). Recently, features based on sensespecific embeddings learned using a combination of large corpora and a sense inventory have been shown to achieve state-of-the-art results for supervised WSD Rothe and Sch¨utze (2015; Jauhar et al. (2015; Taghipour and Ng (2015). Our system makes use of a combination of sense embeddings, context embeddings, and gloss embeddings. Similar approaches have been proposed by Chen et al. (2014) and Pelevina et al. (2016). The main difference to our approach is that they automatically induce sense embeddings and find the best sense by comparing them to context embeddings, while we add gloss embeddings for better performance. Inkpen and Hirst (2003) apply gloss- and context vectors to the disambiguation of near-synonyms in dictionary entries. Also Basile et al. (2014) use a distributional approach to representing definitions"
2018.gwc-1.30,J06-3003,0,0.0551964,"a pre-selection of words in the context of the target word improves Simplified Lesk. In this paper we describe experiments where both methods are used in combination with out method that is based on word- and sense embeddings. 2 Related work In the past few years, much progress has been made on learning word embeddings from unlabeled data that represent the meanings of words as contextual feature vectors. A major advantage of these word embeddings is that they exhibit certain algebraic relations and can, therefore, be used for meaningful semantic operations such as computing word similarity (Turney, 2006), and capturing lexical relationships (Mikolov et al., 2013b). A disadvantage of word embeddings is that they assign a single embedding to each word, thus ignoring the possibility that words may have more than one meaning. This problem can be addressed by associating each word with a number of sensespecific embeddings. For this, several methods have been proposed in recent work. For example, in Reisinger and Mooney (2010) and Huang et al. (2012), a fixed number of senses is learned for each word that has multiple meanings by first clustering the contexts of each token, and subsequently relabel"
2018.gwc-1.30,vasilescu-etal-2004-evaluating,0,0.30317,"s light, there are two possibilities: expansion or reduction. For the first option, the existing words of the context and glosses can be expanded with additional words that have similar meanings. For example, Miller et al. (2012) use a distributional thesaurus, that is computed from a large parsed corpus to lexically expand the context and sense information. They show that, using these expanded context and glosses, improves two variants of Lesk. When reducing the amount of words in either the context or the target words’ sense, methods are required that prohibit the loss of informative words. Vasilescu et al. (2004) shows that a pre-selection of words in the context of the target word improves Simplified Lesk. In this paper we describe experiments where both methods are used in combination with out method that is based on word- and sense embeddings. 2 Related work In the past few years, much progress has been made on learning word embeddings from unlabeled data that represent the meanings of words as contextual feature vectors. A major advantage of these word embeddings is that they exhibit certain algebraic relations and can, therefore, be used for meaningful semantic operations such as computing word s"
2018.gwc-1.30,vossen-etal-2012-dutchsemcor,0,0.0419142,"Missing"
2018.gwc-1.30,R13-1092,0,0.0553929,"Missing"
2018.gwc-1.30,P10-4014,0,0.0276405,"sk, harms our method. Using a lexical selection method on the context words, on the other hand, improves it. The combination of our method with lexical selection enables our method to outperform state-of the art knowledgebased systems. 1 Introduction The quest of automatically finding the correct meaning of a word in context, also known as Word Sense Disambiguation (WSD), is an important topic in Natural Language Processing (NLP). WSD systems that are based on supervised learning methods gain best results (Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli and Lapata, 2007; Navigli, 2009; Zhong and Ng, 2010). However, they require a large amount of manually annotated data for training. Also, even if such a supervised system obtains good results in a certain domain, it is not readily portable to other domains (Escudero et al., 2000). As an alternative to supervised systems, Gertjan van Noord Rijksuniversiteit Groningen, Groningen, g.j.m.van.noord@rug.nl knowledge-based systems do not require manually tagged data and have proven to be applicable to new domains (Agirre et al., 2009). An example of such a system is the Lesk algorithm (Lesk, 1986) that exploits the idea that the overlap between the de"
2020.eamt-1.10,N09-1003,0,0.0834306,"ate-of-the-art SMT systems also require initialization from pretrained embeddings. Therefore, we expect the same trend would appear. 8 We modify the test set by truecasing it in order to match our models. Word Similarity EN - MEN EN - WS353 EN - SIMLEX DE - SIMLEX DE Amount of Data (M) 0.1 1 10 0.138 0.421 0.705 0.018 0.461 0.628 0.011 0.232 0.300 0.017 0.051 0.293 Table 1: The Spearman correlation of the similarity of word pairs (measured by cosine similarity) and human evaluation. Evaluation done using: https://github.com/ kudkudak/word-embeddings-benchmarks MEN (Bruni et al., 2014), WS353 (Agirre et al., 2009), and SIMLEX999 (Hill et al., 2015). We also use Multilingual SIMLEX999 (Leviant and Reichart, 2015) for German and denote this as SIMLEX_DE. As we can see in Table 1, the correlation to human judgment on similarity tasks decreases dramatically as the amount of data used to train the models decreases. The poor correlation when data is limited explains V EC M AP’s poor alignment, as it relies on word similarity being relatively equivalent across languages for its initialization step. 4 Getting More out of Scarce Data With the source of the problem established as the drop in quality of embedding"
2020.eamt-1.10,P17-1042,0,0.506241,"T systems under the assumption c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. that there is a large amount of parallel data available, which is only the case for a select number of language pairs. Recently, there have been approaches that do away with this assumption, requiring only monolingual data, with the first methods based solely around neural MT (NMT), using aligned pretrained embeddings to bootstrap the translation process, and refining the translation with a neural model via denoising and back-translation (Artetxe et al., 2017b; Lample et al., 2017). More recently, statistical MT (SMT) approaches as well as hybrid approaches, combining SMT and NMT, have proven more successful (Lample et al., 2018; Artetxe et al., 2019). While the unsupervised approaches so far have done away with the assumption of parallel data, they still assume an abundance of monolingual data for the two languages, typically assuming at least 10 million sentences per language. This amount of data is not available for every language, notably languages without much of a digital presence. For example, Fulah is a language spoken in West and Central"
2020.eamt-1.10,J82-2005,0,0.719427,"Missing"
2020.eamt-1.10,P18-1073,0,0.0201349,"ential steps: 1. Train monolingual embeddings for each language 2. Align embeddings with a mapping algorithm 3. Train NMT system, initialized with aligned embeddings In the first step, monolingual embeddings (which we will also refer to as pretrained embeddings) are most often trained in the style of WORD 2 VEC ’s skip-gram algorithm (Mikolov et al., 2013). To incorporate sub-word information, Lample et al. (2018) use FAST T EXT (Bojanowski et al., 2017), which formulates a word’s embedding as the sum of its character n-gram embeddings. Artetxe (2019) uses a WORD 2 VEC extension PHRASE 2 VEC (Artetxe et al., 2018b), which learns embeddings of word n-grams up to trigrams, effectively creating embeddings for phrases. The second step involves the alignment of the two monolingual embeddings such that the embeddings of words with identical or similar meaning across language appear close in the shared embedding space. Artetxe et al. achieve this using V EC M AP (Artetxe et al., 2018a), which learns a linear transformation between the two embeddings into a shared space. If there is a large shared vocabulary between the two languages, it is also possible to concatenate the monolingual corpora and train a sing"
2020.eamt-1.10,D18-1399,0,0.03782,"Missing"
2020.eamt-1.10,P19-1019,0,0.523048,"el data available, which is only the case for a select number of language pairs. Recently, there have been approaches that do away with this assumption, requiring only monolingual data, with the first methods based solely around neural MT (NMT), using aligned pretrained embeddings to bootstrap the translation process, and refining the translation with a neural model via denoising and back-translation (Artetxe et al., 2017b; Lample et al., 2017). More recently, statistical MT (SMT) approaches as well as hybrid approaches, combining SMT and NMT, have proven more successful (Lample et al., 2018; Artetxe et al., 2019). While the unsupervised approaches so far have done away with the assumption of parallel data, they still assume an abundance of monolingual data for the two languages, typically assuming at least 10 million sentences per language. This amount of data is not available for every language, notably languages without much of a digital presence. For example, Fulah is a language spoken in West and Central Africa by over 20 million people, however there is a scarce amount of data freely available online. This motivates a new paradigm in unsupervised MT: Low-Resource Unsupervised MT (LRUMT). In this"
2020.eamt-1.10,Q17-1010,0,0.045391,"in Section 5, we present our conclusions and lines for future work. 2 An Unsupervised MT Overview The typical unsupervised NMT pipeline can be broken down into 3 sequential steps: 1. Train monolingual embeddings for each language 2. Align embeddings with a mapping algorithm 3. Train NMT system, initialized with aligned embeddings In the first step, monolingual embeddings (which we will also refer to as pretrained embeddings) are most often trained in the style of WORD 2 VEC ’s skip-gram algorithm (Mikolov et al., 2013). To incorporate sub-word information, Lample et al. (2018) use FAST T EXT (Bojanowski et al., 2017), which formulates a word’s embedding as the sum of its character n-gram embeddings. Artetxe (2019) uses a WORD 2 VEC extension PHRASE 2 VEC (Artetxe et al., 2018b), which learns embeddings of word n-grams up to trigrams, effectively creating embeddings for phrases. The second step involves the alignment of the two monolingual embeddings such that the embeddings of words with identical or similar meaning across language appear close in the shared embedding space. Artetxe et al. achieve this using V EC M AP (Artetxe et al., 2018a), which learns a linear transformation between the two embeddings"
2020.eamt-1.10,P19-1070,0,0.0172496,"tem can achieve a BLEU score of around 6 using embeddings trained on 10 million sentences, even when the NMT system is only trained on 100 thousand sentences per language. We also provide Figure 2, showing the BLI scores of the aligned embeddings (using the English→German test set from Artetxe et al. (2017a)8 ) as we vary the amount of training data used for the embeddings. We can see that the BLI scores decrease dramatically as the amount of sentences decreases, matching the trend of the results from Figure 1. Although BLI has been criticized for not always correlating with downstream tasks (Glavas et al., 2019), in this case, poor alignment corresponds to poor MT performance. In these experiments, we use V EC M AP for aligning embeddings. V EC M AP’s algorithm begins by initializing a bilingual dictionary, which uses a word’s relations to the other words in the same language, with the idea being that “apple” would be close to “pear” but far from “motorcycle” in every language, for example. However, if the quality of embeddings is poor, the random initialization of embeddings has a greater dampening effect. Using embedding similarity tasks (shown in Table 1), we find this to be the case. We measure t"
2020.eamt-1.10,D18-1160,0,0.0135703,"-resource setting. In the following experiments, we use the same settings as mentioned in Section 3, apart from those explicitly mentioned. With the addition of dependency parsing into the pipeline, we apply a parser on the tokenized sentences, while truecasing is learned prior to but applied after parsing. We use the StanfordNLP parser (Qi et al., 2019), using the pretrained English and German models provided to parse our data. Although the dependency parser that we use is supervised, therefore requiring dependency data, it is possible to train a dependency parser in an unsupervised fashion (He et al., 2018). Regardless, a dependency parser extracts linguistic information that is present in a sentence, thus our dependencybased method can still show whether using such linguistic information for training embeddings is useful for their alignment. For training dependency-based word embeddings, we apply Levy and Goldberg (2014)’s dependency-based WORD 2 VEC, and compare this against the standard WORD 2 VEC. For the dependency-based embeddings, we use the same hyperparameters as we use for WORD 2 VEC. To achieve considerable results in unsupervised NMT, it is necessary that we apply Byte-Pair Encoding"
2020.eamt-1.10,J15-4004,0,0.0174303,"re initialization from pretrained embeddings. Therefore, we expect the same trend would appear. 8 We modify the test set by truecasing it in order to match our models. Word Similarity EN - MEN EN - WS353 EN - SIMLEX DE - SIMLEX DE Amount of Data (M) 0.1 1 10 0.138 0.421 0.705 0.018 0.461 0.628 0.011 0.232 0.300 0.017 0.051 0.293 Table 1: The Spearman correlation of the similarity of word pairs (measured by cosine similarity) and human evaluation. Evaluation done using: https://github.com/ kudkudak/word-embeddings-benchmarks MEN (Bruni et al., 2014), WS353 (Agirre et al., 2009), and SIMLEX999 (Hill et al., 2015). We also use Multilingual SIMLEX999 (Leviant and Reichart, 2015) for German and denote this as SIMLEX_DE. As we can see in Table 1, the correlation to human judgment on similarity tasks decreases dramatically as the amount of data used to train the models decreases. The poor correlation when data is limited explains V EC M AP’s poor alignment, as it relies on word similarity being relatively equivalent across languages for its initialization step. 4 Getting More out of Scarce Data With the source of the problem established as the drop in quality of embeddings, we ask ourselves: how can we pre"
2020.eamt-1.10,P07-2045,0,0.00908381,"g our experiments can be found at: https://github.com/Leukas/LRUMT Figure 1: English→German BLEU scores of unsupervised NMT systems where the amount of training data used for the pre-trained embedding training and the amount used for the NMT model training is varied. 2016 for testing, following Lample et al. (2018). The training data is filtered such that sentences that contain between 3-80 words are kept. We then truncate the corpora to sizes ranging from 0.1 to 10 million sentences per language, specified as necessary. We used UDP IPE (Straka and Strakov´a, 2017) for tokenization2 , M OSES (Koehn et al., 2007) for truecasing, and we apply 60 thousand BPE joins (following Lample et al. (2018)) across both corpora using fastBPE.3,4 We train the word embeddings using the WORD 2 VEC skipgram model, with the same hyperparameters as used in Artetxe et al. (2017b), except using an embedding dimension size of 512.5 For embedding alignment, we use the completely unsupervised version of V EC M AP with default parameters. We then train our unsupervised NMT models using Lample et al. (2018)’s implementation, using the default parameters, with the exception of 10 backtranslation processors rather than 30 due to"
2020.eamt-1.10,P14-2050,0,0.333319,"g process. Word embedding algorithms typically define a context-target pair as a word and its neighboring words in a sentence, respectively. While this method works with a large amount of data available, it relies on the fact that a word is seen in several different contexts in order to be represented in the embedding space with respect to its meaning. When data is limited, the contexts contain too much variability to allow for a meaningful representation to be learned. To test this, we use an embedding strategy that has a different definition of the context: dependency-based word embeddings (Levy and Goldberg, 2014). These embeddings model the syntactic similarity between words rather than semantic similarity, providing an embedding representation complementary to standard embeddings. This section presents our findings using Figure 3: Example of a dependency-parsed sentence. dependency-based embeddings (4.1). We also consider the effect of using sub-word information via FAST T EXT (4.2). With the previous two approaches, we find that ensembling models can be useful, and investigate this further (4.3). Finally, we vary context window size and report on its effect (4.4). 4.1 Dependency-Based Embeddings Dep"
2020.eamt-1.10,N15-1144,0,0.0190232,"Missing"
2020.eamt-1.10,P03-1021,0,0.0103026,"c ), and the model is trained to reconstruct the original source sentence, minimizing the difference between s00src and ssrc . Denoising and back-translation are carried out alternately during training. The unsupervised SMT approach is fairly similar, with a replacement of step 3 (or in the hybrid approach, a step added between steps 2 and 3). In Artetxe et al. (2019) for example, a phrase-based SMT model is built by constructing a phrase table that is initialized using the aligned cross-lingual phrase embeddings, and tuning it using an unsupervised variant of the Minimum Error Rate Training (Och, 2003) method. For the hybrid model, the SMT system can then create pseudo-parallel data used to train the NMT model, alongside denoising and back-translation. In the remainder of this paper, we focus on the purely NMT approach to unsupervised MT. 3 The Role of Pretrained Embeddings in Unsupervised MT With the pipeline established, we now turn to the LRUMT setting. In LRUMT, the existing unsupervised approaches fail somewhere along the pipeline, but simply measuring MT performance does not make it clear where this failure occurs. We speculate that the failure is relative to the quality of the pretra"
2020.eamt-1.10,K17-3009,0,0.0219999,"Missing"
2020.emnlp-main.180,D15-1040,0,0.0224932,"oss-lingual transfer. Cross-Lingual Dependency Parsing The availability of consistent dependency treebanks in many languages (McDonald et al., 2013; Nivre et al., 2018) has provided an opportunity for the study of cross-lingual parsing. Early studies trained a delexicalized parser (Zeman and Resnik, 2008; McDonald et al., 2013) on one or more source languages by using either gold or predicted POS labels (Tiedemann, 2015) and applied it to target languages. Building on this, later work used additional features such as typological language properties (Naseem et al., 2012), syntactic embeddings (Duong et al., 2015), and cross-lingual word clusters (T¨ackstr¨om et al., 2012). Among lexicalized approaches, Vilares et al. (2016) learns a bilingual parser on a corpora obtained by merging harmonized treebanks. Ammar et al. (2016) trains a multilingual parser using multilingual word embeddings, token-level language information, language typology features and fine-grained POS tags. More recently, based on mBERT (Devlin et al., 2019), zero-shot transfer in dependency parsing was investigated (Wu and Dredze, 2019; Tran and Bisazza, 2019). Finally Kondratyuk and Straka (2019) trained a multilingual parser on the"
2020.emnlp-main.180,D18-1039,0,0.220448,"es (Johnson et al., 2017; Arivazhagan et al., 2019; Conneau et al., 2020), a problem also known as “the curse of multilinguality”. Generally speaking, a multilingual model without language-specific supervision is likely to suffer from over-generalization and perform poorly on high-resource languages due to limited capacity compared to the monolingual baselines, as verified by our experiments on parsing. In this paper, we strike a good balance between maximum sharing and language-specific capacity in multilingual dependency parsing. Inspired by recently introduced parameter sharing techniques (Platanios et al., 2018; Houlsby et al., 2019), we propose a new multilingual parser, UDapter, that learns to modify its language-specific parameters including the adapter modules, as a function of language embeddings. This allows the model to share parameters across languages, ensuring generalization and transfer ability, but also enables language-specific parameterization in a single multilingual model. Furthermore, we propose not to learn language embeddings from scratch, but to leverage a mix of linguistically curated and predicted typological features as obtained from the URIEL language typology database (Litte"
2020.emnlp-main.180,W15-2137,0,0.0139498,"d to combine language and task adapters, small bottleneck layers (Rebuffi et al., 2018; Houlsby et al., 2019), to address the capacity issue which limits multilingual pre-trained models for cross-lingual transfer. Cross-Lingual Dependency Parsing The availability of consistent dependency treebanks in many languages (McDonald et al., 2013; Nivre et al., 2018) has provided an opportunity for the study of cross-lingual parsing. Early studies trained a delexicalized parser (Zeman and Resnik, 2008; McDonald et al., 2013) on one or more source languages by using either gold or predicted POS labels (Tiedemann, 2015) and applied it to target languages. Building on this, later work used additional features such as typological language properties (Naseem et al., 2012), syntactic embeddings (Duong et al., 2015), and cross-lingual word clusters (T¨ackstr¨om et al., 2012). Among lexicalized approaches, Vilares et al. (2016) learns a bilingual parser on a corpora obtained by merging harmonized treebanks. Ammar et al. (2016) trains a multilingual parser using multilingual word embeddings, token-level language information, language typology features and fine-grained POS tags. More recently, based on mBERT (Devlin"
2020.emnlp-main.180,J19-3005,0,0.110741,"Missing"
2020.emnlp-main.180,W17-0412,0,0.0295288,"ed Attachement Scores (LAS) on the high-resource set are given in Table 1. UDapter consistently outperforms both our monolingual and multilingual baselines in all languages, and beats the previous work, setting a new state of the art, in 9 out of 13 languages. Statistical significance testing8 applied between UDapter and multi/mono-udify confirms that UDapter’s performance is significantly better than the baselines in 11 out of 13 languages (all except en and it). 8 We used paired bootstrap resampling to check whether the difference between two models is significant (p < 0.05) by using Udapi (Popel et al., 2017). 10 difference (udapter, multi-udify) treebank size (K) 20 8 16 6 12 4 8 2 4 0 ko eu tr zh he ar sv fi ru ja hi it en 0 Figure 2: Difference in LAS between UDapter and multi-udify in the high-resource setting. Diamonds indicate the amount of sentences in the corresponding treebank. Among directly comparable baselines, multiudify gives the worst performance in the typologically diverse high-resource setting. This multilingual model is clearly worse than its monolingually trained counterparts mono-udify: 83.0 vs 86.0. This result resounds with previous findings in multilingual NMT (Arivazhagan"
2020.emnlp-main.180,N19-1393,0,0.0935715,"lso enables language-specific parameterization in a single multilingual model. Furthermore, we propose not to learn language embeddings from scratch, but to leverage a mix of linguistically curated and predicted typological features as obtained from the URIEL language typology database (Littell et al., 2017) which supports 3718 languages including all languages represented in UD. While the importance of typological features for cross-lingual parsing is known for both non-neural (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) and neural approaches (Ammar et al., 2016; Scholivet et al., 2019), we are the first to use them 2302 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2302–2315, c November 16–20, 2020. 2020 Association for Computational Linguistics effectively as direct input to a neural parser, without manual selection, over a large number of languages in the context of zero-shot parsing where gold POS labels are not given at test time. In our model, typological features are crucial, leading to a substantial LAS increase on zero-shot languages and no loss on high-resource languages when compared to the language embeddings learne"
2020.emnlp-main.180,N12-1052,0,\N,Missing
2020.emnlp-main.180,P12-1066,0,\N,Missing
2020.emnlp-main.180,D15-1213,0,\N,Missing
2020.emnlp-main.180,P15-1166,0,\N,Missing
2020.emnlp-main.180,Q16-1023,0,\N,Missing
2020.emnlp-main.180,Q17-1024,0,\N,Missing
2020.emnlp-main.180,E17-2002,0,\N,Missing
2020.emnlp-main.180,D18-1103,0,\N,Missing
2020.emnlp-main.180,D18-1543,0,\N,Missing
2020.emnlp-main.180,K18-2020,0,\N,Missing
2020.emnlp-main.180,N19-1388,0,\N,Missing
2020.emnlp-main.180,N19-1423,0,\N,Missing
2020.emnlp-main.180,P13-2017,0,\N,Missing
2020.lrec-1.680,P18-2005,0,0.0604696,"Missing"
2020.lrec-1.680,P18-3010,0,0.0204747,"Missing"
2020.lrec-1.680,P18-1246,0,0.0988842,"Missing"
2020.lrec-1.680,D16-1034,0,0.0641556,"Missing"
2020.lrec-1.680,P18-1197,0,0.0391306,"Missing"
2020.lrec-1.680,P18-1031,0,0.0492424,"Missing"
2020.lrec-1.680,W18-3026,0,0.0278988,"Missing"
2020.lrec-1.680,P17-2014,0,0.0241643,"Missing"
2020.lrec-1.680,S18-1112,0,0.196409,"Missing"
2020.lrec-1.680,W18-0515,0,0.0590939,"Missing"
2020.tlt-1.13,augustinus-etal-2012-example,0,0.0503576,"Missing"
2020.tlt-1.13,W17-0403,1,0.905097,"Missing"
2020.tlt-1.13,I08-2139,0,0.0232032,"standard query language for XML documents. XML documents are, in essence, trees too. Obviously, not all linguistic annotations fit the concept of trees, and in most treebanks there are ways to encode, for instance, discontinuous constituents, secondary edges, enhanced dependencies etc. Also, feature structures such as those that arise in constraint-based grammatical frameworks (LFG, HPSG, . . . ) are directed graphs, not trees. It can be argued, therefore, that graphs are a better representation for linguistic annotation. And indeed, several treebank search systems have been based on graphs (Mírovský, 2008; Proisl and Uhrig, 2012; Bonfante et al., 2018). In this paper, we argue in addition that a graph-like representation is useful because it allows for a straight-forward combination of different types of annotation and annotation layers. In the AlpinoGraph application, four different annotation layers are combined (automatically), including two layers for Universal Dependencies (standard and enhanced) (Nivre et al., 2018), (Bouma and van Noord, 2017), the original Lassy annotation layer (van Eynde, 2005; van Noord et al., 2019), and a simple layer of word pairs inherited from PaQu (Odijk et al"
2020.tlt-1.13,proisl-uhrig-2012-efficient,0,0.0197778,"language for XML documents. XML documents are, in essence, trees too. Obviously, not all linguistic annotations fit the concept of trees, and in most treebanks there are ways to encode, for instance, discontinuous constituents, secondary edges, enhanced dependencies etc. Also, feature structures such as those that arise in constraint-based grammatical frameworks (LFG, HPSG, . . . ) are directed graphs, not trees. It can be argued, therefore, that graphs are a better representation for linguistic annotation. And indeed, several treebank search systems have been based on graphs (Mírovský, 2008; Proisl and Uhrig, 2012; Bonfante et al., 2018). In this paper, we argue in addition that a graph-like representation is useful because it allows for a straight-forward combination of different types of annotation and annotation layers. In the AlpinoGraph application, four different annotation layers are combined (automatically), including two layers for Universal Dependencies (standard and enhanced) (Nivre et al., 2018), (Bouma and van Noord, 2017), the original Lassy annotation layer (van Eynde, 2005; van Noord et al., 2019), and a simple layer of word pairs inherited from PaQu (Odijk et al., 2017). The representa"
2020.tlt-1.13,W03-2414,0,0.0865396,"Missing"
2020.tlt-1.13,2006.jeptalnrecital-invite.2,1,0.743407,"Missing"
2020.wmt-1.130,P07-2045,0,0.00826844,"Missing"
2020.wmt-1.130,P10-2041,0,0.351832,"ences. The 10 million German sentences include all of the data from years 2007 and 2010, and the remaining sentences are taken from 2014.5 Our initial model achieves BLEU scores of 17.43 and 19.05 for DE→HSB and HSB→DE respectively. 3.1 Data Selection We apply two forms of data selection: sentencelevel and document-level. As we have an abundance of German data (D) and limited Upper Sorbian data (H), we are only concerned with data selection for German. To select from D, we first must score our data in terms of its potential to improve the performance of our NMT model. Drawing inspiration from Moore and Lewis (2010), our scoring function is as follows: Score(s) = LMH→D0 (s) − LMD (s) |s| In this equation, s refers to any sentence in the German data, |s |to its token length, LMX (s) to the log probability of s using a language model trained on dataset X , and H → D0 to the dataset obtained by translating H into German using the initial system. A high scoring sentence is thus a sentence that has a high probability according to the Upper Sorbian language model compared to that of the German language model.6 3 The max length increase was found to perform slightly better in early testing. 4 Both steps are lim"
2020.wmt-1.130,P16-1162,0,0.016714,"ak this broad question down into 3 concrete sub-questions, tailored for the unsupervised setting. They are as follows: Setup For Upper Sorbian, we use the 3 monolingual datasets provided by the Sorbian Institute, the Witaj Sprachzentrum, and the web data from CIS, LMU. We also use the Upper Sorbian side of the parallel corpus from train.hsb-de.hsb.gz. For German, we use monolingual data from News Crawl and Common Crawl. For validation and testing, we use the data provided in devtest.tar.gz. All data is tokenized and truecased using the Moses toolkit (Koehn et al., 2007). For BPE segmentation (Sennrich et al., 2016), we apply a joint segmentation for both languages. This is done by first taking a sample of the German data of the same length as the Upper Sorbian data (around 750 thousand sentences). The BPE codes are learned and applied using FastBPE.1 After BPE is applied, we remove duplicate sentences while retaining the order of the corpora.2 We used the XLM model (Conneau and Lample, 2019) using the default parameters, with the excep1 https://github.com/glample/fastBPE For document-level filtering, we do not remove duplicates. 2 1099 Proceedings of the 5th Conference on Machine Translation (WMT), page"
2020.wmt-1.130,P13-2121,0,0.0232964,"st News Crawl data available. 6 The intuition behind subtracting the score of the German language model is that without it a sentence may have a high score due to it containing frequent words in general (e.g. “the”) rather than words that are particularly frequent in the Upper Sorbian dataset (e.g. “Sorbia”). DE→HSB 5.21 16.98 15.08 9.32 17.03 17.60 HSB→DE 5.91 18.45 18.05 8.46 18.19 19.23 Table 1: BLEU scores for XLM trained on data selected with the lowest and highest sentence and documentlevel scores, as well as randomly selected sentences and documents. The language model we use is KenLM (Heafield et al., 2013). We use a trigram model, with all other parameters being the default values. Since we require a portion of the German dataset to train the model, we choose N sentences randomly, with N being equal to the number of sentences in H.7 These sentences are not included during the selection process. For sentence-level selection, we simply order each sentence based on score and select the sentences with the highest scores. For document-level selection, we score each document by averaging its sentence-level scores, and select the documents with the highest scores. To answer our first research question"
2020.wmt-1.9,W18-6315,0,0.0205455,"ing In order to artificially increase the training data, we employ back-translation (BT) (Sennrich et al., 2016b). We consider two variations of this approach: TaggedBT was presented by Caswell et al. (2019) and is similar to the original BT technique of Sennrich et al. (2016b), with the major difference being the addition of a special tag (here <BT>) in front of every back-translated English sentence. Caswell et al. (2019) had shown that this simple manoeuvre resulted in a higher BLEU score when compared to untagged BT based NMTs. StupidBT Rather than performing actual BT which is expensive, Burlot and Yvon (2018) carry out the following: 1. Copy the target side data to the source side. 2. Prepend each token on the source side with a special id. For example, the token tablet becomes bt_tablet. Fine-tuning or transfer learning (Pan and Yang, 2010) is an effective technique to address a domain mismatch between the training set and the testset. While the testset consists of excerpts from newspapers, the training set consists of corpora with genres ranging from religious, political to movie subtitles. In fact, only a third of UFAL is news-oriented. A strategy to circumvent the domain mismatch is to fine-tu"
2020.wmt-1.9,W19-5206,0,0.0243779,"8 87 514 586 15 149 8 503 95 40 166 92 6 61 7 158 10669 1885 625 Total Table 1: Approximate sizes (in thousands) of the Parallel Corpora used for training the NMT models Name Domain Wikipedia Dumps News crawl PMI Wikipedia News Political Total TA Tokens(k) Sentences(k) 4034 1496 207 1669 709 99 5737 2477 Table 2: Approximate sizes (in thousands) of the Tamil Monolingual Corpora 5.2 Back-translation 5.3 Fine-tuning In order to artificially increase the training data, we employ back-translation (BT) (Sennrich et al., 2016b). We consider two variations of this approach: TaggedBT was presented by Caswell et al. (2019) and is similar to the original BT technique of Sennrich et al. (2016b), with the major difference being the addition of a special tag (here <BT>) in front of every back-translated English sentence. Caswell et al. (2019) had shown that this simple manoeuvre resulted in a higher BLEU score when compared to untagged BT based NMTs. StupidBT Rather than performing actual BT which is expensive, Burlot and Yvon (2018) carry out the following: 1. Copy the target side data to the source side. 2. Prepend each token on the source side with a special id. For example, the token tablet becomes bt_tablet. F"
2020.wmt-1.9,Y18-3003,0,0.14736,"rained on a much larger dataset of 190k parallel sentences. They also performed pre-processing steps involving morphological rules based on Tamil suffixes that improved upon the BLEU score of the baseline model (from 9.42 to 9.77). Their dataset (henceforth called UFAL) became the default benchmark for EN-TA translation systems until 2019, and we also use it in our experiments as an additional (general-domain) development set. To the best of our knowledge, there have only been a handful of NMT systems trained on EN→TA. For the Indic languages multilingual tasks of WAT-2018, Sen et al. (2018), Dabre et al. (2018) and Ojha et al. (2018) reported BLEU scores for EN→TA. The Phrasal-based SMT system of Ojha et al. (2018) with a score of 30.53 BLEU outperformed the NMT systems of Sen et al. (2018) (11.88) and Dabre et al. (2018) (18.60), suggesting that the NMT systems were not suitable for translating a highly morphological language such as Tamil. However, the following year, Philip et al. (2019) outperformed Ramasamy et al. (2012) on the UFAL dataset with a BLEU score of 13.05. They report that techniques such as domain adaptation and back-translation can make training NMT systems on low-resource languag"
2020.wmt-1.9,W01-1409,0,0.916802,"ne translation (NMT) systems submitted to the WMT-2020 English-Tamil (EN→TA) news translation task. This task is challenging mainly for two reasons: 1. Differing syntax: English is an IndoEuropean language which is fusional and SVO (Subject-Verb-Object). On the other hard, Tamil is part of the Dravidian language family and is a SOV language that is agglutinative. A good NMT system is expected to discern the various morphological forms on the Tamil target side. 2. Scarcity of training data: Prior to WMT-2020, there existed only a few corpora for parallel EN-TA sentences (Ramasamy et al., 2012; Germann, 2001). This left us with the choice of either only utilizing the low amount of parallel sentences or finding out ways of artificially enlarging the training data. Through our submission we wish to provide solutions to the following questions: Tamil is a Dravidian language spoken by around 80 million people. Tamil morphology is agglutinative and suffixal, i.e words are formed by suffixing morphemes to a lemma (Annamalai et. al 2014, cited in Sarveswaran et al. (2019)). Tamil suffixes can be either derivational (marking a change in PoS and/or meaning) or inflectional. In particular, nouns in Tamil ar"
2020.wmt-1.9,C14-1111,0,0.105253,"egmentation algorithms (Sennrich et al., 2016c; Kudo and Richardson, 2018) that generate sub-words based on simpler frequency criteria to attain a predetermined vocabulary size. In our experiments we try out different vocabulary sizes as well as generating the subwords either individually for each language or jointly learning on both. The SentencePiece (SP) implementation (Kudo and Richardson, 2018) is used to perform this segmentation. Linguistically Motivated Vocabulary Reduction (LMVR) is an unsupervised morphological segmentation algorithm based on Morfessor FlatCat (Kohonen et al., 2010; Grönroos et al., 2014) and proposed by Ataman et al. (2017). LMVR works by imposing an extra condition on the cost function of Morfessor so as to favour vocabularies of the desired size. When comparing regular Subword tokenization to LMVR, Ataman et al. (2017) report a +2.3 BLEU improvement on the EnglishTurkish translation task. Similar to SP, we need to set the vocabulary size prior to running the segmentation. LMVR models are trained separately for Tamil and English, which are then used to segment the respective datasets. 127 Name Domain Wikititles PMI UFAL Koran MkB PIB NLPC Wikimatrix Wikipedia Political Mixed"
2020.wmt-1.9,W10-2210,0,0.296753,"uistically motivated segmentation algorithms (Sennrich et al., 2016c; Kudo and Richardson, 2018) that generate sub-words based on simpler frequency criteria to attain a predetermined vocabulary size. In our experiments we try out different vocabulary sizes as well as generating the subwords either individually for each language or jointly learning on both. The SentencePiece (SP) implementation (Kudo and Richardson, 2018) is used to perform this segmentation. Linguistically Motivated Vocabulary Reduction (LMVR) is an unsupervised morphological segmentation algorithm based on Morfessor FlatCat (Kohonen et al., 2010; Grönroos et al., 2014) and proposed by Ataman et al. (2017). LMVR works by imposing an extra condition on the cost function of Morfessor so as to favour vocabularies of the desired size. When comparing regular Subword tokenization to LMVR, Ataman et al. (2017) report a +2.3 BLEU improvement on the EnglishTurkish translation task. Similar to SP, we need to set the vocabulary size prior to running the segmentation. LMVR models are trained separately for Tamil and English, which are then used to segment the respective datasets. 127 Name Domain Wikititles PMI UFAL Koran MkB PIB NLPC Wikimatrix W"
2020.wmt-1.9,D18-2012,0,0.0336662,"@rug.nl Abstract • Is linguistically motivated subword segmentation beneficial for EN-TA translation? This paper describes our submission for the English-Tamil news translation task of WMT2020. The various techniques and Neural Machine Translation (NMT) models used by our team are presented and discussed, including back-translation, fine-tuning and word dropout. Additionally, our experiments show that using a linguistically motivated subword segmentation technique (Ataman et al., 2017) does not consistently outperform the more widely used, non-linguistically motivated SentencePiece algorithm (Kudo and Richardson, 2018), despite the agglutinative nature of Tamil morphology. 1 • Can the addition of TA monolingual data compensate for the small amount of parallel ENTA sentences despite the domain mismatch? • Can fine-tuning on a corpus of Indian news improve quality on the WMT news translation task? We start our paper with a short description of the Tamil language before delving into the various techniques adopted by our submitted NMT systems. 2 Tamil Language Introduction In this paper we present the neural machine translation (NMT) systems submitted to the WMT-2020 English-Tamil (EN→TA) news translation task."
2020.wmt-1.9,Y18-3011,0,0.110138,"dataset of 190k parallel sentences. They also performed pre-processing steps involving morphological rules based on Tamil suffixes that improved upon the BLEU score of the baseline model (from 9.42 to 9.77). Their dataset (henceforth called UFAL) became the default benchmark for EN-TA translation systems until 2019, and we also use it in our experiments as an additional (general-domain) development set. To the best of our knowledge, there have only been a handful of NMT systems trained on EN→TA. For the Indic languages multilingual tasks of WAT-2018, Sen et al. (2018), Dabre et al. (2018) and Ojha et al. (2018) reported BLEU scores for EN→TA. The Phrasal-based SMT system of Ojha et al. (2018) with a score of 30.53 BLEU outperformed the NMT systems of Sen et al. (2018) (11.88) and Dabre et al. (2018) (18.60), suggesting that the NMT systems were not suitable for translating a highly morphological language such as Tamil. However, the following year, Philip et al. (2019) outperformed Ramasamy et al. (2012) on the UFAL dataset with a BLEU score of 13.05. They report that techniques such as domain adaptation and back-translation can make training NMT systems on low-resource languages possible. 4 Datasets"
2020.wmt-1.9,N19-4009,0,0.021606,"on the English→French translation task. 128 First introduced in Gal and Ghahramani (2016), the word dropout technique was modified by Sennrich et al. (2016a) to randomly drop tokens instead of types during training. They reported an increase of 4-5 BLEU for the English↔Romanian language pair. Furthermore, Sennrich and Zhang (2019) report that introducing word dropout into NMT systems in low-resource settings leads to improvements in BLEU scores. We would hence like to investigate if the same improvements can be observed for EN-TA. 6 Experimental Setup All our NMTs are developed using Fairseq (Ott et al., 2019). Following the architecture setup of Philip et al. (2019) the Transformer-Base implementation (BASE) is used, with slight changes to a few parameters, which are explained below. The encoder and decoder are both set to 5 layers with embedding dimension of 512 and 8 attention heads. The hidden layer dimension is 2048 and layer normalization is applied before each encoder and decoder layer. Other parameters were set as follows: dropout (0.001), weight decay (0.2) and batch size of 4k tokens. Our loss function is cross-entropy with label smoothing of 0.2. The model is trained for 100 epochs with"
2020.wmt-1.9,D19-5215,0,0.403423,"additional (general-domain) development set. To the best of our knowledge, there have only been a handful of NMT systems trained on EN→TA. For the Indic languages multilingual tasks of WAT-2018, Sen et al. (2018), Dabre et al. (2018) and Ojha et al. (2018) reported BLEU scores for EN→TA. The Phrasal-based SMT system of Ojha et al. (2018) with a score of 30.53 BLEU outperformed the NMT systems of Sen et al. (2018) (11.88) and Dabre et al. (2018) (18.60), suggesting that the NMT systems were not suitable for translating a highly morphological language such as Tamil. However, the following year, Philip et al. (2019) outperformed Ramasamy et al. (2012) on the UFAL dataset with a BLEU score of 13.05. They report that techniques such as domain adaptation and back-translation can make training NMT systems on low-resource languages possible. 4 Datasets For our constrained systems, we restrict ourselves to the datasets provided by WMT. Parallel Table 1 presents the various parallel corpora along with their size and genre. The various corpora come from various sources and differ considerably in size. We also observe a very large difference in number of tokens between the two languages, with around 5 times more"
2020.wmt-1.9,W15-3049,0,0.0602357,"we selected the following fine-tuning setup: learning rate of 0.002, batch size of 128, dropout of 0.3, label smoothing with factor of 0.3, and early stopping after 5 epochs without improvements. Word Dropout Following Sennrich and Zhang (2019) we set the source word dropout to 0.3, i.e. the probability of a source word, in a batch, being dropped prior to training is 0.3. 7 Results We report BLEU scores on three testsets: the UFAL testset (Ramasamy et al., 2012), half of the WMT2020 devset (DEV)1 and the official WMT2020 testset. Given the rich morphology of Tamil, we also report CHRF scores (Popović, 2015) on the WMT2020 testset. We ran the program chrF++.py2 with the arguments -nw 0 -b 3 to obtain the CHRF score. From prior experimentation we found that a jointly trained SP model resulted in better BLEU when compared to separate training for each language, and hence perform the majority of SP experiments in Table 3 using a joint segmentation. On the other hand, LMVR being linguistically motivated is supposed to be trained independently for each language. The last two contrastive experiments (Exp8.2 and Exp11.2) were run after the evaluation phase to better assess the impact of LMVR on translat"
2020.wmt-1.9,W12-5611,0,0.637722,"resent the neural machine translation (NMT) systems submitted to the WMT-2020 English-Tamil (EN→TA) news translation task. This task is challenging mainly for two reasons: 1. Differing syntax: English is an IndoEuropean language which is fusional and SVO (Subject-Verb-Object). On the other hard, Tamil is part of the Dravidian language family and is a SOV language that is agglutinative. A good NMT system is expected to discern the various morphological forms on the Tamil target side. 2. Scarcity of training data: Prior to WMT-2020, there existed only a few corpora for parallel EN-TA sentences (Ramasamy et al., 2012; Germann, 2001). This left us with the choice of either only utilizing the low amount of parallel sentences or finding out ways of artificially enlarging the training data. Through our submission we wish to provide solutions to the following questions: Tamil is a Dravidian language spoken by around 80 million people. Tamil morphology is agglutinative and suffixal, i.e words are formed by suffixing morphemes to a lemma (Annamalai et. al 2014, cited in Sarveswaran et al. (2019)). Tamil suffixes can be either derivational (marking a change in PoS and/or meaning) or inflectional. In particular, n"
2020.wmt-1.9,W19-3111,0,0.0237234,"rget side. 2. Scarcity of training data: Prior to WMT-2020, there existed only a few corpora for parallel EN-TA sentences (Ramasamy et al., 2012; Germann, 2001). This left us with the choice of either only utilizing the low amount of parallel sentences or finding out ways of artificially enlarging the training data. Through our submission we wish to provide solutions to the following questions: Tamil is a Dravidian language spoken by around 80 million people. Tamil morphology is agglutinative and suffixal, i.e words are formed by suffixing morphemes to a lemma (Annamalai et. al 2014, cited in Sarveswaran et al. (2019)). Tamil suffixes can be either derivational (marking a change in PoS and/or meaning) or inflectional. In particular, nouns in Tamil are inflected for number, gender, case and animacy while verbs are inflected for tense, mood, aspect, negation, interrogation, information about emphasis, speaker perspective, sentience or rationality, and conditional and causal relations. Table 4 shows examples of the case forms in singular for the noun தகம ’book’. All the aforementioned statements substantiate the fact that Tamil morphology is highly complex. In fact, Ramasamy et al. (2012) identified 716 infle"
2020.wmt-1.9,Y18-3012,0,0.162719,"chical) that were trained on a much larger dataset of 190k parallel sentences. They also performed pre-processing steps involving morphological rules based on Tamil suffixes that improved upon the BLEU score of the baseline model (from 9.42 to 9.77). Their dataset (henceforth called UFAL) became the default benchmark for EN-TA translation systems until 2019, and we also use it in our experiments as an additional (general-domain) development set. To the best of our knowledge, there have only been a handful of NMT systems trained on EN→TA. For the Indic languages multilingual tasks of WAT-2018, Sen et al. (2018), Dabre et al. (2018) and Ojha et al. (2018) reported BLEU scores for EN→TA. The Phrasal-based SMT system of Ojha et al. (2018) with a score of 30.53 BLEU outperformed the NMT systems of Sen et al. (2018) (11.88) and Dabre et al. (2018) (18.60), suggesting that the NMT systems were not suitable for translating a highly morphological language such as Tamil. However, the following year, Philip et al. (2019) outperformed Ramasamy et al. (2012) on the UFAL dataset with a BLEU score of 13.05. They report that techniques such as domain adaptation and back-translation can make training NMT systems on"
2020.wmt-1.9,W16-2323,0,0.122723,"an Press Mixed Mixed EN Tokens(k) TA Tokens(k) Sentences(k) 215 707 3893 2366 104 1123 65 2178 18 87 514 586 15 149 8 503 95 40 166 92 6 61 7 158 10669 1885 625 Total Table 1: Approximate sizes (in thousands) of the Parallel Corpora used for training the NMT models Name Domain Wikipedia Dumps News crawl PMI Wikipedia News Political Total TA Tokens(k) Sentences(k) 4034 1496 207 1669 709 99 5737 2477 Table 2: Approximate sizes (in thousands) of the Tamil Monolingual Corpora 5.2 Back-translation 5.3 Fine-tuning In order to artificially increase the training data, we employ back-translation (BT) (Sennrich et al., 2016b). We consider two variations of this approach: TaggedBT was presented by Caswell et al. (2019) and is similar to the original BT technique of Sennrich et al. (2016b), with the major difference being the addition of a special tag (here <BT>) in front of every back-translated English sentence. Caswell et al. (2019) had shown that this simple manoeuvre resulted in a higher BLEU score when compared to untagged BT based NMTs. StupidBT Rather than performing actual BT which is expensive, Burlot and Yvon (2018) carry out the following: 1. Copy the target side data to the source side. 2. Prepend eac"
2020.wmt-1.9,P16-1009,0,0.23135,"an Press Mixed Mixed EN Tokens(k) TA Tokens(k) Sentences(k) 215 707 3893 2366 104 1123 65 2178 18 87 514 586 15 149 8 503 95 40 166 92 6 61 7 158 10669 1885 625 Total Table 1: Approximate sizes (in thousands) of the Parallel Corpora used for training the NMT models Name Domain Wikipedia Dumps News crawl PMI Wikipedia News Political Total TA Tokens(k) Sentences(k) 4034 1496 207 1669 709 99 5737 2477 Table 2: Approximate sizes (in thousands) of the Tamil Monolingual Corpora 5.2 Back-translation 5.3 Fine-tuning In order to artificially increase the training data, we employ back-translation (BT) (Sennrich et al., 2016b). We consider two variations of this approach: TaggedBT was presented by Caswell et al. (2019) and is similar to the original BT technique of Sennrich et al. (2016b), with the major difference being the addition of a special tag (here <BT>) in front of every back-translated English sentence. Caswell et al. (2019) had shown that this simple manoeuvre resulted in a higher BLEU score when compared to untagged BT based NMTs. StupidBT Rather than performing actual BT which is expensive, Burlot and Yvon (2018) carry out the following: 1. Copy the target side data to the source side. 2. Prepend eac"
2020.wmt-1.9,P16-1162,0,0.335748,"an Press Mixed Mixed EN Tokens(k) TA Tokens(k) Sentences(k) 215 707 3893 2366 104 1123 65 2178 18 87 514 586 15 149 8 503 95 40 166 92 6 61 7 158 10669 1885 625 Total Table 1: Approximate sizes (in thousands) of the Parallel Corpora used for training the NMT models Name Domain Wikipedia Dumps News crawl PMI Wikipedia News Political Total TA Tokens(k) Sentences(k) 4034 1496 207 1669 709 99 5737 2477 Table 2: Approximate sizes (in thousands) of the Tamil Monolingual Corpora 5.2 Back-translation 5.3 Fine-tuning In order to artificially increase the training data, we employ back-translation (BT) (Sennrich et al., 2016b). We consider two variations of this approach: TaggedBT was presented by Caswell et al. (2019) and is similar to the original BT technique of Sennrich et al. (2016b), with the major difference being the addition of a special tag (here <BT>) in front of every back-translated English sentence. Caswell et al. (2019) had shown that this simple manoeuvre resulted in a higher BLEU score when compared to untagged BT based NMTs. StupidBT Rather than performing actual BT which is expensive, Burlot and Yvon (2018) carry out the following: 1. Copy the target side data to the source side. 2. Prepend eac"
2020.wmt-1.9,P19-1021,0,0.139307,"ith matters that are mostly political in nature. Despite the different content, we expect this corpus to be the closest in genre to the testset among the remaining parallel corpora. 5.4 Word Dropout This simple and cost-effective technique was shown to perform almost on a par with regular BT on the English→French translation task. 128 First introduced in Gal and Ghahramani (2016), the word dropout technique was modified by Sennrich et al. (2016a) to randomly drop tokens instead of types during training. They reported an increase of 4-5 BLEU for the English↔Romanian language pair. Furthermore, Sennrich and Zhang (2019) report that introducing word dropout into NMT systems in low-resource settings leads to improvements in BLEU scores. We would hence like to investigate if the same improvements can be observed for EN-TA. 6 Experimental Setup All our NMTs are developed using Fairseq (Ott et al., 2019). Following the architecture setup of Philip et al. (2019) the Transformer-Base implementation (BASE) is used, with slight changes to a few parameters, which are explained below. The encoder and decoder are both set to 5 layers with embedding dimension of 512 and 8 attention heads. The hidden layer dimension is 20"
2021.wat-1.21,P19-1310,0,0.0241054,"7, 2018) for Malayalam and (Lingam et al., 2014; Yadav and Lingam, 2017) for Telugu. findings was also reported by Ramesh et al. (2020) for Tamil and Dandapat and Federmann (2018) for Telugu . To the best of our knowledge and as of 2021, there has not been any scientific publication involving translation to and from Kannada, except for Chakravarthi et al. (2019). One possible reason for this could be the fact that sizeable corpora involving Kannada (i.e. in the order of magnitude of at least thousand sentences) have been readily available only since 2019, with the release of the JW300 Corpus (Agić and Vulić, 2019). Neural Machine Translation On the neural machine translation (NMT) side, there have been a handful of NMT systems trained on English→Tamil. On the aforementioned Indic languages multilingual tasks of WAT-2018, Sen et al. (2018), Dabre et al. (2018) reported only 11.88 and 18.60 BLEU scores, respectively, for English→Tamil. The poor performance of these systems compared to the 30.53 BLEU score of the SMT system (Ojha et al., 2018) showed that those NMT systems were not yet suitable for translating into the morphologically rich Tamil. However, the following year, Philip et al. (2019) outperfor"
2021.wat-1.21,P18-2049,0,0.0187454,"s Religion COVID-19 Technical Religion Technical General Cinema Press Politics Religion General General COVID-19 Technical Mixed General General Kannada 18 <1 70 1 Available in: Malayalam Tamil Telugu 1 <1 <1 45 <1 14 <1 <1 45 <1 <1 <1 26 5 4 18 <1 <1 <1 <1 10 <1 <1 <1 52 <1 <1 3 10 3 9 <1 <1 <1 <1 11 10 1 3 10 8 <1 1 <1 18 Table 2: Composition of training corpora. The numbers indicate the relative size (in percentages) of the corresponding part for that language. out-of-vocabulary (OOV) tokens. This was particularly an issue for translations involving agglutinative languages such as Turkish (Ataman and Federico, 2018) or Malayalam (Manohar et al., 2020). Various segmentation algorithms were brought forward to circumvent this issue and in turn, improve translation quality. Perhaps the most widely used algorithm in NMT to date is the language-agnostic Byte Pair Encoding (BPE) by Sennrich et al. (2016). Initially proposed by Gage (1994), BPE was repurposed by Sennrich et al. (2016) for the task of subword segmentation, and is based on a simple principle whereby pairs of character sequences that are frequently observed in a corpus get merged iteratively until a predetermined dictionary size is attained. In thi"
2021.wat-1.21,2020.lrec-1.444,0,0.0178853,"se NMT systems were not yet suitable for translating into the morphologically rich Tamil. However, the following year, Philip et al. (2019) outperformed Ramasamy et al. (2012) on the UFAL dataset with a BLEU score of 13.05 (the previous best score on this test set was 9.77). They report that techniques such as domain adaptation and back-translation can make training NMT systems on low-resource languages possible. Similar 182 Multilingual NMT Since 2018 several studies have presented multilingual NMT systems that can handle English → Malayalam, Tamil and Telugu translation (Dabre et al., 2018; Choudhary et al., 2020; Ojha et al., 2018; Sen et al., 2018; Yu et al., 2020; Dabre and Chakrabarty, 2020). In particular, Sen et al. (2018) presented results where the BLEU score improved when comparing monolingual and multilingual models. Conversely, Yu et al. (2020) found that NMT systems that were multi-way (Indic ↔ Indic) performed worse than English ↔ Indic systems. To our knowledge, no work so far has explored the effect of the segmentation algorithm and dictionary size on the four languages: Kannada, Malayalam, Tamil and Telugu. 3 Subword Segmentation Techniques Prior to the emergence of subword segmenters,"
2021.wat-1.21,W02-0603,0,0.0939728,"bserved in a corpus get merged iteratively until a predetermined dictionary size is attained. In this paper we use a popular implementation of BPE, called SentencePiece (SP) (Kudo and Richardson, 2018). While purely statistical algorithms are able to segment any token into smaller segments, there is no guarantee that the generated tokens will be linguistically sensible. Unsupervised morphological induction is a rich area of research that also aims at learning a segmentation from data, but in a linguistically motivated way. The most well-known example is Morphessor with its different variants (Creutz and Lagus, 2002; Kohonen et al., 2010; Grönroos et al., 2014). An important obstacle to applying Morfessor to the task of NMT is the lack of a mechanism to determine the dictionary size. To address this, Ataman et al. (2017) proposed a modification of Morfessor FlatCat (Grönroos et al., 2014), called Linguistically Motivated Vocabulary Reduction (LMVR). Specifically, LMVR imposes an extra condition on the cost function of Morfessor Flatcat so as to favour vocabularies of the desired size. In a comparison of LMVR to BPE, Ataman et al. (2017) reported a +2.3 BLEU improvement on the English-Turkish translation"
2021.wat-1.21,2020.wat-1.9,0,0.0270135,"rich Tamil. However, the following year, Philip et al. (2019) outperformed Ramasamy et al. (2012) on the UFAL dataset with a BLEU score of 13.05 (the previous best score on this test set was 9.77). They report that techniques such as domain adaptation and back-translation can make training NMT systems on low-resource languages possible. Similar 182 Multilingual NMT Since 2018 several studies have presented multilingual NMT systems that can handle English → Malayalam, Tamil and Telugu translation (Dabre et al., 2018; Choudhary et al., 2020; Ojha et al., 2018; Sen et al., 2018; Yu et al., 2020; Dabre and Chakrabarty, 2020). In particular, Sen et al. (2018) presented results where the BLEU score improved when comparing monolingual and multilingual models. Conversely, Yu et al. (2020) found that NMT systems that were multi-way (Indic ↔ Indic) performed worse than English ↔ Indic systems. To our knowledge, no work so far has explored the effect of the segmentation algorithm and dictionary size on the four languages: Kannada, Malayalam, Tamil and Telugu. 3 Subword Segmentation Techniques Prior to the emergence of subword segmenters, translation systems were plagued with the issue of Name Domain Bible ELRC GNOME JW3"
2021.wat-1.21,Y18-3003,0,0.0123773,"been any scientific publication involving translation to and from Kannada, except for Chakravarthi et al. (2019). One possible reason for this could be the fact that sizeable corpora involving Kannada (i.e. in the order of magnitude of at least thousand sentences) have been readily available only since 2019, with the release of the JW300 Corpus (Agić and Vulić, 2019). Neural Machine Translation On the neural machine translation (NMT) side, there have been a handful of NMT systems trained on English→Tamil. On the aforementioned Indic languages multilingual tasks of WAT-2018, Sen et al. (2018), Dabre et al. (2018) reported only 11.88 and 18.60 BLEU scores, respectively, for English→Tamil. The poor performance of these systems compared to the 30.53 BLEU score of the SMT system (Ojha et al., 2018) showed that those NMT systems were not yet suitable for translating into the morphologically rich Tamil. However, the following year, Philip et al. (2019) outperformed Ramasamy et al. (2012) on the UFAL dataset with a BLEU score of 13.05 (the previous best score on this test set was 9.77). They report that techniques such as domain adaptation and back-translation can make training NMT systems on low-resource la"
2021.wat-1.21,2020.wmt-1.9,1,0.725145,"Missing"
2021.wat-1.21,W01-1409,0,0.150349,"entence pairs. The analysis of our parallel datasets (section 4.1, Table 3) shows for • What is the optimal subword dictionary size for translating from English into these Dravidian languages? In what follows, we review the relevant previous work (Sect. 2), introduce the two segmenters (Sect. 3), describe the experimental setup (Sect. 4), and present our answers to the above research questions (Sect. 5). 2 Previous Work 2.1 Translation Systems Statistical Machine Translation One of the earliest automatic translation systems for English into a Dravidian language was the English→Tamil system by Germann (2001). They trained a hybrid rule-based/statistical machine translation system that was trained on only 5k English-Tamil parallel sentences. Ramasamy et al. (2012) created SMT systems (phrase-based and hierarchical) which were trained on a dataset of 190k parallel 181 Proceedings of the 8th Workshop on Asian Translation, pages 181–190 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics EN He was born in Thirukkuvalai village in Nagapattinam District on 3rd June, 1924. KN ಅವರು ಗಪಟಣಂ ಯ ರುಕುವಲ ಮದ 1924ರ ಜೂ 3ರಂದು ಜ ದರು. avaru nāgapattan  am jilleya tirukkuval"
2021.wat-1.21,Y18-3011,0,0.119504,"n mātam 3-ām tēti   tirukkuvalaik pirantār. TE ఆయన గపటణం 3న జ ం . ౖ మం 1924 āyana nāgapattan  am jillā tirukkuvālai grāmanlō 1924 jūn 3na janmincāru. ത� Table 1: Example sentence in English along with its translation and transliteration in the four Dravidian languages. sentences (henceforth referred to as UFAL). They also reported that applying pre-processing steps involving morphological rules based on Tamil suffixes improved the BLEU score of the baseline model to a small extent (from 9.42 to 9.77). For the Indic languages multilingual tasks of WAT2018, the Phrasal-based SMT system of Ojha et al. (2018) with a BLEU score of 30.53. Subsequent papers also focused on SMT systems for Malayalam and Telugu with some notable work including: (Anto and Nisha, 2016; Sreelekha and Bhattacharyya, 2017, 2018) for Malayalam and (Lingam et al., 2014; Yadav and Lingam, 2017) for Telugu. findings was also reported by Ramesh et al. (2020) for Tamil and Dandapat and Federmann (2018) for Telugu . To the best of our knowledge and as of 2021, there has not been any scientific publication involving translation to and from Kannada, except for Chakravarthi et al. (2019). One possible reason for this could be the fac"
2021.wat-1.21,N19-4009,0,0.0388989,"Missing"
2021.wat-1.21,P02-1040,0,0.109198,"Missing"
2021.wat-1.21,C14-1111,0,0.0196363,"il a predetermined dictionary size is attained. In this paper we use a popular implementation of BPE, called SentencePiece (SP) (Kudo and Richardson, 2018). While purely statistical algorithms are able to segment any token into smaller segments, there is no guarantee that the generated tokens will be linguistically sensible. Unsupervised morphological induction is a rich area of research that also aims at learning a segmentation from data, but in a linguistically motivated way. The most well-known example is Morphessor with its different variants (Creutz and Lagus, 2002; Kohonen et al., 2010; Grönroos et al., 2014). An important obstacle to applying Morfessor to the task of NMT is the lack of a mechanism to determine the dictionary size. To address this, Ataman et al. (2017) proposed a modification of Morfessor FlatCat (Grönroos et al., 2014), called Linguistically Motivated Vocabulary Reduction (LMVR). Specifically, LMVR imposes an extra condition on the cost function of Morfessor Flatcat so as to favour vocabularies of the desired size. In a comparison of LMVR to BPE, Ataman et al. (2017) reported a +2.3 BLEU improvement on the English-Turkish translation task of WMT18. Given the encouraging results r"
2021.wat-1.21,W10-2210,0,0.038293,"merged iteratively until a predetermined dictionary size is attained. In this paper we use a popular implementation of BPE, called SentencePiece (SP) (Kudo and Richardson, 2018). While purely statistical algorithms are able to segment any token into smaller segments, there is no guarantee that the generated tokens will be linguistically sensible. Unsupervised morphological induction is a rich area of research that also aims at learning a segmentation from data, but in a linguistically motivated way. The most well-known example is Morphessor with its different variants (Creutz and Lagus, 2002; Kohonen et al., 2010; Grönroos et al., 2014). An important obstacle to applying Morfessor to the task of NMT is the lack of a mechanism to determine the dictionary size. To address this, Ataman et al. (2017) proposed a modification of Morfessor FlatCat (Grönroos et al., 2014), called Linguistically Motivated Vocabulary Reduction (LMVR). Specifically, LMVR imposes an extra condition on the cost function of Morfessor Flatcat so as to favour vocabularies of the desired size. In a comparison of LMVR to BPE, Ataman et al. (2017) reported a +2.3 BLEU improvement on the English-Turkish translation task of WMT18. Given t"
2021.wat-1.21,D18-2012,0,0.019646,"brought forward to circumvent this issue and in turn, improve translation quality. Perhaps the most widely used algorithm in NMT to date is the language-agnostic Byte Pair Encoding (BPE) by Sennrich et al. (2016). Initially proposed by Gage (1994), BPE was repurposed by Sennrich et al. (2016) for the task of subword segmentation, and is based on a simple principle whereby pairs of character sequences that are frequently observed in a corpus get merged iteratively until a predetermined dictionary size is attained. In this paper we use a popular implementation of BPE, called SentencePiece (SP) (Kudo and Richardson, 2018). While purely statistical algorithms are able to segment any token into smaller segments, there is no guarantee that the generated tokens will be linguistically sensible. Unsupervised morphological induction is a rich area of research that also aims at learning a segmentation from data, but in a linguistically motivated way. The most well-known example is Morphessor with its different variants (Creutz and Lagus, 2002; Kohonen et al., 2010; Grönroos et al., 2014). An important obstacle to applying Morfessor to the task of NMT is the lack of a mechanism to determine the dictionary size. To addr"
2021.wat-1.21,D19-5215,0,0.078287,"Corpus (Agić and Vulić, 2019). Neural Machine Translation On the neural machine translation (NMT) side, there have been a handful of NMT systems trained on English→Tamil. On the aforementioned Indic languages multilingual tasks of WAT-2018, Sen et al. (2018), Dabre et al. (2018) reported only 11.88 and 18.60 BLEU scores, respectively, for English→Tamil. The poor performance of these systems compared to the 30.53 BLEU score of the SMT system (Ojha et al., 2018) showed that those NMT systems were not yet suitable for translating into the morphologically rich Tamil. However, the following year, Philip et al. (2019) outperformed Ramasamy et al. (2012) on the UFAL dataset with a BLEU score of 13.05 (the previous best score on this test set was 9.77). They report that techniques such as domain adaptation and back-translation can make training NMT systems on low-resource languages possible. Similar 182 Multilingual NMT Since 2018 several studies have presented multilingual NMT systems that can handle English → Malayalam, Tamil and Telugu translation (Dabre et al., 2018; Choudhary et al., 2020; Ojha et al., 2018; Sen et al., 2018; Yu et al., 2020; Dabre and Chakrabarty, 2020). In particular, Sen et al. (2018"
2021.wat-1.21,W15-3049,0,0.0673808,"Missing"
2021.wat-1.21,W18-6319,0,0.0206115,"Missing"
2021.wat-1.21,W12-5611,0,0.176281,"om English into these Dravidian languages? In what follows, we review the relevant previous work (Sect. 2), introduce the two segmenters (Sect. 3), describe the experimental setup (Sect. 4), and present our answers to the above research questions (Sect. 5). 2 Previous Work 2.1 Translation Systems Statistical Machine Translation One of the earliest automatic translation systems for English into a Dravidian language was the English→Tamil system by Germann (2001). They trained a hybrid rule-based/statistical machine translation system that was trained on only 5k English-Tamil parallel sentences. Ramasamy et al. (2012) created SMT systems (phrase-based and hierarchical) which were trained on a dataset of 190k parallel 181 Proceedings of the 8th Workshop on Asian Translation, pages 181–190 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics EN He was born in Thirukkuvalai village in Nagapattinam District on 3rd June, 1924. KN ಅವರು ಗಪಟಣಂ ಯ ರುಕುವಲ ಮದ 1924ರ ಜೂ 3ರಂದು ಜ ದರು. avaru nāgapattan  am jilleya tirukkuvalay grāmadalli 1924ra jūn 3randu janisiddaru. ML 1924ല  നാഗപ ണം ജി യിെല തിരു ുവൈള ഗാമ ിലാണ അേ ഹം ജനി 1924l nāgapattan jillayile tirukkuval ai grāmattilān ad"
2021.wat-1.21,2020.wat-1.22,0,0.027793,"ey also reported that applying pre-processing steps involving morphological rules based on Tamil suffixes improved the BLEU score of the baseline model to a small extent (from 9.42 to 9.77). For the Indic languages multilingual tasks of WAT2018, the Phrasal-based SMT system of Ojha et al. (2018) with a BLEU score of 30.53. Subsequent papers also focused on SMT systems for Malayalam and Telugu with some notable work including: (Anto and Nisha, 2016; Sreelekha and Bhattacharyya, 2017, 2018) for Malayalam and (Lingam et al., 2014; Yadav and Lingam, 2017) for Telugu. findings was also reported by Ramesh et al. (2020) for Tamil and Dandapat and Federmann (2018) for Telugu . To the best of our knowledge and as of 2021, there has not been any scientific publication involving translation to and from Kannada, except for Chakravarthi et al. (2019). One possible reason for this could be the fact that sizeable corpora involving Kannada (i.e. in the order of magnitude of at least thousand sentences) have been readily available only since 2019, with the release of the JW300 Corpus (Agić and Vulić, 2019). Neural Machine Translation On the neural machine translation (NMT) side, there have been a handful of NMT system"
2021.wat-1.21,I11-1062,0,0.0339622,"Missing"
2021.wat-1.21,Y18-3012,0,0.0193296,"021, there has not been any scientific publication involving translation to and from Kannada, except for Chakravarthi et al. (2019). One possible reason for this could be the fact that sizeable corpora involving Kannada (i.e. in the order of magnitude of at least thousand sentences) have been readily available only since 2019, with the release of the JW300 Corpus (Agić and Vulić, 2019). Neural Machine Translation On the neural machine translation (NMT) side, there have been a handful of NMT systems trained on English→Tamil. On the aforementioned Indic languages multilingual tasks of WAT-2018, Sen et al. (2018), Dabre et al. (2018) reported only 11.88 and 18.60 BLEU scores, respectively, for English→Tamil. The poor performance of these systems compared to the 30.53 BLEU score of the SMT system (Ojha et al., 2018) showed that those NMT systems were not yet suitable for translating into the morphologically rich Tamil. However, the following year, Philip et al. (2019) outperformed Ramasamy et al. (2012) on the UFAL dataset with a BLEU score of 13.05 (the previous best score on this test set was 9.77). They report that techniques such as domain adaptation and back-translation can make training NMT syste"
2021.wat-1.21,P16-1162,0,0.0608581,"<1 <1 <1 11 10 1 3 10 8 <1 1 <1 18 Table 2: Composition of training corpora. The numbers indicate the relative size (in percentages) of the corresponding part for that language. out-of-vocabulary (OOV) tokens. This was particularly an issue for translations involving agglutinative languages such as Turkish (Ataman and Federico, 2018) or Malayalam (Manohar et al., 2020). Various segmentation algorithms were brought forward to circumvent this issue and in turn, improve translation quality. Perhaps the most widely used algorithm in NMT to date is the language-agnostic Byte Pair Encoding (BPE) by Sennrich et al. (2016). Initially proposed by Gage (1994), BPE was repurposed by Sennrich et al. (2016) for the task of subword segmentation, and is based on a simple principle whereby pairs of character sequences that are frequently observed in a corpus get merged iteratively until a predetermined dictionary size is attained. In this paper we use a popular implementation of BPE, called SentencePiece (SP) (Kudo and Richardson, 2018). While purely statistical algorithms are able to segment any token into smaller segments, there is no guarantee that the generated tokens will be linguistically sensible. Unsupervised m"
2021.wat-1.21,P19-1021,0,0.0200061,"are flipped when we look at the CHRF scores. SP systems here report higher scores, with +3.5 improvement in Malayalam and +1.1 for Telugu. Given the morphological richness of our target languages, we take CHRF as the more reliable score, and conclude that the purely statistical segmenter SP is a better choice for translation into Dravidian languages in our setup. Larger dictionary sizes better: When observing the effect of the dictionary size, we find that the size 50k gives the highest BLEU scores for Malayalam, Tamil and Telugu. This is in contrast with studies such as (Philip et al., 2019; Sennrich and Zhang, 2019) who suggest to use a smaller dictionary size for low-resource settings. For these language pairs, we see a steady increase in BLEU and CHRF as we increase the dictionary size. For Kannada, the best results are obtained for much smaller dictionary sizes, but in contrast with the other three languages, the differences between the scores for other dictionary sizes is much smaller. For instance, looking at the CHRF scores of SP, the numbers decrease from 48.3 to 46.0, whereas for instance for Malayalam, these numbers range from 47.4 to 63.6. Kannada hardest to translate: When comparing more in ge"
2021.wat-1.21,ahrenberg-2017-comparing,0,0.0241379,"sentence in English along with its translation and transliteration in the four Dravidian languages. sentences (henceforth referred to as UFAL). They also reported that applying pre-processing steps involving morphological rules based on Tamil suffixes improved the BLEU score of the baseline model to a small extent (from 9.42 to 9.77). For the Indic languages multilingual tasks of WAT2018, the Phrasal-based SMT system of Ojha et al. (2018) with a BLEU score of 30.53. Subsequent papers also focused on SMT systems for Malayalam and Telugu with some notable work including: (Anto and Nisha, 2016; Sreelekha and Bhattacharyya, 2017, 2018) for Malayalam and (Lingam et al., 2014; Yadav and Lingam, 2017) for Telugu. findings was also reported by Ramesh et al. (2020) for Tamil and Dandapat and Federmann (2018) for Telugu . To the best of our knowledge and as of 2021, there has not been any scientific publication involving translation to and from Kannada, except for Chakravarthi et al. (2019). One possible reason for this could be the fact that sizeable corpora involving Kannada (i.e. in the order of magnitude of at least thousand sentences) have been readily available only since 2019, with the release of the JW300 Corpus (A"
2021.wat-1.21,L18-1413,0,0.0492773,"Missing"
2021.wat-1.21,tiedemann-2012-parallel,0,0.0136984,"mprovement on the English-Turkish translation task of WMT18. Given the encouraging results reported on the agglutinative Turkish language, we hypothesise that translation into Dravidian languages may also benefit from a linguistically motivated segmenter, and evaluate LMVR against SP across varying vocabulary sizes. 4 Experimental Setup 4.1 Training Corpora The parallel training data is mostly taken from the datasets available for the MultiIndicMT task from WAT 2021. If a certain dataset is not available from the MultiIndicMT training repository, we resorted to extract that dataset from OPUS (Tiedemann, 2012) or WMT20. Table 2 reports on the datasets that we used along with their domain and their source. After extracting and cleaning the data (see below), approximately 8 million English tokens and their corresponding target language tokens are selected as our training corpora. We fixed the number of source tokens across language pairs in or183 Target Language Kannada Malayalam Tamil Telugu Tokens(k) EN Tokens(k) Sentences(k) Source/Target Token Ratio 817 1153 1171 1027 7791 7973 7854 7872 361 458 345 385 9.53 6.91 6.71 7.67 Table 3: Approximate sizes (in thousands) of the parallel training corpora"
C10-2018,W05-1008,0,0.461994,"Missing"
C10-2018,P98-1014,0,0.77487,"Missing"
C10-2018,J93-2002,0,0.168881,"ifier fails to capture the adjective type of a given psp. To account for it, all words predicted to be past participles but not adjectives are assigned two additional adjective types– one with the nonadv and one with the adv feature. For reasons explained later on, a type with the padv feature is not added. After the application of these techniques, all cases of words wrongly predicted to be verbs or adjectives have been eliminated. 4.2 Guessing Subcategorization Frames Our next step is to guess the correct subcategorization feature for verbs. Learning the proper subcat frame is well studied (Brent, 1993; Manning, 1993; Briscoe and Caroll, 1997; Kinyon and Prolo, 2002; O’Donovan et al., 2005). Most of the work follows the ‘classical’ Briscoe and Caroll (1997) approach where the verb and the subcategorized complements are extracted from the output analyses of a probabilistic parser and stored as syntactic patterns. Further, some statistical techniques are applied to select the most probable frames out of the proposed syntactic patterns. Following the observations made in Korhonen et al. (2000), Lapata (1999) and Messiant (2008), we employ a maximum likelihood estimate (MLE) from observed relat"
C10-2018,A97-1052,0,0.0622223,"djective type of a given psp. To account for it, all words predicted to be past participles but not adjectives are assigned two additional adjective types– one with the nonadv and one with the adv feature. For reasons explained later on, a type with the padv feature is not added. After the application of these techniques, all cases of words wrongly predicted to be verbs or adjectives have been eliminated. 4.2 Guessing Subcategorization Frames Our next step is to guess the correct subcategorization feature for verbs. Learning the proper subcat frame is well studied (Brent, 1993; Manning, 1993; Briscoe and Caroll, 1997; Kinyon and Prolo, 2002; O’Donovan et al., 2005). Most of the work follows the ‘classical’ Briscoe and Caroll (1997) approach where the verb and the subcategorized complements are extracted from the output analyses of a probabilistic parser and stored as syntactic patterns. Further, some statistical techniques are applied to select the most probable frames out of the proposed syntactic patterns. Following the observations made in Korhonen et al. (2000), Lapata (1999) and Messiant (2008), we employ a maximum likelihood estimate (MLE) from observed relative frequencies with an empirical thresho"
C10-2018,R09-1012,1,0.440748,"Missing"
C10-2018,W08-1708,1,0.452842,"Missing"
C10-2018,copestake-flickinger-2000-open,0,0.76085,"Missing"
C10-2018,W00-0740,0,0.319367,"Missing"
C10-2018,E03-1041,0,0.533249,"Missing"
C10-2018,kinyon-prolo-2002-identifying,0,0.0141416,"sp. To account for it, all words predicted to be past participles but not adjectives are assigned two additional adjective types– one with the nonadv and one with the adv feature. For reasons explained later on, a type with the padv feature is not added. After the application of these techniques, all cases of words wrongly predicted to be verbs or adjectives have been eliminated. 4.2 Guessing Subcategorization Frames Our next step is to guess the correct subcategorization feature for verbs. Learning the proper subcat frame is well studied (Brent, 1993; Manning, 1993; Briscoe and Caroll, 1997; Kinyon and Prolo, 2002; O’Donovan et al., 2005). Most of the work follows the ‘classical’ Briscoe and Caroll (1997) approach where the verb and the subcategorized complements are extracted from the output analyses of a probabilistic parser and stored as syntactic patterns. Further, some statistical techniques are applied to select the most probable frames out of the proposed syntactic patterns. Following the observations made in Korhonen et al. (2000), Lapata (1999) and Messiant (2008), we employ a maximum likelihood estimate (MLE) from observed relative frequencies with an empirical threshold to filter out low pro"
C10-2018,W00-1325,0,0.0954238,"p is to guess the correct subcategorization feature for verbs. Learning the proper subcat frame is well studied (Brent, 1993; Manning, 1993; Briscoe and Caroll, 1997; Kinyon and Prolo, 2002; O’Donovan et al., 2005). Most of the work follows the ‘classical’ Briscoe and Caroll (1997) approach where the verb and the subcategorized complements are extracted from the output analyses of a probabilistic parser and stored as syntactic patterns. Further, some statistical techniques are applied to select the most probable frames out of the proposed syntactic patterns. Following the observations made in Korhonen et al. (2000), Lapata (1999) and Messiant (2008), we employ a maximum likelihood estimate (MLE) from observed relative frequencies with an empirical threshold to filter out low probability frames. For each word predicted to be a verb, we look up the verb types assigned to it during the parsing with universal types. Then, the MLE for each subcat frame is determined and only frames with MLE of 0.2 and above are considered. For example, jammert (to moan.3SG.PRES) is assigned a single type– verb(hebben,sg3,intransitive). However, the correct subcat features for it are intransitive and sbar. Here is the list of"
C10-2018,P99-1051,0,0.258628,"t subcategorization feature for verbs. Learning the proper subcat frame is well studied (Brent, 1993; Manning, 1993; Briscoe and Caroll, 1997; Kinyon and Prolo, 2002; O’Donovan et al., 2005). Most of the work follows the ‘classical’ Briscoe and Caroll (1997) approach where the verb and the subcategorized complements are extracted from the output analyses of a probabilistic parser and stored as syntactic patterns. Further, some statistical techniques are applied to select the most probable frames out of the proposed syntactic patterns. Following the observations made in Korhonen et al. (2000), Lapata (1999) and Messiant (2008), we employ a maximum likelihood estimate (MLE) from observed relative frequencies with an empirical threshold to filter out low probability frames. For each word predicted to be a verb, we look up the verb types assigned to it during the parsing with universal types. Then, the MLE for each subcat frame is determined and only frames with MLE of 0.2 and above are considered. For example, jammert (to moan.3SG.PRES) is assigned a single type– verb(hebben,sg3,intransitive). However, the correct subcat features for it are intransitive and sbar. Here is the list of all verb types"
C10-2018,W02-2018,0,0.655192,"y our method universal types. The adjectives can be used as adverbs in Dutch and thus, we do not consider the latter to be an open class. We employ a ME-based classifier which, for some unknown word, takes various morphological and syntactic features as input and outputs lexical types. The probability of a lexical type t, given an unknown word and its context c is: (1) P exp( i Θi fi (t,c)) P exp( i Θi fi (t′ ,c)) t′ ∈T p(t|c) = P where fi (t, c) may encode arbitrary characteristics of the context and &lt; Θ1 , Θ2 , ... &gt; can be evaluated by maximising the pseudo-likelihood on a training corpus (Malouf, 2002). Table 1 shows the features for the noun inspraakprocedures (consultation procedures). Row (i) contains 4 separate features derived from the prefix of the word and 4 other suffix features are given in row (ii). The two features in rows (iii) and (iv) indicate whether the word starts with a particle and if it contains a hyphen, respectively. Another source of morphological features is the paradigm of the unknown word which provides information that is otherwise inaccessible. For example, in Dutch, neuter nouns always take the het definite article while all other noun forms are used with the de"
C10-2018,P08-3010,0,0.133178,"n feature for verbs. Learning the proper subcat frame is well studied (Brent, 1993; Manning, 1993; Briscoe and Caroll, 1997; Kinyon and Prolo, 2002; O’Donovan et al., 2005). Most of the work follows the ‘classical’ Briscoe and Caroll (1997) approach where the verb and the subcategorized complements are extracted from the output analyses of a probabilistic parser and stored as syntactic patterns. Further, some statistical techniques are applied to select the most probable frames out of the proposed syntactic patterns. Following the observations made in Korhonen et al. (2000), Lapata (1999) and Messiant (2008), we employ a maximum likelihood estimate (MLE) from observed relative frequencies with an empirical threshold to filter out low probability frames. For each word predicted to be a verb, we look up the verb types assigned to it during the parsing with universal types. Then, the MLE for each subcat frame is determined and only frames with MLE of 0.2 and above are considered. For example, jammert (to moan.3SG.PRES) is assigned a single type– verb(hebben,sg3,intransitive). However, the correct subcat features for it are intransitive and sbar. Here is the list of all verb types assigned to jammert"
C10-2018,J05-3003,0,0.0327204,"Missing"
C10-2018,W01-1815,1,0.913638,"Missing"
C10-2018,2006.jeptalnrecital-invite.2,1,0.864752,"Missing"
C10-2018,zhang-kordoni-2006-automated,0,0.848189,"Missing"
C10-2018,P93-1032,0,\N,Missing
C10-2018,C98-1014,0,\N,Missing
C14-1131,J92-4003,0,0.225544,"ncy language model. By leveraging syntax-based context, resulting clusters are better when evaluated against a wordnet for Dutch. The improvements are stable across parameters such as number of clusters, minimum frequency and granularity. Further refinement is possible through dependency relation selection. Our approach achieves a desired clustering quality with less data, resulting in a decrease in cluster creation times. 1 Introduction Semi-supervised approaches have been successful in various areas of natural language processing. Among a plethora of clustering techniques, Brown clustering (Brown et al., 1992) is popular for its conceptual simplicity, available implementations (Liang, 2005; Stolcke, 2002), and because the resulting word clusters can be helpful for several tasks. Clusters are used as syntactic and semantic generalizations of words, requiring fewer model parameters. Brown clustering (section 2) groups words based on shared context. However, only immediately adjacent words are taken into account as recognized e.g. by Koo et al. (2008), Sagae and Gordon (2009), and Grave et al. (2013). For example, even though verbs constitute an informative context for object nouns, they are rarely co"
C14-1131,W09-3821,0,0.0523247,"Missing"
C14-1131,P01-1017,0,0.00832614,"itself easily to a hierarchy, which is often exploited during feature creation in supervised learning to control cluster granularity (see the end of section 5.2) 1383 3 Extension of the Brown clustering The bigram language model underlying Brown clustering takes the probability of a sentence as the product of probabilities of words based on immediately preceding words. In contrast, we replace this by a dependency language model (DLM), which defines the probability of a sentence over dependency trees (Shen et al., 2008). This probability can be factorized in different ways (Chen et al., 2012; Charniak, 2001; Popel and Mareˇcek, 2010), but the common idea is that a word is conditioned on some history, where the link between the two is a dependency. In practice, the history can include the immediate parent of the word, which can be either a lexical head or the artificial root node, as well as siblings between the child and the parent. Our take on DLM is similar to Charniak (2001) and Popel and Mareˇcek (2010): the probability of a word is conditioned simply on its parent. This is the same view as taken by Grave et al. (2013). The Brown clustering objective is to find such a deterministic clusterin"
C14-1131,P12-1023,0,0.0184731,"not build nor lends itself easily to a hierarchy, which is often exploited during feature creation in supervised learning to control cluster granularity (see the end of section 5.2) 1383 3 Extension of the Brown clustering The bigram language model underlying Brown clustering takes the probability of a sentence as the product of probabilities of words based on immediately preceding words. In contrast, we replace this by a dependency language model (DLM), which defines the probability of a sentence over dependency trees (Shen et al., 2008). This probability can be factorized in different ways (Chen et al., 2012; Charniak, 2001; Popel and Mareˇcek, 2010), but the common idea is that a word is conditioned on some history, where the link between the two is a dependency. In practice, the history can include the immediate parent of the word, which can be either a lexical head or the artificial root node, as well as siblings between the child and the parent. Our take on DLM is similar to Charniak (2001) and Popel and Mareˇcek (2010): the probability of a word is conditioned simply on its parent. This is the same view as taken by Grave et al. (2013). The Brown clustering objective is to find such a determi"
C14-1131,I11-1041,0,0.0154978,"and Crabb´e, 2009; Haffari et al., 2011), named-entity recognition (NER) and chunking (Turian et al., 2010), sentiment analysis (Popat et al., 2013), relation extraction (Plank and Moschitti, 2013), unsupervised semantic role labeling (Titov and Klementiev, 2012), question answering (Momtazi et al., 2010), POS tagging (Owoputi et al., 2013) and speech recognition with recursive neural networks (Shi et al., 2013). Recently, multilingual clustering has also been proposed (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). Among the most frequently recognized limitations (cf. Koo et al. (2008); Chrupala (2011)) are a) the hard nature of the clustering, b) relatively long running time2 and c) insensitivity to wider context. Our method attempts to overcome the final disadvantage. As it requires less data, it also reduces the running time. Leveraging syntactic context for word representations has been explored, among others, in Lin (1998) on distributional thesauri; Haffari et al. (2011) on combining Brown clusters and word groupings from split non-terminals; Sagae and Gordon (2009) on using unlexicalized syntactic context in hierarchical clustering; Van de Cruys (2010) and Pad´o and Lapata (2007) on"
C14-1131,P13-2136,0,0.0138327,"ering has been used extensively in supervised NLP tasks such as parsing (Koo et al., 2008; Candito and Crabb´e, 2009; Haffari et al., 2011), named-entity recognition (NER) and chunking (Turian et al., 2010), sentiment analysis (Popat et al., 2013), relation extraction (Plank and Moschitti, 2013), unsupervised semantic role labeling (Titov and Klementiev, 2012), question answering (Momtazi et al., 2010), POS tagging (Owoputi et al., 2013) and speech recognition with recursive neural networks (Shi et al., 2013). Recently, multilingual clustering has also been proposed (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). Among the most frequently recognized limitations (cf. Koo et al. (2008); Chrupala (2011)) are a) the hard nature of the clustering, b) relatively long running time2 and c) insensitivity to wider context. Our method attempts to overcome the final disadvantage. As it requires less data, it also reduces the running time. Leveraging syntactic context for word representations has been explored, among others, in Lin (1998) on distributional thesauri; Haffari et al. (2011) on combining Brown clusters and word groupings from split non-terminals; Sagae and Gordon (2009) on using unlexicalized syntact"
C14-1131,W13-3511,0,0.0930266,"ous areas of natural language processing. Among a plethora of clustering techniques, Brown clustering (Brown et al., 1992) is popular for its conceptual simplicity, available implementations (Liang, 2005; Stolcke, 2002), and because the resulting word clusters can be helpful for several tasks. Clusters are used as syntactic and semantic generalizations of words, requiring fewer model parameters. Brown clustering (section 2) groups words based on shared context. However, only immediately adjacent words are taken into account as recognized e.g. by Koo et al. (2008), Sagae and Gordon (2009), and Grave et al. (2013). For example, even though verbs constitute an informative context for object nouns, they are rarely considered in Brown clustering, unlike in dependency-based clustering. The difference between the contexts can be illustrated with the following example: bigram contexts dependency contexts The method repeatedly samples the data The bigram context thus fails to capture the relation between the object data and the predicate samples, as well as the one between the subject method and the predicate. Furthermore, the dependency representation rightly ignores some of the less informative contexts com"
C14-1131,P11-2125,0,0.018744,"t frequent words into distinct clusters. Then, the k+1th most frequent word is assigned to a new cluster, and two among the resulting k+1 clusters are merged, i.e. the pair that maximizes the average mutual information of the current clustering. This process is repeated until all words have been merged. The resulting k clusters are then merged to build the binary tree. The version of the algorithm optimized for speed runs in O(k 2 |V|), with |V |the vocabulary size. Brown clustering has been used extensively in supervised NLP tasks such as parsing (Koo et al., 2008; Candito and Crabb´e, 2009; Haffari et al., 2011), named-entity recognition (NER) and chunking (Turian et al., 2010), sentiment analysis (Popat et al., 2013), relation extraction (Plank and Moschitti, 2013), unsupervised semantic role labeling (Titov and Klementiev, 2012), question answering (Momtazi et al., 2010), POS tagging (Owoputi et al., 2013) and speech recognition with recursive neural networks (Shi et al., 2013). Recently, multilingual clustering has also been proposed (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). Among the most frequently recognized limitations (cf. Koo et al. (2008); Chrupala (2011)) are a) the hard nature o"
C14-1131,W11-0315,0,0.0143439,"lustering, and only required a simple modification of the Brown clustering code. 1 We are using the term semantic relatedness in its broadest possible scope. Words or clusters are semantically related when they have any kind of semantic relation: synonymy, meronymy, antonymy, hypernymy etc. (Turney and Pantel, 2010). 2 Although coarser clustering (k&lt;1000) can mean more practical running times, as the clustering depends quadratically on k. 3 This approach allows to capture homonymy/polysemy, with the idea that when a word representation is needed, it can be obtained in a context-sensitive way (Huang et al., 2011; Nepal and Yates, 2014). This is certainly an important advantage over Brown clustering in which the mapping between a word and a cluster is deterministic; however, it comes with its own disadvantages: creating context-sensitive representations requires (potentially) costly inference; furthermore, HMM-based clustering does not build nor lends itself easily to a hierarchy, which is often exploited during feature creation in supervised learning to control cluster granularity (see the end of section 5.2) 1383 3 Extension of the Brown clustering The bigram language model underlying Brown clusteri"
C14-1131,P08-1068,0,0.340014,"pervised approaches have been successful in various areas of natural language processing. Among a plethora of clustering techniques, Brown clustering (Brown et al., 1992) is popular for its conceptual simplicity, available implementations (Liang, 2005; Stolcke, 2002), and because the resulting word clusters can be helpful for several tasks. Clusters are used as syntactic and semantic generalizations of words, requiring fewer model parameters. Brown clustering (section 2) groups words based on shared context. However, only immediately adjacent words are taken into account as recognized e.g. by Koo et al. (2008), Sagae and Gordon (2009), and Grave et al. (2013). For example, even though verbs constitute an informative context for object nouns, they are rarely considered in Brown clustering, unlike in dependency-based clustering. The difference between the contexts can be illustrated with the following example: bigram contexts dependency contexts The method repeatedly samples the data The bigram context thus fails to capture the relation between the object data and the predicate samples, as well as the one between the subject method and the predicate. Furthermore, the dependency representation rightly"
C14-1131,N04-1043,0,0.122611,"Missing"
C14-1131,N10-1046,0,0.0237864,"ated until all words have been merged. The resulting k clusters are then merged to build the binary tree. The version of the algorithm optimized for speed runs in O(k 2 |V|), with |V |the vocabulary size. Brown clustering has been used extensively in supervised NLP tasks such as parsing (Koo et al., 2008; Candito and Crabb´e, 2009; Haffari et al., 2011), named-entity recognition (NER) and chunking (Turian et al., 2010), sentiment analysis (Popat et al., 2013), relation extraction (Plank and Moschitti, 2013), unsupervised semantic role labeling (Titov and Klementiev, 2012), question answering (Momtazi et al., 2010), POS tagging (Owoputi et al., 2013) and speech recognition with recursive neural networks (Shi et al., 2013). Recently, multilingual clustering has also been proposed (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). Among the most frequently recognized limitations (cf. Koo et al. (2008); Chrupala (2011)) are a) the hard nature of the clustering, b) relatively long running time2 and c) insensitivity to wider context. Our method attempts to overcome the final disadvantage. As it requires less data, it also reduces the running time. Leveraging syntactic context for word representations has be"
C14-1131,2006.jeptalnrecital-invite.2,1,0.562775,"Missing"
C14-1131,N13-1039,0,0.042478,"Missing"
C14-1131,J07-2002,0,0.123812,"Missing"
C14-1131,P13-1147,0,0.0140319,"erged, i.e. the pair that maximizes the average mutual information of the current clustering. This process is repeated until all words have been merged. The resulting k clusters are then merged to build the binary tree. The version of the algorithm optimized for speed runs in O(k 2 |V|), with |V |the vocabulary size. Brown clustering has been used extensively in supervised NLP tasks such as parsing (Koo et al., 2008; Candito and Crabb´e, 2009; Haffari et al., 2011), named-entity recognition (NER) and chunking (Turian et al., 2010), sentiment analysis (Popat et al., 2013), relation extraction (Plank and Moschitti, 2013), unsupervised semantic role labeling (Titov and Klementiev, 2012), question answering (Momtazi et al., 2010), POS tagging (Owoputi et al., 2013) and speech recognition with recursive neural networks (Shi et al., 2013). Recently, multilingual clustering has also been proposed (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). Among the most frequently recognized limitations (cf. Koo et al. (2008); Chrupala (2011)) are a) the hard nature of the clustering, b) relatively long running time2 and c) insensitivity to wider context. Our method attempts to overcome the final disadvantage. As it requi"
C14-1131,W10-2105,1,0.838517,"Missing"
C14-1131,P13-1041,0,0.0132794,"two among the resulting k+1 clusters are merged, i.e. the pair that maximizes the average mutual information of the current clustering. This process is repeated until all words have been merged. The resulting k clusters are then merged to build the binary tree. The version of the algorithm optimized for speed runs in O(k 2 |V|), with |V |the vocabulary size. Brown clustering has been used extensively in supervised NLP tasks such as parsing (Koo et al., 2008; Candito and Crabb´e, 2009; Haffari et al., 2011), named-entity recognition (NER) and chunking (Turian et al., 2010), sentiment analysis (Popat et al., 2013), relation extraction (Plank and Moschitti, 2013), unsupervised semantic role labeling (Titov and Klementiev, 2012), question answering (Momtazi et al., 2010), POS tagging (Owoputi et al., 2013) and speech recognition with recursive neural networks (Shi et al., 2013). Recently, multilingual clustering has also been proposed (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). Among the most frequently recognized limitations (cf. Koo et al. (2008); Chrupala (2011)) are a) the hard nature of the clustering, b) relatively long running time2 and c) insensitivity to wider context. Our method attempt"
C14-1131,W09-3829,0,0.113362,"have been successful in various areas of natural language processing. Among a plethora of clustering techniques, Brown clustering (Brown et al., 1992) is popular for its conceptual simplicity, available implementations (Liang, 2005; Stolcke, 2002), and because the resulting word clusters can be helpful for several tasks. Clusters are used as syntactic and semantic generalizations of words, requiring fewer model parameters. Brown clustering (section 2) groups words based on shared context. However, only immediately adjacent words are taken into account as recognized e.g. by Koo et al. (2008), Sagae and Gordon (2009), and Grave et al. (2013). For example, even though verbs constitute an informative context for object nouns, they are rarely considered in Brown clustering, unlike in dependency-based clustering. The difference between the contexts can be illustrated with the following example: bigram contexts dependency contexts The method repeatedly samples the data The bigram context thus fails to capture the relation between the object data and the predicate samples, as well as the one between the subject method and the predicate. Furthermore, the dependency representation rightly ignores some of the less"
C14-1131,P08-1066,0,0.0381511,"es (potentially) costly inference; furthermore, HMM-based clustering does not build nor lends itself easily to a hierarchy, which is often exploited during feature creation in supervised learning to control cluster granularity (see the end of section 5.2) 1383 3 Extension of the Brown clustering The bigram language model underlying Brown clustering takes the probability of a sentence as the product of probabilities of words based on immediately preceding words. In contrast, we replace this by a dependency language model (DLM), which defines the probability of a sentence over dependency trees (Shen et al., 2008). This probability can be factorized in different ways (Chen et al., 2012; Charniak, 2001; Popel and Mareˇcek, 2010), but the common idea is that a word is conditioned on some history, where the link between the two is a dependency. In practice, the history can include the immediate parent of the word, which can be either a lexical head or the artificial root node, as well as siblings between the child and the parent. Our take on DLM is similar to Charniak (2001) and Popel and Mareˇcek (2010): the probability of a word is conditioned simply on its parent. This is the same view as taken by Grav"
C14-1131,P11-1053,0,0.0200785,"ges 1382–1391, Dublin, Ireland, August 23-29 2014. algorithm, and—although we assume that syntactically parsed text is available—it requires much less data for a desired level of clustering quality. 2 The Brown clustering algorithm Brown clustering (Brown et al., 1992) is an agglomerative algorithm that induces a hierarchical clustering of words. It takes a tokenized corpus and groups words into k clusters identified by bit strings, representing paths in the induced binary tree in which the leaves are word clusters. Prefixes of the paths can be used to achieve clusters of coarser granularity (Sun et al., 2011; Turian et al., 2010). The obtained clusters contain words that are semantically related, or are paradigmatic or orthographic variants.1 The algorithm starts by putting k most frequent words into distinct clusters. Then, the k+1th most frequent word is assigned to a new cluster, and two among the resulting k+1 clusters are merged, i.e. the pair that maximizes the average mutual information of the current clustering. This process is repeated until all words have been merged. The resulting k clusters are then merged to build the binary tree. The version of the algorithm optimized for speed runs"
C14-1131,N12-1052,0,0.0542988,"Missing"
C14-1131,E12-1003,0,0.0145585,"n of the current clustering. This process is repeated until all words have been merged. The resulting k clusters are then merged to build the binary tree. The version of the algorithm optimized for speed runs in O(k 2 |V|), with |V |the vocabulary size. Brown clustering has been used extensively in supervised NLP tasks such as parsing (Koo et al., 2008; Candito and Crabb´e, 2009; Haffari et al., 2011), named-entity recognition (NER) and chunking (Turian et al., 2010), sentiment analysis (Popat et al., 2013), relation extraction (Plank and Moschitti, 2013), unsupervised semantic role labeling (Titov and Klementiev, 2012), question answering (Momtazi et al., 2010), POS tagging (Owoputi et al., 2013) and speech recognition with recursive neural networks (Shi et al., 2013). Recently, multilingual clustering has also been proposed (T¨ackstr¨om et al., 2012; Faruqui and Dyer, 2013). Among the most frequently recognized limitations (cf. Koo et al. (2008); Chrupala (2011)) are a) the hard nature of the clustering, b) relatively long running time2 and c) insensitivity to wider context. Our method attempts to overcome the final disadvantage. As it requires less data, it also reduces the running time. Leveraging syntac"
C14-1131,P10-1040,0,0.207226,"lin, Ireland, August 23-29 2014. algorithm, and—although we assume that syntactically parsed text is available—it requires much less data for a desired level of clustering quality. 2 The Brown clustering algorithm Brown clustering (Brown et al., 1992) is an agglomerative algorithm that induces a hierarchical clustering of words. It takes a tokenized corpus and groups words into k clusters identified by bit strings, representing paths in the induced binary tree in which the leaves are word clusters. Prefixes of the paths can be used to achieve clusters of coarser granularity (Sun et al., 2011; Turian et al., 2010). The obtained clusters contain words that are semantically related, or are paradigmatic or orthographic variants.1 The algorithm starts by putting k most frequent words into distinct clusters. Then, the k+1th most frequent word is assigned to a new cluster, and two among the resulting k+1 clusters are merged, i.e. the pair that maximizes the average mutual information of the current clustering. This process is repeated until all words have been merged. The resulting k clusters are then merged to build the binary tree. The version of the algorithm optimized for speed runs in O(k 2 |V|), with |"
C90-2052,T87-1042,0,0.0697824,"additional c o m p o n e n t s (not necessarily reversible) to select tile best translation. Not only is it pos.siblc to build reversible (modules of) MT systems; it has also been claimed that reversible systems are preferable. For example Isabelle [6] clahns that reversible M T systems are to be preferred to oth(:rs be(:~nlse in reversible M T systems a better understanding of the translation re.lation is achieved; such systems will eventually exhibit better practicM performance. Moreover, the a r g u m e n t s in favour of using bidirectional g r a m m a r s in NL1 ), such as those given in [1, 8] carry over to translation as well. Because o[ tile declarative nature of unification- and logic g r a m m a r formMisms g r a m m a r s written in these formalisms are increasingly used in a bidirectional way, thus the sa.me g r a m m a r is used for both parsing and generation. Some recent developments are reported in [3, 24, 16, 21, 2, 18, 19, 22, 20]. In this paper I will show how to use such bidirectional unification g r a m m a r s to build a completely reversible, multilingual, M T system. For each language there is a unification g r a m m a r t h a t defines a reversible relation betwe"
C90-2052,1988.tmi-1.12,0,0.779834,"in reversible M T systems a better understanding of the translation re.lation is achieved; such systems will eventually exhibit better practicM performance. Moreover, the a r g u m e n t s in favour of using bidirectional g r a m m a r s in NL1 ), such as those given in [1, 8] carry over to translation as well. Because o[ tile declarative nature of unification- and logic g r a m m a r formMisms g r a m m a r s written in these formalisms are increasingly used in a bidirectional way, thus the sa.me g r a m m a r is used for both parsing and generation. Some recent developments are reported in [3, 24, 16, 21, 2, 18, 19, 22, 20]. In this paper I will show how to use such bidirectional unification g r a m m a r s to build a completely reversible, multilingual, M T system. For each language there is a unification g r a m m a r t h a t defines a reversible relation between strings and language d e p e n d e n t lneaning representations (logical forms ). Moreover, for each language pair (or set of languages) there is a unification g r a m m a r that defines a reversible relation between such language d e p e n d e n t logicM forms. Translation is thus detined by a series of three unification granrmars. A specific version"
C90-2052,P89-1029,0,0.762576,"ds, the input should not be extended (coherence) and should completely be derived (completeness). This usage of the terms completeness and coherence was introduced in [24]. In the following I will discuss ways to obtain computability of one such partition. It is well known that relations defined by unrestricted unification grammars are not computable in general a~s 301 such grammars have Turing power [13]; it is thus not decidable whether the relation is defined for some given input. Usually some constraint on grammars is defined to remedy this. For example the off-line-parsability constraint [13, 5] ensures that the recognition problem is solvable. Moreover this constraint also implies that the parsing problem as defined here is computable; i.e. the proof procedure will always terminate (because the constraint implies that there is a limit to the depth of possible parse trees for all strings of a given length). llowever the off-line-parsability constraint assumes a context-free base of the formalism. A generalization of the off-line-parsability constraint for any binary relation defined by unification grammars will consist of three parts; the first and third of these parts are usually im"
C90-2052,1989.mtsummit-1.33,0,0.289381,"y require other sources of knowledge, either incorporated in the systenl or provided (interactively) by the. user. T h e relation 'possible translation' is s y m m e t r i c whereas the relation 'best translation' is not. T h u s an M T system may consist of a reversible core, implementi~:g the s y m m e t r i c relation ~possible translation', and additional c o m p o n e n t s (not necessarily reversible) to select tile best translation. Not only is it pos.siblc to build reversible (modules of) MT systems; it has also been claimed that reversible systems are preferable. For example Isabelle [6] clahns that reversible M T systems are to be preferred to oth(:rs be(:~nlse in reversible M T systems a better understanding of the translation re.lation is achieved; such systems will eventually exhibit better practicM performance. Moreover, the a r g u m e n t s in favour of using bidirectional g r a m m a r s in NL1 ), such as those given in [1, 8] carry over to translation as well. Because o[ tile declarative nature of unification- and logic g r a m m a r formMisms g r a m m a r s written in these formalisms are increasingly used in a bidirectional way, thus the sa.me g r a m m a r is use"
C90-2052,C88-1053,0,0.0180275,"relaxation the system will eventually lind the good translations, but it may take a while. On the other hand, if we are to mention all (possibly unbounded) reentrancies explicitly then the transfer grammar will have to be complicated by a threading mechanism to derive such reen trancies. Again, the specific use o:[ reentrancies in the logical forms that are defined will deterlnine whether this relaxation is desired or not. 4 Final remarks The objective to build a reversible MT system using a series of unification grammars is similar to the objective of the C R I T T E R system as expressed in [3, 7], and the work of Zajac in [25]. Instead of using unification grammars C R I T T E R uses logic grammars; Zajac uses a type system including an inheritance mechanism to define transfer-like rules. In these two approaches less attention is being paid to an exact definition of reversibility; although our work may be compatible with these approaches. A somewhat different approach is advocated in [9]. In that approach a system is described where an I, FG grammar for some source language is augnlented with equations that define (part of) the target level representations. A generator derives from th"
C90-2052,E89-1037,0,0.0776578,"ation is desired or not. 4 Final remarks The objective to build a reversible MT system using a series of unification grammars is similar to the objective of the C R I T T E R system as expressed in [3, 7], and the work of Zajac in [25]. Instead of using unification grammars C R I T T E R uses logic grammars; Zajac uses a type system including an inheritance mechanism to define transfer-like rules. In these two approaches less attention is being paid to an exact definition of reversibility; although our work may be compatible with these approaches. A somewhat different approach is advocated in [9]. In that approach a system is described where an I, FG grammar for some source language is augnlented with equations that define (part of) the target level representations. A generator derives from this partial description a string according to some LFG grammar of the target language. Instead of a series of three grammars this architecture thus assumes two grammars, one of which both defines the source language and the relation with the target language. The translation relation is not only defined between logical forms but may relate ~ll levels of representation ( c.structure, f-structure, a-"
C90-2052,P83-1021,0,0.68114,"e fl all feature strnctures fy, such that r(fa, fy) and f~ and f3 are equivalent. In the case of strong equivalence this implies that f l ~ f3 (completeness), and fa U fl (coherence). In other words, the input should not be extended (coherence) and should completely be derived (completeness). This usage of the terms completeness and coherence was introduced in [24]. In the following I will discuss ways to obtain computability of one such partition. It is well known that relations defined by unrestricted unification grammars are not computable in general a~s 301 such grammars have Turing power [13]; it is thus not decidable whether the relation is defined for some given input. Usually some constraint on grammars is defined to remedy this. For example the off-line-parsability constraint [13, 5] ensures that the recognition problem is solvable. Moreover this constraint also implies that the parsing problem as defined here is computable; i.e. the proof procedure will always terminate (because the constraint implies that there is a limit to the depth of possible parse trees for all strings of a given length). llowever the off-line-parsability constraint assumes a context-free base of the fo"
C90-2052,C88-2128,0,0.42694,"in reversible M T systems a better understanding of the translation re.lation is achieved; such systems will eventually exhibit better practicM performance. Moreover, the a r g u m e n t s in favour of using bidirectional g r a m m a r s in NL1 ), such as those given in [1, 8] carry over to translation as well. Because o[ tile declarative nature of unification- and logic g r a m m a r formMisms g r a m m a r s written in these formalisms are increasingly used in a bidirectional way, thus the sa.me g r a m m a r is used for both parsing and generation. Some recent developments are reported in [3, 24, 16, 21, 2, 18, 19, 22, 20]. In this paper I will show how to use such bidirectional unification g r a m m a r s to build a completely reversible, multilingual, M T system. For each language there is a unification g r a m m a r t h a t defines a reversible relation between strings and language d e p e n d e n t lneaning representations (logical forms ). Moreover, for each language pair (or set of languages) there is a unification g r a m m a r that defines a reversible relation between such language d e p e n d e n t logicM forms. Translation is thus detined by a series of three unification granrmars. A specific version"
C90-2052,C88-2150,0,0.3612,"in reversible M T systems a better understanding of the translation re.lation is achieved; such systems will eventually exhibit better practicM performance. Moreover, the a r g u m e n t s in favour of using bidirectional g r a m m a r s in NL1 ), such as those given in [1, 8] carry over to translation as well. Because o[ tile declarative nature of unification- and logic g r a m m a r formMisms g r a m m a r s written in these formalisms are increasingly used in a bidirectional way, thus the sa.me g r a m m a r is used for both parsing and generation. Some recent developments are reported in [3, 24, 16, 21, 2, 18, 19, 22, 20]. In this paper I will show how to use such bidirectional unification g r a m m a r s to build a completely reversible, multilingual, M T system. For each language there is a unification g r a m m a r t h a t defines a reversible relation between strings and language d e p e n d e n t lneaning representations (logical forms ). Moreover, for each language pair (or set of languages) there is a unification g r a m m a r that defines a reversible relation between such language d e p e n d e n t logicM forms. Translation is thus detined by a series of three unification granrmars. A specific version"
C90-2052,P89-1001,0,0.0601199,"ally lind the good translations, but it may take a while. On the other hand, if we are to mention all (possibly unbounded) reentrancies explicitly then the transfer grammar will have to be complicated by a threading mechanism to derive such reen trancies. Again, the specific use o:[ reentrancies in the logical forms that are defined will deterlnine whether this relaxation is desired or not. 4 Final remarks The objective to build a reversible MT system using a series of unification grammars is similar to the objective of the C R I T T E R system as expressed in [3, 7], and the work of Zajac in [25]. Instead of using unification grammars C R I T T E R uses logic grammars; Zajac uses a type system including an inheritance mechanism to define transfer-like rules. In these two approaches less attention is being paid to an exact definition of reversibility; although our work may be compatible with these approaches. A somewhat different approach is advocated in [9]. In that approach a system is described where an I, FG grammar for some source language is augnlented with equations that define (part of) the target level representations. A generator derives from this partial description a string"
C90-2052,E89-1032,0,\N,Missing
C90-2052,C88-1054,0,\N,Missing
C90-2052,P89-1002,1,\N,Missing
C92-2105,C90-2050,1,0.895681,"Missing"
C92-2105,P91-1015,1,0.862057,"Missing"
C92-2105,T87-1041,0,0.0118436,"ke decisions about lexieal and syntactic choices. Hence, the conceptual component would need detailed information about the language to use. But this would blur the distinction between the g r a m m a t ical and the conceptual level, because this would imply that both components share the g r a m m a r (see also Appelt (1989), Meteer (1990), Neumann (1991)). 1 In order to maintain a modular design additional mechanisms are necessary to perform some monitoring of the generator&apos;s output. Several authors argue for such additional mechanisms (Jameson and Wahlster, 1982; De Smedt and Kempen, 1987; Joshi, 1987; Levelt, 1989). For example, Levelt (1989) pointed out tbat &quot;speakers monitor what they are saying and how they are saying it&quot;. In particular he shows t h a t a speaker is also able to note that what she is saying involves a potential ambiguity for the hearer and can handle this problem by means of selfmonitoring. In this paper we describe an approach for self-monitoring which allows to generate unambiguous utterances in such situations where possible misunderstandings by tire user have to be avoided. The proposed method is based on a very strict integration of parsing and generation. During"
C92-2105,T87-1042,0,\N,Missing
C94-1039,J91-3003,0,0.102308,"an take part in unbounded dependency construelions. Lexical treatments of the kind presented in Pollard and Sag (in press), chapter 9 assume that a lexlcal rule is responsible for &apos;moving&apos; Rather than formalizing the &apos;add-adjuncts&apos; rule as a lexical rule we propose to use recursive constraints on lexical categories. Such lexical constraints are then processed using delayed ewduation techniques, a Such an approach is more promising than an off-line approach that precomputes the effect lcf. Miller (1992) for a similar suggestions concerning French. 2inspired by Kasper (in preparation) aRefer to Carpenter (1991) for a proof of TurilLg equivalence of simple eategorial grammar with recurslve lexical rules. VI,;RBAI, VERBAL sc : P •S sere 1 :~ sc:l&apos;.( wod : ar~l : Sem~) : Sere0 ) .S va/: Sere .N(?&apos;Gq,: ~OIll Figure 3: A lexieal rule t h a t adds a single adjunct to the sul)cat list of &gt;t verb. In the. case of n ~uljuncts the rule applies n times. O P_.AI)Vt,;RIIIA L RI,~ST I1.~.1) V E ll.n I A L arglnuc : ~ttod : val[nuc [ : Soa arg resh&quot; : [ qI*oa : Q ] m~d : va&quot;nuclI ,&apos;~,~t,&quot; : 0 Figure 4: A restrictive, adverbial and an olmrator a(lverl)ial. Restrictive adverbials (such as locatives and time adverbia"
D10-1088,W05-1008,0,0.182436,"Missing"
D10-1088,R09-1012,1,0.851931,"Missing"
D10-1088,C10-2018,1,0.806783,"Missing"
D10-1088,W08-1708,1,0.911553,"Missing"
D10-1088,copestake-flickinger-2000-open,0,0.24555,"Missing"
D10-1088,W09-2609,1,0.886194,"Missing"
D10-1088,E03-1041,0,0.288165,"Missing"
D10-1088,W00-1325,0,0.158629,"Missing"
D10-1088,P99-1051,0,0.116477,"Missing"
D10-1088,W02-2018,0,0.0482397,"t|c) = P where fi (t, c) may encode arbitrary characteristics of the context and &lt; Θ1 , Θ2 , ... > is a weighting parameter which maximises the entropy and can be 1 The adjectives can be used as adverbs in Dutch and thus, the latter are not considered to be an open class. 904 Features i) a, af, afw, afwa ii) r, er, ter, ater iii) particle yes #in this case af iv) hyphen no v) nounhhet,sgi, verbhsg1i vi) noun(het,count,sg), noun(de,count,pl) vii) noun(het), noun(count), noun(sg), noun(de) noun(pl) Table 1: Features for afwater evaluated by maximising the pseudo-likelihood on a training corpus (Malouf, 2002). Table 1 shows the features for afwater, the word we discussed in Section 1. Row (i) contains 4 separate features derived from the prefix of the word and 4 other suffix features are given in row (ii). The two features in rows (iii) and (iv) indicate whether the word starts with a particle and if it contains a hyphen, respectively. Further, the method we describe in Cholakov and van Noord (2009) is applied to generate the paradigm(s) of each word in question. This method uses a finite state morphology to generate possible paradigm(s) for a given word. The morphology does not have access to any"
D10-1088,P08-3010,0,0.157359,"Missing"
D10-1088,C08-1080,0,0.34613,"Missing"
D10-1088,W01-1815,1,0.842697,"Missing"
D10-1088,P06-1042,0,0.0588633,"Missing"
D10-1088,2005.jeptalnrecital-long.1,0,0.102009,"Missing"
D10-1088,P04-1057,1,0.814796,"Missing"
D10-1088,2006.jeptalnrecital-invite.2,1,0.918862,"Missing"
D10-1088,D07-1110,0,0.030427,"Missing"
D10-1088,zhang-kordoni-2006-automated,0,0.139379,"Missing"
D10-1088,W06-1206,0,0.0366679,"Missing"
D10-1088,sagot-etal-2006-lefff,0,\N,Missing
D18-1542,P11-1038,0,0.288385,"Missing"
D18-1542,D15-1157,0,0.0288462,"Missing"
D18-1542,W13-1101,0,0.0222535,"c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Original word Cand. 1 (p1) Cand. 2 (p2) Cand. 3 (p3) new new news knew (0.95) (0.03) (0.01) pix pix selfies pictures comming coming comming combing (0.79) (0.08) (0.06) tomoroe tomorrow tomoroe tomorrow’s (0.57) (0.43) (&lt;0.01) (0.54) (0.39) (0.02) Table 1: Output of the normalization model for the example sentence “new pix comming tomoroe” including candidate probabilities. Only the top-3 candidates are shown here. approach, as it focuses on adaptation of the training data (Foster et al., 2011; Khan et al., 2013; Kong et al., 2014; Blodgett et al., 2018). In the remainder of this section we will shortly review work which evaluated the effect of normalization on dependency parsing. Zhang et al. (2013) tune a normalization model for the parsing task, and show performance improvement on a silver treebank obtained from manually normalized data. Daiber and van der Goot (2016) use an existing normalization model as pre-processing for a graph-based dependency parser, and show a small but significant performance improvement. In the shared task of parsing the web (Petrov and McDonald, 2012) held at SANCL 2012"
D18-1542,Q16-1023,0,0.127969,"orward but novel approach in which the top-N best candidates provided by the normalization component are available to the parser. 1 • We show that using normalization as preprocessing improves parser performance for non-standard language, even if pre-trained embeddings and character level information are used. • We propose a novel technique to exploit the top-N candidates provided by the normalization component, and we show that this technique leads to a further increase in parser performance. Introduction Recently, neural network dependency parsers (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016) obtained state-of-the-art performance for dependency parsing. These parsers incorporate character level information (de Lhoneux et al., 2017a; Ballesteros et al., 2015; Nguyen et al., 2017) and can more easily exploit raw text in a semi-supervised setup. These new methods are especially beneficial for words not occurring in the training data. In practice, such unseen words often are spelling mistakes, or alternative spellings of known words. In more classical parsing models, these unseen words were usually clustered using ad-hoc rules. For non-standard domains, the number of unseen words is m"
D18-1542,D14-1108,0,0.116951,"m, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Original word Cand. 1 (p1) Cand. 2 (p2) Cand. 3 (p3) new new news knew (0.95) (0.03) (0.01) pix pix selfies pictures comming coming comming combing (0.79) (0.08) (0.06) tomoroe tomorrow tomoroe tomorrow’s (0.57) (0.43) (&lt;0.01) (0.54) (0.39) (0.02) Table 1: Output of the normalization model for the example sentence “new pix comming tomoroe” including candidate probabilities. Only the top-3 candidates are shown here. approach, as it focuses on adaptation of the training data (Foster et al., 2011; Khan et al., 2013; Kong et al., 2014; Blodgett et al., 2018). In the remainder of this section we will shortly review work which evaluated the effect of normalization on dependency parsing. Zhang et al. (2013) tune a normalization model for the parsing task, and show performance improvement on a silver treebank obtained from manually normalized data. Daiber and van der Goot (2016) use an existing normalization model as pre-processing for a graph-based dependency parser, and show a small but significant performance improvement. In the shared task of parsing the web (Petrov and McDonald, 2012) held at SANCL 2012, some teams used a"
D18-1542,K17-3001,0,0.0354513,"Missing"
D18-1542,P11-1068,0,0.0771915,"Missing"
D18-1542,W17-6314,0,0.0221216,"Missing"
D18-1542,P14-3012,0,0.0233387,"or these different actions, different modules are used to generate candidates, including: the Aspell spell checker2 , word embeddings and a lookup list generated from the training data. Features from these generation modules are complemented with N-gram features from canonical data and non-canonical data. A random forest classifier is used to score and rank the candidates. In this work, we use the top-N candidates and convert the confidence scores of the classifier to probabilities. An example of this output is shown in Table 1. We train MoNoise on 2,577 tweets annotated with normalization by Li and Liu (2014), which only contains word-word replacements. In our inital experiments, we noted that the normalization model wrongfully normalized some words due to the different tokenization in the treebank (e.g. “ca n’t”), because these do not occur in the normalization data. We manually created a list of exceptions, which are not considered for normalization process. 3.2 Normalization LSTM f LSTM f Neural Network Parser As a starting point, we use the shift-reduce UUParser 2.0 (de Lhoneux et al., 2017b; Kiperwasser and Goldberg, 2016). This parser uses the Arc-Hybrid Transition system (Kuhlmann et al., 2"
D18-1542,N18-1088,0,0.0668801,"Missing"
D18-1542,P08-2026,0,0.0430215,"ese unseen words were usually clustered using ad-hoc rules. For non-standard domains, the number of unseen words is much larger. To minimize the degradation in performance, lexical normalization is • A treebank containing non-standard language is created to evaluate the effect of normalization on parser performance. The treebank consists of 10,005 tokens annotated with lexical normalization and Universal Dependencies (Nivre et al., 2017). The treebank has been made publicly available. 2 Related Work Early work on parser adaptation focused on relatively canonical domains, like biomedical data (McClosky and Charniak, 2008). More recently, there has been an increasing interest in parsing of the notoriously noisy domain of social media. A lot of previous work is orthogonal to our 4984 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4984–4991 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Original word Cand. 1 (p1) Cand. 2 (p2) Cand. 3 (p3) new new news knew (0.95) (0.03) (0.01) pix pix selfies pictures comming coming comming combing (0.79) (0.08) (0.06) tomoroe tomorrow tomoroe tomorrow’s (0.57) (0.43) (&lt;0.01) (0.54)"
D18-1542,K17-3014,0,0.0257698,"erformance for non-standard language, even if pre-trained embeddings and character level information are used. • We propose a novel technique to exploit the top-N candidates provided by the normalization component, and we show that this technique leads to a further increase in parser performance. Introduction Recently, neural network dependency parsers (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016) obtained state-of-the-art performance for dependency parsing. These parsers incorporate character level information (de Lhoneux et al., 2017a; Ballesteros et al., 2015; Nguyen et al., 2017) and can more easily exploit raw text in a semi-supervised setup. These new methods are especially beneficial for words not occurring in the training data. In practice, such unseen words often are spelling mistakes, or alternative spellings of known words. In more classical parsing models, these unseen words were usually clustered using ad-hoc rules. For non-standard domains, the number of unseen words is much larger. To minimize the degradation in performance, lexical normalization is • A treebank containing non-standard language is created to evaluate the effect of normalization on parser pe"
D18-1542,P13-1114,0,0.188012,") pix pix selfies pictures comming coming comming combing (0.79) (0.08) (0.06) tomoroe tomorrow tomoroe tomorrow’s (0.57) (0.43) (&lt;0.01) (0.54) (0.39) (0.02) Table 1: Output of the normalization model for the example sentence “new pix comming tomoroe” including candidate probabilities. Only the top-3 candidates are shown here. approach, as it focuses on adaptation of the training data (Foster et al., 2011; Khan et al., 2013; Kong et al., 2014; Blodgett et al., 2018). In the remainder of this section we will shortly review work which evaluated the effect of normalization on dependency parsing. Zhang et al. (2013) tune a normalization model for the parsing task, and show performance improvement on a silver treebank obtained from manually normalized data. Daiber and van der Goot (2016) use an existing normalization model as pre-processing for a graph-based dependency parser, and show a small but significant performance improvement. In the shared task of parsing the web (Petrov and McDonald, 2012) held at SANCL 2012, some teams used a simple rulebased normalization, but the effect on final performance remained untested. Baldwin and Li (2015) examined the theoretical impact of different normalization acti"
D18-1542,N13-1039,0,0.0363963,"Missing"
D18-1542,silveira-etal-2014-gold,0,0.0258688,"Missing"
E09-1093,W05-1511,0,0.0614703,"iteria employed in grammar specialization either require carefully manually tuning, or require more complicated statistical techniques (Samuelsson, 1994); automatically derived cutting criteria, however, perform considerably worse. Discussion One may wonder how the technique introduced in this paper relates to techniques in which the disambiguation model is used directly during parsing to eliminate unlikely partial parses. An example in the context of wide coverage unification-based parsing is the beam thresholding technique employed in the Enju HPSG parser for English (Tsuruoka et al., 2004; Ninomiya et al., 2005). In a beam-search parser, unlikely partial analyses are constructed, and then - based on the probability assigned to these partial analyses - removed from further consideration. One potential advantage of the use of our filters may be, that many of these partial analyses will not even be constructed in the first place, and therefore no time is spent on these alternatives at all. We have not performed a detailed comparison, because the statistical model employed in Alpino contains some features which refer to arbitrary large parts of a parse. Such non-local features are not allowed in the Enju"
E09-1093,P96-1030,0,0.11239,"Missing"
E09-1093,P94-1026,0,0.115976,"Missing"
E09-1093,van-noord-etal-2006-syntactic,1,0.898207,"Missing"
E09-1093,J97-3004,1,0.875762,"Missing"
E09-1093,van-der-wouden-etal-2002-syntactic,0,\N,Missing
E89-1040,E87-1022,0,0.048658,"Missing"
E89-1040,C86-1071,0,0.0549284,"Missing"
E89-1040,1988.tmi-1.6,1,0.718391,"Missing"
E89-1040,C88-1008,0,0.0294003,"Missing"
E93-1010,W89-0206,0,0.130698,"Missing"
E93-1010,E91-1006,0,0.10174,"Missing"
E93-1010,P80-1024,0,0.0691737,"Missing"
E93-1010,W89-0205,0,0.231034,"Missing"
E93-1010,P85-1018,0,0.188651,"Missing"
E93-1010,P91-1015,1,0.879486,"Missing"
E93-1010,P91-1000,0,0.287769,"Missing"
E93-1010,E91-1031,1,\N,Missing
E99-1017,P95-1003,0,0.579257,"d form of backreferencing in such rules. The explicit use of backreferencing leads to more elegant and general solutions. 1 Rules with Backreferences x ~ T(x)/A__p Introduction Context sensitive rewrite rules have been widely used in several areas of natural language processing. Johnson (1972) has shown that such rewrite rules are equivalent to finite state transducers in the special case that they are not allowed to rewrite their own output. An algorithm for compilation into transducers was provided by Kaplan and Kay (1994). Improvements and extensions to this algorithm have been provided by Karttunen (1995), Karttunen (1997), Karttunen (1996) and Mohri and Sproat (1996). In this paper, the algorithm will be extended to provide a limited form of backreferencing. Backreferencing has been implicit in previous research, such as in the ""batch rules"" of Kaplan and Kay (1994), bracketing transducers for finite-state parsing (Karttunen, 1996), and the ""LocalExtension"" operation of Roche and Schabes (1995). The explicit use of backreferencing leads to more elegant and general solutions. Backreferencing is widely used in editors, scripting languages and other tools employing regular expressions (Friedl, 1"
E99-1017,P96-1015,0,0.803302,"les. The explicit use of backreferencing leads to more elegant and general solutions. 1 Rules with Backreferences x ~ T(x)/A__p Introduction Context sensitive rewrite rules have been widely used in several areas of natural language processing. Johnson (1972) has shown that such rewrite rules are equivalent to finite state transducers in the special case that they are not allowed to rewrite their own output. An algorithm for compilation into transducers was provided by Kaplan and Kay (1994). Improvements and extensions to this algorithm have been provided by Karttunen (1995), Karttunen (1997), Karttunen (1996) and Mohri and Sproat (1996). In this paper, the algorithm will be extended to provide a limited form of backreferencing. Backreferencing has been implicit in previous research, such as in the ""batch rules"" of Kaplan and Kay (1994), bracketing transducers for finite-state parsing (Karttunen, 1996), and the ""LocalExtension"" operation of Roche and Schabes (1995). The explicit use of backreferencing leads to more elegant and general solutions. Backreferencing is widely used in editors, scripting languages and other tools employing regular expressions (Friedl, 1997). For example, Emacs 126 (1) Thi"
E99-1017,W98-1301,0,0.0306291,"ain(E) range (E) ident ity (E) inverse (E) empty string concatenation of E1 ... En empty language unionof E l , . . . E n Kleene closure optionality complement difference containment intersection any symbol pair cross-product composition domain of a transduction range of a transduction identity transduction inverse transduction Table 1: Regular expression operators. macro (vowel, {a, e, i , o , u } ) . In such macro definitions, Prolog variables can be used in order to define new n-ary regular expression operators in terms of existing operators. For instance, the lenient_composition operator (Karttunen, 1998) is defined by: macro (priorityiunion (Q ,R), {Q, -domain(Q) o R}). macro (lenient_composition (R, C), priority_union(R o C,R)). Here, priority_union of two regular expressions Q and R is defined as the union of Q and the composition of the complement of the domain of Q with R. Lenient composition of R and C is defined as the priority union of the composition of R and C (on the one hand) and R (on the other hand). Some operators, however, require something more than simple macro expansion for their definition. For example, suppose a user wanted to match n occurrences of some pattern. The FSA U"
E99-1017,P96-1031,0,0.808087,"of backreferencing leads to more elegant and general solutions. 1 Rules with Backreferences x ~ T(x)/A__p Introduction Context sensitive rewrite rules have been widely used in several areas of natural language processing. Johnson (1972) has shown that such rewrite rules are equivalent to finite state transducers in the special case that they are not allowed to rewrite their own output. An algorithm for compilation into transducers was provided by Kaplan and Kay (1994). Improvements and extensions to this algorithm have been provided by Karttunen (1995), Karttunen (1997), Karttunen (1996) and Mohri and Sproat (1996). In this paper, the algorithm will be extended to provide a limited form of backreferencing. Backreferencing has been implicit in previous research, such as in the ""batch rules"" of Kaplan and Kay (1994), bracketing transducers for finite-state parsing (Karttunen, 1996), and the ""LocalExtension"" operation of Roche and Schabes (1995). The explicit use of backreferencing leads to more elegant and general solutions. Backreferencing is widely used in editors, scripting languages and other tools employing regular expressions (Friedl, 1997). For example, Emacs 126 (1) This says that each string x pr"
E99-1017,J94-3001,0,\N,Missing
E99-1017,J95-2004,0,\N,Missing
I11-1086,J99-2004,0,0.0686756,"rd Prediction Methods Kostadin Cholakov† , Gertjan van Noord† , Valia Kordoni‡ , Yi Zhang‡ † University of Groningen, The Netherlands ‡ Saarland University and DFKI GmbH, Germany {k.cholakov,g.j.m.van.noord}@rug.nl {kordoni,yzhang}@coli.uni-sb.de Abstract first type is based on the concept of supertagging while the second one performs LA. Generally, supertagging refers to the process of applying a sequential tagger to assign lexical descriptions associated with each word in an input string, relative to a given grammar. It was introduced as a means to reduce parsing ambiguity of LTAG grammars (Bangalore and Joshi, 1999), and has since been applied within CCG (Clark, 2002; Clark and Curran, 2004) and HPSG (Dridan et al., 2008; Zhang et al., 2010) grammars. Supertagging has also been employed for dealing with unknown words. However, in such methods, the tagger is used to assign lexical descriptions only to the unknown tokens in a given sentence. It is important to note here that henceforth, we will use the term suppertagging in this narrow sense of tagging unknown words only. Supertagging methods often work online. The unknown words are assigned lexical entries when they are encountered in the input during par"
I11-1086,P98-1014,0,0.0567531,"ncreases parsing accuracy compared to the baseline. This difference in quality might not always be crucial since less accurate parses produced by the grammar can still be used successfully in many NLP applications. In such cases, the less complex supertagging methods might be the preferred choice. However, through a small sentence realisation experiment, we give an example of an application where high-quality LA is a prerequisite. Other kinds of LA techniques have also been proposed. Cussens and Pulman (2000) used a symbolic approach employing inductive logic programming, while Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unificationbased approach. However, it is doubtful if those methods are scalable since they have not been applied to large-scale grammars and no meaningful evaluation has been provided. The remainder of the paper is organised as follows. Section 2 describes the resources we employ. Section 3 gives an overview of the supertagging methods previously applied with the GG. Section 4 describes the adaptation of the C& V N method to the GG. Section 5 gives details on the training procedure for the ME-based classifier used in the C& V N technique. Section 6 evaluates the"
I11-1086,W00-0740,0,0.0124041,"of having lower accuracy than the baseline. The application of the adapted C& V N method, on the other hand, increases parsing accuracy compared to the baseline. This difference in quality might not always be crucial since less accurate parses produced by the grammar can still be used successfully in many NLP applications. In such cases, the less complex supertagging methods might be the preferred choice. However, through a small sentence realisation experiment, we give an example of an application where high-quality LA is a prerequisite. Other kinds of LA techniques have also been proposed. Cussens and Pulman (2000) used a symbolic approach employing inductive logic programming, while Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unificationbased approach. However, it is doubtful if those methods are scalable since they have not been applied to large-scale grammars and no meaningful evaluation has been provided. The remainder of the paper is organised as follows. Section 2 describes the resources we employ. Section 3 gives an overview of the supertagging methods previously applied with the GG. Section 4 describes the adaptation of the C& V N method to the GG. Section 5 gives details"
I11-1086,P08-1070,1,0.93808,"en, The Netherlands ‡ Saarland University and DFKI GmbH, Germany {k.cholakov,g.j.m.van.noord}@rug.nl {kordoni,yzhang}@coli.uni-sb.de Abstract first type is based on the concept of supertagging while the second one performs LA. Generally, supertagging refers to the process of applying a sequential tagger to assign lexical descriptions associated with each word in an input string, relative to a given grammar. It was introduced as a means to reduce parsing ambiguity of LTAG grammars (Bangalore and Joshi, 1999), and has since been applied within CCG (Clark, 2002; Clark and Curran, 2004) and HPSG (Dridan et al., 2008; Zhang et al., 2010) grammars. Supertagging has also been employed for dealing with unknown words. However, in such methods, the tagger is used to assign lexical descriptions only to the unknown tokens in a given sentence. It is important to note here that henceforth, we will use the term suppertagging in this narrow sense of tagging unknown words only. Supertagging methods often work online. The unknown words are assigned lexical entries when they are encountered in the input during parsing. Therefore, the focus is primarily on improving the parsing coverage and accuracy of the grammar for a"
I11-1086,W06-1620,0,0.0154973,"s applied with the GG and compares the results to the results reported previously for the suppertagging methods for this corpus. Section 7 explores the possibility of using newly acquired lexical entries in a small sentence realisation task. Section 8 concludes the paper. evaluated in terms of type precision and type recall. For a given LA method, type precision indicates the proportion of correctly predicted lexical entries and type recall indicates how many of the correct lexical entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they sha"
I11-1086,A00-1031,0,0.0596308,"Missing"
I11-1086,I05-1015,0,0.0592628,"Missing"
I11-1086,R09-1012,1,0.894147,"Missing"
I11-1086,C10-2018,1,0.799003,"Missing"
I11-1086,W08-1708,1,0.695363,"e actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where various secondary resources (POS taggers, chunkers, etc.) are used to create an abstraction of words unknown to the ERG and then binary classifiers are employed to learn lexical entries for those words. However, learning is done based on incomplete information obtained by the various resources used. Further, no evaluation of the effect the method has on the"
I11-1086,C04-1041,0,0.034145,"Yi Zhang‡ † University of Groningen, The Netherlands ‡ Saarland University and DFKI GmbH, Germany {k.cholakov,g.j.m.van.noord}@rug.nl {kordoni,yzhang}@coli.uni-sb.de Abstract first type is based on the concept of supertagging while the second one performs LA. Generally, supertagging refers to the process of applying a sequential tagger to assign lexical descriptions associated with each word in an input string, relative to a given grammar. It was introduced as a means to reduce parsing ambiguity of LTAG grammars (Bangalore and Joshi, 1999), and has since been applied within CCG (Clark, 2002; Clark and Curran, 2004) and HPSG (Dridan et al., 2008; Zhang et al., 2010) grammars. Supertagging has also been employed for dealing with unknown words. However, in such methods, the tagger is used to assign lexical descriptions only to the unknown tokens in a given sentence. It is important to note here that henceforth, we will use the term suppertagging in this narrow sense of tagging unknown words only. Supertagging methods often work online. The unknown words are assigned lexical entries when they are encountered in the input during parsing. Therefore, the focus is primarily on improving the parsing coverage and"
I11-1086,copestake-flickinger-2000-open,0,0.0436578,"the possibility of using newly acquired lexical entries in a small sentence realisation task. Section 8 concludes the paper. evaluated in terms of type precision and type recall. For a given LA method, type precision indicates the proportion of correctly predicted lexical entries and type recall indicates how many of the correct lexical entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where va"
I11-1086,E03-1041,0,0.0230605,"mpared to the baseline. This difference in quality might not always be crucial since less accurate parses produced by the grammar can still be used successfully in many NLP applications. In such cases, the less complex supertagging methods might be the preferred choice. However, through a small sentence realisation experiment, we give an example of an application where high-quality LA is a prerequisite. Other kinds of LA techniques have also been proposed. Cussens and Pulman (2000) used a symbolic approach employing inductive logic programming, while Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unificationbased approach. However, it is doubtful if those methods are scalable since they have not been applied to large-scale grammars and no meaningful evaluation has been provided. The remainder of the paper is organised as follows. Section 2 describes the resources we employ. Section 3 gives an overview of the supertagging methods previously applied with the GG. Section 4 describes the adaptation of the C& V N method to the GG. Section 5 gives details on the training procedure for the ME-based classifier used in the C& V N technique. Section 6 evaluates the parsing coverage a"
I11-1086,J03-3001,0,0.0370393,"Missing"
I11-1086,W02-2018,0,0.0211703,"of search hits Yahoo! returns for each form in a given paradigm is combined with some simple heuristics to disambiguate the output of the morphology and to determine the correct paradigm(s). We also apply heuristics to guess the gender for words with generated noun paradigms and to determine if a word which is assigned a verb paradigm starts with a separable particle. (1) p(t|c) = P Θi fi (t,c)) P exp( i P ′ t′ ∈T exp( i Θi fi (t ,c)) where fi (t, c) may encode arbitrary features from the context and < Θ1 , Θ2 , ... > can be evaluated by maximising the pseudo-likelihood on a training corpus (Malouf, 2002). Table 2 shows the features for Aufgabe. Since the stem of the unknown word is added to the lexicon, we also experimented with prefix and suffix features extracted from the stem. We assumed that those could allow for a better generalization of morphological properties but they proved to be less informative for LA. One could argue that there is a simpler approach for mapping the various forms of the unknown word to its stem. For instance, the TreeTagger provides both POS and stem information with high accuracy. However, the generation of the paradigms allows us to consider contexts in which ot"
I11-1086,nicholson-etal-2008-evaluating,1,0.870798,"entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where various secondary resources (POS taggers, chunkers, etc.) are used to create an abstraction of words unknown to the ERG and then binary classifiers are employed to learn lexical entries for those words. However, learning is done based on incomplete information obtained by the various resources used. Further, no evaluation of the ef"
I11-1086,W02-1210,0,0.0115253,"es in a small sentence realisation task. Section 8 concludes the paper. evaluated in terms of type precision and type recall. For a given LA method, type precision indicates the proportion of correctly predicted lexical entries and type recall indicates how many of the correct lexical entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where various secondary resources (POS taggers, chunker"
I11-1086,2006.jeptalnrecital-invite.2,1,0.833811,"Missing"
I11-1086,zhang-kordoni-2006-automated,1,0.955308,"isation task. Section 8 concludes the paper. evaluated in terms of type precision and type recall. For a given LA method, type precision indicates the proportion of correctly predicted lexical entries and type recall indicates how many of the correct lexical entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where various secondary resources (POS taggers, chunkers, etc.) are used to create"
I11-1086,N10-1090,0,0.0140834,"Saarland University and DFKI GmbH, Germany {k.cholakov,g.j.m.van.noord}@rug.nl {kordoni,yzhang}@coli.uni-sb.de Abstract first type is based on the concept of supertagging while the second one performs LA. Generally, supertagging refers to the process of applying a sequential tagger to assign lexical descriptions associated with each word in an input string, relative to a given grammar. It was introduced as a means to reduce parsing ambiguity of LTAG grammars (Bangalore and Joshi, 1999), and has since been applied within CCG (Clark, 2002; Clark and Curran, 2004) and HPSG (Dridan et al., 2008; Zhang et al., 2010) grammars. Supertagging has also been employed for dealing with unknown words. However, in such methods, the tagger is used to assign lexical descriptions only to the unknown tokens in a given sentence. It is important to note here that henceforth, we will use the term suppertagging in this narrow sense of tagging unknown words only. Supertagging methods often work online. The unknown words are assigned lexical entries when they are encountered in the input during parsing. Therefore, the focus is primarily on improving the parsing coverage and accuracy of the grammar for a particular input. Th"
I11-1086,W05-1008,0,\N,Missing
I11-1086,C98-1014,0,\N,Missing
ivanova-van-noord-2014-treelet,J97-4005,0,\N,Missing
ivanova-van-noord-2014-treelet,W09-3827,0,\N,Missing
ivanova-van-noord-2014-treelet,W07-2207,0,\N,Missing
ivanova-van-noord-2014-treelet,W13-3609,0,\N,Missing
ivanova-van-noord-2014-treelet,P12-1101,0,\N,Missing
ivanova-van-noord-2014-treelet,W13-1703,0,\N,Missing
ivanova-van-noord-2014-treelet,W12-2006,0,\N,Missing
ivanova-van-noord-2014-treelet,W13-3604,0,\N,Missing
ivanova-van-noord-2014-treelet,read-etal-2012-wesearch,0,\N,Missing
ivanova-van-noord-2014-treelet,W13-3601,0,\N,Missing
ivanova-van-noord-2014-treelet,W13-5707,1,\N,Missing
ivanova-van-noord-2014-treelet,N12-2006,0,\N,Missing
ivanova-van-noord-2014-treelet,W11-2838,0,\N,Missing
ivanova-van-noord-2014-treelet,P99-1069,0,\N,Missing
J00-1005,W89-0229,0,0.107202,"Missing"
J00-1005,E99-1017,1,0.805947,"Missing"
J00-1005,P97-1058,0,0.0421911,"Missing"
J00-1005,P98-1101,0,0.0398143,"Missing"
J00-1005,1997.iwpt-1.19,0,0.0350795,"ger numbers of jumps the per state and per subset variants consistently beat the FSM library. Experiment: Automata Generated by Approximation Algorithms. The automata used in the previous experiments were randomly generated. However, it may well be that in practice the automata that are to be treated by the algorithm have typical properties not reflected in this test data. For this reason, results are presented for a number of automata that were generated using approximation techniques for context-free grammars; in particular, for automata created by Nederhof, using the technique described in Nederhof (1997), and a small number of automata created using the technique of Pereira and Wright (1997) (as implemented by Nederhof). We have restricted our attention to automata with at least 1,000 states in the input. The automata typically contain lots of jumps. Moreover, the number of states of the resulting automaton is often smaller than the number of states in the input automaton. Results are given in Tables I and 2. One of the most striking examples is the ygrim automaton consisting of 3,382 states and 9,124 jumps. For this example, the per graph implementations ran out of memory (after a long time)"
J00-1005,W98-1302,0,0.0230243,"Missing"
J00-1005,P91-1032,0,0.0759911,"Missing"
J00-1005,W98-1306,1,0.757351,"Missing"
J00-1005,C98-1098,0,\N,Missing
J18-4003,J04-4004,0,0.340778,"Missing"
J18-4003,P11-1028,0,0.105528,"Missing"
J18-4003,P16-1017,0,0.0302415,"Missing"
J18-4003,P13-1166,0,0.550407,"Missing"
J18-4003,P16-1171,0,0.0166494,"ion available earlier, we only sent a single e-mail and no reminder. Finally, for each of the 395 papers in this study, we obtained citation counts from Google Scholar on 10 March 2018. 2.2 Reproducing Results from Selected Studies After having obtained the underlying data and/or code, we attempted to reproduce the results of a random selection of five studies from 2011 (Nakov and Ng 2011; He, Lin, and Alani 2011; Sauper, Haghighi, and Barzilay 2011; Liang, Jordan, and Klein 2011; Branavan, Silver, and Barzilay 2011) and a random6 selection of five studies from 2016 (Coavoux and Crabb´e 2016; Gao et al. 2016; Hu et al. 2016; Nicolai and Kondrak 2016; Tian, Okazaki, and Inui 2016) for which the data and source code was provided, either through links in the paper, or to us after our request. Our approach to reproduce these results was as follows: We used the information provided in the paper and accompanying the source code to reproduce the results. If we were not able to run the source code, or if our results deviated from the results of the authors, we contacted the authors to see if they were able to help. Note that this should only be seen as a minimal reproduction effort: We limited the amount"
J18-4003,P11-1013,0,0.0316126,"Missing"
J18-4003,P16-1228,0,0.0218964,"Missing"
J18-4003,P11-1060,0,0.0278455,"Missing"
J18-4003,W17-1603,0,0.145524,"Missing"
J18-4003,P11-1130,0,0.0160049,"sible. In that case we immediately contacted all authors with a request (similar to the other e-mail) to send us the updated link to the data and/or source code within two weeks. As these authors had already made this information available earlier, we only sent a single e-mail and no reminder. Finally, for each of the 395 papers in this study, we obtained citation counts from Google Scholar on 10 March 2018. 2.2 Reproducing Results from Selected Studies After having obtained the underlying data and/or code, we attempted to reproduce the results of a random selection of five studies from 2011 (Nakov and Ng 2011; He, Lin, and Alani 2011; Sauper, Haghighi, and Barzilay 2011; Liang, Jordan, and Klein 2011; Branavan, Silver, and Barzilay 2011) and a random6 selection of five studies from 2016 (Coavoux and Crabb´e 2016; Gao et al. 2016; Hu et al. 2016; Nicolai and Kondrak 2016; Tian, Okazaki, and Inui 2016) for which the data and source code was provided, either through links in the paper, or to us after our request. Our approach to reproduce these results was as follows: We used the information provided in the paper and accompanying the source code to reproduce the results. If we were not able to run th"
J18-4003,P16-1108,0,0.0249697,"nt a single e-mail and no reminder. Finally, for each of the 395 papers in this study, we obtained citation counts from Google Scholar on 10 March 2018. 2.2 Reproducing Results from Selected Studies After having obtained the underlying data and/or code, we attempted to reproduce the results of a random selection of five studies from 2011 (Nakov and Ng 2011; He, Lin, and Alani 2011; Sauper, Haghighi, and Barzilay 2011; Liang, Jordan, and Klein 2011; Branavan, Silver, and Barzilay 2011) and a random6 selection of five studies from 2016 (Coavoux and Crabb´e 2016; Gao et al. 2016; Hu et al. 2016; Nicolai and Kondrak 2016; Tian, Okazaki, and Inui 2016) for which the data and source code was provided, either through links in the paper, or to us after our request. Our approach to reproduce these results was as follows: We used the information provided in the paper and accompanying the source code to reproduce the results. If we were not able to run the source code, or if our results deviated from the results of the authors, we contacted the authors to see if they were able to help. Note that this should only be seen as a minimal reproduction effort: We limited the amount of human (not CPU) time spent on reproduc"
J18-4003,P11-1036,0,0.145875,"Missing"
J18-4003,P16-1121,0,0.0332346,"Missing"
J18-4003,J03-4003,0,\N,Missing
J18-4003,J08-3010,0,\N,Missing
J90-1004,E89-1032,0,0.0953576,"d&apos;s BUG (Bottom-Up Generator) system, part of MiMo2, an experimental machine translation system for translating international news items of Teletext, which uses a Prolog version of Computational Linguistics Volume 16, Number 1, March 1990 Shieber et al. Semantic Head-Driven Grammar PATR-II similar to that of Hirsh (1987). According to Martin Kay (personal communication), the STREP machine translation project at the Center for the Study of Language and Information uses a version of our algorithm to generate with respect to grammars based on head-driven phrase structure grammar (HPSG). Finally, Calder et al. (1989) report on a generation algorithm for unification categorial grammar that appears to be a special case of ours. 1.2 PRELIMINARIES Despite the general applicability of the algorithm, we will, for the sake of concreteness, describe it and other generation algorithms in terms of their implementation for definiteclause grammars (DCG). For ease of exposition, the encoding will be a bit more cumbersome than is typically found in Prolog DCG interpreters. The standard DCG encoding in Prolog uses the notation (cat o) --&gt; (cat I ) . . . . . (cat,). where the (cat i) are terms representing the grammatica"
J90-1004,J87-1005,1,0.800251,"they might be because during the distribution of store elements among the subject and complements of a verb no check is performed as to whether the variable bound by a store element actually appears in the semantics of the phrase to which it is being assigned, leading to many dead ends in the generation process. Also, the rules are sound for generation but not for analysis, because they do not enforce the constraint that every occurrence of a variable in logical form be outscoped by the variable&apos;s binder. Adding appropriate side conditions to the rules, following the constraints discussed by Hobbs and Shieber (1987) would not be difficult. 38 4 EXTENSIONS Tile basic semantic-head-driven generation algorithm can be augmented in various ways so as to encompass some important analyses and constraints. In particular, we discuss the incorporation of • completeness and coherence constraints, • the postponing of lexical choice, and • the ability to handle certain problematic empty-headed phrases 4.1 COMPLETENESS AND COHERENCE Wedckind (1988) defines completeness and coherence of a generation algorithm as follows. Suppose a generator derives a string w from a logical form s, and the grammar assigns to w the logi"
J90-1004,P83-1021,1,0.813733,"INTRODUCTION The problem of generating a well-formed natural language expression from an encoding of its meaning possesses properties that distinguish it from the converse problem of recovering a meaning encoding from a given natural language expression. This much is axiomatic. In previous work (Shieber 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983), but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with"
J90-1004,J81-4003,1,0.825833,"Missing"
J90-1004,C88-2128,1,0.954033,"evious bottom-up generator, it allows use of semlantically nonmonotonic grammars, yet unlike top-down methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. 1 INTRODUCTION The problem of generating a well-formed natural language expression from an encoding of its meaning possesses properties that distinguish it from the converse problem of recovering a meaning encoding from a given natural language expression. This much is axiomatic. In previous work (Shieber 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983), but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural language expressions, the Earley deduction method is reasonably successful along certain dimensi"
J90-1004,C88-2150,0,0.136143,"a small number of parameters. As a method for generating natural language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose well-foundedness relies on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property; further discussion can be found in Section 2.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called---has its 30 own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. Tile algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be s"
J97-3004,P89-1018,0,0.0420022,"ing 441 Computational Linguistics Volume 23, Number 3 112:s-adv-s / 46 s-np-vp / 87 vp-v vp-vp-np-pp 121 125 give22 Figure 4 Example of a partial derivation tree projected by a history item. techniques. The result of the parser will be a parse forest: a compact representation of all possible parse trees rather than an enumeration of all parse trees. The structure of the parse forest in the head-corner parser is rather unusual, and therefore we will take some time to explain it. Because the head-corner parser uses selective memorization, conventional approaches to constructing parse forests (Billot and Lang 1989) are not applicable. The head-corner parser maintains a table of partial derivation trees, each of which represents a successful path from a lexical head (or gap) up to a goal category. The table consisting of such partial parse trees is called the history table; its items are history items. More specifically, each history item is a triple consisting of a result item reference, a rule name, and a list of triples. The rule n a m e is always the n a m e of a rule without daughters (i.e., a lexical entry or a gap): the (lexical) head. Each triple in the list of triples represents a local tree. It"
J97-3004,E93-1010,1,0.878259,"Missing"
J97-3004,P94-1040,0,0.139374,"tter results in italics. Average CPU-time is only given for those parsers that completed each of the sentences within the time limit. The results are given in Table 6. The bottom-up active chart parser performs better on smaller sentences with a small number of readings. For longer and more ambiguous sentences, the head-corner parser is (much) more efficient. The other parsers are consistently much less efficient. 7.4 Experiment 3: Alvey NL Tools A final set of experiments was performed for the Alvey NL Tools grammar (Grover, Carroll, and Briscoe 1993), similar to the experiments discussed in Carroll (1994). For a longer description of the grammar and the test sets we refer the reader to this publication. The grammar contains 2,363 lexical entries, and 780 rules (8 of which are gaps). The left-corner relation contains 440 pairs; the lexical left-corner relation 452 van Noord Efficient Head-Corner Parsing Table 6 Total and average CPU-time and maximum space requirements for a set of 25 sentences (MiMo2 grammar). Italicized items are offered for cautious comparison. Total (msec) msec/Sentence Maximum Space Time-Outs hc bu-active lc Parser 52,670 52,990 109,750 2,107 2,120 4,390 2,062 30,392 8,570"
J97-3004,W89-0206,0,0.735732,"d by uninstantiated left-most daughters for an active chart parser, as only a search of the chart is carried out and no additional items are added to it. Note, however, that the amount of search required may grow exponentially, if more than one uninstantiated daughter is present: vp(As) --&gt; vp([AI,A21As]), AI, A2. (3) or if the number of daughters is not specified by the rule: vp([AO]) --&gt; vp([AO ..... An]), A1 ..... An. (4) as appears to be the case for some of the rule-schemata used in HPSG. Several authors have suggested parsing algorithms that may be more suitable for lexicalist grammars. Kay (1989) discusses the concept of head-driven parsing. The key idea is that the linguistic concept head can be used to obtain parsing algorithms that are better suited for typical natural language grammars. Most linguistic theories assume that among the daughters introduced by a rule there is one daughter that can be identified as the head of that rule. There are several criteria for deciding which daughter is the head, two of which seem relevant for parsing. First of all, the head of a rule determines to a large extent what other daughters may or must be present, as the head selects the other daughte"
J97-3004,E91-1006,0,0.111483,"nsidered elsewhere. In Satta and Stock (1989), Sikkel and op den Akker (1992, 1993), and Sikkel (1993), chart-based head-corner parsing for context-free grammar is considered. It is shown that, in spite of the fact that bidirectional parsing seemingly leads to more overhead than left-to-right parsing, the worst-case complexity of a head-corner parser does not exceed that of an Earley parser. Some further variations are discussed in Nederhof and Satta (1994). In van Noord (1991, 1993) I argue that head-corner parsing is especially useful for parsing with nonconcatenative grammar formalisms. In Lavelli and Satta (1991) and van Noord (1994) head-driven parsing strategies for Lexicalized Tree Adjoining Grammars are presented. The head-corner parser is closely related to the semantic-head-driven generation algorithm (see Shieber et al. [1990] and references cited there), especially in its purely bottom-up incarnation. 1.2 Selective Memorization The head-corner parser is in many respects different from traditional chart parsers. An important difference follows from the fact that in the head-corner parser only larger chunks of computation are memorized. Backtracking still plays an important role for the implemen"
J97-3004,P94-1029,0,0.0133365,"sented by Pereira and Shieber (1987), which itself is a version without memorization of the BUP parser (Matsumoto et al. 1983). Head-corner parsing has also been considered elsewhere. In Satta and Stock (1989), Sikkel and op den Akker (1992, 1993), and Sikkel (1993), chart-based head-corner parsing for context-free grammar is considered. It is shown that, in spite of the fact that bidirectional parsing seemingly leads to more overhead than left-to-right parsing, the worst-case complexity of a head-corner parser does not exceed that of an Earley parser. Some further variations are discussed in Nederhof and Satta (1994). In van Noord (1991, 1993) I argue that head-corner parsing is especially useful for parsing with nonconcatenative grammar formalisms. In Lavelli and Satta (1991) and van Noord (1994) head-driven parsing strategies for Lexicalized Tree Adjoining Grammars are presented. The head-corner parser is closely related to the semantic-head-driven generation algorithm (see Shieber et al. [1990] and references cited there), especially in its purely bottom-up incarnation. 1.2 Selective Memorization The head-corner parser is in many respects different from traditional chart parsers. An important differenc"
J97-3004,P96-1032,0,0.111495,"Missing"
J97-3004,P80-1024,0,0.0754173,"with goal-weakening (as applied to head-corner and left-corner parsing) is substantially more efficient than conventional chart parsing. We conclude that at least for some grammars, head-corner parsing is a good option. 2. A Specification of the Head-Corner Parser Head-corner parsing is a radical approach to head-driven parsing in that it gives up the idea that parsing should proceed from left to right. Rather, processing in a head-corner parser is bidirectional, starting from a head outward (island-driven). A head-corner parser can be thought of as a generalization of the left-corner parser (Rosenkrantz and Lewis 1970; Matsumoto et al. 1983; Pereira and Shieber 1987). As in the left-corner parser, the flow of information in a head-corner parser is both bottom-up and topdown. In order to explain the parser, I first introduce some terminology. I assume that grammars are defined in the Definite Clause Grammar formalism (Pereira and Warren 1980). Without any loss of generality I assume that no external Prolog calls (the ones that are defined within { and }) are used, and that all lexical material is introduced in rules that have no other right-hand-side members (these rules are called lexical 429 Computational"
J97-3004,W89-0205,0,0.0246002,"which rule selection is uniformly driven by either the left-most or rightmost daughter. Furthermore, by selecting potential heads on the basis of a head-corner table (comparable to the left-corner table of a left-corner parser) it may use top-down filtering to minimize the search-space. This head-corner parser generalizes the leftcorner parser. Kay&apos;s presentation is reminiscent of the left-corner parser as presented by Pereira and Shieber (1987), which itself is a version without memorization of the BUP parser (Matsumoto et al. 1983). Head-corner parsing has also been considered elsewhere. In Satta and Stock (1989), Sikkel and op den Akker (1992, 1993), and Sikkel (1993), chart-based head-corner parsing for context-free grammar is considered. It is shown that, in spite of the fact that bidirectional parsing seemingly leads to more overhead than left-to-right parsing, the worst-case complexity of a head-corner parser does not exceed that of an Earley parser. Some further variations are discussed in Nederhof and Satta (1994). In van Noord (1991, 1993) I argue that head-corner parsing is especially useful for parsing with nonconcatenative grammar formalisms. In Lavelli and Satta (1991) and van Noord (1994)"
J97-3004,P85-1018,0,0.647351,"is used. Consequently, the set of constituent structures defined by a grammar cannot be read off the rule set directly, but is defined by the interaction of the rule schemata and the lexical categories. Applying standard parsing algorithms to such grammars is unsatisfactory for a number of reasons. Earley parsing is intractable in general, as the rule set is simply too general. For some grammars, naive top-down prediction may even fail to terminate. Alfa-informatica& BCN. E-mail: vannoord@let.rug.nl (~ 1997 Associationfor ComputationalLinguistics Computational Linguistics Volume 23, Number 3 Shieber (1985) therefore proposes a modified version of the Earley parser, using restricted top-down prediction. While this modification often leads to better termination properties of the parsing method, in practice it easily leads to a complete trivialization of the top-down prediction step, thus leading to inferior performance. Bottom-up parsing is far more attractive for lexicalist formalisms, as it is driven by the syntactic information associated with lexical elements, but certain inadequacies remain. Most importantly, the selection of rules to be considered for application may not be very efficient."
J97-3004,J90-1004,1,0.860596,"Missing"
J97-3004,1993.iwpt-1.21,0,0.0280763,"Missing"
J97-3004,J87-1004,0,0.0181906,"each of the two derivation trees of the sentence I see a man at home, for the grammar of Billot and Lang (1989). The start symbol of this grammar is nt6. Note that all nodes, except for substitution nodes, are associated with a rule (or lexical entry) of the original grammar. Root nodes have a nonterminal symbol before the colon, and the corresponding rule identifier after the colon. The set of derived trees for this tree substitution grammar equals the set of derivation trees of the parse (ignoring the nonterminal symbols of the tree substitution grammar). consider the g r a m m a r used b y Tomita (1987) a n d Billot and Lang (1989), given here in (17) a n d (18). (I) s --&gt; np, vp. (2) s --&gt; s, pp. (3) np --&gt; n. (5) np --&gt; rip, pp. (6) pp --&gt; prep, [&apos;I&apos;] . n [man] . v --&gt; [see] . [at]. det--&gt; [a]. n--&gt; [home]. (4) np --&gt; det, n. (17) rip. (7) vp --&gt; v, rip. n --&gt; prep--&gt; --&gt; (18) The sentence I see a m a n at h o m e has two derivations, according to this g r a m m a r . The lexicalized tree substitution g r a m m a r in Figure 5, which is constructed b y the headcorner parser, derives exactly these two derivations. N o t e that the item references are used in the s a m e m a n n e r as the c"
J97-3004,P91-1015,1,0.933576,"Missing"
J97-3004,H93-1101,0,0.0539749,"Missing"
J97-3004,P95-1022,1,0.881076,"Missing"
J97-3004,P95-1014,0,\N,Missing
L18-1109,D13-1007,0,0.219977,"Missing"
L18-1109,N15-1045,0,0.017364,"ns (e.g. ‘lol’7→‘laughing out loud’). However, this might not be the desired output, since one might argue that this expansion does not represent the intended meaning. This reveals another potential use for a taxonomy of normalization actions: it enables us to filter the categories before training, and thus learn a model which only handles the desired categories. Normalization There is ample of previous work on normalization, but there is no consensus about the scope of the normalization task. Some existing corpora with normalization annotation for English are shown in Table 1. The corpora by Baldwin and Li (2015) and Kaljahi et al. (2015) are also annotated with error categories. However, the guidelines for the annotation of these corpora are substantially different compared to the other, more commonly used, corpora. The taxonomy proposed by Baldwin and Li (2015) has a very high percentage of normalizations since it allows deletion and insertion of tokens as well as the correction of capitalization. In contrast, The Foreebank (Kaljahi et al., 2015) has a very low percentage of normalized words. This is due to its more canonical forum domain; it is mostly focused on grammatical error correction. In the"
L18-1109,W15-4319,0,0.131708,"Missing"
L18-1109,D15-1157,0,0.0255192,"Missing"
L18-1109,P14-3012,0,0.421067,"Missing"
L18-1109,W14-1701,0,0.0350461,"tion replacements. Existing error taxonomies are unfortunately not suitable for the task of normalization, since the categories are substantially different. For machine translation, taxonomies as the Multidimensional Quality Metrics (Mariana, 2014) are proposed, which contains 3 main categories: accuracy, verity and fluency. The last category would be the most relevant for normalization, but the normalization task compromises a different variety of errors and anomalies. In grammatical error correction, often a more detailed taxonomy for errors is used; the default benchmark has 28 categories (Ng et al., 2014). However, many of the errors in this taxonomy are not annotated in the normalization benchmarks and many normalization replacements are not included in this taxonomy. Different benchmarks for normalization specify the task slightly different; one striking example is the inclusion of the expansions of phrasal abbreviations (e.g. ‘lol’7→‘laughing out loud’). However, this might not be the desired output, since one might argue that this expansion does not represent the intended meaning. This reveals another potential use for a taxonomy of normalization actions: it enables us to filter the catego"
L18-1109,reynaert-2008-errors,0,0.0172914,"the different categories of the taxonomy. The results of this evaluation reveal that some of the problematic categories only include minor transformations, whereas most regular transformations are solved quite well. Keywords: normalization, user generated content, social media, error taxonomy 1. 2. Introduction For other natural language processing tasks, such as grammatical error correction and machine translation, there already exist detailed error taxonomies, which help in getting insights in the strengths and weaknesses of systems. For normalization, such an evaluation does not exist yet. Reynaert (2008) proposed an evaluation framework which evaluates the different sub-tasks in more detail; enabling the evaluation of error detection, candidate generation and candidate ranking. In the more recent shared task on lexical normalization hosted at the WNUT workshop (Baldwin et al., 2015b), the outputs of systems were evaluated on precision, recall and F1 score. Orthogonal to these approaches, we propose a more in-depth evaluation of normalization, focusing on categories of different normalization replacements. Existing error taxonomies are unfortunately not suitable for the task of normalization,"
N16-1160,N12-1095,0,0.104572,"Multilingual models. The research on using multilingual information in the learning of multi-sense embedding models is scarce. Guo et al. (2014) perform a sense induction step based on clustering translations prior to learning word embeddings. Once the translations are clustered, they are mapped to a source corpus using WSD heuristics, after which a recurrent neural network is trained to obtain sense-specific representations. Unlike in our work, the sense induction and embedding learning components are entirely separated, without a possibility for one to influence another. In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of Lin and Wu (2009). Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space. A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann"
N16-1160,P14-1023,0,0.0189679,"s based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time. 1 Introduction Approaches to learning word embeddings (i.e. realvalued vectors) relying on word context have received much attention in recent years, and the induced representations have been shown to capture syntactic and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University o"
N16-1160,P91-1034,0,0.563356,"m another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerous parallel corpora exist nowadays. Our model, which can be seen as an autoencoder with a discrete hidden layer encoding word senses, leverages bilingual data in its encoding part, while the decoder predicts the surrounding words relying on the 1346 Proceedings of NAACL-HLT 2016, pages 1346–1356, c San Dieg"
N16-1160,P12-1015,0,0.0297571,"n step based on the sentential context provided for each word in the pair. The other benchmarks we use provide the ratings for the word pairs without context. WS-353 contains 353 human-rated word pairs (Finkelstein et al., 2001), while Agirre et al. (2009) separate this benchmark for similarity (WS-SIM) and relatedness (WS-REL). The RG-65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) benchmarks contain nouns only. The MTurk-287 (Radinsky et al., 2011) and MTurk-771 (Halawi et al., 2012) include word pairs whose similarity was crowdsourced from AMT. Similarly, MEN (Bruni et al., 2012) is an AMT-annotated dataset of 3000 word pairs. The YP130 (Yang and Powers, 2006) and Verb-143 (Baker et al., 2014) measure verb similarity. Rare-Word (Luong et al., 2013) contains 2034 rare-word pairs. Finally, SimLex-999 (Hill et al., 2014b) is intended to measure pure similarity as opposed to relatedness. For these benchmarks, we prepare the word representations by taking a uniform average of all sense embeddings per word. The evaluation is carried out using the tool described in Faruqui and Dyer (2014a). Due to space constraints, we report the results by averaging over all benchmarks (Sim"
N16-1160,D14-1110,0,0.147602,"been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et"
N16-1160,J81-4005,0,0.699143,"Missing"
N16-1160,J94-4003,0,0.27129,"ondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerous parallel corpora exist nowadays. Our model, which can be seen as an autoencoder with a discrete hidden layer encoding word senses, leverages bilingual data in its encoding part, while the decoder predicts the surrounding words relying on the 1346 Proceedings of NAACL-HLT 2016, pages"
N16-1160,W12-3131,0,0.0419226,"Missing"
N16-1160,P14-1129,0,0.00942358,"are not entirely peaked, which makes weighting beneficial. ∗ Unlike in training, the sense prediction step here does not use the crosslingual context Ci′ since it is not available in the evaluation tasks. In this work, instead of marginalizing out the unobservable crosslingual context, we simply ignore it in computation. Sometimes, even the first-language context is missing, as is the situation in many word similarity tasks.PIn that case, we just use the uniform average, 1/|S| s∈S ϕi,s . 3 Word affiliation from alignments In defining the crosslingual signal we draw on a heuristic inspired by Devlin et al. (2014). The secondlanguage context words are taken to be the multiset of words around and including the pivot affiliated to xi : (6) Ci′ = {x′ai −m , ..., x′ai , ..., x′ai +m }, where x′ai is the word affiliated to xi and the parameter m regulates the context window size. By choosing m = 0, only the affiliated word is used as l′ context, and by choosing m = ∞, the l′ context is the entire sentence (≈uniform alignment). To obtain the index ai , we use the following: 1) If xi aligns to exactly one second-language word, ai is the index of the word it aligns to. 2) If xi aligns to multiple words, ai is"
N16-1160,P02-1033,0,0.365828,"14; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerous parallel corpora exist nowadays. Our model, which can be seen as an autoencoder with a discrete hidden layer encoding word senses, leverages bilingual data in its encoding part, while the decoder predicts the surrounding words relying on the 1346 Proc"
N16-1160,P10-4002,0,0.0503289,"re x′ai is the word affiliated to xi and the parameter m regulates the context window size. By choosing m = 0, only the affiliated word is used as l′ context, and by choosing m = ∞, the l′ context is the entire sentence (≈uniform alignment). To obtain the index ai , we use the following: 1) If xi aligns to exactly one second-language word, ai is the index of the word it aligns to. 2) If xi aligns to multiple words, ai is the index of the aligned word in the middle (and rounding down when necessary). 3) If xi is unaligned, Ci′ is empty, therefore no l′ context is used. We use the cdec aligner (Dyer et al., 2010) to wordalign the parallel corpora. 4 Parameters and Set-up their default values can be examined in the source code available online. 4.2 Bilingual data In a large body of work on multilingual word representations, Europarl (Koehn, 2005) is the preferred source of parallel data. However, the domain of Europarl is rather constrained, whereas we would like to obtain word representations of more general language, also to carry out an effective evaluation on semantic similarity datasets where domains are usually broader. We therefore use the following parallel corpora: News Commentary (Bojar et al"
N16-1160,P13-2136,0,0.0224997,"D heuristics, after which a recurrent neural network is trained to obtain sense-specific representations. Unlike in our work, the sense induction and embedding learning components are entirely separated, without a possibility for one to influence another. In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of Lin and Wu (2009). Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space. A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T¨ackstr¨om et al., 2012). The recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest. The latter work on the infinite Skip-Gram model in which the embedding dimensionality is stoc"
N16-1160,P14-5004,0,0.0786572,"representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is ea"
N16-1160,E14-1049,0,0.293144,"representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is ea"
N16-1160,N15-1003,0,0.0138046,"which would allow to weight the contribution of different senses more accurately for the multi-sense models. Why, then, does simply averaging the M U and B I M U embeddings lead to better results than when using the S G embeddings? We hypothesize that the single-sense model tends to over-represent the dominant sense with its generic, one-vector-per-word representation, whereas the uniformly averaged embeddings yielded by the multisense models better encode the range of potential senses. Similar observations have been made in the context of selectional preference modeling of polysemous verbs (Greenberg et al., 2015). In POS tagging, the relationship between M U and B I M U models is similar as discussed above. Overall, however, neither of the multi-sense models outperforms the S G embeddings. The neural network tagger may be able to implicitly perform disambiguation on top of single-sense S G embeddings, similarly to what has been argued in Li and Jurafsky (2015). The tagging accuracies obtained with M U on CZ-EN and FR-EN are similar to the one obtained by Li and Jurafsky with their multi-sense model (93.8), while the accuracy of S G is more competitive in our case (around 94.0 compared to 92.5), althou"
N16-1160,C14-1048,0,0.53625,"embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings."
N16-1160,P14-1006,0,0.130951,"tan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003;"
N16-1160,P12-1092,0,0.943861,"to capture syntactic and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotati"
N16-1160,J98-1001,0,0.107447,"Missing"
N16-1160,N03-1015,0,0.0604604,"unsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerous parallel corpora exist nowadays. Our model, which can be seen as an autoencoder with a discrete hidden layer encoding word senses, leverages bilingual data in its encoding part, while the decoder predicts the su"
N16-1160,C12-1089,1,0.672633,"an et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in an"
N16-1160,2005.mtsummit-papers.11,0,0.0362331,"o obtain the index ai , we use the following: 1) If xi aligns to exactly one second-language word, ai is the index of the word it aligns to. 2) If xi aligns to multiple words, ai is the index of the aligned word in the middle (and rounding down when necessary). 3) If xi is unaligned, Ci′ is empty, therefore no l′ context is used. We use the cdec aligner (Dyer et al., 2010) to wordalign the parallel corpora. 4 Parameters and Set-up their default values can be examined in the source code available online. 4.2 Bilingual data In a large body of work on multilingual word representations, Europarl (Koehn, 2005) is the preferred source of parallel data. However, the domain of Europarl is rather constrained, whereas we would like to obtain word representations of more general language, also to carry out an effective evaluation on semantic similarity datasets where domains are usually broader. We therefore use the following parallel corpora: News Commentary (Bojar et al., 2013) (NC), Yandex-1M4 (RU-EN), CzEng 1.0 (Bojar et al., 2012) (CZ-EN) from which we exclude the EU legislation texts, and GigaFrEn (Callison-Burch et al., 2009) (FR-EN). The sizes of the corpora are reported in Table 1. The word repr"
N16-1160,W14-1618,0,0.0178525,"sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time. 1 Introduction Approaches to learning word embeddings (i.e. realvalued vectors) relying on word context have received much attention in recent years, and the induced representations have been shown to capture syntactic and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g."
N16-1160,D15-1200,0,0.371076,"rinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P"
N16-1160,P09-1116,0,0.0258784,"et al. (2014) perform a sense induction step based on clustering translations prior to learning word embeddings. Once the translations are clustered, they are mapped to a source corpus using WSD heuristics, after which a recurrent neural network is trained to obtain sense-specific representations. Unlike in our work, the sense induction and embedding learning components are entirely separated, without a possibility for one to influence another. In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of Lin and Wu (2009). Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space. A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T¨ackstr¨om et al."
N16-1160,N15-1028,0,0.0147469,". Once the translations are clustered, they are mapped to a source corpus using WSD heuristics, after which a recurrent neural network is trained to obtain sense-specific representations. Unlike in our work, the sense induction and embedding learning components are entirely separated, without a possibility for one to influence another. In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of Lin and Wu (2009). Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space. A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T¨ackstr¨om et al., 2012). The recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest. The"
N16-1160,W13-3512,0,0.0151987,"ns 353 human-rated word pairs (Finkelstein et al., 2001), while Agirre et al. (2009) separate this benchmark for similarity (WS-SIM) and relatedness (WS-REL). The RG-65 (Rubenstein and Goodenough, 1965) and the MC-30 (Miller and Charles, 1991) benchmarks contain nouns only. The MTurk-287 (Radinsky et al., 2011) and MTurk-771 (Halawi et al., 2012) include word pairs whose similarity was crowdsourced from AMT. Similarly, MEN (Bruni et al., 2012) is an AMT-annotated dataset of 3000 word pairs. The YP130 (Yang and Powers, 2006) and Verb-143 (Baker et al., 2014) measure verb similarity. Rare-Word (Luong et al., 2013) contains 2034 rare-word pairs. Finally, SimLex-999 (Hill et al., 2014b) is intended to measure pure similarity as opposed to relatedness. For these benchmarks, we prepare the word representations by taking a uniform average of all sense embeddings per word. The evaluation is carried out using the tool described in Faruqui and Dyer (2014a). Due to space constraints, we report the results by averaging over all benchmarks (Similarity), and include the individual results in the online repository. 5.2 Supersense similarity We also evaluate on a task measuring the similarity between the embeddings—"
N16-1160,Q16-1017,1,0.554196,"art et al., 1986; Bengio et 1347 al., 2013). Autoencoders are trained to reproduce their input by first mapping their input to a (lower dimensional) hidden layer and then predicting an approximation of the input relying on this hidden layer. In our case, the hidden layer is not a real-valued vector, but is a categorical variable encoding the sense of a word. Discrete-state autoencoders have been successful in several natural language processing applications, including POS tagging and word alignment (Ammar et al., 2014), semantic role induction (Titov and Khoddam, 2015) and relation discovery (Marcheggiani and Titov, 2016). More formally, our model consists of two components: an encoding part which assigns a sense to a pivot word, and a reconstruction (decoding) part recovering context words based on the pivot word and its sense. As predictions are probabilistic (‘soft’), the reconstruction step involves summation over all potential word senses. The goal is to find embedding parameters which minimize the error in recovering context words based on the pivot word and the sense assignment. Parameters of both encoding and reconstruction are jointly optimized. Intuitively, a good sense assignment should make the rec"
N16-1160,D14-1113,0,0.791023,"erties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blun"
N16-1160,P03-1058,0,0.112642,"Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerous parallel corpora exist nowadays. Our model, which can be seen as an autoencoder with a discrete hidden layer encoding word senses, leverages bilingual data in its encoding part, while the decoder predicts the surrounding words r"
N16-1160,W14-1609,0,0.0102159,"though crosslingual information is not available at test time. 1 Introduction Approaches to learning word embeddings (i.e. realvalued vectors) relying on word context have received much attention in recent years, and the induced representations have been shown to capture syntactic and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The"
N16-1160,N10-1013,0,0.34779,"rical study. We comment here briefly on other choices of k ∈ {2, 4, 5}. We have found k = 2 to be a good choice on the RU-EN and FR-EN corpora (but not on CZ-EN), with an around 0.2-point improvement over k = 3 on SCWS and in POS tagging. With the larger values of k, the performance tends to degrade. For example, on RU-EN, the k = 5 score on SCWS is about 0.6 point below our default setting. 7 Additional Related Work Multi-sense models. One line of research has dealt with sense induction as a separate, clustering problem that is followed by an embedding learning component (Huang et al., 2012; Reisinger and Mooney, 2010). In another, the sense assignment and the embeddings are trained jointly (Neelakantan et al., 2014; Tian et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2015). Neelakantan et al. (2014) propose an extension of Skip-Gram (Mikolov et al., 2013a) by introducing sense-specific parameters together with the k-means-inspired ‘centroid’ vectors that keep track of the contexts in which word senses have occurred. They explore two model variants, one in which the number of senses is the same for all words, and another in which a threshold value determines the number of senses for each word. The re"
N16-1160,N12-1052,0,0.0135432,"Missing"
N16-1160,C14-1016,0,0.411031,"c and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrating multilingual data, with largely two distinct goals in mind. The first goal has been to obtain representations for several languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et a"
N16-1160,N15-1001,1,0.835683,"eneral structure from neural autoencoders (Rumelhart et al., 1986; Bengio et 1347 al., 2013). Autoencoders are trained to reproduce their input by first mapping their input to a (lower dimensional) hidden layer and then predicting an approximation of the input relying on this hidden layer. In our case, the hidden layer is not a real-valued vector, but is a categorical variable encoding the sense of a word. Discrete-state autoencoders have been successful in several natural language processing applications, including POS tagging and word alignment (Ammar et al., 2014), semantic role induction (Titov and Khoddam, 2015) and relation discovery (Marcheggiani and Titov, 2016). More formally, our model consists of two components: an encoding part which assigns a sense to a pivot word, and a reconstruction (decoding) part recovering context words based on the pivot word and its sense. As predictions are probabilistic (‘soft’), the reconstruction step involves summation over all potential word senses. The goal is to find embedding parameters which minimize the error in recovering context words based on the pivot word and the sense assignment. Parameters of both encoding and reconstruction are jointly optimized. In"
N16-1160,P12-1068,1,0.786917,"he same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for many language pairs as numerou"
N16-1160,D15-1243,0,0.0275842,"we prepare the word representations by taking a uniform average of all sense embeddings per word. The evaluation is carried out using the tool described in Faruqui and Dyer (2014a). Due to space constraints, we report the results by averaging over all benchmarks (Similarity), and include the individual results in the online repository. 5.2 Supersense similarity We also evaluate on a task measuring the similarity between the embeddings—in our case uniformly averaged in the case of multi-sense embeddings—and a matrix of supersense features extracted from the English SemCor, using the Qvec tool (Tsvetkov et al., 2015). We choose this method because it has been shown to output scores that correlate well with extrinsic tasks, e.g. text classification and sentiment analysis. We believe that this, in combination with word similarity tasks from the previous section, can give a reliable picture of the generic quality of word embeddings studied in this work. 5.3 POS tagging As our downstream evaluation task, we use the learned word representations to initialize the embedding layer of a neural network tagging model. We use the same convolutional architecture as Li and Juraf1350 sky (2015): an input layer taking a"
N16-1160,P10-1040,0,0.00925279,"monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time. 1 Introduction Approaches to learning word embeddings (i.e. realvalued vectors) relying on word context have received much attention in recent years, and the induced representations have been shown to capture syntactic and semantic properties of words. They have been evaluated intrinsically (Mikolov et al., 2013a; Baroni et al., 2014; Levy and Goldberg, 2014) and have also been used in concrete NLP applications to deal with word sparsity and improve generalization (Turian et al., 2010; Collobert et al., 2011; Bansal et al., 2014; Passos et al., 2014). While most work to date has focused on developing embedding models which represent a word with a single vector, some researchers have attempted to capture polysemy explicitly and have encoded properties of each word with multiple vectors (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Chen et al., 2014; Li and Jurafsky, 2015). Gertjan van Noord University of Groningen Netherlands g.j.m.van.noord@rug.nl In parallel to this work on multi-sense word embeddings, another line of research has investigated integrat"
N16-1160,P14-1011,0,0.0182934,"urce corpus using WSD heuristics, after which a recurrent neural network is trained to obtain sense-specific representations. Unlike in our work, the sense induction and embedding learning components are entirely separated, without a possibility for one to influence another. In a similar vein, Bansal et al. (2012) use bilingual corpora to perform soft word clustering, extending the previous work on the monolingual case of Lin and Wu (2009). Single-sense representations in the multilingual context have been studied more extensively (Lu et al., 2015; Faruqui and Dyer, 2014b; Hill et al., 2014a; Zhang et al., 2014; Faruqui and Dyer, 2013; Zou et al., 2013), with a goal of bringing the representations in the same semantic space. A related line of work concerns the crosslingual setting, where one tries to leverage training data in one language to build models for typically lower-resource languages (Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014; Soyer et al., 2014; Klementiev et al., 2012; T¨ackstr¨om et al., 2012). The recent works of Kawakami and Dyer (2015) and Nalisnick and Ravi (2015) are also of interest. The latter work on the infinite Skip-Gram model in which the embeddin"
N16-1160,D13-1141,0,0.114152,"ral languages in the same semantic space, which then enables the transfer of a model (e.g., a syntactic parser) trained on annotated training data in one language to another language lacking this annotation (Klementiev et al., 2012; Hermann and Blunsom, 2014; Gouws et al., 2014; Chandar A P et al., 2014). Secondly, information from another language can also be leveraged to yield better firstlanguage embeddings (Guo et al., 2014). Our paper falls in the latter, much less explored category. We adhere to the view of multilingual learning as a means of language grounding (Faruqui and Dyer, 2014b; Zou et al., 2013; Titov and Klementiev, 2012; Snyder and Barzilay, 2010; Naseem et al., 2009). Intuitively, polysemy in one language can be at least partially resolved by looking at the translation of the word and its context in another language (Kaji, 2003; Ng et al., 2003; Diab and Resnik, 2002; Ide, 2000; Dagan and Itai, 1994; Brown et al., 1991). Better sense assignment can then lead to better sense-specific word embeddings. We propose a model that uses second-language embeddings as a supervisory signal in learning multisense representations in the first language. This supervision is easy to obtain for ma"
N16-1160,J15-4004,0,\N,Missing
N16-1160,W09-0401,0,\N,Missing
N16-1160,N09-1003,0,\N,Missing
N16-1160,D14-1034,0,\N,Missing
N16-1160,bojar-etal-2012-joy,0,\N,Missing
N16-1160,P14-2131,0,\N,Missing
N16-1160,W13-2201,0,\N,Missing
oostdijk-etal-2008-coi,schuurman-2008-spatiotemporal,1,\N,Missing
oostdijk-etal-2008-coi,reynaert-2006-corpus,1,\N,Missing
oostdijk-etal-2008-coi,oostdijk-boves-2006-user,1,\N,Missing
oostdijk-etal-2008-coi,W97-1502,0,\N,Missing
oostdijk-etal-2008-coi,van-den-bosch-etal-2006-transferring,1,\N,Missing
oostdijk-etal-2008-coi,van-noord-etal-2006-syntactic,1,\N,Missing
oostdijk-etal-2008-coi,W07-1513,1,\N,Missing
oostdijk-etal-2008-coi,2006.jeptalnrecital-invite.2,1,\N,Missing
oostdijk-etal-2008-coi,W03-2414,1,\N,Missing
P04-1057,J01-1001,0,0.0062439,"d/or for large corpora. On the other hand, if a more compact data-structure is used, speed becomes an issue. Church (1995) shows that suffix arrays can be used for efficiently computing the frequency of n-grams, in particular for larger n. If the corpus size increases, the memory required for the suffix array may become problematic. We propose a new combination of suffix arrays with perfect hash finite automata, which reduces typical memory requirements by a factor of five, in combination with a modest increase in processing efficiency. 4.1 Suffix arrays Suffix arrays (Manber and Myers, 1990; Yamamoto and Church, 2001) are a simple, but useful datastructure for various text-processing tasks. A corpus is a sequence of characters. A suffix array s is an array consisting of all suffixes of the corpus, sorted alphabetically. For example, if the corpus is the string abba, the suffix array is ha,abba,ba,bbai. Rather than writing out each suffix, we use integers i to refer to the suffix starting at position i in the corpus. Thus, in this case the suffix array consists of the integers h3, 0, 2, 1i. It is straightforward to compute the suffix array. For a corpus of k + 1 characters, we initialize the suffix array by"
P11-1157,W06-1615,0,0.163561,"tivation Previous research on domain adaptation has focused on the task of adapting a system trained on one domain, say newspaper text, to a particular new domain, say biomedical data. Usually, some amount of (labeled or unlabeled) data from the new domain was given – which has been determined by a human. However, with the growth of the web, more and more data is becoming available, where each document “is potentially its own domain” (McClosky et al., 2010). It is not straightforward to determine Most previous work on domain adaptation, for instance Hara et al. (2005), McClosky et al. (2006), Blitzer et al. (2006), Daum´e III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that domains will be ‘given’. Moreover, a given corpus might not always be as homogeneous as originally t"
P11-1157,W06-2920,0,0.0504599,"eywords seem to come from a controlled vocabulary. There are 76 distinct topic markers. The three most frequent keywords are: TENDER OFFERS, MERGERS, ACQUISITIONS (TNM), EARNINGS (ERN), STOCK MARKET, OFFERINGS (STK). This reflects the fact that a lot of articles come from the financial domain. But the corpus also contains articles from more distant domains, like MARKETING, ADVERTISING (MKT), COMPUTERS AND INFORMATION TECHNOLOGY (CPR), HEALTH CARE PROVIDERS, MEDICINE, DENTISTRY (HEA), PETROLEUM (PET). 4 a system that can be trained on a variety of languages given training data in CoNLL format (Buchholz and Marsi, 2006). Additionally, the parser implements both projective and non-projective parsing algorithms. The projective algorithm is used for the experiments on English, while the non-projective variant is used for Dutch. We train the parser using default settings. MST takes PoS-tagged data as input; we use gold-standard tags in the experiments. We estimate topic models using Latent Dirichlet Allocation (Blei et al., 2003) implemented in the MALLET4 toolkit. Like Lippincott et al. (2010), we set the number of topics to 100, and otherwise use standard settings (no further optimization). We experimented wit"
P11-1157,P07-1033,0,0.652472,"Missing"
P11-1157,W01-0521,0,0.0719896,"ch model or data is most beneficial for an arbitrary piece of new text. Moreover, if we had such a measure, a related question is whether it can tell us something more about what is actually meant by “domain”. So far, it was mostly arbitrarily used to refer to some kind of coherent unit (related to topic, style or genre), e.g.: newspaper text, biomedical abstracts, questions, fiction. It is well known that parsing accuracy suffers when a model is applied to out-of-domain data. It is also known that the most beneficial data to parse a given domain is data that matches the domain (Sekine, 1997; Gildea, 2001). Hence, an important task is to select appropriate domains. However, most previous work on domain adaptation relied on the implicit assumption that domains are somehow given. As more and more data becomes available, automatic ways to select data that is beneficial for a new (unknown) target domain are becoming attractive. This paper evaluates various ways to automatically acquire related training data for a given test set. The results show that an unsupervised technique based on topic models is effective – it outperforms random data selection on both languages examined, English and Dutch. Mor"
P11-1157,I05-1018,0,0.0298232,"available for English. 1 Introduction and Motivation Previous research on domain adaptation has focused on the task of adapting a system trained on one domain, say newspaper text, to a particular new domain, say biomedical data. Usually, some amount of (labeled or unlabeled) data from the new domain was given – which has been determined by a human. However, with the growth of the web, more and more data is becoming available, where each document “is potentially its own domain” (McClosky et al., 2010). It is not straightforward to determine Most previous work on domain adaptation, for instance Hara et al. (2005), McClosky et al. (2006), Blitzer et al. (2006), Daum´e III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that domains will be ‘given’. Moreover, a given corpus mig"
P11-1157,J04-3001,0,0.0144241,"Missing"
P11-1157,P07-1034,0,0.0844254,"Missing"
P11-1157,W07-2416,0,0.025415,"arser on the training section (02-21). The result on the standard test set (section 23) is identical to previously reported results (excluding punctuation tokens: LAS 87.50, Unlabeled Attachment Score (UAS) 90.75; with punctuation tokens: LAS 87.07, UAS 89.95). The latter has been reported in (Surdeanu and Manning, 2010). English - Genia (G) & Brown (B) For the Domain Adaptation experiments, we added 1,552 articles from the GENIA10 treebank (biomedical abstracts from Medline) and 190 files from the Brown corpus to the pool of data. We converted the data to CoNLL format with the LTH converter (Johansson and Nugues, 2007). The size of the test files is, respectively: Genia 1,360 sentences with an average number of 26.20 words per sentence; the Brown test set is the same as used in the CoNLL 2008 shared task and contains 426 sentences with a mean of 16.80 words. 8 Using the LTH converter: http://nlp.cs.lth.se/ software/treebank_converter/ 9 This was a non-trivial task, as we actually noticed that some sentences have been omitted from the CoNLL 2008 shared task. 10 We use the GENIA distribution in Penn Treebank format available at http://bllip.cs.brown.edu/download/ genia1.0-division-rel1.tar.gz 1570 5 Experimen"
P11-1157,C10-1078,0,0.0387069,"Missing"
P11-1157,N06-1020,0,0.061119,"h. 1 Introduction and Motivation Previous research on domain adaptation has focused on the task of adapting a system trained on one domain, say newspaper text, to a particular new domain, say biomedical data. Usually, some amount of (labeled or unlabeled) data from the new domain was given – which has been determined by a human. However, with the growth of the web, more and more data is becoming available, where each document “is potentially its own domain” (McClosky et al., 2010). It is not straightforward to determine Most previous work on domain adaptation, for instance Hara et al. (2005), McClosky et al. (2006), Blitzer et al. (2006), Daum´e III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that domains will be ‘given’. Moreover, a given corpus might not always be as homo"
P11-1157,N10-1004,0,0.686263,"nglish and Dutch. Moreover, the technique works better than manually assigned labels gathered from meta-data that is available for English. 1 Introduction and Motivation Previous research on domain adaptation has focused on the task of adapting a system trained on one domain, say newspaper text, to a particular new domain, say biomedical data. Usually, some amount of (labeled or unlabeled) data from the new domain was given – which has been determined by a human. However, with the growth of the web, more and more data is becoming available, where each document “is potentially its own domain” (McClosky et al., 2010). It is not straightforward to determine Most previous work on domain adaptation, for instance Hara et al. (2005), McClosky et al. (2006), Blitzer et al. (2006), Daum´e III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous un"
P11-1157,H05-1066,0,0.232443,"Missing"
P11-1157,P10-2041,0,0.0284131,"mance loss. Their goal is different, but related: rather than finding related data for a new domain, they want to estimate the loss in accuracy of a PoS tagger when applied to a new domain. We will briefly discuss results obtained with the Renyi divergence in Section 5.1. Lippincott et al. (2010) examine subdomain variation in biomedicine corpora and propose awareness of NLP tools to such variation. However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis, 2010). A subset of the available data is automatically selected as training data for a Language Model based on a scoring mechanism that compares crossentropy scores. Their approach considerably outperformed random selection and two previous proposed approaches both based on perplexity scoring.1 3 Measures of Domain Similarity 3.1 Measuring Similarity Automatically Feature Representations A similarity function may be defined over any set of events that are con1 We tested data selection by perplexity scoring, but found the Language Models too small to be useful in our setting. sidered to be relevant"
P11-1157,D10-1069,0,0.0797885,"Missing"
P11-1157,plank-simaan-2008-subdomain,1,0.91806,"Missing"
P11-1157,W10-2105,1,0.835843,"Missing"
P11-1157,D08-1093,0,0.0157287,", it can be easily applied to other tasks or languages for which annotated (or automatically annotated) data is available. 2 Related Work The work most related to ours is McClosky et al. (2010). They try to find the best combination of source models to parse data from a new domain, which is related to Plank and Sima’an (2008). In the latter, unlabeled data was used to create several parsers by weighting trees in the WSJ according to their similarity to the subdomain. McClosky et al. (2010) coined the term multiple source domain adaptation. Inspired by work on parsing accuracy 1567 prediction (Ravi et al., 2008), they train a linear regression model to predict the best (linear interpolation) of source domain models. Similar to us, McClosky et al. (2010) regard a target domain as mixture of source domains, but they focus on phrasestructure parsing. Furthermore, our approach differs from theirs in two respects: we do not treat source corpora as one entity and try to mix models, but rather consider articles as base units and try to find subsets of related articles (the most similar articles); moreover, instead of creating a supervised model (in their case to predict parsing accuracy), our approach is ‘s"
P11-1157,A97-1015,0,0.157385,"determine which model or data is most beneficial for an arbitrary piece of new text. Moreover, if we had such a measure, a related question is whether it can tell us something more about what is actually meant by “domain”. So far, it was mostly arbitrarily used to refer to some kind of coherent unit (related to topic, style or genre), e.g.: newspaper text, biomedical abstracts, questions, fiction. It is well known that parsing accuracy suffers when a model is applied to out-of-domain data. It is also known that the most beneficial data to parse a given domain is data that matches the domain (Sekine, 1997; Gildea, 2001). Hence, an important task is to select appropriate domains. However, most previous work on domain adaptation relied on the implicit assumption that domains are somehow given. As more and more data becomes available, automatic ways to select data that is beneficial for a new (unknown) target domain are becoming attractive. This paper evaluates various ways to automatically acquire related training data for a given test set. The results show that an unsupervised technique based on topic models is effective – it outperforms random data selection on both languages examined, English"
P11-1157,W10-2605,0,0.309959,"Missing"
P11-1157,2006.jeptalnrecital-invite.2,1,0.787305,"Missing"
P11-1157,P09-1076,0,0.139875,"III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that domains will be ‘given’. Moreover, a given corpus might not always be as homogeneous as originally thought (Webber, 2009; Lippincott et al., 2010). For instance, recent work has shown that the well-known Penn Treebank (PT) Wall Street Journal (WSJ) actually contains a variety of genres, including letters, wit and short verse (Webber, 2009). In this study we take a different approach. Rather than viewing a given corpus as a monolithic entity, 1566 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1566–1576, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics we break it down to the article-level and disregard corpora boundaries. Given"
P11-1157,P10-1077,0,0.0155103,"surface characteristics (unlabeled text). This has the advantage that we do not need to rely on additional supervised tools; moreover, it is interesting to know how far we can get with this level of information only. We examine the following feature representations: relative frequencies of words, relative frequencies of character tetragrams, and topic models. Our motivation was as follows. Relative frequencies of words are a simple and effective representation used e.g. in text classification (Manning and Sch¨utze, 1999), while character n-grams have proven successful in genre classification (Wu et al., 2010). Topic models (Blei et al., 2003; Steyvers and Griffiths, 2007) can be considered an advanced model over word distributions: every article is represented by a topic distribution, which in turn is a distribution over words. Similarity between documents can be measured by comparing topic distributions. Similarity Functions There are many possible similarity (or distance) functions. They fall broadly into two categories: probabilistically-motivated and geometrically-motivated functions. The similarity functions examined in this study will be described in the following. The Kullback-Leibler (KL)"
P11-1157,C00-2137,0,0.0404575,"PoS tagging using two different taggers: MXPOST, the MaxEnt tagger of Ratnaparkhi5 and Citar,6 a trigram HMM tagger. In all experiments, parsing performance is measured as Labeled Attachment Score (LAS), the percentage of tokens with correct dependency edge and label. To compute LAS, we use the CoNLL 2007 evaluation script7 with punctuation tokens excluded from scoring (as was the default setting in CoNLL 2006). PoS tagging accuracy is measured as the percentage of correctly labeled words out of all words. Statistical significance is determined by Approximate Randomization Test (Noreen, 1989; Yeh, 2000) with 10,000 iterations. Experimental Setup 4.2 4.1 Tools & Evaluation The parsing system used in this study is the MST parser (McDonald et al., 2005), a state-of-the-art data-driven graph-based dependency parser. It is English - WSJ For English, we use the portion of the Penn Treebank Wall Street Journal (WSJ) that has been made available in the CoNLL 2008 shared 4 3 It is not known what IN stands for, as also stated in Mark Liberman’s notes in the readme of the ACL/DCI corpus. However, a reviewer suggested that IN might stand for “index terms” which seems plausible. 1569 Data 5 http://mallet"
P11-1157,N10-1091,0,\N,Missing
P11-2034,J97-4005,0,0.403553,"y of attribute-value grammars is their reversibility. Attribute-value grammars are usually coupled with separate statistical components for parse selection and fluency ranking. We propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for"
P11-2034,P09-2025,0,0.0818586,"treebank. The resulting dependency structures are fed into the Alpino chart generator to construct derivations for each dependency structure. The derivations for which the corresponding sentences are closest to the original sentence in the treebank are marked correct. Due to a limit on generation time, some longer sentences and corresponding dependency structures were excluded from the data. As a result, the average sentence length was 15.7 tokens, with a maximum of 26 tokens. To compare a realization to the correct sentence, we use the General Text Matcher (GTM) method (Melamed et al., 2003; Cahill, 2009). 3.4 Training the models Models are trained by taking an informative sample of Ω(c) for each c in the training data (Osborne, 2000). This sample consists of at most 100 randomly selected derivations. Frequency-based feature selection is applied (Ratnaparkhi, 1999). A feature f partitions Ω(c), if there are derivations d and d0 in Ω(c) such that f (c, d) 6= f (c, d0 ). A feature is used if it partitions the informative sample of Ω(c) for at least two c. Table 1 lists the resulting characteristics of the training data for each model. We estimate the parameters of the conditional 197 Inputs 3688"
P11-2034,P04-1014,0,0.0375364,"oduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To Gertjan van Noord University of Groningen g.j.m.van.noord@rug.nl some"
P11-2034,W07-1203,0,0.0121463,"mars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To Gertjan van Noord University of Groningen g.j.m.van.noord@rug.nl some extent this is"
P11-2034,A00-2021,0,0.25022,"e Kok and van Noord, 2010). In the experiments, the cdbl part of the Alpino Treebank (van der Beek et al., 2002) is used as training data (7,154 sentences). The WR-P-P-H part (2,267 sentences) of the LASSY corpus (van Noord et al., 2010), which consists of text from the Trouw 2001 newspaper, is used for testing. 3.1 Features The features that we use in the experiment are the same features which are available in the Alpino parser and generator. In the following section, these features are described in some detail. Word adjacency. Two word adjacency features are used as auxiliary distributions (Johnson and Riezler, 2000). The first feature is the probability of the sentence according to a word trigram model. The second feature is the probability of the sentence according to a tag trigram model that uses the partof-speech tags assigned by the Alpino system. In both models, linear interpolation smoothing for unknown trigrams, and Laplacian smoothing for unknown words and tags is applied. The trigram models have been trained on the Twente Nieuws Corpus corpus (approximately 110 million words), excluding the Trouw 2001 corpus. In conventional parsing tasks, the value of the word trigram model is the same for all"
P11-2034,P99-1069,0,0.219742,"se selection and fluency ranking. We propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied b"
P11-2034,T75-1004,0,0.795418,"Missing"
P11-2034,N03-2021,0,0.082798,"ency structure in the treebank. The resulting dependency structures are fed into the Alpino chart generator to construct derivations for each dependency structure. The derivations for which the corresponding sentences are closest to the original sentence in the treebank are marked correct. Due to a limit on generation time, some longer sentences and corresponding dependency structures were excluded from the data. As a result, the average sentence length was 15.7 tokens, with a maximum of 26 tokens. To compare a realization to the correct sentence, we use the General Text Matcher (GTM) method (Melamed et al., 2003; Cahill, 2009). 3.4 Training the models Models are trained by taking an informative sample of Ω(c) for each c in the training data (Osborne, 2000). This sample consists of at most 100 randomly selected derivations. Frequency-based feature selection is applied (Ratnaparkhi, 1999). A feature f partitions Ω(c), if there are derivations d and d0 in Ω(c) such that f (c, d) 6= f (c, d0 ). A feature is used if it partitions the informative sample of Ω(c) for at least two c. Table 1 lists the resulting characteristics of the training data for each model. We estimate the parameters of the conditional"
P11-2034,P05-1011,0,0.0220256,"fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To Gertjan van Noord University of Groningen g.j."
P11-2034,W05-1510,0,0.0216691,"if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To Gertjan van Noord University of Groningen g.j.m.van.noord@rug.nl some extent this is reasonable, because some features are only relevant in a certain direction. For instance, features that represent aspects of the surface word order are important for generation, but irrelevant for parsing. Similarly, features which describe aspects of the logical form are important for parsing, but irrelevant for ge"
P11-2034,C00-1085,0,0.686985,"ivation constructed by rule 233’. In addition, there are features describing more complex syntactic patterns such as: fronting of subjects and other noun phrases, orderings in the middle field, long-distance dependencies, and parallelism of conjuncts in coordination. 3.2 Parse disambiguation Earlier we assumed that a treebank is a set of correct derivations. In practice, however, a treebank only contains an abstraction of such derivations (in our case sentences with corresponding dependency structures), thus abstracting away from syntactic details needed in a parse disambiguation model. As in Osborne (2000), the derivations for the parse disambiguation model are created by parsing the training corpus. In the current setting, up to at most 3000 derivations are created for every sentence. These derivations are then compared to the gold standard dependency structure to judge the quality of the parses. For a given sentence, the parses with the highest concept accuracy (van Noord, 2006) are considered correct, the rest is treated as incorrect. Generation Parse Reversible Fluency ranking For fluency ranking we also need access to full derivations. To ensure that the system is able to generate from the"
P11-2034,P02-1035,0,0.0482951,"cal model is employed both for parse selection and fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generatio"
P11-2034,2006.jeptalnrecital-invite.2,1,0.880111,"Missing"
P11-2034,W07-2201,1,0.915836,"Missing"
P11-2034,W06-1661,0,0.055508,"ining a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To Gertjan van Noord University of Groningen g.j.m.van.noord@rug.nl some extent this is reasonable, because some features are only relevant in a certain direction. For instance, features that represent aspects of the surface word order are important for generation, but irrelevant for parsing. Similarly, features which describe aspects of the logical form are important for par"
P11-2034,W98-1426,0,\N,Missing
P11-2034,J88-1004,0,\N,Missing
P11-2034,W02-2103,0,\N,Missing
P11-2034,W00-1505,0,\N,Missing
P11-2034,W05-1612,0,\N,Missing
P11-2034,J90-3001,0,\N,Missing
P11-2034,W96-0411,0,\N,Missing
P11-2034,A00-2023,0,\N,Missing
P11-2034,C94-1039,1,\N,Missing
P11-2034,W03-1013,0,\N,Missing
P11-2034,W08-2222,0,\N,Missing
P11-2034,J08-1003,0,\N,Missing
P11-2034,W07-2303,0,\N,Missing
P11-2034,J96-1002,0,\N,Missing
P11-2034,P84-1018,0,\N,Missing
P11-2034,P95-1034,0,\N,Missing
P11-2034,P02-1036,0,\N,Missing
P11-2034,C88-2128,0,\N,Missing
P11-2034,P90-1026,0,\N,Missing
P11-2034,P04-1057,1,\N,Missing
P11-2034,P99-1018,0,\N,Missing
P11-2034,A00-1031,0,\N,Missing
P11-2034,W09-0608,0,\N,Missing
P11-2034,P06-1042,0,\N,Missing
P11-2034,W11-2708,1,\N,Missing
P11-2034,2009.eamt-1.21,0,\N,Missing
P11-2034,P85-1015,0,\N,Missing
P11-2034,P83-1021,0,\N,Missing
P11-2034,I05-1015,0,\N,Missing
P11-2034,W05-0908,0,\N,Missing
P11-2034,P96-1027,0,\N,Missing
P11-2034,W04-3223,0,\N,Missing
P11-2034,2005.mtsummit-papers.15,0,\N,Missing
P11-2034,W03-1020,0,\N,Missing
P11-2034,W09-2609,1,\N,Missing
P11-2034,W02-2018,0,\N,Missing
P11-2034,W10-4216,1,\N,Missing
P11-2034,P11-1111,0,\N,Missing
P17-2078,W15-4319,0,0.0222329,"Missing"
P17-2078,W15-4322,0,0.0234935,"Missing"
P17-2078,N15-1045,0,0.0438056,"Missing"
P17-2078,P11-2124,0,0.0289107,"of special characters or capitals and their position in the sentence. Parsing word lattices is not a new problem. The parsing as intersection algorithm (Bar-Hillel et al., 1961) laid the theoretical background for efMethod We first describe how an existing normalization model is modified for this specific use. Then we discuss how we integrate this normalization into the parsing model. 1 492 www.aspell.net Corpus ficiently deriving the best parse tree of a word lattice given a context-free grammar. Previous work on parsing a word lattice in a PCFG-LA setup includes Constant et al. (2013), and Goldberg and Elhadad (2011) for the Berkeley Parser. However, these models do not support probabilities, which are naturally provided by the normalization in our setup. Another problem is the handling of word ambiguities, which is crucial in our model. Our adaptations to the Berkeley Parser resemble the adaptations done by Goldberg and Elhadad (2011). In addition, we allow multiple words on the same position. For every POS tag in every position we only keep the highest scoring word. This suffices, since there is no syntactic ambiguity possible with only unary rules from POS tags to words, and therefore it is impossible"
P17-2078,P11-1038,0,0.0258361,"Missing"
P17-2078,D15-1157,0,0.0233651,"Missing"
P17-2078,D14-1108,0,0.051295,"ed approach; the extra information from the normalization can be useful for parsing. The non-canonical language use on social media introduces many difficulties for existing NLP models. For some NLP tasks, there has already been an effort to annotate enough data to train models, e.g. named entity recognition (Baldwin et al., 2015), sentiment analysis (Nakov et al., 2016) and paraphrase detection (Xu et al., 2015). For parsing social media texts, such a resource is not available yet, although there are some small treebanks that can be used for development/testing purposes (Foster et al., 2011; Kong et al., 2014; Kaljahi et al., 2015; Daiber and van der Goot, 2016). To the best of our knowledge, the only treebank big enough to train a supervised parser for user generated content is the English Web Treebank (Petrov and McDonald, 2012). This treebank consists of constituency trees from five different web domains, not including the domain of social media. 491 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 491–497 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2078"
P17-2078,L16-1102,1,0.698969,"Missing"
P17-2078,P14-3012,0,0.0606542,"Missing"
P17-2078,N10-1060,0,0.0700475,"Missing"
P17-2078,J93-2004,0,0.0597362,"Missing"
P17-2078,S16-1001,0,0.0194231,"ce would be ‘this is nice’. The parser can disambiguate this word graph because it has access to the syntactic context: ‘is’ is usually tagged as VBZ, while ‘as’ is mostly tagged as IN. This example shows the main motivation for using an integrated approach; the extra information from the normalization can be useful for parsing. The non-canonical language use on social media introduces many difficulties for existing NLP models. For some NLP tasks, there has already been an effort to annotate enough data to train models, e.g. named entity recognition (Baldwin et al., 2015), sentiment analysis (Nakov et al., 2016) and paraphrase detection (Xu et al., 2015). For parsing social media texts, such a resource is not available yet, although there are some small treebanks that can be used for development/testing purposes (Foster et al., 2011; Kong et al., 2014; Kaljahi et al., 2015; Daiber and van der Goot, 2016). To the best of our knowledge, the only treebank big enough to train a supervised parser for user generated content is the English Web Treebank (Petrov and McDonald, 2012). This treebank consists of constituency trees from five different web domains, not including the domain of social media. 491 Proc"
P17-2078,N07-1051,0,0.105385,"ificant improvement with an F1 score of 67.36, while using the best normalization sequence results in an F1 score of only 66.94. 1 this (0.5) as (0.5) nice (0.7) ths (0.3) is (0.4) nive (0.2) thus (0.2) 1 s (0.1) 2 rice (0.1) 3 Figure 1: A possible output of the normalization model for the sentence ‘ths s nice’. The magnitude of domain adaptation problems for the social media domain becomes clear when training the Berkeley parser on newswire text, and comparing its in-domain performance with performance on the Twitter domain. The Berkeley parser achieves an F1 score above 90 on newswire text (Petrov and Klein, 2007). An empirical experiment that we carried out on a Twitter treebank shows that the F1 score drops below 70 for this domain. Introduction Annotating a new training treebank for this domain would not only be an expensive solution, the ever-changing nature of social media makes this approach less effective over time. We propose an approach in which we integrate normalization into the parsing model. The normalization model provides the parser with different normalization candidates for each word in the input sentence. Existing algorithms can then be used to find the optimal parse tree over this la"
P17-2078,S15-2001,0,0.0217443,"Missing"
P17-2078,P13-1114,0,0.177102,"Missing"
P89-1002,E89-1032,0,0.151303,"ieve the underlying method to be more broadly applicable. A variant of our method is used in Van Noord&apos;s BUG (Bottom-Up Generator) system, part of MiMo2, an experimental machine translation system for translating international news items of Teletext, which uses a Prolog version of PATI~-II similar to that of Hirsh (1987). According to Martin Kay (personal communication), the STREP machine translation project at the Center for the Study of Language and Information uses a version of our algorithm to generate with respect to grammars based on head-driven phrase-structure grammar (HPSG). Finally, Calder et al. (1989) report on a generation algorithm for unification categorial grammar that appears to be a special case of ours. 3 Problems with Generators Existing Existing generation algorithms have efficiency or termination problems with respect to certain classes of grammars. We review the problems of both top-down and bottom-up regimes in this section. 3.1 Problems with Top-Down Generators Consider a naive top-down generation mechanism that takes as input the semantics to generate from and a corresponding syntactic category and builds a complete tree, top-down, left-to-right by applying rules of the gramm"
P89-1002,1988.tmi-1.12,0,0.737425,"ction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose welbfoundedness relies t D e p a r t m e n t of Linguistics Rijksuniversiteit Utrecht Utrecht, Netherlands on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property, discussed further in Section 3.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called--has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance of a uniform architecture for pa"
P89-1002,J87-1005,1,0.63248,"than necessary because the distribution of store elements among the subject and complements of a verb does not check whether the variable bound by a store element actually appears in the semantics of the phrase to which it is being assigned, leading to many dead ends in the generation process. Also, the rules are sound for generation but not for analysis, because they do not enforce the constraint that every occurrence of a variable in logical form be outscoped by the variable&apos;s binder. Adding appropriate side conditions to the rules, following the constraints discussed by Hobbs and Shieber (Hobbs and Shieber, 1987) would not be difficult. 6.3 Postponing Lexical Choice As it stands, the generation algorithm chooses particular lexical forms on-line. This approach can lead to a certain amount of unnecessary nondeterminism. For instance, the choice of verb form might depend on syntactic features of the verb&apos;s subject available only after the subject has been generated. This nondeterminism can be eliminated by deferring lexical choice to a postprocess. The generator will yield a list of lexical items instead of a list of words. To this list a small phonological front end is applied. BUG uses such a mechanism"
P89-1002,P83-1021,1,0.865954,"iven fashion. 1 Introduction The problem of generating a well-formed naturallanguage expression from an encoding of its meaning possesses certain properties which distinguish it from the converse problem of recovering a meaning encoding from a given natural-language expression. In previous work (Shieber, 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural-language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983) but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural-language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with r"
P89-1002,P85-1018,1,0.928086,"ortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called--has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance of a uniform architecture for parsing and generation--just as the extended Earley parser (Shieber, 1985b) and the bottom-up generator were instances of the generalized Earley deduction architecture= our efforts to date have been aimed foremost toward the development of the algorithm for generation alone. We will have little to say about its relation to parsing, leaving such questions for later research.1 2 Applicability of the Algorithm As does the Earley-based generator, the new algorithm assumes that the grammar is a unificationbased or logic grammar with a phrase-structure backbone and complex nonterminMs. Furthermore, and again consistent with previous work, we assume that the nonterminals"
P89-1002,C88-2128,1,0.294441,"n Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. 1 Introduction The problem of generating a well-formed naturallanguage expression from an encoding of its meaning possesses certain properties which distinguish it from the converse problem of recovering a meaning encoding from a given natural-language expression. In previous work (Shieber, 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural-language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983) but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural-language expressions, the Earley deduction method is reasonably successful along certain dimensio"
P89-1002,C88-2150,0,0.450177,"ns, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose welbfoundedness relies t D e p a r t m e n t of Linguistics Rijksuniversiteit Utrecht Utrecht, Netherlands on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property, discussed further in Section 3.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called--has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance"
P89-1002,J81-4003,1,\N,Missing
P91-1015,E89-1032,0,0.0275864,"his parser do not carry over U w(Di) C_ w(M) to this generalized version, as redundant search i=l paths for CF-based grammars turn out to be genLinearity requires that the difference of the car- uine parts of the search space for F-LCFR gramdinalities of these sets is a constant factor; i.e. a mars. rule may only introduce a fixed number of words The advantage of my algorithm is that it both syncategorematically: employs bottom-up and top-down filtering in a straightforward way. The algorithm is closely reIw(M)l- IU w(Oi)) = c,c a constant lated to head-driven generators (van Noord, 1989; i=1 Calder et al., 1989; Shieber et al., 1989; van NoCF-based formalisms clearly fulfill this require- ord, 1990a; Shieber et ai., 1990). The algorithm ment, as do Head Grammars, grammars using proceeds in a bottom-up, head-driven fashion. In sequence union, and TAG's. I assume in the re- modern linguistic theories very much information mainder of this paper that I.Jin=l w(Di) = w(M), is defined in lexical entries, whereas rules are refor all rules other than lexical entries (i.e. all duced to very general (and very uninformative) words are introduced on a terminal). Note though schemata. More information usually im"
P91-1015,C90-3017,0,0.0369856,"Missing"
P91-1015,J90-1004,1,0.89617,"Missing"
P91-1015,E89-1014,0,0.0394963,"Missing"
P91-1015,C90-2052,1,\N,Missing
P91-1015,C88-2147,0,\N,Missing
P91-1015,P85-1021,0,\N,Missing
P91-1015,P87-1015,0,\N,Missing
P91-1015,P85-1015,0,\N,Missing
P91-1015,W89-0206,0,\N,Missing
P91-1015,P89-1002,1,\N,Missing
P94-1021,E93-1016,0,0.0255646,"position and scope of adjuncts in such constructions. Delayed evaluation is used to process grammars containing recursive constraints. 1 Recursive 2.1 Introduction Subject-verb agreement Consider a categorial treatment of subject-verb agreement with intransitive ( NP[NOM]S ) and transitive ((NP[NOM]S)/NP[ACC]) verbs defined as follows: Combinations of Categorial Grammar (co) and unification naturally lead to the introduction of polymorphic categories. Thus, Karttunen (1989) categorizes NP&apos;s as X/X, where x is a verbal category, Zeevat el al. (1987) assign the category X/(NPX) to NP&apos;s, and Emms (1993) extends the Lambek-calculus with polymorphic categories to account for coordination, quantifier scope, and extraction. (1) lez(walks,X):iv(X). /ez(kisses, X) :- tv(X). vat[ eat s ] The role of polymorphism has been restricted, however, by the fact that in previous work categories were defined as feature structures using the simple, nonrecursive, constraints familiar from feature description languages such as PATR. Relational constraints can be used to define a range of polymorphic categories that are beyond the expressive capabilities of previous approaches. iv( dir&apos;&apos; arg [ catnp ] )&quot; case n"
P94-1021,J91-3003,0,\N,Missing
P94-1021,J93-4001,0,\N,Missing
P95-1022,P89-1018,0,0.0458607,"ere 'Joe's"" is the name of a restaurant) using a finite state transducer. In a straightforward approach this would also lead to a finite-state automaton with cycles. It can be shown that the computation of the intersection of a FSA and a CFG requires only a rain159 imal generalization of existing parsing algorithms. We simply replace the usual string positions with the names of the states in the FSA. It is also straightforward to show that the complexity of this process is cubic in the number of states of the FSA (in the case of ordinary parsing the number of states equals n + 1) (Lang, 1974; Billot and Lang, 1989) (assuming the right-hand-sides of grammar rules have at most two categories). In this paper w e investigate whether the same techniques can be applied in case the grammar is a constraint-based grammar rather than a CFG. For specificity we will take the grammar to be a Definite Clause G r a m m a r (DCG) (Pereira and Warren, 1980). A DCG is a simple example of a family of constraintbased grammar formalisms that are widely used in natural language analysis (and generation). The main findings of this paper can be extended to other members of that family of constraint-based grammar formalisms. wi"
P95-1022,C88-1075,0,0.133496,"4). Such techniques might be of use both in the case of written and spoken language input. In the latter case another possible application concerns the treatment of phenomena such as repairs (Carter, 1994). Note that we allow the input to be a full FSA (possibly including cycles, etc.) since some of the above-mentioned techniques indeed result in cycles. Whereas an ordinary word-graph always defines a finite language, a FSA of course can easily define an infinite number of sentences. Cycles might emerge to treat unknown sequences of words, i.e. sentences with unknown parts of unknown lengths (Lang, 1988). As suggested by an ACL reviewer, one could also try to model haplology phenomena (such as t h e ' s in English sentences like 'The chef at Joe's hat', where 'Joe's"" is the name of a restaurant) using a finite state transducer. In a straightforward approach this would also lead to a finite-state automaton with cycles. It can be shown that the computation of the intersection of a FSA and a CFG requires only a rain159 imal generalization of existing parsing algorithms. We simply replace the usual string positions with the names of the states in the FSA. It is also straightforward to show that t"
P95-1022,P83-1021,0,0.0745937,"ustrates that this method is not very useful yet; all the work has still to be done. start(qO), final(q2). trans(qO,a,ql), trans(qO,b, q2). trans(ql,a,qO). trans(q2,b, q2). 161 A1 Aa A2 10111 1 10 B~ B1 B3 lU 10 0 Figure 2: Instance of a PCP problem. As AI 10111 A1 A3 1 10 1 + B2 BI 10 = 101111110 + B1 B3 111 111 = 101111110 Figure 3: Illustration of a solution for the PCP problem of figure 2. nal actions defined in curly braces). 3.1 But if we use existing techniques for parsing DCGs, then we are also confronted with an undecidability problem: the recognition problem for DCGs is undecidable (Pereira and Warren, 1983). A fortiori the problem of deciding whether the intersection of a FSA and a DCG is empty or not is undecidable. This undecidability result is usually circumvented by considering subsets of DCGs which can be recognized effectively. For example, we can restrict the attention to DCGs of which the contextfree skeleton does not contain cycles. Recognition for such 'off-line parsable' grammars is decidable (Pereira and Warren, 1983). Most existing constraint-based parsing algorithms will terminate for grammars that exhibit the property that for each string there is only a finite number of possible"
P95-1022,C88-2118,0,0.0262341,"uncertain about the actual string of words that has been uttered, and thus produces a word lattice of the most promising hypotheses, rather than a single sequence of words. FSA of course generalizes such word lattices. As another example, certain techniques to deal with ill-formed input can be characterized as finite state transducers (Lang, 1989); the composition of an input string with such a finite state transducer results in a FSA that can then be input for syntactic parsing. Such an approach allows for the treatment of missing, extraneous, interchanged or misused words (Teitelbaum, 1973; Saito and Tomita, 1988; Nederhof and Bertsch, 1994). Such techniques might be of use both in the case of written and spoken language input. In the latter case another possible application concerns the treatment of phenomena such as repairs (Carter, 1994). Note that we allow the input to be a full FSA (possibly including cycles, etc.) since some of the above-mentioned techniques indeed result in cycles. Whereas an ordinary word-graph always defines a finite language, a FSA of course can easily define an infinite number of sentences. Cycles might emerge to treat unknown sequences of words, i.e. sentences with unknown"
R09-1012,adolphs-2008-acquiring,0,0.0612124,"mber of occurrences found, we try to identify the correct root and paradigm of the unknown word. Commercial search engines have already been successfully used for various NLP tasks (Keller and Lapata, 2003) and it is our claim that they are sucient for ours as well. Since the whole paradigm of the unknown word is generated, it would be very dicult to nd a large number of occurrences for each form in a wrong paradigm. For example, the generated adjective and verb forms for schnabbel have no or very few occurrences on the web and they can be safely rejected. A similar approach, described in (Adolphs, 2008), applies nite state techniques to generate possible inectional classes for unknown German words. However, disambiguation is done by using metrics based on frequency counts obtained from a corpus. Thus disambiguation depends heavily on the size and the gender of the corpus which is a drawback in comparison with the virtually unlimited data in the web our method has access to. If a word is, for instance, both a noun and a verb, it is possible that it would occur only as a noun in a given corpus and the method would fail to deal with the morphological ambiguity. (Nakov et al., 2003) use a rule"
R09-1012,J03-3005,0,0.0425275,"a rst person singular present verb. As a consequence, three possible roots are produced. Since there is no way to know which of them is the correct one, in generation mode, all possible paradigms for each of these roots are generated. The problem of disambiguating the output of the FSTs is dealt with in the second phase. We use Yahoo to search the web for each root and generated paradigm form and, based on the number of occurrences found, we try to identify the correct root and paradigm of the unknown word. Commercial search engines have already been successfully used for various NLP tasks (Keller and Lapata, 2003) and it is our claim that they are sucient for ours as well. Since the whole paradigm of the unknown word is generated, it would be very dicult to nd a large number of occurrences for each form in a wrong paradigm. For example, the generated adjective and verb forms for schnabbel have no or very few occurrences on the web and they can be safely rejected. A similar approach, described in (Adolphs, 2008), applies nite state techniques to generate possible inectional classes for unknown German words. However, disambiguation is done by using metrics based on frequency counts obtained from a c"
R09-1012,J97-3003,0,0.065648,"done by using metrics based on frequency counts obtained from a corpus. Thus disambiguation depends heavily on the size and the gender of the corpus which is a drawback in comparison with the virtually unlimited data in the web our method has access to. If a word is, for instance, both a noun and a verb, it is possible that it would occur only as a noun in a given corpus and the method would fail to deal with the morphological ambiguity. (Nakov et al., 2003) use a rule-based approach to guess the morphological classes of unknown German nouns where each induced rule is ranked in the manner of (Mikheev, 1997). However, it is not clear if the method can scale to other word classes. (van den Bosch and Daelemans, 1999) apply memory-based learning to provide a detailed morphological analysis of Dutch. The method is tested on frequent dictionary words and only an estimate is provided about its expected performance on real-world data. We should mention that the work described here is part of an algorithm for the automated acquisition of lexical types for words unknown to the Alpino grammar and parser (van Noord, 2006). The information provided by the generated paradigms is used as features in a statisti"
R09-1012,W01-1815,1,0.754763,"Missing"
R09-1012,P99-1037,0,0.0439016,"Missing"
R09-1012,2006.jeptalnrecital-invite.2,1,0.770477,"Missing"
R11-1049,P98-1014,0,0.123822,"Missing"
R11-1049,I05-1015,0,0.0147384,"igate if the acquired lexical entries affect sentence realisation. The GG adopts Minimal Recursion Semantics (MRS, Copestake et al. (2005)) as semantic representation. This, together with the fine-grained linguistic information in the GG lexical types, allows for finding the textual realisations for a given input semantic representation. Sentence realisation with the GG is performed within the LKB grammar engineering platform which provides an efficient generation engine. This engine is essentially a chart-based generator (Kay, 1996) with various optimisations for MRS and packed parse forest (Carroll and Oepen, 2005). As there are less ordering constraints in the semantic representation (comparing to the word sequence in parsing inputs), the computation is intrinsically more expensive. While in parsing the ambiguity in the less constrained lexical entries acquired with LA dissolves quickly in its context, there is a potential risk of overgeneration in sentence realisation. We conduct an indicative experiment with 14 unknown words from the test set used in Section 4.1. These words have been assigned verb types by the classifier. The focus of the experiment is on verbs because of the large number of possibl"
R11-1049,R09-1012,1,0.892635,"Missing"
R11-1049,C10-2018,1,0.7426,"Missing"
R11-1049,D10-1088,1,0.807892,"Missing"
R11-1049,W08-1708,1,0.874614,"Missing"
R11-1049,copestake-flickinger-2000-open,0,0.148867,"Missing"
R11-1049,W00-0740,0,0.0329213,"Missing"
R11-1049,E03-1041,0,0.0367437,"Missing"
R11-1049,P96-1027,0,0.0164094,"luation, extending the evaluation methodology of C& V N, we also investigate if the acquired lexical entries affect sentence realisation. The GG adopts Minimal Recursion Semantics (MRS, Copestake et al. (2005)) as semantic representation. This, together with the fine-grained linguistic information in the GG lexical types, allows for finding the textual realisations for a given input semantic representation. Sentence realisation with the GG is performed within the LKB grammar engineering platform which provides an efficient generation engine. This engine is essentially a chart-based generator (Kay, 1996) with various optimisations for MRS and packed parse forest (Carroll and Oepen, 2005). As there are less ordering constraints in the semantic representation (comparing to the word sequence in parsing inputs), the computation is intrinsically more expensive. While in parsing the ambiguity in the less constrained lexical entries acquired with LA dissolves quickly in its context, there is a potential risk of overgeneration in sentence realisation. We conduct an indicative experiment with 14 unknown words from the test set used in Section 4.1. These words have been assigned verb types by the class"
R11-1049,J03-3001,0,0.0093745,"alization of morphological properties but they proved to be less informative for the classifier. Further, the paradigm generation method outputs a single paradigm for Abfahrten indicating that this word is a singular feminine noun. This information is explicitly used as a feature in the classifier which is shown in row (v) of Table 1. entries in the lexicon mapped onto it and it is assigned to at least 15 distinct words occurring in large corpora parsed with PET and the GG. The parsed corpus we use consists of roughly 2.5M sentences randomly selected from the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). The Wacky project aims at the creation of large corpora for different languages, including German, from various web sources, such as online newspapers and magazines, legal texts, internet fora, etc. Following these criteria, we have selected 39 open-class types out of the 411 lexical types defined in the GG. As described in Section 2.2, we re-defined the type definitions of the 39 types which resulted in the creation of 68 expanded types. This number is smaller than the 611 types used in the experiments with Alpino because the GG does not have a full form lexicon. Table 2 gives more details"
R11-1049,W02-2018,0,0.00948395,"the word. All other morphological forms are derived by applying various morphological rules defined in the GG to the word stem. For this reason, we employ the paradigm not only as a source of features for the classifier but also as a way to map the unknown word to its stem. The stem for nouns is the singular nominative noun form, for adjectives it is the base nonin(1) p(t|c) = P Θi fi (t,c)) P exp( i P ′ t′ ∈T exp( i Θi fi (t ,c)) where fi (t, c) may encode arbitrary characteristics of the context and &lt; Θ1 , Θ2 , ... > can be evaluated by maximising the pseudo-likelihood on a training corpus (Malouf, 2002). Table 1 shows the features for Abfahrten. Row (i) contains 4 separate features derived from the prefix of the word and 4 other suffix features are 2 357 TADM; http://tadm.sourceforge.net/ given in row (ii). The two features in rows (iii) and (iv) indicate whether the word starts with a separable particle and if it contains a hyphen, respectively. Since it is the stem of the unknown word we add to the lexicon, we also experimented with prefix and suffix features extracted from the stem. We assumed that those could allow for a better generalization of morphological properties but they proved t"
R11-1049,2006.jeptalnrecital-invite.2,1,0.887295,"Missing"
R11-1049,zhang-kordoni-2006-automated,1,0.84341,"Missing"
R11-1049,W05-1008,0,\N,Missing
R11-1049,C98-1014,0,\N,Missing
R19-1140,D16-1250,0,0.147197,"We perform experiments on Turkish and Finnish as a pair of morphologically complex languages and compare our approach with the baseline models. 2 The Morpheme-Based Alignment Model Figure 1: Morpheme-based cross-lingual alignment model that contains a source side word encoder for morphemes. The encoder is trained to learn morpheme representations in the target space Baselines In this paper, we consider two baseline models. As the first baseline, we employ a simple projection-based CLE method which learns a mapping between embedding spaces by solving the Procrustes problem (Smith et al., 2017; Artetxe et al., 2016). This method first learns a linear transformation matrix to minimize the distance between vectors of word pairs in a seed dictionary by imposing the orthogonality constraint (Gower et al., 2004) and then it uses this matrix to transform the source language embedding space to represent both languages in a shared embedding space. The baseline method is denoted by Procrustes in this paper. As the second baseline, we use relaxed crossdomain similarity local scaling (RCSLS) (Joulin et al., 2018). RCSLS optimizes the transformation matrix by maximizing the cross-domain similarity 1 Code available a"
R19-1140,Q17-1010,0,0.158277,"in similarity local scaling (RCSLS) (Joulin et al., 2018). RCSLS optimizes the transformation matrix by maximizing the cross-domain similarity 1 Code available at: https://bitbucket.org/ ahmetustunn/morphology-sensitive-cle local scaling (CSLS) score, instead of minimizing the distance between word pairs in the training dictionary. CSLS is a modification of cosine similarity commonly used in information retrieval. In this way, RCSLS relaxes the orthogonality constraint used in Procrustes according to a retrieval criterion. Note that for the both baseline models and our model, we use fastText (Bojanowski et al., 2017) to generate monolingual word embeddings. Fasttext represents words as sequence of character ngrams but in many cases this is suboptimal since ¨ un not all character n-grams are morphemes (Ust¨ et al., 2018). Besides that the aim of this study is to incorporate morphology into cross-lingual training, whereas fastText is designed for monolingual training. Morpheme-based Model In the morphemebased model, we extend the projection-based baseline (Procrustes) in order to exploit subword (morpheme) level information for the crosslingual mapping. Our model starts by splitting all words in the source"
R19-1140,W16-1603,0,0.0220837,"Missing"
R19-1140,D17-1070,0,0.210942,"In the figure, x denotes the fixed length word representation generated by the word encoder through morphemes and y represents the target side word embedding. The source encoder is trained to mimic target word embeddings in the bilingual dictionary by minimizing the loss function: Lalign = dist(x, y) − λ(dist(xc , y) + dist(x, yc )) where (x, y) corresponds to the source and target word embeddings, (xc , yc ) is a contrastive 1223 term. λ2 controls the effect of the negative samples in the alignment loss. We use the cosine similarity for the distance measure. For the encoder model, following (Conneau et al., 2017a), we use bidirectional LSTMs with max pooling. It encodes the words in both the forward and the backward direction to capture unidirectional information, then it combines the resulting numbers to form a fixed-size vector by selecting the maximum value over each dimension of the hidden units. Figure 2 shows the encoder model. In the figure, each word vector u is computed from morphemes mn through the bidirectional LSTM encoder. features for both languages by using the Universal Dependency Treebanks (Nivre et al., 2016) and the Universal Morphology (Sylak-Glassman, 2016) project.3 Each word pa"
R19-1140,D18-1269,0,0.0426043,"Missing"
R19-1140,P15-1119,0,0.0733669,"Missing"
R19-1140,D18-1330,0,0.0258817,"Missing"
R19-1140,C12-1089,0,0.0880734,"Missing"
R19-1140,W15-1844,0,0.0343659,"Missing"
R19-1140,P18-1072,0,0.0437631,"Missing"
R19-1140,W18-3019,1,0.859248,"Missing"
R19-1140,P15-2118,0,0.026681,"Missing"
R19-1140,N16-1156,0,0.0306812,"Missing"
S15-2007,S14-2114,1,0.894383,"Missing"
S15-2007,W08-2222,0,0.034094,"S) tags are provided by a tagger that is adapted to twitter (Derczynski et al., 2013). Named entity tags are also obtained from an adapted tagger (Ritter et al., 2011). 3 Method The model is based on a state-of-the-art semantic similarity prediction model (Bjerva et al., 2014). It is mainly based on overlap features extracted from different parsers, but also includes synset overlap, and a Compositional Distributional Semantic Model (CDSM). The parsers used in this model are a constituency parser (Steedman, 2001), logical parser Paradox (Claessen and S¨orensson, 2003) and the DRS parser Boxer (Bos, 2008). 3.1 Features Our model uses 25 features in total. Due to space constraints we cannot describe them all in detail here. Instead we group the features as follows: • Lexical features: word overlap, proportional sentence length difference. • POS: noun overlap, verb overlap. • Logical model: instance overlap, relation overlap. 41 • DRS: agent overlap, patient overlap, DRS complexity. • Entailments: binary features for: neutral, entailment and contradiction predictions. • CDSM: The cosine distance between the element wise addition of the vectors in each sentence is used. • Synsets (WordNet): The d"
S15-2007,P09-1053,0,0.207983,"we use the raw and the normalized sentence in the model. For each feature, scores are calculated for both versions of the sentence. The highest of these scores be used as input for our maximum entropy model. 4 Evaluation This chapter is divided in the two sub tasks of paraphrase detection and similarity prediction. A strong 2 www.aspell.net 42 Figure 2: F-Score for the different classifiers. P is the threshold that decides if a sentence pair is a paraphrase. baseline is used, namely a state-of-the art model for clean text: a logistic regression model that uses simple lexical overlap features (Das and Smith, 2009). 4.1 Paraphrase Detection The evaluation is done on expert annotations, which are only available for the test set. The binary and multi-class classifiers are evaluated separately. Additionally, we also tried to improve the system by using normalization. The precision and recall of both classifiers is plotted in Figure 1. In this graph the differences are barely visible, therefore it looks like both models are approximately equal. If we look at the F-scores of Figure 2, the differences are bigger. The highest F-scores of both classifiers are 0.604 and 0.610 for respectively the binary and the"
S15-2007,R13-1026,0,0.0328925,"luation. Using this data, we end up with two different types of gold data per sentence pair. Firstly, we have the binary gold data that indicates if a sentence pair is a paraphrase. Secondly, we have the raw annotations that can be used as a similarity score. These annotations are normalized by dividing them by their maximum score (5), so we end up with h0.0, 0.2, 0.4, 0.6, 0.8, 1.0i as possible similarity scores. The tweets in the corpus are already tokenized using TweetMotif (O’Connor et al., 2010). Additionally, Part Of Speech (POS) tags are provided by a tagger that is adapted to twitter (Derczynski et al., 2013). Named entity tags are also obtained from an adapted tagger (Ritter et al., 2011). 3 Method The model is based on a state-of-the-art semantic similarity prediction model (Bjerva et al., 2014). It is mainly based on overlap features extracted from different parsers, but also includes synset overlap, and a Compositional Distributional Semantic Model (CDSM). The parsers used in this model are a constituency parser (Steedman, 2001), logical parser Paradox (Claessen and S¨orensson, 2003) and the DRS parser Boxer (Bos, 2008). 3.1 Features Our model uses 25 features in total. Due to space constraint"
S15-2007,N12-1019,0,0.17262,"is then trained on these features. In addition to the detection of paraphrases, a similarity score is also predicted, using the probabilities of the classifier. To improve the results, normalization is used as preprocessing step. 2 Our final system achieves a F1 score of 0.620 (10th out of 18 teams), and a Pearson correlation of 0.515 (6th out of 13 teams). 1 Introduction A good paraphrase detection system can be useful in many natural language processing tasks, like searching, translating or summarization. For clean texts, F1 scores as high as 0.84 have been reported on paraphrase detection (Madnani et al., 2012). However, previous research focused almost solely on clean text. Thanks to the Twitter Paraphrase Corpus (Xu et al., 2014), this has now changed. Carrying out this task on noisy texts is a new challenge. The abundant availability of social media data Data The Twitter Paraphrase Corpus consists of two distinct parts, the training data differs significantly from the test data. The 17,790 tweet pairs for training are collected between April 24th and May 3rd, 2014. These tweets are selected based on the trending topics of that period. Annotation of the training data is done by human annotators fr"
S15-2007,S14-2001,0,0.056164,"Missing"
S15-2007,D11-1141,0,0.0273333,"pair. Firstly, we have the binary gold data that indicates if a sentence pair is a paraphrase. Secondly, we have the raw annotations that can be used as a similarity score. These annotations are normalized by dividing them by their maximum score (5), so we end up with h0.0, 0.2, 0.4, 0.6, 0.8, 1.0i as possible similarity scores. The tweets in the corpus are already tokenized using TweetMotif (O’Connor et al., 2010). Additionally, Part Of Speech (POS) tags are provided by a tagger that is adapted to twitter (Derczynski et al., 2013). Named entity tags are also obtained from an adapted tagger (Ritter et al., 2011). 3 Method The model is based on a state-of-the-art semantic similarity prediction model (Bjerva et al., 2014). It is mainly based on overlap features extracted from different parsers, but also includes synset overlap, and a Compositional Distributional Semantic Model (CDSM). The parsers used in this model are a constituency parser (Steedman, 2001), logical parser Paradox (Claessen and S¨orensson, 2003) and the DRS parser Boxer (Bos, 2008). 3.1 Features Our model uses 25 features in total. Due to space constraints we cannot describe them all in detail here. Instead we group the features as fol"
S15-2007,S15-2001,0,0.090933,"Missing"
S15-2007,D11-1061,0,0.24707,"Missing"
S15-2007,Q14-1034,0,\N,Missing
van-noord-etal-2006-syntactic,W00-1505,0,\N,Missing
van-noord-etal-2006-syntactic,bouma-kloosterman-2002-querying,0,\N,Missing
van-noord-etal-2006-syntactic,W97-1502,0,\N,Missing
van-noord-etal-2006-syntactic,C00-2144,0,\N,Missing
van-noord-etal-2006-syntactic,2006.jeptalnrecital-invite.2,1,\N,Missing
van-noord-etal-2006-syntactic,vandeghinste-etal-2006-metis,1,\N,Missing
van-noord-etal-2006-syntactic,I05-7011,0,\N,Missing
van-noord-etal-2006-syntactic,W03-2414,1,\N,Missing
W00-1804,J94-3001,0,\N,Missing
W00-1804,E99-1017,1,\N,Missing
W00-1804,P95-1003,0,\N,Missing
W00-1804,C94-2163,0,\N,Missing
W07-2201,P06-1105,0,0.0366909,"labeled material to extend training data is normally not succesfull, there have been successful variants of self-learning for 2 parsing as well. For instance, in McClosky et al. (2006) self-learning is used to improve a two-phase parser reranker, with very good results for the classical Wall Street Journal parsing task. Clearly, the idea that selection restrictions ought to be useful for parsing accuracy is not new. However, as far as we know this is the first time that automatically acquired selection restrictions have been shown to improve parsing accuracy results. Related research includes Abekawa and Okumura (2006) and Kawahara and Kurohashi (2006) where statistical information between verbs and case elements is collected on the basis of large automatically analysed corpora. 2 Background: Alpino parser The experiments are performed using the Alpino parser for Dutch. In this section we briefly describe the parser, as well as the corpora that we have used in the experiments described later. 2.1 Grammar and Lexicon The Alpino system is a linguistically motivated, wide-coverage grammar and parser for Dutch in the tradition of HPSG. It consists of over 600 grammar rules and a large lexicon of over 100,000 le"
W07-2201,bouma-kloosterman-2002-querying,0,0.0119968,"ed Corpora Over the course of about a year, Alpino has been used to parse most of the TwNC-02 (Twente Newspaper Corpus), Dutch Wikipedia, and the Duch part of Europarl. TwNC consists of Dutch newspaper texts from 1994 - 2004. We did not use the material from Trouw 2001, since part of that material is used in the test set used below. We used the 200 node Beowulf Linux cluster of the HighPerformance Computing center of the University of Groningen. The dependency structures are stored in XML. The XML files can be processed and searched in various ways, for instance, using XPATH, XSLT and Xquery (Bouma and Kloosterman, 2002). Some quantitative information of this parsed corpus is listed in table 1. In the experiments described below, we do not distinguish between full and fragment parses; sentences without a parse are obviously ignored. 3 3.1 Bilexical preferences Association Score The parsed corpora described in the previous section have been used in order to compute association scores between lexical dependencies. The parses constructed by Alpino are dependency structures. In such dependency structures, the basic dependencies are of the form r(w1 , w2 ) where r is a relation such as subject, object, modifier, p"
W07-2201,van-noord-etal-2006-syntactic,1,0.852762,"Missing"
W07-2201,J90-1003,0,0.510404,"nk you You must have drunk Campari b. De wijn die Elvis zou hebben gedronken als hij wijn zou hebben gedronken The wine Elvis would have drunk if he had drunk wine The wine that would have drunk Elvis if he had drunk wine c. De paus heeft tweehonderd daklozen te eten gehad The pope had twohunderd homeless people for dinner In this paper, we describe an alternative approach in which we employ pointwise mutual information association score in the maximum entropy disambiguation model. Pointwise mutual information (Fano, 1961) was used to measure strength of selection restrictions for instance by Church and Hanks (1990). The association scores used here are estimated using a very large parsed corpus of 500 million words (27 million sentences). We show that the incorporation of this additional knowledge source improves parsing accuracy. Because the association scores are estimated on the basis of a large corpus that is parsed by the parser that we aim to improve upon, this technique can be described as a somewhat particular instance of self-training. Self-training has been investigated for statistical parsing before. Although naively adding self-labeled material to extend training data is normally not succesf"
W07-2201,2006.jeptalnrecital-invite.2,1,0.401327,"Missing"
W07-2201,A00-2021,0,0.432695,"cy between wijn and drink, as can be seen in the dependency structure 6 1 su name Elvis2 mod adv niet3 hd verb drink4 Figure 3: Dependency structure produced for relative clause given in figure 3. Sets of dependencies are extended in such cases, to make the relation between the noun and the role it plays in the relative clause explicit. 3.3 Using association scores as features The association scores for all dependencies are used in our maximum entropy disambiguation model as follows. The technique is reminiscent of the inclusion of auxiliary distributions in stochastic attributevalue grammar (Johnson and Riezler, 2000). Recall that a maximum entropy disambiguation model exploits features. Features are properties of parses, and we can use such features to describe any property of parses that we believe is of importance for disambiguation. For the disambiguation model, a parse is fully characterized by a vector of feature counts. We introduce features z(t, r) for each of the major POS labels t (verb, noun, adjective, adverb, . . . ) and each of the dependency relations r. The ‘count’ of such a feature is determined by the association scores for actually occuring dependency pairs. For example, if in a given pa"
W07-2201,N06-1023,0,0.0276528,"ning data is normally not succesfull, there have been successful variants of self-learning for 2 parsing as well. For instance, in McClosky et al. (2006) self-learning is used to improve a two-phase parser reranker, with very good results for the classical Wall Street Journal parsing task. Clearly, the idea that selection restrictions ought to be useful for parsing accuracy is not new. However, as far as we know this is the first time that automatically acquired selection restrictions have been shown to improve parsing accuracy results. Related research includes Abekawa and Okumura (2006) and Kawahara and Kurohashi (2006) where statistical information between verbs and case elements is collected on the basis of large automatically analysed corpora. 2 Background: Alpino parser The experiments are performed using the Alpino parser for Dutch. In this section we briefly describe the parser, as well as the corpora that we have used in the experiments described later. 2.1 Grammar and Lexicon The Alpino system is a linguistically motivated, wide-coverage grammar and parser for Dutch in the tradition of HPSG. It consists of over 600 grammar rules and a large lexicon of over 100,000 lexemes and various rules to recogni"
W07-2201,N06-1020,0,0.0498294,"ords (27 million sentences). We show that the incorporation of this additional knowledge source improves parsing accuracy. Because the association scores are estimated on the basis of a large corpus that is parsed by the parser that we aim to improve upon, this technique can be described as a somewhat particular instance of self-training. Self-training has been investigated for statistical parsing before. Although naively adding self-labeled material to extend training data is normally not succesfull, there have been successful variants of self-learning for 2 parsing as well. For instance, in McClosky et al. (2006) self-learning is used to improve a two-phase parser reranker, with very good results for the classical Wall Street Journal parsing task. Clearly, the idea that selection restrictions ought to be useful for parsing accuracy is not new. However, as far as we know this is the first time that automatically acquired selection restrictions have been shown to improve parsing accuracy results. Related research includes Abekawa and Okumura (2006) and Kawahara and Kurohashi (2006) where statistical information between verbs and case elements is collected on the basis of large automatically analysed cor"
W07-2201,E95-1016,0,\N,Missing
W07-2201,W89-0240,0,\N,Missing
W07-2201,H89-2012,0,\N,Missing
W07-2201,C08-1054,0,\N,Missing
W07-2201,P90-1004,0,\N,Missing
W07-2201,J01-3003,0,\N,Missing
W07-2201,van-der-wouden-etal-2002-syntactic,0,\N,Missing
W07-2201,W02-2018,0,\N,Missing
W07-2201,W07-1503,0,\N,Missing
W07-2205,W02-1503,1,0.818483,"Baldwin University of Melbourne tim@csse.unimelb.edu.au Julia Hockenmaier University of Pennsylvania juliahr@cis.upenn.edu Mark Dras Macquarie University madras@ics.mq.edu.au Tracy Holloway King PARC thking@parc.com Abstract Gertjan van Noord University of Groningen vannoord@let.rug.nl dependencies or the underlying predicate-argument structure directly. Aspects of this research have often had their own separate fora, such as the ACL 2005 workshop on deep lexical acquisition (Baldwin et al., 2005), as well as the TAG+ (Kallmeyer and Becker, 2006), Alpino (van der Beek et al., 2005), ParGram (Butt et al., 2002) and DELPH-IN (Oepen et al., 2002) projects and meetings. However, the fundamental approaches to building a linguistically-founded system and many of the techniques used to engineer efficient systems are common across these projects and independent of the specific grammar formalism chosen. As such, we felt the need for a common meeting in which experiences could be shared among a wider community, similar to the role played by recent meetings on grammar engineering (Wintner, 2006; Bender and King, 2007). As the organizers of the ACL 2007 Deep Linguistic Processing workshop (Baldwin et al., 2007"
W08-1302,J97-4005,0,0.271387,"e for the input the model gets. However, as soon as the model is applied to another domain, or text genre (Lease et al., 2006), accuracy degrades considerably. For example, the performance of a parser trained on the Wall Street Journal (newspaper text) significantly drops when evaluated on the more varied Brown (fiction/nonfiction) corpus (Gildea, 2001). A simple solution to improve performance on a new domain is to construct a parser specifically 2 Background: MaxEnt Models Maximum Entropy (MaxEnt) models are widely used in Natural Language Processing (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997). In this framework, a disambiguation model is specic 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 9 Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 9–16 Manchester, August 2008 fied by a set of feature functions describing properties of the data, together with their associated weights. The weights are learned during the training procedure so that their estimated value determines the contribution of each feature"
W08-1302,W98-1512,0,0.0139634,"= Ep˜[fj ], where j ∈ 1, ..., m. In MaxEnt estimation, the default model q0 is often only implicit (Velldal and Oepen, 2005) and not stated in the model equation, since the model is assumed to be uniform (e.g. the constant func1 tion Ω(s) for sentence s, where Ω(s) is the set of parse trees associated with s). Thus, we seek the model with minimum KL divergence from the uniform distribution, which means we search model p with maximum entropy (uncertainty) subject to given constraints (Abney, 1997). In alternative, if q0 is not uniform then p is called a minimum divergence model (according to (Berger and Printz, 1998)). In the statistical parsing literature, the default model q0 that can be used to incorporate prior knowledge is also referred to as base model (Berger and Printz, 1998), default or reference distribution (Hara et al., 2005; Johnson et al., 1999; Velldal and Oepen, 2005). The solution to the estimation problem of finding distribution p, that satisfies the expectedvalue constraints and minimally diverges from q0 , has been shown to take a specific parametric form (Berger and Printz, 1998): pθ (ω, s) = 1 q0 exp Zθ Pm j=1 θj fj (ω) Since the sum in equation 2 ranges over all possible parse trees"
W08-1302,J96-1002,0,0.00899713,"ebank it was trained on is representative for the input the model gets. However, as soon as the model is applied to another domain, or text genre (Lease et al., 2006), accuracy degrades considerably. For example, the performance of a parser trained on the Wall Street Journal (newspaper text) significantly drops when evaluated on the more varied Brown (fiction/nonfiction) corpus (Gildea, 2001). A simple solution to improve performance on a new domain is to construct a parser specifically 2 Background: MaxEnt Models Maximum Entropy (MaxEnt) models are widely used in Natural Language Processing (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997). In this framework, a disambiguation model is specic 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 9 Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 9–16 Manchester, August 2008 fied by a set of feature functions describing properties of the data, together with their associated weights. The weights are learned during the training procedure so that their estimated value determines"
W08-1302,W07-2201,1,0.827647,"Missing"
W08-1302,A00-2021,0,0.583501,"ndlabeling a considerable amount of training data which is clearly very expensive and leads to an unsatisfactory solution. In alternative, techniques for domain adaptation, also known as parser adaptation (McClosky et al., 2006) or genre portability (Lease et al., 2006), try to leverage either a small amount of already existing annotated data (Hara et al., 2005) or unlabeled data (McClosky et al., 2006) of one domain to parse data from a different domain. In this study we examine an approach that assumes a limited amount of already annotated in-domain data. We explore auxiliary distributions (Johnson and Riezler, 2000) for domain adaptation, originally suggested for the incorporation of lexical selectional preferences into a parsing system. We gauge the effect of exploiting a more general, out-ofdomain model for parser adaptation to overcome the limited amount of in-domain training data. The approach is examined on two application domains, question answering and spoken data. For the empirical trials, we use Alpino (van Noord and Malouf, 2005; van Noord, 2006), a robust computational analyzer for Dutch. Alpino employs a discriminative approach to parse selection that bases its decision on a Maximum Entropy ("
W08-1302,2005.mtsummit-papers.15,0,0.0609118,"ecreasing weight. Once a model is trained, it can be applied to parse selection that chooses the parse with the highest sum of feature weights. During the training procedure, the weights vector is estimated to best fit the training data. In more detail, given m features with their corresponding empirical expectation Ep˜[fj ] and a default model q0 , we seek a model p that has minimum Kullback-Leibler (KL) divergence from the default model q0 , subject to the expected-value constraints: Ep [fj ] = Ep˜[fj ], where j ∈ 1, ..., m. In MaxEnt estimation, the default model q0 is often only implicit (Velldal and Oepen, 2005) and not stated in the model equation, since the model is assumed to be uniform (e.g. the constant func1 tion Ω(s) for sentence s, where Ω(s) is the set of parse trees associated with s). Thus, we seek the model with minimum KL divergence from the uniform distribution, which means we search model p with maximum entropy (uncertainty) subject to given constraints (Abney, 1997). In alternative, if q0 is not uniform then p is called a minimum divergence model (according to (Berger and Printz, 1998)). In the statistical parsing literature, the default model q0 that can be used to incorporate prior"
W08-1302,P99-1069,0,0.0800964,"ce s, where Ω(s) is the set of parse trees associated with s). Thus, we seek the model with minimum KL divergence from the uniform distribution, which means we search model p with maximum entropy (uncertainty) subject to given constraints (Abney, 1997). In alternative, if q0 is not uniform then p is called a minimum divergence model (according to (Berger and Printz, 1998)). In the statistical parsing literature, the default model q0 that can be used to incorporate prior knowledge is also referred to as base model (Berger and Printz, 1998), default or reference distribution (Hara et al., 2005; Johnson et al., 1999; Velldal and Oepen, 2005). The solution to the estimation problem of finding distribution p, that satisfies the expectedvalue constraints and minimally diverges from q0 , has been shown to take a specific parametric form (Berger and Printz, 1998): pθ (ω, s) = 1 q0 exp Zθ Pm j=1 θj fj (ω) Since the sum in equation 2 ranges over all possible parse trees ω ′ ∈ Ω admitted by the grammar, calculating the normalization constant renders the estimation process expensive or even intractable (Johnson et al., 1999). To tackle this problem, Johnson et al. (1999) redefine the estimation procedure by consi"
W08-1302,N06-1020,0,0.0625602,"uation Model Gertjan van Noord University of Groningen The Netherlands G.J.M.van.Noord@rug.nl Barbara Plank University of Groningen The Netherlands B.Plank@rug.nl Abstract for that domain. However, this amounts to handlabeling a considerable amount of training data which is clearly very expensive and leads to an unsatisfactory solution. In alternative, techniques for domain adaptation, also known as parser adaptation (McClosky et al., 2006) or genre portability (Lease et al., 2006), try to leverage either a small amount of already existing annotated data (Hara et al., 2005) or unlabeled data (McClosky et al., 2006) of one domain to parse data from a different domain. In this study we examine an approach that assumes a limited amount of already annotated in-domain data. We explore auxiliary distributions (Johnson and Riezler, 2000) for domain adaptation, originally suggested for the incorporation of lexical selectional preferences into a parsing system. We gauge the effect of exploiting a more general, out-ofdomain model for parser adaptation to overcome the limited amount of in-domain training data. The approach is examined on two application domains, question answering and spoken data. For the empirica"
W08-1302,oostdijk-2000-spoken,0,0.439741,"imation step and simply assign the two parameters equal values (equal weights), the method reduces to POU T (ω|s) × PIN (ω|s), i.e. just multiplying the respective model probabilities. 4 Experiments and Results 4.1 Experimental design The general model is trained on the Alpino Treebank (van Noord, 2006) (newspaper text; approximately 7,000 sentences). For the domain-specific corpora, in the first set of experiments (section 4.3) we consider the Alpino CLEF Treebank (questions; approximately 1,800 sentences). In the second part (section 4.4) we evaluate the approach on the Spoken Dutch corpus (Oostdijk, 2000) (CGN, ’Corpus Gesproken Nederlands’; spoken data; size varies, ranging from 17 to 1,193 sentences). The CGN corpus contains a variety of components/subdomains to account for the various dimensions of language use (Oostdijk, 2000). 4.2 Evaluation metric The output of the parser is evaluated by comparing the generated dependency structure for a corpus sentence to the gold standard dependency structure in a treebank. For this comparison, we represent the dependency structure (a directed acyclic graph) as a set of named dependency relations. To compare such sets of dependency relations, we count"
W08-1302,C00-1085,0,0.0234198,"ting the normalization constant renders the estimation process expensive or even intractable (Johnson et al., 1999). To tackle this problem, Johnson et al. (1999) redefine the estimation procedure by considering the conditional rather than the joint probability. Pm 1 q0 exp j=1 θj fj (ω) (3) Zθ with Zθ as in equation 2, but instead, summing over ω ′ ∈ Ω(s), where Ω(s) is the set of parse trees associated with sentence s. Thus, the probability of a parse tree is estimated by summing only over the possible parses of a specific sentence. Still, calculating Ω(s) is computationally very expensive (Osborne, 2000), because the number of parses is in the worst case exponential with respect to sentence length. Therefore, Osborne (2000) proposes a solution based on informative samples. He shows that is suffices to train on an informative subset of available training data to accurately estimate the model parameters. Alpino implements the Osborne-style approach to Maximum Entropy parsing. The standard version of the Alpino parser is trained on the Alpino newspaper Treebank (van Noord, 2006). Pθ (ω|s) = 3 Exploring auxiliary distributions for domain adaptation 3.1 Auxiliary distributions Auxiliary distributi"
W08-1302,2006.jeptalnrecital-invite.2,1,0.926399,"Missing"
W08-1302,I05-1018,0,\N,Missing
W08-1302,W01-0521,0,\N,Missing
W09-0107,bouma-kloosterman-2002-querying,1,0.808842,"structures are generated by the lexicon and grammar rules as the value of a dedicated feature. The dependency structures are based on CGN (Corpus Gesproken Nederlands, Corpus of Spoken Dutch) (Hoekstra et al., 2003), D-Coi and LASSY (van Noord et al., 2006). Dependency structures are stored in XML. Advantages of the use of XML include the availability of general purpose search and visualization software. For instance, we exploit XPATH (standard XML query language) to search in large sets of dependency structures, and Xquery to extract information from such large sets of dependency structures (Bouma and Kloosterman, 2002; Bouma and Kloosterman, 2007). 2 Figure 2: Dependency structure for Lager was de koers dan gisteren An anonymous reviewer criticized the analysis, because the extraposition principle would also allow the rightward extraction of comparative phrases licensed by comparatives in topic position. The extraposition principle would have to allow for this in the light of examples such as Extraposition of comparative objects out of topic The first illustration of our thesis that parsed corpora provide an interesting new resource for linguists, constitutes more of an anecdote than a systematic study. We"
W09-0107,van-noord-etal-2006-syntactic,1,0.894225,"Missing"
W09-0107,W07-1503,1,0.855808,"he lexicon and grammar rules as the value of a dedicated feature. The dependency structures are based on CGN (Corpus Gesproken Nederlands, Corpus of Spoken Dutch) (Hoekstra et al., 2003), D-Coi and LASSY (van Noord et al., 2006). Dependency structures are stored in XML. Advantages of the use of XML include the availability of general purpose search and visualization software. For instance, we exploit XPATH (standard XML query language) to search in large sets of dependency structures, and Xquery to extract information from such large sets of dependency structures (Bouma and Kloosterman, 2002; Bouma and Kloosterman, 2007). 2 Figure 2: Dependency structure for Lager was de koers dan gisteren An anonymous reviewer criticized the analysis, because the extraposition principle would also allow the rightward extraction of comparative phrases licensed by comparatives in topic position. The extraposition principle would have to allow for this in the light of examples such as Extraposition of comparative objects out of topic The first illustration of our thesis that parsed corpora provide an interesting new resource for linguists, constitutes more of an anecdote than a systematic study. We include the example, presente"
W09-0107,2006.jeptalnrecital-invite.2,1,0.916168,"Missing"
W09-0107,J07-4004,0,0.0260151,"mous improvements have been achieved in this area. Parsers based on constraint-based formalisms such as HPSG, LFG, and CCG are now fast enough for many applications; they are robust; and they perform much more accurately than previously by incorporating, typically, a statistical disambiguation component. As a consequence, such parsers now obtain competitive, if not superior, performance. Zaenen (2004), for instance, points out that the (LFGbased) XLE parser is fast, has a statistical disambiguation component, and is robust, and thus allows full parsing to be incorporated in many applications. Clark and Curran (2007) show that both accurate and highly efficient parsing is possible using a CCG. As a consequence of this development, massive amounts of parsed sentences now become available. Such large collections of syntactically annotated but not manually verified syntactic analyses are a very useful resource for many purposes. In this position paper we focus on one purpose: linguistic analysis. Our claim is, that very large parsed corpora are an important resource for linguists. Such very large parsed corpora can be used to search systematically for specific infrequent syntactic configurations of interest,"
W09-0107,van-der-wouden-etal-2002-syntactic,0,\N,Missing
W09-2609,copestake-flickinger-2000-open,0,0.0546448,"thods. Both methods follow the same basic principle: first, a large (unannotated) corpus is parsed. After parsing, the sentences can be split up in a list of parsable and a list of unparsable sentences. Words or n-grams that occur in the list of unparsable sentences, but that do not occur in the list of parsable sentences have a high suspicion of being the cause of the parsing error. Introduction In the past decade wide-coverage grammars and parsers have been developed for various languages, such as the Alpino parser and grammar (Bouma et al., 2001) for Dutch and the English Resource Grammar (Copestake and Flickinger, 2000). Such grammars account for a large number of grammatical and lexical phenomena, and achieve high accuracies. Still, they are usually tailored to general domain texts and fail to reach the same accuracy for domain-specific texts, due to missing lexicon entries, fixed expressions, and grammatical constructs. When parsing new texts there are usually two types of parsing errors: 2.1 Suspicion as a ratio Van Noord (2004) defines the suspicion of a word as a ratio: C(w|error) S(w) = (1) C(w) • The parser returns an incorrect parse. While the parser may have constructed the correct where C(w) is the"
W09-2609,P06-1042,0,0.563777,"Missing"
W09-2609,P04-1057,1,\N,Missing
W10-2105,oostdijk-2000-spoken,0,0.048333,"pus) as target data. All datasets are described next. CoNLL2006 This is the testfile for Dutch that was used in the CoNLL 2006 shared task on multilingual dependency parsing. The file consists of 386 sentences from an institutional brochure (about youth healthcare). We use this file to check our data-driven models against state-of-the-art. Source: Cdb The cdb (Alpino Treebank) consists of 140,000 words (7,136 sentences) from the Eindhoven corpus (newspaper text). It is a collection of text fragments from 6 Dutch newspapers. The collection has been annotated according to the guidelines of CGN (Oostdijk, 2000) and stored in XML format. It is the standard treebank used to train the disambiguation component of the Alpino parser. Note that cdb is a subset of the training corpus used in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). The CoNLL training data additionally contained a mix of nonnewspaper text,1 which we exclude here on purpose to keep a clean baseline. Alpino to CoNLL format In order to train the MST and Malt parser and evaluate it on the various Wikipedia and DPC articles, we needed to convert the Alpino Treebank format into the tabular CoNLL format. To this end, we adapted the tr"
W10-2105,W06-1615,0,0.0535997,"esources in the new domain) is a much more realistic situation but is clearly also considerably more difficult. Current studies on semisupervised approaches show very mixed results. Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. “no team was able to improve target domain performance substantially over a state-of-the-art baseline”. On the other hand, there have been positive results as well. For instance, McClosky et al. (2006) improved a statistical parser by self-training. Structural Correspondence Learning (Blitzer et al., 2006) was effective for PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), while only modest gains were obtained for structured output tasks like parsing. In the past decade several parsing systems for natural language have emerged, which use different methods and formalisms. For instance, systems that employ a handcrafted grammar and a statistical disambiguation component versus purely statistical data-driven systems. What they have in common is the lack of portability to new domains: their performance might decrease substantially as the distance between test and trai"
W10-2105,W08-1302,1,0.863604,"Missing"
W10-2105,P07-1056,0,0.0308274,"re difficult. Current studies on semisupervised approaches show very mixed results. Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. “no team was able to improve target domain performance substantially over a state-of-the-art baseline”. On the other hand, there have been positive results as well. For instance, McClosky et al. (2006) improved a statistical parser by self-training. Structural Correspondence Learning (Blitzer et al., 2006) was effective for PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), while only modest gains were obtained for structured output tasks like parsing. In the past decade several parsing systems for natural language have emerged, which use different methods and formalisms. For instance, systems that employ a handcrafted grammar and a statistical disambiguation component versus purely statistical data-driven systems. What they have in common is the lack of portability to new domains: their performance might decrease substantially as the distance between test and training domain increases. Yet, to which degree do they suffer from this problem, i.e. which kind of p"
W10-2105,W06-2920,0,0.400972,"ure with a separate second stage classifier to label the dependency edges. (3) MALT Parser (Nivre et al., 2007) is a datadriven transition-based dependency parser. Malt parser uses SVMs to learn a classifier that predicts the next parsing action. Instances represent parser configurations and the label to predict determines the next parser action. Both data-driven parsers (MST and Malt) are thus not specific for the Dutch Language, however, they can be trained on a variety of languages given that the training corpus complies with the columnbased format introduced in the 2006 CoNLL shared task (Buchholz and Marsi, 2006). Additionally, both parsers implement projective and non-projective parsing algorithms, where the latter will be used in our experiments on the relatively free word order language Dutch. Despite that, we train the data-driven parsers using their default settings (e.g. first order features for MST, SVM with polynomial kernel for Malt). 4 Wikipedia LOC (location) KUN (arts) POL (politics) SPO (sports) HIS (history) BUS (business) NOB (nobility) COM (comics) MUS (music) HOL (holidays) Total Example articles Belgium, Antwerp (city) Tervuren school Belgium elections 2003 Kim Clijsters History of B"
W10-2105,P07-1079,0,0.0138845,"is given here in terms of f-score of named dependencies. sents parses oracle arbitrary model 536 45011 95.74 76.56 89.39 (2) MST Parser (McDonald et al., 2005) is a Most previous work has focused on a single parsing system in isolation (Gildea, 2001; Hara et al., 2005; McClosky et al., 2006). However, there is an observable trend towards combining different parsing systems to exploit complementary strengths. For instance, Nivre and McDonald (2008) combine two data-driven systems to improve dependency accuracy. Similarly, two studies successfully combined grammar-based and datadriven systems: Sagae et al. (2007) incorporate data-driven dependencies as soft-constraint in a HPSG-based system for parsing the Wallstreet Journal. In the same spirit (but the other direction), Zhang and Wang (2009) use a deepgrammar based backbone to improve data-driven parsing accuracy. They incorporate features from the grammar-based backbone into the data-driven system to achieve better generalization across domains. This is the work most closest to ours. However, which kind of system (hand-crafted versus purely statistical) is more affected by the domain, and thus more sensitive to domain shifts? To the best of our know"
W10-2105,P07-1033,0,0.0468113,"Missing"
W10-2105,W09-2609,1,0.87949,"Missing"
W10-2105,P04-1057,1,0.85536,"Missing"
W10-2105,2006.jeptalnrecital-invite.2,1,0.910589,"Missing"
W10-2105,W01-0521,0,0.694075,"system is more affected by domain shifts? Intuitively, grammar-driven systems should be less affected by domain changes. To investigate this hypothesis, an empirical investigation on Dutch is carried out. The performance variation of a grammar-driven versus two data-driven systems across domains is evaluated, and a simple measure to quantify domain sensitivity proposed. This will give an estimate of which parsing system is more affected by domain shifts, and thus more in need for adaptation techniques. 1 For parsing, most previous work on domain adaptation has focused on data-driven systems (Gildea, 2001; McClosky et al., 2006; Dredze et al., 2007), i.e. systems employing (constituent or dependency based) treebank grammars. Only few studies examined the adaptation of grammar-based systems (Hara et al., 2005; Plank and van Noord, 2008), i.e. systems employing a hand-crafted grammar with a statistical disambiguation component. This may be motivated by the fact that potential gains for this task are inherently bound by the grammar. Yet, domain adaptation poses a challenge for both kinds of parsing systems. But to what extent do these different kinds of systems suffer from the problem? We test th"
W10-2105,W07-2201,1,0.900086,"Missing"
W10-2105,I05-1018,0,0.484071,"ut. The performance variation of a grammar-driven versus two data-driven systems across domains is evaluated, and a simple measure to quantify domain sensitivity proposed. This will give an estimate of which parsing system is more affected by domain shifts, and thus more in need for adaptation techniques. 1 For parsing, most previous work on domain adaptation has focused on data-driven systems (Gildea, 2001; McClosky et al., 2006; Dredze et al., 2007), i.e. systems employing (constituent or dependency based) treebank grammars. Only few studies examined the adaptation of grammar-based systems (Hara et al., 2005; Plank and van Noord, 2008), i.e. systems employing a hand-crafted grammar with a statistical disambiguation component. This may be motivated by the fact that potential gains for this task are inherently bound by the grammar. Yet, domain adaptation poses a challenge for both kinds of parsing systems. But to what extent do these different kinds of systems suffer from the problem? We test the hypothesis that grammar-driven systems are less affected by domain changes. We empirically investigate this in a case-study on Dutch. Introduction Most modern Natural Language Processing (NLP) systems are"
W10-2105,N06-1020,0,0.548125,"isingly difficult to beat” (Daum´e III, 2007). In contrast, semi-supervised adaptation (i.e. no annotated resources in the new domain) is a much more realistic situation but is clearly also considerably more difficult. Current studies on semisupervised approaches show very mixed results. Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. “no team was able to improve target domain performance substantially over a state-of-the-art baseline”. On the other hand, there have been positive results as well. For instance, McClosky et al. (2006) improved a statistical parser by self-training. Structural Correspondence Learning (Blitzer et al., 2006) was effective for PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), while only modest gains were obtained for structured output tasks like parsing. In the past decade several parsing systems for natural language have emerged, which use different methods and formalisms. For instance, systems that employ a handcrafted grammar and a statistical disambiguation component versus purely statistical data-driven systems. What they have in common is the lack of portab"
W10-2105,H05-1066,0,0.137483,"Missing"
W10-2105,C00-2137,0,0.0752196,"r instance, because the syntactic annotation of Alpino allows words to be dependent on more than a single head (’secondary edges’) (van Noord, 2006). However, such edges are ignored in the CoNLL format; just a single head per token is allowed. Furthermore, there is another simplification. As the Dutch tagger used in the CoNLL 2006 shared task did not have the concept of multiwords, the organizers chose to treat them as a single token (Buchholz and Marsi, 2006). We here follow the CoNLL 2006 task setup. To determine whether results are significant, we us the Approximate Randomization Test (see Yeh (2000)) with 1000 random shuffles. 5 µtarget p = LASpi , sdtarget = p N s i= 1 PN i=1 (LASpi − µtarget )2 p N −1 However, standard deviation is highly influenced by outliers. Furthermore, this measure does not take the source domain performance (baseline) into consideration nor the size of the target domain itself. We thus propose to measure the domain sensitivity of a system, i.e. its average domain variation (adv), as weighted average difference from the baseline (source) mean, where weights represents the size of the various domains: PN adv = i i i=1 w ∗ ∆p , PN i i=1 w with size(wi ) ∆ip = LASpi"
W10-2105,P09-1043,0,0.546416,"ork has focused on a single parsing system in isolation (Gildea, 2001; Hara et al., 2005; McClosky et al., 2006). However, there is an observable trend towards combining different parsing systems to exploit complementary strengths. For instance, Nivre and McDonald (2008) combine two data-driven systems to improve dependency accuracy. Similarly, two studies successfully combined grammar-based and datadriven systems: Sagae et al. (2007) incorporate data-driven dependencies as soft-constraint in a HPSG-based system for parsing the Wallstreet Journal. In the same spirit (but the other direction), Zhang and Wang (2009) use a deepgrammar based backbone to improve data-driven parsing accuracy. They incorporate features from the grammar-based backbone into the data-driven system to achieve better generalization across domains. This is the work most closest to ours. However, which kind of system (hand-crafted versus purely statistical) is more affected by the domain, and thus more sensitive to domain shifts? To the best of our knowledge, no study has yet addressed this issue. We thus assess the performance variation of three dependency parsing systems for Dutch across domains, and propose a simple measure to qu"
W10-2105,P08-1108,0,0.0257329,"ount of parses can be constructed for some sentences. Furthermore, the maximum entropy disambiguation component does a good job in selecting good parses from those. Accuracy is given here in terms of f-score of named dependencies. sents parses oracle arbitrary model 536 45011 95.74 76.56 89.39 (2) MST Parser (McDonald et al., 2005) is a Most previous work has focused on a single parsing system in isolation (Gildea, 2001; Hara et al., 2005; McClosky et al., 2006). However, there is an observable trend towards combining different parsing systems to exploit complementary strengths. For instance, Nivre and McDonald (2008) combine two data-driven systems to improve dependency accuracy. Similarly, two studies successfully combined grammar-based and datadriven systems: Sagae et al. (2007) incorporate data-driven dependencies as soft-constraint in a HPSG-based system for parsing the Wallstreet Journal. In the same spirit (but the other direction), Zhang and Wang (2009) use a deepgrammar based backbone to improve data-driven parsing accuracy. They incorporate features from the grammar-based backbone into the data-driven system to achieve better generalization across domains. This is the work most closest to ours. H"
W10-2105,E09-1093,1,\N,Missing
W15-2502,W13-3307,1,0.83777,"eir intrinsic evaluation results. In addition, a more detailed manual analysis of Englishto-Czech translation was carried out. 1 Introduction Over the last years, the interest in addressing coreference-related issues in Machine Translation (MT) has increased. Multiple works focused on using information coming from a Coreference Resolution (CR) system to improve pronoun translation in phrase-based frameworks (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012). A similar task was addressed in the TectoMT deep syntax tree-to-tree translation system (Žabokrtský et al., 2008). Novák et al. (2013a; 2013b) presented specialized models for the personal pronoun it and reflexive pronouns in English-Czech translation, which resulted in an improvement in terms of human evaluation. Although these models were tailored to pronoun translation, they only addressed cases 2 Pronouns in the target languages The system of anaphoric pronouns is similar for Czech and Dutch, both containing personal, possessive, reflexive, relative, and demonstrative pronouns.1 In the present work, we mainly concentrate on a subset of anaphoric pronouns whose form cannot be reliably determined without knowing the close"
W15-2502,I13-1142,1,0.853875,"eir intrinsic evaluation results. In addition, a more detailed manual analysis of Englishto-Czech translation was carried out. 1 Introduction Over the last years, the interest in addressing coreference-related issues in Machine Translation (MT) has increased. Multiple works focused on using information coming from a Coreference Resolution (CR) system to improve pronoun translation in phrase-based frameworks (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012). A similar task was addressed in the TectoMT deep syntax tree-to-tree translation system (Žabokrtský et al., 2008). Novák et al. (2013a; 2013b) presented specialized models for the personal pronoun it and reflexive pronouns in English-Czech translation, which resulted in an improvement in terms of human evaluation. Although these models were tailored to pronoun translation, they only addressed cases 2 Pronouns in the target languages The system of anaphoric pronouns is similar for Czech and Dutch, both containing personal, possessive, reflexive, relative, and demonstrative pronouns.1 In the present work, we mainly concentrate on a subset of anaphoric pronouns whose form cannot be reliably determined without knowing the close"
W15-2502,W12-3102,0,0.0561762,"Missing"
W15-2502,E12-3001,0,0.32765,"ign rules that take advantage of this information. The resolvers’ performance measured by translation quality is contrasted with their intrinsic evaluation results. In addition, a more detailed manual analysis of Englishto-Czech translation was carried out. 1 Introduction Over the last years, the interest in addressing coreference-related issues in Machine Translation (MT) has increased. Multiple works focused on using information coming from a Coreference Resolution (CR) system to improve pronoun translation in phrase-based frameworks (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012). A similar task was addressed in the TectoMT deep syntax tree-to-tree translation system (Žabokrtský et al., 2008). Novák et al. (2013a; 2013b) presented specialized models for the personal pronoun it and reflexive pronouns in English-Czech translation, which resulted in an improvement in terms of human evaluation. Although these models were tailored to pronoun translation, they only addressed cases 2 Pronouns in the target languages The system of anaphoric pronouns is similar for Czech and Dutch, both containing personal, possessive, reflexive, relative, and demonstrative pronouns.1 In the p"
W15-2502,hajic-etal-2012-announcing,0,0.139024,"Missing"
W15-2502,2010.iwslt-papers.10,0,0.165695,"anaphoric information, and design rules that take advantage of this information. The resolvers’ performance measured by translation quality is contrasted with their intrinsic evaluation results. In addition, a more detailed manual analysis of Englishto-Czech translation was carried out. 1 Introduction Over the last years, the interest in addressing coreference-related issues in Machine Translation (MT) has increased. Multiple works focused on using information coming from a Coreference Resolution (CR) system to improve pronoun translation in phrase-based frameworks (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012). A similar task was addressed in the TectoMT deep syntax tree-to-tree translation system (Žabokrtský et al., 2008). Novák et al. (2013a; 2013b) presented specialized models for the personal pronoun it and reflexive pronouns in English-Czech translation, which resulted in an improvement in terms of human evaluation. Although these models were tailored to pronoun translation, they only addressed cases 2 Pronouns in the target languages The system of anaphoric pronouns is similar for Czech and Dutch, both containing personal, possessive, reflexive, relative, and demonstrative pro"
W15-2502,W12-4501,0,0.0246673,"r and number3 with their antecedent. However, their usage is limited by the We apply the Treex CR system, the BART system and the Stanford Deterministic CR system in our experiments4 . As neither BART nor the Stanford 2 In that case, some of the pronoun’s properties can be reconstructed from the verb thanks to subject-verb agreement. 3 Possessor’s gender and number in case of possessive relative pronouns. 4 The reasons for choosing the latter two systems are twofold: they are freely available and they perform close to the state of the art, as confirmed by the results of CoNLL2012 Shared Task (Pradhan et al., 2012). 18 4 system target relative pronouns, we combine these two systems with a Treex module for relative pronouns (the Treex-relat module). 3.1 TectoMT (Žabokrtský et al., 2008) is a tree-totree machine translation system whose translation process follows the analysis-transfer-synthesis pipeline. In the analysis stage, the source sentence is transformed into a deep syntax dependency representation based on the Prague tectogrammatics theory (Sgall et al., 1986). At this point, CR systems are applied to interlink the tree representation with coreference relations. The source language tree structure"
W15-2502,2005.mtsummit-papers.11,0,0.0050895,"ives can be inferred solely using the source pronoun. Therefore, only personal and relative pronouns are targeted with the following coreference-based rules: • Impose agreement in gender (het- or de- type) for personal pronouns translated from the English pronoun it. • For relative pronouns, a corresponding form is picked based on whether the pronoun is bound in a prepositional phrase, refers to a verb phrase, a person, or a het- or de- noun. 5 Automatic evaluation The TectoMT translation models were trained on parallel data from CzEng 1.0 (Bojar et al., 2012) and a concatenation of Europarl (Koehn, 2005), Dutch parallel corpus (Macken et al., 2007) and KDE4 localizations (Tiedemann, 2009), for Czech and Dutch, respectively. We tested the English-Czech and English-Dutch translation systems on datasets from two different domains: the news domain, represented by English-Czech test set for the WMT 2012 Translation Task (Callison-Burch et al., 2012) as well as the last 36 documents from English-Dutch News Commentary data set (Tiedemann, 2012),7 and the IT domain, represented by the corresponding pairs of the QTLeap Corpus Batch 2 (Osenova et al., 2015).8 The evaluation was conducted for several co"
W15-2502,W10-1737,0,0.128642,"Missing"
W15-2502,J13-4004,0,0.0630933,"ieke Oele,‡ Gertjan van Noord,‡ ∗ Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics mnovak@ufal.mff.cuni.cz ‡ University of Groningen, The Netherlands {d.oele,g.j.m.van.noord}@rug.nl Abstract where anaphora information is in fact not needed. For proper translation of other pronouns, however, coreference must be involved. The present work concentrates on exploiting coreference for deep syntax MT. We integrate three coreference resolvers for English into TectoMT system, namely the Treex CR (Popel and Žabokrtský, 2010), the Stanford CR (Lee et al., 2013), and BART (Versley et al., 2008), and observe their effects on translation quality. Taking linguistic observations on the target languages into account, we design rules that make use of the information supplied by the CR systems. We apply this approach to English-Czech and English-Dutch translation. This paper is structured as follows. In Section 2, we introduce the grammar of Czech and Dutch pronouns with a special emphasis on cases where form depends on anaphoric relations. Section 3 gives a brief description of the used CR systems. In Section 4, the TectoMT system is presented, along with"
W15-2502,J01-4004,0,0.0429715,"euristics. 3.2 BART BART 2.0 (Versley et al., 2008; Uryupina et al., 2012) is a modular toolkit for end-to-end coreference resolution. It is based on mention-pair model, which means that a classifier makes a decision for every pair of mentions whether they belong to the same coreference cluster or not. Subsequently, the mentions paired by pairwise decisions need to be partitioned into coreference chains. The model is trained using the WEKA machine-learning toolkit (Witten and Frank, 2005). Features for English are identical to those used in virtually all state-of-theart coreference resolvers (Soon et al., 2001). 3.3 The TectoMT System and Coreference 4.1 Rules Using Coreference During the transfer and the synthesis stage, language-dependent rules that make use of the projected coreference relations are applied. The rules are based on linguistic observations presented in Section 2. Even if a given grammatical property is ruled by the antecedent, it is not always necessary to use anaphora information. The correct form in the target language can be inferred from the source language word itself. For example, genders in English and Czech are of a different nature. While the gender of English pronouns is"
W15-2502,2007.mtsummit-papers.42,0,0.0148409,"source pronoun. Therefore, only personal and relative pronouns are targeted with the following coreference-based rules: • Impose agreement in gender (het- or de- type) for personal pronouns translated from the English pronoun it. • For relative pronouns, a corresponding form is picked based on whether the pronoun is bound in a prepositional phrase, refers to a verb phrase, a person, or a het- or de- noun. 5 Automatic evaluation The TectoMT translation models were trained on parallel data from CzEng 1.0 (Bojar et al., 2012) and a concatenation of Europarl (Koehn, 2005), Dutch parallel corpus (Macken et al., 2007) and KDE4 localizations (Tiedemann, 2009), for Czech and Dutch, respectively. We tested the English-Czech and English-Dutch translation systems on datasets from two different domains: the news domain, represented by English-Czech test set for the WMT 2012 Translation Task (Callison-Burch et al., 2012) as well as the last 36 documents from English-Dutch News Commentary data set (Tiedemann, 2012),7 and the IT domain, represented by the corresponding pairs of the QTLeap Corpus Batch 2 (Osenova et al., 2015).8 The evaluation was conducted for several configurations of TectoMT. The Baseline systems"
W15-2502,tiedemann-2012-parallel,0,0.0112446,"e- noun. 5 Automatic evaluation The TectoMT translation models were trained on parallel data from CzEng 1.0 (Bojar et al., 2012) and a concatenation of Europarl (Koehn, 2005), Dutch parallel corpus (Macken et al., 2007) and KDE4 localizations (Tiedemann, 2009), for Czech and Dutch, respectively. We tested the English-Czech and English-Dutch translation systems on datasets from two different domains: the news domain, represented by English-Czech test set for the WMT 2012 Translation Task (Callison-Burch et al., 2012) as well as the last 36 documents from English-Dutch News Commentary data set (Tiedemann, 2012),7 and the IT domain, represented by the corresponding pairs of the QTLeap Corpus Batch 2 (Osenova et al., 2015).8 The evaluation was conducted for several configurations of TectoMT. The Baseline systems did not use any coreference-related rules while the remaining configurations apply all TectoMT coref6 Manual analysis of the results BLEU score has previously been shown not to be suitable for measuring small modifications such as changes in pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012; Hardmeier, 2014). Despite these findings, we succeeded in getting a bett"
W15-2502,W12-4515,0,0.0143241,"Resolution System This system is a part of the Treex framework (Popel and Žabokrtský, 2010) and has been used for English-to-Czech translation in the TectoMT system (Žabokrtský et al., 2008). It consists of several modules; each of them focuses on a specific type of coreferential relations in English:5 anaphora of relative pronouns (the Treex-relat module) and personal, possessive, and reflexive pronouns (the Treex-other module). All the modules are rule-based, making use of syntactic representation of the sentence as well as simple context heuristics. 3.2 BART BART 2.0 (Versley et al., 2008; Uryupina et al., 2012) is a modular toolkit for end-to-end coreference resolution. It is based on mention-pair model, which means that a classifier makes a decision for every pair of mentions whether they belong to the same coreference cluster or not. Subsequently, the mentions paired by pairwise decisions need to be partitioned into coreference chains. The model is trained using the WEKA machine-learning toolkit (Witten and Frank, 2005). Features for English are identical to those used in virtually all state-of-theart coreference resolvers (Soon et al., 2001). 3.3 The TectoMT System and Coreference 4.1 Rules Using"
W15-2502,P08-4003,0,0.124061,",‡ ∗ Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics mnovak@ufal.mff.cuni.cz ‡ University of Groningen, The Netherlands {d.oele,g.j.m.van.noord}@rug.nl Abstract where anaphora information is in fact not needed. For proper translation of other pronouns, however, coreference must be involved. The present work concentrates on exploiting coreference for deep syntax MT. We integrate three coreference resolvers for English into TectoMT system, namely the Treex CR (Popel and Žabokrtský, 2010), the Stanford CR (Lee et al., 2013), and BART (Versley et al., 2008), and observe their effects on translation quality. Taking linguistic observations on the target languages into account, we design rules that make use of the information supplied by the CR systems. We apply this approach to English-Czech and English-Dutch translation. This paper is structured as follows. In Section 2, we introduce the grammar of Czech and Dutch pronouns with a special emphasis on cases where form depends on anaphoric relations. Section 3 gives a brief description of the used CR systems. In Section 4, the TectoMT system is presented, along with our rules exploiting coreference"
W15-2502,W08-0325,0,0.0845955,"Missing"
W15-5709,J02-2001,0,0.0725221,"ations of deleting, modifying or adding lexical units in order to form more natural sentences with a correct meaning. The problem of lexical choice in MT has not yet been investigated thoroughly, probably due to the fact that its output is hard to evaluate. For example, when a different lemma is returned than the one from the gold standard it might still be appropriate to the context but marked as an error by the evaluation method. Stede (1993) was the first to recognize the need to involve semantic context. A number of algorithms and models have been developed for lexical choice, for example Edmonds and Hirst (2002) developed a model for choosing between words with similar core meanings but with different connotations. WordNet has not often been used as a dictionary for lexical choice in generation, even though work exists on the usefulness of such a resource for NLG-related tasks such as domain adaptation and paraphrasing (Jing, 1998). For instance, Basile (2014) proposed an unsupervised algorithm for lexical choice from WordNet synsets called Ksel that exploits the WordNet hierarchy of hypernyms/hyponyms to produce the most appropriate lemma for a given synset. Also, the use of Hidden Markov Tree model"
W15-5709,W98-0718,0,0.16051,"rd it might still be appropriate to the context but marked as an error by the evaluation method. Stede (1993) was the first to recognize the need to involve semantic context. A number of algorithms and models have been developed for lexical choice, for example Edmonds and Hirst (2002) developed a model for choosing between words with similar core meanings but with different connotations. WordNet has not often been used as a dictionary for lexical choice in generation, even though work exists on the usefulness of such a resource for NLG-related tasks such as domain adaptation and paraphrasing (Jing, 1998). For instance, Basile (2014) proposed an unsupervised algorithm for lexical choice from WordNet synsets called Ksel that exploits the WordNet hierarchy of hypernyms/hyponyms to produce the most appropriate lemma for a given synset. Also, the use of Hidden Markov Tree models for lexical choice in Wordnet synsets is novel. Crouse et al. (1996) introduced the adaptation of Hidden Markov Chains to tree models for 74 signal processing. The corresponding adaptation of the classic Viterbi algorithm, used to restore the hidden state tree, was introduced by Durand et al. (2004). Previous applications"
W15-5709,W13-2263,0,0.020895,"use of Hidden Markov Tree models for lexical choice in Wordnet synsets is novel. Crouse et al. (1996) introduced the adaptation of Hidden Markov Chains to tree models for 74 signal processing. The corresponding adaptation of the classic Viterbi algorithm, used to restore the hidden state tree, was introduced by Durand et al. (2004). Previous applications of the tree model are: image segmentation, signal classification, denoising and image document categorization (Durand et al., 2004). The use of these models in natural language processing is fairly new and has been applied to word alignment (Kondo et al., 2013) and Machine Translation (Žabokrtský and Popel, 2009). The latter was the first to apply HMTMs to lexical choice using a variant of the Viterbi algorithm in the transfer phase of a deep-syntax based machine translation system. To tackle the problem of lexical choice we propose the mapping of a dependency structure over synsets to a dependency structure over lemmas while taking into account both context information as the frequency of the lemma and synset combination. A dependency tree is a labeled tree in which nodes correspond to the words of a sentence. It contains edges that represent the g"
W15-5709,P04-1036,0,0.0433682,"rond” in any context) and “voorstelling”, has a very frequent transition probability, causing an inaccurate substitution for both lemmas in this sentence. Domain clearly is a problem when choosing the right sense. The frequency distribution of the senses of lemmas depends on the genre and domain of the text under consideration. A possible solution to finding the right synset, without using context information, is to use Word 78 Sense Disambiguation (WSD) from untagged text. This method aims to obtain, for each target word, the sense which is predominant in the target, domain-specific, corpus. McCarthy et al. (2004), for example, used such a corpus to construct a distributional thesaurus of related words. Subsequently, they disambiguated each target word using pairwise similarity measures based on WordNet, taking as pairs the target word and each of the most related words according to the distributional thesaurus up to a certain threshold. This method would not only allow our system to consider more lemmas for replacement, because more frequency information on synsets would then be available, it would also have a bigger chance of starting from correct input synsets. In future work we therefore intend to"
W15-5709,E93-1055,0,0.443745,"hoice can be formulated as follows: given a semantic or conceptual specification, find its best realization. We can define the process of lexical choice as the operations of deleting, modifying or adding lexical units in order to form more natural sentences with a correct meaning. The problem of lexical choice in MT has not yet been investigated thoroughly, probably due to the fact that its output is hard to evaluate. For example, when a different lemma is returned than the one from the gold standard it might still be appropriate to the context but marked as an error by the evaluation method. Stede (1993) was the first to recognize the need to involve semantic context. A number of algorithms and models have been developed for lexical choice, for example Edmonds and Hirst (2002) developed a model for choosing between words with similar core meanings but with different connotations. WordNet has not often been used as a dictionary for lexical choice in generation, even though work exists on the usefulness of such a resource for NLG-related tasks such as domain adaptation and paraphrasing (Jing, 1998). For instance, Basile (2014) proposed an unsupervised algorithm for lexical choice from WordNet s"
W15-5709,2006.jeptalnrecital-invite.2,1,0.802514,"Missing"
W15-5709,vossen-etal-2012-dutchsemcor,0,0.0432482,"Missing"
W15-5709,P09-2037,0,0.19708,"choice in Wordnet synsets is novel. Crouse et al. (1996) introduced the adaptation of Hidden Markov Chains to tree models for 74 signal processing. The corresponding adaptation of the classic Viterbi algorithm, used to restore the hidden state tree, was introduced by Durand et al. (2004). Previous applications of the tree model are: image segmentation, signal classification, denoising and image document categorization (Durand et al., 2004). The use of these models in natural language processing is fairly new and has been applied to word alignment (Kondo et al., 2013) and Machine Translation (Žabokrtský and Popel, 2009). The latter was the first to apply HMTMs to lexical choice using a variant of the Viterbi algorithm in the transfer phase of a deep-syntax based machine translation system. To tackle the problem of lexical choice we propose the mapping of a dependency structure over synsets to a dependency structure over lemmas while taking into account both context information as the frequency of the lemma and synset combination. A dependency tree is a labeled tree in which nodes correspond to the words of a sentence. It contains edges that represent the grammatical relations between those words, such as nom"
W16-2332,N03-1017,0,0.00891535,"provide several translation options for each node along with their estimated probability. The best options are then selected using a Hidden Markov Tree Model (HMTM) ˇ with a target-language tree model (Zabokrtsk´ y and Popel, 2009). For this specific task, where we need to work on a specific domain, an extended version of TectoMT was used allowing interpolation of multiple TMs (Rosa et al., 2015). Moses All the systems submitted that were based on Moses have been trained on a phrase-based model by Giza++ or mGiza with “grow-diag-finaland” symmetrization and “msd-bidirectional-fe” reordering (Koehn et al., 2003). For the language pairs where big quantities of domain-specific monolingual data were available along with the generic domain data, separate language models (domain-specific and generic) were interpolated against our ICT domain-specific development set. For LM training and interpolation, the SRILM toolkit (Stolcke, 2002) was used. The method of truecasing has been adopted for several language pairs where it proved useful. 3 TectoMT The deep translation is based on the TectoMT system, an open-source MT system based on the Treex platform for general natural-language processing. TectoMT uses a c"
W16-2332,C10-3009,0,0.0136209,"obabilities in both directions, lexical weightings in both directions, a phrase length penalty, a ”phrase-mslr-fe” lexicalized reordering model and a target language model. As for the language model, a 5-gram model was trained. The weights for the different components were adjusted to optimize BLEU using MERT tuning over the Batch1 development set, with an n-best list of size 100. For the TectoMT system, EU-Treex existing tools were used in order to get the a-layer. Eustagger is a robust and wide coverage morphological analyzer and POS tagger. The dependency parser is based on the MATE-tools (Bjrkelund et al., 2010). Basque models have been trained using the Basque Dependency Treebank (BDT) corpus (Aduriz et al., 2003). Transformation from the a-level analysis into t-level is partially performed with language-independent blocks thanks to the support of Interset (Zeman, 2008). The English-to-Basque TectoMT system uses the PaCo2 and the Batch1 corpora to train two separate translation models, and they are used to create an interpolated list of translation candidates. In addition to that, the terminological equivalences extracted from the localization PO files (VLC, LO and KDE) as well as the domain terms e"
W16-2332,W16-2334,1,0.789389,"Missing"
W16-2332,2005.mtsummit-papers.11,0,0.0123228,"PO files (VLC, LO and KDE) as well as the domain terms extracted from Wikipedia are used to identify domain terms before syntactic analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used to identify the elements that should be maintained untranslated on the output. Both systems were trained using the same training corpora: the 7th version of the Europarl corpus was used for both translation and language modDutch The Moses system for Dutch was trained on the third version of the Europarl corpus (Koehn, 2005) and the in-domain KDE4 Localization data (Tiedemann, 2012). Words are aligned with GIZA++ and tuning was done with MERT. The applied heuristics for the Dutch baselines were set to “grow-diag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first"
W16-2332,P14-5010,0,0.00318097,"ed Moses with the following factors: ENWordForm-BGLemma|Lemma|BGPOStag, where ENWordForm-BGLemma is an English word form when there is no appropriate Bulgarian one, or the Bulgarian lemma; BGPOStag is the appropriate Bulgarian tag representing grammatical features like number, tense, etc. adaptation and MERT training. Batch2 domain corpus was used for testing during development. The Moses system, EU-Moses, uses factored models to allow lemma-based word-alignment. After word alignment, the rest of the training process is based on lowercased word-forms and standard parameters: Stanford CoreNLP (Manning et al., 2014) and Eustagger (Alegria et al., 2002) tools are used for tokenization and lemmatization, MGIZA for word alignment with the ”growdiag-final-and” symmetrization heuristic, a maximum length of 75 tokens per sentence and 5 tokens per phrase, translation probabilities in both directions, lexical weightings in both directions, a phrase length penalty, a ”phrase-mslr-fe” lexicalized reordering model and a target language model. As for the language model, a 5-gram model was trained. The weights for the different components were adjusted to optimize BLEU using MERT tuning over the Batch1 development se"
W16-2332,W15-4101,1,0.800257,"was performed by the Moses tokenizer. No lemmatization or compound splitting was used and the casing was obtained with the Moses truecaser. For the training, a phrase-based model was used with a language model order of 5, with Kneser-Ney smoothing, which was interpolated using the SRILM tool. The word alignment was done with Giza++ on full forms and the final tuning was done using MERT. The Europarl corpus was used for the training data, both as monolingual data for training language models and as parallel data for training the phrase-table. Regarding the English-to-Portuguese TectoMT system (Silva et al., 2015)(Rodrigues et al., 2016a), PT-Treex, in order to get the a-layer the Portuguese system resorted to LX-Suite (Branco and Silva, 2006), a set of pre-existing shallow processing tools for Portuguese that include a sentence segmenter, a tokenizer, a POS tagger, a morphological analyser and a dependency parser, all with state-of-the-art performance. Treex blocks were created to be called and interfaced with these tools. After running the shallow processing tools, the dependency output of the parser is converted into Universal Dependencies (UD) (de Marneffe et al., 2014). These dependencies are then"
W16-2332,W10-1730,1,0.894585,"Missing"
W16-2332,W15-5712,1,0.718195,"ansfer, and synthesis 4 Basque Both English-Basque submissions are trained on the same training corpora. That is, the PaCO2eneu corpus for translation and language modeling, and the in-domain Batch1 corpus for domain 436 tors retrieved from POS tagged, lemmatized parallel corpora; and BG-DeepMoses — a system that also is based on standard factored Moses but the translation is done in two steps: (1) semanticsbased translation of the source language text to a mixed source-target language text which is then (2) translated to the target language via Moses. The latter system builds on Simov et al. (2015). As training data for both systems the following corpora were used: the Setimes parallel corpus, the Europarl parallel corpus and a corpus created on the basis of the documentation of LibreOffice. The corpora are linguistically processed with the IXA2 pipeline for the English part and the BTB pipeline for the Bulgarian. The analyses include POS tagging, lemmatization and WSD, using the UKB system,3 which provides graph-based methods for Word Sense Disambiguation and lexical similarity measurements. For the BG-Moses system, the following factors have been constructed: WordForm|Lemma|POStag. Fo"
W16-2332,H05-1066,0,0.184414,"Missing"
W16-2332,P14-5003,0,0.0466518,"Missing"
W16-2332,2006.jeptalnrecital-invite.2,1,0.754357,"Missing"
W16-2332,tiedemann-2012-parallel,0,0.0377489,"extracted from Wikipedia are used to identify domain terms before syntactic analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used to identify the elements that should be maintained untranslated on the output. Both systems were trained using the same training corpora: the 7th version of the Europarl corpus was used for both translation and language modDutch The Moses system for Dutch was trained on the third version of the Europarl corpus (Koehn, 2005) and the in-domain KDE4 Localization data (Tiedemann, 2012). Words are aligned with GIZA++ and tuning was done with MERT. The applied heuristics for the Dutch baselines were set to “grow-diag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first model with over 1.9 million sentences from Europarl (Koehn,"
W16-2332,L16-1094,1,0.833385,"ag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first model with over 1.9 million sentences from Europarl (Koehn, 2005) and the second model composed of the Batch1, the Microsoft Terminology Collection and ˇ the LibreOffice localization data (Stajner et al., 2016). Each pair of parallel sentences, one in English and one in Portuguese, are analyzed by Treex up to the t-layer level, where each pair of trees are fed into the model. The TectoMT synthesis (Rodrigues et al., 2016b) included other two lexical-semanticsrelated modules, the HideIT and gazetteers. The HideIT module handles entities that do not require translation such as URLs and shell commands. The gazetteers are specialized lexicons that handle the translation of named entities from the ITdomain such as menu items and button names. Finally, synset IDs were used as additional contextual feature"
W16-2332,P09-2037,1,0.925288,"uage-specific additions and distinguishes two levels of syntactic description: and Spanish, Charles University in Prague for Czech, by University of Groningen for Dutch, by University of Lisbon for Portuguese and by IICTBAS of the Bulgarian Academy of Sciences for Bulgarian. For each language two different systems were submitted, corresponding to different phases of the project, namely a phrase-based MT system built using Moses (Koehn et al., 2007), and a system exploiting deep language engineering approaches, that in all the languages but Bulgarian was imˇ plemented using TectoMT (Zabokrtsk´ y and Popel, 2009). For Bulgarian, its second MT system is not based on TectoMT, but on exploiting deep factors in Moses. All 12 systems are constrained, that is trained only on the data provided by the WMT16 IT-task organizers. We present briefly the Moses common setting and the TectoMT structure and then more detailed information for each language system are provided. In the last Section, results based on BLEU and TrueSkill are given and discussed. 2 • Surface dependency syntax (a-layer) – surface dependency trees containing all the tokens in the sentence. • Deep syntax (t-layer) – dependency trees that conta"
W16-2332,W08-0325,0,0.300026,"Missing"
W16-2332,L16-1438,1,0.826183,"Missing"
W16-2332,zeman-2008-reusable,0,0.0263149,"djusted to optimize BLEU using MERT tuning over the Batch1 development set, with an n-best list of size 100. For the TectoMT system, EU-Treex existing tools were used in order to get the a-layer. Eustagger is a robust and wide coverage morphological analyzer and POS tagger. The dependency parser is based on the MATE-tools (Bjrkelund et al., 2010). Basque models have been trained using the Basque Dependency Treebank (BDT) corpus (Aduriz et al., 2003). Transformation from the a-level analysis into t-level is partially performed with language-independent blocks thanks to the support of Interset (Zeman, 2008). The English-to-Basque TectoMT system uses the PaCo2 and the Batch1 corpora to train two separate translation models, and they are used to create an interpolated list of translation candidates. In addition to that, the terminological equivalences extracted from the localization PO files (VLC, LO and KDE) as well as the domain terms extracted from Wikipedia are used to identify domain terms before syntactical analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used, to identify the elements that s"
W16-2332,W15-5711,1,0.91741,"Missing"
W16-2332,de-marneffe-etal-2014-universal,0,\N,Missing
W16-2332,E06-2024,1,\N,Missing
W16-2332,P07-2045,0,\N,Missing
W16-2332,W13-2208,0,\N,Missing
W16-2332,bojar-etal-2012-joy,1,\N,Missing
W16-2332,L16-1441,1,\N,Missing
W17-0403,W15-2103,0,0.0275219,"n crucially relies on the fact that we can use the conversion script to convert Alpino output to UD. 2 Conversion Process Conversion of a manually verified treebank to UD is possible if the underlying annotation contains the information that is required to do a mapping from the original annotation to POS-tags and bilexical dependencies that is conformant with the annotation guidelines of the UD project. By doing an automatic conversion, we follow a strategy that has been used to create many of the other UD treebanks as well (Zeman et al., 2014; Johannsen et al., 2015; Øvrelid and Hohle, 2016; Ahrenberg, 2015; Lynn and Foster, 2016). Conversion of Lassy to UD POS-tags can be achieved by means of a simple set of case statements that refer to the original POS-tag and a small set of morphological feature values. The only case that is more involved is the distinction between verbs and auxiliaries. This distinction is missing in the POS-tags and morphological features of the Lassy treebanks, but can be reconstructed using the lemma and valency of the verb (i.e., a limited set of verbs that select for only a subject and a 2 Currently, no secondary edges are used in the Small. 20 UD Lassy Lassy UD Interp"
W17-0403,P16-1231,0,0.040033,"Missing"
W17-0403,W06-2920,0,0.0789443,"nt cases are listed in Table 1. We have used the conversion script to create UD 3 All sentences from the training section containing the adverb ook (also). 21 root predicative phrase as xcomp, and a case of an incomplete word (part of a coordination) marked as X (in accordance with the original annotation but not the best option according to UD), We also tried to compare Lassy Small with the UD Dutch corpus that has been included in UD since v1.2. The latter corpus is a conversion of the Alpino treebank (van der Beek et al., 2002). It was used in the CONLL X shared task on dependency parsing (Buchholz and Marsi, 2006) and converted at that point to CONLL format. The UD version is based on a conversion to HamleDT to UD (Zeman et al., 2014). The various conversion steps have lead to loss of information,4 and apparent mistakes,5 and the quality of this corpus in general seems to be lower than the UD Lassy Small corpus. A more systematic comparison will be possible once we have been able to reconstruct the original sources of the material included in the Alpino treebank fragment used for CONLL. At that point, it will also be possible to create an improved version of the data using the current conversion script"
W17-0403,Q16-1032,0,0.0434811,"Missing"
W17-0403,meyers-etal-2004-annotating,0,0.0709743,"ociated with the verbal domain as well as dependents associated with the nominal domain, as in example (2). Here, a verb clearly heads a nominal phrase, as it is introduced by a determiner. Yet, at the same time, it selects an inherent reflexive pronoun, something that is not possible for nouns. The dependency annotation for this example in Figure 2 also shows that the PP phrase is labeled nmod, giving preference to the nominal interpretation of verzekeren. Note that the the parallelism between (1) and (2) suggests that it could perhaps also have been labeled obl. In fact, the NomBank corpus (Meyers et al., 2004) adopts the rule that the same semantic role labels should be used as much as possible for verbs and nominalised versions of these verbs. Cross-lingual comparison The inventory of dependency labels in UD is a mixed functional-structural system, which distinguishes oblique arguments, for instance, on the basis of their part-of-speech, i.e. a PP dependent is labeled obl, a dependent clause advcl, and an adverbial advmod. Also, attachment to predicates is differentiated from attachment to nominals. The original Lassy Small treebank has both phrasal categories and dependency labels, and seems to m"
W17-0403,L16-1250,0,0.0217454,"y treebank. The comparison crucially relies on the fact that we can use the conversion script to convert Alpino output to UD. 2 Conversion Process Conversion of a manually verified treebank to UD is possible if the underlying annotation contains the information that is required to do a mapping from the original annotation to POS-tags and bilexical dependencies that is conformant with the annotation guidelines of the UD project. By doing an automatic conversion, we follow a strategy that has been used to create many of the other UD treebanks as well (Zeman et al., 2014; Johannsen et al., 2015; Øvrelid and Hohle, 2016; Ahrenberg, 2015; Lynn and Foster, 2016). Conversion of Lassy to UD POS-tags can be achieved by means of a simple set of case statements that refer to the original POS-tag and a small set of morphological feature values. The only case that is more involved is the distinction between verbs and auxiliaries. This distinction is missing in the POS-tags and morphological features of the Lassy treebanks, but can be reconstructed using the lemma and valency of the verb (i.e., a limited set of verbs that select for only a subject and a 2 Currently, no secondary edges are used in the Small. 20 UD Lass"
W17-0403,2006.jeptalnrecital-invite.2,1,0.851114,"Missing"
W17-5043,W17-5007,0,0.156163,"n explored (Tetreault et al., 2013). Ensemble systems, which combine the predictions of several classifiers and output the most likely class label via voting or probability-averaging, have further been shown to provide a boost in accuracy compared to the single-classifier approach. Such systems, however, are not light-weight. In training several classifiers simultaneously, quick training speeds are typically sacrificed in favor of a (usually marginal) performance gain. This paper is thus concerned with exploring each of these classification methods as they pertain to the NLI Shared Task 2017 (Malmasi et al., 2017). In this paper, we explore the performance of a linear SVM trained on languageindependent character features for the NLI Shared Task 2017. Our basic system (G RONINGEN) achieves the best performance (87.56 F1-score) on the evaluation set using only 1-9 character n-grams as features. We compare this against several ensemble and meta-classifiers in order to examine how the linear system fares when combined with other, especially non-linear classifiers. Special emphasis is placed on the topic bias that exists by virtue of the assessment essay prompt distribution. 1 Introduction Native Language I"
W17-5043,C16-1333,1,0.848832,"dropout, 20 epochs, trained with the adam optimization algorithm (Kingma and Ba, 2014) for 20 iterations with a batch size of 50. 3.3.2 Deep Residual Networks Deep residual networks (resnets) are a class of convolutional neural networks (CNNs), which consist of several convolutional blocks with skip connections in between (He et al., 2016). Such skip connections facilitate error propagation to earlier layers in the network, which allows for building deeper networks. Resnets have been shown to be useful for NLP tasks, such as text classification (Conneau et al., 2016), and sequence labelling (Bjerva et al., 2016). We applied resnets with four residual blocks in our en385 semble experiments, each containing two successive one-dimensional convolutions. Each such block is followed by an average pooling layer and dropout (p = 0.5, Srivastava et al. (2014)). The resnets were applied to several input representations: word unigrams, and character 4-6-grams. The outputs of each resnet are concatenated before passing through two fully connected layers. We trained the resnet over 50 epochs with adam, using the model with the lowest validation loss. In addition to dropout, we used weight decay for regularization"
W17-5043,C12-1025,0,0.0743422,"ect their L2 writing. At a large scale, this could be extended to enhance existing teaching pedagogies and tailor them towards students of a particular L1. NLI is another natural fit for forensic linguistics, where it can be used to detect the native language (and potentially the nationality) of an anonymous writer. NLI is typically framed as a multi-class classification problem, wherein a classifier is trained on more than two native languages simultaneously. As with many text-classification tasks, Support Vector Machines (SVM) have consistently produced the best results for the task, e.g., (Brooke and Hirst, 2012). However, other classifiers, such as Random Forests and Logistic Regression, have also been explored (Tetreault et al., 2013). Ensemble systems, which combine the predictions of several classifiers and output the most likely class label via voting or probability-averaging, have further been shown to provide a boost in accuracy compared to the single-classifier approach. Such systems, however, are not light-weight. In training several classifiers simultaneously, quick training speeds are typically sacrificed in favor of a (usually marginal) performance gain. This paper is thus concerned with e"
W17-5043,P16-2067,1,0.895832,"Missing"
W17-5043,W13-1706,0,0.128826,"students of a particular L1. NLI is another natural fit for forensic linguistics, where it can be used to detect the native language (and potentially the nationality) of an anonymous writer. NLI is typically framed as a multi-class classification problem, wherein a classifier is trained on more than two native languages simultaneously. As with many text-classification tasks, Support Vector Machines (SVM) have consistently produced the best results for the task, e.g., (Brooke and Hirst, 2012). However, other classifiers, such as Random Forests and Logistic Regression, have also been explored (Tetreault et al., 2013). Ensemble systems, which combine the predictions of several classifiers and output the most likely class label via voting or probability-averaging, have further been shown to provide a boost in accuracy compared to the single-classifier approach. Such systems, however, are not light-weight. In training several classifiers simultaneously, quick training speeds are typically sacrificed in favor of a (usually marginal) performance gain. This paper is thus concerned with exploring each of these classification methods as they pertain to the NLI Shared Task 2017 (Malmasi et al., 2017). In this pape"
W17-5043,D14-1142,0,0.431151,"Missing"
W17-5043,W13-1714,0,0.414695,"Missing"
W17-6931,J14-1003,0,0.0431286,"Missing"
W17-6931,E09-1005,0,0.120916,"Missing"
W17-6931,C14-1151,0,0.0190957,"Rothe and Sch¨utze, 2015; Jauhar et al., 2015; Taghipour and Ng, 2015). Our system makes use of a combination of sense embeddings, context embeddings, and gloss embeddings. Somewhat similar approaches have been proposed by Chen et al. (2014) and Pelevina et al. (2016). The main difference to our approach is that they automatically induce sense embeddings and find the best sense by comparing them to context embeddings, while we add gloss embeddings for better performance. Inkpen and Hirst (2003) apply gloss- and context vectors to the disambiguation of near-synonyms in dictionary entries. Also Basile et al. (2014) use a distributional approach, however, it requires a sense-tagged corpus while our system does not rely on any tagged data. 3 Method Our WSD algorithm takes sentences as input and outputs a preferred sense for each polysemous word. Given a sentence w1 . . . wi of i words, we retrieve a set of word senses from the sense inventory for each word w. Then, for each sense s of each word w, we consider the similarity of its lexeme (the combination of a word and one of its senses (Rothe and Sch¨utze, 2015) with the context and the similarity of the gloss with the context. For each potential sense s"
W17-6931,D14-1110,0,0.227901,"ch experiments that rely on WordNet-based senses, such as machine translation and information retrieval and extraction systems (see Morato et al. (2004) for examples of such systems). Recently, features based on sense-specific embeddings learned using a combination of large corpora and a sense inventory have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Sch¨utze, 2015; Jauhar et al., 2015; Taghipour and Ng, 2015). Our system makes use of a combination of sense embeddings, context embeddings, and gloss embeddings. Somewhat similar approaches have been proposed by Chen et al. (2014) and Pelevina et al. (2016). The main difference to our approach is that they automatically induce sense embeddings and find the best sense by comparing them to context embeddings, while we add gloss embeddings for better performance. Inkpen and Hirst (2003) apply gloss- and context vectors to the disambiguation of near-synonyms in dictionary entries. Also Basile et al. (2014) use a distributional approach, however, it requires a sense-tagged corpus while our system does not rely on any tagged data. 3 Method Our WSD algorithm takes sentences as input and outputs a preferred sense for each poly"
W17-6931,W00-1322,0,0.279449,"Missing"
W17-6931,P12-1092,0,0.0576893,"mbeddings is that they exhibit certain algebraic relations and can, therefore, be used for meaningful semantic operations such as computing word similarity (Turney, 2006), and capturing lexical relationships (Mikolov et al., 2013). A disadvantage of word embeddings is that they assign a single embedding to each word, thus ignoring the possibility that words may have more than one meaning. This problem can be addressed by associating each word with a series of sense-specific embeddings. For this, several methods have been proposed in recent work. For example, in Reisinger and Mooney (2010) and Huang et al. (2012), a fixed number of senses is learned for each word that has multiple meanings by first clustering the contexts of each token, and subsequently relabeling each word token with the clustered sense before learning embeddings. Although previously mentioned sense embedding methods have demonstrated good performance, they use automatically induced senses. They are, therefore, not readily applicable to NLP applications and research experiments that rely on WordNet-based senses, such as machine translation and information retrieval and extraction systems (see Morato et al. (2004) for examples of such"
W17-6931,N15-1070,0,0.0390146,"Missing"
W17-6931,kilgarriff-rosenzweig-2000-english,0,0.229094,"Missing"
W17-6931,N13-1090,0,0.0440138,"pora and a sense inventory such as WordNet, and therefore does not rely on annotated data. Also, it is readily applicable to other languages if a sense inventory is available. 2 Related work In the past few years, much progress has been made on learning word embeddings from unlabeled data that represent the meanings of words as contextual feature vectors. A major advantage of these word embeddings is that they exhibit certain algebraic relations and can, therefore, be used for meaningful semantic operations such as computing word similarity (Turney, 2006), and capturing lexical relationships (Mikolov et al., 2013). A disadvantage of word embeddings is that they assign a single embedding to each word, thus ignoring the possibility that words may have more than one meaning. This problem can be addressed by associating each word with a series of sense-specific embeddings. For this, several methods have been proposed in recent work. For example, in Reisinger and Mooney (2010) and Huang et al. (2012), a fixed number of senses is learned for each word that has multiple meanings by first clustering the contexts of each token, and subsequently relabeling each word token with the clustered sense before learning"
W17-6931,C12-1109,0,0.0227735,"Missing"
W17-6931,S01-1005,0,0.10972,"Missing"
W17-6931,W16-1620,0,0.0195397,"y on WordNet-based senses, such as machine translation and information retrieval and extraction systems (see Morato et al. (2004) for examples of such systems). Recently, features based on sense-specific embeddings learned using a combination of large corpora and a sense inventory have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Sch¨utze, 2015; Jauhar et al., 2015; Taghipour and Ng, 2015). Our system makes use of a combination of sense embeddings, context embeddings, and gloss embeddings. Somewhat similar approaches have been proposed by Chen et al. (2014) and Pelevina et al. (2016). The main difference to our approach is that they automatically induce sense embeddings and find the best sense by comparing them to context embeddings, while we add gloss embeddings for better performance. Inkpen and Hirst (2003) apply gloss- and context vectors to the disambiguation of near-synonyms in dictionary entries. Also Basile et al. (2014) use a distributional approach, however, it requires a sense-tagged corpus while our system does not rely on any tagged data. 3 Method Our WSD algorithm takes sentences as input and outputs a preferred sense for each polysemous word. Given a senten"
W17-6931,S07-1016,0,0.0190385,"text. Evaluation on both Dutch and English datasets shows that our method outperforms other Lesk methods and improves upon a state-of-theart knowledge-based system. Additional experiments confirm the effect of the use of glosses and indicate that our approach works well in different domains. 1 Introduction The quest of automatically finding the correct meaning of a word in context, also known as Word Sense Disambiguation (WSD), is an important topic in natural language processing. Although the best performing WSD systems are those based on supervised learning methods (Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli and Lapata, 2007; Navigli, 2009; Zhong and Ng, 2010), a large amount of manually annotated data is required for training. Furthermore, even if such a supervised system obtains good results in a certain domain, it is not readily portable to other domains (Escudero et al., 2000). As an alternative to supervised systems, knowledge-based systems do not require manually tagged data and have proven to be applicable to new domains (Agirre et al., 2009). They only require two types of information: a set of dictionary entries with definitions for each possible word meaning, and the context in"
W17-6931,N10-1013,0,0.0418658,"major advantage of these word embeddings is that they exhibit certain algebraic relations and can, therefore, be used for meaningful semantic operations such as computing word similarity (Turney, 2006), and capturing lexical relationships (Mikolov et al., 2013). A disadvantage of word embeddings is that they assign a single embedding to each word, thus ignoring the possibility that words may have more than one meaning. This problem can be addressed by associating each word with a series of sense-specific embeddings. For this, several methods have been proposed in recent work. For example, in Reisinger and Mooney (2010) and Huang et al. (2012), a fixed number of senses is learned for each word that has multiple meanings by first clustering the contexts of each token, and subsequently relabeling each word token with the clustered sense before learning embeddings. Although previously mentioned sense embedding methods have demonstrated good performance, they use automatically induced senses. They are, therefore, not readily applicable to NLP applications and research experiments that rely on WordNet-based senses, such as machine translation and information retrieval and extraction systems (see Morato et al. (20"
W17-6931,P15-1173,0,0.0842018,"Missing"
W17-6931,W04-0811,0,0.0446293,"ss of a sense and the context. Evaluation on both Dutch and English datasets shows that our method outperforms other Lesk methods and improves upon a state-of-theart knowledge-based system. Additional experiments confirm the effect of the use of glosses and indicate that our approach works well in different domains. 1 Introduction The quest of automatically finding the correct meaning of a word in context, also known as Word Sense Disambiguation (WSD), is an important topic in natural language processing. Although the best performing WSD systems are those based on supervised learning methods (Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli and Lapata, 2007; Navigli, 2009; Zhong and Ng, 2010), a large amount of manually annotated data is required for training. Furthermore, even if such a supervised system obtains good results in a certain domain, it is not readily portable to other domains (Escudero et al., 2000). As an alternative to supervised systems, knowledge-based systems do not require manually tagged data and have proven to be applicable to new domains (Agirre et al., 2009). They only require two types of information: a set of dictionary entries with definitions for each possible word meani"
W17-6931,N15-1035,0,0.0207383,"ed sense embedding methods have demonstrated good performance, they use automatically induced senses. They are, therefore, not readily applicable to NLP applications and research experiments that rely on WordNet-based senses, such as machine translation and information retrieval and extraction systems (see Morato et al. (2004) for examples of such systems). Recently, features based on sense-specific embeddings learned using a combination of large corpora and a sense inventory have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Sch¨utze, 2015; Jauhar et al., 2015; Taghipour and Ng, 2015). Our system makes use of a combination of sense embeddings, context embeddings, and gloss embeddings. Somewhat similar approaches have been proposed by Chen et al. (2014) and Pelevina et al. (2016). The main difference to our approach is that they automatically induce sense embeddings and find the best sense by comparing them to context embeddings, while we add gloss embeddings for better performance. Inkpen and Hirst (2003) apply gloss- and context vectors to the disambiguation of near-synonyms in dictionary entries. Also Basile et al. (2014) use a distributional approach, however, it requir"
W17-6931,J06-3003,0,0.0665014,"method is that it only requires large unlabeled corpora and a sense inventory such as WordNet, and therefore does not rely on annotated data. Also, it is readily applicable to other languages if a sense inventory is available. 2 Related work In the past few years, much progress has been made on learning word embeddings from unlabeled data that represent the meanings of words as contextual feature vectors. A major advantage of these word embeddings is that they exhibit certain algebraic relations and can, therefore, be used for meaningful semantic operations such as computing word similarity (Turney, 2006), and capturing lexical relationships (Mikolov et al., 2013). A disadvantage of word embeddings is that they assign a single embedding to each word, thus ignoring the possibility that words may have more than one meaning. This problem can be addressed by associating each word with a series of sense-specific embeddings. For this, several methods have been proposed in recent work. For example, in Reisinger and Mooney (2010) and Huang et al. (2012), a fixed number of senses is learned for each word that has multiple meanings by first clustering the contexts of each token, and subsequently relabel"
W17-6931,vasilescu-etal-2004-evaluating,0,0.150641,"Missing"
W17-6931,vossen-etal-2012-dutchsemcor,0,0.0638124,"Missing"
W17-6931,R13-1092,0,0.0605408,"Missing"
W17-6931,P10-4014,0,0.0296788,"our method outperforms other Lesk methods and improves upon a state-of-theart knowledge-based system. Additional experiments confirm the effect of the use of glosses and indicate that our approach works well in different domains. 1 Introduction The quest of automatically finding the correct meaning of a word in context, also known as Word Sense Disambiguation (WSD), is an important topic in natural language processing. Although the best performing WSD systems are those based on supervised learning methods (Snyder and Palmer, 2004; Pradhan et al., 2007; Navigli and Lapata, 2007; Navigli, 2009; Zhong and Ng, 2010), a large amount of manually annotated data is required for training. Furthermore, even if such a supervised system obtains good results in a certain domain, it is not readily portable to other domains (Escudero et al., 2000). As an alternative to supervised systems, knowledge-based systems do not require manually tagged data and have proven to be applicable to new domains (Agirre et al., 2009). They only require two types of information: a set of dictionary entries with definitions for each possible word meaning, and the context in which the word occurs. An example of such a system is the Les"
W19-4206,W13-3520,0,0.0695643,"Missing"
W19-4206,W16-2002,0,0.0246736,"tional Linguistics architecture for both lemmatization and tagging which are described in Section 3.2 and 3.3. using dataset embeddings. 2 Related work 3 Our system is based on three main approaches which are heavily studied in existing literature. These are sequence-to-sequence learning, multitask learning and multi-lingual learning. Recent work on computational morphology showed that neural sequence-to-sequence (seq2seq) models (Sutskever et al., 2014; Bahdanau et al., 2014) have yielded new stateof-the-art performance on various tasks including morphological reinflection and lemmatization (Cotterell et al., 2016, 2017, 2018). Building on this, Dayanık et al. (2018) utilize different levels of representations such as character-level, word-level and sentence-level in the encoder of their seq2seq architecture based on previous work (Heigold et al., 2017). Multi-task learning approaches for jointly learning related tasks have been successfully employed on syntactic and semantic tasks (Søgaard and Goldberg, 2016; Plank et al., 2016). In the context of morphological analysis, this has been used by Kementchedjhieva et al. (2018), who jointly learn morphosyntactic tags and inflections for a word in a given c"
W19-4206,D18-1335,0,0.0550576,"Missing"
W19-4206,N19-1155,0,0.0141909,"nt state of the decoder ht , the character attention cct and the morphological tags ti,1:γ of the target word. The probability of the output lemma characters are then predicted through a softmax layer. (8) e ht = decoder(ht , cct , ti,1:γ ) p(li,t |e ht ) = softmax(e ht ) (9) 3.3 (15) In order to exploit morphological features during lemmatization, we give the morphological tags ti:γ which are predicted by the tag decoder, as part of input to the lemma decoder. Independent of their order, the entire set of the tags are encoded by a simple feed-forward layer as described in the baseline model (Malaviya et al., 2019) and the resulting vector is concatenated with the input embeddings for each target word. The last part of the lemma decoder is the attention network which is the same character-level attention model as in the tag decoder. The character attention mechanism allows the lemma decoder to compute an attention vector cct based on the output states of the word encoder. The attention vector is then passed into a concatenation layer to generate the output state e ht of the decoder for each lemma character li,t . (11) (12) j e ht = tanh(Wcc [cct ; ht ]) (14) (10) Together with the context attention, we"
W19-4206,W18-6011,0,0.0251833,"ve epochs (patience). 4.1 Dataset Embeddings Experiments In this section, we will describe the data used in our experiments as well as evaluate the effectiveness of our external embeddings setup and the dataset embeddings with in a variety of settings. In all experiments we use +E and -E to indicate the model with and without external embeddings, and +D and -D for dataset embeddings. 5.1 Data The test data of SIGMORPHON 2019 task 2 consists of a collection of datasets released in the Universal Dependencies project (Nivre et al., 2018), which are automatically converted to the UniMorph Schema (McCarthy et al., 2018). In total, we evaluate our model on 107 datasets, covering 66 languages. After empirically looking at the trade-off between data-size and training time, we decided to limit each dataset to its first 250,000 tokens for all experiments. This speeded up the training considerably, with almost no loss in performance. For the tuning of our model, we selected a sub-set of datasets from the main benchmark. More specifically, we aimed to get a diversion of language-family, size, and morphological richness (here proxied by the average amount of morphological tags per word). To ensure we do not overfit"
W19-4206,W14-1701,0,0.0217034,"then separately generate lemmas and morphological tags using these representations by using multiple attention mechanisms. Our contributions are threefold: Introduction This paper presents our model for the SIGMORPHON 2019 Task 2 on morphological analysis and lemmatization in context (McCarthy et al., 2019). The task is to generate a lemma and a sequence of morphological tags, which are called morphosyntactic descriptions (MSD), for each word in a given sentence. This task is important because it can be used to improve several downstream NLP applications such as grammatical error correction (Ng et al., 2014), machine translation (Conforti et al., 2018) and multilingual parsing (Zeman et al., 2018). Table 1 shows the lemma and morphological tags of: Johnny likes cats. The first sub-task, Lemmatization, is to transform an inflected word form to its lemma which is its base-form (or dictionary form), as in the example of likes to like. The second sub-task, morphological tagging, is to predict morphological properties of words as a sequence of tags, including a part of speech tag. These morphological tags specify the inflections encoded in word-forms. In the • We introduce the use of multiple attentio"
W19-4206,P16-2067,0,0.0231974,"models (Sutskever et al., 2014; Bahdanau et al., 2014) have yielded new stateof-the-art performance on various tasks including morphological reinflection and lemmatization (Cotterell et al., 2016, 2017, 2018). Building on this, Dayanık et al. (2018) utilize different levels of representations such as character-level, word-level and sentence-level in the encoder of their seq2seq architecture based on previous work (Heigold et al., 2017). Multi-task learning approaches for jointly learning related tasks have been successfully employed on syntactic and semantic tasks (Søgaard and Goldberg, 2016; Plank et al., 2016). In the context of morphological analysis, this has been used by Kementchedjhieva et al. (2018), who jointly learn morphosyntactic tags and inflections for a word in a given context, and use a shared encoder within a multi-task architecture consisting of multiple decoder similar to our model. Multi-lingual learning approaches which benefit from joint learning for multiple languages is also studied on various tasks with different architectures. Ammar et al. (2016) uses a language embedding that contains information considering the language, word-order properties and typological properties for"
W19-4206,K18-2011,0,0.0503291,"Missing"
W19-4206,P16-2038,0,0.0278568,"uence-to-sequence (seq2seq) models (Sutskever et al., 2014; Bahdanau et al., 2014) have yielded new stateof-the-art performance on various tasks including morphological reinflection and lemmatization (Cotterell et al., 2016, 2017, 2018). Building on this, Dayanık et al. (2018) utilize different levels of representations such as character-level, word-level and sentence-level in the encoder of their seq2seq architecture based on previous work (Heigold et al., 2017). Multi-task learning approaches for jointly learning related tasks have been successfully employed on syntactic and semantic tasks (Søgaard and Goldberg, 2016; Plank et al., 2016). In the context of morphological analysis, this has been used by Kementchedjhieva et al. (2018), who jointly learn morphosyntactic tags and inflections for a word in a given context, and use a shared encoder within a multi-task architecture consisting of multiple decoder similar to our model. Multi-lingual learning approaches which benefit from joint learning for multiple languages is also studied on various tasks with different architectures. Ammar et al. (2016) uses a language embedding that contains information considering the language, word-order properties and typolo"
W19-4206,D18-1473,0,0.0538249,"Missing"
W91-0103,W89-0206,0,0.030614,"uch an organization of a grammar is the starting point of a class of generation algorithms that have become popular recently (Calder et al., 1989; Shieber et al., 1989; Shieber el al., 1990). These semantic-head-driven algorithms are both geared towards the input semantic representation and the information contained in lexical entries. If the above sketched approach to semantic interpretation is followed systematically, it is possible to show that such a semantic-head-driven gen12 eration algorithm terminates (Dymetman et al., 1990). In van Noord (1991) I define a head-driven parser (based on Kay (1989)) for a class of constraint-based grammars in which the construction of strings may use more complex operations that simple context-free concatenation. Again, this algorithm is geared towards the input (string) and the information found in lexical entries. In this paper I investigate an approach where the construction of strings is defined lexically. Grammar rules simply percolate strings upwards. Such an approach seems feasible if we allow for powerful constraints to be defined. The head-corner parser knows about strings and performs operations on them; in the types of grammars defined here t"
W91-0103,W91-0108,0,0.041628,"ation L of a lexical entry, if E is an element of L's subcat list (i.e. (L synsem sc r* f ) ~ E), then: size[(E phon)] < size[(L phon)] size[(E synsem sere)] < size[(L synsem sere)] The most straightforward way to satisfy this condition is for an element of a subcat list to share its semantics with a proper part of the semantics of the lexical entry, and to include the elements of the subcat list in its word-order domain. Possible i n p u t s . In order to prove termination of the algorithm we need to make some assumptions about possible inputs. For a discussion cf. van Noord (1990b) and also Thompson (1991, this volume). The input to parsing and generation is specified as the goal 4 Some examples Verb raising. First I show how Reape's analysis of Dutch and German verb raising constructions can be incorporated in the current grammar (Reape, 1989; Reape, 1990a). For a linguistic discussion of verb-raising constructions the reader is referred to Reape's papers. A verb raiser such as the German verb 'versprechen' (promise) selects three arguments, a vp, an object np and a subject np. The word-order domain of the vp is unioned into the word order domain of versprechen. This is necessary because in G"
W91-0103,E89-1014,0,0.146835,"bcat list of the ultimate head of the tree. Furthermore, the string and the semantic representation of each of the non heads in the derivation tree is determined by the subcat list as well. A specific condition on the relation between elements in the subcat list and their seL mantics and string representation ensures termination. This condition on lexical entries can be seen as a lexicalized !and computationally motivated version of GB's projection principle. W o r d - o r d e r d o m a i n s . The string associated with a linguistic object (sign) is defined in terms of its word-order domain (Reape, 1989; Reape, 1990a). I take a word=order domain as a sequence of signs. Each of the§e signs is associated with a word-order domain recursively, or with a sequence of words. A word-order domain is thus a tree. Linear precedence rules are defined that constrain possible orderings o f signs in such a word-order domain. Surface strings are a direct function of word-order domains.' In the lexicon, the wordorder domain of a lexical entry is defined by sharing parts of this domain with the arguments it subcategorizes for. Word-order domains are percolated upward. Hence word-order domains are constructed"
W91-0103,C88-2128,0,0.0757446,"Missing"
W97-0614,P96-1009,0,0.0261893,"d by Philips Dialogue Systems in Aachen (Aust et al., 1995), adapted to Dutch. This German system processes spoken input using ""concept spotting"", which means that the smallest information-carrying units in the input are extracted, such as names of train stations and expressions of time, and these are translated more or less individually into updates of the internal database representing the dialogue state. The words between the concepts thus perceived are ignored. The use of concept spotting is common in spokenlanguage information systems (Ward, 1989; Jackson et al., 1991; Aust et al., 1995; Allen et al., 1996). Arguments in favour of this kind of shallow parsing is that it is relatively easy to develop the NLP component, since larger sentence constructs do not have 66 to be taken into account, and that the robustness of the parser is enhanced, since sources of ungrammaticality occurring between concepts are skipped and therefore do not hinder the translation of the utterance to updates. The prototype presently under construction departs from the use of concept spotting. The grammar for OVIS describes grarnrnat&apos;icaluser utterances, i.e. whole sentences are described. Yet, as part of this it also des"
W97-0614,H91-1034,0,0.0202964,"is a version of a German system developed by Philips Dialogue Systems in Aachen (Aust et al., 1995), adapted to Dutch. This German system processes spoken input using ""concept spotting"", which means that the smallest information-carrying units in the input are extracted, such as names of train stations and expressions of time, and these are translated more or less individually into updates of the internal database representing the dialogue state. The words between the concepts thus perceived are ignored. The use of concept spotting is common in spokenlanguage information systems (Ward, 1989; Jackson et al., 1991; Aust et al., 1995; Allen et al., 1996). Arguments in favour of this kind of shallow parsing is that it is relatively easy to develop the NLP component, since larger sentence constructs do not have 66 to be taken into account, and that the robustness of the parser is enhanced, since sources of ungrammaticality occurring between concepts are skipped and therefore do not hinder the translation of the utterance to updates. The prototype presently under construction departs from the use of concept spotting. The grammar for OVIS describes grarnrnat&apos;icaluser utterances, i.e. whole sentences are des"
W97-0614,H89-1043,0,0.0331708,"easible in terms of accuracy and computational resources, and thus is a viable alternative to pure concept spotting. Although the added benefit of grammatical analysis over concept spotting is not clear for our relatively simple application, the grammatical approach may become essential as soon as the application is extended in such a way that mor~ complicated grammatical constructions need to be recognized. In that case, simple concept spotting may not be able to correctly process all constructions, whereas the capabilities of the grammatical approach extend much further. Whereas some (e.g. (Moore et al., 1989)) argue that grammatical analysis may improve recognition accuracy, our current experiments have as yet not been able to reveal a clear advantage in this respect. As the basis for our implementation we have chosen definite-clause grammars (DCGs) (Pereira and Warren, 1980), a flexible formalism which is related to various kinds of common linguistics description, and which allows application of various parsing algorithms. DCGs can be translated directly into Prolog, for which interpreters and compilers exist that are fast enough to handle real-time processing of spoken input. The grammar for OVI"
W97-0614,J97-3004,1,0.875354,"Missing"
W97-0614,H89-1018,0,0.0360997,"ation, which is a version of a German system developed by Philips Dialogue Systems in Aachen (Aust et al., 1995), adapted to Dutch. This German system processes spoken input using ""concept spotting"", which means that the smallest information-carrying units in the input are extracted, such as names of train stations and expressions of time, and these are translated more or less individually into updates of the internal database representing the dialogue state. The words between the concepts thus perceived are ignored. The use of concept spotting is common in spokenlanguage information systems (Ward, 1989; Jackson et al., 1991; Aust et al., 1995; Allen et al., 1996). Arguments in favour of this kind of shallow parsing is that it is relatively easy to develop the NLP component, since larger sentence constructs do not have 66 to be taken into account, and that the robustness of the parser is enhanced, since sources of ungrammaticality occurring between concepts are skipped and therefore do not hinder the translation of the utterance to updates. The prototype presently under construction departs from the use of concept spotting. The grammar for OVIS describes grarnrnat&apos;icaluser utterances, i.e. w"
W97-1513,E93-1010,1,0.858596,"Missing"
W97-1513,P94-1021,1,0.886061,"Missing"
W97-1513,C94-1039,1,0.88654,"Missing"
W97-1513,C96-2197,0,0.067302,"Missing"
W98-1306,P97-1058,0,0.181817,"arge number of emoves. The paper identifies three subset construction algorithms which treat e-moves.A number of experiments has been performed which indicate that the algorithms diff~ considerably in practice. Furthermore, the experiments suggest that the average number of emoves per state can be used to predict which algorithm is likely to perform best for a given input automatorL 1 Introduction In experimenting with finite-state approximation techniques for context-free and more powerful grammatical formalisms (such as the techniques presented in Pereira and Wright (1997), Nederhof (1997), Evans (1997)) we have found that the resulting automata often are extremely large. Moreover, the automata contain many e-moves (jumps). And finally, if such automata are determinised then the resulting automata are often smaller. It turns out that a straightforward implementation of the subset construction determinisation algorithm performs badly for such inputs. As a motivating example, consider the definite-clause grammar that has been developed for the OVIS2 Spoken Dialogue System. This grammar is described in detail in van Noord et al. (1997). After removing the feature constraints of this grammar, an"
W98-1306,J97-2003,0,0.0289619,"ton, including minimisation and determinisation. FinaUy, we support user-defined regular expression operators. - We also provide operators for transductions such as composition, cross-product, samelength-cross-product, domain, range, identity and in~cersion. - Determinisation and Minimisation. Three different minimisation algorithms are supported: Hopcroft&apos;s algorithm (Hopcroft, 1971), Hopcroft and Ullmart&apos;s algorithm (Hopcroft and Ullman, 1979), and Brzozowski&apos;s algorithm (Brzozowski, 1962). - Determinisation and minimisation of string-to-string and string-to-weight transducers (Mohri, 1996; Mohri, 1997). - Visuuli~tion. Support includes built-in visualisation (TCl/Tk, TeX+PicTeX, TeX+PsTricks, Postscript) and interfaces to third party graph visualisation software (Graphviz (dot), VCG, daWmci). Random generation of finite automata (an extension of the algorithm in Leslie (1995) to allow the generation of finite automata containing e-moves). m m m m m nm u m - m m m [] m 3 Subset Construction 3.1 Problem statement Let a finite-state machine M be specified by a tuple (Q, 22, 6, S, F) where Q is a finite set of states, is a finite alphabet, 6 is a function from Q x (27 u {e}) --* 2Q. Furthermore"
W98-1306,1997.iwpt-1.19,0,0.441973,"ata with a very large number of emoves. The paper identifies three subset construction algorithms which treat e-moves.A number of experiments has been performed which indicate that the algorithms diff~ considerably in practice. Furthermore, the experiments suggest that the average number of emoves per state can be used to predict which algorithm is likely to perform best for a given input automatorL 1 Introduction In experimenting with finite-state approximation techniques for context-free and more powerful grammatical formalisms (such as the techniques presented in Pereira and Wright (1997), Nederhof (1997), Evans (1997)) we have found that the resulting automata often are extremely large. Moreover, the automata contain many e-moves (jumps). And finally, if such automata are determinised then the resulting automata are often smaller. It turns out that a straightforward implementation of the subset construction determinisation algorithm performs badly for such inputs. As a motivating example, consider the definite-clause grammar that has been developed for the OVIS2 Spoken Dialogue System. This grammar is described in detail in van Noord et al. (1997). After removing the feature constraints of th"
zhao-van-noord-2010-pos,W01-1815,1,\N,Missing
zhao-van-noord-2010-pos,J00-4006,0,\N,Missing
zhao-van-noord-2010-pos,W02-2018,0,\N,Missing
zhao-van-noord-2010-pos,P06-1088,0,\N,Missing
zhao-van-noord-2010-pos,A00-1031,0,\N,Missing
