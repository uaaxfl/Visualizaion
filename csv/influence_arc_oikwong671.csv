2001.mtsummit-road.9,O98-3002,0,0.0350694,", MT evaluation, Chinese-English MT, Translation systems, Region-sensitive Romanisation Introduction There are many different aspects to consider in the evaluation of machine translation (MT) systems, including intelligibility, accuracy, error analysis and so on (e.g. Arnold et al., 1994; EAGLES, 1999). However, few seem to directly address the identification and translation of personal names within texts, especially between languages of different families, typically Chinese and English. While there are studies on the identification of personal names from Chinese texts (e.g. Sun et al., 1995; Chen & Bai, 1998), this paper discusses the importance of the translation component and evaluates several existing Chinese-English MT systems for their capability in this context. The translation of personal names might seem trivial between some languages. For instance, Bill Clinton is always Bill Clinton, in English or in French, written and sometimes even pronounced the same. However, between languages from different families such as Chinese and English, the complexity is often beyond description by a few simple rules. For example, the international Kung Fu film star, , is known all over the world as Jackie"
2009.mtsummit-wpt.3,P91-1022,0,0.544113,"ntence alignment in Sec. 4., and introduce sentence filtering, including the evaluation of its impact on SMT in Sec. 5 as well as the final parallel corpus in Sec. 6, and conclude this paper. http://www.itl.nist.gov/iad/mig/tests/mt/ Revised on 2009/08/06 -17- 2 Related Work To get parallel sentences from parallel corpora, different approaches can be used for sentence alignment. The approaches can be based on a) sentence length, b) lexical information in bilingual dictionaries, c) statistical translation model, or d) the composite of more than one approach. The sentence-length-based approach (Brown et al. 1991; Gale and Church, 1991) aligns sentences based on the number of words or characters in each sentence. Dictionary-based techniques use extensive online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly d"
2009.mtsummit-wpt.3,P93-1002,0,0.601926,"mposite of more than one approach. The sentence-length-based approach (Brown et al. 1991; Gale and Church, 1991) aligns sentences based on the number of words or characters in each sentence. Dictionary-based techniques use extensive online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the w"
2009.mtsummit-wpt.3,P91-1023,0,0.379605,"Sec. 4., and introduce sentence filtering, including the evaluation of its impact on SMT in Sec. 5 as well as the final parallel corpus in Sec. 6, and conclude this paper. http://www.itl.nist.gov/iad/mig/tests/mt/ Revised on 2009/08/06 -17- 2 Related Work To get parallel sentences from parallel corpora, different approaches can be used for sentence alignment. The approaches can be based on a) sentence length, b) lexical information in bilingual dictionaries, c) statistical translation model, or d) the composite of more than one approach. The sentence-length-based approach (Brown et al. 1991; Gale and Church, 1991) aligns sentences based on the number of words or characters in each sentence. Dictionary-based techniques use extensive online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment"
2009.mtsummit-wpt.3,2001.mtsummit-papers.30,0,0.717795,"Missing"
2009.mtsummit-wpt.3,P07-2045,0,0.00531596,"icate fusing strategies than simply using average or multiplication. Filter is shown to be the best among all ensemble methods, which can be explained by the good filtering effects of Len and DictN for misaligned sentences among the highly ranked sentence pairs in the sorted list of Tran. 5.3 Impact of Sentence Filtering on SMT Although the experiment shows that sentence filtering can help identify really parallel sentences, we may wonder whether the sentence filtering actually leads to better SMT performance. Therefore, we evaluated the impact of sentence filtering on SMT. The Moses toolkit (Koehn et al., 2007) was used to conduct Chinese-&gt;English SMT experiments and BLEU and NIST scores are used as the evaluation metrics. We followed the instruction of the baseline system for the shared task in the 2008 ACL workshop on SMT. 9 8 Before the ensemble of individual scores, we first need to normalize the scores into the range between 0 and 1 according to their distributions: the length-based and dictionary-based scores are already within the range; the translation score roughly follows a linear distribution. The weights for Tran, Len, DictN are 99, 30 and 16, respectively. They are got by the exhaustive"
2009.mtsummit-wpt.3,2005.mtsummit-papers.11,0,0.131317,"Missing"
2009.mtsummit-wpt.3,ma-2006-champollion,0,0.401073,"g/tests/mt/ Revised on 2009/08/06 -17- 2 Related Work To get parallel sentences from parallel corpora, different approaches can be used for sentence alignment. The approaches can be based on a) sentence length, b) lexical information in bilingual dictionaries, c) statistical translation model, or d) the composite of more than one approach. The sentence-length-based approach (Brown et al. 1991; Gale and Church, 1991) aligns sentences based on the number of words or characters in each sentence. Dictionary-based techniques use extensive online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does"
2009.mtsummit-wpt.3,moore-2002-fast,0,0.792182,"e online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the web for low-density language pairs. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and"
2009.mtsummit-wpt.3,J03-1002,0,0.0138393,"Missing"
2009.mtsummit-wpt.3,J05-4003,0,0.0820068,"ord-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the web for low-density language pairs. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and Fung (2005) exploited Inversion Transduction Grammar to retrieve truly parallel sentence translations from large collections of highly non-parallel docuements. Utiyama and Isahara (2003) aligned articles and sentences from noisy parallel news articles, then sorted the aligned sentences according to a similarity measure, and selected only the highly ranked aligned sentence alignments. Although the construction of our ChineseE"
2009.mtsummit-wpt.3,J03-3002,0,0.0858242,"words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the web for low-density language pairs. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and Fung (2005) exploited Inversion Transduction Grammar to retrieve truly parallel sentence translations from large collections of highly non-parallel docuements. Utiyama and Isahara (2003) aligned articles and sentences from noisy parallel news articles, then sorted the aligned sentences according to a sim"
2009.mtsummit-wpt.3,2007.mtsummit-papers.63,0,0.166161,"c, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and Fung (2005) exploited Inversion Transduction Grammar to retrieve truly parallel sentence translations from large collections of highly non-parallel docuements. Utiyama and Isahara (2003) aligned articles and sentences from noisy parallel news articles, then sorted the aligned sentences according to a similarity measure, and selected only the highly ranked aligned sentence alignments. Although the construction of our ChineseEnglish patent parallel corpus is similar to that of the Japanese-English one (Utiyama and Isahara, 2007), we have made the following modifications on the basis of our data: 1) all sections of the patents, instead of only two parts in the description section, were used to find sentence alignments; 2) for sentence filtering, we integrated three individual measures, including the dictionary-based one (Utiyama and Isahara, 2007), and the experiments showed the combination of measures can improve the performance of sentence filtering. We also did SMT experiments, showing that filtering out misaligned sentences could improve SMT performance. 3 The Chinese-English Patents Parallel We use about 7000 Chi"
2009.mtsummit-wpt.3,D08-1058,0,0.0160422,"S c and S e respectively. 3) The bidirectional translation probability score Pt (Tran): it combines the translation probability value of both directions (i.e. Chinese-&gt;English and English-&gt;Chinese), instead of using only one direction (Moore, 2002; Chen, 2003). It is computed as follows: log ( P(S e |S c ))  log ( P(S c |S e )) lc  le where P( S e |S c ) denotes the probability that a translator will produce S e in English when presented with S c in Chinese, and vice versa for P(Sc |Se ) . pt ( S c , S e )  A wide variety of ensemble methods have been used in various fields (Polikar, 2006; Wan, 2008). -20- We evaluate the following8: 1) Average (Avg): the average of the individual scores; 2) Multiplication (Mul): the product of the individual scores; 3) Linear Combination (LinC): the weighted average by associating each individual score with a weight, indicating the relative confidence in the value; 4) Filter: use Pt for sorting, but if Pd or Pt of a sentence pair is lower than a predefined threshold, that pair will be moved to the end of the sorting list. The thresholds can be empirically set based on the data. 5.2 Empirical Evaluation of Sentence Filtering To assess the performance of i"
2009.mtsummit-wpt.3,I05-1023,0,0.0259188,"(2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the web for low-density language pairs. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and Fung (2005) exploited Inversion Transduction Grammar to retrieve truly parallel sentence translations from large collections of highly non-parallel docuements. Utiyama and Isahara (2003) aligned articles and sentences from noisy parallel news articles, then sorted the aligned sentences according to a similarity measure, and selected only the highly ranked aligned sentence alignments. Although the construction of our ChineseEnglish patent parallel corpus is similar to that of the Japanese-English one (Utiyama and Isahara, 2007), we have made the following modifications on the basis of our data: 1) all sec"
2009.mtsummit-wpt.3,J93-1004,0,\N,Missing
2009.mtsummit-wpt.3,J93-2003,0,\N,Missing
2009.mtsummit-wpt.3,P03-1010,0,\N,Missing
2018.gwc-1.23,P13-1133,0,0.0565985,"Missing"
2018.gwc-1.23,huang-etal-2004-sinica,0,0.17809,"Missing"
2018.gwc-1.23,W13-4302,0,0.178443,"tual gaps are identified, they may be handled by the addition or omission of synsets in the new wordnet. While the approach has the merit of good coverage, reliance on translation equivalents may be at the expense of forming non-synsets in the target language wordnet, for which great caution has to be exerted. Past experience from building multilingual wordnets has observed various difficulties, mostly arising from cross-linguistic differences in lexicalisation, conceptual space and sense distinction (e.g. Vossen, 1998). This paper discusses further observations from the Chinese Open Wordnet (Wang and Bond, 2013), which added new translations from authoritative bilingual dictionaries as a means to increase coverage, to show that translation equivalents need to be very carefully screened to avoid some potential and easily overlooked pitfalls. While a good coverage is appreciated, especially with a view to use the wordnets in a variety of computational and human language applications, it is suggested that alternative representations including additional relational pointers be used to accommodate cross-linguistic differences without disturbing the basic infrastructure of WordNet, in particular its basic"
2020.cogalex-1.14,J10-4006,0,0.137236,"the mental lexicon as a vast network (e.g. Aitchison, 2003). The interconnection of words in such a network can be used to account for and model various phenomena of the semantic memory like tip-of-the-tongue problem (e.g. Zock and Biemann, 2016). Free word associations include associative relations of different types and strengths. Their statistical modelling from large corpora (e.g. Church and Hanks, 1990; Wettler and Rapp, 1993) has contributed to lexicography for finding collocations and thesaural groups. There is a class of models and methods under distributional semantics (Harris, 1954; Baroni and Lenci, 2010; Clark, 2012), where word senses are represented by means of word co-occurrence vectors. The main assumption is that similar words appear in similar contexts, and by comparing the similarity of the vector spaces, it makes a popular approach for extracting paradigmatically related words (e.g. Agirre et al., 2001; Biemann et al., 2004; Hill et al., 2015; Santus et al., 2016). Word embedding (Mikolov et al., 2013) is a vector model among the latest trends. 3.1 Associations for Different Purposes Kwong (2016) has shown from a comparison of English and Chinese free association norms that the assoc"
2020.cogalex-1.14,biemann-etal-2004-automatic,0,0.108759,"eir statistical modelling from large corpora (e.g. Church and Hanks, 1990; Wettler and Rapp, 1993) has contributed to lexicography for finding collocations and thesaural groups. There is a class of models and methods under distributional semantics (Harris, 1954; Baroni and Lenci, 2010; Clark, 2012), where word senses are represented by means of word co-occurrence vectors. The main assumption is that similar words appear in similar contexts, and by comparing the similarity of the vector spaces, it makes a popular approach for extracting paradigmatically related words (e.g. Agirre et al., 2001; Biemann et al., 2004; Hill et al., 2015; Santus et al., 2016). Word embedding (Mikolov et al., 2013) is a vector model among the latest trends. 3.1 Associations for Different Purposes Kwong (2016) has shown from a comparison of English and Chinese free association norms that the association patterns are quite different. Free associations tend to be paradigmatic relations in English (e.g. correct – right), but syntagmatic or collocational relations in Chinese (e.g. 正確 correct – 答案 answer). Collocations and thesaural groups obtained from large corpora, like those computed by the Sketch Engine (Kilgarriff et al., 20"
2020.cogalex-1.14,J15-4004,0,0.0204057,"ing from large corpora (e.g. Church and Hanks, 1990; Wettler and Rapp, 1993) has contributed to lexicography for finding collocations and thesaural groups. There is a class of models and methods under distributional semantics (Harris, 1954; Baroni and Lenci, 2010; Clark, 2012), where word senses are represented by means of word co-occurrence vectors. The main assumption is that similar words appear in similar contexts, and by comparing the similarity of the vector spaces, it makes a popular approach for extracting paradigmatically related words (e.g. Agirre et al., 2001; Biemann et al., 2004; Hill et al., 2015; Santus et al., 2016). Word embedding (Mikolov et al., 2013) is a vector model among the latest trends. 3.1 Associations for Different Purposes Kwong (2016) has shown from a comparison of English and Chinese free association norms that the association patterns are quite different. Free associations tend to be paradigmatic relations in English (e.g. correct – right), but syntagmatic or collocational relations in Chinese (e.g. 正確 correct – 答案 answer). Collocations and thesaural groups obtained from large corpora, like those computed by the Sketch Engine (Kilgarriff et al., 2004), may not always"
2020.cogalex-1.14,Y16-2023,1,0.841688,"is a class of models and methods under distributional semantics (Harris, 1954; Baroni and Lenci, 2010; Clark, 2012), where word senses are represented by means of word co-occurrence vectors. The main assumption is that similar words appear in similar contexts, and by comparing the similarity of the vector spaces, it makes a popular approach for extracting paradigmatically related words (e.g. Agirre et al., 2001; Biemann et al., 2004; Hill et al., 2015; Santus et al., 2016). Word embedding (Mikolov et al., 2013) is a vector model among the latest trends. 3.1 Associations for Different Purposes Kwong (2016) has shown from a comparison of English and Chinese free association norms that the association patterns are quite different. Free associations tend to be paradigmatic relations in English (e.g. correct – right), but syntagmatic or collocational relations in Chinese (e.g. 正確 correct – 答案 answer). Collocations and thesaural groups obtained from large corpora, like those computed by the Sketch Engine (Kilgarriff et al., 2004), may not always agree with the word association norms. Sometimes apparently strong associations may not rank high. The main problem, however, is that the associations are n"
2020.cogalex-1.14,D14-1162,0,0.0882412,"t if they do not usually modify contributions. Similarly, it is useful to know what remarkable often modifies, but for this task they would not be informative if they are not also closely associated with contributions. Hence, we need to be able to refer to the associations relevant for a particular purpose. In other words, free associations should be reprioritised for specific language tasks. 3.2 An Example In this example, we try to address the kind of situations discussed in Section 2.1. Cosine similarities between words are computed with the pre-trained GloVe (6B tokens, 50d) word vectors (Pennington et al., 2014). As the models learn the word representations from their usual contexts, word embeddings are known for their good job on computing word similarity and analogies, which are surprisingly intuitive and interesting. But in contrast to what is usually highlighted, words with high similarities are not restricted to paradigmatically related words. As shown in Figure 1, although remarkable and outstanding are expected to be similar to each other, the similarity scores may actually be even higher between the adjectives and the nouns they modify. For instance, the similarities for remarkable – achievem"
2020.cogalex-1.14,L16-1722,0,0.0120414,"ora (e.g. Church and Hanks, 1990; Wettler and Rapp, 1993) has contributed to lexicography for finding collocations and thesaural groups. There is a class of models and methods under distributional semantics (Harris, 1954; Baroni and Lenci, 2010; Clark, 2012), where word senses are represented by means of word co-occurrence vectors. The main assumption is that similar words appear in similar contexts, and by comparing the similarity of the vector spaces, it makes a popular approach for extracting paradigmatically related words (e.g. Agirre et al., 2001; Biemann et al., 2004; Hill et al., 2015; Santus et al., 2016). Word embedding (Mikolov et al., 2013) is a vector model among the latest trends. 3.1 Associations for Different Purposes Kwong (2016) has shown from a comparison of English and Chinese free association norms that the association patterns are quite different. Free associations tend to be paradigmatic relations in English (e.g. correct – right), but syntagmatic or collocational relations in Chinese (e.g. 正確 correct – 答案 answer). Collocations and thesaural groups obtained from large corpora, like those computed by the Sketch Engine (Kilgarriff et al., 2004), may not always agree with the word a"
2020.cogalex-1.14,W93-0310,0,0.564183,"erein, to facilitate translators’ work and to inspire them of the possibilities for rendition. Lexical access is largely concerned with word associations which form the basis of modelling the mental lexicon as a vast network (e.g. Aitchison, 2003). The interconnection of words in such a network can be used to account for and model various phenomena of the semantic memory like tip-of-the-tongue problem (e.g. Zock and Biemann, 2016). Free word associations include associative relations of different types and strengths. Their statistical modelling from large corpora (e.g. Church and Hanks, 1990; Wettler and Rapp, 1993) has contributed to lexicography for finding collocations and thesaural groups. There is a class of models and methods under distributional semantics (Harris, 1954; Baroni and Lenci, 2010; Clark, 2012), where word senses are represented by means of word co-occurrence vectors. The main assumption is that similar words appear in similar contexts, and by comparing the similarity of the vector spaces, it makes a popular approach for extracting paradigmatically related words (e.g. Agirre et al., 2001; Biemann et al., 2004; Hill et al., 2015; Santus et al., 2016). Word embedding (Mikolov et al., 201"
2020.cogalex-1.14,W16-5308,0,0.019921,"Associations The issue here is therefore to expand lexical access routes in dictionaries, on top of the thesaural and collocational information as well as example sentences already found therein, to facilitate translators’ work and to inspire them of the possibilities for rendition. Lexical access is largely concerned with word associations which form the basis of modelling the mental lexicon as a vast network (e.g. Aitchison, 2003). The interconnection of words in such a network can be used to account for and model various phenomena of the semantic memory like tip-of-the-tongue problem (e.g. Zock and Biemann, 2016). Free word associations include associative relations of different types and strengths. Their statistical modelling from large corpora (e.g. Church and Hanks, 1990; Wettler and Rapp, 1993) has contributed to lexicography for finding collocations and thesaural groups. There is a class of models and methods under distributional semantics (Harris, 1954; Baroni and Lenci, 2010; Clark, 2012), where word senses are represented by means of word co-occurrence vectors. The main assumption is that similar words appear in similar contexts, and by comparing the similarity of the vector spaces, it makes a"
C08-1058,P99-1016,0,0.0941831,"Missing"
C08-1058,W02-0903,0,0.0512252,"Missing"
C08-1058,C92-2082,0,0.0612395,"Missing"
C08-1058,D07-1034,1,0.124581,"ant part of the lexical knowledge, which will be useful and critical for many NLP applications, including natural language understanding, information retrieval, and machine translation. Tsou and Kwong (2006) proposed a comprehensive Pan-Chinese lexical resource, using a large and unique synchronous Chinese corpus as an authentic source of lexical variation among various Chinese speech communities. They also studied the feasibility of taking an existing Chinese thesaurus as leverage and classifying new words from various Chinese communities with respect to the classificatory structure therein (Kwong and Tsou, 2007). They used the catego1 The transcriptions in brackets are based on Hanyu Pinyin. 457 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 457–464 Manchester, August 2008 ries at the subclass level of the Tongyici Cilin (同 義詞詞林, abbreviated as Cilin hereafter) for the task. The classification was done by comparing the similarity of a target word (i.e. the word to be classified) and individual categories of words in the thesaurus based on a feature vector of cooccurring words in a corpus. Since words in the thesaurus are mostly based on lexical item"
C08-1058,W04-2103,0,0.0329367,"Missing"
C08-1058,W97-0313,0,0.124893,"Missing"
C08-1058,W02-1028,0,0.060491,"Missing"
C08-1058,W97-0803,0,0.0327694,"Missing"
C08-1058,P03-2011,0,0.0447363,"Missing"
C08-1058,tsou-kwong-2006-toward,1,0.850149,"y used as 居 屋 to mean general housing in Mainland China, it is rarely seen in the Hong Kong context; and 下崗 (xia4gang3) is specific, if not exclusive, to Mainland China for referring to a special concept of unemployment. Existing Chinese lexical resources are often based on language use in one particular region and are therefore not comprehensive enough to capture the substantial regional variation as an important part of the lexical knowledge, which will be useful and critical for many NLP applications, including natural language understanding, information retrieval, and machine translation. Tsou and Kwong (2006) proposed a comprehensive Pan-Chinese lexical resource, using a large and unique synchronous Chinese corpus as an authentic source of lexical variation among various Chinese speech communities. They also studied the feasibility of taking an existing Chinese thesaurus as leverage and classifying new words from various Chinese communities with respect to the classificatory structure therein (Kwong and Tsou, 2007). They used the catego1 The transcriptions in brackets are based on Hanyu Pinyin. 457 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 4"
C08-1058,P98-2127,0,0.19061,"Missing"
C08-1058,C98-2122,0,\N,Missing
C08-1058,W06-0101,0,\N,Missing
C98-2240,P81-1030,0,0.14027,"(Ill.), require a wide range of resources to supply the necessary lexical semantic information. For instance, Calzolari (1988) proposed a lexieal database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the a t t e m p t s to enrich lexical information, m a n y have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonethdess, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoe. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget's Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate ditferent lexical re"
C98-2240,P84-1036,0,0.168215,"Missing"
C98-2240,P86-1018,0,0.0539991,"e necessary lexical semantic information. For instance, Calzolari (1988) proposed a lexieal database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the a t t e m p t s to enrich lexical information, m a n y have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonethdess, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoe. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget's Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate ditferent lexical resources, the problem is: how do we recoi&gt; cile the rich but vari"
C98-2240,C92-2070,0,0.199093,"tion to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonethdess, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoe. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget's Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate ditferent lexical resources, the problem is: how do we recoi&gt; cile the rich but variable information in dictionary 1487 senses with the cruder but more stable taxonomies like those in thesauri7 This work is intended to fill this gap. We use WordNet as a mediator in the process, in the following, we will outline an algorithm to m a p word senses in a dictionary to semantic classes in some established classitication scheme. 2 Inter-relatedness of the Resources Tlle three lexical resources used in this work are the 1987 revision of Roget's Thesaurus ( R O G E"
C98-2240,P85-1037,0,\N,Missing
D07-1034,P99-1016,0,0.216167,"words within a semantic hierarchy, and to group similar words together into a class. Previous work on automatic methods for 326 building semantic lexicons could be divided into two main groups. One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. Riloff and Shepherd, 1997; Thelen and Riloff, 2002). For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. Caraballo (1999) selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a thesaurus from a Chinese newspaper corpus. Another line of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect"
D07-1034,W02-0903,0,0.530291,"traction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a thesaurus from a Chinese newspaper corpus. Another line of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004). An early effort along this line is Hearst (1992), who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet. Ciaramita (2002) compared several models in classifying nouns with respect to a simplified version of WordNet and signified the gain in performance with morphological features. For Chinese, Tseng (2003) proposed a method based on morphological similarity to assign a Cilin category to unknown words from the Sinica corpus which were not in the Chinese Electronic Dictionary and Cilin; but somehow the test data were taken from Cilin, and therefore could not really demonstrate the effectiveness with unknown words found in the Sinica corpus. The current work attempts to classify new words with an existing thesaural"
D07-1034,W02-0908,0,0.0178704,"matic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. Riloff and Shepherd, 1997; Thelen and Riloff, 2002). For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. Caraballo (1999) selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a thesaurus from a Chinese newspaper corpus. Another line of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004). An early effort along this line is Hearst (1992), who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntact"
D07-1034,C92-2082,0,0.0702386,"asure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a thesaurus from a Chinese newspaper corpus. Another line of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004). An early effort along this line is Hearst (1992), who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet. Ciaramita (2002) compared several models in classifying nouns with respect to a simplified version of WordNet and signified the gain in performance with morphological features. For Chinese, Tseng (2003) proposed a method based on morphological similarity to assign a Cilin category to unknown words from the Sinica corpus which were not in the Chinese Electronic Dictionary and Cilin; but somehow the test data were taken from Cilin, and therefo"
D07-1034,P98-2127,0,0.176498,"esented and discussed with future directions in Section 5, followed by a conclusion in Section 6. 2 Related Work To build a semantic lexicon, one has to identify the relation between words within a semantic hierarchy, and to group similar words together into a class. Previous work on automatic methods for 326 building semantic lexicons could be divided into two main groups. One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. Riloff and Shepherd, 1997; Thelen and Riloff, 2002). For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. Caraballo (1999) selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a th"
D07-1034,W04-2103,0,0.447163,"n noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a thesaurus from a Chinese newspaper corpus. Another line of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004). An early effort along this line is Hearst (1992), who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet. Ciaramita (2002) compared several models in classifying nouns with respect to a simplified version of WordNet and signified the gain in performance with morphological features. For Chinese, Tseng (2003) proposed a method based on morphological similarity to assign a Cilin category to unknown words from the Sinica corpus which were not in the Chinese Electronic Dictionary and Cilin; but someho"
D07-1034,W97-0313,0,0.243638,"ls used and the experimental setup respectively. Results will be presented and discussed with future directions in Section 5, followed by a conclusion in Section 6. 2 Related Work To build a semantic lexicon, one has to identify the relation between words within a semantic hierarchy, and to group similar words together into a class. Previous work on automatic methods for 326 building semantic lexicons could be divided into two main groups. One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. Riloff and Shepherd, 1997; Thelen and Riloff, 2002). For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. Caraballo (1999) selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2"
D07-1034,W02-1028,0,0.170847,"l setup respectively. Results will be presented and discussed with future directions in Section 5, followed by a conclusion in Section 6. 2 Related Work To build a semantic lexicon, one has to identify the relation between words within a semantic hierarchy, and to group similar words together into a class. Previous work on automatic methods for 326 building semantic lexicons could be divided into two main groups. One is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. Riloff and Shepherd, 1997; Thelen and Riloff, 2002). For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. Caraballo (1999) selected head nouns from conjunctions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature cluste"
D07-1034,W97-0803,0,0.49039,"tions and appositives in noun phrases, and used the cosine similarity measure with a bottomup clustering technique to construct a noun hierarchy from text. Curran and Moens (2002) explored a new similarity measure for automatic thesaurus extraction which better compromises with the speed/performance tradeoff. You and Chen (2006) used a feature clustering method to create a thesaurus from a Chinese newspaper corpus. Another line of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004). An early effort along this line is Hearst (1992), who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet. Ciaramita (2002) compared several models in classifying nouns with respect to a simplified version of WordNet and signified the gain in performance with morphological features. For Chinese, Tseng (2003) proposed a method based on morphological similarity to assign a Cilin category to unknown words from the Sinica corpus which were not in the Chinese Electronic Dictionary and Cil"
D07-1034,P03-2011,0,0.29255,"e of research, which is more closely related with the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. Tokunaga et al., 1997; Pekar, 2004). An early effort along this line is Hearst (1992), who attempted to identify hyponyms from large text corpora, based on a set of lexico-syntactic patterns, to augment and critique the content of WordNet. Ciaramita (2002) compared several models in classifying nouns with respect to a simplified version of WordNet and signified the gain in performance with morphological features. For Chinese, Tseng (2003) proposed a method based on morphological similarity to assign a Cilin category to unknown words from the Sinica corpus which were not in the Chinese Electronic Dictionary and Cilin; but somehow the test data were taken from Cilin, and therefore could not really demonstrate the effectiveness with unknown words found in the Sinica corpus. The current work attempts to classify new words with an existing thesaural classificatory structure. However, the usual practice in past studies is to test with a portion of data from the thesaurus itself and evaluate the results against the original classific"
D07-1034,tsou-kwong-2006-toward,1,0.486994,"truction. 1 Introduction Large-scale semantic lexicons are important resources for many natural language processing (NLP) tasks. For a significant world language such as Chinese, it is especially critical to capture the substantial regional variation as an important part of the lexical knowledge, which will be useful for many NLP applications, including natural language understanding, information retrieval, and machine translation. Existing Chinese lexical resources, however, are often based on language use in one particular region and thus lack the desired comprehensiveness. Toward this end, Tsou and Kwong (2006) proposed a comprehensive Pan-Chinese lexical resource, based on a large and unique synchronous Chinese corpus as an authentic source for lexical acquisition and analysis across various Chinese speech communities. To allow maximum versatility and portability, it is expected to document the core and universal substances of the language on the one hand, and also the more subtle variations found in different communities on the other. Different Chinese speech communities might share lexical items in the same form but with different meanings. For instance, the word 居屋 refers to general housing in M"
D07-1034,C98-2122,0,\N,Missing
D07-1034,W06-0101,0,\N,Missing
I05-1070,W04-2413,0,0.0467895,"Missing"
I05-1070,P98-1013,0,0.00485599,"to be labelled. We will also compare the results on two training and testing datasets. In Section 2, related work will be reviewed. In Section 3, the data used in the current study will be introduced. Our proposed method will be explained in Section 4, and the experiment reported in Section 5. Results and future work will be discussed in Section 6, followed by conclusions in Section 7. 2 Related Work The definition of semantic roles falls on a continuum from abstract ones to very specific ones. Gildea and Jurafsky [6], for instance, used a set of roles defined according to the FrameNet model [2], thus corresponding to the frame elements in individual frames under a particular domain to which a given verb belongs. Lexical entries (in fact not limited to verbs, in the case of FrameNet) falling under the same frame will share the same set of roles. Gildea and Palmer [7] defined roles with respect to individual predicates in the PropBank, without explicit naming. To date PropBank and FrameNet are the two main resources in English for training semantic role labelling systems. The theoretical treatment of semantic roles is also varied in Chinese. In practice, for example, the semantic role"
I05-1070,W04-2412,0,0.0211378,"is study we only make an effort to eliminate multiple tagging of the same role to the same target verb in a sentence on either side of the target verb, but not if they appear on both sides of the target verb. This should certainly be dealt with in future experiments. The differential degradation of performance between textbook data and news data also suggests the varied importance of constituent boundaries to simple sentences and complex ones, and hence possibly their varied requirements for full parse information for the semantic labelling task. 6 Discussion According to Carreras and Màrquez [3], the state-of-the-art results for semantic role labelling systems based on shallow syntactic information is about 15 lower than those with access to gold standard parse trees, i.e., around 60. Our experimental results for the headword location condition, with no syntactic information available 812 O.Y. Kwong and B.K. Tsou at all, give an F1 score of 52.89 and 44.35 respectively for textbook data and news data. This further degradation in performance is nevertheless within expectation, but whether this is also a result of the difference between English and Chinese remains to be seen. In respon"
I05-1070,J02-3001,0,0.710058,"ces with this kind of information would in turn facilitate the development of such applications. Large-scale production of annotated resources is often labour-intensive, and thus needs automatic labelling to streamline the work. The task can essentially be perceived as a two-phase process, namely to recognise the constituents bearing some semantic relationship to the target verb in a sentence, and then to label them with the corresponding semantic roles. In their seminal proposal, Gildea and Jurafsky approached the task using various features such as headword, phrase type, and parse tree path [6]. Such features have remained the basic and essential features in subsequent research, irrespective of the variation in the actual learning components. In addition, parsed sentences are often required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling [7, 8]. More precisely, in semantic role labelling, parse information is rather more critical for the identification of boundaries for candidate constituents than for the extraction R. Dale et al. (Eds.): IJCNLP 200"
I05-1070,P02-1031,0,0.0847852,"entence, and then to label them with the corresponding semantic roles. In their seminal proposal, Gildea and Jurafsky approached the task using various features such as headword, phrase type, and parse tree path [6]. Such features have remained the basic and essential features in subsequent research, irrespective of the variation in the actual learning components. In addition, parsed sentences are often required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling [7, 8]. More precisely, in semantic role labelling, parse information is rather more critical for the identification of boundaries for candidate constituents than for the extraction R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 804 – 814, 2005. © Springer-Verlag Berlin Heidelberg 2005 Semantic Role Tagging for Chinese at the Lexical Level 805 of training data. Its limited function in training, for instance, is reflected in the low coverage reported (e.g. [21]). However, given the imperfection of existing automatic parsers, which are far from producing gold standard parses, many thus resort to s"
I05-1070,W03-1008,0,0.0140963,"entence, and then to label them with the corresponding semantic roles. In their seminal proposal, Gildea and Jurafsky approached the task using various features such as headword, phrase type, and parse tree path [6]. Such features have remained the basic and essential features in subsequent research, irrespective of the variation in the actual learning components. In addition, parsed sentences are often required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling [7, 8]. More precisely, in semantic role labelling, parse information is rather more critical for the identification of boundaries for candidate constituents than for the extraction R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 804 – 814, 2005. © Springer-Verlag Berlin Heidelberg 2005 Semantic Role Tagging for Chinese at the Lexical Level 805 of training data. Its limited function in training, for instance, is reflected in the low coverage reported (e.g. [21]). However, given the imperfection of existing automatic parsers, which are far from producing gold standard parses, many thus resort to s"
I05-1070,W04-2416,0,0.0495149,"Missing"
I05-1070,W04-2417,0,0.0456926,"Missing"
I05-1070,kingsbury-palmer-2002-treebank,0,0.174303,"ent boundaries are known, while parse information might be more important for complicated sentences than simple ones. Several ways to improve the headword identification results were suggested, and we also plan to explore some class-based techniques for the task, with reference to existing semantic lexicons. 1 Introduction As the development of language resources progresses from POS-tagged corpora to syntactically annotated treebanks, the inclusion of semantic information such as predicate-argument relations is becoming indispensable. The expansion of the Penn Treebank into a Proposition Bank [11] is a typical move in this direction. Lexical resources also need to be enhanced with semantic information (e.g. [5]). In fact the ability to identify semantic role relations correctly is essential to many applications such as information extraction and machine translation; and making available resources with this kind of information would in turn facilitate the development of such applications. Large-scale production of annotated resources is often labour-intensive, and thus needs automatic labelling to streamline the work. The task can essentially be perceived as a two-phase process, namely"
I05-1070,W04-0832,0,0.0515194,"Missing"
I05-1070,E03-1081,1,0.773653,"rimary school Chinese textbooks popularly used in Hong Kong were taken for reference. The two publishers were Keys Press [22] and Modern Education Research Society Ltd [23]. Texts for Primary One to Six were digitised, segmented into words, and annotated with parts-of-speech (POS). The two sets of textbooks amount to a text collection of about 165K character tokens and upon segmentation about 109K word tokens (about 15K word types). There were about 2,500 transitive verb types, with frequency ranging from 1 to 926. The complex examples were taken from a subset of the LIVAC synchronous corpus1 [13, 18]. The subcorpus consists of newspaper texts from Hong Kong, including local news, international news, financial news, sports news, and entertainment news, collected in 1997-98. The texts were segmented into words and POS-tagged, amounting to about 1.8M character tokens and upon segmentation about 1M word tokens (about 47K word types). There were about 7,400 transitive verb types, with frequency ranging from 1 to just over 6,300. 3.2 Training and Testing Data For the current study, a set of 41 transitive verbs common to the two corpora (hereafter referred to as textbook corpus and news corpus),"
I05-1070,W04-0803,0,0.0616379,"Missing"
I05-1070,W04-0841,0,0.0620925,"Missing"
I05-1070,W04-2422,0,0.0387254,"Missing"
I05-1070,W04-3212,0,0.0185973,"overall performance. Another area of interest is to look at the behaviour of near-synonymous predicates in the tagging process. Many predicates may be unseen in the training data, but while the probability estimation could be generalized from near-synonyms as suggested by a semantic lexicon, whether the similarity and subtle differences between nearsynonyms with respect to the argument structure and the corresponding syntactic realisation could be distinguished would also be worth studying. Related to this is the possibility of augmenting the feature set with semantic features. Xue and Palmer [20], for instance, looked into new features such as syntactic frame, lexicalized constituent type, etc., and found that enriching the feature set improved the labelling performance. Another direction of future work is on the location of constituent boundaries upon the identification of the headword. As mentioned earlier on, this could probably be tackled by some finite state techniques or with the help of simple chunkers. 詞林 同義詞 Semantic Role Tagging for Chinese at the Lexical Level 813 7 Conclusion The study reported in this paper has thus tackled the unknown constituent boundary condition in se"
I05-1070,W04-1116,0,0.0122922,"g and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling [7, 8]. More precisely, in semantic role labelling, parse information is rather more critical for the identification of boundaries for candidate constituents than for the extraction R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 804 – 814, 2005. © Springer-Verlag Berlin Heidelberg 2005 Semantic Role Tagging for Chinese at the Lexical Level 805 of training data. Its limited function in training, for instance, is reflected in the low coverage reported (e.g. [21]). However, given the imperfection of existing automatic parsers, which are far from producing gold standard parses, many thus resort to shallow syntactic information from simple chunking, though results often turn out to be less satisfactory than with full parses. This limitation is even more pertinent for the application of semantic role labelling to languages which do not have sophisticated parsing resources. In the case of Chinese, for example, there is considerable variability in its syntax-semantics interface; and when one has more nested and complex sentences such as those from news art"
I05-1070,W04-3213,0,\N,Missing
I05-1070,C04-1179,0,\N,Missing
I05-1070,C98-1013,0,\N,Missing
I05-1070,W05-0620,0,\N,Missing
kwong-2010-constructing,J87-1002,0,\N,Missing
kwong-2010-constructing,C88-2120,0,\N,Missing
kwong-2010-constructing,J92-4007,0,\N,Missing
O05-5009,Y00-1012,0,0.0213692,"Missing"
O05-5009,Y03-1022,1,0.858808,"Missing"
O05-5009,O00-2002,0,\N,Missing
P09-2006,I08-4003,0,0.274447,"homophones more reasonably especially when the training data is limited. In this paper we report some work in progress and compare E2C in Cantonese and Mandarin Chinese. Related work will be briefly reviewed in Section 2. Some characteristics of E2C will be discussed in Section 3. Work in progress will be reported in Section 4, followed by a conclusion with future work in Section 5. 2 Related Work There are basically two categories of work on machine transliteration. First, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Kuo and Li, 2008). Second, statistical models are built for transliteration. These models could be phoneme-based (e.g. Knight and Graehl, 1998), grapheme-based (e.g. Li et al., 2004), hybrid (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006) and semantic (e.g. Li et al., 2007) features. Li et al. (2004) used a Joint Source-Channel Model under the direct orthographic mapping 21 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 21–24, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP The homophonous third character gives rise to multiple alternative transliterations in this ex"
P09-2006,P04-1021,0,0.64355,"朗拿 度 long5-naa4-dou6 in Cantonese, other phonetically similar candidates like 朗娜度 long5naa4-dou6 or 郎拿刀 long4-naa4-dou11 are least likely. Beyond linguistic and phonetic properties, many other social and cognitive factors such as dialect, gender, domain, meaning, and perception, are simultaneously influencing the naming process and superimposing on the surface graphemic correspondence. The abundance of homophones in Chinese further complicates the problem. Past studies on phoneme-based E2C have reported their adverse effects (e.g. Virga and Khudanpur, 2003). Direct orthographic mapping (e.g. Li et al., 2004), making use of individual Chinese graphemes, tends 1 Mandarin names are transcribed in Hanyu Pinyin and Cantonese names are transcribed in Jyutping published by the Linguistic Society of Hong Kong. to overcome the problem and model the character choice directly. Meanwhile, Chinese is a typical tonal language and the tone information can help distinguish certain homophones. Phoneme mapping studies seldom make use of tone information. Transliteration is also an open problem, as new names come up everyday and there is no absolute or one-to-one transliterated version for any name. Although direct"
P09-2006,P07-1016,0,0.557891,"Missing"
P09-2006,I05-1040,0,0.181577,"riefly reviewed in Section 2. Some characteristics of E2C will be discussed in Section 3. Work in progress will be reported in Section 4, followed by a conclusion with future work in Section 5. 2 Related Work There are basically two categories of work on machine transliteration. First, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Kuo and Li, 2008). Second, statistical models are built for transliteration. These models could be phoneme-based (e.g. Knight and Graehl, 1998), grapheme-based (e.g. Li et al., 2004), hybrid (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006) and semantic (e.g. Li et al., 2007) features. Li et al. (2004) used a Joint Source-Channel Model under the direct orthographic mapping 21 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 21–24, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP The homophonous third character gives rise to multiple alternative transliterations in this example, where orthographically 利 lei6, 莉 lei6 and 里 lei5 are observed for “ry” in transliteration data. One cannot really say any of the combinations is “right” or “wrong”, but perhaps only “bet"
P09-2006,W06-1630,0,0.327293,"ics of E2C will be discussed in Section 3. Work in progress will be reported in Section 4, followed by a conclusion with future work in Section 5. 2 Related Work There are basically two categories of work on machine transliteration. First, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Kuo and Li, 2008). Second, statistical models are built for transliteration. These models could be phoneme-based (e.g. Knight and Graehl, 1998), grapheme-based (e.g. Li et al., 2004), hybrid (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006) and semantic (e.g. Li et al., 2007) features. Li et al. (2004) used a Joint Source-Channel Model under the direct orthographic mapping 21 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 21–24, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP The homophonous third character gives rise to multiple alternative transliterations in this example, where orthographically 利 lei6, 莉 lei6 and 里 lei5 are observed for “ry” in transliteration data. One cannot really say any of the combinations is “right” or “wrong”, but perhaps only “better” or “worse”. Such judgement is more cognit"
P09-2006,W03-1508,0,0.187213,"r instance, while the Brazilian striker Ronaldo is rendered as 朗拿 度 long5-naa4-dou6 in Cantonese, other phonetically similar candidates like 朗娜度 long5naa4-dou6 or 郎拿刀 long4-naa4-dou11 are least likely. Beyond linguistic and phonetic properties, many other social and cognitive factors such as dialect, gender, domain, meaning, and perception, are simultaneously influencing the naming process and superimposing on the surface graphemic correspondence. The abundance of homophones in Chinese further complicates the problem. Past studies on phoneme-based E2C have reported their adverse effects (e.g. Virga and Khudanpur, 2003). Direct orthographic mapping (e.g. Li et al., 2004), making use of individual Chinese graphemes, tends 1 Mandarin names are transcribed in Hanyu Pinyin and Cantonese names are transcribed in Jyutping published by the Linguistic Society of Hong Kong. to overcome the problem and model the character choice directly. Meanwhile, Chinese is a typical tonal language and the tone information can help distinguish certain homophones. Phoneme mapping studies seldom make use of tone information. Transliteration is also an open problem, as new names come up everyday and there is no absolute or one-to-one"
P09-2006,J98-4003,0,\N,Missing
P98-2245,P81-1030,0,0.148938,"n retrieval (IR), require a wide range of resources to supply the necessary lexical semantic information. For instance, Calzolari (1988) proposed a lexical database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the attempts to enrich lexical information, many have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoc. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget&apos;s Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate different lexical r"
P98-2245,P84-1036,0,0.0770217,"R), require a wide range of resources to supply the necessary lexical semantic information. For instance, Calzolari (1988) proposed a lexical database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the attempts to enrich lexical information, many have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoc. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget&apos;s Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate different lexical resources, the pro"
P98-2245,P86-1018,0,0.0540759,"supply the necessary lexical semantic information. For instance, Calzolari (1988) proposed a lexical database in Italian which has the features of both a dictionary and a thesaurus; and Klavans and Tzoukermann (1995) tried to build a fuller bilingual lexicon by enhancing machine-readable dictionaries with large corpora. Among the attempts to enrich lexical information, many have been directed to the analysis of dictionary definitions and the transformation of the implicit information to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoc. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget&apos;s Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate different lexical resources, the problem is: how do we reconcile the rich but varia"
P98-2245,C92-2070,0,0.444905,"ion to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolari, 1984; Chodorow et al., 1985; Markowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries are also infamous of their non-standardised sense granularity, and the taxonomies obtained from definitions are inevitably ad hoc. It would therefore be a good idea if we can unify our lexical semantic knowledge by some existing, and widely exploited, classifications such as the system in Roget&apos;s Thesaurus (Roget, 1852), which has remained intact for years and has been used in WSD (Yarowsky, 1992). While the objective is to integrate different lexical resources, the problem is: how do we reconcile the rich but variable information in dictionary 1487 senses with the cruder but more stable taxonomies like those in thesauri? This work is intended to fill this gap. We use WordNet as a mediator in the process. In the following, we will outline an algorithm to map word senses in a dictionary to semantic classes in some established classification scheme. 2 Inter-relatedness of the Resources The three lexical resources used in this work are the 1987 revision of Roget&apos;s Thesaurus (ROGET) (Kirkp"
P98-2245,P85-1037,0,\N,Missing
S07-1020,W04-0828,0,0.016387,"al sensitivity of WSD, which would inform and enhance the development of WSD systems in return. 1 Introduction In recent years, many research teams all over the world have gained rich experience on word sense disambiguation (WSD) from the shared tasks of the SENSEVAL workshops. The need for multiple knowledge sources has become a golden rule, and the “lexical sensitivity” once remarked by Resnik and Yarowsky (1997) is addressed by various means in statistical classifiers, such as learning an optimal combination of the various knowledge sources for individual target words (e.g. Mihalcea, 2002; Escudero et al., 2004). Another common practice is to use an ensemble of classifiers. As pointed out by Mihalcea et al. (2004), among the participating systems in the SENSEVAL-3 English lexical sample task, “several of the top performance systems are based on combination of multiple classifiers, which shows once again that voting scheme that combine several learning algorithms outperform the accuracy of individual classifiers”. However, the advancement in WSD is rarely accompanied by any extensive account on the cognitive aspects of the task or qualitative analysis of the relation between the disambiguation results"
S07-1020,J98-1006,0,0.026373,"d word on its right Word forms of the words at fixed positions from the target word, including the first and second word on its left and the first and second word on its right Knowledge Sources Only the training data provided by the task organisers was used to train the system. We used four major types of contextual features, which could be classified into Target features, Local features, Topical features and Syntactic features, as described in Table 1. All features were converted to binary features. 2.3 optimally disambiguated with other types of information. Intrinsic Nature of Target Words Leacock et al. (1998), for example, observed that “the benefits of adding topical to local context alone depend on syntactic category as well as on the characteristics of the individual word”. In other words, some target words happen to be more “topical” than others and might therefore be more susceptible to topical contextual features during disambiguation. Others, however, might only be 110 W-10…W+10 P-2 P0 P-1 P0 P0 P+1 P0 P+2 P-2 P-1 P0 P0 P+1 P+2 Topical Features Content words appearing within the window of ten words on each side of the target word Syntactic Features POS bigrams composed of the target word an"
S07-1020,W04-0807,0,0.0335614,"uction In recent years, many research teams all over the world have gained rich experience on word sense disambiguation (WSD) from the shared tasks of the SENSEVAL workshops. The need for multiple knowledge sources has become a golden rule, and the “lexical sensitivity” once remarked by Resnik and Yarowsky (1997) is addressed by various means in statistical classifiers, such as learning an optimal combination of the various knowledge sources for individual target words (e.g. Mihalcea, 2002; Escudero et al., 2004). Another common practice is to use an ensemble of classifiers. As pointed out by Mihalcea et al. (2004), among the participating systems in the SENSEVAL-3 English lexical sample task, “several of the top performance systems are based on combination of multiple classifiers, which shows once again that voting scheme that combine several learning algorithms outperform the accuracy of individual classifiers”. However, the advancement in WSD is rarely accompanied by any extensive account on the cognitive aspects of the task or qualitative analysis of the relation between the disambiguation results and the nature of individual target words underlying the apparent lexical sensitivity of the task. Give"
S07-1020,W97-0213,0,0.0355547,"statistical WSD. Despite the insignificant improvement observed in this preliminary attempt, more systematic analysis remains to be done for a cognitively plausible account of the factors underlying the lexical sensitivity of WSD, which would inform and enhance the development of WSD systems in return. 1 Introduction In recent years, many research teams all over the world have gained rich experience on word sense disambiguation (WSD) from the shared tasks of the SENSEVAL workshops. The need for multiple knowledge sources has become a golden rule, and the “lexical sensitivity” once remarked by Resnik and Yarowsky (1997) is addressed by various means in statistical classifiers, such as learning an optimal combination of the various knowledge sources for individual target words (e.g. Mihalcea, 2002; Escudero et al., 2004). Another common practice is to use an ensemble of classifiers. As pointed out by Mihalcea et al. (2004), among the participating systems in the SENSEVAL-3 English lexical sample task, “several of the top performance systems are based on combination of multiple classifiers, which shows once again that voting scheme that combine several learning algorithms outperform the accuracy of individual"
tsou-kwong-2006-toward,C82-2013,0,\N,Missing
tsou-kwong-2006-toward,P99-1016,0,\N,Missing
tsou-kwong-2006-toward,O05-5009,1,\N,Missing
tsou-kwong-2006-toward,Y96-1018,0,\N,Missing
W02-1404,J96-1001,0,\N,Missing
W02-1404,A97-1050,0,\N,Missing
W02-1404,N01-1020,0,\N,Missing
W02-1404,C00-1015,0,\N,Missing
W02-1404,P91-1023,0,\N,Missing
W02-1404,P00-1050,0,\N,Missing
W02-1404,P94-1012,0,\N,Missing
W02-1404,O97-4004,0,\N,Missing
W02-1802,C00-1015,0,0.0276459,"on, Hong Kong csrluk@comp.polyu.edu.hk the significant differences in lexicon, syntax, semantics and styles. The discussion in the paper is based on issues arising from the extraction of bilingual legal terms from aligned Chinese-English legal corpus in the implementation of a bilingual a text retrieval system for the Judiciary of the Hong Kong Special Administrative Region (HKSAR) Government. Much attention in computational terminology has been directed to the development of algorithms for extraction from parallel texts. For example, Chinese-English (Wu and Xia 1995), Swedish-English-Polish (Borin 2000), and Chinese-Korean (Huang and Choi 2000). Despite considerable progress, bilingual terminology so generated is often not ready for immediate and practical use. Machine extraction is often the first step of terminology extraction and must be used in conjunction with rigorous and well-managed manual efforts which are critical for the production of consistent and useable multilingual terminology. However, there has been relatively little discussion on the significance of human intervention. The process is far from being straightforward because of the different purposes of alignment, the require"
W02-1802,P00-1050,0,0.0283399,"du.hk the significant differences in lexicon, syntax, semantics and styles. The discussion in the paper is based on issues arising from the extraction of bilingual legal terms from aligned Chinese-English legal corpus in the implementation of a bilingual a text retrieval system for the Judiciary of the Hong Kong Special Administrative Region (HKSAR) Government. Much attention in computational terminology has been directed to the development of algorithms for extraction from parallel texts. For example, Chinese-English (Wu and Xia 1995), Swedish-English-Polish (Borin 2000), and Chinese-Korean (Huang and Choi 2000). Despite considerable progress, bilingual terminology so generated is often not ready for immediate and practical use. Machine extraction is often the first step of terminology extraction and must be used in conjunction with rigorous and well-managed manual efforts which are critical for the production of consistent and useable multilingual terminology. However, there has been relatively little discussion on the significance of human intervention. The process is far from being straightforward because of the different purposes of alignment, the requirements of target users and the corpus type."
W05-1001,P98-1013,0,0.0218422,"parse information for indicating constituent boundaries in semantic role labelling. In Section 2, related work will be reviewed. In Section 3, the data used in the current study will be introduced. Our proposed method will be explained in Section 4, and the experiment reported in Section 5. Results and future work will be discussed in Section 6, followed by conclusions in Section 7. 2 Related Work The definition of semantic roles falls on a continuum from abstract ones to very specific ones. Gildea and Jurafsky (2002), for instance, used a set of roles defined according to the FrameNet model (Baker et al., 1998), thus corresponding to the frame elements in individual frames under a particular domain to which a given verb belongs. Lexical entries (in fact not limited to verbs, in the case of FrameNet) falling under the same frame will share the same set of roles. Gildea and Palmer (2002) defined roles with respect to individual predicates in the PropBank, without explicit naming. To date PropBank and FrameNet are the two main resources in English for training semantic role labelling systems, as in the CoNLL-2004 shared task (Carreras and Màrquez, 2004) and SENSEVAL-3 (Litkowski, 2004). The theoretical"
W05-1001,W04-2412,0,0.0540291,"d a set of roles defined according to the FrameNet model (Baker et al., 1998), thus corresponding to the frame elements in individual frames under a particular domain to which a given verb belongs. Lexical entries (in fact not limited to verbs, in the case of FrameNet) falling under the same frame will share the same set of roles. Gildea and Palmer (2002) defined roles with respect to individual predicates in the PropBank, without explicit naming. To date PropBank and FrameNet are the two main resources in English for training semantic role labelling systems, as in the CoNLL-2004 shared task (Carreras and Màrquez, 2004) and SENSEVAL-3 (Litkowski, 2004). The theoretical treatment of semantic roles is also varied in Chinese. In practice, for example, the semantic roles in the Sinica Treebank mark not only verbal arguments but also modifier-head relations (You and Chen, 2004). In our present study, we go for a set of more abstract semantic roles similar to the thematic roles for English used in VerbNet (Kipper et al., 2002). These roles are generalisable to most Chinese verbs and are not 2 dependent on particular predicates. They will be further introduced in Section 3. Approaches in automatic semantic role lab"
W05-1001,J02-3001,0,0.858357,"rrectly is essential to many applications such as information extraction and machine translation; and making available resources with this kind of information would in turn facilitate the development of such applications. Large-scale production of annotated resources is often labour intensive, and thus calls for automatic labelling to streamline the process. The task is essentially done in two phases, namely recognising the constituents bearing some semantic relationship to the target verb in a sentence, and then labelling them with the corresponding semantic roles. In their seminal proposal, Gildea and Jurafsky (2002) approached the task using various features such as headword, phrase type, and parse tree path. While such features have remained the basic and essential features in subsequent research, parsed sentences are nevertheless required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling (Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003). More precisely, parse information is rather more critical for the identification of boundaries of candidate constituents than for"
W05-1001,P02-1031,0,0.399679,"bearing some semantic relationship to the target verb in a sentence, and then labelling them with the corresponding semantic roles. In their seminal proposal, Gildea and Jurafsky (2002) approached the task using various features such as headword, phrase type, and parse tree path. While such features have remained the basic and essential features in subsequent research, parsed sentences are nevertheless required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling (Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003). More precisely, parse information is rather more critical for the identification of boundaries of candidate constituents than for the extraction of training data. Its limited function in training, for instance, is reflected in the low coverage reported (e.g. You and Chen, 2004). As full parses are not always accessible, many thus resort to shallow syntactic information from simple chunking, even though results often turn out to be less satisfactory than with full parses. This limitation is even more pertinent for the application of semantic role labelling to la"
W05-1001,W03-1008,0,0.0247883,"ationship to the target verb in a sentence, and then labelling them with the corresponding semantic roles. In their seminal proposal, Gildea and Jurafsky (2002) approached the task using various features such as headword, phrase type, and parse tree path. While such features have remained the basic and essential features in subsequent research, parsed sentences are nevertheless required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling (Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003). More precisely, parse information is rather more critical for the identification of boundaries of candidate constituents than for the extraction of training data. Its limited function in training, for instance, is reflected in the low coverage reported (e.g. You and Chen, 2004). As full parses are not always accessible, many thus resort to shallow syntactic information from simple chunking, even though results often turn out to be less satisfactory than with full parses. This limitation is even more pertinent for the application of semantic role labelling to languages which do not have sophi"
W05-1001,kingsbury-palmer-2002-treebank,0,0.0212537,"he training and testing data is important especially in view of the characteristic syntaxsemantics interface in Chinese. We also plan to explore some class-based techniques for the task with reference to existing semantic lexicons, and to modify the method and augment the feature set with more linguistic input. 1 Introduction As the development of language resources progresses from POS-tagged corpora to syntactically annotated treebanks, the inclusion of semantic information such as predicate-argument relations becomes indispensable. The expansion of the Penn Treebank into a Proposition Bank (Kingsbury and Palmer, 2002) is a typical move in this direction. Lexical resources also need to be enhanced with semantic information (e.g. Fellbaum et al., 2001). The ability to identify semantic role relations correctly is essential to many applications such as information extraction and machine translation; and making available resources with this kind of information would in turn facilitate the development of such applications. Large-scale production of annotated resources is often labour intensive, and thus calls for automatic labelling to streamline the process. The task is essentially done in two phases, namely r"
W05-1001,E03-1081,1,0.812695,"sets of primary school Chinese textbooks popularly used in Hong Kong were taken for reference. The two publishers were Keys Press and Modern Education Research Society Ltd. Texts for Primary One to Six were digitised, segmented into words, and annotated with parts-of-speech (POS). This results in a text collection of about 165K character tokens and upon segmentation about 109K word tokens (about 15K word types). There were about 2,500 transitive verb types, with frequency ranging from 1 to 926. The complex examples were taken from a subset of the LIVAC synchronous corpus1 (Tsou et al., 2000; Kwong and Tsou, 2003). The subcorpus consists of newspaper texts from Hong Kong, including local news, international news, financial news, sports news, and entertainment news, collected in 1997-98. The texts were segmented into words and POS-tagged, resulting in about 1.8M character tokens and upon segmentation about 1M word tokens (about 47K word types). There were about 7,400 transitive verb types, with frequency ranging from 1 to just over 6,300. 1 http://www.livac.org 3 3.2 Training and Testing Data For the current study, a set of 41 transitive verbs common to the two corpora (hereafter referred to as textbook"
W05-1001,W04-0803,0,0.0893424,"ameNet model (Baker et al., 1998), thus corresponding to the frame elements in individual frames under a particular domain to which a given verb belongs. Lexical entries (in fact not limited to verbs, in the case of FrameNet) falling under the same frame will share the same set of roles. Gildea and Palmer (2002) defined roles with respect to individual predicates in the PropBank, without explicit naming. To date PropBank and FrameNet are the two main resources in English for training semantic role labelling systems, as in the CoNLL-2004 shared task (Carreras and Màrquez, 2004) and SENSEVAL-3 (Litkowski, 2004). The theoretical treatment of semantic roles is also varied in Chinese. In practice, for example, the semantic roles in the Sinica Treebank mark not only verbal arguments but also modifier-head relations (You and Chen, 2004). In our present study, we go for a set of more abstract semantic roles similar to the thematic roles for English used in VerbNet (Kipper et al., 2002). These roles are generalisable to most Chinese verbs and are not 2 dependent on particular predicates. They will be further introduced in Section 3. Approaches in automatic semantic role labelling are mostly statistical, ty"
W05-1001,N04-1032,0,0.185424,"Missing"
W05-1001,W04-3212,0,0.0922808,"ting, which we expect to improve the overall performance. Another area of interest is to look at the behaviour of near-synonymous predicates in the tagging process. Many predicates may be unseen in the training data, but while the probability estimation could be generalized from near-synonyms as suggested by a semantic lexicon, whether the similarity and subtle differences between near-synonyms with respect to the argument structure and the corresponding syntactic realisation could be distinguished would also be worth studying. Related to this is the possibility of augmenting the feature set. Xue and Palmer (2004), for instance, looked into new features such as syntactic frame, lexicalized constituent type, etc., and found that enriching the feature set improved the labelling performance. In particular, given the importance of data homogeneity as observed from the experimental results, and the challenges posed by the characteristic nature of Chinese, we intend to improve our method and feature set with more linguistic consideration. 7 Conclusion The study reported in this paper has thus tackled semantic role labelling in Chinese in the absence of parse information, by attempting to locate the correspon"
W05-1001,W04-1116,0,0.0561841,"the basic and essential features in subsequent research, parsed sentences are nevertheless required, for extracting the path features during training and providing the argument boundaries during testing. The parse information is deemed important for the performance of role labelling (Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003). More precisely, parse information is rather more critical for the identification of boundaries of candidate constituents than for the extraction of training data. Its limited function in training, for instance, is reflected in the low coverage reported (e.g. You and Chen, 2004). As full parses are not always accessible, many thus resort to shallow syntactic information from simple chunking, even though results often turn out to be less satisfactory than with full parses. This limitation is even more pertinent for the application of semantic role labelling to languages which do not have sophisticated parsing resources. In the case of Chinese, for example, there is con1 Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 1–9, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics siderable variability in its syntax-semantics inter"
W05-1001,W04-3213,0,\N,Missing
W05-1001,C98-1013,0,\N,Missing
W06-0102,xia-etal-2000-developing,0,0.0450783,"t al., 1989; Riloff and Shepherd, 1999; Lin et al, 2003). Compared to the development of thesauri and lexical databases, and research into semantic networks for major languages such as English, similar work for the Chinese language is less mature. This gap was partly due to the lack of authoritative Chinese corpora as a basis for analysis, but has been gradually reduced with the recent availability of large Chinese corpora including the LIVAC synchronous corpus (Tsou and Lai, 2003) used in this work and further described below, the Sinica Corpus (Chen et al., 1996), the Chinese Penn Treebank (Xia et al., 2000), and the like. An important issue which is seldom addressed in the construction of Chinese lexical databases is the problem of versatility and portability. For a language such as Chinese which is spoken in many different communities, different linguistic norms have emerged as a result of the individualistic evolution and development of the language within a particular community and culture. Such variations are seldom adequately reflected in existing lexical resources, which often only draw reference from one particular source. For instance, Tongyici Cilin (同義詞詞林) (Mei et al., 1984) is a thesa"
W06-0102,C82-2013,0,\N,Missing
W06-0102,P99-1016,0,\N,Missing
W06-0102,huang-etal-2004-sinica,0,\N,Missing
W06-0102,Y96-1018,0,\N,Missing
W09-3516,P04-1021,0,0.100074,"tai4-er31. Although direct 1 2 Related Work There are basically two categories of work on machine transliteration. First, various alignment models are used for acquiring transliteration The transcriptions in this paper are in Hanyu Pinyin. 76 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 76–79, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP lexicons from parallel corpora and other resources (e.g. Kuo and Li, 2008). Second, statistical models are built for transliteration. These models could be phoneme-based (e.g. Knight and Graehl, 1998), grapheme-based (e.g. Li et al., 2004), hybrid (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006) and semantic (e.g. Li et al., 2007) features. The core of our systems is based on Li et al.’s (2004) Joint Source-Channel Model under the direct orthographic mapping framework, which skips the middle phonemic representation in conventional phoneme-based methods and models the segmentation and alignment preferences by means of contextual n-grams of the transliteration segment pairs (or token pairs in their terminology). A bigram model under their framework is thus as follows: K Score( S ) ≈ ∏ P ( sk |lc( sk − 1)) P ( sk"
W09-3516,P07-1016,0,0.0125608,"ion. First, various alignment models are used for acquiring transliteration The transcriptions in this paper are in Hanyu Pinyin. 76 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 76–79, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP lexicons from parallel corpora and other resources (e.g. Kuo and Li, 2008). Second, statistical models are built for transliteration. These models could be phoneme-based (e.g. Knight and Graehl, 1998), grapheme-based (e.g. Li et al., 2004), hybrid (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006) and semantic (e.g. Li et al., 2007) features. The core of our systems is based on Li et al.’s (2004) Joint Source-Channel Model under the direct orthographic mapping framework, which skips the middle phonemic representation in conventional phoneme-based methods and models the segmentation and alignment preferences by means of contextual n-grams of the transliteration segment pairs (or token pairs in their terminology). A bigram model under their framework is thus as follows: K Score( S ) ≈ ∏ P ( sk |lc( sk − 1)) P ( sk |fc ( sk + 1)) k =1 where S is a segmentation sequence with K segments, sk is the kth segment in S, lc(sk-1) i"
W09-3516,I05-1040,0,0.0143569,"1 2 Related Work There are basically two categories of work on machine transliteration. First, various alignment models are used for acquiring transliteration The transcriptions in this paper are in Hanyu Pinyin. 76 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 76–79, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP lexicons from parallel corpora and other resources (e.g. Kuo and Li, 2008). Second, statistical models are built for transliteration. These models could be phoneme-based (e.g. Knight and Graehl, 1998), grapheme-based (e.g. Li et al., 2004), hybrid (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006) and semantic (e.g. Li et al., 2007) features. The core of our systems is based on Li et al.’s (2004) Joint Source-Channel Model under the direct orthographic mapping framework, which skips the middle phonemic representation in conventional phoneme-based methods and models the segmentation and alignment preferences by means of contextual n-grams of the transliteration segment pairs (or token pairs in their terminology). A bigram model under their framework is thus as follows: K Score( S ) ≈ ∏ P ( sk |lc( sk − 1)) P ( sk |fc ( sk + 1)) k =1 where S"
W09-3516,W06-1630,0,0.0219518,"ories of work on machine transliteration. First, various alignment models are used for acquiring transliteration The transcriptions in this paper are in Hanyu Pinyin. 76 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 76–79, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP lexicons from parallel corpora and other resources (e.g. Kuo and Li, 2008). Second, statistical models are built for transliteration. These models could be phoneme-based (e.g. Knight and Graehl, 1998), grapheme-based (e.g. Li et al., 2004), hybrid (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006) and semantic (e.g. Li et al., 2007) features. The core of our systems is based on Li et al.’s (2004) Joint Source-Channel Model under the direct orthographic mapping framework, which skips the middle phonemic representation in conventional phoneme-based methods and models the segmentation and alignment preferences by means of contextual n-grams of the transliteration segment pairs (or token pairs in their terminology). A bigram model under their framework is thus as follows: K Score( S ) ≈ ∏ P ( sk |lc( sk − 1)) P ( sk |fc ( sk + 1)) k =1 where S is a segmentation sequence with K segments, sk"
W09-3516,W03-1508,0,0.359616,"Missing"
W09-3516,W09-3502,0,\N,Missing
W09-3516,I08-4003,0,\N,Missing
W09-3516,J98-4003,0,\N,Missing
W09-3537,P04-1021,0,0.742784,"ce when foreign names are transliterated into Chinese. Underlying the large sample space, however, is not entirely a random distribution. On the one hand, there are no more than a few hundred Chinese characters which are used in names (e.g. Sproat et al., 1996). On the other hand, beyond linguistic and phonetic properties, many other social and cognitive factors such as dialect, gender, domain, meaning, and perception, are simultaneously influencing the naming process and superimposing on the surface graphemic correspondence. As the state-of-the-art approach, direct orthographic mapping (e.g. Li et al., 2004), making use of graphemic correspondence between English and Chinese directly, has been shown to outperform phoneme-based methods (e.g. Virga and Khudanpur, 2003). In fact, transliteration of foreign names into Chinese is often based on the surface orthographic forms, as exemplified in the transliteration of Beckham, where the supposedly silent h in “ham” is taken as pronounced, resulting in 汉姆 han4-mu3 in Mandarin Chinese and 咸 haam4 in Cantonese1. However, as we have observed, there is considerable graphemic ambiguity in E2C, where an English segment might correspond to different Chinese seg"
W09-3537,P07-1016,0,0.0597951,"t models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Lee et al., 2006; Jin et al., 2008; Kuo and Li, 2008). On the other hand, statistical transliteration models are built for transliterating personal names and other proper names, such as by means of noisy channel models or direct models amongst others, phonemebased (e.g. Knight and Graehl, 1998; Virga and Khudanpur, 2003), or grapheme-based (e.g. Li et al., 2004), or a combination of them (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006; Yoon et al., 2007) and semantic (e.g. Li et al., 2007) features. Li et al. (2004), for instance, used a Joint Source-Channel Model under the direct orthographic mapping (DOM) framework, skipping the middle phonemic representation in conventional phoneme-based methods, and modelling the segmentation and alignment preferences by means of contextual n-grams of the transliteration units. Their method was shown to outperform phoneme-based methods and those based on the noisy channel model. The n-gram model used in Li et al. (2004) was based on previous local context of grapheme pairs. However, as we are going to show in Section 3, contexts on both sid"
W09-3537,I05-1040,0,0.147934,"here are basically two categories of work on machine transliteration. On the one hand, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Lee et al., 2006; Jin et al., 2008; Kuo and Li, 2008). On the other hand, statistical transliteration models are built for transliterating personal names and other proper names, such as by means of noisy channel models or direct models amongst others, phonemebased (e.g. Knight and Graehl, 1998; Virga and Khudanpur, 2003), or grapheme-based (e.g. Li et al., 2004), or a combination of them (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006; Yoon et al., 2007) and semantic (e.g. Li et al., 2007) features. Li et al. (2004), for instance, used a Joint Source-Channel Model under the direct orthographic mapping (DOM) framework, skipping the middle phonemic representation in conventional phoneme-based methods, and modelling the segmentation and alignment preferences by means of contextual n-grams of the transliteration units. Their method was shown to outperform phoneme-based methods and those based on the noisy channel model. The n-gram model used in Li et al. (2004) was based on previous"
W09-3537,J96-3004,0,0.0544612,"(referred to as E2C hereafter) of personal names. Unlike many other languages, Chinese names are characteristic in their relatively free choice and combination of characters, particularly for given names. Such apparent flexibility does not only account for the virtually infinite number of authentic Chinese names, but also leads to a considerable sample space when foreign names are transliterated into Chinese. Underlying the large sample space, however, is not entirely a random distribution. On the one hand, there are no more than a few hundred Chinese characters which are used in names (e.g. Sproat et al., 1996). On the other hand, beyond linguistic and phonetic properties, many other social and cognitive factors such as dialect, gender, domain, meaning, and perception, are simultaneously influencing the naming process and superimposing on the surface graphemic correspondence. As the state-of-the-art approach, direct orthographic mapping (e.g. Li et al., 2004), making use of graphemic correspondence between English and Chinese directly, has been shown to outperform phoneme-based methods (e.g. Virga and Khudanpur, 2003). In fact, transliteration of foreign names into Chinese is often based on the surf"
W09-3537,W06-1630,0,0.141864,"chine transliteration. On the one hand, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Lee et al., 2006; Jin et al., 2008; Kuo and Li, 2008). On the other hand, statistical transliteration models are built for transliterating personal names and other proper names, such as by means of noisy channel models or direct models amongst others, phonemebased (e.g. Knight and Graehl, 1998; Virga and Khudanpur, 2003), or grapheme-based (e.g. Li et al., 2004), or a combination of them (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006; Yoon et al., 2007) and semantic (e.g. Li et al., 2007) features. Li et al. (2004), for instance, used a Joint Source-Channel Model under the direct orthographic mapping (DOM) framework, skipping the middle phonemic representation in conventional phoneme-based methods, and modelling the segmentation and alignment preferences by means of contextual n-grams of the transliteration units. Their method was shown to outperform phoneme-based methods and those based on the noisy channel model. The n-gram model used in Li et al. (2004) was based on previous local context of grapheme pairs. However, as"
W09-3537,W03-1508,0,0.278684,"d, there are no more than a few hundred Chinese characters which are used in names (e.g. Sproat et al., 1996). On the other hand, beyond linguistic and phonetic properties, many other social and cognitive factors such as dialect, gender, domain, meaning, and perception, are simultaneously influencing the naming process and superimposing on the surface graphemic correspondence. As the state-of-the-art approach, direct orthographic mapping (e.g. Li et al., 2004), making use of graphemic correspondence between English and Chinese directly, has been shown to outperform phoneme-based methods (e.g. Virga and Khudanpur, 2003). In fact, transliteration of foreign names into Chinese is often based on the surface orthographic forms, as exemplified in the transliteration of Beckham, where the supposedly silent h in “ham” is taken as pronounced, resulting in 汉姆 han4-mu3 in Mandarin Chinese and 咸 haam4 in Cantonese1. However, as we have observed, there is considerable graphemic ambiguity in E2C, where an English segment might correspond to different Chinese segments. Such multiple mappings, to a large extent, is associated with the phonological context embedding the English segment, thus affecting its expected pronuncia"
W09-3537,I08-4002,0,0.0262955,"pond to exactly a syllable, although it often does. In Section 2, we will briefly review some related work. In Section 3, we will discuss some observations on graphemic ambiguity in E2C. The proposed method will be presented in Section 4. Experiments will be reported in Section 5, with results discussed in Section 6, followed by a conclusion in Section 7. 2 Related Work There are basically two categories of work on machine transliteration. On the one hand, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Lee et al., 2006; Jin et al., 2008; Kuo and Li, 2008). On the other hand, statistical transliteration models are built for transliterating personal names and other proper names, such as by means of noisy channel models or direct models amongst others, phonemebased (e.g. Knight and Graehl, 1998; Virga and Khudanpur, 2003), or grapheme-based (e.g. Li et al., 2004), or a combination of them (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006; Yoon et al., 2007) and semantic (e.g. Li et al., 2007) features. Li et al. (2004), for instance, used a Joint Source-Channel Model under the direct orthographic mapping (DOM) fr"
W09-3537,P07-1015,0,0.0665577,"ion. On the one hand, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Lee et al., 2006; Jin et al., 2008; Kuo and Li, 2008). On the other hand, statistical transliteration models are built for transliterating personal names and other proper names, such as by means of noisy channel models or direct models amongst others, phonemebased (e.g. Knight and Graehl, 1998; Virga and Khudanpur, 2003), or grapheme-based (e.g. Li et al., 2004), or a combination of them (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006; Yoon et al., 2007) and semantic (e.g. Li et al., 2007) features. Li et al. (2004), for instance, used a Joint Source-Channel Model under the direct orthographic mapping (DOM) framework, skipping the middle phonemic representation in conventional phoneme-based methods, and modelling the segmentation and alignment preferences by means of contextual n-grams of the transliteration units. Their method was shown to outperform phoneme-based methods and those based on the noisy channel model. The n-gram model used in Li et al. (2004) was based on previous local context of grapheme pairs. However, as we are going to sho"
W09-3537,I08-4003,0,0.0159146,"syllable, although it often does. In Section 2, we will briefly review some related work. In Section 3, we will discuss some observations on graphemic ambiguity in E2C. The proposed method will be presented in Section 4. Experiments will be reported in Section 5, with results discussed in Section 6, followed by a conclusion in Section 7. 2 Related Work There are basically two categories of work on machine transliteration. On the one hand, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Lee et al., 2006; Jin et al., 2008; Kuo and Li, 2008). On the other hand, statistical transliteration models are built for transliterating personal names and other proper names, such as by means of noisy channel models or direct models amongst others, phonemebased (e.g. Knight and Graehl, 1998; Virga and Khudanpur, 2003), or grapheme-based (e.g. Li et al., 2004), or a combination of them (Oh and Choi, 2005), or based on phonetic (e.g. Tao et al., 2006; Yoon et al., 2007) and semantic (e.g. Li et al., 2007) features. Li et al. (2004), for instance, used a Joint Source-Channel Model under the direct orthographic mapping (DOM) framework, skipping t"
W09-3537,P09-2006,1,\N,Missing
W10-4110,P91-1022,0,0.515199,"Missing"
W10-4110,J93-2003,0,0.0453684,"Missing"
W10-4110,2007.mtsummit-papers.9,0,0.0661439,"Missing"
W10-4110,P93-1002,0,0.315112,"Missing"
W10-4110,J07-2003,0,0.119073,"Missing"
W10-4110,P91-1023,0,0.712647,"Chinese bilingual patents. To our knowledge, this is the largest single parallel corpus in terms of sentence pairs. Moreover, we estimate the potential for mining multilingual parallel corpora involving English, Chinese, Japanese, Korean, German, etc., which would to some extent reduce the parallel data acquisition bottleneck in multilingual information processing. 1 Introduction Multilingual data are critical resources for building many applications, such as machine translation (MT) and cross-lingual information retrieval. Many parallel corpora have been built, such as the Canadian Hansards (Gale and Church, 1991), the Europarl corpus (Koehn, 2005), the Arabic-English and English-Chinese parallel corpora used in the NIST Open MT Evaluation. However, few parallel corpora exist for many language pairs, such as Chinese-Japanese, Japanese-Korean, ChineseFrench or Japanese-German. Even for language pairs with several parallel corpora, such as Chinese-English and Arabic-English, the size of parallel corpora is still a major limitation for SMT systems to achieve higher performance. In this paper, we present a way which could, to some extent, reduce the parallel data acquisition bottleneck in multilingual lang"
W10-4110,2001.mtsummit-papers.30,0,0.151725,"ice and then file its international application also in Chinese under the PCT. Later on, it may have the patent translated into English and file it in USA patent office, which means the patent becomes bilingual. If the applicant continues to file it in Japan with Japanese, it would be trilingual. Even more, it would be quadrilingual or involve more languages when it is filed in other countries with more languages. Such multilingual patents are considered comparable (or noisy parallel) because they are not parallel in the strict sense but still closely related in terms of information conveyed (Higuchi et al., 2001; Lu et al., 2009). 4 A Large English-Chinese Parallel Corpus Mined from Bilingual Patents In this section, we introduce the English-Chinese bilingual patents harvested from the Web and the method to mine parallel sentences from them. SMT experiments on the final parallel corpus are also described. 4.1 Harvesting English-Chinese Bilingual Patents The official patent office in China is the State Intellectual Property Office (SIPO). In early 2009, by searching on its website, we found about 200K Chinese patents previously filed as PCT applications in English and crawled their bibliographical dat"
W10-4110,2005.mtsummit-papers.11,0,0.0292542,"this is the largest single parallel corpus in terms of sentence pairs. Moreover, we estimate the potential for mining multilingual parallel corpora involving English, Chinese, Japanese, Korean, German, etc., which would to some extent reduce the parallel data acquisition bottleneck in multilingual information processing. 1 Introduction Multilingual data are critical resources for building many applications, such as machine translation (MT) and cross-lingual information retrieval. Many parallel corpora have been built, such as the Canadian Hansards (Gale and Church, 1991), the Europarl corpus (Koehn, 2005), the Arabic-English and English-Chinese parallel corpora used in the NIST Open MT Evaluation. However, few parallel corpora exist for many language pairs, such as Chinese-Japanese, Japanese-Korean, ChineseFrench or Japanese-German. Even for language pairs with several parallel corpora, such as Chinese-English and Arabic-English, the size of parallel corpora is still a major limitation for SMT systems to achieve higher performance. In this paper, we present a way which could, to some extent, reduce the parallel data acquisition bottleneck in multilingual language processing. Based on multiling"
W10-4110,P07-2045,0,0.00362542,"Missing"
W10-4110,P08-1113,0,0.0425735,"Missing"
W10-4110,P09-1098,0,0.0330593,"Missing"
W10-4110,2009.mtsummit-wpt.3,1,0.729366,"international application also in Chinese under the PCT. Later on, it may have the patent translated into English and file it in USA patent office, which means the patent becomes bilingual. If the applicant continues to file it in Japan with Japanese, it would be trilingual. Even more, it would be quadrilingual or involve more languages when it is filed in other countries with more languages. Such multilingual patents are considered comparable (or noisy parallel) because they are not parallel in the strict sense but still closely related in terms of information conveyed (Higuchi et al., 2001; Lu et al., 2009). 4 A Large English-Chinese Parallel Corpus Mined from Bilingual Patents In this section, we introduce the English-Chinese bilingual patents harvested from the Web and the method to mine parallel sentences from them. SMT experiments on the final parallel corpus are also described. 4.1 Harvesting English-Chinese Bilingual Patents The official patent office in China is the State Intellectual Property Office (SIPO). In early 2009, by searching on its website, we found about 200K Chinese patents previously filed as PCT applications in English and crawled their bibliographical data, titles, abstrac"
W10-4110,ma-2006-champollion,0,0.291728,"duced in Section 4, followed by the quantity estimation of multilingual patents involving other language pairs in Section 5. We discuss the results in Section 6, and conclude in Section 7. 2 Related Work Parallel sentences could be extracted from parallel documents or comparable corpora. Different approaches have been proposed to align sentences in parallel documents consisting of the same content in different languages based on the following information: a) the sentence length in bilingual sentences (Brown et al. 1991; Gale and Church, 1991); b) lexical information in bilingual dictionaries (Ma, 2006); c) statistical translation model (Chen, 1993), or the composite of more than one approach (Simard and Plamondon, 1998; Moore, 2002). To overcome the lack of parallel documents, comparable corpora are also used to mine parallel sentences, which raises further challenges since the bilingual contents are not strictly parallel. For instance, Zhao and Vogel (2002) investigated the mining of parallel sentences for Web bilingual news. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maxi"
W10-4110,moore-2002-fast,0,0.149931,"scuss the results in Section 6, and conclude in Section 7. 2 Related Work Parallel sentences could be extracted from parallel documents or comparable corpora. Different approaches have been proposed to align sentences in parallel documents consisting of the same content in different languages based on the following information: a) the sentence length in bilingual sentences (Brown et al. 1991; Gale and Church, 1991); b) lexical information in bilingual dictionaries (Ma, 2006); c) statistical translation model (Chen, 1993), or the composite of more than one approach (Simard and Plamondon, 1998; Moore, 2002). To overcome the lack of parallel documents, comparable corpora are also used to mine parallel sentences, which raises further challenges since the bilingual contents are not strictly parallel. For instance, Zhao and Vogel (2002) investigated the mining of parallel sentences for Web bilingual news. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Cao et al., (2007) and Lin et al., (2008) proposed two different methods utilizing the parenthesis pattern to"
W10-4110,J05-4003,0,0.0723704,"Missing"
W10-4110,J04-4002,0,0.170472,"Missing"
W10-4110,2007.mtsummit-papers.63,0,0.11045,"sion The websites from which the Chinese and English patents were downloaded were quite slow to access, and were occasionally down during access. To avoid too much workload for the websites, the downloading speed had been limited. Some large patents would cost much time for the websites to respond and had be specifically handled. It took considerable efforts to obtain these comparable patents. In addition our English-Chinese corpus mined in this study is at least one order of magnitude larger, we give some other differences between ours and those introduced in Section 2 (Higuchi et al., 2001; Utiyama and Isahara, 2007; Lu et al, 2009) 1) Their bilingual patents were identified by the priority information in the US patents, and could not be easily extended to language pairs without English; while our method using PCT applications as the pivot could be easily extended to other language pairs as illustrated in Section 5. 2) The translation process is different: their patents were filed in USA Patent Office in English by translating from Japanese or Chinese, while our patents were first filed in English as a PCT application, and later translated into Chinese. The different translation processes may have differ"
W10-4110,J93-1004,0,\N,Missing
W11-3215,I05-1040,0,0.0253578,"lds which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint Source-Channel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009a). This paper reports on our participation in the NEWS 2011 shared task on transliteration generation with a syllable-based Backward Maximum Matching system. The system uses the Onset First Principle to syllabify English names and align them with Chinese names. The bilingual lexicon containing aligned segments of various syllable lengths subsequently allows direct transliteration by chunks. The"
W11-3215,W09-3507,0,0.0190295,"Based Maximum Matching Oi Yee Kwong Department of Chinese, Translation and Linguistics City University of Hong Kong Tat Chee Avenue, Kowloon, Hong Kong Olivia.Kwong@cityu.edu.hk 2 Abstract The reports of the shared task in NEWS 2009 (Li et al., 2009) and NEWS 2010 (Li et al., 2010) highlighted two particularly popular approaches for transliteration generation among the participating systems. One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint Source-Channel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like te"
W11-3215,W10-2406,0,0.0515876,"Missing"
W11-3215,W10-2409,0,0.040354,"Missing"
W11-3215,P11-2094,0,0.0670309,"n and Linguistics City University of Hong Kong Tat Chee Avenue, Kowloon, Hong Kong Olivia.Kwong@cityu.edu.hk 2 Abstract The reports of the shared task in NEWS 2009 (Li et al., 2009) and NEWS 2010 (Li et al., 2010) highlighted two particularly popular approaches for transliteration generation among the participating systems. One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint Source-Channel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful i"
W11-3215,W06-1630,0,0.0223396,"ayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint Source-Channel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009a). This paper reports on our participation in the NEWS 2011 shared task on transliteration generation with a syllable-based Backward Maximum Matching system. The system uses the Onset First Principle to syllabify English names and align them with Chinese names. The bilingual lexicon containing aligned segments of various syllable lengths subsequently allows direct transliteration by chunks. The official results suggest that our system could potentially be improved with a re-ranking module for English-to-Chinese transliteration, while its performance on Chinese-to"
W11-3215,W10-2410,0,0.0257557,"0; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint Source-Channel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009a). This paper reports on our participation in the NEWS 2011 shared task on transliteration generation with a syllable-based Backward Maximum Matching system. The system uses the Onset First Principle to syllabify English names and align them with Chinese names. The bilingual lexicon containing aligned segments of various syllable lengths subseque"
W11-3215,P09-2006,1,0.820147,"Missing"
W11-3215,W09-3537,1,0.853145,"Missing"
W11-3215,P04-1021,0,0.203129,"10) highlighted two particularly popular approaches for transliteration generation among the participating systems. One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint Source-Channel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009a). This paper reports on our participation in the NEWS 2011 shared task on transliteration generation with a syllable-based Backward Max"
W11-3215,P07-1016,0,0.0201114,"pproach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint Source-Channel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009a). This paper reports on our participation in the NEWS 2011 shared task on transliteration generation with a syllable-based Backward Maximum Matching system. The system uses the Onset First Principle to syllabify English names and align them with Chinese names. The bilingual lexicon containing aligned segments of various syllable lengths subsequently allows direct transliteration by chunks. The official results suggest that our system could potentially be improved with a re-ranking module for English-to-Chinese transliteration, while its performance on Chinese-to-English back tra"
W11-3215,W09-3501,0,\N,Missing
W11-3215,W10-2401,0,\N,Missing
W11-3215,J98-4003,0,\N,Missing
W15-3905,W10-2406,0,0.0225288,"panese form 34 Proceedings of the Fifth Named Entity Workshop, joint with 53rd ACL and the 7th IJCNLP, pages 34–42, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics as pronounced, resulting in 漢姆 (Hanyu Pinyin: han4-mu3) in Mandarin Chinese and 咸 (Jyutping: haam4) in Cantonese. The reports of the shared task in NEWS 2009 (Li et al., 2009) and NEWS 2010 (Li et al., 2010) highlighted two particularly popular approaches for transliteration generation among the participating systems. One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). More recent shared tasks have shown a wider array of promising techniques (Zhang et al., 2011; Zhang et al., 2012), although the absolute results as measured by Word Accuracy in Top-1 (ACC), Fuzziness in Top-1 (Mean Fscore), and Mean Reciprocal Rank (MRR) have not really demonstrated any remarkable boost. whole range of possibilities which may be more appreciated by actual translation tasks. We therefore propose deeper error analysis in transliteration evaluation, and an"
W15-3905,W10-2409,0,0.0235704,"-ri-n-to-n). The Japanese form 34 Proceedings of the Fifth Named Entity Workshop, joint with 53rd ACL and the 7th IJCNLP, pages 34–42, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics as pronounced, resulting in 漢姆 (Hanyu Pinyin: han4-mu3) in Mandarin Chinese and 咸 (Jyutping: haam4) in Cantonese. The reports of the shared task in NEWS 2009 (Li et al., 2009) and NEWS 2010 (Li et al., 2010) highlighted two particularly popular approaches for transliteration generation among the participating systems. One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). More recent shared tasks have shown a wider array of promising techniques (Zhang et al., 2011; Zhang et al., 2012), although the absolute results as measured by Word Accuracy in Top-1 (ACC), Fuzziness in Top-1 (Mean Fscore), and Mean Reciprocal Rank (MRR) have not really demonstrated any remarkable boost. whole range of possibilities which may be more appreciated by actual translation tasks. We therefore propose deeper error analysis in translite"
W15-3905,W06-1630,0,0.0246269,"other hand, statistical transliteration models are built for transliterating personal names and other proper names, and these models can be based on phonemes (e.g. Knight and Graehl, 1998; Virga and Khudanpur, 2003), graphemes (e.g. Li et al., 2004), or their combination (e.g. Oh and Choi, 2005). They may operate on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005). In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Yoon et al., 2007; Kwong, 2009). The baseline in current English-Chinese transliteration generation research often refers to Li et al. (2004). They used a Joint SourceChannel Model under the direct orthographic mapping (DOM) framework, which skips the middle phonemic representation in conventional phoneme-based methods, and models the segmentation and alignment preferences by means of contextual n-grams of the transliteration units. Their method was shown to outperform phoneme-based methods and those based on the noisy channel model. In fact, transliteration of foreign names"
W15-3905,W03-1508,0,0.0270487,"of name transliteration for computer-aided translation are proposed, followed by a conclusion with future work in Section 7. 2 Related Work There are basically two categories of work on machine transliteration. On the one hand, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Lee et al., 2006; Jin et al., 2008; Kuo and Li, 2008). On the other hand, statistical transliteration models are built for transliterating personal names and other proper names, and these models can be based on phonemes (e.g. Knight and Graehl, 1998; Virga and Khudanpur, 2003), graphemes (e.g. Li et al., 2004), or their combination (e.g. Oh and Choi, 2005). They may operate on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005). In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Yoon et al., 2007; Kwong, 2009). The baseline in current English-Chinese transliteration generation research often refers to Li et al. (2004). They used a Joint SourceChannel Model"
W15-3905,W10-2410,0,0.0162935,"machine transliteration. On the one hand, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Lee et al., 2006; Jin et al., 2008; Kuo and Li, 2008). On the other hand, statistical transliteration models are built for transliterating personal names and other proper names, and these models can be based on phonemes (e.g. Knight and Graehl, 1998; Virga and Khudanpur, 2003), graphemes (e.g. Li et al., 2004), or their combination (e.g. Oh and Choi, 2005). They may operate on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005). In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Yoon et al., 2007; Kwong, 2009). The baseline in current English-Chinese transliteration generation research often refers to Li et al. (2004). They used a Joint SourceChannel Model under the direct orthographic mapping (DOM) framework, which skips the middle phonemic representation in conventional phoneme-based methods, and models the segmentation and alignment preferences"
W15-3905,P09-2006,1,0.734587,"ilt for transliterating personal names and other proper names, and these models can be based on phonemes (e.g. Knight and Graehl, 1998; Virga and Khudanpur, 2003), graphemes (e.g. Li et al., 2004), or their combination (e.g. Oh and Choi, 2005). They may operate on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005). In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Yoon et al., 2007; Kwong, 2009). The baseline in current English-Chinese transliteration generation research often refers to Li et al. (2004). They used a Joint SourceChannel Model under the direct orthographic mapping (DOM) framework, which skips the middle phonemic representation in conventional phoneme-based methods, and models the segmentation and alignment preferences by means of contextual n-grams of the transliteration units. Their method was shown to outperform phoneme-based methods and those based on the noisy channel model. In fact, transliteration of foreign names into Chinese is often based on the surface orthog"
W15-3905,P07-1015,0,0.0417574,"Missing"
W15-3905,P07-1016,0,0.065627,"Missing"
W15-3905,P04-1021,0,0.0375765,"ranslation are proposed, followed by a conclusion with future work in Section 7. 2 Related Work There are basically two categories of work on machine transliteration. On the one hand, various alignment models are used for acquiring transliteration lexicons from parallel corpora and other resources (e.g. Lee et al., 2006; Jin et al., 2008; Kuo and Li, 2008). On the other hand, statistical transliteration models are built for transliterating personal names and other proper names, and these models can be based on phonemes (e.g. Knight and Graehl, 1998; Virga and Khudanpur, 2003), graphemes (e.g. Li et al., 2004), or their combination (e.g. Oh and Choi, 2005). They may operate on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005). In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Yoon et al., 2007; Kwong, 2009). The baseline in current English-Chinese transliteration generation research often refers to Li et al. (2004). They used a Joint SourceChannel Model under the direct orthographic mapp"
W15-3905,W09-3507,0,0.0226314,"syllables is included. Transliterations in Hong Kong, however, are much more variable, and there are many ways to render a particular syllable. No. 1 2 3 4 5 6 7 8 9 10 Hong Kong S 斯 SON 遜 S 史 L 爾 TON 頓 G 格 O 奧 A 亞 A 艾 BA 巴 Mainland China S 斯 L 爾 D 德 T 特 C 克 SON 森 RI 里 B 布 G 格 K 克 particular, has more tones than Mandarin, and the sound-tone combination is more important in names pronounced in Cantonese. Names which sound “nice” (or more “musical”) are often preferred to those which sound “monotonous”. It is thus important to consider the tone combination in transliteration. To this end, Kwong (2009) has shown that the improvement from including tones in a Joint Source-Channel model for automatic transliteration was more apparent for Cantonese data. Taiwan Region S 斯 D 德 T 特 K 克 B 布 SON 森 C 克 S 史 RO 羅 TON 頓 5.4 Gender difference is often reflected in the character choice for the transliterated names. Table 4 shows the most frequent characters for transliterating male and female given names in Mainland China and Taiwan region as analysed from Dataset N2b. No. Mainland China Taiwan Region Male Female Male Female 斯 娜 斯 莉 1 爾 麗 爾 娜 2 里 莉 克 拉 3 特 拉 瑞 絲 4 德 爾 德 妮 5 克 特 特 瑪 6 利 絲 艾 西 7 尼 妮 尼 琳 8"
W15-3905,I05-1040,0,\N,Missing
W15-3905,W09-3501,0,\N,Missing
W15-3905,W10-2401,0,\N,Missing
W15-3905,I08-4003,0,\N,Missing
W15-3905,I08-4002,0,\N,Missing
W15-3905,W12-4402,0,\N,Missing
W15-3905,J98-4003,0,\N,Missing
W18-3719,Y16-2023,1,0.850149,"Missing"
W18-3719,biemann-etal-2004-automatic,0,0.0374537,"ucturing during language learning when crosslinguistic differences are encountered. Such cognitive aspects may not have been sufficiently modelled in static bilingual linguistic lexicons, especially between two very different languages like English and Chinese. In the following we will compare the word associations obtained from various resources, and evaluate them against the information need in our earlier example situated in the translation context. While there are various ways to model different associative relations from large corpora (e.g. Church and Hanks, 1990; Wettler and Rapp, 1993; Biemann et al., 2004; Kilgarriff et al., 2004; Hill et al., 2015), certain knots remain to be untied for them to be better utilised in language applications. First, corpus-based modelling of associations often focuses on specific relations (e.g. similarity, hierarchical relations, collocations, etc.), but in real-life lexical access, a combination of relations is often retrieved, as shown in human word association norms (e.g. Moss and Older, 1996). Moreover, some associations are bound to be more relevant than others in a given context, and they are readily activated regardless of their normal associative strengt"
W18-3719,J90-1003,0,0.32104,"ingual mental lexicon undergoes conceptual restructuring during language learning when crosslinguistic differences are encountered. Such cognitive aspects may not have been sufficiently modelled in static bilingual linguistic lexicons, especially between two very different languages like English and Chinese. In the following we will compare the word associations obtained from various resources, and evaluate them against the information need in our earlier example situated in the translation context. While there are various ways to model different associative relations from large corpora (e.g. Church and Hanks, 1990; Wettler and Rapp, 1993; Biemann et al., 2004; Kilgarriff et al., 2004; Hill et al., 2015), certain knots remain to be untied for them to be better utilised in language applications. First, corpus-based modelling of associations often focuses on specific relations (e.g. similarity, hierarchical relations, collocations, etc.), but in real-life lexical access, a combination of relations is often retrieved, as shown in human word association norms (e.g. Moss and Older, 1996). Moreover, some associations are bound to be more relevant than others in a given context, and they are readily activated"
W18-3719,P07-2011,0,0.0170854,"Missing"
W18-3719,J15-4004,0,0.0195673,"guistic differences are encountered. Such cognitive aspects may not have been sufficiently modelled in static bilingual linguistic lexicons, especially between two very different languages like English and Chinese. In the following we will compare the word associations obtained from various resources, and evaluate them against the information need in our earlier example situated in the translation context. While there are various ways to model different associative relations from large corpora (e.g. Church and Hanks, 1990; Wettler and Rapp, 1993; Biemann et al., 2004; Kilgarriff et al., 2004; Hill et al., 2015), certain knots remain to be untied for them to be better utilised in language applications. First, corpus-based modelling of associations often focuses on specific relations (e.g. similarity, hierarchical relations, collocations, etc.), but in real-life lexical access, a combination of relations is often retrieved, as shown in human word association norms (e.g. Moss and Older, 1996). Moreover, some associations are bound to be more relevant than others in a given context, and they are readily activated regardless of their normal associative strengths. Second, for tasks requiring bilingual lex"
W18-3719,W93-0310,0,0.486743,"dergoes conceptual restructuring during language learning when crosslinguistic differences are encountered. Such cognitive aspects may not have been sufficiently modelled in static bilingual linguistic lexicons, especially between two very different languages like English and Chinese. In the following we will compare the word associations obtained from various resources, and evaluate them against the information need in our earlier example situated in the translation context. While there are various ways to model different associative relations from large corpora (e.g. Church and Hanks, 1990; Wettler and Rapp, 1993; Biemann et al., 2004; Kilgarriff et al., 2004; Hill et al., 2015), certain knots remain to be untied for them to be better utilised in language applications. First, corpus-based modelling of associations often focuses on specific relations (e.g. similarity, hierarchical relations, collocations, etc.), but in real-life lexical access, a combination of relations is often retrieved, as shown in human word association norms (e.g. Moss and Older, 1996). Moreover, some associations are bound to be more relevant than others in a given context, and they are readily activated regardless of their norm"
W98-0710,P81-1030,0,0.0148865,"as thesauri. However, itdoes not often sufficeto depend on any single resource, either because it does not contain all required information or the information is not organised in a way suitable for the purpose. Merging different resources is therefore necessary. Calzolaxi's (1988) Italian lexical database, Knight and Luk's (1994) P A N G L O S S ontology, and Klavans and Tzoukermann's (1995) bilingual lexicon axe some responses to this need. Many attempts have also been made to transform the implicit information in dictionary definitions to explicit knowledge bases for computational purposes (Amsler, 1981; Calzolaxi, 1984; Chodorow et al., 1985; Maxkowitz et al., 1986; Klavans et al., 1990; Vossen and Copestake, 1993). Nonetheless, dictionaries axe also infamous for their nonstandaxdised sense granularity, and the taxonomies obtained from definitions axe inevitably ad hoc. It would therefore be a good idea if we could integrate 73 such information from dictionaries with some existing, and widely exploited, classifications such as the system in Roget's Thesaurus (Roget, 1852), which has remained intact for years. We can see at least the following ways in which an integration of lexical resource"
W98-0710,P84-1036,0,0.155085,"Missing"
W98-0710,P86-1018,0,0.0934413,"Missing"
W98-0710,C92-2070,0,0.0176983,"CE definition. That means we believe D~: ~ Gl(X) and Dzn(XUCUA) ~ ¢. We did not include coordinate terms (called &quot;siblings&quot; in Knight and Luk (1994)) because we found that while nouns in WN usually have many coordinate terms, the chance of hitting them in LDOCE definitions is hardly high enough to worth the computation effort. 2.2 T h e A l g o r i t h m Our algorithm defines a mapping chain from LDOCE to ROGET through WN. It is based on shallow processing within the resources themselves, exploiting their inter-relatedness, and does not rely on extensive statistical data (e.g. as suggested in Yarowsky (1992)). Given a word with part of speech, W(p), the core steps are as follows: S t e p 1: From LDOCE, get the sense definitions Dr, ..., Dt under the entry W(p). S t e p 2: From WN, find all the synsets Also collect the corresponding gloss definitions, Gl(Sn), if any, the hypernym synsets Hyp(S,~), and the coordinate synsets Co(S,~). Sn{wt,w2,...} such that W(p) E Sn. S t e p 3: Compute a similarity score matrix ,4 for the LDOCE senses and the WN synsets. A similarity score A(i, j) is computed for the i th LDOCE sense and the jta WN synset using a weighted sum of the overlaps between the LDOCE sens"
W98-0710,P85-1037,0,\N,Missing
Y01-1010,J98-1003,0,0.0187897,"emplars for representing different types of information, into an integrated repository. By integration, we mean that the various resources are linked in some way but the different types of information are preserved in their original structures in individual resources. Such a linkage thus enables us to access the different types of information simultaneously, compatibly, and flexibly. Although such resource integration has been suggested (e.g. Yarowsky, 1992) or implemented in some way (e.g. Sanfilippo & Poznariski, 1992; Knight & Luk, 1994; McHale & Crowter, 1994; Klavans & Tzoukermann, 1995; Chen & Chang, 1998), surprisingly few studies have actually made use of multiple existing lexical resources simultaneously in WSD. Rather, most studies to date which use multiple knowledge sources for WSD are only maximising the exploitation of a single (type of) resource. One study which actually used information distributed across various resources is perhaps 109 McRoy (1992), but the resources were tailor-made in her study and the linkage between them was manually imposed. We shall explore how an automatically integrated resource could be put into practical use. In this study, we made use of a simple but effe"
Y01-1010,P85-1037,0,0.165179,"equire a lot of different types of lexical information to perform well. There are basically three ways of forming comprehensive resources. One is to start from scratch and encode as much information as desired into a lexical database manually (e.g. Wilks, 1975; Small & Rieger, 1982; Miller et al., 1990). However, this is heavy work, and the time and effort often limit the coverage. Some people have therefore taken an alternative route, by acquiring lexical semantic information semi-automatically from existing resources such as machine-readable dictionaries (e.g. Amsler, 1981; Calzolari, 1988; Chodorow et al., 1985; Vossen et al., 1989) and corpora (e.g. Resnik, 1993; Riloff & Jones, 1999). Nevertheless, this approach may not be entirely satisfactory. A single resource may not contain all types of information. Even if it does (e.g. a comprehensive dictionary might contain thesaural information implicitly in its meaning definitions), the extraction of some information is not always straightforward, rendering the acquired information incomplete or unreliable. Thus a third and possibly better approach is to combine various existing resources, especially different types of them which are exemplars for repre"
Y01-1010,P98-2245,1,0.843489,"d it contains word senses which were derived from actual usages of words in the Brown Corpus. As a consequence, there may be some non-central (and perhaps mildly technical) senses in WN16 which 3 Brown Corpus c01, 4&apos; paragraph. 115 one might not find in an ordinary resource, e.g. the &quot;fourth dimension&quot; sense of &quot;time&quot; is found in WN16, but not ROGET. Apart from the origin of senses, WN16 and ROGET also seem to differ in the aspects of senses they emphasise. The latter difference may account for the weak correlation of the latter with other resources in terms of the number of senses for words (Kwong, 1998b). Such a disproportion of senses across resources is also obvious in the current analysis. For example, &quot;concert&quot; has only one sense in WN16, namely &quot;a performance of music&quot;. But it is under six heads in ROGET: 24(Agreement), 181(Concurrence), 410(Melody), 412(Music), 414(Musical instruments), and 706(Cooperation) -- everything but &quot;performance&quot;! This variation of focus thus leads to unexpected mappings and demands some flexibility in our judgement of appropriateness. For example, given that &quot;concert&quot; is not under &quot;performance&quot; in ROGET, we might need to evaluate the mapping with respect to"
Y01-1010,W98-0710,1,0.834127,"d it contains word senses which were derived from actual usages of words in the Brown Corpus. As a consequence, there may be some non-central (and perhaps mildly technical) senses in WN16 which 3 Brown Corpus c01, 4&apos; paragraph. 115 one might not find in an ordinary resource, e.g. the &quot;fourth dimension&quot; sense of &quot;time&quot; is found in WN16, but not ROGET. Apart from the origin of senses, WN16 and ROGET also seem to differ in the aspects of senses they emphasise. The latter difference may account for the weak correlation of the latter with other resources in terms of the number of senses for words (Kwong, 1998b). Such a disproportion of senses across resources is also obvious in the current analysis. For example, &quot;concert&quot; has only one sense in WN16, namely &quot;a performance of music&quot;. But it is under six heads in ROGET: 24(Agreement), 181(Concurrence), 410(Melody), 412(Music), 414(Musical instruments), and 706(Cooperation) -- everything but &quot;performance&quot;! This variation of focus thus leads to unexpected mappings and demands some flexibility in our judgement of appropriateness. For example, given that &quot;concert&quot; is not under &quot;performance&quot; in ROGET, we might need to evaluate the mapping with respect to"
Y01-1010,J92-1001,0,0.0300753,"bly, and flexibly. Although such resource integration has been suggested (e.g. Yarowsky, 1992) or implemented in some way (e.g. Sanfilippo & Poznariski, 1992; Knight & Luk, 1994; McHale & Crowter, 1994; Klavans & Tzoukermann, 1995; Chen & Chang, 1998), surprisingly few studies have actually made use of multiple existing lexical resources simultaneously in WSD. Rather, most studies to date which use multiple knowledge sources for WSD are only maximising the exploitation of a single (type of) resource. One study which actually used information distributed across various resources is perhaps 109 McRoy (1992), but the resources were tailor-made in her study and the linkage between them was manually imposed. We shall explore how an automatically integrated resource could be put into practical use. In this study, we made use of a simple but effective structurally-based sense-mapping algorithm to link all noun senses shared by two existing lexical resources, namely WordNet and Roget&apos;s Thesaurus, to form an Integrated Lexical Resource (ILR). The algorithm will be presented in Section 2, followed by a description of the linking process in Section 3. The ILR will be evaluated and its properties discusse"
Y01-1010,W97-0213,0,0.0144515,"= 0 The ability to quantify such comparisons means that we can combine different resources into a single sense inventory, but preserve the structures of individual resources to allow the flexible use of different types of information for WSD. This is the most significant property of the ILR, and the ultimate motivation for integrating multiple existing resources. In actual WSD, the above comparison is normally between senses of different words. However, comparing senses of the same word, our example serves to illustrate another potential use of the mapping results, in terms of WSD evaluation. Resnik and Yarowsky (1997) suggested that wrong answers from WSD systems should be given different penalties depending on the closeness of the wrong answer to the expected one. Our sense-mapping matrix can therefore serve as a kind of confusion matrix, and the similarity function above offers an alternative way for determining the closeness of senses. In the case of &quot;performance&quot;, a system which fails to distinguish between senses 1 and 2 should therefore receive less penalty than one which fails between senses 1 and 4. 6 Conclusion We have performed a large-scale automatic sense linkage between WordNet 1.6 and Roget&apos;s"
Y01-1010,A92-1011,0,0.0928825,"Missing"
Y01-1010,C92-2070,0,0.0388756,"mplete or unreliable. Thus a third and possibly better approach is to combine various existing resources, especially different types of them which are exemplars for representing different types of information, into an integrated repository. By integration, we mean that the various resources are linked in some way but the different types of information are preserved in their original structures in individual resources. Such a linkage thus enables us to access the different types of information simultaneously, compatibly, and flexibly. Although such resource integration has been suggested (e.g. Yarowsky, 1992) or implemented in some way (e.g. Sanfilippo & Poznariski, 1992; Knight & Luk, 1994; McHale & Crowter, 1994; Klavans & Tzoukermann, 1995; Chen & Chang, 1998), surprisingly few studies have actually made use of multiple existing lexical resources simultaneously in WSD. Rather, most studies to date which use multiple knowledge sources for WSD are only maximising the exploitation of a single (type of) resource. One study which actually used information distributed across various resources is perhaps 109 McRoy (1992), but the resources were tailor-made in her study and the linkage between them was"
Y02-1024,J93-1007,0,0.0958574,"Missing"
Y02-1024,J96-1001,0,0.0922172,"Missing"
Y03-1022,E03-1081,1,\N,Missing
Y08-1023,W02-0805,0,0.161639,"rom various resources, including dictionary definitions, word association norms, lexical and knowledge bases, as well as corpus 236 data from authentic texts. In the current study, we explore the feasibility of simulating human judgements on concreteness from dictionary definitions. It has been suggested that WSD systems should be less penalised if they fail to distinguish between closely related word senses than if they fail between distinct senses. This issue of sense similarity is addressed by Resnik and Yarowsky (1999) with quantitative characterisation in terms of sense proximity, and by Chugur et al. (2002) in terms of sense stability. WSD is often considered a lexically sensitive task, in which individual target words might vary in their difficulty and require different treatment. Pedersen (2002) assessed the difficulty of test instances in the SENSEVAL-2 English lexical sample task by analysing the agreement among participating systems. We base our analysis on the target nouns for the English lexical sample tasks and system performance reported in SENSEVAL-1 and SENSEVAL-4 (officially SemEVAL-2007). Different systems might vary in the disambiguating information and computational approaches the"
Y08-1023,W97-0213,0,0.0607721,"of the task. Keywords: Lexical concreteness, Word sense disambiguation 1. Introduction The SENSEVAL (and recently SemEval) exercises have revealed a lot of issues on automatic word sense disambiguation (WSD), and allowed researchers to learn more about the linguistic and technical aspects of the task. System performance often depends on many factors, including the feature set, availability of training instances, and language models, amongst others. One important linguistic factor is the fine-grainedness of the sense inventory and the semantic closeness among the senses of a word. To this end, Resnik and Yarowsky (1997) suggested that closely related senses are more difficult for WSD, and therefore systems should be penalised less if they fail to distinguish between similar senses than if they fail to tell distinct senses apart. Despite being a psychologically valid and intrinsic property of words and senses, concreteness is seldom addressed in WSD literature. Psychologists have shown, from lexical decision and naming tasks, that abstract words are harder to understand than concrete words, and are often acquired later (e.g. Bleasdale, 1987; Kroll and Merves, 1986; Yore and Ollila, * The work described in thi"
Y08-1023,S07-1016,0,\N,Missing
Y11-1002,W10-2406,0,0.0244221,"produced by state-of-theart methods. We will briefly review related work in Section 2, and introduce the datasets used in this study in Section 3. The system will be described in Section 4. Experiments and results will be reported in Section 5, followed by future work and conclusion in Section 6. 2 Related Work The reports of the shared task in NEWS 2009 (Li et al., 2009) and NEWS 2010 (Li et al., 2010) highlighted two particularly popular approaches for transliteration generation among the participating systems. One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint SourceChannel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai"
Y11-1002,P11-2094,0,0.0908857,"eriments and results will be reported in Section 5, followed by future work and conclusion in Section 6. 2 Related Work The reports of the shared task in NEWS 2009 (Li et al., 2009) and NEWS 2010 (Li et al., 2010) highlighted two particularly popular approaches for transliteration generation among the participating systems. One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint SourceChannel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in"
Y11-1002,P09-2006,1,0.762781,"ent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint SourceChannel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009). 3 Datasets In the current study, we used the transliteration data provided by the organiser of the transliteration generation shared task (English-to-Chinese track) in the Named Entities Workshop 2009 (NEWS 2009) for testing, which are mostly based on name pairs from Xinhua News Agency (1992). There are 31,961 English-Chinese name pairs in the training set, 2,896 pairs in the development set, and another 2,896 English names in the test set. The Chinese transliterations basically correspond to Mandarin Chinese pronunciations of the English names, as used by the media in Mainland China. The tr"
Y11-1002,P07-1016,0,0.0200046,"approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint SourceChannel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009). 3 Datasets In the current study, we used the transliteration data provided by the organiser of the transliteration generation shared task (English-to-Chinese track) in the Named Entities Workshop 2009 (NEWS 2009) for testing, which are mostly based on name pairs from Xinhua News Agency (1992). There are 31,961 English-Chinese name pairs in the training set, 2,896 pairs in the development set, and another 2,896 English names in the test set. The Chinese transliterations basically correspond to Mandarin Chinese pronunciations of the English names, as used by the media in Mainland"
Y11-1002,P04-1021,0,0.243784,"10) highlighted two particularly popular approaches for transliteration generation among the participating systems. One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint SourceChannel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009). 3 Datasets In the current study, we used the transliteration data provided by the organiser of the transliteration generation shared ta"
Y11-1002,I05-1040,0,0.025881,"elds which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint SourceChannel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009). 3 Datasets In the current study, we used the transliteration data provided by the organiser of the transliteration generation shared task (English-to-Chinese track) in the Named Entities Workshop 2009 (NEWS 2009) for testing, which are mostly based on name pairs from Xinhua News Agency (1992). There are 31,961 English-Chinese name pairs in the training set, 2,896 pairs in the development set,"
Y11-1002,W09-3507,0,0.0181779,"is study in Section 3. The system will be described in Section 4. Experiments and results will be reported in Section 5, followed by future work and conclusion in Section 6. 2 Related Work The reports of the shared task in NEWS 2009 (Li et al., 2009) and NEWS 2010 (Li et al., 2010) highlighted two particularly popular approaches for transliteration generation among the participating systems. One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint SourceChannel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like tem"
Y11-1002,W10-2409,0,0.0217111,"omparable to those produced by state-of-theart methods. We will briefly review related work in Section 2, and introduce the datasets used in this study in Section 3. The system will be described in Section 4. Experiments and results will be reported in Section 5, followed by future work and conclusion in Section 6. 2 Related Work The reports of the shared task in NEWS 2009 (Li et al., 2009) and NEWS 2010 (Li et al., 2010) highlighted two particularly popular approaches for transliteration generation among the participating systems. One is phrase-based statistical machine transliteration (e.g. Song et al., 2010; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint SourceChannel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wu"
Y11-1002,W06-1630,0,0.0264473,"Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint SourceChannel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009). 3 Datasets In the current study, we used the transliteration data provided by the organiser of the transliteration generation shared task (English-to-Chinese track) in the Named Entities Workshop 2009 (NEWS 2009) for testing, which are mostly based on name pairs from Xinhua News Agency (1992). There are 31,961 English-Chinese name pairs in the training set, 2,896 pairs in the development set, and another 2,896 English names in the test set. The Chinese transliterations basically correspond to Mandarin Chinese pronunciations of the English names, as used by the"
Y11-1002,W10-2410,0,0.0251132,"10; Finch and Sumita, 2010) and the other is Conditional Random Fields which treats the task as one of sequence labelling (e.g. Shishtla et al., 2009). Besides these popular methods, for instance, Huang et al. (2011) used a non-parametric Bayesian learning approach in a recent study. Regarding the basic unit of transliteration, traditional systems are mostly phoneme-based (e.g. Knight and Graehl, 1998). Li et al. (2004) suggested a grapheme-based Joint SourceChannel Model within the Direct Orthographic Mapping framework. Models based on characters (e.g. Shishtla et al., 2009), syllables (e.g. Wutiwiwatchai and Thangthai, 2010), as well as hybrid units (e.g. Oh and Choi, 2005), are also seen. In addition to phonetic features, others like temporal, semantic, and tonal features have also been found useful in transliteration (e.g. Tao et al., 2006; Li et al., 2007; Kwong, 2009). 3 Datasets In the current study, we used the transliteration data provided by the organiser of the transliteration generation shared task (English-to-Chinese track) in the Named Entities Workshop 2009 (NEWS 2009) for testing, which are mostly based on name pairs from Xinhua News Agency (1992). There are 31,961 English-Chinese name pairs in the"
Y11-1002,W09-3501,0,\N,Missing
Y11-1002,W10-2401,0,\N,Missing
Y11-1002,W11-3202,0,\N,Missing
Y11-1002,J98-4003,0,\N,Missing
Y11-1007,P85-1037,0,0.621451,"ico-collocational context in the definition (Hanks, 1987). Although this style has been thoroughly used by the COBUILD series of dictionaries, it has not really dominated the dictionary market as anticipated but continues to co-exist with most conventional defining styles in many other dictionaries. In the current study, we focus on the surface structure of the various traditional defining styles for clues on concreteness. 2.2 Scoring Definitions by Surface Structure Computational linguists have made use of dictionary definitions to semi-automatically acquire simple ontologies for nouns (e.g. Chodorow et al., 1985; Vossen et al., 1989). For instance, apart from real genus, Vossen et al. (1989) have also observed other pseudo genus like empty kernels (e.g. a kind of …) and those shifting the definition to some non-NPs (e.g. a manner of speaking …), which they called linkers and shunters respectively in the LINKS Project. In this study, we also exploit the regularity exhibited in definitions to reveal how concreteness is perceived in the eyes of lexicographers. Based on extensive observation and analysis of dictionary definitions, we started with the definition patterns roughly outlined in Kwong (2008) a"
Y11-1007,W08-2123,0,0.0144135,"ishing features. Scores are thus assigned according to the presence or absence of various surface structures. Figure 1 roughly maps various patterns to a 7-point scale of concreteness. Highly Concrete Genus + Differentiae + Prototype 7 6 Genus + Differentiae / Prototype 5 Pre-modified genus (non-mass) / Person 4 Empty genus / measurement 3 Synonymous phrases 2 NP with only mass nouns / Shunters Others, explain by usage 1 Highly Abstract Figure 1: Mapping Definition Patterns to 7-Point Scale To analyse the structure of the definitions, we make use of the dependency parser from Lund University (Johansson and Nugues, 2008) and detect the various definition patterns by means of the presence or absence of certain dependency relations obtained from the parse results. Only those parses consisting of a root (ROOT) and a predicative complement (PRD) in the form of a noun will be of interest to us. Any of the following dependency relations on the PRD would be considered differentiae: Apposition (APPO), Location (LOC), Modifier of nominal (NMOD), Object complement (OPRD), and Temporal (TMP). If any of such relations has a dependency from words like “usually”, “often”, “typically”, etc., it would be considered a prototy"
Y11-1029,W04-0213,0,0.0148788,"iving a descriptive account of discourse relations holding between adjacent text spans, indicating the coherence and structure exhibited among natural text. A text is thus divided into units, essentially clauses, hierarchically structured and functionally organised with respect to a set of discourse relations, e.g. EVIDENCE, ELABORATION, CONCESSION, etc. Each relation defines how the two involved text spans, the nucleus and the satellite, functionally relate to each other with respect to the effect on the reader. RST relations are annotated in many corpora, e.g. the Potsdam Commentary Corpus (Stede, 2004). Also centering on discourse relations, the annotation in the Penn Discourse Treebank (PDTB) emphasises less on the effect intended on the reader, but focuses on the semantic link between discourse segments. The annotation is mostly lexically triggered with discourse connectives including explicit connectives like subordinating conjunctions, coordinating conjunctions and adverbials, as well as implicit ones inferred by readers (Miltsakaki et al., 2008). A discourse connective is viewed as a predicate taking two abstract objects such as propositions, events, or situations as its arguments (Arg"
Y12-1044,W02-0817,0,0.0156249,"implemented in the WEKA package (Hall et al., 2009), with all default settings, were used. For tokenisation and tagging of the data, the tokeniser and tagger available with the Lund University dependency parser (Johansson and Nugues, 2008) were used, although we did not use the parser specifically for this study. 3.1 Dataset The data available for target nouns tested in the SENSEVAL-3 English lexical sample task were used. According to Mihalcea et al. (2004), the examples were extracted from the British National Corpus and the sense annotation was done using the Open Mind Word Expert system (Chklovski and Mihalcea 2002), and the sense inventory used for the nouns was WordNet 1.7.1 (Miller, 1995). Table 1 shows the target nouns with the number of senses and the distribution of concrete and abstract senses, as well as the number of training and testing instances for each noun. There are 20 items, with 3 to 9 senses, averaging at 5.35 senses.1 The number of training examples for each sense varies considerably. The concrete/abstract classification of the senses was based on the lexicographer files in WordNet. Senses are organised under 45 lexicographer files based on syntactic category and logical groupings, and"
Y12-1044,W02-0805,0,0.0312523,"biguation, and to consider the outcome given by different knowledge sources with different levels of confidence? As Resnik and Yarowsky (1999) remarked, WSD is a highly lexically sensitive task which in effect requires specialized disambiguators for each polysemous word. But in what way precisely is the combination of algorithms and knowledge sources sensitive to individual (groups of) lexical items? Factors like the number of senses and how closely they are related will have an impact on the difficulty of disambiguation, and the varied difficulty may be reflected from the system performance (Chugur et al., 2002; Pedersen, 2002), but there is still more to learn, especially from an inter-disciplinary perspective. For instance, Krahmer (2010) encouraged mutual learning between computational linguists and psychologists, using as an example the possible influence of the general distinction between concrete and abstract language on perception shown in psychology studies, while such effects are somehow largely ignored in computational linguistics. We have also raised similar concerns for research on automatic word sense disambiguation (Kwong, 2012). In this study, we refer to the Context Availability Mode"
Y12-1044,S01-1001,0,0.0533776,"ensitive way. 1 Introduction Word sense ambiguities tend to escape people’s awareness in everyday communication, except in deliberately biased artificial examples or when context is severely limited, since otherwise we almost effortlessly resolve them using a variety of linguistic and extra-linguistic knowledge. This wide range of information is often rendered as various knowledge sources in automatic word sense disambiguation (WSD) systems, partially modelled with different feature sets. As exemplified in recent SENSEVAL and SEMEVAL evaluation exercises (e.g. Kilgarriff and Rosenzweig, 1999; Edmonds and Cotton, 2001; Mihalcea et al., 2004), state-of-the-art WSD systems are mostly based on supervised approaches. Machine learning algorithms are trained on sense-tagged examples, using a wide range of features extracted from the text approximating a variety of knowledge sources deemed useful for the purpose. Ensembles of different types of classifiers based on different feature sets with some voting scheme often report better performance than individual classifiers alone, though the advantage may just be marginal. While complex interactions between learning algorithms and knowledge sources have been observed"
Y12-1044,W08-2123,0,0.0125861,"hm based on Support Vector Machines, with various knowledge sources (including topical contexts, local collocations, and local syntactic contexts) and their combinations on the noun samples in the SENSEVAL-3 English lexical sample task. The most frequent sense was used as the baseline. The disambiguation results were analysed and compared across individual target words. The algorithms implemented in the WEKA package (Hall et al., 2009), with all default settings, were used. For tokenisation and tagging of the data, the tokeniser and tagger available with the Lund University dependency parser (Johansson and Nugues, 2008) were used, although we did not use the parser specifically for this study. 3.1 Dataset The data available for target nouns tested in the SENSEVAL-3 English lexical sample task were used. According to Mihalcea et al. (2004), the examples were extracted from the British National Corpus and the sense annotation was done using the Open Mind Word Expert system (Chklovski and Mihalcea 2002), and the sense inventory used for the nouns was WordNet 1.7.1 (Miller, 1995). Table 1 shows the target nouns with the number of senses and the distribution of concrete and abstract senses, as well as the number"
Y12-1044,W04-0807,0,0.409242,"on Word sense ambiguities tend to escape people’s awareness in everyday communication, except in deliberately biased artificial examples or when context is severely limited, since otherwise we almost effortlessly resolve them using a variety of linguistic and extra-linguistic knowledge. This wide range of information is often rendered as various knowledge sources in automatic word sense disambiguation (WSD) systems, partially modelled with different feature sets. As exemplified in recent SENSEVAL and SEMEVAL evaluation exercises (e.g. Kilgarriff and Rosenzweig, 1999; Edmonds and Cotton, 2001; Mihalcea et al., 2004), state-of-the-art WSD systems are mostly based on supervised approaches. Machine learning algorithms are trained on sense-tagged examples, using a wide range of features extracted from the text approximating a variety of knowledge sources deemed useful for the purpose. Ensembles of different types of classifiers based on different feature sets with some voting scheme often report better performance than individual classifiers alone, though the advantage may just be marginal. While complex interactions between learning algorithms and knowledge sources have been observed (e.g. Mihalcea, 2002; Y"
Y12-1044,J10-2007,0,0.0172106,"1999) remarked, WSD is a highly lexically sensitive task which in effect requires specialized disambiguators for each polysemous word. But in what way precisely is the combination of algorithms and knowledge sources sensitive to individual (groups of) lexical items? Factors like the number of senses and how closely they are related will have an impact on the difficulty of disambiguation, and the varied difficulty may be reflected from the system performance (Chugur et al., 2002; Pedersen, 2002), but there is still more to learn, especially from an inter-disciplinary perspective. For instance, Krahmer (2010) encouraged mutual learning between computational linguists and psychologists, using as an example the possible influence of the general distinction between concrete and abstract language on perception shown in psychology studies, while such effects are somehow largely ignored in computational linguistics. We have also raised similar concerns for research on automatic word sense disambiguation (Kwong, 2012). In this study, we refer to the Context Availability Model in psycholinguistics (Schwanenflugel, 1991), which is used to explain human comprehension processes in general and more specifical"
Y12-1044,Y08-1023,1,0.859045,"ess have been considered important semantic characteristics which influence human lexical processing (e.g. Taft, 1991). While polysemy (in terms of sense number and granularity) and familiarity (in terms of frequency or prior probability) have also been addressed by computational linguists to account for differential system performance, the concreteness effect is somehow seldom discussed in the WSD literature. A few examples include: Jorgensen 410 (1990) suggested that concreteness of a word may increase agreement between judges for sorting word usages and concrete words are easier to define; Kwong (2008) studied the relation between concreteness and system performance in SENSEVAL-2, though the findings were not particularly conclusive, partly because of the confusion from discussing concreteness at both the sense and word level; Yuret and Yatbaz (2010) mentioned that the abstract classes were responsible for most of the errors in their supersense tagging with unsupervised method. Given the significance of the concreteness effect in human lexical processing (e.g. Paivio et al., 1968; Kroll and Merves, 1986; Bleasdale, 1987; Schwanenflugel, 1991), more in-depth analysis of the concreteness effe"
Y12-1044,J10-1004,0,0.0295904,"e also been addressed by computational linguists to account for differential system performance, the concreteness effect is somehow seldom discussed in the WSD literature. A few examples include: Jorgensen 410 (1990) suggested that concreteness of a word may increase agreement between judges for sorting word usages and concrete words are easier to define; Kwong (2008) studied the relation between concreteness and system performance in SENSEVAL-2, though the findings were not particularly conclusive, partly because of the confusion from discussing concreteness at both the sense and word level; Yuret and Yatbaz (2010) mentioned that the abstract classes were responsible for most of the errors in their supersense tagging with unsupervised method. Given the significance of the concreteness effect in human lexical processing (e.g. Paivio et al., 1968; Kroll and Merves, 1986; Bleasdale, 1987; Schwanenflugel, 1991), more in-depth analysis of the concreteness effect is definitely needed especially for mainstream supervised WSD. Psychologists have put forth various plausible explanations to account for the concreteness effect observed in human lexical processing, one of which is the context availability model. It"
Y13-1013,W93-0310,0,0.882796,"Missing"
Y13-1013,P06-1036,0,0.0772927,"Missing"
Y13-1013,Y12-1044,1,0.866386,"Missing"
Y15-2002,C10-1004,0,0.0165608,"pinions reported or quoted in news articles (e.g. Wiebe and Wilson, 2002; Tsou et al., 2005; Ku et al., 2006). Systems often leverage some sentiment lexicons (e.g. Wilson et al., 2005; Esuli and Sebastiani, 2006) and are thus primarily lexically based (e.g. Pang et al., 2002; Turney, 2002; Polanyi and Zaenen, 2006; Li et al., 2012), although tasks requiring more fine-grained information like opinion holders and targets would require more than simple lexical clues (e.g. Kim and Hovy, 2006; Lu et al., 2010; Zirn et al., 2011). Approaches using multi-lingual data are also gaining attention (e.g. Banea et al., 2010). Subjectivity may be associated with various communicative functions, such as presenting one’s stance, giving advice, making prediction, evaluating and commenting, etc. Such functions are achieved with a combination of rhetorical devices including but not limited to lexical choices. Corpus-based discourse analysis has thus often relied on multiple linguistic patterns to characterise register variations (Biber, 1988; Kaufer and Ishizaki, 2006). Multi-dimensional analysis, as explained and applied in Biber (1988) as well as Conrad and Biber (2001), makes use of multivariate statistical techniqu"
Y15-2002,esuli-sebastiani-2006-sentiwordnet,0,0.0893797,"mpts to classify the polarity of subjective views as positive, neutral, or negative. A comprehensive survey can be found in Pang and Lee (2008), and Liu (2010). Past studies have mostly been concerned with written data, typically first-hand opinions like movie reviews (e.g. Pang et al., 2002), product reviews (e.g. Hu and Liu, 2004), or debates on web forums (e.g. Somasundaran and Wiebe, 2009), and second-hand opinions reported or quoted in news articles (e.g. Wiebe and Wilson, 2002; Tsou et al., 2005; Ku et al., 2006). Systems often leverage some sentiment lexicons (e.g. Wilson et al., 2005; Esuli and Sebastiani, 2006) and are thus primarily lexically based (e.g. Pang et al., 2002; Turney, 2002; Polanyi and Zaenen, 2006; Li et al., 2012), although tasks requiring more fine-grained information like opinion holders and targets would require more than simple lexical clues (e.g. Kim and Hovy, 2006; Lu et al., 2010; Zirn et al., 2011). Approaches using multi-lingual data are also gaining attention (e.g. Banea et al., 2010). Subjectivity may be associated with various communicative functions, such as presenting one’s stance, giving advice, making prediction, evaluating and commenting, etc. Such functions are achi"
Y15-2002,P02-1053,0,0.0105528,"ehensive survey can be found in Pang and Lee (2008), and Liu (2010). Past studies have mostly been concerned with written data, typically first-hand opinions like movie reviews (e.g. Pang et al., 2002), product reviews (e.g. Hu and Liu, 2004), or debates on web forums (e.g. Somasundaran and Wiebe, 2009), and second-hand opinions reported or quoted in news articles (e.g. Wiebe and Wilson, 2002; Tsou et al., 2005; Ku et al., 2006). Systems often leverage some sentiment lexicons (e.g. Wilson et al., 2005; Esuli and Sebastiani, 2006) and are thus primarily lexically based (e.g. Pang et al., 2002; Turney, 2002; Polanyi and Zaenen, 2006; Li et al., 2012), although tasks requiring more fine-grained information like opinion holders and targets would require more than simple lexical clues (e.g. Kim and Hovy, 2006; Lu et al., 2010; Zirn et al., 2011). Approaches using multi-lingual data are also gaining attention (e.g. Banea et al., 2010). Subjectivity may be associated with various communicative functions, such as presenting one’s stance, giving advice, making prediction, evaluating and commenting, etc. Such functions are achieved with a combination of rhetorical devices including but not limited to le"
Y15-2002,D12-1013,0,0.0159496,"Lee (2008), and Liu (2010). Past studies have mostly been concerned with written data, typically first-hand opinions like movie reviews (e.g. Pang et al., 2002), product reviews (e.g. Hu and Liu, 2004), or debates on web forums (e.g. Somasundaran and Wiebe, 2009), and second-hand opinions reported or quoted in news articles (e.g. Wiebe and Wilson, 2002; Tsou et al., 2005; Ku et al., 2006). Systems often leverage some sentiment lexicons (e.g. Wilson et al., 2005; Esuli and Sebastiani, 2006) and are thus primarily lexically based (e.g. Pang et al., 2002; Turney, 2002; Polanyi and Zaenen, 2006; Li et al., 2012), although tasks requiring more fine-grained information like opinion holders and targets would require more than simple lexical clues (e.g. Kim and Hovy, 2006; Lu et al., 2010; Zirn et al., 2011). Approaches using multi-lingual data are also gaining attention (e.g. Banea et al., 2010). Subjectivity may be associated with various communicative functions, such as presenting one’s stance, giving advice, making prediction, evaluating and commenting, etc. Such functions are achieved with a combination of rhetorical devices including but not limited to lexical choices. Corpus-based discourse analys"
Y15-2002,W02-1011,0,0.0174301,"tivity and sentiment analysis. Subjectivity analysis aims at distinguishing opinionated sentences from factual statements, where the former is also known as private states, referring to one’s mental and emotional states which may express one’s attitude, feeling, beliefs, evaluation, speculation, etc. Sentiment analysis attempts to classify the polarity of subjective views as positive, neutral, or negative. A comprehensive survey can be found in Pang and Lee (2008), and Liu (2010). Past studies have mostly been concerned with written data, typically first-hand opinions like movie reviews (e.g. Pang et al., 2002), product reviews (e.g. Hu and Liu, 2004), or debates on web forums (e.g. Somasundaran and Wiebe, 2009), and second-hand opinions reported or quoted in news articles (e.g. Wiebe and Wilson, 2002; Tsou et al., 2005; Ku et al., 2006). Systems often leverage some sentiment lexicons (e.g. Wilson et al., 2005; Esuli and Sebastiani, 2006) and are thus primarily lexically based (e.g. Pang et al., 2002; Turney, 2002; Polanyi and Zaenen, 2006; Li et al., 2012), although tasks requiring more fine-grained information like opinion holders and targets would require more than simple lexical clues (e.g. Kim"
Y15-2002,P09-1026,0,0.0344055,"ences from factual statements, where the former is also known as private states, referring to one’s mental and emotional states which may express one’s attitude, feeling, beliefs, evaluation, speculation, etc. Sentiment analysis attempts to classify the polarity of subjective views as positive, neutral, or negative. A comprehensive survey can be found in Pang and Lee (2008), and Liu (2010). Past studies have mostly been concerned with written data, typically first-hand opinions like movie reviews (e.g. Pang et al., 2002), product reviews (e.g. Hu and Liu, 2004), or debates on web forums (e.g. Somasundaran and Wiebe, 2009), and second-hand opinions reported or quoted in news articles (e.g. Wiebe and Wilson, 2002; Tsou et al., 2005; Ku et al., 2006). Systems often leverage some sentiment lexicons (e.g. Wilson et al., 2005; Esuli and Sebastiani, 2006) and are thus primarily lexically based (e.g. Pang et al., 2002; Turney, 2002; Polanyi and Zaenen, 2006; Li et al., 2012), although tasks requiring more fine-grained information like opinion holders and targets would require more than simple lexical clues (e.g. Kim and Hovy, 2006; Lu et al., 2010; Zirn et al., 2011). Approaches using multi-lingual data are also gaini"
Y15-2002,W02-2034,0,0.062214,"s mental and emotional states which may express one’s attitude, feeling, beliefs, evaluation, speculation, etc. Sentiment analysis attempts to classify the polarity of subjective views as positive, neutral, or negative. A comprehensive survey can be found in Pang and Lee (2008), and Liu (2010). Past studies have mostly been concerned with written data, typically first-hand opinions like movie reviews (e.g. Pang et al., 2002), product reviews (e.g. Hu and Liu, 2004), or debates on web forums (e.g. Somasundaran and Wiebe, 2009), and second-hand opinions reported or quoted in news articles (e.g. Wiebe and Wilson, 2002; Tsou et al., 2005; Ku et al., 2006). Systems often leverage some sentiment lexicons (e.g. Wilson et al., 2005; Esuli and Sebastiani, 2006) and are thus primarily lexically based (e.g. Pang et al., 2002; Turney, 2002; Polanyi and Zaenen, 2006; Li et al., 2012), although tasks requiring more fine-grained information like opinion holders and targets would require more than simple lexical clues (e.g. Kim and Hovy, 2006; Lu et al., 2010; Zirn et al., 2011). Approaches using multi-lingual data are also gaining attention (e.g. Banea et al., 2010). Subjectivity may be associated with various communi"
Y15-2002,H05-1044,0,0.0302902,"ntiment analysis attempts to classify the polarity of subjective views as positive, neutral, or negative. A comprehensive survey can be found in Pang and Lee (2008), and Liu (2010). Past studies have mostly been concerned with written data, typically first-hand opinions like movie reviews (e.g. Pang et al., 2002), product reviews (e.g. Hu and Liu, 2004), or debates on web forums (e.g. Somasundaran and Wiebe, 2009), and second-hand opinions reported or quoted in news articles (e.g. Wiebe and Wilson, 2002; Tsou et al., 2005; Ku et al., 2006). Systems often leverage some sentiment lexicons (e.g. Wilson et al., 2005; Esuli and Sebastiani, 2006) and are thus primarily lexically based (e.g. Pang et al., 2002; Turney, 2002; Polanyi and Zaenen, 2006; Li et al., 2012), although tasks requiring more fine-grained information like opinion holders and targets would require more than simple lexical clues (e.g. Kim and Hovy, 2006; Lu et al., 2010; Zirn et al., 2011). Approaches using multi-lingual data are also gaining attention (e.g. Banea et al., 2010). Subjectivity may be associated with various communicative functions, such as presenting one’s stance, giving advice, making prediction, evaluating and commenting,"
Y15-2002,I11-1038,0,0.0142542,"u, 2004), or debates on web forums (e.g. Somasundaran and Wiebe, 2009), and second-hand opinions reported or quoted in news articles (e.g. Wiebe and Wilson, 2002; Tsou et al., 2005; Ku et al., 2006). Systems often leverage some sentiment lexicons (e.g. Wilson et al., 2005; Esuli and Sebastiani, 2006) and are thus primarily lexically based (e.g. Pang et al., 2002; Turney, 2002; Polanyi and Zaenen, 2006; Li et al., 2012), although tasks requiring more fine-grained information like opinion holders and targets would require more than simple lexical clues (e.g. Kim and Hovy, 2006; Lu et al., 2010; Zirn et al., 2011). Approaches using multi-lingual data are also gaining attention (e.g. Banea et al., 2010). Subjectivity may be associated with various communicative functions, such as presenting one’s stance, giving advice, making prediction, evaluating and commenting, etc. Such functions are achieved with a combination of rhetorical devices including but not limited to lexical choices. Corpus-based discourse analysis has thus often relied on multiple linguistic patterns to characterise register variations (Biber, 1988; Kaufer and Ishizaki, 2006). Multi-dimensional analysis, as explained and applied in Biber"
Y15-2002,W06-0301,0,\N,Missing
Y15-2002,J93-2001,0,\N,Missing
Y16-2023,W08-1901,0,0.025867,"often assumes an extensive inter-connection of words, which is largely inspired by psychological models of the mental lexicon (e.g. Aitchison, 2003; De Deyne et al., 2016). Enhancement of word access in electronic dictionaries thus focuses on identifying, capturing and making available a wide range of word associations to enable words to be searched via multiple routes. To this end, empirical evidence from psycholinguistic data, especially word association norms, offers valuable information about the variety of associative relations and their relative significance in the mental word web (e.g. Joyce and Srdanović, 2008; Kwong, 2013). At the same time, computational linguists and lexicographers have attempted to model such relations and even the corresponding associative strengths (e.g. Church and Hanks, 1990; Kilgarriff et al., 2004), not necessarily as ambitious as to reconstruct the 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 249 human mental lexicon, but often aiming to enhance lexical access with a mechanism taking advantage of the organisation of the mental word repository. For instance, even when a user fails to name t"
Y16-2023,Y13-1013,1,0.921143,"inter-connection of words, which is largely inspired by psychological models of the mental lexicon (e.g. Aitchison, 2003; De Deyne et al., 2016). Enhancement of word access in electronic dictionaries thus focuses on identifying, capturing and making available a wide range of word associations to enable words to be searched via multiple routes. To this end, empirical evidence from psycholinguistic data, especially word association norms, offers valuable information about the variety of associative relations and their relative significance in the mental word web (e.g. Joyce and Srdanović, 2008; Kwong, 2013). At the same time, computational linguists and lexicographers have attempted to model such relations and even the corresponding associative strengths (e.g. Church and Hanks, 1990; Kilgarriff et al., 2004), not necessarily as ambitious as to reconstruct the 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 249 human mental lexicon, but often aiming to enhance lexical access with a mechanism taking advantage of the organisation of the mental word repository. For instance, even when a user fails to name the target word"
Y16-2023,C82-1034,0,0.74219,"Missing"
Y16-2023,W14-4701,0,0.0147199,"f et al., 2004), not necessarily as ambitious as to reconstruct the 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 249 human mental lexicon, but often aiming to enhance lexical access with a mechanism taking advantage of the organisation of the mental word repository. For instance, even when a user fails to name the target word, as in the tip-of-the-tongue situation, he or she should be enabled to access the word by means of other closely associated words that can be thought of (e.g. Sinopaknikova and Smrž, 2006; Rapp and Zock, 2014; Zock et al., 2010). A very wide range of associative relations have been revealed from word association norms, but as they are elicited in isolation, their readiness to be computationally modelled and their relevance in specific applications might vary. In this study, we further explore the implications from word association norms especially with respect to bilingual dictionary access. In Section 2, we first compare among several existing word association norms for the distribution of different associative types. In Section 3, we then investigate how thoroughly such associations could be mod"
Y16-2023,P07-2011,0,0.607288,"Missing"
Y16-2023,W93-0310,0,0.744191,"Missing"
Y16-2023,P98-2127,0,0.447878,"Missing"
Y16-2023,J90-1003,0,\N,Missing
Y16-2023,C98-2122,0,\N,Missing
Y18-1010,P05-1074,0,0.171026,"al and lexico-syntactic features of paraphrases, which produced more than 9,000 pairs of lexical paraphrases including relations other than synonymy. Human judgment, with or without context, was solicited. Lexical paraphrases, which often involve a replacement with synonyms or hypernyms, do not give a complete account of paraphrase itself or serve ap90 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 plications adequately. Phrasal and sentential paraphrases are indispensable (e.g. Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; CallisonBurch, 2008; Ganitkevitch et al., 2011). For phrasal paraphrases, syntactic categories often become a basic point of reference. Bannard and Callison-Burch (2005), for instance, used a bilingual parallel corpus and obtained English paraphrases by pivoting through foreign language phrases. With reference to phrase-based statistical machine translation, they aligned phrases in the corpus. Those mapping to the same phrase in another language are considered candidates and ranked by a paraphrase probability defined in terms of two translation model probabilities. The extracted paraphrases"
Y18-1010,N03-1003,0,0.0677347,"orithm based on contextual and lexico-syntactic features of paraphrases, which produced more than 9,000 pairs of lexical paraphrases including relations other than synonymy. Human judgment, with or without context, was solicited. Lexical paraphrases, which often involve a replacement with synonyms or hypernyms, do not give a complete account of paraphrase itself or serve ap90 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 plications adequately. Phrasal and sentential paraphrases are indispensable (e.g. Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; CallisonBurch, 2008; Ganitkevitch et al., 2011). For phrasal paraphrases, syntactic categories often become a basic point of reference. Bannard and Callison-Burch (2005), for instance, used a bilingual parallel corpus and obtained English paraphrases by pivoting through foreign language phrases. With reference to phrase-based statistical machine translation, they aligned phrases in the corpus. Those mapping to the same phrase in another language are considered candidates and ranked by a paraphrase probability defined in terms of two translation model probabi"
Y18-1010,P01-1008,0,0.182542,"ree translation from bilingual parallel corpora. Section 4 describes the experimental setup. Section 5 discusses preliminary results and future plans, followed by a conclusion in Section 6. 清晰的记忆 2 记得清清楚楚 Related work For two decades by now, methods on paraphrase extraction and generation have mostly been datadriven (Madnani and Dorr, 2010). Monolingual or bilingual corpora may be used, sometimes also with the help of existing lexical resources (e.g. Wu and Zhou, 2003). Earlier methods primarily rely on distributional similarity for finding paraphrases from identical surrounding context (e.g. Barzilay and McKeown, 2001; Ibrahim et al., 2003). For example, Barzilay and McKeown (2001) utilized a monolingual corpus consisting of multiple English translations of the same novels by foreign authors. The approach takes advantage of the many words shared by the parallel translations, assuming that phrases in aligned sentences appearing in similar contexts are paraphrases. They used a co-training algorithm based on contextual and lexico-syntactic features of paraphrases, which produced more than 9,000 pairs of lexical paraphrases including relations other than synonymy. Human judgment, with or without context, was s"
Y18-1010,D08-1021,0,0.0945071,"sed to literal translation) will be of more interest to us. Three types of paraphrases are often included in paraphrase databases (e.g. Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014): lexical, phrasal, and syntactic paraphrases. Although the inclusion of these paraphrase types has recognized that paraphrases could go beyond the replacement by synonyms or synonymous phrases, there is nevertheless a restriction of syntactic category, as it at most allows the substitution of expressions that match a given syntactic type, even when non-constituent phrases are accommodated (e.g. Callison-Burch, 2008), as long as labels in syntactic trees are used as the point of reference. Paraphrases in different phrasal categories and syntactic constructions are particularly important in translation, as individual possible renditions would have their strengths and weaknesses in a given context with a certain literary style for a specific communicative purpose. In addition to fidelity, this is even more salient when fluency is concerned across language pairs with very distinct linguistic properties, where literal translation is not always the 89 32nd Pacific Asia Conference on Language, Information and C"
Y18-1010,ganitkevitch-callison-burch-2014-multilingual,0,0.0557823,"hand, and for inspiring machine translation’s further improvement on fluency on the other. Paraphrase, in this paper, is therefore used in a slightly restricted sense to refer to alternative expressions in a target language which are not only semantically equivalent, but also fulfill other contextual criteria to be qualified as translation of the text in a source language. Along the continuum of equivalence, free translation (as opposed to literal translation) will be of more interest to us. Three types of paraphrases are often included in paraphrase databases (e.g. Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014): lexical, phrasal, and syntactic paraphrases. Although the inclusion of these paraphrase types has recognized that paraphrases could go beyond the replacement by synonyms or synonymous phrases, there is nevertheless a restriction of syntactic category, as it at most allows the substitution of expressions that match a given syntactic type, even when non-constituent phrases are accommodated (e.g. Callison-Burch, 2008), as long as labels in syntactic trees are used as the point of reference. Paraphrases in different phrasal categories and syntactic constructions are particularly important in tra"
Y18-1010,D11-1108,0,0.0229146,"Missing"
Y18-1010,N13-1092,0,0.0949831,"Missing"
Y18-1010,W03-1608,0,0.0417523,"al parallel corpora. Section 4 describes the experimental setup. Section 5 discusses preliminary results and future plans, followed by a conclusion in Section 6. 清晰的记忆 2 记得清清楚楚 Related work For two decades by now, methods on paraphrase extraction and generation have mostly been datadriven (Madnani and Dorr, 2010). Monolingual or bilingual corpora may be used, sometimes also with the help of existing lexical resources (e.g. Wu and Zhou, 2003). Earlier methods primarily rely on distributional similarity for finding paraphrases from identical surrounding context (e.g. Barzilay and McKeown, 2001; Ibrahim et al., 2003). For example, Barzilay and McKeown (2001) utilized a monolingual corpus consisting of multiple English translations of the same novels by foreign authors. The approach takes advantage of the many words shared by the parallel translations, assuming that phrases in aligned sentences appearing in similar contexts are paraphrases. They used a co-training algorithm based on contextual and lexico-syntactic features of paraphrases, which produced more than 9,000 pairs of lexical paraphrases including relations other than synonymy. Human judgment, with or without context, was solicited. Lexical parap"
Y18-1010,D15-1166,0,0.145894,"Missing"
Y18-1010,J10-3003,0,0.0641549,"iminary results show that the approach is promising and paraphrases at both sentential and sub-sentential levels covering diverse surface forms could be identified. The extracted data, upon further filtering, have great potential to supplement the example sentences available in existing bilingual dictionaries in an effective and systematic way. 1 Introduction In any language, the same meaning can often be expressed in alternative ways, or paraphrased. The recognition and generation of such meaningequivalent forms are deemed important for various natural language processing (NLP) applications (Madnani and Dorr, 2010). In the inter-lingual context, typically in translation, there is inevitably a trace of paraphrasing, whether or not it is employed consciously as a strategy in the process. Notwithstanding the different interpretation of terminology and research objectives across disciplines (e.g. translation vs paraphrase, literal translation vs free translation, etc. in translation studies, or paraphrase generation vs query expansion in the NLP community), access to such context-sensitive equivalents is essential especially when fluency, in addition to fidelity, is concerned, for machine and human translat"
Y18-1010,E17-1083,0,0.0383599,"aningpreserving transformations (Ganitkevitch et al., 2011). However, such constraints may not be appropriate for our current purpose, as the kind of paraphrase, or free translation, that we find useful often appears as different syntactic constructions on the one hand, and may not be accompanied by regular and predictable transformation patterns on the other. The pivoting approach has stayed in the mainstream of paraphrase extraction, with large paraphrase databases like the PPDB and multilingual PPDB created in the meantime (Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014). Mallinson et al. (2017) revisited the approach from the perspective of neural machine translation (NMT), without reference to any underlying grammar or creating any phrase table. In particular, pivoting is done with the NMT model, in the form of oneto-one back-translation or multi-pivoting through the K-best translations. Their system, PARANET, makes use of the attention mechanism for identifying semantically equivalent parts between the paraphrase sentence and the source sentence, with each word of the paraphrase sentence attending to words in the pivot sentence and each word in the pivot sentence attending to word"
Y18-1010,E17-3017,0,0.0370093,"Missing"
Y18-1010,P03-1016,0,0.059389,"ion 2, we will review related NLP work on paraphrase extraction and generation. In Section 3, we will introduce our proposed method for finding free translation from bilingual parallel corpora. Section 4 describes the experimental setup. Section 5 discusses preliminary results and future plans, followed by a conclusion in Section 6. 清晰的记忆 2 记得清清楚楚 Related work For two decades by now, methods on paraphrase extraction and generation have mostly been datadriven (Madnani and Dorr, 2010). Monolingual or bilingual corpora may be used, sometimes also with the help of existing lexical resources (e.g. Wu and Zhou, 2003). Earlier methods primarily rely on distributional similarity for finding paraphrases from identical surrounding context (e.g. Barzilay and McKeown, 2001; Ibrahim et al., 2003). For example, Barzilay and McKeown (2001) utilized a monolingual corpus consisting of multiple English translations of the same novels by foreign authors. The approach takes advantage of the many words shared by the parallel translations, assuming that phrases in aligned sentences appearing in similar contexts are paraphrases. They used a co-training algorithm based on contextual and lexico-syntactic features of paraphr"
Y18-1036,Y15-2002,1,0.836231,"erence on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the author PACLIC 32 most of the recordings made during 1997 and 1998. The audio recordings for the corpus involve some 100 speakers, mostly in their 20s and 30s, balanced for gender. A supplementary set of recordings from radio chat shows was added at a later stage. The data used in this study are 42 files of faceto-face conversations 3 , with about 130K Chinese characters4, noting that the proximal demonstrative is transcribed as 哩 li1 in HKCanCor. Corpus of Verbal Comments (HKVerCom) According to Kwong (2015), the corpus contains transcribed spoken Cantonese data from television and radio programmes broadcasted in Hong Kong during late 2013 to early 2014. They cover various domains (politics and current affairs, economics and finance, and food and entertainment) presented in various ways (interviews, phone-in programmes, singing contests, and food/film critics). The current study made use of the corpus data from TV interviews, radio phone-in programmes and financial commentaries by stock analysts, which contain about 249K Chinese characters. Cantonese Interpreting Corpus (HKLECSIC) This is a bilin"
