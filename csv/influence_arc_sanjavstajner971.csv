C18-1027,W10-1607,0,0.0739437,"Missing"
C18-1027,N16-1120,0,0.0823466,"e unknown to the reader (lexical complexity). It can contain long sentences with difficult syntactic structures or syntactic structures unknown to the reader (syntactic complexity), especially in the case of children or non-native language learners. A much less studied level of text complexity is the conceptual or semantic level, which accounts for the amount of background knowledge required to understand the meaning of the text. While many methods have been proposed so far for automatic assessment of lexical and syntactic text complexity (Vajjala and Meurers, 2014; Vajjala and Meurers, 2015; Ambati et al., 2016), automatic assessment of conceptual text complexity has never been attempted so far. Similarly, in the task of automatic text simplification, only two systems attempted at simplifying the overall text structure by ˇ eliminating irrelevant pieces of information (Narayan and Gardent, 2014; Stajner and Glavaˇs, 2017). Texts that are conceptually complex tend to contain difficult and abstract concepts which are not easy to relate to each other, or they mention numerous entities which are not closely related, requiring substantial background knowledge. Kintsch and van Dijk (1978) distinguish betwe"
C18-1027,P11-2117,0,0.0534347,"ber of effective relations, but the number of related pairs of concepts. The intuition behind using this measure is quite straightforward, since a denser text segment graph can be interpreted as a text in which all concepts are highly related, and accessible from multiple directions. Our hypothesis is therefore that the denser the graph of a text segment, the simpler the text (H8). 5 Experimental Setup 5.1 Data For our experiments, we use the freely available English portion of Newsela language learners corpus (Newsela, 2016). Unlike the original Wikipedia and Simple English Wikipedia corpus (Coster and Kauchak, 2011), where the simplified version is not the result of a direct simplification of the original article, the simplified Newsela articles are direct simplifications of the original articles, manually simplified by trained human editors following strict simplification guidelines (Xu et al., 2016). This ensured the quality of simplifications in the Newsela dataset. More importantly, as the target readers are children and second language learners, and each text is provided in five different difficulty levels, conceptual simplification is expected to be an important component in the Newsela dataset. Ad"
C18-1027,P14-1041,0,0.0132141,"complexity is the conceptual or semantic level, which accounts for the amount of background knowledge required to understand the meaning of the text. While many methods have been proposed so far for automatic assessment of lexical and syntactic text complexity (Vajjala and Meurers, 2014; Vajjala and Meurers, 2015; Ambati et al., 2016), automatic assessment of conceptual text complexity has never been attempted so far. Similarly, in the task of automatic text simplification, only two systems attempted at simplifying the overall text structure by ˇ eliminating irrelevant pieces of information (Narayan and Gardent, 2014; Stajner and Glavaˇs, 2017). Texts that are conceptually complex tend to contain difficult and abstract concepts which are not easy to relate to each other, or they mention numerous entities which are not closely related, requiring substantial background knowledge. Kintsch and van Dijk (1978) distinguish between the micro- and macrostructure of text semantics. The former takes into account the individual concepts and their local relations, while the latter considers their organization at a global level. Conceptual complexity is reflected in both those components and thus conceptual simplifica"
C18-1027,E14-1031,0,0.100693,"s levels. It can contain many infrequent words that are unknown to the reader (lexical complexity). It can contain long sentences with difficult syntactic structures or syntactic structures unknown to the reader (syntactic complexity), especially in the case of children or non-native language learners. A much less studied level of text complexity is the conceptual or semantic level, which accounts for the amount of background knowledge required to understand the meaning of the text. While many methods have been proposed so far for automatic assessment of lexical and syntactic text complexity (Vajjala and Meurers, 2014; Vajjala and Meurers, 2015; Ambati et al., 2016), automatic assessment of conceptual text complexity has never been attempted so far. Similarly, in the task of automatic text simplification, only two systems attempted at simplifying the overall text structure by ˇ eliminating irrelevant pieces of information (Narayan and Gardent, 2014; Stajner and Glavaˇs, 2017). Texts that are conceptually complex tend to contain difficult and abstract concepts which are not easy to relate to each other, or they mention numerous entities which are not closely related, requiring substantial background knowled"
C18-1027,W14-5606,1,0.795025,"e 27th International Conference on Computational Linguistics, pages 318–330 Santa Fe, New Mexico, USA, August 20-26, 2018. text complexity. Our experiments, carried out over documents from a high-quality language learners corpus for English, show that the graph structure of encyclopedic knowledge can be leveraged in order to accurately assess conceptual complexity of informational text (i.e. news stories). This is particularly important as simplification of such texts improves social inclusion of many target audiences (Carroll et ˇ al., 1998; Alu´ısio and Gasperin, 2010; Saggion et al., 2011; Stajner et al., 2014). 2 Knowledge Graphs for Measuring Conceptual Complexity Language as a networked system has been vastly researched over the last decades, especially with the purpose of capturing word semantics (Borge-Holthoefer and Arenas, 2010). One of the core concepts that have been proposed is that of semantic networks. Various types of semantic networks have been studied, with the aim of explaining the organization of human semantic knowledge: hierarchical networks (Collins and Quillian, 1969), associative networks (Nelson et al., 2004), thesauri such as Roget’s thesaurus (Jarmasz and Szpakowicz, 2012),"
C18-1027,Q16-1029,0,0.0150018,"is therefore that the denser the graph of a text segment, the simpler the text (H8). 5 Experimental Setup 5.1 Data For our experiments, we use the freely available English portion of Newsela language learners corpus (Newsela, 2016). Unlike the original Wikipedia and Simple English Wikipedia corpus (Coster and Kauchak, 2011), where the simplified version is not the result of a direct simplification of the original article, the simplified Newsela articles are direct simplifications of the original articles, manually simplified by trained human editors following strict simplification guidelines (Xu et al., 2016). This ensured the quality of simplifications in the Newsela dataset. More importantly, as the target readers are children and second language learners, and each text is provided in five different difficulty levels, conceptual simplification is expected to be an important component in the Newsela dataset. Additionally, the Newsela dataset allows us to control for the topic in our experiments, which is particularly important in order to isolate the effect of simplification from the influence of the story topics. Different topics naturally lead to different parts of the knowledge graph being act"
C18-3005,W10-1001,0,0.0156229,"placing it with synonyms which are simpler to read or understand, while syntactic simplification is concerned with transforming sentences containing syntactic phenomena which may hinder readability and comprehension (e.g. complex subordination phenomena, passive voice constructions) into simpler equivalents. Several ATS projects were conducted aimed at producing simplification systems for different audiences and languages. The PSET (Practical Simplification of English Texts) project was a UK initiative to produce adapted texts for aphasic people (Carroll et al., 1998). The PorSimples project (Aluisio et al., 2010) developed an automatic system and editing assistance tool to simplify texts for people with low literacy levels in Brazil. The Simplext project (Saggion et al., 2015) developed simplification technology for Spanish speakers with intellectual disabilities. The FIRST project (Mart´ın-Valdivia et al., 2014) developed a semi-automatic text adaptation tool for English, Spanish and Bulgarian to improve accessibility of written texts to people with autism, while the Able to Include project (Saggion et al., 2017; Ferr´es et al., 2016) targeted people with intellectual disabilities on the Web. All tho"
C18-3005,C96-2183,0,0.736586,"he vocabulary, syntax, and discourse levels of the text. Over the last years research in automatic text simplification has intensified not only in the number of human languages being addressed but also in the number of techniques being proposed to deal with it from initial rule-based approaches to current data-driven techniques. The aim of this tutorial is to provide a comprehensive overview of past and current research on automatic text simplification. 1 Introduction Automatic text simplification (ATS) appeared as an area of research in natural language processing (NLP) in the late nineties (Chandrasekar et al., 1996). Its goal is to automatically transform given input (text or sentences) into a simpler variant without significantly changing the input original meaning (Saggion, 2017). What is considered a simpler variant clearly depends on who/what is the target readership/application. Initially, ATS was proposed as a pre-processing step to improve various NLP tasks, e.g. machine translation, information extraction, summarisation, and semantic role labeling. In such scenario, a simpler variant is the one that improves the performance of the targeted NLP task, when used instead of the original input text. L"
C18-3005,W11-1601,0,0.0997524,"al linguistics, and natural language processing. We will present the techniques used to transform written texts in each of those projects and make an in-depth discussion of what those projects had in common in terms of techniques and resources, and in what they differed. 19 1.1 Data-driven Paradigm in Simplification With the emergence of Simple English Wikipedia and its (comparable) alignment with English Wikipedia, which for the first time offered a large parallel dataset for training of the ATS systems, the approaches to ATS shifted from rule-based (Siddharthan, 2006) to purely data-driven (Coster and Kauchak, 2011; Zhu et al., 2010; Kauchak, 2013), and later hybrid ones (Siddharthan and Mandya, 2014). It created opportunity for stronger NLP component of the systems and new challenges in text/sentence generation, but at the cost of blurring the final goal of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the prev"
C18-3005,N15-1022,0,0.0529265,"Missing"
C18-3005,P13-1151,0,0.0233496,"ing. We will present the techniques used to transform written texts in each of those projects and make an in-depth discussion of what those projects had in common in terms of techniques and resources, and in what they differed. 19 1.1 Data-driven Paradigm in Simplification With the emergence of Simple English Wikipedia and its (comparable) alignment with English Wikipedia, which for the first time offered a large parallel dataset for training of the ATS systems, the approaches to ATS shifted from rule-based (Siddharthan, 2006) to purely data-driven (Coster and Kauchak, 2011; Zhu et al., 2010; Kauchak, 2013), and later hybrid ones (Siddharthan and Mandya, 2014). It created opportunity for stronger NLP component of the systems and new challenges in text/sentence generation, but at the cost of blurring the final goal of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed idea of approachin"
C18-3005,P17-2014,1,0.893151,"Missing"
C18-3005,I17-3001,0,0.0238948,", given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed idea of approaching ATS as a monolingual machine translation (MT) task (Specia, 2010; Coster and Kauchak, 2011), Xu et al. (2016) proposed an MT-based ATS system for English built upon Newsela and the large paraphrase database (Pavlick and Callison-Burch, 2016). The manual sentence alignment of English Newsela (Xu et al., 2015), improved automatic alignment of EW-SEW corpus (Hwang et al., ˇ 2015), and the recently released free tools for sentence alignment (Paetzold et al., 2017; Stajner et al., ˇ 2017; Stajner et al., 2018), offered new opportunities for data-driven ATS. In 2017, several ATS systems exploring various deep learning architectures appeared, using the new alignments of Wikipedia and Newsela for training. Sequence-to-sequence neural models (Nisioi et al., ˇ 2017; Stajner and Nisioi, 2018), and the neural model based on reinforcement learning techniques (Zhang and Lapata, 2017) showed a dominance of neural ATS approaches over the previous data-driven approaches in terms of quality of generated output (better grammaticality and meaning preservation). The q"
C18-3005,P16-2024,0,0.0388999,"l of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed idea of approaching ATS as a monolingual machine translation (MT) task (Specia, 2010; Coster and Kauchak, 2011), Xu et al. (2016) proposed an MT-based ATS system for English built upon Newsela and the large paraphrase database (Pavlick and Callison-Burch, 2016). The manual sentence alignment of English Newsela (Xu et al., 2015), improved automatic alignment of EW-SEW corpus (Hwang et al., ˇ 2015), and the recently released free tools for sentence alignment (Paetzold et al., 2017; Stajner et al., ˇ 2017; Stajner et al., 2018), offered new opportunities for data-driven ATS. In 2017, several ATS systems exploring various deep learning architectures appeared, using the new alignments of Wikipedia and Newsela for training. Sequence-to-sequence neural models (Nisioi et al., ˇ 2017; Stajner and Nisioi, 2018), and the neural model based on reinforcement lea"
C18-3005,E14-1076,0,0.070711,"d to transform written texts in each of those projects and make an in-depth discussion of what those projects had in common in terms of techniques and resources, and in what they differed. 19 1.1 Data-driven Paradigm in Simplification With the emergence of Simple English Wikipedia and its (comparable) alignment with English Wikipedia, which for the first time offered a large parallel dataset for training of the ATS systems, the approaches to ATS shifted from rule-based (Siddharthan, 2006) to purely data-driven (Coster and Kauchak, 2011; Zhu et al., 2010; Kauchak, 2013), and later hybrid ones (Siddharthan and Mandya, 2014). It created opportunity for stronger NLP component of the systems and new challenges in text/sentence generation, but at the cost of blurring the final goal of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed idea of approaching ATS as a monolingual machine translation (MT) task ("
C18-3005,L18-1479,1,0.769042,"Missing"
C18-3005,P17-2016,1,0.885434,"Missing"
C18-3005,L18-1615,1,0.815163,"implifications at five different text complexity levels. Following the previously proposed idea of approaching ATS as a monolingual machine translation (MT) task (Specia, 2010; Coster and Kauchak, 2011), Xu et al. (2016) proposed an MT-based ATS system for English built upon Newsela and the large paraphrase database (Pavlick and Callison-Burch, 2016). The manual sentence alignment of English Newsela (Xu et al., 2015), improved automatic alignment of EW-SEW corpus (Hwang et al., ˇ 2015), and the recently released free tools for sentence alignment (Paetzold et al., 2017; Stajner et al., ˇ 2017; Stajner et al., 2018), offered new opportunities for data-driven ATS. In 2017, several ATS systems exploring various deep learning architectures appeared, using the new alignments of Wikipedia and Newsela for training. Sequence-to-sequence neural models (Nisioi et al., ˇ 2017; Stajner and Nisioi, 2018), and the neural model based on reinforcement learning techniques (Zhang and Lapata, 2017) showed a dominance of neural ATS approaches over the previous data-driven approaches in terms of quality of generated output (better grammaticality and meaning preservation). The question of simplicity of the generated output a"
C18-3005,Q15-1021,0,0.0193383,"ble) alignment with English Wikipedia, which for the first time offered a large parallel dataset for training of the ATS systems, the approaches to ATS shifted from rule-based (Siddharthan, 2006) to purely data-driven (Coster and Kauchak, 2011; Zhu et al., 2010; Kauchak, 2013), and later hybrid ones (Siddharthan and Mandya, 2014). It created opportunity for stronger NLP component of the systems and new challenges in text/sentence generation, but at the cost of blurring the final goal of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed idea of approaching ATS as a monolingual machine translation (MT) task (Specia, 2010; Coster and Kauchak, 2011), Xu et al. (2016) proposed an MT-based ATS system for English built upon Newsela and the large paraphrase database (Pavlick and Callison-Burch, 2016). The manual sentence alignment of English Newsela (Xu et al., 2015), improved automatic alignme"
C18-3005,Q16-1029,0,0.0319258,"component of the systems and new challenges in text/sentence generation, but at the cost of blurring the final goal of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed idea of approaching ATS as a monolingual machine translation (MT) task (Specia, 2010; Coster and Kauchak, 2011), Xu et al. (2016) proposed an MT-based ATS system for English built upon Newsela and the large paraphrase database (Pavlick and Callison-Burch, 2016). The manual sentence alignment of English Newsela (Xu et al., 2015), improved automatic alignment of EW-SEW corpus (Hwang et al., ˇ 2015), and the recently released free tools for sentence alignment (Paetzold et al., 2017; Stajner et al., ˇ 2017; Stajner et al., 2018), offered new opportunities for data-driven ATS. In 2017, several ATS systems exploring various deep learning architectures appeared, using the new alignments of Wikipedia and Newsela for training. S"
C18-3005,D17-1062,0,0.0305122,"nce alignment of English Newsela (Xu et al., 2015), improved automatic alignment of EW-SEW corpus (Hwang et al., ˇ 2015), and the recently released free tools for sentence alignment (Paetzold et al., 2017; Stajner et al., ˇ 2017; Stajner et al., 2018), offered new opportunities for data-driven ATS. In 2017, several ATS systems exploring various deep learning architectures appeared, using the new alignments of Wikipedia and Newsela for training. Sequence-to-sequence neural models (Nisioi et al., ˇ 2017; Stajner and Nisioi, 2018), and the neural model based on reinforcement learning techniques (Zhang and Lapata, 2017) showed a dominance of neural ATS approaches over the previous data-driven approaches in terms of quality of generated output (better grammaticality and meaning preservation). The question of simplicity of the generated output and the adaptability of those models to different text genres and languages other than English, is still present. While solving the problems of grammaticality and meaning preservation, the neural TS systems introduced a new challenge, showing problems in dealing with abundance of name entities present both in news articles and Wikipedia articles. 2 Tutorial Overview In t"
C18-3005,C10-1152,0,0.0500442,"l language processing. We will present the techniques used to transform written texts in each of those projects and make an in-depth discussion of what those projects had in common in terms of techniques and resources, and in what they differed. 19 1.1 Data-driven Paradigm in Simplification With the emergence of Simple English Wikipedia and its (comparable) alignment with English Wikipedia, which for the first time offered a large parallel dataset for training of the ATS systems, the approaches to ATS shifted from rule-based (Siddharthan, 2006) to purely data-driven (Coster and Kauchak, 2011; Zhu et al., 2010; Kauchak, 2013), and later hybrid ones (Siddharthan and Mandya, 2014). It created opportunity for stronger NLP component of the systems and new challenges in text/sentence generation, but at the cost of blurring the final goal of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed id"
I13-1043,W03-1004,0,0.0117411,"vances of natural language processing (NLP) tools and techniques, new approaches to readability assessment have emerged. Schwarm and Ostendorf (2005), and Petersen and Ostendorf (2009), used statistical language modeling and support vector machines to show that more complex features (e.g. average height of the parse tree, average number of noun and verb phrases, etc.) give better readability prediction than the traditional Flesch-Kincaid readability formula. They based their approach on the texts from Weekly Reader4 , and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features which should improve automatic readability assessment of texts for people with cognitive disoped for English (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), and Dutch (Ruiter et al., 2010). With the emergence of these systems, the question we are faced with is how to auto"
I13-1043,E99-1042,0,0.089991,"es indicate that lexically and syntactically complex texts can be very difficult for non-native speakers and people with various reading impairments (e.g. autistic, aphasic, dyslexic or deaf people). Aphasic people, for instance, may encounter problems with less frequent words and some particular sentence constructions (Devlin, 1999). They also have problems in understanding syntactic constructions which do not follow the canonical subject-verbobject structure (e.g. passive constructions), and especially those sentences which are semantically reversible, e.g. “The boy was kissed by the girl” (Carroll et al., 1999). Additionally, aphasic readers may have additional problems with 1 http://www.plainlanguage.gov/ http://november5th.net/resources/Mencap/MakingMyself-Clear.pdf 3 http://www.w3.org/TR/WCAG20/ 2 374 International Joint Conference on Natural Language Processing, pages 374–382, Nagoya, Japan, 14-18 October 2013. (Automated Readability Index (Smith and Senter, 1967)), or US healthcare documents intended for the general public (the SMOG grading (McLaughlin, 1969)). Some of these first readability formulae are still widely in use, given their simplicity (they require only the average sentence and wo"
I13-1043,W11-1601,0,0.0181138,"tures (e.g. average height of the parse tree, average number of noun and verb phrases, etc.) give better readability prediction than the traditional Flesch-Kincaid readability formula. They based their approach on the texts from Weekly Reader4 , and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features which should improve automatic readability assessment of texts for people with cognitive disoped for English (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), and Dutch (Ruiter et al., 2010). With the emergence of these systems, the question we are faced with is how to automatically evaluate their performance given that the access to the target users might be difficult. This study is an attempt to address this issue. We focus on text simplification systems for Spanish and investigate whether some of the already existing readability indices cou"
I13-1043,E09-1027,0,0.0758898,"to readability assessment have emerged. Schwarm and Ostendorf (2005), and Petersen and Ostendorf (2009), used statistical language modeling and support vector machines to show that more complex features (e.g. average height of the parse tree, average number of noun and verb phrases, etc.) give better readability prediction than the traditional Flesch-Kincaid readability formula. They based their approach on the texts from Weekly Reader4 , and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features which should improve automatic readability assessment of texts for people with cognitive disoped for English (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), and Dutch (Ruiter et al., 2010). With the emergence of these systems, the question we are faced with is how to automatically evaluate their performance given that the access to the targe"
I13-1043,van-oosten-etal-2010-towards,0,0.0704143,"Missing"
I13-1043,D11-1038,0,0.0180526,"of the parse tree, average number of noun and verb phrases, etc.) give better readability prediction than the traditional Flesch-Kincaid readability formula. They based their approach on the texts from Weekly Reader4 , and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features which should improve automatic readability assessment of texts for people with cognitive disoped for English (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), and Dutch (Ruiter et al., 2010). With the emergence of these systems, the question we are faced with is how to automatically evaluate their performance given that the access to the target users might be difficult. This study is an attempt to address this issue. We focus on text simplification systems for Spanish and investigate whether some of the already existing readability indices could be used for the automati"
I13-1043,P12-1107,0,0.0282904,"Missing"
I13-1043,C10-1152,0,0.0149525,"t more complex features (e.g. average height of the parse tree, average number of noun and verb phrases, etc.) give better readability prediction than the traditional Flesch-Kincaid readability formula. They based their approach on the texts from Weekly Reader4 , and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features which should improve automatic readability assessment of texts for people with cognitive disoped for English (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), and Dutch (Ruiter et al., 2010). With the emergence of these systems, the question we are faced with is how to automatically evaluate their performance given that the access to the target users might be difficult. This study is an attempt to address this issue. We focus on text simplification systems for Spanish and investigate whether some of the already existi"
I13-1043,ruiter-etal-2010-human,0,0.0243733,"and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features which should improve automatic readability assessment of texts for people with cognitive disoped for English (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), and Dutch (Ruiter et al., 2010). With the emergence of these systems, the question we are faced with is how to automatically evaluate their performance given that the access to the target users might be difficult. This study is an attempt to address this issue. We focus on text simplification systems for Spanish and investigate whether some of the already existing readability indices could be used for the automatic evaluation of these systems. Using a corpus of original news texts and their manual simplifications which followed specific guidelines for writing for people with cognitive disabilities, we show that two lexical"
I13-1043,P05-1065,0,0.206827,"ae for Dutch represent the adaptations of the Flesch Reading Ease score, while Spaulding’s Spanish readability formula (Spaulding, 1956) could be seen as an adaptation of the Dale-Chall formula (Dale and Chall, 1948)). Oosten et al. (2010) showed that readability formulae which are solely based on superficial text characteristics (average sentence and word length) seem to be strongly correlated even across different languages (English, Dutch, and Swedish). With the recent advances of natural language processing (NLP) tools and techniques, new approaches to readability assessment have emerged. Schwarm and Ostendorf (2005), and Petersen and Ostendorf (2009), used statistical language modeling and support vector machines to show that more complex features (e.g. average height of the parse tree, average number of noun and verb phrases, etc.) give better readability prediction than the traditional Flesch-Kincaid readability formula. They based their approach on the texts from Weekly Reader4 , and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features"
I17-2068,P15-2011,1,0.911458,"Missing"
I17-2068,W14-1214,0,0.0544009,"Missing"
I17-2068,P14-2075,0,0.413365,"with simpler words, e.g. (Paetzold and Specia, 2016b), which seems to significantly improve the results (Paetzold and Specia, 2015). Most LS systems focus on simplifying news articles (Alu´ısio et al., 2008; Carroll et al., 1999; ˇ Saggion et al., 2015; Glavaˇs and Stajner, 2015). However, only small amounts of newswire texts are available that contain annotations for manual simplifications. Most LS systems rely on sentence alignments between English Wikipedia and English Simple Wikipedia (Coster and Kauchak, 2011). Thus, existing CWI datasets cover mostly the Wikipedia Genre (Shardlow, 2013; Horn et al., 2014; Paetzold and Specia, 2016a). We collect a new CWI dataset (CWIG3G2) covering three genres: professionally written news articles, amateurishly written news articles (WikiNews), and Wikipedia articles. Then, we test whether or not the complex word (CW) annotations collected on one genre can be used for Complex word identification (CWI) is an important task in text accessibility. However, due to the scarcity of CWI datasets, previous studies have only addressed this problem on Wikipedia sentences and have solely taken into account the needs of non-native English speakers. We collect a new CWI d"
I17-2068,C12-1023,0,0.444938,"Missing"
I17-2068,P13-1151,0,0.311838,"r genre and also explore if the native and non-native user groups share the same lexical-semantic simplification needs. 2 Related Work Previous datasets relied on Simple Wikipedia and edit histories as a ‘gold standard’ annotation of CWs, despite the fact that the use of Simple Wikipedia as a ‘gold standard’ for text simplification has been disputed (Amancio and Specia, 2014; Xu et al., 2015). Currently, the largest CWI dataset is the SemEval-2016 (Task 11) dataset (Paetzold and Specia, 2016a). It consists of 9,200 sentences collected from previous datasets (Shardlow, 2013; Horn et al., 2014; Kauchak, 2013). For the creation of the SemEval-2016 CWI dataset, annotators were asked to annotate (only) one word within a given sentence as complex or not. In the training set (200 sentences), each target word was annotated by 20 people, whilst in the test set (9,000 sentences) each target word was annotated by a single annotator from a pool of 400 annotators. The goal of the shared task was to predict the complexity of a word for a non-native speaker based on the annotations of a larger group of nonnative speakers. This introduced strong biases and inconsistencies in the test set, resulting in very low"
I17-2068,E99-1042,0,0.284754,"t LS systems have the functionality of replacing potentially complex words with synonyms or related words that are easier to understand and yet still fit into context. Some of these systems treat all content words in a text as potentially difficult words, e.g. (Horn et al., ˇ 2014; Glavaˇs and Stajner, 2015). Other systems try to detect complex words first and then perform the replacement with simpler words, e.g. (Paetzold and Specia, 2016b), which seems to significantly improve the results (Paetzold and Specia, 2015). Most LS systems focus on simplifying news articles (Alu´ısio et al., 2008; Carroll et al., 1999; ˇ Saggion et al., 2015; Glavaˇs and Stajner, 2015). However, only small amounts of newswire texts are available that contain annotations for manual simplifications. Most LS systems rely on sentence alignments between English Wikipedia and English Simple Wikipedia (Coster and Kauchak, 2011). Thus, existing CWI datasets cover mostly the Wikipedia Genre (Shardlow, 2013; Horn et al., 2014; Paetzold and Specia, 2016a). We collect a new CWI dataset (CWIG3G2) covering three genres: professionally written news articles, amateurishly written news articles (WikiNews), and Wikipedia articles. Then, we"
I17-2068,P11-2117,0,0.10862,"aˇs and Stajner, 2015). Other systems try to detect complex words first and then perform the replacement with simpler words, e.g. (Paetzold and Specia, 2016b), which seems to significantly improve the results (Paetzold and Specia, 2015). Most LS systems focus on simplifying news articles (Alu´ısio et al., 2008; Carroll et al., 1999; ˇ Saggion et al., 2015; Glavaˇs and Stajner, 2015). However, only small amounts of newswire texts are available that contain annotations for manual simplifications. Most LS systems rely on sentence alignments between English Wikipedia and English Simple Wikipedia (Coster and Kauchak, 2011). Thus, existing CWI datasets cover mostly the Wikipedia Genre (Shardlow, 2013; Horn et al., 2014; Paetzold and Specia, 2016a). We collect a new CWI dataset (CWIG3G2) covering three genres: professionally written news articles, amateurishly written news articles (WikiNews), and Wikipedia articles. Then, we test whether or not the complex word (CW) annotations collected on one genre can be used for Complex word identification (CWI) is an important task in text accessibility. However, due to the scarcity of CWI datasets, previous studies have only addressed this problem on Wikipedia sentences an"
I17-2068,S16-1151,0,0.0616333,"(F-score) system Features We use four different categories of features. Frequency and length features: Due to the common use of these features in selecting the most simple lexical substitution candidate (Bott et al., ˇ 2012; Glavaˇs and Stajner, 2015), we use three length features: the number of vowels (vow), syllables (syl), and characters (len) and three frequency features: the frequency of the word in Simple Wikipedia (sim), the frequency of the word in the paragraph (of HIT) (wf p), and the frequency of the word in the Google Web 1T 5-Grams (wbt). Syntactic features: Based on the work of Davoodi and Kosseim (2016), the part of speech (POS) tag influences the complexity of the word. We used POS tags (pos) predicted by the Stanford POS tagger (Toutanova et al., 2003). Word embeddings features: Following the work ˇ of Glavaˇs and Stajner (2015), as well as Paetzold and Specia (2016b), we train a word2vec model (Mikolov et al., 2013) using English Wikipedia and the AQUAINT corpus of English news texts (Graff, 2002). We train 200-dimensional embeddings using skip-gram training and a window size of 5. We use the word2vec representations of CPs as a feature (emb), and also compute the cosine similarities betw"
I17-2068,P15-4015,0,0.0681752,"a certain level of difficulty and output a text in a simplified form without changing its meaning. Most LS systems have the functionality of replacing potentially complex words with synonyms or related words that are easier to understand and yet still fit into context. Some of these systems treat all content words in a text as potentially difficult words, e.g. (Horn et al., ˇ 2014; Glavaˇs and Stajner, 2015). Other systems try to detect complex words first and then perform the replacement with simpler words, e.g. (Paetzold and Specia, 2016b), which seems to significantly improve the results (Paetzold and Specia, 2015). Most LS systems focus on simplifying news articles (Alu´ısio et al., 2008; Carroll et al., 1999; ˇ Saggion et al., 2015; Glavaˇs and Stajner, 2015). However, only small amounts of newswire texts are available that contain annotations for manual simplifications. Most LS systems rely on sentence alignments between English Wikipedia and English Simple Wikipedia (Coster and Kauchak, 2011). Thus, existing CWI datasets cover mostly the Wikipedia Genre (Shardlow, 2013; Horn et al., 2014; Paetzold and Specia, 2016a). We collect a new CWI dataset (CWIG3G2) covering three genres: professionally writte"
I17-2068,E09-1027,0,0.1301,"Missing"
I17-2068,W16-4912,0,0.127728,"lification (LS) of texts have been proposed. LS systems take as input a text of a certain level of difficulty and output a text in a simplified form without changing its meaning. Most LS systems have the functionality of replacing potentially complex words with synonyms or related words that are easier to understand and yet still fit into context. Some of these systems treat all content words in a text as potentially difficult words, e.g. (Horn et al., ˇ 2014; Glavaˇs and Stajner, 2015). Other systems try to detect complex words first and then perform the replacement with simpler words, e.g. (Paetzold and Specia, 2016b), which seems to significantly improve the results (Paetzold and Specia, 2015). Most LS systems focus on simplifying news articles (Alu´ısio et al., 2008; Carroll et al., 1999; ˇ Saggion et al., 2015; Glavaˇs and Stajner, 2015). However, only small amounts of newswire texts are available that contain annotations for manual simplifications. Most LS systems rely on sentence alignments between English Wikipedia and English Simple Wikipedia (Coster and Kauchak, 2011). Thus, existing CWI datasets cover mostly the Wikipedia Genre (Shardlow, 2013; Horn et al., 2014; Paetzold and Specia, 2016a). We"
I17-2068,W13-2908,0,0.423947,"the replacement with simpler words, e.g. (Paetzold and Specia, 2016b), which seems to significantly improve the results (Paetzold and Specia, 2015). Most LS systems focus on simplifying news articles (Alu´ısio et al., 2008; Carroll et al., 1999; ˇ Saggion et al., 2015; Glavaˇs and Stajner, 2015). However, only small amounts of newswire texts are available that contain annotations for manual simplifications. Most LS systems rely on sentence alignments between English Wikipedia and English Simple Wikipedia (Coster and Kauchak, 2011). Thus, existing CWI datasets cover mostly the Wikipedia Genre (Shardlow, 2013; Horn et al., 2014; Paetzold and Specia, 2016a). We collect a new CWI dataset (CWIG3G2) covering three genres: professionally written news articles, amateurishly written news articles (WikiNews), and Wikipedia articles. Then, we test whether or not the complex word (CW) annotations collected on one genre can be used for Complex word identification (CWI) is an important task in text accessibility. However, due to the scarcity of CWI datasets, previous studies have only addressed this problem on Wikipedia sentences and have solely taken into account the needs of non-native English speakers. We"
I17-2068,N03-1033,0,0.00671505,"the most simple lexical substitution candidate (Bott et al., ˇ 2012; Glavaˇs and Stajner, 2015), we use three length features: the number of vowels (vow), syllables (syl), and characters (len) and three frequency features: the frequency of the word in Simple Wikipedia (sim), the frequency of the word in the paragraph (of HIT) (wf p), and the frequency of the word in the Google Web 1T 5-Grams (wbt). Syntactic features: Based on the work of Davoodi and Kosseim (2016), the part of speech (POS) tag influences the complexity of the word. We used POS tags (pos) predicted by the Stanford POS tagger (Toutanova et al., 2003). Word embeddings features: Following the work ˇ of Glavaˇs and Stajner (2015), as well as Paetzold and Specia (2016b), we train a word2vec model (Mikolov et al., 2013) using English Wikipedia and the AQUAINT corpus of English news texts (Graff, 2002). We train 200-dimensional embeddings using skip-gram training and a window size of 5. We use the word2vec representations of CPs as a feature (emb), and also compute the cosine similarities between the vector representations of CP and the paragraph (cosP ) and the sentence which contains it (cosS). The paragraph and sentence representations are c"
I17-2068,S16-1146,0,0.110144,"Missing"
I17-2068,Q15-1021,0,0.0339024,"Missing"
I17-2068,yimam-etal-2017-multilingual,1,0.489571,"Missing"
L16-1094,costa-etal-2014-translation,0,0.0331141,"Missing"
L16-1094,W11-2123,0,0.0171962,"(Silva et al., 2015) developed under the QTLeap project. For the PBSMT system, we used the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 5 We would like to thank Eleftherios Avramidis and Lukas Poustka for making the LibreOffice corpus available to us. 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). All PBSMT systems were tuned on the IT1 corpus using minimum error rate training (MERT) (Och, 2003). Their language models were built on a 2,121,382 sentence corpus (target side of the full EP+FAPESP corpora) using the KenLM (Heafield, 2011) 5-gram language model. The stack size was limited to 100 hypotheses during decoding. For each of the two systems (TectoMT and PBSMT), we performed four baseline experiments (using the EP1, EP2, FAPESP, or IT1 corpus as the training dataset) and five experiments which exploited three different strategies for enlarging the small in-domain training dataset (IT1) by adding: (1) a larger out-of-domain dataset (IT1+EP2 or IT1+FAPESP), (2) quasi in-domain data (IT1+TERM), and (3) a combination of both (IT1+EP2+TERM or IT1+FAPESP+TERM). 594 Group TectoMT better rated than PBSMT TectoMT and PBSMT rate"
L16-1094,N03-1017,0,0.0218682,"ortuguese translation using two different approaches: a hybrid MT system (TectoMT) and a PBSMT system in the Moses toolkit. All models were tested on the same dataset (IT2). For the TectoMT system, we used the English to Portuguese TectoMT system (Silva et al., 2015) developed under the QTLeap project. For the PBSMT system, we used the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 5 We would like to thank Eleftherios Avramidis and Lukas Poustka for making the LibreOffice corpus available to us. 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). All PBSMT systems were tuned on the IT1 corpus using minimum error rate training (MERT) (Och, 2003). Their language models were built on a 2,121,382 sentence corpus (target side of the full EP+FAPESP corpora) using the KenLM (Heafield, 2011) 5-gram language model. The stack size was limited to 100 hypotheses during decoding. For each of the two systems (TectoMT and PBSMT), we performed four baseline experiments (using the EP1, EP2, FAPESP, or IT1 corpus as the training dataset) and five experiments which exploited three different strategies for enlarging the small in-domain training dataset"
L16-1094,P07-2045,0,0.0302813,"and-crafted rules for the synthesis phase of the hybrid MT system being developed. This QTLeap corpus consists of recorded interactions of real users with experts to obtain technical support via chat, which were translated by professional translators into the eight languages of the project.2 We perform a series of MT experiments using two sysˇ tems: (1) the TectoMT (Zabokrtsk´ y et al., 2008) adapted to English-Portuguese translation (Silva et al., 2015), as a hybrid system with hybrid analysis, rule-based synthesis and statistically based transfer, and (2) the standard PBSMT system in Moses (Koehn et al., 2007). We vary the training datasets exploring three different strategies to overcome the problem of small amount of training data: (1) adding larger out-of-domain dataset, (2) adding in-domain bilingual terminology, (3) adding both. The main contribution of this paper is the in-depth error analysis, showing the error patterns in each of the systems (TectoMT and PBSMT) when trained on the same datasets, thus directly contrasting those two approaches and showing the main advantages of the hybrid MT system. We further propose a set of rules for improving the synthesis stage in the TectoMT system to e"
L16-1094,2009.mtsummit-papers.7,0,0.0147301,"ransforming the translated t-tree into an a-tree and then linearise the a-tree into a plain surface form of the output sentence. These modules are language-specific and take care of word order, agreement (e.g. subject-predicate agreement or noun-adjective agreement), insertion of grammatical words (such as prepositions, articles, particles, etc.), inflections, and capitalisation. 2.2. English to Portuguese Machine Translation The studies concerning EN-PT MT are very scarce and mostly report on results of the PBSMT systems. The best results (BLEU = 0.55) were obtained on the JRC-Acquis corpus (Koehn et al., 2009), followed by the results obtained using a significantly smaller FAPESP corpus (Aziz and Specia, 2011) of scientific news texts (BLEU = 0.46). The PBSMT systems trained on Europarl corpus (and interpolated with models trained on datasets from the same domain as the test datasets) and tested on domain-specific corpora – TED talks and TAP (Portuguese airline) magazine – achieved significantly lower BLEU scores, 0.20 and 0.19 respectively (Costa et al., 2014). Google Translate achieved better, but still not very high, BLEU scores (0.28 and 0.26, respectively) on the same task (Costa et al., 2014)"
L16-1094,W04-3250,0,0.112153,"ESP IT1 TERM / / / / / / 162,350 / / / 2,000 / / 2,000 / 162,350 2,000 / / 2,000 14,025 / 2,000 14,025 162,350 2,000 178,375 Total 1,960,407 162,350 162, 350 2,000 164,350 164,350 16,025 178,375 14,025 BLEU score TectoMT PBSMT 19.34 18.99 17.76 17.53 19.43 19.20 20.77 21.55 19.77 20.79 20.53 *22.64 *21.89 *22.73 *21.04 *22.25 *21.63 *23.53 Table 2: Number of sentence pairs (terms and MWE in the case of the TERM corpus) used for the training and the achieved BLEU score. Best results of each system are shown in bold. Results of the systems which significantly (using paired bootstrap resampling (Koehn, 2004)) outperformed all four baselines are shown with an ‘*’. and a small portion of LibreOffice terminology (995 terms).5 Several examples from each corpus are given in Table 1. 3.2. Experimental Setup We performed a series of MT experiments for English to Portuguese translation using two different approaches: a hybrid MT system (TectoMT) and a PBSMT system in the Moses toolkit. All models were tested on the same dataset (IT2). For the TectoMT system, we used the English to Portuguese TectoMT system (Silva et al., 2015) developed under the QTLeap project. For the PBSMT system, we used the GIZA++ i"
L16-1094,2005.mtsummit-papers.11,0,0.121199,"l., 2014). There have been two studies comparing a hybrid and a PBSMT system for EN-PT language pair (Silva et al., 2015; ˇ Stajner et al., 2015), reporting the performances of the two approaches as comparable. None of those studies, however, performs an error analysis to directly compare the errors made by those systems. 3. Machine Translation Experiments The corpora used in MT experiments, experimental setup and the results of the automatic evaluation of all MT systems are presented in the next three subsections. 3.1. Corpora We used six corpora in this study: 1. EP1 – Full Europarl corpus (Koehn, 2005) with English on the source side and Portuguese on the target side (1,960,407 sentence pairs). 2. EP2 – A smaller portion of Europarl corpus which has the same size as the FAPESP corpus (162,350 sentence pairs). 3. FAPESP – A Portuguese-English bilingual collection of the online issue of the scientific news Brazilian magazine “Revista Pesquisa FAPESP”3 (Aziz and Specia, 2011). 4. IT1 – An in-domain IT corpus with 2,000 sentence pairs (1,000 questions and 1,000 answers) compiled under the QTLeap project (QTLeap corpus, batch 1). This corpus was used as the training dataset (or a part of the tra"
L16-1094,W10-1730,0,0.652819,"Missing"
L16-1094,J03-1002,0,0.0100679,"Missing"
L16-1094,P03-1021,0,0.0177018,"oses toolkit. All models were tested on the same dataset (IT2). For the TectoMT system, we used the English to Portuguese TectoMT system (Silva et al., 2015) developed under the QTLeap project. For the PBSMT system, we used the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 5 We would like to thank Eleftherios Avramidis and Lukas Poustka for making the LibreOffice corpus available to us. 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). All PBSMT systems were tuned on the IT1 corpus using minimum error rate training (MERT) (Och, 2003). Their language models were built on a 2,121,382 sentence corpus (target side of the full EP+FAPESP corpora) using the KenLM (Heafield, 2011) 5-gram language model. The stack size was limited to 100 hypotheses during decoding. For each of the two systems (TectoMT and PBSMT), we performed four baseline experiments (using the EP1, EP2, FAPESP, or IT1 corpus as the training dataset) and five experiments which exploited three different strategies for enlarging the small in-domain training dataset (IT1) by adding: (1) a larger out-of-domain dataset (IT1+EP2 or IT1+FAPESP), (2) quasi in-domain data"
L16-1094,P02-1040,0,0.101156,"c¸a˜ o” instead of “ir a codificac¸a˜ o” “escolha a canal” instead of “escolha o canal” “junto de sua perfil” instead of “junto ao seu perfil” “bot˜oes que diz” instead of “dizem” Open Network Broadcast → “Difus˜ao de rede de abrir”, instead of “Abrir Difus˜ao de Rede” Table 4: Classification of adequacy and fluency errors. 3.3. Results of the Translation Experiments The training datasets of each of the nine experiments (performed for both MT systems) are presented in Table 2, together with the results of the automatic evaluation of both systems (TectoMT and PBSMT) in terms of the BLEU score (Papineni et al., 2002). Our results indicated that addition of a larger out-of-domain corpus only improves the performance of the PBSMT system, while it deteriorates the performance of the TectoMT system. The TectoMT achieves best improvements with addition of the bilingual terminology only. 4. Hybrid vs Statistical MT System It is known that BLEU scores cannot be used for comparing two MT systems with different architectures (TectoMT and PBSMT in our case), but only for comparing different versions of the same system (either PBSMT or TectoMT in our case). Therefore, we focused on the results of the IT1+TERM experi"
L16-1094,W15-4101,1,0.850808,"the EN-PT language pair exists for the IT domain, a small ENPT corpus was compiled under the QTLeap project in order to enable in-domain examples and guide the hand-crafted rules for the synthesis phase of the hybrid MT system being developed. This QTLeap corpus consists of recorded interactions of real users with experts to obtain technical support via chat, which were translated by professional translators into the eight languages of the project.2 We perform a series of MT experiments using two sysˇ tems: (1) the TectoMT (Zabokrtsk´ y et al., 2008) adapted to English-Portuguese translation (Silva et al., 2015), as a hybrid system with hybrid analysis, rule-based synthesis and statistically based transfer, and (2) the standard PBSMT system in Moses (Koehn et al., 2007). We vary the training datasets exploring three different strategies to overcome the problem of small amount of training data: (1) adding larger out-of-domain dataset, (2) adding in-domain bilingual terminology, (3) adding both. The main contribution of this paper is the in-depth error analysis, showing the error patterns in each of the systems (TectoMT and PBSMT) when trained on the same datasets, thus directly contrasting those two a"
L16-1094,W15-5713,1,0.813691,"Missing"
L16-1094,W08-0325,0,0.55815,"Missing"
L16-1438,W13-2201,0,0.0355127,"Maximum Entropy context-sensitive translation models (Mareˇcek et al., 2010). The analysis phase (which converts the input sentence into the a-layer and then the t-layer) and the synthesis phase (which converts the translated t-layer representation to the a-layer and then to the output surface string) are mostly rule-based. They use a modular structure, allowing for its components to be inherited, and adapted, for different languages. Figure 1: TectoMT architecture. The English-Czech TectoMT system achieved very good results on the Workshop on Statistical Machine Translation 2013 shared task (Bojar et al., 2013). Those results, together with the widespread concerns that the state-of-theart PBSMT may be reaching a performance ceiling, make us believe that the use of the linguistically rich MT approaches, such as TectoMT, could be a good way forward towards higher quality MT. In order to adapt the English-Czech TectoMT to a new language pair, it is only necessary to adapt the synthesis phase of the original system and train a transfer model (handled by the tree-to-tree maximum entropy translation model (Mareˇcek et al., 2010)) which takes around two weeks to train over the whole Europarl corpus. The bl"
L16-1438,E06-2024,1,0.810503,"d the formeme to help separating the lexical from the syntactic information. For example, the formeme of the semantic noun in a subject position is represented as n:sub. For the morphological categories the grammateme tectogrammatical representation is ˇ used (Zabokrtsk´ y et al., 2008). For the Portuguese synthesis adaptation, the process begun by adapting the existing English blocks (Popel and ˇ Zabokrtsk` y, 2010), with the corresponding Portuguese linguistic phenomena. The next step was to create new blocks for Portuguese-specific phenomena. We resorted to the existing tools developed by (Branco and Silva, 2006) whenever possible, due to the higher accuracy over the available tools within the original TectoMT system. For verbal conjugation and for nominal inflection, we used the LX-Conjugator and LX-Inflector to generate surface forms (described in more details in Section 3.1.). We created new TectoMT blocks to call and incorporate these tools into the TectoMT pipeline. The English to Portuguese TectoMT system was being improved iteratively, controlling for the BLEU score and human error analysis of 1,000 sentences in each step. The set of 1,000 sentences (QTLEAP-IT1) was compiled under the QTLeap pr"
L16-1438,P07-2045,0,0.00679985,"r the translation from English to Portuguese, and discuss the findings of the conducted error analysis. As the test corpus, we use 1, 000 parallel sentences from the IT domain (QTLEAP-IT2), compiled under the QTLeap project in a similar way as the 1, 000 parallel sentences used for the development of the synthesis phase (none of the 1, 000 test sentences can be found among the 1, 000 sentences used for development). For the automatic evaluation, we use the BLEU score (Papineni et al., 2002) and compare our TectoMT system with the standard phrase-based SMT system built using the Moses toolkit (Koehn et al., 2007). Both systems were trained over the full Europarl corpus (Koehn, 2005), consisting of 1,960,407 sentence pairs. The PBSMT was tuned using the QTLEAP-IT1 dataset (the same dataset which was used for the development of the rules for the synthesis phase of the TectoMT system). As can be seen from the results presented in Table 1, our adaptation of TectoMT system to EN→PT translation task significantly outperforms the standard PBSMT system on the IT test set (the difference is statistically significant at a 0.05 level of significance, using the paired bootstrap resampling (Koehn, 2004)). Comparin"
L16-1438,W04-3250,0,0.08534,"it (Koehn et al., 2007). Both systems were trained over the full Europarl corpus (Koehn, 2005), consisting of 1,960,407 sentence pairs. The PBSMT was tuned using the QTLEAP-IT1 dataset (the same dataset which was used for the development of the rules for the synthesis phase of the TectoMT system). As can be seen from the results presented in Table 1, our adaptation of TectoMT system to EN→PT translation task significantly outperforms the standard PBSMT system on the IT test set (the difference is statistically significant at a 0.05 level of significance, using the paired bootstrap resampling (Koehn, 2004)). Comparing to other languages pairs translation (Rosa et al., 2015) using TectoMT, the translation from English to Portuguese falls within the same range of score values. Other Blocks CopyTtree performs a deep-copy of the current transfer ttrees into an a-tree. CliticExceptions handles the clitics in Portuguese. MarkSubject fills the Afun label (analytical functions which correspond to syntactic functions such as subject, predicate, object and attribute) with the Sb attribute marking a subject. The values are read from the formeme. For Portuguese, if the formeme is a possessive noun or a nou"
L16-1438,2005.mtsummit-papers.11,0,0.651924,"in the case of domain-specific MT for which there is not enough parallel data for training a statistical machine translation system. Keywords: Hybrid Machine Translation, Tecto MT, Portuguese Synthesis 1. Introduction Phrase-based statistical machine translation (PBSMT) is considered as state-of-the-art MT approach whenever sufficiently large parallel (or comparable) datasets for training are available. However, for many language pairs and translation directions (English to Portuguese among them) large training datasets only exists for few domains, such as parliamentary discussions (Europarl (Koehn, 2005)) or legal documents (JRC-Acquis corpus (Steinberger et al., 2006)). In such cases, it is assumed that a rule-based or a hybrid MT system lead to better results as it can better overcome the data sparsity and generalise over the unseen word forms (especially useful in the case of morphologically rich languages). The main concern is, however, that adaptation of a rule-based or a hybrid MT system (its rules) to a new language pair or a new domain may require considerable time and effort. In this paper, we focus on a hybrid MT system (TectoMT ˇ (Popel and Zabokrtsk` y, 2010)) and show that the ad"
L16-1438,W10-1730,0,0.655214,"Missing"
L16-1438,P02-1040,0,0.0968438,"report on the automatic evaluation of the full translation pipeline (which contains analysis, transfer and synthesis phases) for the translation from English to Portuguese, and discuss the findings of the conducted error analysis. As the test corpus, we use 1, 000 parallel sentences from the IT domain (QTLEAP-IT2), compiled under the QTLeap project in a similar way as the 1, 000 parallel sentences used for the development of the synthesis phase (none of the 1, 000 test sentences can be found among the 1, 000 sentences used for development). For the automatic evaluation, we use the BLEU score (Papineni et al., 2002) and compare our TectoMT system with the standard phrase-based SMT system built using the Moses toolkit (Koehn et al., 2007). Both systems were trained over the full Europarl corpus (Koehn, 2005), consisting of 1,960,407 sentence pairs. The PBSMT was tuned using the QTLEAP-IT1 dataset (the same dataset which was used for the development of the rules for the synthesis phase of the TectoMT system). As can be seen from the results presented in Table 1, our adaptation of TectoMT system to EN→PT translation task significantly outperforms the standard PBSMT system on the IT test set (the difference"
L16-1438,W15-5711,0,0.420349,"Missing"
L16-1438,steinberger-etal-2006-jrc,0,0.0351353,"not enough parallel data for training a statistical machine translation system. Keywords: Hybrid Machine Translation, Tecto MT, Portuguese Synthesis 1. Introduction Phrase-based statistical machine translation (PBSMT) is considered as state-of-the-art MT approach whenever sufficiently large parallel (or comparable) datasets for training are available. However, for many language pairs and translation directions (English to Portuguese among them) large training datasets only exists for few domains, such as parliamentary discussions (Europarl (Koehn, 2005)) or legal documents (JRC-Acquis corpus (Steinberger et al., 2006)). In such cases, it is assumed that a rule-based or a hybrid MT system lead to better results as it can better overcome the data sparsity and generalise over the unseen word forms (especially useful in the case of morphologically rich languages). The main concern is, however, that adaptation of a rule-based or a hybrid MT system (its rules) to a new language pair or a new domain may require considerable time and effort. In this paper, we focus on a hybrid MT system (TectoMT ˇ (Popel and Zabokrtsk` y, 2010)) and show that the adaptation of the existing TectoMT system for English-Czech language"
L16-1438,W08-0325,0,0.135757,"Missing"
L16-1438,zeman-2008-reusable,0,0.640337,"for the gender concordance. ImposeAttrAgr sets the gender, number and person according to their governing nouns. SecondPersonPoliteness sets politeness person for Portuguese (third person). This occurs when the lemma is represented as a PersPron (all personal and possessive pronouns, including reflexive pronouns) and the corresponding t-node is in the second person. 3.1.6. Inflect Blocks GenerateWordforms generates the corresponding word forms for each lemma by using the LX-Conjugator for the verb nodes and the LX-Inflector for the adjective and nouns nodes. This block also uses the Interset (Zeman, 2008) partof-speech, number, mood, tense, person and lemma. This block also handles the inflection for the superlative degree. GeneratePronouns generates pronouns by using the formeme and the Interset person, gender and number. This block also handles the possessive, dative, accusative and oblique case pronouns. 3.1.7. An example of the a-trees and t-trees is given in the Figure 2. This example also illustrates the use of AddGender and AddPrepos blocks. From the AddGender block the node ’imagem’ gets the tag ’fem’ which will make possible to the block AddPrepos to add the preposition ’a’. In this s"
L18-1479,W11-1601,0,0.0818853,"w literacy levels, or people with various kinds of cognitive or reading impairments). It is usually applied on the sentence level and encompasses three major simplification operations: sentence splitting, deletions, and lexical paraphrases (Xu et al., 2016). In recent years, the problem of automated sentence simplification has often been addressed as the monolingual machine translation (MT) task of translating from original to simple sentences. The MT models used were, however, adapted to the specificities of the TS task, e.g. by adding phrasal deletions to the standard phrase-based MT model (Coster and Kauchak, 2011) to account for a common sentence shortening and phrasal deletions in TS, or by reranking the output of the phrase-based MT model (Wubben et al., 2012), since in the standard phrase-based MT model applied on TS, the first hypothesis tends to leave the input unchanged (Specia, 2010; Coster and Kauchak, 2011). Until recently, the state of the art for automated text simplification was the syntax-based machine translation system (SBMT) with specific optimizations for TS (Xu et al., 2016), such as the use of a large paraphrase database (PPDB) to boost the coverage of the phrasal simplifications, an"
L18-1479,N15-1022,0,0.375144,"fied versions on four different complexity levels, following strict guidelines and quality control – the Newsela corpus (Newsela, 2016)1 – has been recently released. Xu et al. (2016) show that it has better potential than the EW– SEW dataset for the TS task. However, up until recently, the Newsela corpus was only provided with alignments at the document level. We use the freely available software2 for sentenceˇ alignment across different Newsela levels (Stajner et al., ˇ 2017; Stajner et al., 2018), and then train neural text simplifications models on the sentence-aligned Newsela and EW–SEW (Hwang et al., 2015) datasets. We compare our systems with the SBMT system (Xu et al., 2016) and the recently proposed state-of-the-art reinforcement learning NTS model (Zhang and Lapata, 2017) to show that a simple neural architecture can be efficiently used for in-domain and cross-domain TS. Last but not least, we provide a detailed human and automatic evaluation of neural sequence-to-sequence models trained and tested in-domain and cross-domain on each of the two corpora, to discuss the ability of these models to generalize across registers. 2. Methodology In this section, we describe our neural text simplific"
L18-1479,P13-1151,0,0.0935183,"tokens for the original (O) and simplified (S) versions of Wikipedia dataset used to train our models. 2.2.2. Wikipedia Datasets Our Wikipedia dataset consists of the latest sentencealigned version (Hwang et al., 2015) based on manual and automatic alignments between standard English Wikipedia and Simple English Wikipedia (EW–SEW). We discard the uncategorized matches, and use only good matches and partial matches which were above the 0.45 threshold (Hwang et al., 2015), totaling to 280K aligned sentences (around 150K full matches and 130K partial matches). Unlike the earlier EW–SEW version4 (Kauchak, 2013) which only contains full matches (167K pairs), the newer dataset that we use also contains partial matches, and is thus not only larger, but also allows for learning sentence shortening transformations. From this dataset, we remove those sentence pairs whose original sentences are present in the Wikipedia test set compiled by Xu et al. (2016). We also opt for this test set, as it contains, for each of the 359 original sentences, eight manually simplified versions that can be used as multiple references for more accurate calculation of automatic evaluation scores. Statistics regarding the size"
L18-1479,P17-4012,0,0.0324154,". 2. Methodology In this section, we describe our neural text simplification (NTS) models, the datasets used for training and testing, and the evaluation procedures. 3026 * Both authors have contributed equally to this work Freely available for research upon request at Newsela.com 2 https://github.com/neosyon/SimpTextAlign 1 2.1. Following the success of neural sequence-to-sequence models in TS (Nisioi et al., 2017), our simplification systems are based on neural networks (Graves, 2012) with global attention in combination with input feeding (Luong et al., 2015). We use the OpenNMT framework (Klein et al., 2017) to train and build an architecture with two LSTM layers (Hochreiter and Schmidhuber, 1997), 500 hidden units, embedding size of 300, and 0.3 dropout probability (Srivastava et al., 2014). We train the model for 14 epochs, regardless of the dataset used, with stochastic gradient descent optimizer and a learning rate decay of 0.7 starting from epoch 7. To be able to have comparable results in-domain and across multiple corpora, we do not use pre-trained embeddings. Several changes made in the meantime for the OpenNMT framework and other third party libraries trigger different results than the o"
L18-1479,P16-1100,0,0.0217709,"i et al. (2017). To be able to have a comparable overview across systems, we set the beam size to 12 and re-generate the output of our systems. The models trained on Wikipedia together with the outputs on the same corpus are publicly released.3 It may be the case that this type of sequence-to-sequence architecture has by now become a standardized vanilla model for machine translation (Bojar et al., 2017). Nevertheless, for text simplification, we notice a few particularities that helped improve the learning. First of all, for our datasets, we do not use sub-word models (Sennrich et al., 2015; Luong and Manning, 2016), since English does not present rich morphological agglutinations. Secondly, we observe that a size of 300 for internal word embeddings is enough for both Newsela and Wikipedia datasets, the system producing lexical changes without the use of any external information. And lastly, we note the importance of keeping a reduced size of the vocabulary - no more than 50,000 words. On the one hand, this limits the amount of low frequency words that the model learns in order to produce lexical simplifications, but on the other hand, it ensures the grammaticality and meaning preservation of the output,"
L18-1479,D15-1166,0,0.0571894,"lity of these models to generalize across registers. 2. Methodology In this section, we describe our neural text simplification (NTS) models, the datasets used for training and testing, and the evaluation procedures. 3026 * Both authors have contributed equally to this work Freely available for research upon request at Newsela.com 2 https://github.com/neosyon/SimpTextAlign 1 2.1. Following the success of neural sequence-to-sequence models in TS (Nisioi et al., 2017), our simplification systems are based on neural networks (Graves, 2012) with global attention in combination with input feeding (Luong et al., 2015). We use the OpenNMT framework (Klein et al., 2017) to train and build an architecture with two LSTM layers (Hochreiter and Schmidhuber, 1997), 500 hidden units, embedding size of 300, and 0.3 dropout probability (Srivastava et al., 2014). We train the model for 14 epochs, regardless of the dataset used, with stochastic gradient descent optimizer and a learning rate decay of 0.7 starting from epoch 7. To be able to have comparable results in-domain and across multiple corpora, we do not use pre-trained embeddings. Several changes made in the meantime for the OpenNMT framework and other third p"
L18-1479,P17-2014,1,0.780559,"cture with two LSTM layers (Hochreiter and Schmidhuber, 1997), 500 hidden units, embedding size of 300, and 0.3 dropout probability (Srivastava et al., 2014). We train the model for 14 epochs, regardless of the dataset used, with stochastic gradient descent optimizer and a learning rate decay of 0.7 starting from epoch 7. To be able to have comparable results in-domain and across multiple corpora, we do not use pre-trained embeddings. Several changes made in the meantime for the OpenNMT framework and other third party libraries trigger different results than the ones reported by Nisioi et al. (2017). To be able to have a comparable overview across systems, we set the beam size to 12 and re-generate the output of our systems. The models trained on Wikipedia together with the outputs on the same corpus are publicly released.3 It may be the case that this type of sequence-to-sequence architecture has by now become a standardized vanilla model for machine translation (Bojar et al., 2017). Nevertheless, for text simplification, we notice a few particularities that helped improve the learning. First of all, for our datasets, we do not use sub-word models (Sennrich et al., 2015; Luong and Manni"
L18-1479,P02-1040,0,0.102282,"st set compiled by Xu et al. (2016). We also opt for this test set, as it contains, for each of the 359 original sentences, eight manually simplified versions that can be used as multiple references for more accurate calculation of automatic evaluation scores. Statistics regarding the size of the Wikipedia datasets are rendered in Table 3. Unlike the Newsela datasets, the Wikipedia datasets do not contain examples of sentence splitting, as the original EW–SEW dataset (Hwang et al., 2015) only contains one-to-one sentence alignments. 2.3. Evaluation Procedures 2.3.1. Automatic Evaluation BLEU (Papineni et al., 2002) is a standardized metric for machine translation evaluation that reports a similarity score between the output of a system and the ‘gold standard’ references. In this paper, we report BLEU with NIST smoothing as implemented in NLTK (Bird et al., 2009). One downside of this score for text simplification, however, stems from the sole comparison of the output against references without considering the initial sentence. Based on this idea, a metric that compares system output against references and against input - SARI (Xu et al., 2016), has been proposed to reward additions, copying, and deletio"
L18-1479,P15-2135,1,0.933778,"Missing"
L18-1479,P17-2016,1,0.851243,"Missing"
L18-1479,L18-1615,1,0.787767,"; Stajner et al., 2015; Xu et al., 2016). Another parallel corpus of original news articles and their manually simplified versions on four different complexity levels, following strict guidelines and quality control – the Newsela corpus (Newsela, 2016)1 – has been recently released. Xu et al. (2016) show that it has better potential than the EW– SEW dataset for the TS task. However, up until recently, the Newsela corpus was only provided with alignments at the document level. We use the freely available software2 for sentenceˇ alignment across different Newsela levels (Stajner et al., ˇ 2017; Stajner et al., 2018), and then train neural text simplifications models on the sentence-aligned Newsela and EW–SEW (Hwang et al., 2015) datasets. We compare our systems with the SBMT system (Xu et al., 2016) and the recently proposed state-of-the-art reinforcement learning NTS model (Zhang and Lapata, 2017) to show that a simple neural architecture can be efficiently used for in-domain and cross-domain TS. Last but not least, we provide a detailed human and automatic evaluation of neural sequence-to-sequence models trained and tested in-domain and cross-domain on each of the two corpora, to discuss the ability of"
L18-1479,P12-1107,0,0.333776,"Missing"
L18-1479,Q16-1029,0,0.1569,"cross-domain text simplification. Keywords: neural text simplification, sequence-to-sequence models, evaluation 1. Introduction The aim of text simplification (TS) is to transform given texts into their syntactically and/or lexically simpler variants which are more understandable for the target population (e.g. children, non-native speakers, people with low literacy levels, or people with various kinds of cognitive or reading impairments). It is usually applied on the sentence level and encompasses three major simplification operations: sentence splitting, deletions, and lexical paraphrases (Xu et al., 2016). In recent years, the problem of automated sentence simplification has often been addressed as the monolingual machine translation (MT) task of translating from original to simple sentences. The MT models used were, however, adapted to the specificities of the TS task, e.g. by adding phrasal deletions to the standard phrase-based MT model (Coster and Kauchak, 2011) to account for a common sentence shortening and phrasal deletions in TS, or by reranking the output of the phrase-based MT model (Wubben et al., 2012), since in the standard phrase-based MT model applied on TS, the first hypothesis"
L18-1479,D17-1062,0,0.435308,") to boost the coverage of the phrasal simplifications, and the use of SARI, a tuning metric that particularly rewards simplicity (Xu et al., 2016). Following more recent advancements in machine translation using neural networks, we proposed a neural text simplification system, which significantly outperformed the state of the art on various evaluation metrics for the Wikipedia dataset (Nisioi et al., 2017). Our model was constructed as a vanilla encoder-decoder architecture with global attention and input feeding. More recently, a neural network model fine-tuned using reinforcement learning (Zhang and Lapata, 2017) has been proposed for text simplification, the authors reporting several improvements over the previous systems. Given that they were proposed around the same time, the two neural TS models have not yet been directly compared. One commonly raised issue with most supervised TS systems is the usage of English Wikipedia – Simple English Wikipedia (EW–SEW) sentence-aligned corpora for training the systems, especially since the quality of SEW for modeling TS has often been disputed (Amancio and Speˇ cia, 2014; Stajner et al., 2015; Xu et al., 2016). Another parallel corpus of original news article"
L18-1615,W14-1214,0,0.241936,"Missing"
L18-1615,W11-1603,0,0.00995647,"English and Spanish. At the beginning of 2016, the Newsela corpora contained around 2,000 original news articles in English and around 250 original news articles in Spanish (both with their corresponding manually simplified versions at four different simplification levels). The current state-of-the-art systems for automatic sentencealignment of original and manually simplified text are the Greedy Structural WikNet (GSWN) method (Hwang et al., 2015) used for sentence-alignment of original and simple English Wikipedia, and the HMM-based (using Hidden Markov Model and Viterbi algorithm) method (Bott and Saggion, 2011) used for sentence-alignment of the Spanish Simplext corpus (Saggion et al., 2015). The HMMbased method can be applied to any language as it does not require any language-specific resources. It is based on two hypothesis: (H1) that the original order of information is preserved, and (H2) that every ‘simple’ sentence has a corresponding ‘original’ sentence. The GSWN method does not assume H1 or H2, but it only allows for ‘1-1’ sentence alignments (which is very restricting for TS) and it is language-dependent as it requires the English Wiktionary2 . In this paper, we present a freely available"
L18-1615,W12-2910,0,0.199518,"offsets of ‘complex’ text snippets that correspond to the previous and the next aligned ‘simple’ snippets. For instance, if L = {(s1 , c4 ), (s3 , c7 )} and U = {s2 }, then the search space for the alignments of s2 is reduced to {c4 ...c7 }. We denote the MST-LIS alignment strategy by adding ‘*’ to the name of the similarity method (e.g. C3G*). simpler versions (at different levels of simplification) for each of them, and sentence-aligned them with seven different alignment strategies offered by the CATS tool: C3G, C3G*, CWASA, CWASA*, WAVG, WAVG*, C3G-2step, and the HMM-based alignment tool (Bott et al., 2012). Then we asked two native speakers of English (first trained on additional 3 original articles and their corresponding simplified versions) and two native speakers of Spanish (first trained in the same manner) to classify the obtained sentence pairs (a total of approx. 3,500 sentence-pairs for each language) in one of the four classes: 2.2.1. Modeling Sentence Splitting and Compression In both alignment strategies (MST and MST-LIS), we allow the same original sentence to be aligned with multiple simple sentences, in order to allow for modeling both sentence splitting and sentence compression"
L18-1615,N15-1022,0,0.242225,"ntences and their manual simplifications. The parallel TS corpus for Brazilian Portuguese, compiled for the purposes of the PorSimples project (Alu´ısio et al., 2008) contains around 4,500 aligned sentences, and the parallel TS corpus for Spanish, compiled for the purposes of the Simplext project (Saggion et al., 2015) contains only around 1,000 aligned sentences. The largest existing TS comparable corpora is the English Wikipedia – Simple English Wikipedia (EW–SEW), consisting of 170,000 sentence pairs (Kauchak, 2013), or 150,000 full matches and 130,000 partial matches in the newer version (Hwang et al., 2015). In both cases, the sentences were automatically aligned from comparable English Wikipedia and Simple English Wikipedia articles. However, the use of EW–SEW dataset for modeling TS has been disputed (Amancio and ˇ Specia, 2014; Stajner et al., 2015; Xu et al., 2015) for several reasons: (1) the simplified articles are not necessarily direct simplifications of the original articles; (2) the quality of simplifications is not checked; (3) the dataset does not cover sentence splitting which is one of the most common operations in text simplification. The Newsela corpora1 of document-aligned news"
L18-1615,P13-1151,0,0.145199,"TS is the scarcity and limited size of parallel TS corpora which would contain original sentences and their manual simplifications. The parallel TS corpus for Brazilian Portuguese, compiled for the purposes of the PorSimples project (Alu´ısio et al., 2008) contains around 4,500 aligned sentences, and the parallel TS corpus for Spanish, compiled for the purposes of the Simplext project (Saggion et al., 2015) contains only around 1,000 aligned sentences. The largest existing TS comparable corpora is the English Wikipedia – Simple English Wikipedia (EW–SEW), consisting of 170,000 sentence pairs (Kauchak, 2013), or 150,000 full matches and 130,000 partial matches in the newer version (Hwang et al., 2015). In both cases, the sentences were automatically aligned from comparable English Wikipedia and Simple English Wikipedia articles. However, the use of EW–SEW dataset for modeling TS has been disputed (Amancio and ˇ Specia, 2014; Stajner et al., 2015; Xu et al., 2015) for several reasons: (1) the simplified articles are not necessarily direct simplifications of the original articles; (2) the quality of simplifications is not checked; (3) the dataset does not cover sentence splitting which is one of th"
L18-1615,P15-2135,1,0.888286,"Missing"
L18-1615,P17-2016,1,0.914231,"pecia, 2014; Stajner et al., 2015; Xu et al., 2015) for several reasons: (1) the simplified articles are not necessarily direct simplifications of the original articles; (2) the quality of simplifications is not checked; (3) the dataset does not cover sentence splitting which is one of the most common operations in text simplification. The Newsela corpora1 of document-aligned news texts, manually simplified at four different simplification levels have been freely available for a few years for research purposes. These corpora have several advantages over the EW– ˇ SEW dataset (Xu et al., 2015; Stajner et al., 2017): (1) simplified texts present direct simplifications of the original articles; (2) simplification was performed by trained human editors, following strict guidelines; (3) by sentencealigning those corpora one can get training material for simplifications at various levels, i.e. train different simplification models depending on the intended reader group; and (4) they provide comparable training material in two languages, English and Spanish. At the beginning of 2016, the Newsela corpora contained around 2,000 original news articles in English and around 250 original news articles in Spanish ("
P15-2011,P11-1027,0,0.0145039,"omplexity reduction (or gain) that would be introduced should the simplification candidate replace the original word. Language model features. The rationale for having language model features is obvious – a simplification candidate is more likely to be a compatible substitute if it fits into the sequence of words preceding and following the original word. Let w−2 w−1 ww1 w2 be the context of the original word w. We consider a simplification candidate c to be a good substitute for w if w−2 w−1 cw1 w2 is a likely sequence according to the language model. We employed the Berkeley language model (Pauls and Klein, 2011) to compute the likelihoods. Since Berkeley LM contains only bigrams and trigrams, we retrieve the likelihoods for ngrams w−1 c, cw1 , w−2 w−1 c, cw1 w2 , and w−1 cw1 , for each simplification candidate c. 3.3 has no dedicated component for deciding whether simplifying a word is necessary, it accounts for this implicitly by performing the simplification only if the best candidate has lower information content than the original word (lines 13–15). Since simplification candidates need not have the same POS tag as the original word, to preserve grammaticality, we transform the chosen candidate in"
P15-2011,P11-2087,0,0.0299036,"Missing"
P15-2011,D14-1162,0,0.125697,"rpora (Biran et al., 2011) to supervised methods learning substitutions from the sentence-aligned corpora (Horn et al., 2014). Using simplified corpora improves the simplification performance, but reduces method applicability to the few languages for which such corpora exist. The research question motivating this work relates to achieving comparable simplification performance without resorting to simplified corpora or lexicons like WordNet. Observing that “simple” words appear in regular (i.e., “complex”, not simplified) text as well, we exploit recent advances in word vector representations (Pennington et al., 2014) to find suitable simplifications for complex words. We evaluate the performance of our resource-light approach (1) automatically, on two existing lexical simplification datasets and (2) manually, via human judgements of grammaticality, simplicity, and meaning preservation. The obtained results support the claim that effective lexical simplification can be achieved without using simplified corpora. Simplification of lexically complex texts, by replacing complex words with their simpler synonyms, helps non-native speakers, children, and language-impaired people understand text better. Recent le"
P15-2011,shardlow-2014-open,0,0.205273,"fying multi-word expressions. In this work, we associate word complexity with the commonness of the word in the corpus, and not with the length of the word. Net, for a predefined set of complex words (Carroll et al., 1998; Bautista et al., 2009), and then choosing the “simplest” of these synonyms, typically using some frequency-based (Devlin and Tait, 1998; De Belder and Moens, 2010) or lengthbased heuristics (Bautista et al., 2009). The main shortcomings of the rule-based systems include low recall (De Belder and Moens, 2010) and misclassification of simple words as complex (and vice versa) (Shardlow, 2014). The paradigm shift from knowledge-based to data-driven simplification came with the creation of Simple Wikipedia, which, aligned with the “original” Wikipedia, constitutes a large comparable corpus to learn from. Yatskar et al. (2010) used the edit history of Simple Wikipedia to recognize lexical simplifications. They employed a probabilistic model to discern simplification edits from other types of content changes. Biran et al. (2011) presented an unsupervised method for learning substitution pairs from a corpus of comparable texts from Wikipedia and Simple Wikipedia, although they exploite"
P15-2011,S12-1046,0,0.030363,"r grammaticality (0.71), followed by meaning preservation (0.62) and simplicity (0.57), which we consider to be a fair agreement, especially for inherently subjective notions of simplicity and meaning preservation. The results of human evaluation are shown in Table 3. In addition to grammaticality (Gr), simplicity (Smp), and meaning preservation (MP), we measured the percentage of sentences with at least one change made by the system (Ch). The results imply that the sentences produced by L IGHTRanking Task We next evaluated L IGHT-LS on the SemEval2012 lexical simplification task for English (Specia et al., 2012), which focused on ranking a target word (in a context) and three candidate replacements, from the simplest to the most complex. To account for the peculiarity of the task where the target word is also one of the simplification candidates, we modified the features as follows (otherwise, an unfair advantage would be given to the target word): (1) we excluded the semantic similarity feature, and (2) we used the information content of the candidate instead of the difference of information contents. We used the official SemEval task evaluation script to compute the Cohen’s kappa index for the agre"
P15-2011,D11-1038,0,0.0181616,"ex words, which results in very low accuracy. Our method numerically outperforms the supervised method of Horn et al. (2014), but the difference is not statistically significant. 4.2 κ baseline-random baseline-frequency 0.013 0.471 Jauhar and Specia (2012) L IGHT-LS 0.496 0.540 our information content-based feature (the higher the frequency, the lower the information content). 4.3 Human Evaluation Although automated task-specific evaluations provide useful indications of a method’s performance, they are not as reliable as human assessment of simplification quality. In line with previous work (Woodsend and Lapata, 2011; Wubben et al., 2012), we let human evaluators judge the grammaticality, simplicity, and meaning preservation of the simplified text. We compiled a dataset of 80 sentence-aligned pairs from Wikipedia and Simple Wikipedia and simplified the original sentences with L IGHT-LS and the publicly available system of Biran et al. (2011). We then let two annotators (with prior experience in simplification annotations) grade grammaticality and simplicity for the manual simplification from Simple Wikipedia and simplifications produced by each of the two systems (total of 320 annotations per annotator)."
P15-2011,R13-2011,1,0.886639,"Missing"
P15-2011,P12-1107,0,0.407788,"Missing"
P15-2011,P14-2075,0,0.35549,"p in Computational Linguistics SanjaStajner@wlv.ac.uk Goran Glavaˇs University of Zagreb Faculty of Electrical Engineering and Computing goran.glavas@fer.hr Abstract ber of languages diminishes the impact of these simplification methods. The emergence of the Simple Wikipedia1 shifted the focus towards the data-driven approaches to lexical simplification, ranging from unsupervised methods leveraging either the metadata (Yatskar et al., 2010) or co-occurrence statistics of the simplified corpora (Biran et al., 2011) to supervised methods learning substitutions from the sentence-aligned corpora (Horn et al., 2014). Using simplified corpora improves the simplification performance, but reduces method applicability to the few languages for which such corpora exist. The research question motivating this work relates to achieving comparable simplification performance without resorting to simplified corpora or lexicons like WordNet. Observing that “simple” words appear in regular (i.e., “complex”, not simplified) text as well, we exploit recent advances in word vector representations (Pennington et al., 2014) to find suitable simplifications for complex words. We evaluate the performance of our resource-ligh"
P15-2011,N10-1056,0,0.0151031,"a et al., 2009), and then choosing the “simplest” of these synonyms, typically using some frequency-based (Devlin and Tait, 1998; De Belder and Moens, 2010) or lengthbased heuristics (Bautista et al., 2009). The main shortcomings of the rule-based systems include low recall (De Belder and Moens, 2010) and misclassification of simple words as complex (and vice versa) (Shardlow, 2014). The paradigm shift from knowledge-based to data-driven simplification came with the creation of Simple Wikipedia, which, aligned with the “original” Wikipedia, constitutes a large comparable corpus to learn from. Yatskar et al. (2010) used the edit history of Simple Wikipedia to recognize lexical simplifications. They employed a probabilistic model to discern simplification edits from other types of content changes. Biran et al. (2011) presented an unsupervised method for learning substitution pairs from a corpus of comparable texts from Wikipedia and Simple Wikipedia, although they exploited the (co-)occurrence statistics of the simplified corpora rather than its metadata. Horn et al. (2014) proposed a supervised framework for learning simplification rules. Using a sentence-aligned simplified corpus, they generated the ca"
P15-2011,S12-1066,0,0.130251,"ccount for the peculiarity of the task where the target word is also one of the simplification candidates, we modified the features as follows (otherwise, an unfair advantage would be given to the target word): (1) we excluded the semantic similarity feature, and (2) we used the information content of the candidate instead of the difference of information contents. We used the official SemEval task evaluation script to compute the Cohen’s kappa index for the agreement on the ordering for each pair of candidates. The performance of L IGHT-LS together with results of the best-performing system (Jauhar and Specia, 2012) from the SemEval-2012 task and two baselines (random and frequency-based) is given in Table 2. L IGHT-LS significantly outperforms the supervised model by Jauhar and Specia (2012) with p < 0.05, according to the nonparametric stratified shuffling test (Yeh, 2000). An interesting observation is that the competitive frequency-based baseline highly correlates with 66 Table 4: Example simplifications Source Sentence Original sentence The contrast between a high level of education and a low level of political rights was particularly great in Aarau, and the city refused to send troops to defend the"
P15-2011,C00-2137,0,0.019585,"e information content of the candidate instead of the difference of information contents. We used the official SemEval task evaluation script to compute the Cohen’s kappa index for the agreement on the ordering for each pair of candidates. The performance of L IGHT-LS together with results of the best-performing system (Jauhar and Specia, 2012) from the SemEval-2012 task and two baselines (random and frequency-based) is given in Table 2. L IGHT-LS significantly outperforms the supervised model by Jauhar and Specia (2012) with p < 0.05, according to the nonparametric stratified shuffling test (Yeh, 2000). An interesting observation is that the competitive frequency-based baseline highly correlates with 66 Table 4: Example simplifications Source Sentence Original sentence The contrast between a high level of education and a low level of political rights was particularly great in Aarau, and the city refused to send troops to defend the Bernese border. The separate between a high level of education and a low level of political rights was particularly great in Aarau , and the city refused to send troops to defend the Bernese border. The contrast between a high level of education and a low level o"
P15-2011,C10-1152,0,0.342819,"Missing"
P15-2135,P02-1040,0,0.0943817,"Missing"
P15-2135,W03-1004,0,0.0433291,"a greater impact than the size of the datasets on the system’s performance. 3.1 Corpora Wikipedia is a comparable TS corpus of 137,000 automatically aligned sentence pairs from English Wikipedia and Simple English Wikipedia1 , previously used by Coster and Kauchak (2011a). We use a small portion of this corpus (240 sentence pairs) to build the first test set (WikiTest), and 88,000 sentence pairs from the remaining sentence pairs to build translation models. EncBrit is a comparable TS corpus of original sentences from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and bu"
P15-2135,W11-1601,0,0.829994,"e. Our results indicate that only the former can influence the MT output significantly. In order to explore the last issue, we test our models on two different test sets and perform human evaluation of the output of several systems. Introduction In the last few years, a growing number of studies have addressed the text simplification (TS) task as a monolingual machine translation (MT) problem of translating sentences from ‘original’ to ‘simple’ language. Several studies reported promising results using standard phrase-based statistical machine translation (PB-SMT) for this task (Specia, 2010; Coster and Kauchak, 2011a; Wubben et al., 2012), but made no attempt to explain the reasons behind the success of their systems. Specia (2010) obtained reasonably good results (BLEU = 60.75) 823 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 823–828, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 3 Related Work Methodology We focus on the two TS corpora available for English (Wikipedia and EncBrit) and train a series of translation models on tra"
P15-2135,P11-2117,0,0.7907,"e. Our results indicate that only the former can influence the MT output significantly. In order to explore the last issue, we test our models on two different test sets and perform human evaluation of the output of several systems. Introduction In the last few years, a growing number of studies have addressed the text simplification (TS) task as a monolingual machine translation (MT) problem of translating sentences from ‘original’ to ‘simple’ language. Several studies reported promising results using standard phrase-based statistical machine translation (PB-SMT) for this task (Specia, 2010; Coster and Kauchak, 2011a; Wubben et al., 2012), but made no attempt to explain the reasons behind the success of their systems. Specia (2010) obtained reasonably good results (BLEU = 60.75) 823 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 823–828, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 3 Related Work Methodology We focus on the two TS corpora available for English (Wikipedia and EncBrit) and train a series of translation models on tra"
P15-2135,P12-1107,0,0.362968,"Missing"
P15-2135,N03-1017,0,0.0275149,"build translation models. EncBrit is a comparable TS corpus of original sentences from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and build a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002). We limit our stack size to 500 hypotheses during decoding. 3.3 Training and development datasets We tokenise and shuffle the initial dataset of 167,689 aligned sentences from the Wikipedia dataset.4 Using the simplified sentences as references and the original sentences as hypotheses, 1 http://www.cs.middlebury.edu/ ˜dkauchak/simplific"
P15-2135,J03-1002,0,0.0084212,"the first test set (WikiTest), and 88,000 sentence pairs from the remaining sentence pairs to build translation models. EncBrit is a comparable TS corpus of original sentences from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and build a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002). We limit our stack size to 500 hypotheses during decoding. 3.3 Training and development datasets We tokenise and shuffle the initial dataset of 167,689 aligned sentences from the Wikipedia dataset.4 Using the simplified sentences as references"
P15-2135,P03-1021,0,0.0829358,"from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and build a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002). We limit our stack size to 500 hypotheses during decoding. 3.3 Training and development datasets We tokenise and shuffle the initial dataset of 167,689 aligned sentences from the Wikipedia dataset.4 Using the simplified sentences as references and the original sentences as hypotheses, 1 http://www.cs.middlebury.edu/ ˜dkauchak/simplification/ 2 http://www.cs.columbia.edu/˜noemie/ alignment/ 3 Version 2.0 docu"
P15-2135,P07-2045,0,\N,Missing
P17-2014,P15-2011,1,0.177095,"Missing"
P17-2014,P82-1020,0,0.855934,"Missing"
P17-2014,N15-1022,0,0.379655,"nd that the quality of simplifications in Simple English Wikipedia has been disputed before (Amancio and Specia, 2014; Xu et al., 2015), for tuning and testing we use the dataset previously released by Xu et al. (2016), which contains 2000 sentences for tuning and 359 for testing, each with eight simplification variants obtained by eight Amazon Mechanical Turkers.3 The tune subset is also used as reference corpus in combination with BLEU and SARI to select the best beam size and hypothesis for prediction reranking. Dataset To train our models, we use the publicly available dataset provided by Hwang et al. (2015) based on manual and automatic alignments between standard English Wikipedia and Simple English Wikipedia (EW–SEW). We discard the uncategorized matches, and use only good matches and partial matches which were above the 0.45 threshold (Hwang et al., 2015), totaling to 280K aligned sentences (around 150K full matches and 130K partial matches). It is one of the largest freely available resources for text simplification, and unlike the previously used EW–SEW corpus2 (Kauchak, 2013), which only contains full matches (167K pairs), the newer dataset also contains partial matches. Therefore, it is n"
P17-2014,P13-1151,0,0.6984,"approach can increase the evaluation scores for neural machine translation, while in our case, for monolingual data, we believe it can be helpful to create better alignments. Our approach does not involve the use of character-based models (Sennrich et al., 2015; Luong and Manning, 2016) to handle out of vocabulary words and entities. Instead, we make use of alignment probabilities between the predictions and the original sentences to retrieve the original words. yt 2.1 Furthermore, we are interested to explore whether large scale pre-trained embeddings can improve text simplification models. Kauchak (2013) indicates that combining normal data with simplified data can increase the performance of ATS systems. Therefore, we construct a secondary model (NTSw2v) using a combination of pre-trained word2vec from Google News corpus (Mikolov et al., 2013a) of size 300 and locally trained embeddings of size 200. To ensure good representations of lowˇ uˇrek and frequency words, we use word2vec (Reh˚ Sojka, 2010; Mikolov et al., 2013b) to train skipgram with hierarchical softmax and we set a window of 10 words. Following Garten et al. (2015) who showed that simple concatenation can improve the word represe"
P17-2014,P17-4012,0,0.0225325,"(Bojar et al., 2016). Unlike classical statistical machine translation (SMT) systems (Koehn, 2010), neural networks are being trained end-to-end, without the need to have external decoders, language models or phrase tables. The architectures are relatively simpler and more flexible, making possible the use of character models (Luong and Manning, 2016) or even training multilingual systems in one go (Firat et al., 2016). Automated text simplification (ATS) systems are meant to transform original texts into differ∗ Liviu P. Dinu1 2 Neural Text Simplification (NTS) We use the OpenNMT framework (Klein et al., 2017) to train and build our architecture with two LSTM layers (Hochreiter and Schmidhuber, 1997), hidden states of size 500 and 500 hidden units, and a 0.3 dropout probability (Srivastava et al., 2014). The vocabulary size is set to 50,000 and we train the model for 15 epochs with plain SGD optimizer, and after epoch 8 we halve the Both authors have contributed equally to this work 85 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 85–91 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https:"
P17-2014,J10-4005,0,0.0113865,"troduction Neural sequence to sequence models have been successfully used in many applications (Graves, 2012), from speech and signal processing to text processing or dialogue systems (Serban et al., 2015). Neural machine translation (Cho et al., 2014; Bahdanau et al., 2014) is a particular type of sequence to sequence model that recently attracted a lot of attention from industry (Wu et al., 2016) and academia, especially due to the capability to obtain state-of-the-art results for various translation tasks (Bojar et al., 2016). Unlike classical statistical machine translation (SMT) systems (Koehn, 2010), neural networks are being trained end-to-end, without the need to have external decoders, language models or phrase tables. The architectures are relatively simpler and more flexible, making possible the use of character models (Luong and Manning, 2016) or even training multilingual systems in one go (Firat et al., 2016). Automated text simplification (ATS) systems are meant to transform original texts into differ∗ Liviu P. Dinu1 2 Neural Text Simplification (NTS) We use the OpenNMT framework (Klein et al., 2017) to train and build our architecture with two LSTM layers (Hochreiter and Schmid"
P17-2014,P15-2135,1,0.839028,"nter, University of Bucharest, Romania 2 Data and Web Science Group, University of Mannheim, Germany 3 Oracle Corporation, Romania {sergiu.nisioi,ldinu}@fmi.unibuc.ro {sanja,simone}@informatik.uni-mannheim.de Abstract ent (simpler) variants which would be understood by wider audiences and more successfully processed by various NLP tools. In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences. So far, attempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the appl"
P17-2014,P16-1100,0,0.0236322,"Missing"
P17-2014,D15-1166,0,0.0238032,"ges 85–91 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2014 learning rate. At the end of each epoch we save the current state of the model and predict the perplexity values of the models on the development set. We employ early-stopping and select the model resulted from the epoch with the best perplexity to avoid over-fitting. The parameters are initialized over uniform distribution with support [-0.1, 0.1]. Additionally, for the decoder we employ global attention in combination with input feeding as described by Luong et al. (2015). The architecture1 is depicted in Figure 1, with the input feeding approach represented only for the last hidden state of the decoder. method, to the input at the next step, presumably making the model keep track of anterior alignment decisions. Luong et al. (2015) showed this approach can increase the evaluation scores for neural machine translation, while in our case, for monolingual data, we believe it can be helpful to create better alignments. Our approach does not involve the use of character-based models (Sennrich et al., 2015; Luong and Manning, 2016) to handle out of vocabulary words"
P17-2014,1983.tc-1.13,0,0.485554,"Missing"
P17-2014,P12-1107,0,0.670247,"Missing"
P17-2014,W13-4813,0,0.0618928,"d be understood by wider audiences and more successfully processed by various NLP tools. In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences. So far, attempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evaluation to directly compare ou"
P17-2014,Q16-1029,0,0.720017,"In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences. So far, attempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evaluation to directly compare our systems with the current state-of-the-art (supervised) MT-based and unsupervi"
P17-2014,W16-4912,0,0.266157,"tempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evaluation to directly compare our systems with the current state-of-the-art (supervised) MT-based and unsupervised lexical simplification systems. We present the first attempt at using sequence to sequence neural networks to model text simplification (TS). Unlike the previously proposed automated TS sy"
P17-2014,P02-1040,0,0.118354,"tSimplification 86 described previously (NTS and NTS-w2v). Beam search works by generating the first k hypotheses at each step ordered by the log-likelihood of the target sentence given the input sentence. By default, we use a beam size of 5 and take the first hypothesis, but we also observe that higher beam size and lower-ranked hypotheses can generate good simplification results. Therefore, we generate the first two candidate hypotheses for each beam size from 5 to 12. We then attempt to find the best beam size and hypothesis based on two metrics: the traditional MT-evaluation metric, BLEU (Papineni et al., 2002; Bird et al., 2009) with NIST smoothing (Bird et al., 2009), and SARI (Xu et al., 2016), a recent text-simplification metric. 2.3 in the corpus. A brief analysis of the vocabulary is rendered in Table 1. The dataset we use contains an abundant amount of named entities and consequently a large amount of low frequency words, but the majority of entities are not part of the model’s 50,000 words vocabulary due to their small frequency. These words are replaced with ’UNK’ symbols during training. At prediction time, we replace the unknown words with the highest probability score from the attention"
P17-2014,C10-1152,0,0.729324,"ariants which would be understood by wider audiences and more successfully processed by various NLP tools. In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences. So far, attempts were made at standard phrase-based SMT (PBSMT) models (Speˇ cia, 2010; Stajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016). Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, ˇ with reported good success (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016). To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evalu"
P17-2014,N13-1092,0,\N,Missing
P17-2014,W16-2301,0,\N,Missing
P17-2014,P05-1045,0,\N,Missing
P17-2016,J03-1002,0,0.0481646,"Missing"
P17-2016,W10-1607,0,0.0716558,"Missing"
P17-2016,W16-4912,0,0.0987186,"in {sanja,simone,heiner}@informatik.uni-mannheim.de marc.franco@symanto.net, prosso@prhlt.upv.es 1 Abstract (EW–SEW) dataset, which contains 160-280,000 sentence pairs, depending on whether we want to model only traditional sentence rewritings or also to model content reduction and stronger paraphrasing (Hwang et al., 2015). For Spanish, the largest existing parallel TS corpus contains only 1,000 sentence pairs thus impeding the use of fully supervised approaches. The best unsupervised lexical simplification (LS) systems for English which leverage word-embeddings (Glavaˇs ˇ and Stajner, 2015; Paetzold and Specia, 2016) seem to perform more lexical substitutions but at the cost of having less grammatical output and more often changed meaning. However, there have been no direct comparisons of supervised and unsupervised state-of-the-art approaches so far. The Newsela corpora1 offers over 2,000 original news articles in English and around 250 in Spanish, manually simplified to 3–4 different complexity levels following strict guidelines (Xu et al., 2015). Although it was suggested that it has better quality than the EW–SEW corpus (Xu et al., 2015), Newsela has not yet been used for training end-to-end ATS syste"
P17-2016,W11-1603,0,0.346565,"ish is the English Wikipedia – Simple English Wikipedia 1 Freely available: https://newsela.com/data/ 97 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 97–102 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2016 3 able software;2 (2) compare the performances of lexically- and semantically-based alignment methods across various text complexity levels; (3) test the hypothesis that the original order of information is preserved during manual simplification (Bott and Saggion, 2011) by offering customized MST-LIS alignment strategy (Section 3.1); and (4) show that the new sentence-alignments lead to the state-of-the-art ATS systems even in a basic phrase-based statistical machine translation (PBSMT) approach to text simplifications. 2 Approach Having a set of ‘simple’ text snippets S and a set of ‘complex’ text snippets C, we offer two strategies (Section 3.1) to obtain the alignments (si , cj ), where si ∈ S, cj ∈ C. Each alignment strategy, in turn, can use one of the three methods (Section 3.2) to calculate similarity scores between text snippets (either paragraphs or"
P17-2016,P15-2011,1,0.868371,"Missing"
P17-2016,P08-1040,0,0.0631219,"even the standard phrase-based statistical machine translation models for ATS can outperform the state-of-the-art ATS systems. 1 Introduction Automated text simplification (ATS) tries to automatically transform (syntactically, lexically and/or semantically) complex sentences into their simpler variants without significantly altering the original meaning. It has attracted much attention recently as it could make texts more accessible to wider audiences (Alu´ısio and Gasperin, 2010; Saggion et al., 2015), and used as a pre-processing step, improve performances of various NLP tasks and systems (Vickrey and Koller, 2008; Evans, 2011; ˇ Stajner and Popovi´c, 2016). However, the state-of-the-art ATS systems still do not reach satisfying performances and require ˇ some human post-editing (Stajner and Popovi´c, 2016). While the best supervised approaches generally lead to grammatical output with preserved original meaning, they are overcautious, making almost no changes to the input sentences (Speˇ cia, 2010; Stajner et al., 2015), probably due to the limited size or bad quality of parallel TS corpora used for training. The largest existing sentence-aligned TS dataset for English is the English Wikipedia – Simpl"
P17-2016,N15-1022,0,0.197548,"ext Simplification Systems 1 ˇ Sanja Stajner , Marc Franco-Salvador2,3 , Simone Paolo Ponzetto1 , Paolo Rosso3 , Heiner Stuckenschmidt1 DWS Research Group, University of Mannheim, Germany 2 Symanto Research, Nuremberg, Germany 3 PRHLT Research Center, Universitat Polit`ecnica de Val`encia, Spain {sanja,simone,heiner}@informatik.uni-mannheim.de marc.franco@symanto.net, prosso@prhlt.upv.es 1 Abstract (EW–SEW) dataset, which contains 160-280,000 sentence pairs, depending on whether we want to model only traditional sentence rewritings or also to model content reduction and stronger paraphrasing (Hwang et al., 2015). For Spanish, the largest existing parallel TS corpus contains only 1,000 sentence pairs thus impeding the use of fully supervised approaches. The best unsupervised lexical simplification (LS) systems for English which leverage word-embeddings (Glavaˇs ˇ and Stajner, 2015; Paetzold and Specia, 2016) seem to perform more lexical substitutions but at the cost of having less grammatical output and more often changed meaning. However, there have been no direct comparisons of supervised and unsupervised state-of-the-art approaches so far. The Newsela corpora1 offers over 2,000 original news articl"
P17-2016,P15-2135,1,0.883593,"xts more accessible to wider audiences (Alu´ısio and Gasperin, 2010; Saggion et al., 2015), and used as a pre-processing step, improve performances of various NLP tasks and systems (Vickrey and Koller, 2008; Evans, 2011; ˇ Stajner and Popovi´c, 2016). However, the state-of-the-art ATS systems still do not reach satisfying performances and require ˇ some human post-editing (Stajner and Popovi´c, 2016). While the best supervised approaches generally lead to grammatical output with preserved original meaning, they are overcautious, making almost no changes to the input sentences (Speˇ cia, 2010; Stajner et al., 2015), probably due to the limited size or bad quality of parallel TS corpora used for training. The largest existing sentence-aligned TS dataset for English is the English Wikipedia – Simple English Wikipedia 1 Freely available: https://newsela.com/data/ 97 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 97–102 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2016 3 able software;2 (2) compare the performances of lexically- and semantically-based alignment meth"
P17-2016,P07-2045,0,0.0103568,"3G-2s), which first aligns paragraphs using the best paragraph-alignment method (C3G) and then within each paragraph align sentences with the best sentence-alignment method (C3G), led to more good+partial alignments than the ‘direct’ sentence-alignment C3G method. 5 Extrinsic Evaluation Finally, we test our new English Newsela (C3G2s) sentence-alignments (both for the neighbouring levels – neighb. and for all levels – all) and Newsela sentence-alignments for neighboring levels obtained with HMM-method6 (Bott and Saggion, 2011) in the ATS task using standard PBSMT models7 in the Moses toolkit (Koehn et al., 2007). We vary the training dataset and the corpus used to build language models (LMs), while keeping always the same 2,000 sentence pairs for tuning (Xu et al., 2016) and the first 70 sentence 8 Both freely available from: https://github.com/ cocoxu/simplification/ 9 We use the output of the original SBMT (Xu et al., 2016) ˇ and LightLS (Glavaˇs and Stajner, 2015) systems, obtained from the authors. 10 Those cases in which the two annotators did not agree are additionally evaluated by a third annotator to obtain majority. 6 Given that the performance of the HMM-method was poor for non-neighboring"
P17-2016,W16-3411,1,0.914076,"Missing"
P17-2016,N03-1017,0,0.0759083,"Missing"
P17-2016,Q16-1029,0,0.186509,"he HMM method on the paragraph-alignment 0–1 task, there is still significant difference in the obtained scores (in some cases, our methods led to good matches whereas the HMM only led to partial matches). 99 Sentence 0–1 0–4 3–4 C3G 98.3 56.1 81.1 C3G* 96.7 54.7 78.8 CWASA 98.3 45.3 79.7 CWASA* 96.1 42.1 76.4 WAVG 97.8 56.1 79.7 WAVG* 96.1 50.0 79.7 C3G-2s 98.5 57.8 83.5 HMM 86.2 25.2 65.6 Method pairs of their test set8 for our human evaluation. Using that particular test set allow us to compare our (PBSMT) systems with the output of the stateof-the-art syntax-based MT (SBMT) system for TS (Xu et al., 2016) which is not freely available. We compare: (1) the performance of the standard PBSMT model which uses only the already available EW–SEW dataset (Hwang et al., 2015) with the performances of the same PBSMT models but this time using the combination of the EW–SEW dataset and our newly-created Newsela datasets; (2) the latter PBSMT models (which use both EW–SEW and new Newsela datasets) against the state-of-the-art supervised ATS system (Xu et al., 2016), and one of the recently proposed unsupervised lexical simplification systems, the LightLS ˇ system (Glavaˇs and Stajner, 2015).9 We perform th"
P19-1377,N16-1120,0,0.013454,"Valerjev, 2010). While psycholinguistic literature abounds in research and demonstration of such processes (Just and Carpenter, 1980; Kutas and Hillyard, 1984; Carroll and Slowiaczek, 1986), there is a gap in understanding if they can be modeled in an automated way for capturing the cognitive load required by texts. At the same time, the recent advances in the publication of encyclopedic knowledge graphs provide an unprecedented opportunity for modeling human knowledge at scale. We focus on conceptual complexity which, as opposed to lexical and syntactic complexity (Vajjala and Meurers, 2014; Ambati et al., 2016), has received very little attention so far. Conceptual complexity accounts for the background knowledge necessary to understand mentioned concepts as well as the implicit connections that the reader has to access between the mentioned concepts in order to fully understand a text. It plays an important role in making texts accessible to children, non-native speakers, as well as people with low literacy levels or intellectual disabilities (Arf´e et al., 2017). Apart from being one of the main factors for understanding the story, conceptual complexity also influences the readers’ interest in the"
P19-1377,C18-1027,1,0.585241,"Missing"
P19-1377,E14-1031,0,0.0164203,"75; Neely, 1991; Gulan and Valerjev, 2010). While psycholinguistic literature abounds in research and demonstration of such processes (Just and Carpenter, 1980; Kutas and Hillyard, 1984; Carroll and Slowiaczek, 1986), there is a gap in understanding if they can be modeled in an automated way for capturing the cognitive load required by texts. At the same time, the recent advances in the publication of encyclopedic knowledge graphs provide an unprecedented opportunity for modeling human knowledge at scale. We focus on conceptual complexity which, as opposed to lexical and syntactic complexity (Vajjala and Meurers, 2014; Ambati et al., 2016), has received very little attention so far. Conceptual complexity accounts for the background knowledge necessary to understand mentioned concepts as well as the implicit connections that the reader has to access between the mentioned concepts in order to fully understand a text. It plays an important role in making texts accessible to children, non-native speakers, as well as people with low literacy levels or intellectual disabilities (Arf´e et al., 2017). Apart from being one of the main factors for understanding the story, conceptual complexity also influences the re"
P19-1377,D08-1080,0,0.0124008,"itive architecture (Anderson and Lebiere, 1998). They implement a very basic spreading activation model for scoring facts in the knowledge base for answering natural language, factual questions such as “What is the population of Philadelphia?”. Several other approaches have been proposed for extending ACTR with knowledge and reasoning (Ball et al., 2004; Oltramari and Lebiere, 2012), but none of them aim to assess the complexity of texts. With respect to spreading activation, it has long been adopted as a methodology for information retrieval (Crestani, 1997), used for document summarization (Nastase, 2008), document similarity (Syed et al., 2008), as well as cross-domain recommendation (Heitmann and Hayes, 2016), among others. Nevertheless, there is no prior attempt to apply spreading activation to the recently developed encyclopedic knowledge graphs with the purpose of modeling reading comprehension. Framework for Unsupervised Assessment of Conceptual Complexity Our framework tracks the activation of concepts in working memory during reading processes. We consider an encyclopedic knowledge graph, DBpedia1 , as a proxy to long-term memory over which spreading activation processes run and bring"
R13-2011,W06-0901,0,0.039649,"rely on lexical and syntactic simplification, performing little content reduction, the exception being deletion of parenthetical expressions (Drndarevic et al., 2013). On the one hand, lack of content reduction has been recognized as one of the main shortcomings of automated systems (Drndarevic et al., 2013) which produce much worse simplification results compared to human. On the other hand, information extraction techniques help identify relevant content (e.g., named entities, events), but have not yet proven useful for text simplification. However, significant advances in event extraction (Ahn, 2006; Bethard, 2008; Llorens et al., 2010; Grover et al., 2010), achieved as the result of standardization efforts (Pustejovsky et al., 2003a; Pustejovsky et al., 2003b) and dedicated tasks (ACE, 2005; Verhagen et al., 2010), encourage event-oriented simplification attempts. To the best of our knowledge, the only reported work exploiting events for text simplification is that of Barlacchi and Tonelli (2013). They extract factual events from a set of Italian children’s stories and eliminate non-mandatory event arguments. They evaluate simplified texts using only the automated score which can hardly"
R13-2011,P05-1045,0,0.0161841,"Missing"
R13-2011,N07-4002,0,0.011865,"Carroll et al., 1998; Devlin and Unthank, 2006), cognitive disabilities (Saggion et al., 2011), autism (Orasan et al., 2013), congenital deafness (Inui et al., 2003), and low literacy (Alu´ısio et al., 2008). Most of these approaches rely on rule-based lexical and syntactic simplification. Syntactic simplification is usually carried out by recursively applying a set of hand-crafted rules at a sentence level, not considering interactions across sentence boundaries. Lexical simplification usually substitutes difficult words with their simpler synonyms (Carroll et al., 1998; Lal and Ruger, 2002; Burstein et al., 2007). we offer two different simplification schemes. To the best of our knowledge, this is the first work on event-based text simplification for English. 3 Event-Centered Simplification The simplification schemes we propose exploit the structure of extracted event mentions. We employ robust event extraction that involves supervised extraction of factual event anchors (i.e., words that convey the core meaning of the event) and the rulebased extraction of event arguments of coarse semantic types. Although a thorough description of the event extraction system is outside the scope of this paper, we de"
R13-2011,S10-1074,0,0.0183608,"rforming little content reduction, the exception being deletion of parenthetical expressions (Drndarevic et al., 2013). On the one hand, lack of content reduction has been recognized as one of the main shortcomings of automated systems (Drndarevic et al., 2013) which produce much worse simplification results compared to human. On the other hand, information extraction techniques help identify relevant content (e.g., named entities, events), but have not yet proven useful for text simplification. However, significant advances in event extraction (Ahn, 2006; Bethard, 2008; Llorens et al., 2010; Grover et al., 2010), achieved as the result of standardization efforts (Pustejovsky et al., 2003a; Pustejovsky et al., 2003b) and dedicated tasks (ACE, 2005; Verhagen et al., 2010), encourage event-oriented simplification attempts. To the best of our knowledge, the only reported work exploiting events for text simplification is that of Barlacchi and Tonelli (2013). They extract factual events from a set of Italian children’s stories and eliminate non-mandatory event arguments. They evaluate simplified texts using only the automated score which can hardly account for grammaticality and information relevance of th"
R13-2011,E99-1042,0,0.194837,"Missing"
R13-2011,W03-1602,0,0.0759543,"s confrontation in years”. However, studies indicate that people with reading disabilities, especially people with intellectual disabilities, have difficulties discriminating relevant 2 Related Work Several projects dealt with automated text simplification for people with different reading difficulties: 71 Proceedings of the Student Research Workshop associated with RANLP 2013, pages 71–78, Hissar, Bulgaria, 9-11 September 2013. people with alexia (Carroll et al., 1998; Devlin and Unthank, 2006), cognitive disabilities (Saggion et al., 2011), autism (Orasan et al., 2013), congenital deafness (Inui et al., 2003), and low literacy (Alu´ısio et al., 2008). Most of these approaches rely on rule-based lexical and syntactic simplification. Syntactic simplification is usually carried out by recursively applying a set of hand-crafted rules at a sentence level, not considering interactions across sentence boundaries. Lexical simplification usually substitutes difficult words with their simpler synonyms (Carroll et al., 1998; Lal and Ruger, 2002; Burstein et al., 2007). we offer two different simplification schemes. To the best of our knowledge, this is the first work on event-based text simplification for En"
R13-2011,P03-1054,0,0.00587134,"t is well-simplified if its readability is increased, while its grammaticality (syntactic correctness), meaning, and information relevance (semantic correctness) are preserved. We measure the readability of the simplified text automatically with two commonly used formulae. However, we rely on human assessment of grammaticality and relevance, given that these aspects are difficult to capture automatically (Wubben et al., 2012). We employ a syntactically motivated baseline that retains only the main clause of a sentence and discards all subordinate clauses. We used Stanford constituency parser (Klein and Manning, 2003) to identify the main and subordinate clauses. transform events with nominal anchors into separate sentences, as such events tend to have very few arguments and are often arguments of verbal events. For example, in “China and Philippines resolved a naval standoff” mention “standoff” is a target of the mention “resolved”. Thirdly, we convert gerundive events that govern the clausal complement of the main sentence event into past simple for preserving grammaticality of the output. E.g., “Philippines disputed China’s territorial claims, triggering the naval confrontation” is transformed into “Phi"
R13-2011,W11-1902,0,0.0255955,"l-Megrahi has died at his home. Baset al-Megrahi was released from a Scottish prison.” Algorithm 2. Event-wise simplification input: sentence s input: set of event mentions E // set of event-output sentence pairs It has been shown that anaphoric mentions cause difficulties for people with cognitive disabilities (Ehrlich et al., 1999; Shapiro and Milkes, 2004). To investigate this phenomenon, we additionally employ pronominal anaphora resolution on top of event-wise simplification scheme. To resolve reference of anaphoric pronouns, we use the coreference resolution tool from Stanford Core NLP (Lee et al., 2011). An example of the original text snippet accompanied by its (1) sentence-wise simplification, (2) event-wise simplification, and (3) event-wise simplification with anaphoric pronoun resolution is given in Table 2. S = {} // initialize output token set for each event foreach e in E do S = S ∪ (e, {}) // list of original sentence tokens T = tokenize(s) foreach token t in T do foreach event mention e in E do // set of event tokens a = anchor (e) A = anchorAndArgumentTokens(e) // part of verbal, non-reporting event if t in A & PoS (a) 6= N & type(t) 6= Rep do // token is gerundive anchor if t = a"
R13-2011,S10-1063,0,0.0332637,"tic simplification, performing little content reduction, the exception being deletion of parenthetical expressions (Drndarevic et al., 2013). On the one hand, lack of content reduction has been recognized as one of the main shortcomings of automated systems (Drndarevic et al., 2013) which produce much worse simplification results compared to human. On the other hand, information extraction techniques help identify relevant content (e.g., named entities, events), but have not yet proven useful for text simplification. However, significant advances in event extraction (Ahn, 2006; Bethard, 2008; Llorens et al., 2010; Grover et al., 2010), achieved as the result of standardization efforts (Pustejovsky et al., 2003a; Pustejovsky et al., 2003b) and dedicated tasks (ACE, 2005; Verhagen et al., 2010), encourage event-oriented simplification attempts. To the best of our knowledge, the only reported work exploiting events for text simplification is that of Barlacchi and Tonelli (2013). They extract factual events from a set of Italian children’s stories and eliminate non-mandatory event arguments. They evaluate simplified texts using only the automated score which can hardly account for grammaticality and infor"
R13-2011,D11-1038,0,0.272641,"Missing"
R13-2011,P12-1107,0,0.258972,"Missing"
R13-2011,N10-2011,0,0.0699518,"Missing"
R13-2011,P94-1019,0,\N,Missing
R13-2011,S10-1010,0,\N,Missing
R13-2011,chang-manning-2012-sutime,0,\N,Missing
R15-1079,P07-2045,0,0.010973,"Missing"
R15-1079,J03-1002,0,0.00467958,"ngs of Recent Advances in Natural Language Processing, pages 611–617, Hissar, Bulgaria, Sep 7–9 2015. iments on three different datasets and languages following the methods proposed in previous studies (Specia, 2010; Coster and Kauchak, 2011a). Training Dev. Test Total Selection • We perform automatic evaluation in terms of the document-wise (BLEU) and the sentencewise BLEU score (S-BLEU). 3.2 PorSim 800 200 100 1100 random Translation Experiments We run three MT experiments using the standard PB-SMT models (Koehn et al., 2003) implemented in the Moses toolkit (Koehn et al., 2007) and GIZA++ (Och and Ney, 2003) to obtain the word alignment. The English experiment uses the Wiki aligned corpus for translation model (TM) and the English part of the Europarl corpora2 for building the language model (LM). The Spanish experiment uses the EsSim dataset to build the TM and the Spanish Europarl for the LM. The Brazilian Portuguese experiment uses the PorSim dataset for the TM and the L´acio-Web corpus3 in Brazilian Portuguese for the LM4 . The sentence pairs for training, development and test sets are selected randomly from the initial dataset. • We calculate sentence-wise BLEU score on the training and deve"
R15-1079,W10-1607,0,0.068022,"Missing"
R15-1079,E14-1076,0,0.0130431,"nature of the sentence pairs used for training and tuning of the PB-SMT models. Introduction Text Simplification (TS) aims to convert complex texts into simpler variants which are more accessible to a wider audiences, e.g. non-native speakers, children, and people diagnosed with intellectual disability, autism, aphasia, dyslexia or congenital deafness. In the last twenty years, many automatic text simplification systems have been proposed, varying from rule-based, e.g. (Brouwers et al., 2014; Saggion et al., 2015) to datadriven, e.g. (Zhu et al., 2010; Woodsend and Lapata, 2011), and hybrid (Siddharthan and Angrosh, 2014). Since 2010, there have been several attempts to approach TS as a machine translation (MT) problem (Specia, 2010; Coster and ˇ Kauchak, 2011a; Stajner, 2014). Instead of translating sentences from one language to another, the goal of text simplification is to translate sentences from ‘original’ to ‘simplified’ language. In this paper, we seek to explore the main reasons for the success or failure of the phrasebased statistical machine translation (PB-SMT) 3 Methodology We apply the following methodology: • We run MT-based text simplification exper611 Proceedings of Recent Advances in Natural"
R15-1079,W14-1206,0,0.0315214,"Missing"
R15-1079,W11-1601,0,0.360071,"dings suggest that the standard phrase-based approach might not be appropriate to learn strong simplifications which are needed for certain target populations. 1 2 Related Work Specia (2010) achieves BLEU score of 60.75 on a small (only 4,483 sentence pairs) dataset in Brazilian Portuguese, using the standard phrasebased translation model (Koehn et al., 2003) in the Moses toolkit (Koehn et al., 2007). The dataset consists of original sentences and their corresponding manually simplified versions obtained under the PorSimples project (Alu´ısio and Gasperin, 2010) following specific guidelines. Coster and Kauchak (2011a) exploit the same translation model to learn how to simplify English sentences using 137,000 sentence pairs from Wikipedia and Simple English Wikipedia. They show that those results (BLEU = 59.87) can be improved by adding phrasal deletion to the probabilistic translation model, reaching the BLEU score of 60.46. Both those approaches seem to outperform all previous non-MT approaches to TS for English. The fact that Specia (2010) and Coster and Kauchak (2011a) achieve similar performances of the PB-SMT system in spite of large differences in size of their datasets motivates our hypothesis tha"
R15-1079,P15-2135,1,0.876423,"Missing"
R15-1079,P11-2117,0,0.399424,"dings suggest that the standard phrase-based approach might not be appropriate to learn strong simplifications which are needed for certain target populations. 1 2 Related Work Specia (2010) achieves BLEU score of 60.75 on a small (only 4,483 sentence pairs) dataset in Brazilian Portuguese, using the standard phrasebased translation model (Koehn et al., 2003) in the Moses toolkit (Koehn et al., 2007). The dataset consists of original sentences and their corresponding manually simplified versions obtained under the PorSimples project (Alu´ısio and Gasperin, 2010) following specific guidelines. Coster and Kauchak (2011a) exploit the same translation model to learn how to simplify English sentences using 137,000 sentence pairs from Wikipedia and Simple English Wikipedia. They show that those results (BLEU = 59.87) can be improved by adding phrasal deletion to the probabilistic translation model, reaching the BLEU score of 60.46. Both those approaches seem to outperform all previous non-MT approaches to TS for English. The fact that Specia (2010) and Coster and Kauchak (2011a) achieve similar performances of the PB-SMT system in spite of large differences in size of their datasets motivates our hypothesis tha"
R15-1079,N03-1017,0,0.112247,"g the standard phrasebased translation model. However, our results indicate that this approach works well only when the training dataset consists mostly of those sentence pairs in which the simplified sentence is already very similar to its original. Our findings suggest that the standard phrase-based approach might not be appropriate to learn strong simplifications which are needed for certain target populations. 1 2 Related Work Specia (2010) achieves BLEU score of 60.75 on a small (only 4,483 sentence pairs) dataset in Brazilian Portuguese, using the standard phrasebased translation model (Koehn et al., 2003) in the Moses toolkit (Koehn et al., 2007). The dataset consists of original sentences and their corresponding manually simplified versions obtained under the PorSimples project (Alu´ısio and Gasperin, 2010) following specific guidelines. Coster and Kauchak (2011a) exploit the same translation model to learn how to simplify English sentences using 137,000 sentence pairs from Wikipedia and Simple English Wikipedia. They show that those results (BLEU = 59.87) can be improved by adding phrasal deletion to the probabilistic translation model, reaching the BLEU score of 60.46. Both those approaches"
R15-1079,D11-1038,0,0.0296337,"e size of the datasets but rather in the nature of the sentence pairs used for training and tuning of the PB-SMT models. Introduction Text Simplification (TS) aims to convert complex texts into simpler variants which are more accessible to a wider audiences, e.g. non-native speakers, children, and people diagnosed with intellectual disability, autism, aphasia, dyslexia or congenital deafness. In the last twenty years, many automatic text simplification systems have been proposed, varying from rule-based, e.g. (Brouwers et al., 2014; Saggion et al., 2015) to datadriven, e.g. (Zhu et al., 2010; Woodsend and Lapata, 2011), and hybrid (Siddharthan and Angrosh, 2014). Since 2010, there have been several attempts to approach TS as a machine translation (MT) problem (Specia, 2010; Coster and ˇ Kauchak, 2011a; Stajner, 2014). Instead of translating sentences from one language to another, the goal of text simplification is to translate sentences from ‘original’ to ‘simplified’ language. In this paper, we seek to explore the main reasons for the success or failure of the phrasebased statistical machine translation (PB-SMT) 3 Methodology We apply the following methodology: • We run MT-based text simplification exper61"
R15-1079,C10-1152,0,0.414816,"tion (TS) task as a machine translation (MT) problem. They report promising results in learning how to translate from ‘original’ to ‘simplified’ language using the standard phrasebased translation model. However, our results indicate that this approach works well only when the training dataset consists mostly of those sentence pairs in which the simplified sentence is already very similar to its original. Our findings suggest that the standard phrase-based approach might not be appropriate to learn strong simplifications which are needed for certain target populations. 1 2 Related Work Specia (2010) achieves BLEU score of 60.75 on a small (only 4,483 sentence pairs) dataset in Brazilian Portuguese, using the standard phrasebased translation model (Koehn et al., 2003) in the Moses toolkit (Koehn et al., 2007). The dataset consists of original sentences and their corresponding manually simplified versions obtained under the PorSimples project (Alu´ısio and Gasperin, 2010) following specific guidelines. Coster and Kauchak (2011a) exploit the same translation model to learn how to simplify English sentences using 137,000 sentence pairs from Wikipedia and Simple English Wikipedia. They show t"
R15-1080,W10-1607,0,0.0760938,"Missing"
R15-1080,J03-1002,0,0.0260985,"V word was complex), but it will not necessarily deteriorate the grammaticality and meaning preservation of the output sentence (as would be the case in cross-lingual SMT). The problem of accuracy is still present even in monolingual SMT. A small model will not have high enough probability mass to be able to generalise well all the linguistic phenomena a good translation should encompass. The translation model will suffer from a low number of examples and thus might not be able to estimate the probabilities correctly. The unsupervised alignment model implemented in Moses using GIZA++ aligner (Och and Ney, 2003) will have rough statistics for the alignment estimation if computed from a small number of parallel sentences. Related Work With the emergence of the Simple English Wikipedia1 , which together with the “original” English Wikipedia offered a large comparable text simplification (TS) corpus (137,000 sentence pairs), the focus of the ATS for English was shifted towards data-driven approaches. Most of them applied various SMT techniques, either phrasebased (Coster and Kauchak, 2011; Wubben et al., 2012), or syntax-based (Zhu et al., 2010; Woodsend and Lapata, 2011). In other languages, TS corpora"
R15-1080,C12-1023,1,0.848194,"(“heavy” or “light”) significantly influences the results, and (2) the model built using the “light” corpora can still learn some useful simplifications deˇ spite the very small size of the dataset (Stajner, 2014). 2.1 2.3 State-of-the-Art ATS System for Spanish The current state-of-the-art text simplification system for Spanish (Saggion et al., 2015) was built under the Simplext project.2 It employs a modular approach to TS, consisting of three main modules: a rule-based syntactic and lexical simplification modules (Drndarevi´c et al., 2013); and a synonymbased lexical simplification module (Bott et al., 2012). According to the recent evaluation of the full Simplext system (Saggion et al., 2015), the system achieved human scores for grammaticality, meaning preservation, and simplicity comparable to those of the current state-of-the-art data-driven text simplification systems for English (Wubben et al., 2012; Angrosh and Siddharthan, 2014). SMT for Low-Resourced Languages The main problems in SMT applied to lowresourced languages (“simplified” Spanish can be seen as such) are the accuracy and coverage (Irvine and Callison-Burch, 2013). The first problem is the result of the fact that the model does"
R15-1080,P02-1040,0,0.0924012,"g the Light corpus was rated better than the output of the systems built using the Heavy corpus on the same test sentence (Table 5), and (2) the output of the HIERO models was rated better than the output of Results The results of the automatic and human evaluations are presented in the next two subsections. 4.1 Light Manual PB HIERO 4.03 3.91 4.61 5 4 5 5 5 5 4.57 4.40 3.62 5 5 4 5 5 4 2.99 2.93 4.40 3 3 5 2 2 5 Automatic Evaluation We compared the performances of the systems using two automatic MT evaluation metrics, the sentence-level BLEU score (S-BLEU)3 and the document-level BLEU score (Papineni et al., 2002). As the baseline, we used the system which makes no changes to the input (i.e. output of the system is the original sentence). This seems as a natural baseline for this specific task (ATS), as all previous studies (Specia, 2010; Coster and ˇ Kauchak, 2011; Stajner, 2014) reported that their systems are overcautious, usually making only a few or no changes to the input sentence, and only slightly outperform this baseline. For calculating the S-BLEU and BLEU scores, we used the manual simplification (‘gold standard’) as the reference, and the original sentences and the outputs of the four syste"
R15-1080,W14-1206,0,0.0369078,"Missing"
R15-1080,J07-2003,0,0.0290448,"r´oxima reuni´on del Comit´e del Patrimonio Mundial, que ser´a en Bahrein en junio de 2011. Los expertos presentar´an un informe sobre Pompeya en la pr´oxima reuni´on sobre la cultura del mundo. Esta reuni´on ser´a en junio de 2011. Table 1: Different levels of simplification (deviations from the original sentence are shown in italics) Corpus Light Light Heavy Heavy in the next three subsections. 3.1 Corpora In order to test the influence of the level of simplification in TS datasets (“heavy” or “light”) on the system performance, we trained the standard PB-SMT (Koehn et al., 2003) and HIERO (Chiang, 2007) models in the Moses toolkit (Koehn et al., 2007) on two TS corpora: Training 659 659 725 725 Dev. 100 100 100 100 Test 94 94 94 94 Table 2: SMT experiments tokens. The sizes of the datasets used in the four experiments are given in Table 2. 1. Heavy – The TS corpus built under the Simplext project (Saggion et al., 2011), aimed at simplifying texts for people with intellectual disabilities. The original news stories were simplified manually by trained human editors, following detailed guidelines (Anula, 2007). 3.3 Evaluation In order to obtain better insights into the potential problems in the"
R15-1080,W11-1601,0,0.234658,"able to estimate the probabilities correctly. The unsupervised alignment model implemented in Moses using GIZA++ aligner (Och and Ney, 2003) will have rough statistics for the alignment estimation if computed from a small number of parallel sentences. Related Work With the emergence of the Simple English Wikipedia1 , which together with the “original” English Wikipedia offered a large comparable text simplification (TS) corpus (137,000 sentence pairs), the focus of the ATS for English was shifted towards data-driven approaches. Most of them applied various SMT techniques, either phrasebased (Coster and Kauchak, 2011; Wubben et al., 2012), or syntax-based (Zhu et al., 2010; Woodsend and Lapata, 2011). In other languages, TS corpora either do not exist or they are very limited in size (only up to 1,000 sentence pairs). The only known exception to this is the case of Brazilian Portuguese for which there is a parallel TS corpus with 4,483 sentence pairs, built under the PorSimples project (Alu´ısio and Gasperin, 2010), aimed at simplifying texts for low literacy readers. This corpus has been used to train the standard PB-SMT model for ATS (Specia, 2010), and the reported results were promising (BLEU = 60.75)"
R15-1080,W13-2233,0,0.0218352,"s (Drndarevi´c et al., 2013); and a synonymbased lexical simplification module (Bott et al., 2012). According to the recent evaluation of the full Simplext system (Saggion et al., 2015), the system achieved human scores for grammaticality, meaning preservation, and simplicity comparable to those of the current state-of-the-art data-driven text simplification systems for English (Wubben et al., 2012; Angrosh and Siddharthan, 2014). SMT for Low-Resourced Languages The main problems in SMT applied to lowresourced languages (“simplified” Spanish can be seen as such) are the accuracy and coverage (Irvine and Callison-Burch, 2013). The first problem is the result of the fact that the model does not have enough data to estimate good probabilities over the possible translations and therefore ensure correctness of the translation pairs. The second problem occurs when the model and its word coverage are small, which leads to a high number of out-of-vocabulary (OOV) words. Words which 1 Monolingual SMT 3 Methodology The corpora, translation/simplification experiments, and the evaluation procedure are presented 2 https://simple.wikipedia.org/wiki/Main Page 619 www.simplext.es Version Original Light Heavy Example Los expertos"
R15-1080,D11-1038,0,0.0471341,"lemented in Moses using GIZA++ aligner (Och and Ney, 2003) will have rough statistics for the alignment estimation if computed from a small number of parallel sentences. Related Work With the emergence of the Simple English Wikipedia1 , which together with the “original” English Wikipedia offered a large comparable text simplification (TS) corpus (137,000 sentence pairs), the focus of the ATS for English was shifted towards data-driven approaches. Most of them applied various SMT techniques, either phrasebased (Coster and Kauchak, 2011; Wubben et al., 2012), or syntax-based (Zhu et al., 2010; Woodsend and Lapata, 2011). In other languages, TS corpora either do not exist or they are very limited in size (only up to 1,000 sentence pairs). The only known exception to this is the case of Brazilian Portuguese for which there is a parallel TS corpus with 4,483 sentence pairs, built under the PorSimples project (Alu´ısio and Gasperin, 2010), aimed at simplifying texts for low literacy readers. This corpus has been used to train the standard PB-SMT model for ATS (Specia, 2010), and the reported results were promising (BLEU = 60.75) despite the small size of the dataset. The recent attempt at using the standard PB-S"
R15-1080,N03-1017,0,0.0361006,"onservaci´on de Pompeya en la pr´oxima reuni´on del Comit´e del Patrimonio Mundial, que ser´a en Bahrein en junio de 2011. Los expertos presentar´an un informe sobre Pompeya en la pr´oxima reuni´on sobre la cultura del mundo. Esta reuni´on ser´a en junio de 2011. Table 1: Different levels of simplification (deviations from the original sentence are shown in italics) Corpus Light Light Heavy Heavy in the next three subsections. 3.1 Corpora In order to test the influence of the level of simplification in TS datasets (“heavy” or “light”) on the system performance, we trained the standard PB-SMT (Koehn et al., 2003) and HIERO (Chiang, 2007) models in the Moses toolkit (Koehn et al., 2007) on two TS corpora: Training 659 659 725 725 Dev. 100 100 100 100 Test 94 94 94 94 Table 2: SMT experiments tokens. The sizes of the datasets used in the four experiments are given in Table 2. 1. Heavy – The TS corpus built under the Simplext project (Saggion et al., 2011), aimed at simplifying texts for people with intellectual disabilities. The original news stories were simplified manually by trained human editors, following detailed guidelines (Anula, 2007). 3.3 Evaluation In order to obtain better insights into the"
R15-1080,P12-1107,0,0.294938,"Missing"
R15-1080,C10-1152,0,0.264479,"Missing"
R15-1080,2005.mtsummit-papers.11,0,0.0101995,"An example of an original sentence and its corresponding manual simplifications in the two corpora is given in Table 1. 3.2 Model PB-SMT HIERO PB-SMT HIERO SMT Models In order to compare the impact of different SMT models (PB vs. HIERO) on the system performance, the language model (LM) and the test set (consisting of 47 sentence pairs from each of the two corpora) were kept the same for all systems. Ideally, the LM should be trained on a large corpus of “simplified” Spanish. However, as such a corpus has not been compiled yet, we trained the LM on a subset of the Europarl v7 Spanish corpus (Koehn, 2005) using the SRILM toolkit (Stolcke, 2002). In order to reduce the complexity of the sentences used for the training of the LM, we filtered out all sentences that contain more than 15 620 System Corpus Light PB Heavy Light HIERO Heavy Baseline S-BLEU 0.3742 0.3662 0.3718 0.2959 0.3645 BLEU 0.3374 0.3313 0.3336 0.2718 0.3260 Heavy PB HIERO Mean 1.74 1.77 G Median 1 1 Mode 1 1 Mean 1.98 1.93 M Median 1 1 Mode 1 1 Mean 2.31 2.29 S Median 2 2 Mode 1 1 Aspect Table 3: Automatic evaluation sponding simplified variants) were selected randomly from the test set under the criterion that they have been mo"
R15-1080,W14-5604,1,0.736955,"produced by the two best SMT systems and the third by the Simplext system) in order to directly compare the performances of the SMT systems with the state-ofthe-art (rule-based) text simplification system for Spanish (Section 2.3). We obtained a total of 260 human scores for each aspect-system-corpora combination in each of the two evaluation phases. The 40 original sentences for human evaluation (and their corre2. Light – The TS corpus consisting of various texts (some of which present in the Heavy corpus) and their manual simplifications obtained using only six main simplifiˇ cation rules (Mitkov and Stajner, 2014). In both corpora, the sentence-alignment was manually checked and corrected where necessary. An example of an original sentence and its corresponding manual simplifications in the two corpora is given in Table 1. 3.2 Model PB-SMT HIERO PB-SMT HIERO SMT Models In order to compare the impact of different SMT models (PB vs. HIERO) on the system performance, the language model (LM) and the test set (consisting of 47 sentence pairs from each of the two corpora) were kept the same for all systems. Ideally, the LM should be trained on a large corpus of “simplified” Spanish. However, as such a corpus"
R15-1080,P07-2045,0,\N,Missing
R15-1080,W14-4404,0,\N,Missing
R19-1131,2013.mtsummit-posters.8,0,0.0195825,"the output), the fluency of the translation can be improved. With regards to improving translation adequacy and post-editting effort, our experiments show that the type of the translation architecture (PBMT or NMT), and the strategy for using ATS system, both play a significant role. 2 Related Work For many language pairs (e.g. English-French, English-Spanish, English-Hindu), attempts were made at rewriting input sentences using paraphrasing or textual entailment to improve the performance of MT systems (Callison-Burch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015; Mirkin et al., 2013a,b). However, they all focus only on out-of-vocabulary words, or difficult to translate shorter n-grams. ˇ Stajner and Popovi´c (2016) went one step further, using lexico-syntactic automatic text simplification systems as a pre-processing step for English-to-Serbian machine translation. In this way, they covered both lexical and syntactic transformations on the source side. The ATS outputs were manually inspected by human editors who were also allowed to do minor revisions (correcting the tense, gender, article, etc.) in order to preserve grammaticality and the original meaning on the source"
R19-1131,W10-1607,0,0.0550848,"Missing"
R19-1131,P13-4015,0,0.0215202,"the output), the fluency of the translation can be improved. With regards to improving translation adequacy and post-editting effort, our experiments show that the type of the translation architecture (PBMT or NMT), and the strategy for using ATS system, both play a significant role. 2 Related Work For many language pairs (e.g. English-French, English-Spanish, English-Hindu), attempts were made at rewriting input sentences using paraphrasing or textual entailment to improve the performance of MT systems (Callison-Burch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015; Mirkin et al., 2013a,b). However, they all focus only on out-of-vocabulary words, or difficult to translate shorter n-grams. ˇ Stajner and Popovi´c (2016) went one step further, using lexico-syntactic automatic text simplification systems as a pre-processing step for English-to-Serbian machine translation. In this way, they covered both lexical and syntactic transformations on the source side. The ATS outputs were manually inspected by human editors who were also allowed to do minor revisions (correcting the tense, gender, article, etc.) in order to preserve grammaticality and the original meaning on the source"
R19-1131,2010.eamt-1.31,0,0.0104101,", and with minimal manual correction of the output), the fluency of the translation can be improved. With regards to improving translation adequacy and post-editting effort, our experiments show that the type of the translation architecture (PBMT or NMT), and the strategy for using ATS system, both play a significant role. 2 Related Work For many language pairs (e.g. English-French, English-Spanish, English-Hindu), attempts were made at rewriting input sentences using paraphrasing or textual entailment to improve the performance of MT systems (Callison-Burch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015; Mirkin et al., 2013a,b). However, they all focus only on out-of-vocabulary words, or difficult to translate shorter n-grams. ˇ Stajner and Popovi´c (2016) went one step further, using lexico-syntactic automatic text simplification systems as a pre-processing step for English-to-Serbian machine translation. In this way, they covered both lexical and syntactic transformations on the source side. The ATS outputs were manually inspected by human editors who were also allowed to do minor revisions (correcting the tense, gender, article, etc.) in order to preserve grammaticalit"
R19-1131,N06-1003,0,0.0867505,"ng for its grammaticality and meaning preservation, and with minimal manual correction of the output), the fluency of the translation can be improved. With regards to improving translation adequacy and post-editting effort, our experiments show that the type of the translation architecture (PBMT or NMT), and the strategy for using ATS system, both play a significant role. 2 Related Work For many language pairs (e.g. English-French, English-Spanish, English-Hindu), attempts were made at rewriting input sentences using paraphrasing or textual entailment to improve the performance of MT systems (Callison-Burch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015; Mirkin et al., 2013a,b). However, they all focus only on out-of-vocabulary words, or difficult to translate shorter n-grams. ˇ Stajner and Popovi´c (2016) went one step further, using lexico-syntactic automatic text simplification systems as a pre-processing step for English-to-Serbian machine translation. In this way, they covered both lexical and syntactic transformations on the source side. The ATS outputs were manually inspected by human editors who were also allowed to do minor revisions (correcting the tense, gender, article,"
R19-1131,C96-2183,0,0.808785,"s, a simpler variant is the one that requires a shorter reading time and leads to better text comprehension scores. In the case of text or sentence simplification used as a preprocessing step for a given natural language processing (NLP) task, e.g. machine translation (MT), information extraction (IE), summarization, and semantic role labeling (SRL), a simpler variant is the one that leads to better performances of that NLP system. Text simplification was originally proposed as a pre-processing step for machine translation (Chandrasekar, 1994) and later for information extraction and parsing (Chandrasekar et al., 1996). At those early stages, automated text simplification (ATS) was not mature enough to help improving performances of those systems. Instead, the idea was explored only hypothetically, using manual text simplification (Chandrasekar, 1994; Vickrey and Koller, 2008). Evans (2011) later showed that an automated simplification of coordinate structures can improve IE systems. Later, the focus of the ATS shifted towards text accessibility and better social inclusion, having the main goal of making texts easier to understand by 1141 Proceedings of Recent Advances in Natural Language Processing, pages"
R19-1131,P11-2117,0,0.177598,"n of the ATS output (see Section 3). We find that success of the ATS used as a pre-processing step heavily depends on the type of the MT system used (PBMT or NMT). 3 Experimental Setup We randomly selected 10 original articles from the 100 news articles automatically simplified by the state-of-the-art lexico-syntactic ATS system (Siddharthan and Angrosh, 2014) in the work of ˇ Stajner and Glavaˇs (2017). The ATS system that consists of a rule-based syntactic simplification module and a supervised lexical simplification module built upon the English Wikipedia - Simple English Wikipedia corpus (Coster and Kauchak, 2011). We further explored three possible scenarios in which ATS can be used as a pre-processing step for MT (Figure 1): 1142 • Scenario 1 (Corrected): Automatically simplified sentences are manually corrected before being used as the source sentences for MT, to ensure the preservation of the original meaning and the grammaticality of the MT input; • Scenario 2 (Filtered): Automatically simplioriginal texts STEP 1 automated text simplification automatically simplified sentences Scenario 1 STEP 2.1 Scenario 2 correcting (post editing) sentences with low G and M STEP 3 STEP 2.2 Scenario 3 filter sent"
R19-1131,W15-4913,1,0.861372,"Missing"
R19-1131,E14-1076,0,0.0262015,"rbian translation, we also use the current stateof-the-art neural MT system for that language pair (see Section 3.1). We explore three different scenarios for using ATS as the pre-processing step, in search for fully automatic use of ATS in MT, without human correction of the ATS output (see Section 3). We find that success of the ATS used as a pre-processing step heavily depends on the type of the MT system used (PBMT or NMT). 3 Experimental Setup We randomly selected 10 original articles from the 100 news articles automatically simplified by the state-of-the-art lexico-syntactic ATS system (Siddharthan and Angrosh, 2014) in the work of ˇ Stajner and Glavaˇs (2017). The ATS system that consists of a rule-based syntactic simplification module and a supervised lexical simplification module built upon the English Wikipedia - Simple English Wikipedia corpus (Coster and Kauchak, 2011). We further explored three possible scenarios in which ATS can be used as a pre-processing step for MT (Figure 1): 1142 • Scenario 1 (Corrected): Automatically simplified sentences are manually corrected before being used as the source sentences for MT, to ensure the preservation of the original meaning and the grammaticality of the M"
R19-1131,P08-1040,0,0.0367483,"tion (MT), information extraction (IE), summarization, and semantic role labeling (SRL), a simpler variant is the one that leads to better performances of that NLP system. Text simplification was originally proposed as a pre-processing step for machine translation (Chandrasekar, 1994) and later for information extraction and parsing (Chandrasekar et al., 1996). At those early stages, automated text simplification (ATS) was not mature enough to help improving performances of those systems. Instead, the idea was explored only hypothetically, using manual text simplification (Chandrasekar, 1994; Vickrey and Koller, 2008). Evans (2011) later showed that an automated simplification of coordinate structures can improve IE systems. Later, the focus of the ATS shifted towards text accessibility and better social inclusion, having the main goal of making texts easier to understand by 1141 Proceedings of Recent Advances in Natural Language Processing, pages 1141–1150, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_131 various target readers, e.g. people with low literacy levels (Alu´ısio and Gasperin, 2010), or people with some kind of reading or cognitive impairments, such as aphasia (De"
R19-1131,W16-3411,1,0.913909,"Missing"
S19-2057,P17-1036,0,0.0288822,"nant OTHERS class. The model architecture is similar to our threeinput one (see Figure 1). However, the three conversational turns (T1, T2, and T3) are fed to the model as a single concatenated input, with additional tokens to mark the turn boundaries. The auxiliary output is connected to the output of the attention. This forces the attention weights to favour the emotional vs. OTHERS task. We use the pretrained word embeddings described in Section 3.1.1. Our dense layers use the leaky version (LReLU) of the Rectified Linear Unit (ReLU) activation. In addition, we use the attention mechanism (He et al., 2017). Finally, we use the batch normalisation (Ioffe and Szegedy, 2015) to process the attention output. 3.1.3 4 Results We evaluate our systems using precision (P) and recall (R) per each emotional class, and the micro F1 -score over the three “emotional” classes (the metric used by the task organisers for the official evaluation). The results for the baseline and the four neural systems are presented in Table 4. The results of the best ensemble models (trained on the per class probabilities of the four neural models) are presented in Table 5. We can notice that: (1) Our best neural system (IN3)"
S19-2057,P04-3031,0,0.137361,"s very difficult even for humans (Section 2.2); (b) for this task, neural approaches outperform a strong non-neural baseline (Section 4); (c) an ensemble of neural systems with differ2.1 Preprocessing The language of this corpus presents many of the features of micro-blogging language: large use of contractions (e.g. I’m gonna bother), elongations (e.g. a vacation tooooooo!), non-standard use of punctuation (e.g. gonna explain you later..!), incorrect spelling (e.g. U r). To properly handle this language, we build a simple preprocessing pipeline which consists of: (1) the NLTK TweetTokenizer (Bird and Loper, 2004); and (2) a normalisation strategy that reduces sparseness by lowercasing all the words and converting elongations like looool to lol. These steps are used in all the experiments. Some of our models use additional preprocessing described further in the text. ∗ * The first four authors have contributed equally to this work and are ordered alphabetically. 330 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 330–334 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics ID 71 78 91 140 TURN 1 TURN Not good I hate Sir"
S19-2057,Q17-1010,0,0.0176628,"Missing"
S19-2057,N16-1174,0,0.0214153,"rong non-neural baseline we set up a linear SVM model with word and character n-grams (1-6) as features.1 3.1 Neural Models We propose four neural network models that slightly differ on their objective. 3.1.1 Three-Input Model (IN3) Having the three conversation turns (T1, T2, and T3), we explicitly represent the position of each sequence in the conversation by creating an input branch for each turn. The branches are identical and represent the text using word embeddings that feed a 2-layer bidirectional Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997). An attention mechanism (Yang et al., 2016) combines its hidden states. This architecture allows to independently process and attend to the most relevant parts of T1, T2, and T3. The information is later combined by a simple concatenation and few fully connected dense layers. The model architecture is shown in Figure 1. c Symanto ReWe use a proprietary model search to obtain 300-dimensional word embeddings on the English Wikipedia. The performance Experimental Setup We first randomly selected two times 2754 instances from the official training set, maintaining the class ratio that was announced for the official dev and test sets (4:4:4"
S19-2057,S19-2005,0,0.0241968,"its smallest class; (2) up-sampling the emotionrelated labels with an in-house dataset; and (3) up-sampling by duplicating a random portion of the dataset. None of these solutions worked, and therefore, we trained our best models using the data provided by the organisers. Introduction Detecting emotions in text is a key task in many scenarios, such as social listening, personalised marketing, customer caring, or in building emotionally intelligent chat-bots: in this last case, the task complexity increases, since a bot’s response might influence the user’s emotion. The EmoContext shared task (Chatterjee et al., 2019) was posed as a sequence classification task. Given a set of three conversational turns (human– bot–human), the goal is to predict the emotion of the third turn. The label space contains the emotions SAD , ANGRY and HAPPY, and the label OTHERS denoting anything else (emotional or non-emotional), as illustrated in Table 1. In this paper, we present our approaches to EmoContext shared task, and describe our best system in details. Additionally, we show that: (a) this task is very difficult even for humans (Section 2.2); (b) for this task, neural approaches outperform a strong non-neural baseline"
W11-4112,2008.amta-papers.5,1,0.883804,"Missing"
W11-4112,P97-1032,0,0.120473,"Missing"
W14-1201,R13-2011,1,0.875676,"Missing"
W14-1201,E99-1042,0,0.198958,"ality and meaning preservation in text snippets. As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. 1 Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum ˇ disorder (Stajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the origin"
W14-1201,W03-1602,0,0.0740564,"sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. 1 Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum ˇ disorder (Stajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Wood• It is the first study to explore the possibil"
W14-1201,C96-2183,0,0.779222,"tences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum ˇ disorder (Stajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Wood• It is the first study to explore the possibility of replacing human assessment of the quality of TS system output with automatic evaluation. • It is the first study to investigate the correlation of human assessment of TS system output with MT evaluation metrics. • It proposes a decision-making procedu"
W14-1201,W11-1601,0,0.139643,"Missing"
W14-1201,P03-1054,0,0.00867588,"hu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 ˇ Table 1: IAA from (Glavaˇs and Stajner, 2013) The simplified versions of original sentences were obtained by using four different simplification methods: baseline, sentence-wise, eventwise, and pronominal anaphora. The baseline retains only the main clause of a sentence, and discards all subordinate clauses, based on the output of the Stanford constituency parser (Klein and Manning, 2003). Sentence-wise simplification eliminates all those tokens in the original sentence that do not belong to any of the extracted factual event mentions, while the event-wise simplification transforms each factual event mention into a separate sentence of the output. The last simplification scheme (pronominal anaphora) additionally employs pronominal anaphora resolution on top of the event-wise simplification scheme.3 Methodology 3.2 All experiments were conducted on a freely available sentence-level dataset1 , fully described in ˇ (Glavaˇs and Stajner, 2013), and the two datasets we derived from"
W14-1201,P02-1040,0,0.106571,"ree of simplification. Woodsend and Lapata (2011b), and Glavaˇs ˇ and Stajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 ˇ Table 1: IAA from (Glavaˇs and Stajner, 2013) The simplified versions of original sentences were obtained by using four different simplification methods: baseline, sentence-wise, eventwise, and pronominal anaphora. The baseline retains only the main clause of a sentence, and discards all subordinate clauses, based on the output of the Stanford constituency parser (Klein and Manning, 2003). Sentence-wise simplification eliminates all those tokens in the original sentence that do not belong to any of the extracted factual event menti"
W14-1201,D11-1038,0,0.180995,"n for Computational Linguistics 2 Category weighted κ Pearson MAE Grammaticality 0.68 0.77 0.18 Meaning 0.53 0.67 0.37 Simplicity 0.54 0.60 0.28 Related Work The output of the TS system proposed by Siddharthan (2006) was rated for grammaticality and meaning preservation by three human evaluators. Similarly, Drndarevic et al. (2013) evaluated the grammaticality and the meaning preservation of automatically simplified Spanish sentences on a Likert scale with the help of twenty-five human annotators. Additionally, the authors used seven readability metrics to assess the degree of simplification. Woodsend and Lapata (2011b), and Glavaˇs ˇ and Stajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002),"
W14-1201,P12-1107,0,0.436511,"Missing"
W14-1201,W11-2112,0,0.0548789,"fully automatic manner – using some readability measures or average sentence length as features ˇ (as in (Drndarevi´c et al., 2013; Glavaˇs and Stajner, 4 2013) for example). 3.4 3.5 Goal After we obtained the six automatic metrics (cosine, METEOR, TERp, TINE, T-BLEU, and SRL), we performed two sets of experiments, trying to answer two main questions: Features: MT Evaluation Metrics In all experiments, we focused on six commonly used MT evaluation metrics. These are cosine similarity (using the bag-of-words representation), METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011), and two components of TINE: T-BLEU (which differs from the standard BLEU (Papineni et al., 2002) by using 3-grams, 2-grams, and 1-grams when there are no 4-grams found, where the “original” BLEU would give score ‘0’) and SRL (which is the component of TINE based on semantic role labeling using SENNA4 ). Although these two components contribute equally to TINE (thus being linearly correlated with TINE), we wanted to investigate which one of them contributes more to the correlation of TINE with human judgements. Given their different natures, we expect T-BLEU to contribute more to the correlat"
W14-1201,C10-1152,0,0.652024,"012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Wood• It is the first study to explore the possibility of replacing human assessment of the quality of TS system output with automatic evaluation. • It is the first study to investigate the correlation of human assessment of TS system output with MT evaluation metrics. • It proposes a decision-making procedure for the classification of simplified sentences into: (1) those which are acceptable; (2) those which need further post-editing; and (3) those which should be discarded. 1 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @"
W14-1201,ruiter-etal-2010-human,0,0.038341,"Missing"
W14-1201,W09-0441,0,0.384081,"vaˇs ˇ and Stajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 ˇ Table 1: IAA from (Glavaˇs and Stajner, 2013) The simplified versions of original sentences were obtained by using four different simplification methods: baseline, sentence-wise, eventwise, and pronominal anaphora. The baseline retains only the main clause of a sentence, and discards all subordinate clauses, based on the output of the Stanford constituency parser (Klein and Manning, 2003). Sentence-wise simplification eliminates all those tokens in the original sentence that do not belong to any of the extracted factual event mentions, while the event-wise simplification transforms each"
W14-1201,W11-2107,0,\N,Missing
W14-5604,P02-1040,0,0.130489,"so recorded the time needed to simplify each text as an indication of, among other things, ease of use of (and clarity for) each set of rules and its productivity in general; these results are reported in Tables 6 and 7 of the following section. Several experiments were conducted to assess the inter-annotator agreement. We believe that the interannotator agreement is another good indicator as to how straightforward it is to apply a specific set of simplification rules and how reliable the simplification process is in general. We compute the interannotator agreement in terms of the BLEU score (Papineni et al., 2002). BLEU score is widely used in MT to compare the reference translation with the output of the system (translation hypothesis). Here we use the BLEU score to compare the simple sentences produced by one annotator with the corresponding sentences of another annotator. We measure the inter-annotator agreement for all three pairs of annotators (Table 8). In addition, we examined how many times each of the rules was selected by each writer which in our view would be not only a way of accounting for agreement and but also assessing the usefulness of every rule and how balanced a set of rules is in g"
W14-5604,I13-1043,1,0.826895,"eriments. In order to minimise potential familiarity effect (texts which already have been simplified are expected to be simplified faster and more efficiently as they are familiar to the writers), we allowed a few days interval between each time a specific text was simplified using different rules. We applied the Spauldings Spanish Readability index – SSR (Spaulding, 1956) as well as the Lexical Complexity index – LC (Anula, 2007) to assess the readability of the simplified texts. Both metrics have ˇ shown a good correlation with the possible reading obstacles for various target populations (Stajner and Saggion, 2013), and were used for the evaluation of the automatic TS system in Simplext (Drndarevi´c et al., 2013). We also asked a third writer to simplify samples from the texts used by the first two writers which were pre-assessed to be of comparable complexity, with a view to establishing whether familiarisation has an effect on the output. The results of these readability experiments are presented in Tables 4 and 5 of the following section. We also recorded the time needed to simplify each text as an indication of, among other things, ease of use of (and clarity for) each set of rules and its productiv"
W14-5606,W13-1502,0,0.0461289,"Missing"
W14-5606,W03-1004,0,0.0639734,"ences. The main page states that SEW is for everyone, including children and adults who are learning English. All previously mentioned studies conducted on that corpus evaluated the quality of the generated output in terms of grammaticality, meaning preservation, and simplicity, but not usefulness. Also, there have been no comparisons of the types of transformations present in EW–SEW with any of the other TS corpora in English which were simplified with a specific target population in mind, e.g. Encyclopedia Britannica and its manually simplified versions for children – Britannica Elementary (Barzilay and Elhadad, 2003)6 , Guardian Weekly and its manually simplified versions for language learners (Allen, 2009), and the FIRST corpus of various texts simplified for people with autism spectrum disorder (ASD)7 . In this study, we compare the original and simplified texts of the four aforementioned TS corpora in terms of nineteen features which measure the complexity of texts for people with ASD. Although these features were derived from user requirements for people with ASD, many of them are known to present reading obstacles for other target populations as well (e.g. children or language learners). Given the la"
W14-5606,P11-2087,0,0.0174705,"t twenty years, many ATS systems have been proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4 , together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf 2 http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf 3 http://www.w3.org/WAI/ 4 http://simple.wikipedia.org/wiki/Main Page 5 http://wikipedia.org/wiki/Main Page 53 Proceedings o"
W14-5606,W11-1601,0,0.0130254,"osed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4 , together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf 2 http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf 3 http://www.w3.org/WAI/ 4 http://simple.wikipedia.org/wiki/Main Page 5 http://wikipedia.org/wiki/Main Page 53 Proceedings of the Workshop on Automatic Text Simplification: Meth"
W14-5606,R13-1029,1,0.828783,"iculty inferring contextual information or may have trouble understanding mental verbs, emotional language, and long sentences with complex syntactic structure (Tager-Flusberg, 1981; Kover et al., 2012). To address these difficulties, a tool is being developed in the FIRST project8 to assist in the process of making texts more accessible for people with ASD. To achieve this, three modues are exploited: 1. Structural complexity processor, which detects syntactically complex sentences and generates alternatives to such sentences in the form of sequences of shorter sentences (Evans et al., 2014; Dornescu et al., 2013). 2. Meaning disambiguator, which resolves pronominal references, performs word sense disambiguation, and detects lexicalised (conventional) metaphors (Barbu et al., 2013). 3. Personalised document generator, which aggregates the output of processors 1 and 2 and generates additional elements such as glossaries, illustrative images, and document summaries. The system, named Open Book, is deployed as an editing tool for healthcare and educational service providers. It functions semi-automatically, exploiting the three processors and requiring the user to authorise the application of the conversi"
W14-5606,W14-1215,1,0.834926,". They may have difficulty inferring contextual information or may have trouble understanding mental verbs, emotional language, and long sentences with complex syntactic structure (Tager-Flusberg, 1981; Kover et al., 2012). To address these difficulties, a tool is being developed in the FIRST project8 to assist in the process of making texts more accessible for people with ASD. To achieve this, three modues are exploited: 1. Structural complexity processor, which detects syntactically complex sentences and generates alternatives to such sentences in the form of sequences of shorter sentences (Evans et al., 2014; Dornescu et al., 2013). 2. Meaning disambiguator, which resolves pronominal references, performs word sense disambiguation, and detects lexicalised (conventional) metaphors (Barbu et al., 2013). 3. Personalised document generator, which aggregates the output of processors 1 and 2 and generates additional elements such as glossaries, illustrative images, and document summaries. The system, named Open Book, is deployed as an editing tool for healthcare and educational service providers. It functions semi-automatically, exploiting the three processors and requiring the user to authorise the app"
W14-5606,grover-etal-2000-lt,0,0.0638007,"sentences in the text [UR309] Indicate the range of vocabulary used in the text [UR401, UR425, UR504, UR505, UR511] Table 3: Text complexity formulae (w – the number of words in the text; s – the number of sentences in the text; ptyp – the number of polysemic word types in the text; c – the number of commas in the text; typ – the number of word types in the text; tok – the number of word tokens in the text) Scores for these measures, and the text complexity formulae that exploit them where obtained automatically by the tokeniser, part-of-speech tagger, and lemmatiser distributed with LT TTT2 (Grover et al., 2000). Detection of the features used to derive complexity measures also involved the use of additional resources such as WordNet, gazetteers of rare illative, comparative, and adversative conjunctions, negatives (words and prefixes) and a set of lexico-syntactic patterns used to detect passive verbs (presented in Figure 1). 57 am/are/is/was/were wRB * w{V BN |V BD} am/are/is/was/were wRB * being wRB * w{V BN |V BD} have/has/had wRB * been wRB * w{V BN |V BD} will wRB * be wRB * w{V BN |V BD} am/is/are wRB * going wRB * to wRB * be wRB * w{V BN |V BD} wM D wRB * be w{V BN |V BD} wM D wRB * have wRB"
W14-5606,W03-1602,0,0.0429539,"writing”2 , and the W3C – Web Accessibility Initiative guidelines3 . However, manual adaptation of texts cannot match the speed with which new texts are published on the web in order to provide up to date information. The aim of Automatic Text Simplification (ATS) is to automatically (or at least semi-automatically) convert complex sentences into a more accessible form while preserving their original meaning. In the last twenty years, many ATS systems have been proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4 , together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creative Commons Attribu"
W14-5606,P13-1151,0,0.0150547,"of 20 texts from the Encyclopedia Britannica and their manually simplified versions aimed at children – Britannica Elementary (Barzilay and Elhadad, 2003)11 . 3. The corpus of 110 randomly selected corresponding articles from EW and SEW. Here, it is important to note that, in general, articles from SEW do not represent direct simplifications of the articles from EW, they just have a matching topic. For this reason, we did not use complete EW and SEW articles. We only used those sentences in original and simplified versions, which existed in the sentence-aligned parallel corpora version 2.012 (Kauchak, 2013). 4. The corpus of 25 texts on various topics manually simplified for people with autism, compiled in the FIRST project13 , for the purpose of a piloting task14 . The texts were simplified by carers of people with ASD in accordance with specified guidelines. 10 http://www.onestopenglish.com/ http://www.cs.columbia.edu/ noemie/alignment/ 12 http://www.cs.middlebury.edu/ dkauchak/simplification/ 13 www.first-asd.eu 14 http://www.first-asd.eu/?q=system/files/FIRST D7.2 20130228 annex.pdf 11 56 4.2 Text Features Relevant to User Requirements In this paper, a set of 15 text complexity measures and"
W14-5606,W10-0406,0,0.0605365,"Missing"
W14-5606,D11-1038,0,0.0206253,"ATS systems have been proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4 , together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf 2 http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf 3 http://www.w3.org/WAI/ 4 http://simple.wikipedia.org/wiki/Main Page 5 http://wikipedia.org/wiki/Main Page 53 Proceedings of the Workshop on Automatic"
W14-5606,P12-1107,0,0.0335249,"Missing"
W14-5606,N10-1056,0,0.0286108,"al meaning. In the last twenty years, many ATS systems have been proposed for different target populations in various languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4 , together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf 2 http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf 3 http://www.w3.org/WAI/ 4 http://simple.wikipedia.org/wiki/Main Page 5 http://wikipedia.org/wiki/Main P"
W14-5606,C10-1152,0,0.0161292,"s languages (Carroll et al., 1998; Devlin and Unthank, 2006; Saggion et al., 2011; Inui et al., 2003; Alu´ısio et al., 2008). Due to the scarcity of parallel corpora of original and manually simplified texts, most of these systems are rule-based. The emergence of Simple English Wikipedia (SEW)4 , together with the existing English Wikipedia (EW)5 provided a large amount of parallel TS training data, which motivated a shift in English TS from rule-based to data-driven approaches (Yatskar et al., 2010; Biran et al., 2011; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Wubben et al., 2012; Zhu et al., 2010). However, no assessment has This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 http://www.plainlanguage.gov/howto/guidelines/bigdoc/fullbigdoc.pdf 2 http://www.easy-read-online.co.uk/media/10609/making-myself-clear.pdf 3 http://www.w3.org/WAI/ 4 http://simple.wikipedia.org/wiki/Main Page 5 http://wikipedia.org/wiki/Main Page 53 Proceedings of the Workshop on Automatic Text Simplification: Methods and Applications in the Multilingual"
W15-5713,2003.mtsummit-systems.1,0,0.0960608,", we also conduct a human evaluation and error analysis of their output sentences. The remainder of the paper is organised as follows: Section 2 introduces studies that are relevant to our work; Section 3 describes the corpora, MT systems, experimental setup, goals and evaluation procedures; Section 4 presents and discusses the results of both automatic and human evaluation; and Section 5 summarises the findings of this study and gives directions for future work. 2 Related Work The rule-based machine translation (MT) systems, such as Systran (Toma, 1977), ETAP-3 (Boguslavsky, 1995), and Lucy (Alonso and Thurmair, 2003), required linguistic expertise to operate and were difficult This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 106 Proceedings of the 1st Deep Machine Translation Workshop (DMTW 2015), pages 106–115, Praha, Czech Republic, 3–4 September 2015. to adapt to different languages. The emergence of the word-based IBM models (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993) heralded a new approach to MT – statistical machine trans"
W15-5713,D11-1033,0,0.0237704,"s data for a particular language pair, another problem arises if the SMT system is needed for a different domain, as the training data may not cover the specific vocabulary or sentence constructions present in the targeted domain. In order to address this problem, many domain-adaptation techniques for SMT have been proposed, ranging from simply adding out-of-domain data to the small amount of in-domain data for training (Foster and Kuhn, 2007) to more sophisticated techniques, such as selecting only particular sentences from the out-of-domain data which are most similar to the in-domain data (Axelrod et al., 2011) or are similar to the sentences with the lowest translation quality (Banerjee et al., 2015). Hybrid MT systems, in turn, aim to exploit the best of both SMT and rule-based approaches, usually either by combining rule-based transfer with statistical language models in the synthesis phase (Habash and Dorr, 2002), or by combining rule-based with statistical approaches at different points of the Vauquois ˇ triangle, as the TectoMT system (Zabokrtsk´ y et al., 2008) that we use in this study. 2.1 English-Portuguese MT The English-Portuguese translation model built using the standard PBSMT system i"
W15-5713,E06-2024,1,0.763003,". Figure 2: An example of the a-trees and t-trees in the TectoMT system (the input EN sentence: “Try pressing the F11 key.” translated into the output PT sentence: “Tente carregar na tecla f11.”) 109 After the transfer of the English t-trees into Portuguese t-trees, the synthesis phase constructs a flat surface form of the sentence from the Portuguese t-tree. This is achieved using additional rule-based blocks which take care of word reordering, insertion of negations, prepositions, conjuctions, correct agreement, compound verb forms, etc. The synthesis stage for Portuguese uses the LX-Suite (Branco and Silva, 2006) to perform such tasks. The expected advantage of the TectoMT system over the standard PBSMT system is that the TectoMT translates t-tree nodes (and not the inflected forms) and should thus be able to generalise over the unseen morphological forms. This is particularly important for translation into morphologically rich languages (such as Portuguese) where data sparseness presents a problem for a purely statistically driven MT systems. 3.2.2 PBSMT In all experiments, we use the same PBSMT model (Koehn et al., 2007), GIZA++ implementation of the IBM word alignment model 4 (Och and Ney, 2003), a"
W15-5713,C88-1016,0,0.487963,"nslation (MT) systems, such as Systran (Toma, 1977), ETAP-3 (Boguslavsky, 1995), and Lucy (Alonso and Thurmair, 2003), required linguistic expertise to operate and were difficult This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 106 Proceedings of the 1st Deep Machine Translation Workshop (DMTW 2015), pages 106–115, Praha, Czech Republic, 3–4 September 2015. to adapt to different languages. The emergence of the word-based IBM models (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993) heralded a new approach to MT – statistical machine translation (SMT) systems. Later, the word-based SMT models were replaced by better-performing phrase-based (Koehn et al., 2007) or hierarchical phrase-based (Li et al., 2009) SMT systems. However, it was noticed that those shallow SMT approaches which do not use any deeper linguistic information or syntax are not able to capture long-distance dependences and may lead to problems with word order and grammatical and semantic cohesion (Fishel et al., 2012). Shallow syntax-based SMT systems tried to addr"
W15-5713,J90-2002,0,0.847136,"s, such as Systran (Toma, 1977), ETAP-3 (Boguslavsky, 1995), and Lucy (Alonso and Thurmair, 2003), required linguistic expertise to operate and were difficult This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 106 Proceedings of the 1st Deep Machine Translation Workshop (DMTW 2015), pages 106–115, Praha, Czech Republic, 3–4 September 2015. to adapt to different languages. The emergence of the word-based IBM models (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993) heralded a new approach to MT – statistical machine translation (SMT) systems. Later, the word-based SMT models were replaced by better-performing phrase-based (Koehn et al., 2007) or hierarchical phrase-based (Li et al., 2009) SMT systems. However, it was noticed that those shallow SMT approaches which do not use any deeper linguistic information or syntax are not able to capture long-distance dependences and may lead to problems with word order and grammatical and semantic cohesion (Fishel et al., 2012). Shallow syntax-based SMT systems tried to address those issues usi"
W15-5713,P03-2041,0,0.0179195,"ion or syntax are not able to capture long-distance dependences and may lead to problems with word order and grammatical and semantic cohesion (Fishel et al., 2012). Shallow syntax-based SMT systems tried to address those issues using three different approaches: a tree-to-string translation, where linguistic information is applied only on the source side (Huang et al., 2006); a string-to-tree translation, where linguistic information is applied only on the target side (Galley et al., 2004), and a tree-to-tree translation, where linguistic information is applied on both source and target side (Eisner, 2003). However, for the majority of language pairs, phrase-based SMT systems still produce better results. The main limitation of SMT systems is that they require large amounts of parallel (or at least comparable) training data, which is hard to obtain for language pairs not covered by the Europarl corpora (Koehn, 2005). Even if Europarl contains data for a particular language pair, another problem arises if the SMT system is needed for a different domain, as the training data may not cover the specific vocabulary or sentence constructions present in the targeted domain. In order to address this pr"
W15-5713,fishel-etal-2012-terra,0,0.0227633,"ages. The emergence of the word-based IBM models (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993) heralded a new approach to MT – statistical machine translation (SMT) systems. Later, the word-based SMT models were replaced by better-performing phrase-based (Koehn et al., 2007) or hierarchical phrase-based (Li et al., 2009) SMT systems. However, it was noticed that those shallow SMT approaches which do not use any deeper linguistic information or syntax are not able to capture long-distance dependences and may lead to problems with word order and grammatical and semantic cohesion (Fishel et al., 2012). Shallow syntax-based SMT systems tried to address those issues using three different approaches: a tree-to-string translation, where linguistic information is applied only on the source side (Huang et al., 2006); a string-to-tree translation, where linguistic information is applied only on the target side (Galley et al., 2004), and a tree-to-tree translation, where linguistic information is applied on both source and target side (Eisner, 2003). However, for the majority of language pairs, phrase-based SMT systems still produce better results. The main limitation of SMT systems is that they r"
W15-5713,W07-0717,0,0.033265,"unts of parallel (or at least comparable) training data, which is hard to obtain for language pairs not covered by the Europarl corpora (Koehn, 2005). Even if Europarl contains data for a particular language pair, another problem arises if the SMT system is needed for a different domain, as the training data may not cover the specific vocabulary or sentence constructions present in the targeted domain. In order to address this problem, many domain-adaptation techniques for SMT have been proposed, ranging from simply adding out-of-domain data to the small amount of in-domain data for training (Foster and Kuhn, 2007) to more sophisticated techniques, such as selecting only particular sentences from the out-of-domain data which are most similar to the in-domain data (Axelrod et al., 2011) or are similar to the sentences with the lowest translation quality (Banerjee et al., 2015). Hybrid MT systems, in turn, aim to exploit the best of both SMT and rule-based approaches, usually either by combining rule-based transfer with statistical language models in the synthesis phase (Habash and Dorr, 2002), or by combining rule-based with statistical approaches at different points of the Vauquois ˇ triangle, as the Te"
W15-5713,habash-dorr-2002-handling,0,0.141261,"Missing"
W15-5713,W06-3601,0,0.0301905,"models were replaced by better-performing phrase-based (Koehn et al., 2007) or hierarchical phrase-based (Li et al., 2009) SMT systems. However, it was noticed that those shallow SMT approaches which do not use any deeper linguistic information or syntax are not able to capture long-distance dependences and may lead to problems with word order and grammatical and semantic cohesion (Fishel et al., 2012). Shallow syntax-based SMT systems tried to address those issues using three different approaches: a tree-to-string translation, where linguistic information is applied only on the source side (Huang et al., 2006); a string-to-tree translation, where linguistic information is applied only on the target side (Galley et al., 2004), and a tree-to-tree translation, where linguistic information is applied on both source and target side (Eisner, 2003). However, for the majority of language pairs, phrase-based SMT systems still produce better results. The main limitation of SMT systems is that they require large amounts of parallel (or at least comparable) training data, which is hard to obtain for language pairs not covered by the Europarl corpora (Koehn, 2005). Even if Europarl contains data for a particula"
W15-5713,N03-1017,0,0.00875554,"m over the standard PBSMT system is that the TectoMT translates t-tree nodes (and not the inflected forms) and should thus be able to generalise over the unseen morphological forms. This is particularly important for translation into morphologically rich languages (such as Portuguese) where data sparseness presents a problem for a purely statistically driven MT systems. 3.2.2 PBSMT In all experiments, we use the same PBSMT model (Koehn et al., 2007), GIZA++ implementation of the IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics as described by Koehn et al. (2003). We tune the systems using MERT (Minimum Error Rate Training (Och, 2003)) and build a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the whole target side (Portuguese) of the English to Portuguese Europarl corpus (Koehn, 2005), which contains 1,960,407 sentences. 3.3 Experiments In all experiments, the PBSMT system uses the in-domain IT1 corpus for tuning, and the language model (LM) is trained on all sentences in the Portuguese side of the Europarl corpus (EP)6 . All experiments (in both TectoMT and PBSMT systems) are evaluated on the same test dataset"
W15-5713,2009.mtsummit-papers.7,0,0.508415,"Missing"
W15-5713,W04-3250,0,0.288249,".99 20.77 21.55 21.89 22.73 20.97 *21.08 21.16 21.66 22.20 22.16 Table 2: Translation experiments setup – type and the size of the corpora used (the number of sentence pairs for the IT1, IT2, and EP corpora, and the number of unigram or multiword expression pairs in the case of the TERM corpus), and the results of the automatic evaluation (the results of the systems which significantly outperformed both baselines are shown in bold; the ‘*’ marks the result which is significantly lower than the result for the BaselineIT; statistical significance is calculated using paired bootstrap resampling (Koehn, 2004)) 4.2 Human Evaluation Results The results of our human evaluation of the fluency and adequacy of the output are presented in Table 3. For each sentence we additionally calculate the Total score (for each annotator separately) as the rounded arithmetic mean of its Fluency and Adequacy scores. The TectoMT system achieved significantly higher adequacy score and total score than the PBSMT system. The mean and median value of the fluency score in the TectoMT system was higher than in the PBSMT system, but the reported difference was not statistically significant (at a 0.05 level of significance us"
W15-5713,2005.mtsummit-papers.11,0,0.402738,"tion is applied only on the source side (Huang et al., 2006); a string-to-tree translation, where linguistic information is applied only on the target side (Galley et al., 2004), and a tree-to-tree translation, where linguistic information is applied on both source and target side (Eisner, 2003). However, for the majority of language pairs, phrase-based SMT systems still produce better results. The main limitation of SMT systems is that they require large amounts of parallel (or at least comparable) training data, which is hard to obtain for language pairs not covered by the Europarl corpora (Koehn, 2005). Even if Europarl contains data for a particular language pair, another problem arises if the SMT system is needed for a different domain, as the training data may not cover the specific vocabulary or sentence constructions present in the targeted domain. In order to address this problem, many domain-adaptation techniques for SMT have been proposed, ranging from simply adding out-of-domain data to the small amount of in-domain data for training (Foster and Kuhn, 2007) to more sophisticated techniques, such as selecting only particular sentences from the out-of-domain data which are most simil"
W15-5713,W09-0424,0,0.0214156,"umbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 106 Proceedings of the 1st Deep Machine Translation Workshop (DMTW 2015), pages 106–115, Praha, Czech Republic, 3–4 September 2015. to adapt to different languages. The emergence of the word-based IBM models (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993) heralded a new approach to MT – statistical machine translation (SMT) systems. Later, the word-based SMT models were replaced by better-performing phrase-based (Koehn et al., 2007) or hierarchical phrase-based (Li et al., 2009) SMT systems. However, it was noticed that those shallow SMT approaches which do not use any deeper linguistic information or syntax are not able to capture long-distance dependences and may lead to problems with word order and grammatical and semantic cohesion (Fishel et al., 2012). Shallow syntax-based SMT systems tried to address those issues using three different approaches: a tree-to-string translation, where linguistic information is applied only on the source side (Huang et al., 2006); a string-to-tree translation, where linguistic information is applied only on the target side (Galley"
W15-5713,W10-1730,0,0.546275,"Missing"
W15-5713,H05-1066,0,0.22079,"Missing"
W15-5713,J03-1002,0,0.00549994,"anco and Silva, 2006) to perform such tasks. The expected advantage of the TectoMT system over the standard PBSMT system is that the TectoMT translates t-tree nodes (and not the inflected forms) and should thus be able to generalise over the unseen morphological forms. This is particularly important for translation into morphologically rich languages (such as Portuguese) where data sparseness presents a problem for a purely statistically driven MT systems. 3.2.2 PBSMT In all experiments, we use the same PBSMT model (Koehn et al., 2007), GIZA++ implementation of the IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics as described by Koehn et al. (2003). We tune the systems using MERT (Minimum Error Rate Training (Och, 2003)) and build a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the whole target side (Portuguese) of the English to Portuguese Europarl corpus (Koehn, 2005), which contains 1,960,407 sentences. 3.3 Experiments In all experiments, the PBSMT system uses the in-domain IT1 corpus for tuning, and the language model (LM) is trained on all sentences in the Portuguese side of the Europarl corpus (EP)6 . Al"
W15-5713,P03-1021,0,0.0278888,"not the inflected forms) and should thus be able to generalise over the unseen morphological forms. This is particularly important for translation into morphologically rich languages (such as Portuguese) where data sparseness presents a problem for a purely statistically driven MT systems. 3.2.2 PBSMT In all experiments, we use the same PBSMT model (Koehn et al., 2007), GIZA++ implementation of the IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics as described by Koehn et al. (2003). We tune the systems using MERT (Minimum Error Rate Training (Och, 2003)) and build a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the whole target side (Portuguese) of the English to Portuguese Europarl corpus (Koehn, 2005), which contains 1,960,407 sentences. 3.3 Experiments In all experiments, the PBSMT system uses the in-domain IT1 corpus for tuning, and the language model (LM) is trained on all sentences in the Portuguese side of the Europarl corpus (EP)6 . All experiments (in both TectoMT and PBSMT systems) are evaluated on the same test dataset (IT2). In order to obtain two baselines for each MT approach (TectoMT and"
W15-5713,P02-1040,0,0.108547,"Missing"
W15-5713,W14-1007,0,0.0475046,"Missing"
W15-5713,W07-1709,0,0.0681898,"Missing"
W15-5713,W08-0325,0,0.269184,"te este per´ıodo de sess˜oes. Table 1: Examples from the corpora 3. IT2 – Another in-domain IT corpus, with 1,000 sentence pairs (answers only) compiled under the QTLeap project, and comparable with the IT1 corpus.3 4. TERM – A parallel corpus of IT terminology (unigrams or multiword expressions), which consists of the Microsoft Terminology Collection4 (13,030 terms) and a small portion of LibreOffice terminology5 (995 terms). Examples from each corpora are presented in Table 1. 3.2 Systems This section describes the two MT systems used for the experiments. 3.2.1 TectoMT ˇ TectoMT (Zabokrtsk´ y et al., 2008) is a structural MT system which uses two layers of structural description, the shallow a-layer and the deep t-layer, performing the transfer on the t-layer (Figure 1). It encompasses three phases along the Vauquois triangle: analysis (which transforms the input sentence into the a-layer and t-layer in a two-step process), transfer (at the t-layer), and synthesis (which converts the translated t-layer representation to the a-layer and then to the output surface string). The analysis and synthesis phases are hybrid, while the transfer phase is mostly statistical, based on the Maximum Entropy co"
W15-5713,P07-2045,0,\N,Missing
W15-5713,D07-1096,0,\N,Missing
W16-3411,E14-1076,0,0.186775,") could lead to improvements in the quality of machine translation (Chandrasekar, 1994). Since then, a great number of ATS systems has been proposed not only for English, but also for other languages, e.g. Basque (Aranzabe et al., 2013), Portuguese (Specia, 2010), Spanish (Saggion et al., 2015), French (Brauwers et al., 2014), and Italian (Barlacchi and Tonelli, 2013). For English, the state-of-the-art ATS systems range from those performing only lexˇ ical (Glavaˇs and Stajner, 2015) or only syntactic (Siddharthan, 2011) simplification, to those combining lexical and syntactic simplification (Angrosh and Siddharthan, 2014). Recently, several ATS systems have been proposed which do not only simplify given text/sentences but also reduce the amount of information contained by removing highlevel details, such as appositions, adverbial phrases, or purely descriptive sentences ˇ ((Glavaˇs and Stajner, 2013), (Siddharthan et al., 2014), (Narayan and Gardent, 2014)). However, in these twenty years, the motivation for building ATS systems has shifted from improving text processing systems to making texts more accessible to wider audiences (e.g. children, non-native speakers, people with low literacy levels, and people w"
W16-3411,W14-1206,0,0.0740635,"Missing"
W16-3411,C96-2183,0,0.776808,"s the results of our experiments, while Section 5 summarises the main findings and presents ideas for future research. 2 Related Work Automatic text simplification (ATS) systems aim to transform original texts into their lexically and syntactically simpler variants. In theory, they could also simplify texts on the discourse level, but most of the systems still operate only on the sentence level. The motivation for building the first ATS systems was to improve the performance of machine translation systems and other text processing tasks, e.g. parsing, information retrieval, and summarisation (Chandrasekar et al., 1996). It was argued that simplified sentences (which have simpler sentential structures and reduced ambiguity) could lead to improvements in the quality of machine translation (Chandrasekar, 1994). Since then, a great number of ATS systems has been proposed not only for English, but also for other languages, e.g. Basque (Aranzabe et al., 2013), Portuguese (Specia, 2010), Spanish (Saggion et al., 2015), French (Brauwers et al., 2014), and Italian (Barlacchi and Tonelli, 2013). For English, the state-of-the-art ATS systems range from those performing only lexˇ ical (Glavaˇs and Stajner, 2015) or onl"
W16-3411,W14-3348,0,0.0158963,"uments and manuals, i.e. technical domain. Approximately 20.7M sentences, in total, were used for training (20.5M subtitles, 200,000 news, 30,000 technical), and 2,000 sentences were used for tuning (retaining the same proportions of the sentences from the three corpora as in the training dataset). The English-to-Serbian part of the ASISTENT system (Arˇcan et al., 2016) was tested on 2,000 sentences from the three corpora used for training and tuning (the 2,000 sentences which were not used for training and tuning) and achieved a 38.88 BLEU score (Papineni et al., 2002), a 31.18 METEOR score (Denkowski and Lavie, 2014), and a 61.62 chrF3 score (Popovi´c, 2015). 3.3 Evaluation Procedure From the initial set of 100 news articles, we randomly selected 65 original sentences and evaluated all translation outputs (from original sentences, and TS-A and TS-C systems, which led to a total of 195 target sentences) with respect to the following aspects: 1. adequacy, i.e. meaning preservation 2. fluency, i.e. grammaticality 3. technical post-editing effort, i.e. amount of necessary edit operations Each of the tasks has been carried out separately, i.e. the evaluation of adequacy and fluency were carried out in two sepa"
W16-3411,R13-2011,1,0.876726,"Missing"
W16-3411,P15-2011,1,0.866013,"Missing"
W16-3411,N03-1017,0,0.0232797,"ar-old man was arrested on April 30 on suspicion. A 21-year-old man was released on jail until May 29. TS-A (PE) A 21-year-old man was arrested on April 30 on suspicion of murder. A 21-year-old man was released on bail until May 29. This subset of sentences was later used for MT experiments and human evaluation and postediting. Can Text Simplification help Machine Translation? 3.2 235 Statistical Machine Translation System For the machine translation from English to Serbian, we used the ASISTENT system.6 It is a freely available SMT system, based on the widely used phrase-based SMT framework (Koehn et al., 2003) and it supports translations from English to Slovene, Croatian and Serbian and vice versa. Additionally, translations between those three Slavic languages are also possible. The system was trained using the Moses toolkit (Koehn et al., 2007). The word alignments were built with GIZA++ (Och and Ney, 2003), and the 5-gram language model was built using the SRILM toolkit (Stolcke, 2002) The training dataset originates from the OPUS website7 (Tiedemann, 2012) where three domains were available for the Serbian-English language pair: the enhanced version of the SEtimes corpus8 (Tyers and Alperen, 2"
W16-3411,P14-1041,0,0.0732302,"lacchi and Tonelli, 2013). For English, the state-of-the-art ATS systems range from those performing only lexˇ ical (Glavaˇs and Stajner, 2015) or only syntactic (Siddharthan, 2011) simplification, to those combining lexical and syntactic simplification (Angrosh and Siddharthan, 2014). Recently, several ATS systems have been proposed which do not only simplify given text/sentences but also reduce the amount of information contained by removing highlevel details, such as appositions, adverbial phrases, or purely descriptive sentences ˇ ((Glavaˇs and Stajner, 2013), (Siddharthan et al., 2014), (Narayan and Gardent, 2014)). However, in these twenty years, the motivation for building ATS systems has shifted from improving text processing systems to making texts more accessible to wider audiences (e.g. children, non-native speakers, people with low literacy levels, and people with various language or learning disabilities). Therefore, ATS systems have only been evaluated for the quality of the generated output, its readability levels, and usefulness in making texts more accessible to target populations (reducing reading speed and improving comprehension). To the best of our knowledge, there has been no evaluatio"
W16-3411,J03-1002,0,0.00520082,"ation and postediting. Can Text Simplification help Machine Translation? 3.2 235 Statistical Machine Translation System For the machine translation from English to Serbian, we used the ASISTENT system.6 It is a freely available SMT system, based on the widely used phrase-based SMT framework (Koehn et al., 2003) and it supports translations from English to Slovene, Croatian and Serbian and vice versa. Additionally, translations between those three Slavic languages are also possible. The system was trained using the Moses toolkit (Koehn et al., 2007). The word alignments were built with GIZA++ (Och and Ney, 2003), and the 5-gram language model was built using the SRILM toolkit (Stolcke, 2002) The training dataset originates from the OPUS website7 (Tiedemann, 2012) where three domains were available for the Serbian-English language pair: the enhanced version of the SEtimes corpus8 (Tyers and Alperen, 2010) containing “news and views from South-East Europe”, OpenSubtitles9 , and the KDE localisation documents and manuals, i.e. technical domain. Approximately 20.7M sentences, in total, were used for training (20.5M subtitles, 200,000 news, 30,000 technical), and 2,000 sentences were used for tuning (reta"
W16-3411,P02-1040,0,0.103535,"OpenSubtitles9 , and the KDE localisation documents and manuals, i.e. technical domain. Approximately 20.7M sentences, in total, were used for training (20.5M subtitles, 200,000 news, 30,000 technical), and 2,000 sentences were used for tuning (retaining the same proportions of the sentences from the three corpora as in the training dataset). The English-to-Serbian part of the ASISTENT system (Arˇcan et al., 2016) was tested on 2,000 sentences from the three corpora used for training and tuning (the 2,000 sentences which were not used for training and tuning) and achieved a 38.88 BLEU score (Papineni et al., 2002), a 31.18 METEOR score (Denkowski and Lavie, 2014), and a 61.62 chrF3 score (Popovi´c, 2015). 3.3 Evaluation Procedure From the initial set of 100 news articles, we randomly selected 65 original sentences and evaluated all translation outputs (from original sentences, and TS-A and TS-C systems, which led to a total of 195 target sentences) with respect to the following aspects: 1. adequacy, i.e. meaning preservation 2. fluency, i.e. grammaticality 3. technical post-editing effort, i.e. amount of necessary edit operations Each of the tasks has been carried out separately, i.e. the evaluation of"
W16-3411,W15-3049,1,0.898515,"Missing"
W16-3411,W15-4913,1,0.914144,"Missing"
W16-3411,W11-2802,0,0.130393,"ed that simplified sentences (which have simpler sentential structures and reduced ambiguity) could lead to improvements in the quality of machine translation (Chandrasekar, 1994). Since then, a great number of ATS systems has been proposed not only for English, but also for other languages, e.g. Basque (Aranzabe et al., 2013), Portuguese (Specia, 2010), Spanish (Saggion et al., 2015), French (Brauwers et al., 2014), and Italian (Barlacchi and Tonelli, 2013). For English, the state-of-the-art ATS systems range from those performing only lexˇ ical (Glavaˇs and Stajner, 2015) or only syntactic (Siddharthan, 2011) simplification, to those combining lexical and syntactic simplification (Angrosh and Siddharthan, 2014). Recently, several ATS systems have been proposed which do not only simplify given text/sentences but also reduce the amount of information contained by removing highlevel details, such as appositions, adverbial phrases, or purely descriptive sentences ˇ ((Glavaˇs and Stajner, 2013), (Siddharthan et al., 2014), (Narayan and Gardent, 2014)). However, in these twenty years, the motivation for building ATS systems has shifted from improving text processing systems to making texts more accessib"
W16-3411,tiedemann-2012-parallel,0,0.0118122,"lish to Serbian, we used the ASISTENT system.6 It is a freely available SMT system, based on the widely used phrase-based SMT framework (Koehn et al., 2003) and it supports translations from English to Slovene, Croatian and Serbian and vice versa. Additionally, translations between those three Slavic languages are also possible. The system was trained using the Moses toolkit (Koehn et al., 2007). The word alignments were built with GIZA++ (Och and Ney, 2003), and the 5-gram language model was built using the SRILM toolkit (Stolcke, 2002) The training dataset originates from the OPUS website7 (Tiedemann, 2012) where three domains were available for the Serbian-English language pair: the enhanced version of the SEtimes corpus8 (Tyers and Alperen, 2010) containing “news and views from South-East Europe”, OpenSubtitles9 , and the KDE localisation documents and manuals, i.e. technical domain. Approximately 20.7M sentences, in total, were used for training (20.5M subtitles, 200,000 news, 30,000 technical), and 2,000 sentences were used for tuning (retaining the same proportions of the sentences from the three corpora as in the training dataset). The English-to-Serbian part of the ASISTENT system (Arˇcan"
W17-5030,P15-2011,1,0.880809,"Missing"
W17-5030,C12-1023,0,0.029599,"s and sentences (Just and Carpenter, 1980). A series of studies investigating the effects of word frequency, verb complexity and lexical ambiguity (Juhasz and Rayner, 2003; Rayner et al., 2012), as well as contextual effects on word perception (Ehrlich and Rayner, 1981) concluded that long, rare and ambiguous words are more likely to be fixated longer and their processing requires more cognitive effort from the reader. These are also words that are likely to be replaced with shorter and more frequent ones during lexical simplification aimed at making text more accessible to wider populations (Bott et al., 2012; Glavaˇs and ˇ Stajner, 2015). Eye tracking has also been extensively used for the investigation of reading-related disorders owing to its capacity to provide information about the online processing of the text. For example, aphasic readers show “qualitatively different gaze fixation patterns” when answering comprehension questions (Dickey et al., 2007) and readers with dyslexia have been found to exhibit longer fixation durations and less efficient scanning techniques (Kim and Lombardino, 2016). In spite of the decades-long tradition of using gaze data to investigate word processing among ne"
W17-5030,S12-1066,0,0.0292614,"72 http://alt.qcri.org/semeval2016/task11/ https://simple.wikipedia.org feature (Wr´obel, 2016). In an earlier organised shared task on English Lexical Substitution at SemEval-2012,4 which had the aim of providing a framework for evaluation of lexical simplification systems, for each given sentence containing one target ‘complex’ word and four substitution candidates, participating systems were competing in ranking the four given substitution candidates according to their simplicity, i.e. how easy they are to be understood by fluent but non-native English speakers. The best performing system (Jauhar and Specia, 2012) used a combination of collocational features and four psycholinguistic measures extracted from the MRC (Machine Readable Dictionary) Psycholinguistic Database (Coltheart, 1981): Next, we identify which particular words (in their specific contexts) impose heavier cognitive load on each group of participants by clustering them as challenging or not, based on viewing time of each participant individually (Section 4.1), and then classifying them into four classes depending on the number of participants who found them challenging (Section 4.2). Finally, we investigate the lexical properties which"
W17-5030,N16-1050,0,0.18899,"utistic and non-autistic adolescents while reading individual sentences, suggesting that the reading task imposed an overall heavier cognitive load on the participants from the ASD group. Brock et al. (2008) also used gaze data1 and showed that both the ASD and the control participants were able to use context to successfully dis1.2 Complex Word Identification Complex Word Identification (CWI) task received high attention only recently, with findings suggesting that using a CWI module at the beginning of a lexical simplification (LS) pipeline significantly improves performances of LS systems (Paetzold and Specia, 2016c) and with the recently organised SemEval-2016 CWI shared task.2 The goal of the shared task was building CWI systems which would identify challenging words for non-native English speakers. The dataset consisted of sentences (without context), each with one content word (noun, verb, adjective, or adverb) marked as a target word. The training dataset contained 200 sentences, where each target word was annotated by 20 non-native English speakers as ‘easy’ or ‘complex’, depending on whether they understood its meaning or not. The participants were asked to mark the word as ‘complex’ even if they"
W17-5030,L16-1491,0,0.197677,"utistic and non-autistic adolescents while reading individual sentences, suggesting that the reading task imposed an overall heavier cognitive load on the participants from the ASD group. Brock et al. (2008) also used gaze data1 and showed that both the ASD and the control participants were able to use context to successfully dis1.2 Complex Word Identification Complex Word Identification (CWI) task received high attention only recently, with findings suggesting that using a CWI module at the beginning of a lexical simplification (LS) pipeline significantly improves performances of LS systems (Paetzold and Specia, 2016c) and with the recently organised SemEval-2016 CWI shared task.2 The goal of the shared task was building CWI systems which would identify challenging words for non-native English speakers. The dataset consisted of sentences (without context), each with one content word (noun, verb, adjective, or adverb) marked as a target word. The training dataset contained 200 sentences, where each target word was annotated by 20 non-native English speakers as ‘easy’ or ‘complex’, depending on whether they understood its meaning or not. The participants were asked to mark the word as ‘complex’ even if they"
W17-5030,S16-1146,0,0.027834,"Missing"
W18-0507,W14-1206,0,0.0720755,"Missing"
W18-0507,W18-0519,0,0.153197,"Missing"
W18-0507,S16-1156,0,0.0850516,"Missing"
W18-0507,S16-1151,0,0.0753176,"Missing"
W18-0507,W18-0539,1,0.893727,"Missing"
W18-0507,W18-0538,0,0.100445,"lower log-probability for complex words. The systems submitted performed the best out of all systems for the cross-lingual task (the French dataset) both for the binary and probabilistic classification tasks, showing a promising direction in the creation of CWI dataset for new languages. 1 74 https://code.google.com/archive/p/word2vec/ tant features. Their best system shows an average performance compared to the other systems in the shared task for the monolingual English binary classification track. NLP-CIC present systems for the English and Spanish multilingual binary classification tasks (Aroyehun et al., 2018). The feature sets include morphological features such as frequency counts of target word on large corpora such as Wikipedia and Simple Wikipedia, syntactic and lexical features, psycholinguistic features from the MRC psycholinguistic database and entity features using the OpenNLP and CoreNLP tools, and word embedding distance as a feature which is computed between the target word and the sentence. Tree learners such as Random Forest, Gradient Boosted, and Tree Ensembles are used to train different classifiers. Furthermore, a deep learning approach based on 2D convolutional (CNN) and word embe"
W18-0507,S16-1148,0,0.0982594,"Missing"
W18-0507,W18-0518,0,0.065579,"on the ensemble techniques where AdaBoost classifier with 5000 estimators achieves the highest results, followed by the bootstrap aggregation classifier of Random Forest. All the features are used for the N EWS and W IKI N EWS datasets, but for the W IKIPEDIA dataset, MCR psycholinguistic features are excluded. For the probabilistic classification task, the same feature setups are used and the Linear Regression algorithm is used to estimate values of targets. CoastalCPH describe systems developed for multilingual and cross-lingual domains for the binary and probabilistic classification tasks (Bingel and Bjerva, 2018). Unlike most systems, they have focused mainly on German, Spanish, and French datasets in order to investigate if multitask learning can be applied to the cross-lingual CWI task. They have devised two models, using languageagnostic approach with an ensemble that comprises of Random Forests (random forest classifiers for the binary classification task and random forest regressors for the probabilistic classification tasks, with 100 trees) and feed-forward neural networks. Camb describes different systems (Gooding and Kochmar, 2018) they have developed for the monolingual English datasets both"
W18-0507,W18-0520,0,0.193647,"l domains for the binary and probabilistic classification tasks (Bingel and Bjerva, 2018). Unlike most systems, they have focused mainly on German, Spanish, and French datasets in order to investigate if multitask learning can be applied to the cross-lingual CWI task. They have devised two models, using languageagnostic approach with an ensemble that comprises of Random Forests (random forest classifiers for the binary classification task and random forest regressors for the probabilistic classification tasks, with 100 trees) and feed-forward neural networks. Camb describes different systems (Gooding and Kochmar, 2018) they have developed for the monolingual English datasets both for the binary and probabilistic classification tasks. They have used features that are based on the insights of the CWI shared task 2016 (Paetzold and Specia, 2016a) such as lexical features (word length, number of syllables, WordNet features such as the number of synsets), word n-gram and POS tags, and dependency parse relations. In addition, they have used features such as the number of words grammatically related to the target word, psycholinguistic features from the MRC database, CEFR (Common European Framework of Reference fo"
W18-0507,S16-1160,0,0.102164,"Missing"
W18-0507,W18-0540,0,0.0627627,"Missing"
W18-0507,W18-0521,0,0.24579,"layers followed by a sigmoid layer, which predicted the probability of the target word being complex. Their experiments show that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For the probabilistic classification task, their LSTMs achieve the best results. TMU submitted multilingual and cross-lingual CWI systems for both of the binary and probabilistic classification tasks (Kajiwara and Komachi, 2018). The systems use two variants of frequency features from the learner corpus (Lang-8 corpus) from Mizumoto et al. (2011) and from the general domain corpus (Wikipedia and WikiNews). The list of features used in building the model include the number of characters in the target word, number of words in the target phrase, and frequency of the target word in learner corpus (Lang-8 corpus) and general domain corpus (Wikipedia and WikiNews). Random forest classifiers are used for the binary classification task while random forest regressors are used for the probabilistic classification task using th"
W18-0507,S16-1164,0,0.0425763,"Missing"
W18-0507,S16-1149,1,0.538636,"elines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identification (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT’2018. The second CWI shared task follows a successful first edition featuring 21 teams organized at SemEval’2016 (Paetzold and Specia, 2016a). While the first CWI shared task targeted an English dataset, the second edition focused on multilingualism providing datasets containing four languages: English, German, French, and Spanish. • English monolingual CWI; • German monolingual CWI; • Spanish monolingual CWI; and • Multilingual CWI with a French test set. For the first three tracks, participants were provided with training and testing data for the same language. For French, participants were provided only with a French test set and no French training data. In the CWI 2016, the task was cast as binary classification. To be able t"
W18-0507,S16-1162,0,0.138871,"Missing"
W18-0507,S16-1158,0,0.0719992,"Missing"
W18-0507,S16-1163,0,0.176167,"Missing"
W18-0507,D14-1162,0,0.0887949,"earning methods, 2) using the average embedding of target words as an input to a neural network, and 3) modeling the context of the target words using an LSTM. For the feature engineering-based systems, features such as linguistic, psycholinguistic, and language model features were used to train different binary and probabilistic classifiers. Lexical features include word length, number of syllables, and number of senses, hypernyms, and hyponyms in WordNet. For N-gram features, probabilities of the n-gram containing the target words were For embedding-based systems, a pre-trained GloVe model (Pennington et al., 2014) was used to get the vector representations of target words. For MWE, the average of the vectors is used. In the first approach, the resulting vector is passed on to a neural network with two ReLu layers followed by a sigmoid layer, which predicted the probability of the target word being complex. Their experiments show that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For t"
W18-0507,S16-1154,1,0.847385,"Missing"
W18-0507,W18-0541,0,0.102296,"Missing"
W18-0507,S16-1153,1,0.879975,"Missing"
W18-0507,S16-1161,0,0.200613,"Missing"
W18-0507,S16-1147,0,0.032877,"Missing"
W18-0507,S16-1157,0,0.212408,"Missing"
W18-0507,I11-1017,0,0.0188214,"that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For the probabilistic classification task, their LSTMs achieve the best results. TMU submitted multilingual and cross-lingual CWI systems for both of the binary and probabilistic classification tasks (Kajiwara and Komachi, 2018). The systems use two variants of frequency features from the learner corpus (Lang-8 corpus) from Mizumoto et al. (2011) and from the general domain corpus (Wikipedia and WikiNews). The list of features used in building the model include the number of characters in the target word, number of words in the target phrase, and frequency of the target word in learner corpus (Lang-8 corpus) and general domain corpus (Wikipedia and WikiNews). Random forest classifiers are used for the binary classification task while random forest regressors are used for the probabilistic classification task using the scikit-learn library. Feature ablation shows that both the length, frequency, and probability features (based on corpu"
W18-0507,P13-3015,0,0.247331,"ative speakers. To train their systems, participants received a labeled training set where words in context were annotated regarding their complexity. One month later, an unlabeled test set was provided and participating teams were required to upload their predictions for evaluation. More information about the data collection is presented in Section 3. Given the multilingual dataset provided, the CWI challenge was divided into four tracks: Introduction The most common first step in lexical simplification pipelines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identification (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT’2018. The second CWI shared task follows a successful first edition featuring 21 teams organized at SemEval’2016 (Paetzold and Specia, 2016a). While the first CWI shared task targeted an English dataset, the second e"
W18-0507,S16-1152,0,0.262443,"Missing"
W18-0507,S16-1159,0,0.0576998,"Missing"
W18-0507,L16-1035,1,0.906663,"Missing"
W18-0507,W18-0522,0,0.230666,"n dependency relation, frequency features based on the BNC, Wikipedia, and Dale and Chall list corpora, number of synsets and senses in WordNet, and so on. The experiment is conducted using the Weka machine learning framework using the Support vector machine (with linear and radial basis function kernels), Na¨ıve Bayes, Logistic Regression, Random Tree, and Random Forest classification algorithms. The final experiments employ Support Vector Machines and Random Forest classifiers. CFILT IITB Developed ensemble-based classification systems for the English monolingual binary classification task (Wani et al., 2018). Lexical features based on WordNet for the target word are extracted as follows: 1) Degree of Polysemy: number of senses of the target word in WordNet, 2) Hyponym and Hypernym Tree Depth: the position of the word in WordNet’s hierarchical tree, and 3) Holonym and Meronym Counts: based on the relationship of the target word to its components (meronyms) or to the things it is contained in (Holonym’s). Additional feature classes include size-based features such as word count, word length, vowel counts, and syllable counts. They also use vocabulary-based features such as Ogden Basic (from Ogden’s"
W18-0507,S16-1146,0,0.112937,"Missing"
W18-0507,I17-2068,1,0.54282,"least 10 native English speakers and 10 non-native English speakers so that it is possible to investigate if native and non-native speakers have different CWI needs. 5) Complex words are not pre-highlighted, as in previous contributions, so that annotators are not biased to the pre-selection of the complex phrases. 6) In addition to single words, we allowed the annotation of multi-word expressions (MWE), up to a size of 50 characters. Table 2 shows the total, native, and non-native number of annotators that participated in the annotation task. Datasets We have used the CWIG3G2 datasets from (Yimam et al., 2017b,a) for the complex word identification (CWI) shared task 2018. The datasets are collected for multiple languages (English, German, Spanish). The English datasets cover different text genres, namely News (professionally written news), WikiNews (news written by amateurs), and Wikipedia articles. Below, we will briefly describe the annotation process and the statistics of collected datasets. For detail explanation of the datasets, please refer to the works of Yimam et al. (2017b,a) Furthermore, to bolster the cross-lingual CWI experiment, we have collected a CWI dataset for French. The French d"
W18-0507,yimam-etal-2017-multilingual,1,0.591517,"least 10 native English speakers and 10 non-native English speakers so that it is possible to investigate if native and non-native speakers have different CWI needs. 5) Complex words are not pre-highlighted, as in previous contributions, so that annotators are not biased to the pre-selection of the complex phrases. 6) In addition to single words, we allowed the annotation of multi-word expressions (MWE), up to a size of 50 characters. Table 2 shows the total, native, and non-native number of annotators that participated in the annotation task. Datasets We have used the CWIG3G2 datasets from (Yimam et al., 2017b,a) for the complex word identification (CWI) shared task 2018. The datasets are collected for multiple languages (English, German, Spanish). The English datasets cover different text genres, namely News (professionally written news), WikiNews (news written by amateurs), and Wikipedia articles. Below, we will briefly describe the annotation process and the statistics of collected datasets. For detail explanation of the datasets, please refer to the works of Yimam et al. (2017b,a) Furthermore, to bolster the cross-lingual CWI experiment, we have collected a CWI dataset for French. The French d"
W18-0507,W17-5910,1,0.857961,"Missing"
W18-0507,S16-1155,1,0.729801,"Missing"
W18-0507,S16-1150,0,\N,Missing
W18-3104,D14-1152,0,0.179993,"ence, which both annotators labeled uncertain: Related Work Loughran and McDonald (2011) introduced financial dictionaries spanning the categories of positive, negative, litigious, strong modal, weak modal, and—most important for us—uncertain words. Perhaps not surprisingly, they find that the cumulative tf–idf of uncertain terms in a set of 10-Ks shares a positive and highly significant relation with future stock return volatility. To quantify the improvement of our new expansions over this dictionary, we use a regression setup similar to their subsequent paper (Loughran and McDonald, 2014). Tsai and Wang (2014) automatically expanded said dictionaries by training word embeddings and adding the 20 most cosine similar terms to each original dictionary term. Using a dataset of 10-Ks, they show that this expansion improves a prediction of future stock return volatility. In contrast to them, we provide a systematic analysis how a domain-specific vs. a general-domain (RQ1) and an automatic vs. a semi-automatic expansion (RQ2) perform in a set of regressions. Furthermore, for the first time in the community, we perform a binary sentence classification task on 10-Ks to assess directly whether our models are"
W18-3104,P09-2044,0,0.22122,"oogle.com/archive/p/word2vec 4 http://www.crsp.com 5 http://dws.informatik.uni-mannheim.de/en/people/ researchers/christoph-kilian-theil/ 3 https://sraf.nd.edu/textual-analysis/resources 33 additional imprecisions, which speaks in favor of an uncertain label. Yet, the referenced risk is nonopaque and said imprecisions could be attributed to legal requirements as inherent to any regulated corporate disclosure; hence, a case could also be made for an certain label. Nevertheless, the IAA measured as κ (Cohen, 1960) was 0.73, which can be considered “substantial” (Landis and Koch, 1977). Notably, Ganter and Strube (2009) report an even lower pairwise IAA with 0.45 ≤ κ ≤ 0.80, x ¯κ = 0.56 for an annotation of Wikipedia sentences as certain or uncertain. Despite making use of highly trained ˇ domain experts, Stajner et al. (2017) also obtained a lower IAA with 0.47 ≤ κ ≤ 0.70, x ¯κ = 0.61 for a comparable annotation task. They sampled their sentences from transcribed debates held by the U.S. central bank’s monetary policy committee (FOMC). Given our comparably high IAA, we were confident of our annotation quality and let the first annotator annotate an additional 900 sentences, thus forming our newly created da"
W18-3104,D08-1103,0,0.0314389,"others to advance the field. 4 4.1 Figure 1: Candidate terms for “probably” in our domain-specific embedding model. Terms retained/removed by the annotator are marked by dots/triangles. Dimensionality is reduced through t-SNE (van der Maaten and Hinton, 2008). ent vocabulary. An exemplary overview of added terms according to both models can be found in our Online Appendix. We found that antonyms—despite their opposite meaning—were frequently embedded in similar semantic spaces. Coalescing relations of synonymy and antonymy is a well-known and often undesired property of distributional models (Mohammed et al., 2008). Hence, it can be explained why both UncGen as well as UncSpec contain the token “certainly” as cosine similar term to “probably” (similarity of 0.68 and 0.45, respectively). In addition, other irrelevant terms (e.g. “event”, “significance”) appeared frequently in close proximity to uncertain terms. Methodology Expanding the Dictionary To answer RQ1, we first determined the 20 most cosine similar terms according to the generic embedding model for each of the 297 terms of Unc. We chose 20 as the number of added terms since this is the value suggested by Tsai and Wang (2014). After lowercasing,"
W18-7006,de-marneffe-etal-2006-generating,0,0.243939,"Missing"
W18-7006,R13-2011,1,0.893703,"Missing"
W18-7006,P15-2011,1,0.923176,"Missing"
W18-7006,2010.eamt-1.31,0,0.0117803,"ssary. We also explore in which way simplification of relative clauses can improve the quality of translations, and which types of English relative clauses pose problems to machine translation into Serbian and German. We focus on English-to-Serbian and English-toGerman machine translation, as both target languages are morphologically rich and structurally different from English. 2 2.1 2.2 ATS for Improving MT Many works have so far proposed to rewrite input sentences using paraphrasing or textual entailment to improve machine translation, e.g. (CallisonBurch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015). Mirkin et al. (2013a,b) go one step further, proposing an interactive tool which identifies sentences which are most likely to be translated poorly, offers possible rewritings for the human editor, and then performs translation. Although such approach requires some human post-editing effort, the effort is just monolingual (at the source side only). All these approaches, although being proposed and tested on different language pairs (English-French, EnglishSpanish, English-Hindu), only focus on out-ofRelated work Automatic Text Simplification Automatic text simplification"
W18-7006,P09-1089,0,0.0346317,"ifications where necessary. We also explore in which way simplification of relative clauses can improve the quality of translations, and which types of English relative clauses pose problems to machine translation into Serbian and German. We focus on English-to-Serbian and English-toGerman machine translation, as both target languages are morphologically rich and structurally different from English. 2 2.1 2.2 ATS for Improving MT Many works have so far proposed to rewrite input sentences using paraphrasing or textual entailment to improve machine translation, e.g. (CallisonBurch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015). Mirkin et al. (2013a,b) go one step further, proposing an interactive tool which identifies sentences which are most likely to be translated poorly, offers possible rewritings for the human editor, and then performs translation. Although such approach requires some human post-editing effort, the effort is just monolingual (at the source side only). All these approaches, although being proposed and tested on different language pairs (English-French, EnglishSpanish, English-Hindu), only focus on out-ofRelated work Automatic Text Simplification Automatic"
W18-7006,N15-1156,0,0.0452024,"Missing"
W18-7006,2013.mtsummit-posters.8,0,0.221298,"lification of relative clauses can improve the quality of translations, and which types of English relative clauses pose problems to machine translation into Serbian and German. We focus on English-to-Serbian and English-toGerman machine translation, as both target languages are morphologically rich and structurally different from English. 2 2.1 2.2 ATS for Improving MT Many works have so far proposed to rewrite input sentences using paraphrasing or textual entailment to improve machine translation, e.g. (CallisonBurch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015). Mirkin et al. (2013a,b) go one step further, proposing an interactive tool which identifies sentences which are most likely to be translated poorly, offers possible rewritings for the human editor, and then performs translation. Although such approach requires some human post-editing effort, the effort is just monolingual (at the source side only). All these approaches, although being proposed and tested on different language pairs (English-French, EnglishSpanish, English-Hindu), only focus on out-ofRelated work Automatic Text Simplification Automatic text simplification systems are usually divided into lexical"
W18-7006,P13-4015,0,0.751716,"lification of relative clauses can improve the quality of translations, and which types of English relative clauses pose problems to machine translation into Serbian and German. We focus on English-to-Serbian and English-toGerman machine translation, as both target languages are morphologically rich and structurally different from English. 2 2.1 2.2 ATS for Improving MT Many works have so far proposed to rewrite input sentences using paraphrasing or textual entailment to improve machine translation, e.g. (CallisonBurch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015). Mirkin et al. (2013a,b) go one step further, proposing an interactive tool which identifies sentences which are most likely to be translated poorly, offers possible rewritings for the human editor, and then performs translation. Although such approach requires some human post-editing effort, the effort is just monolingual (at the source side only). All these approaches, although being proposed and tested on different language pairs (English-French, EnglishSpanish, English-Hindu), only focus on out-ofRelated work Automatic Text Simplification Automatic text simplification systems are usually divided into lexical"
W18-7006,N15-2002,0,0.0442498,"Missing"
W18-7006,W14-1206,0,0.0442268,"Missing"
W18-7006,W15-3049,1,0.89239,"Missing"
W18-7006,N06-1003,0,0.169772,"Missing"
W18-7006,C96-2183,0,0.579378,"post-edit operations) to obtain correct translation. We find that larger improvements can be achieved for more complex target languages, as well as for MT systems with lower overall performance. The improvements mainly originate from correctly simplified sentences with relatively complex structure, while simpler structures are already translated sufficiently well using the original source sentences. 1 Introduction Text simplification (TS) was initially proposed in the late nineties as a pre-processing step that would improve machine translation (MT), information extraction (IE), and parsing (Chandrasekar et al., 1996). At that time, text simplification was done manually and focused mainly on syntactic transformations. In the last 20 years, many automatic text simplification (ATS) systems were proposed for various languages. Most of them were done with the goal of making texts more understandable to humans. The most mature systems are those proposed for English language. The initial goal of using automatic syntactic simplification for improving MT systems has been forgotten, with the only exception being the recent work ˇ of Stajner and Popovi´c (2016), where two lexicosyntactic ATS systems were used for tr"
W18-7006,W11-2802,0,0.299523,"human editor, and then performs translation. Although such approach requires some human post-editing effort, the effort is just monolingual (at the source side only). All these approaches, although being proposed and tested on different language pairs (English-French, EnglishSpanish, English-Hindu), only focus on out-ofRelated work Automatic Text Simplification Automatic text simplification systems are usually divided into lexical simplification (LS) systems ˇ (e.g. (Baeza-Yates et al., 2015; Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016)), syntactic simplification (SS) systems (e.g. (Siddharthan, 2011; ˇ Aranzabe et al., 2012; Glavaˇs and Stajner, 2013; Brouwers et al., 2014)), and lexico-syntactic simplification (LSS) systems (e.g. (Siddharthan and 40 score 5 vocabulary words, or difficult to translate shorter n-grams. ˇ The recent work of Stajner and Popovi´c (2016), investigated the impact of lexico-syntactic automatic text simplification systems on Englishto-Serbian machine translation. They used two lexico-simplification systems: the EvLex system ˇ (Stajner and Glavaˇs, 2017) which performs sentence splitting, lexical substitution, and content reduction, and a “classical” lexico-synta"
W18-7006,E14-1076,0,0.570372,"tate-of-the-art lexical simplificaˇ tion systems are unsupervised (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016), and although they have a decent coverage (better than the supervised LS systems) they often lead to ungrammatiˇ cal output or change of original meaning (Stajner and Glavaˇs, 2017). The changes in meaning are not subtle, but rather essential, and as such, those systems are suitable as a preprocessing step in machine translation only with a manual correction of ˇ their output (Stajner and Popovi´c, 2016). The state-of-the-art syntactic simplification systems are rule-based (Siddharthan and Angrosh, 2014; Saggion et al., 2015), and as such, provide more grammatical output, at the cost of being too conservative and often not making any changes at all. Out of all syntactic simplification operations, simplification of the relative clauses is the most studied and the most reliable one, especially for English. Therefore, in this study, we focus only on this type of transformations hoping to minimize the necessity for manually correcting simplification output. Cameron’s submitted text reads in part like a plot summary of the Lorax film provided on the Internet Movie Database website, which begins:"
W18-7006,E17-1100,0,0.0452159,"Missing"
W18-7006,W16-3411,1,0.906631,"Missing"
yimam-etal-2017-multilingual,E09-1027,0,\N,Missing
