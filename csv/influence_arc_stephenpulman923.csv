2021.naacl-industry.25,N15-1024,0,0.0366251,"Missing"
2021.naacl-industry.25,L18-1707,0,0.0215253,"017), although crucially, forms of names are frequent (“Shaq” for “Shaquille O’Neal”); and users often mention new entities un- both these works assume gold standard NER and focus purely on the EL component. known to the system. Traditionally, a pipelined architecture of NER In order to address these issues we propose a followed by EL has been used to address the ennovel Named Entity Understanding (NEU) system tity linking task (Lin et al., 2012; Derczynski et al., that combines and optimizes NER and EL for noisy spoken natural language utterances. We pass multi- 2015; Bontcheva et al., 2017; Bowden et al., 2018). Since these approaches rely only on the best NER ple NER hypotheses to EL for reranking, enabling hypothesis, errors from NER propagate to the EL NER to benefit from EL by including information step. To alleviate this, joint models have been profrom the knowledge base (KB). posed: Sil and Yates (2013) proposed an NER+EL ∗ Equal contributions. † model which re-ranks candidate mentions and engraceyx.scut@gmail.com ‡ lli9@apple.com tity links produced by their base model. Our work 196 Proceedings of NAACL HLT 2021: IndustryTrack Papers, pages 196–204 June 6–11, 2021. ©2021 Association for Compu"
2021.naacl-industry.25,N19-1423,0,0.0274221,"ned GloVe word embeddings 1 (Pennington et al., 2014) to pass through another biLSTM and fed into a CRF model to produce the final label prediction based on a score s(y˜i , x; θ) that jointly optimizes the probability of labels for the tokens and the transition score for the entire sequence y˜i = (y1 , . . . , yT ) given the input x: s(y˜i , x; θ) = T X (ψt,θ (yt ) + φt,t+1 (yt , yt+1 )) , t=0 where ψt,θ is the biLSTM prediction score from the label yt of the tth token, and φ(j, k) is the transition score from label j to label k. 1 We also tried more recent contextual embeddings such as BERT (Devlin et al., 2019), and empirically observed very little difference in performance when compared to GloVe. So we adopt GloVE, which is substantially more efficient in terms of inference time required. During training, we maximize the probability of the correct label sequence pseq , which is defined as pseq (y˜i , x; θ) = P exp(s(y˜i , x; θ)) , ˜j , x; θ)) y˜j ∈S exp (s(y where y˜i is the label sequence for hypothesis i, and S is the set of all possible label sequences. During inference, we generate up to 5 NER alternatives for each utterance using beam search. We also calculate a mention level confidence pmen f"
2021.naacl-industry.25,K17-1008,0,0.0667323,"Missing"
2021.naacl-industry.25,P19-1609,0,0.0158877,"year the song was released in etc.) that we leverage to automatically construct these relationship pointers. 197 entity in the knowledge base to its related entities and this relational information is leveraged by the EL model for entity disambiguation (described in 5.2). We then compute the tf-idf score for all the n-gram terms present in the entities and store them in the inverted index. For each hypothesis predicted by the NER model we query the retrieval engine with the corresponding text. We first send the query through a highprecision seq-to-seq correction model (Schmaltz et al., 2017; Ge et al., 2019) trained using common errors observed in usage. Next, we construct ngram features from the corrected query in a similar way to the indexing phase and retrieve all entities matching these n-gram features in our inverted index. Additionally, we use synonyms derived from usage for each term in the query to expand our search criteria: for example, our synonym list for “Friend&quot; contains “Friends&quot;, which matches the TV show name which would have been missed if only the original term was used. For each entity retrieved, we get the tf-idf score for the terms present in the query chunk from the inverte"
2021.naacl-industry.25,N13-1122,0,0.0212206,"ucial for a satisfying user experience. How- set of features rather than phonemes as input, and unlike the latter, we are able to use a deep model ever, NER and EL methods that work well on written text often perform poorly in such appli- because of the large volume of data available. EL has been well explored in the context of clean cations: utterances are relatively short (with just 5 (Martins et al., 2019; Kolitsas et al., 2018; Luo tokens, on average), so there is not much context to et al., 2015) and noisy text inputs (Eshel et al., help disambiguate; speech recognizers make errors 2017; Guo et al., 2013; Liu et al., 2013), but as with (“Play Bohemian raspberry” for “Play Bohemian NER, there have been only a few efforts to explore Rhapsody&quot;); users also make mistakes (“Cristiano EL in the context of transcribed speech (Benton and Nando” for “Cristiano Ronaldo”); non-canonical Dredze, 2015; Gao et al., 2017), although crucially, forms of names are frequent (“Shaq” for “Shaquille O’Neal”); and users often mention new entities un- both these works assume gold standard NER and focus purely on the EL component. known to the system. Traditionally, a pipelined architecture of NER In order to address"
2021.naacl-industry.25,K18-1050,0,0.0198164,"ers (2018) and Coucke et al. acting with virtual assistants (e.g. “Call Jon”, “Play Adele hello”, “Score for Warrior Kings game”) (2018); however, unlike the former, we use a richer is crucial for a satisfying user experience. How- set of features rather than phonemes as input, and unlike the latter, we are able to use a deep model ever, NER and EL methods that work well on written text often perform poorly in such appli- because of the large volume of data available. EL has been well explored in the context of clean cations: utterances are relatively short (with just 5 (Martins et al., 2019; Kolitsas et al., 2018; Luo tokens, on average), so there is not much context to et al., 2015) and noisy text inputs (Eshel et al., help disambiguate; speech recognizers make errors 2017; Guo et al., 2013; Liu et al., 2013), but as with (“Play Bohemian raspberry” for “Play Bohemian NER, there have been only a few efforts to explore Rhapsody&quot;); users also make mistakes (“Cristiano EL in the context of transcribed speech (Benton and Nando” for “Cristiano Ronaldo”); non-canonical Dredze, 2015; Gao et al., 2017), although crucially, forms of names are frequent (“Shaq” for “Shaquille O’Neal”); and users often mention ne"
2021.naacl-industry.25,N16-1030,0,0.0378992,"retrieval stage, for each NER hypothesis, we construct a structured search query and retrieve the top-c candidates from the retrieval engine. In the ranking stage, we use a neural network to rank these candidate entity links within each NER hypothesis while simultaneously using rich signals (entity popularity, similarity between entity embeddings, the relation across multiple entities in one utterance, etc.) from these entity links as additional features to re-rank the NER hypotheses from the previous step, thus jointly addressing both the NER and EL tasks. 3.1 NER For the NER task, following Lample et al. (2016) we use a combination of character and word level features. They are extracted by a bi-directional LSTM (biLSTM) (Hochreiter and Schmidhuber, 1997), and then concatenated with pre-trained GloVe word embeddings 1 (Pennington et al., 2014) to pass through another biLSTM and fed into a CRF model to produce the final label prediction based on a score s(y˜i , x; θ) that jointly optimizes the probability of labels for the tokens and the transition score for the entire sequence y˜i = (y1 , . . . , yT ) given the input x: s(y˜i , x; θ) = T X (ψt,θ (yt ) + φt,t+1 (yt , yt+1 )) , t=0 where ψt,θ is the b"
2021.naacl-industry.25,W12-3016,0,0.0232132,"&quot;); users also make mistakes (“Cristiano EL in the context of transcribed speech (Benton and Nando” for “Cristiano Ronaldo”); non-canonical Dredze, 2015; Gao et al., 2017), although crucially, forms of names are frequent (“Shaq” for “Shaquille O’Neal”); and users often mention new entities un- both these works assume gold standard NER and focus purely on the EL component. known to the system. Traditionally, a pipelined architecture of NER In order to address these issues we propose a followed by EL has been used to address the ennovel Named Entity Understanding (NEU) system tity linking task (Lin et al., 2012; Derczynski et al., that combines and optimizes NER and EL for noisy spoken natural language utterances. We pass multi- 2015; Bontcheva et al., 2017; Bowden et al., 2018). Since these approaches rely only on the best NER ple NER hypotheses to EL for reranking, enabling hypothesis, errors from NER propagate to the EL NER to benefit from EL by including information step. To alleviate this, joint models have been profrom the knowledge base (KB). posed: Sil and Yates (2013) proposed an NER+EL ∗ Equal contributions. † model which re-ranks candidate mentions and engraceyx.scut@gmail.com ‡ lli9@appl"
2021.naacl-industry.25,P13-1128,0,0.0360844,"ying user experience. How- set of features rather than phonemes as input, and unlike the latter, we are able to use a deep model ever, NER and EL methods that work well on written text often perform poorly in such appli- because of the large volume of data available. EL has been well explored in the context of clean cations: utterances are relatively short (with just 5 (Martins et al., 2019; Kolitsas et al., 2018; Luo tokens, on average), so there is not much context to et al., 2015) and noisy text inputs (Eshel et al., help disambiguate; speech recognizers make errors 2017; Guo et al., 2013; Liu et al., 2013), but as with (“Play Bohemian raspberry” for “Play Bohemian NER, there have been only a few efforts to explore Rhapsody&quot;); users also make mistakes (“Cristiano EL in the context of transcribed speech (Benton and Nando” for “Cristiano Ronaldo”); non-canonical Dredze, 2015; Gao et al., 2017), although crucially, forms of names are frequent (“Shaq” for “Shaquille O’Neal”); and users often mention new entities un- both these works assume gold standard NER and focus purely on the EL component. known to the system. Traditionally, a pipelined architecture of NER In order to address these issues we pr"
2021.naacl-industry.25,D15-1104,0,0.028036,"step. To alleviate this, joint models have been profrom the knowledge base (KB). posed: Sil and Yates (2013) proposed an NER+EL ∗ Equal contributions. † model which re-ranks candidate mentions and engraceyx.scut@gmail.com ‡ lli9@apple.com tity links produced by their base model. Our work 196 Proceedings of NAACL HLT 2021: IndustryTrack Papers, pages 196–204 June 6–11, 2021. ©2021 Association for Computational Linguistics differs in that we use a high precision NER system, while they use a large number of heuristically obtained Noun Phrase (NP) chunks and word n-grams as input to the EL stage. Luo et al. (2015) jointly train an NER and EL system using a probabilistic graphical model. However, these systems are trained and tested on clean text and do not address the noise problems we are concerned with. 3 Architecture Design For a given utterance, we first detect and label entities using the NER model and generate the top-l candidate hypotheses using beam search. The EL model consists of two stages: (i) candidate retrieval and (ii) joint linking and re-ranking. In the retrieval stage, for each NER hypothesis, we construct a structured search query and retrieve the top-c candidates from the retrieval"
2021.naacl-industry.25,P19-2026,0,0.0166976,"t to Abujabal and Gaspers (2018) and Coucke et al. acting with virtual assistants (e.g. “Call Jon”, “Play Adele hello”, “Score for Warrior Kings game”) (2018); however, unlike the former, we use a richer is crucial for a satisfying user experience. How- set of features rather than phonemes as input, and unlike the latter, we are able to use a deep model ever, NER and EL methods that work well on written text often perform poorly in such appli- because of the large volume of data available. EL has been well explored in the context of clean cations: utterances are relatively short (with just 5 (Martins et al., 2019; Kolitsas et al., 2018; Luo tokens, on average), so there is not much context to et al., 2015) and noisy text inputs (Eshel et al., help disambiguate; speech recognizers make errors 2017; Guo et al., 2013; Liu et al., 2013), but as with (“Play Bohemian raspberry” for “Play Bohemian NER, there have been only a few efforts to explore Rhapsody&quot;); users also make mistakes (“Cristiano EL in the context of transcribed speech (Benton and Nando” for “Cristiano Ronaldo”); non-canonical Dredze, 2015; Gao et al., 2017), although crucially, forms of names are frequent (“Shaq” for “Shaquille O’Neal”); and"
2021.naacl-industry.25,D14-1162,0,0.0857408,"ach NER hypothesis while simultaneously using rich signals (entity popularity, similarity between entity embeddings, the relation across multiple entities in one utterance, etc.) from these entity links as additional features to re-rank the NER hypotheses from the previous step, thus jointly addressing both the NER and EL tasks. 3.1 NER For the NER task, following Lample et al. (2016) we use a combination of character and word level features. They are extracted by a bi-directional LSTM (biLSTM) (Hochreiter and Schmidhuber, 1997), and then concatenated with pre-trained GloVe word embeddings 1 (Pennington et al., 2014) to pass through another biLSTM and fed into a CRF model to produce the final label prediction based on a score s(y˜i , x; θ) that jointly optimizes the probability of labels for the tokens and the transition score for the entire sequence y˜i = (y1 , . . . , yT ) given the input x: s(y˜i , x; θ) = T X (ψt,θ (yt ) + φt,t+1 (yt , yt+1 )) , t=0 where ψt,θ is the biLSTM prediction score from the label yt of the tth token, and φ(j, k) is the transition score from label j to label k. 1 We also tried more recent contextual embeddings such as BERT (Devlin et al., 2019), and empirically observed very l"
2021.naacl-industry.25,W09-1119,0,0.122084,"vious NER task. To evaluate entity linking quality, we adopt a strict F1 metric similar to the one used for NER. Besides entity boundary and entity type, the resolved entity also needs to be correct for the entity prediction to be counted as a true positive. For NER model training, we use standard minibatch gradient descent using the Adam optimizer with an initial learning rate of 0.001, a scheduled learning rate decay of 0.99, LSTM with a hidden layer of size 350 and a batch size of 256. We apply a dropout of 0.5 to the embedding and biLSTM layers, and include token level gazetteer features (Ratinov and Roth, 2009) to boost performance in recognizing common entities. We linearly project these gazetteer features and concatenate the projection with the 200 dimensional word embeddings and 100 dimensional character embeddings which are then fed into the biLSTM followed by the CRF. For EL, the character CNN model we use has two layers, each with 100 convolution kernels of size 3, 4, and 5. Character embedding are 25 dimensional and trained end to end with the entity linking task. The MLP for embedding similarity takes the concatenation of two name embeddings, as well as their element-wise sum, difference, mi"
2021.naacl-industry.25,D17-1298,0,0.0145549,"he music artist, album, year the song was released in etc.) that we leverage to automatically construct these relationship pointers. 197 entity in the knowledge base to its related entities and this relational information is leveraged by the EL model for entity disambiguation (described in 5.2). We then compute the tf-idf score for all the n-gram terms present in the entities and store them in the inverted index. For each hypothesis predicted by the NER model we query the retrieval engine with the corresponding text. We first send the query through a highprecision seq-to-seq correction model (Schmaltz et al., 2017; Ge et al., 2019) trained using common errors observed in usage. Next, we construct ngram features from the corrected query in a similar way to the indexing phase and retrieve all entities matching these n-gram features in our inverted index. Additionally, we use synonyms derived from usage for each term in the query to expand our search criteria: for example, our synonym list for “Friend&quot; contains “Friends&quot;, which matches the TV show name which would have been missed if only the original term was used. For each entity retrieved, we get the tf-idf score for the terms present in the query chun"
2021.naacl-main.44,W14-3348,0,0.0273802,"the P-value to be 0.0201. We use several automated metrics to compare the rewrites with the ground truth and compute their Pearson correlation with the human judgements (see Table 3 for results). Exact Match is a binary variable that indicates the token set overlap applied after the standard preprocessing: lower-casing, stemming, punctuation and stopword removal. ROUGE (Lin, 2004) reflects similarity between two texts in terms of n-gram overlap (R-1 for unigrams; R-2 for bigrams and R-L for the longest common n-gram). We report the mean for precision (P), recall (R) and F-measure (F). METEOR (Denkowski and Lavie, 2014) is a machine translation metric based on exact, stem, synonym, and paraphrase matches between words and phrases. BLEU (Papineni et al., 2002) is a text similarity metric that uses a modified form of precision and n-grams from candidate and reference texts. Embeddings group several unsupervised approaches that produce a sentence-level vector representation: Universal Sentence Encoder (Cer et al., 2018) and InferSent (Conneau et al., 2017). Search Results – we use both question rewrites in Google Search and compare the overlap between the produced page ranks in terms of the standard IR metrics:"
2021.naacl-main.44,D19-1605,0,0.20294,"ides a resource for the commu- putational bottleneck was a QR model that learns nity to develop, evaluate, and advance methods for to sample tokens from the conversational context end-to-end, open-domain conversational QA. as a pre-processing step before QA. 521 Table 1: The datasets that QReCC extends to open-domain conversational QA (QuAC, CAsT and NQ) and the datasets that are complementary to QReCC (CANARD and SaaC). RC - Reading Comprehension, PR - Passage Retrieval, QR - Question Rewriting. Dataset QuAC (Choi et al., 2018) NQ (Kwiatkowski et al., 2019) CAsT (Dalton et al., 2019) CANARD (Elgohary et al., 2019) OR-QuAC (Qu et al., 2020) SaaC (Ren et al., 2020) QReCC (our work) #Dialogues 13.6K 0 80 5.6K 5.6K 80 13.7K Question Rewriting. CANARD (Elgohary et al., 2019) provides rewrites for the conversational questions from the QuAC dataset. QR effectively modifies all follow-up questions such that they can be correctly interpreted outside of the conversational context as well. This extension to the conversational QA task proved especially useful while allowing retrieval models to incorporate conversational context (Voskarides et al., 2020; Vakulenko et al., 2020; Lin et al., 2020). More recently, Qu"
2021.naacl-main.44,D18-1443,0,0.0387354,"pointer-generator decoder previously proposed for task-oriented dialogues (Quan et al., 2019). Generator is a Transformer decoder model with a language modeling head (linear layer in the size of the vocabulary) (Radford et al., 2019). Generator + Multiple-choice model has a second head for the auxiliary classification task that distinguishes between the correct rewrite and several noisy rewrites as negative samples (inspired by TransferTransfo (Wolf et al., 2019b)). CopyTransformer uses one of the attention heads of the Transformer as a pointer to copy tokens from the input sequence directly (Gehrmann et al., 2018). Transformer++ model has two language modeling heads that produce separate vocabulary distributions, which are then combined via a parameterized weighted sum (the coefficients are produced by combining the output of the first attention head and the input embeddings). on MS MARCO. We then retrieve the top-100 relevant passages per question. Afterwards, we use BERT-Large fine-tuned for the task of reading comprehension. This model takes a question and each of the relevant passages as input and produces the answer span (Wolf et al., 2019a). BERT-Large produces a score (SBERT ), which is combined"
2021.naacl-main.44,Q19-1026,0,0.19664,"to passages. To produce the first baseline, we augment an open-domain QA model with a QR component that allows us to extend it to a conversational scenario. We evaluate this approach on the QReCC dataset, reporting the end-to-end effectiveness as well as the effectiveness on the individual subtasks separately. 2 Related Work QReCC builds upon three publicly available datasets and further extends them to the opendomain conversational QA setting: Question Answering in Context (QuAC) (Choi et al., 2018), TREC Conversational Assistant Track (CAsT) (Dalton et al., 2019) and Natural Questions (NQ) (Kwiatkowski et al., 2019). QReCC is the first large-scale dataset that supports the tasks of QR, passage retrieval, and reading comprehension (see Table 1 for the dataset comparison). Open-domain QA. Reading comprehension (RC) approaches were recently extended to incorporate a retrieval subtask (Chen et al., 2017; Yang et al., 2019; Lee et al., 2019). This task is also referred to as machine reading at scale (Chen et al., 2017) or end-to-end QA (Yang et al., 2019). In this setup a reading comprehension component is preceded by a document retrieval component. The answer spans are extracted from documents retrieved from"
2021.naacl-main.44,P19-1612,0,0.074759,"ilds upon three publicly available datasets and further extends them to the opendomain conversational QA setting: Question Answering in Context (QuAC) (Choi et al., 2018), TREC Conversational Assistant Track (CAsT) (Dalton et al., 2019) and Natural Questions (NQ) (Kwiatkowski et al., 2019). QReCC is the first large-scale dataset that supports the tasks of QR, passage retrieval, and reading comprehension (see Table 1 for the dataset comparison). Open-domain QA. Reading comprehension (RC) approaches were recently extended to incorporate a retrieval subtask (Chen et al., 2017; Yang et al., 2019; Lee et al., 2019). This task is also referred to as machine reading at scale (Chen et al., 2017) or end-to-end QA (Yang et al., 2019). In this setup a reading comprehension component is preceded by a document retrieval component. The answer spans are extracted from documents retrieved from a document collection, given as input. The standard approach to end-to-end open-domain QA is (1) use an efficient filtering approach to reduce the number of candidate passages to the top-k of the most relevant ones (usually BM25 based on the bag-of-words representation); and then (2) re-rank the subset of the top-k relevant"
2021.naacl-main.44,N18-2108,0,0.115359,"Missing"
2021.naacl-main.44,Q19-1016,0,0.0227373,"writes and answer provenance links. Orange indicates coreference cases where the highlighted token should be replaced with its antecedent (in bold). Blue indicates the tokens that should be generated to make the question unambiguous outside of the conversational context. Introduction It is often not possible to address a complex information need with a single question. Consequently, there is a clear need to extend open-domain question answering (QA) to a conversational setting. This task is commonly referred to as conversational (interactive or sequential) QA (Webb, 2006; Saeidi et al., 2018; Reddy et al., 2019). Conversational QA requests an answer conditioned on both the question and the previous conversation turns as context. Previously proposed large-scale benchmarks for conversational QA, such as QuAC and CoQA, limit the topic of conversation to the content of a single document. In practice, however, the answers can be distributed across several documents that are relevant to the conversation, or the topic of the conversation may also drift. To investigate this phenomena and develop approaches suitable for the complexities of this task, we introduce a new dataset for open-domain conversational Q"
2021.naacl-main.44,D19-5829,1,0.629176,"Missing"
2021.naacl-main.44,D18-1233,0,0.0818937,"ReCC with question rewrites and answer provenance links. Orange indicates coreference cases where the highlighted token should be replaced with its antecedent (in bold). Blue indicates the tokens that should be generated to make the question unambiguous outside of the conversational context. Introduction It is often not possible to address a complex information need with a single question. Consequently, there is a clear need to extend open-domain question answering (QA) to a conversational setting. This task is commonly referred to as conversational (interactive or sequential) QA (Webb, 2006; Saeidi et al., 2018; Reddy et al., 2019). Conversational QA requests an answer conditioned on both the question and the previous conversation turns as context. Previously proposed large-scale benchmarks for conversational QA, such as QuAC and CoQA, limit the topic of conversation to the content of a single document. In practice, however, the answers can be distributed across several documents that are relevant to the conversation, or the topic of the conversation may also drift. To investigate this phenomena and develop approaches suitable for the complexities of this task, we introduce a new dataset for open-do"
2021.naacl-main.44,K16-1028,0,0.125716,"by human annotators is left for future work. 6 Question Rewriting Metrics Validation BLEU has typically been used in previous work for measuring the quality of QR (Elgohary et al., 2019; Lin et al., 2020). We conduct a systematic evaluation and compare BLEU with alternative metrics, previously applied in summarization and translation, to ensure the most reliable metrics we can obtain for the model selection. Our evaluation shows that BLEU does not compare favourably with other metrics in evaluating the quality of QR. Task. We took a random sample of 10K questions and used a seq-to-seq model (Nallapati et al., 2016) trained with questions and conversation context from the QReCC dataset to generate question rewrites. These generated rewrites were compared to the ground truth rewrites produced by human annotators. Different annotators graded each modelgenerated rewrite with a binary label: 0 (incorrect rewrite) or 1 (correct rewrite). For a question rewrite to be correct it does not have to exactly 2 We use the version of a web page, which is the closest to the end date of the dialogue collection (November 24, 2019). match the ground truth rewrite, but it should correctly capture the conversational context"
2021.naacl-main.44,2021.naacl-main.44,1,0.0530913,"Missing"
2021.naacl-main.44,P02-1040,0,0.109618,"h the human judgements (see Table 3 for results). Exact Match is a binary variable that indicates the token set overlap applied after the standard preprocessing: lower-casing, stemming, punctuation and stopword removal. ROUGE (Lin, 2004) reflects similarity between two texts in terms of n-gram overlap (R-1 for unigrams; R-2 for bigrams and R-L for the longest common n-gram). We report the mean for precision (P), recall (R) and F-measure (F). METEOR (Denkowski and Lavie, 2014) is a machine translation metric based on exact, stem, synonym, and paraphrase matches between words and phrases. BLEU (Papineni et al., 2002) is a text similarity metric that uses a modified form of precision and n-grams from candidate and reference texts. Embeddings group several unsupervised approaches that produce a sentence-level vector representation: Universal Sentence Encoder (Cer et al., 2018) and InferSent (Conneau et al., 2017). Search Results – we use both question rewrites in Google Search and compare the overlap between the produced page ranks in terms of the standard IR metrics: Recall@k for the top-k links, Average Recall (AR) and Normalized Discounted Cumulative Gain (NDCG). The best performing metric in our experim"
2021.naacl-main.44,D19-1462,0,0.128628,"ict all coreference mentions with the corresponding if the passage is irrelevant. The scores obtained antecedents from the cluster. 525 Figure 3: Rouge-1R, USE and R@10 metrics of baseline co-reference model, top-3 encoder-decoder models, and Transformer++ model based on dialogue turn number PointerGenerator uses a bi-LSTM encoder and a pointer-generator decoder, which allows to copy and generate tokens (Elgohary et al., 2019). GECOR uses two bi-GRU encoders, one for user utterance and other for dialogue context, and a pointer-generator decoder previously proposed for task-oriented dialogues (Quan et al., 2019). Generator is a Transformer decoder model with a language modeling head (linear layer in the size of the vocabulary) (Radford et al., 2019). Generator + Multiple-choice model has a second head for the auxiliary classification task that distinguishes between the correct rewrite and several noisy rewrites as negative samples (inspired by TransferTransfo (Wolf et al., 2019b)). CopyTransformer uses one of the attention heads of the Transformer as a pointer to copy tokens from the input sequence directly (Gehrmann et al., 2018). Transformer++ model has two language modeling heads that produce sepa"
2021.naacl-main.44,N19-4013,0,0.189488,"Missing"
2021.nlp4convai-1.22,2021.naacl-industry.25,1,0.748675,"m ASR into either a pre-trained baseline or pause-sensitive contextual embedding (refer Section 4). We pass the so-obtained contextual embedding representations into a relatively simple shallow parsing model. The shallow parser is a sequence tagger model that feeds the contextual embedding corresponding to each text token into a single-layered BiLSTM with a CRF on top, obtaining a predicted label for each token as the output (refer Fig. 4). During training, our shallow parser is optimized by minimizing the Negative Log Likelihood (NLL) over the data. Our shallow parser is identical to that of Muralidharan et al. (2021), except that we feed the output of the BiLSTM into a CRF layer, which we empirically observe boosts performance in all cases. 4 Pre-training Embeddings with Pause Information The baseline version of the shallow parser uses a text-only embedding. This embedding is a BERTstyle text-based language model (Devlin et al., 2019) trained without the next sentence prediction auxiliary task of the original BERT architecture. To investigate the effect of pauses, we compare 2 “thank you next"" is the ASR recognition result for the song “Thank u, Next"". the baseline architecture with an identical BERTbased"
2021.nlp4convai-1.22,N19-1423,0,0.0803948,"le et al., 2019) and sentiment analysis (Rahman et al., 2020). However, they require the data from different modalities to be aligned in order to produce the embeddings, again increasing the cost of generating training data. We work within this paradigm of multi-modal grounded embeddings using pause markers in spoken utterances for the task of shallow parsing (Ladhak et al., 2016; Vijayakumar et al., 2017; Rahman et al., 2020; Liu et al., 2020; Lan et al., 2020). We propose a novel self-supervised architecture adding a pause prediction task to traditional textbased BERT-style language models (Devlin et al., 2019). To the best of our knowledge, this is the first study on grounding textual embeddings in spoken pause signals. We pre-train our embeddings using token-level pauses from the ASR system, and consume the representations in the downstream parsing task without fine-tuning. We do not input pause duration in our downstream task and, therefore, do not require data from different modalities to be aligned. This sets our approach apart from end-to-end approaches that use speech signals as input during inference, requiring additional task-specific annotation (such as Chi et al. (2020); Liu et al. (2020)"
2021.nlp4convai-1.22,W95-0107,0,0.243986,"hout as UsageData). Given the sampling methodology, it can be concluded that there are no speaker-dependent effects in our dataset. The utterances have been taken from an upstream ASR system which provides each utterance hypothesis with pause durations following each token, in milliseconds. In order to test the findings of Seifart et al. (2018) in the voice assistant setting, we randomly sample UsageData utterances from the sports, movies, and music domains in English and French (referred as EngFrPauseData); see Table 1 for an overview. We manually annotate EngFrPauseData with BIO-style tags (Ramshaw and Marcus, 1995; Tjong Kim Sang and De Meulder, 2003); O for non-entity, B for beginning of an entity, I for inside an entity. Furthermore, if a token is tagged with a B or I, it contains an additional entity tag (e.g., Artist). We 2 Analysis of Pause Duration combine the pause information from ASR with the In Seifart et al. (2018), time-aligned spoken lan- named entity tags to derive pauses before, during, guage corpora in multiple languages were com- and after an entity. We aggregate pause and token pared in terms of speech rate and pause duration. duration statistics across all utterances for all doThe va"
2021.nlp4convai-1.22,D17-1096,0,0.0662885,"Missing"
C02-1105,P00-1065,0,\N,Missing
C02-1105,C92-2117,0,\N,Missing
C02-1105,H94-1020,0,\N,Missing
C02-1105,P98-1013,0,\N,Missing
C02-1105,C98-1013,0,\N,Missing
C02-1105,palmer-etal-2000-semantic,0,\N,Missing
C02-1105,H01-1010,0,\N,Missing
C04-1027,H90-1021,0,0.0181915,"about the way we view the structure of the world. Our method for inducing domain theories relies on this inversion, since in the general case it is a much easier job to disambiguate sentences than to directly encode the theory that we are drawing on in so doing. Our strategy for trying to build a domain theory is to try to capitalise on the information that is tacitly contained in those disambiguation decisions. 2 Some background (Pulman, 2000) showed that it was possible to learn a simple domain theory from a disambiguated corpus: a subset of the ATIS (air travel information service) corpus (Doddington and Godfrey, 1990). Ambiguous sentences were annotated as shown to indicate the preferred reading: [i,would,like, [the,cheapest,flight, from,washington,to,atlanta] ] [do,they,[serve,a,meal],on, [the,flight,from,san_francisco,to,atlanta]] [i,would,like, [a,flight,from,boston, to,san_francisco, [that,leaves,before,’8:00’] ] ] The ‘good’ and the ‘bad’ parses were used to produce simplified first order logical forms representing the semantic content of the various readings of the sentences. The ‘good’ readings were used as positive evidence, and the ‘bad’ readings (or more accurately, the bad parts of some of the r"
C04-1027,C02-1105,1,0.622497,"the notion of logical consistency is too strong a test in many cases. Note also that the results of the theory induction process are perfectly comprehensible - the outcome is a theory with some logical structure, rather than a black box. The method requires a fully parsed corpus with corresponding logical forms. Using a similar technique, we have experimented with slightly larger datasets, using the Penn Tree Bank (Marcus et al., 1994) since the syntactic annotations for sentences given there are intended to be complete enough for semantic interpretation, in principle, at least. In practice, (Liakata and Pulman, 2002) report, it is by no means easy to do this. It is possible to recover partial logical forms from a large proportion of the treebank, but these are not complete or accurate enough to simply replicate the ATIS experiment. In the work reported here, we selected about 40 texts containing the verb ‘resign’, all reporting, among other things, ‘company succession’ events, a scenario familiar from the Message Understanding Conference (MUC) task (Grishman and Sundheim, 1995). The texts amounted to almost 4000 words in all. Then we corrected and completed some automatically produced logical forms by han"
C04-1027,H94-1020,0,0.0387888,"s involved in that experiment were too small for the results to be statistically meaningful, the experiment proved that the method works in principle, although of course in reality the notion of logical consistency is too strong a test in many cases. Note also that the results of the theory induction process are perfectly comprehensible - the outcome is a theory with some logical structure, rather than a black box. The method requires a fully parsed corpus with corresponding logical forms. Using a similar technique, we have experimented with slightly larger datasets, using the Penn Tree Bank (Marcus et al., 1994) since the syntactic annotations for sentences given there are intended to be complete enough for semantic interpretation, in principle, at least. In practice, (Liakata and Pulman, 2002) report, it is by no means easy to do this. It is possible to recover partial logical forms from a large proportion of the treebank, but these are not complete or accurate enough to simply replicate the ATIS experiment. In the work reported here, we selected about 40 texts containing the verb ‘resign’, all reporting, among other things, ‘company succession’ events, a scenario familiar from the Message Understan"
C04-1027,C96-1079,0,\N,Missing
C08-1022,N07-2045,0,0.105695,"Missing"
C08-1022,P06-4020,0,0.0212739,"Missing"
C08-1022,W07-1604,0,0.433069,"Missing"
C08-1022,W07-1607,1,0.849108,"Missing"
C08-1022,I08-1059,0,0.663071,"Missing"
C08-1022,izumi-etal-2004-overview,0,\N,Missing
C08-1022,I08-2082,0,\N,Missing
C08-2012,P04-1015,0,0.0169311,"head, it would be difficult to derive the relatedness between (Shanghai) and (economic) or (law), since the two words are even not heads of the conjuncts (eco© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. 47 Coling 2008: Companion volume – Posters and Demonstrations, pages 47–50 Manchester, August 2008 nomic development) and (law development). As to analysis of NPs, there have been a lot of work on statistical techniques for lexical dependency parsing of sentences (Collins and Roark, 2004; McDonald et al., 2005), and these techniques potentially can be used for analysis of NPs if appropriate resources for NPs are available. However, these techniques are all meant to building a dependency tree, while the conceptual relatedness in NPs may form a graph, with multidependency allowed. Additionally, these methods generally suffer from the difficulty of local estimation from limited contexts and the structural information is difficult to be exploited (Califf and Mooney, 2003). Recently, relational learning methods in general and inductive logic programming (ILP) in particular have at"
C08-2012,H05-1066,0,0.0246052,"Missing"
C08-2012,C92-2070,0,\N,Missing
C12-2054,D11-1129,1,0.180334,"ng little attention to the grammatical relations that hold between the words. Following a different path, Coecke et al. (2010) provide a solution that offers compositional abilities to distributional models while at the same time avoids all the above pitfalls. Based on the abstract setting of category theory, the authors develop a generic mathematical framework whereby the meaning of a sentence of any length and structure can, in principle, be turned into a vector, following the rules of the grammar. Implementations of this model for transitive and intransitive sentences have been provided by Grefenstette and Sadrzadeh (2011a,b). However, although their method outperforms the multiplicative and additive models of Mitchell and Lapata (2008) on simple transitive sentences, it has a non-scalability problem. Specifically, the concrete structures used in the actual computations are not faithful to the linguistic types of the underlying type-logic, hence the model does not generalize to more complex phrases and sentences where a relational structure can be found nested in another relational structure. Furthermore, the vectors obtained for sentences of different grammatical structures live in different vector spaces: se"
C12-2054,W11-2507,1,0.199913,"ng little attention to the grammatical relations that hold between the words. Following a different path, Coecke et al. (2010) provide a solution that offers compositional abilities to distributional models while at the same time avoids all the above pitfalls. Based on the abstract setting of category theory, the authors develop a generic mathematical framework whereby the meaning of a sentence of any length and structure can, in principle, be turned into a vector, following the rules of the grammar. Implementations of this model for transitive and intransitive sentences have been provided by Grefenstette and Sadrzadeh (2011a,b). However, although their method outperforms the multiplicative and additive models of Mitchell and Lapata (2008) on simple transitive sentences, it has a non-scalability problem. Specifically, the concrete structures used in the actual computations are not faithful to the linguistic types of the underlying type-logic, hence the model does not generalize to more complex phrases and sentences where a relational structure can be found nested in another relational structure. Furthermore, the vectors obtained for sentences of different grammatical structures live in different vector spaces: se"
C12-2054,W10-2805,0,0.0432442,"This approach has been proved useful in many natural language tasks (Curran, 2004; Schütze, 1998; Landauer and Dumais, 1997; Manning et al., 2008), but until now it lacks any means of compositionality that would allow the combination of two word vectors into a new one following some grammar rule. In fact, compositional abilities of distributional models have been subject of much discussion and research in recent years. For example, Mitchell and Lapata (2008) present results for intransitive sentences, Erk and Padó (2004) work on transitive verb phrases, while Baroni and Zamparelli (2010) and Guevara (2010) provide comprehensive analyses of adjective-noun phrases. Despite the experimental strength of these approaches, most of them only deal with phrases and sentences of two words. On the other hand, Socher et al. (2010, 2011) use recursive neural networks in order to produce vectors for sentences of arbitrary length with good results. However, their method is somehow detached from the formal semantics view, paying little attention to the grammatical relations that hold between the words. Following a different path, Coecke et al. (2010) provide a solution that offers compositional abilities to di"
C12-2054,P08-1028,0,0.856989,"et of words as the basis of a vector space and a window of size k, then counting how many times the word in question has co-occurred with each base in that window. This approach has been proved useful in many natural language tasks (Curran, 2004; Schütze, 1998; Landauer and Dumais, 1997; Manning et al., 2008), but until now it lacks any means of compositionality that would allow the combination of two word vectors into a new one following some grammar rule. In fact, compositional abilities of distributional models have been subject of much discussion and research in recent years. For example, Mitchell and Lapata (2008) present results for intransitive sentences, Erk and Padó (2004) work on transitive verb phrases, while Baroni and Zamparelli (2010) and Guevara (2010) provide comprehensive analyses of adjective-noun phrases. Despite the experimental strength of these approaches, most of them only deal with phrases and sentences of two words. On the other hand, Socher et al. (2010, 2011) use recursive neural networks in order to produce vectors for sentences of arbitrary length with good results. However, their method is somehow detached from the formal semantics view, paying little attention to the grammatic"
C12-2054,D10-1115,0,\N,Missing
C12-2054,D08-1094,0,\N,Missing
C12-2054,J98-1004,0,\N,Missing
C96-1077,P95-1004,1,0.906936,"Missing"
C96-1077,E95-1028,0,0.0524638,"sallows (P1...Pi-:,, Pi, Pi+,...P~) or B coercively disallows (Pt...Pi-I, Pi...P~): this is (2) with the restriction j = i + 1 or j = i; this allows epenthetic rules to be used but rnay in certain cases be counteriI&gt; tuitive for the user when insertion rules are used. For example, the rule (E* (g, g), u x E~, E~ x v, E*) (&apos;change &apos;u to v aft;re&apos; a g&apos;) would not disallow a string-tuple partitioned as ...(.(I, g), (e, c), (u, u)... assmning some CR rule allows (e, e). Earlier versions of the partition fbrmalism could not (in practice) cope with multiple lexical charactors in SC r u l e s , see (Carter, 1995, §4.1). This is not tit(; case here. The tbllowing rules illustrate the formalism: V B - * =&gt; RI: V b * B R2: b - B b - * =&gt; * 456 R3: c c d b ¢&gt; d R1 and R2 illustrate the iterative application of rules on strings: they sanction the lexical-surface strings (VBBB,Vbbb), where the second (B,b) pair serves as the centre of the first application of R2 and as the left context of the second application of the same rule. R,a is an cpenthetic rule which also demonstrates centres of unequal length. (We assume that &lt;V,V), (c,c) and (d,d) are sanctioned by other identity rules.) The conditions in Defin"
C96-1077,J94-3001,0,0.702271,"ation is an alternative to the standard one deriving from Koskenniemi&apos;s work: it is believed to have some practical descriptive advantages, and is quite widely used, but has a different interpretation. Etficient interpreters exist for the notation, but until now it has not been clear how to compile to equivalent automata in a transparent way. The present paper shows how to do this, using some of the conceptual tools provided by Kaplan and Kay&apos;s regular relations calculus. 1 Introduction Two-level formalisins based on that introduced by (Koskenniemi, 1983) (see also (Ritchie et al., 1992) and (Kaplan and Kay, 1994)) are widely used in practical NLP systems, and are deservedly regarded as something of a standard. However, there is at least one serious rival two-level notation in existence, developed in response to practical difficulties encountered in writing large-scale morphological descriptions using Koskenniemi&apos;s notation. Tile formalism was first introduced in (Black et al., 1987), was adapted by (Ruessink, 1989), and an extended version of it was proposed for use in the European Commission&apos;s ALEP language engineering platform (Pulman, 1991). A flmther extension to the formalisln was described in (P"
C96-1077,C94-1066,0,0.145267,"Missing"
C96-1077,C94-1029,1,0.868096,"ho-synta(:tic etfects to be cleanly described. (5) There ix a simple and etfMent direct interpreter for tt,e rule forrnalism. Tile partition formalism has been implemented in the European Commission&apos;s ALEP system tbr natural language engineering, distributed to over 30 sites. Descriptions of 9 EU languages arc t)eing develot)e(1. A version has also be,en implemented within SI{.I&apos;s Core l,anguage Engine (Carl;er, 1995) and has been used to develot) descriptions of English, French, Spanish, Polish, Swedish, and Korean morphology. An N-level extension of the formalism has also been developed by (Kiraz, 1994; Kiraz, 1996b) arrd used to de.-. scribe t;he morphology of Syria(: and other Semitic languages, arrd by (Bowden an(t Kiraz, 1995) for error dete(&apos;,tion in noncon(:atenative strings. This 1)m.&apos;tition-l)ased two-level formalism is thus a serious riwll to the standard Koskcnniemi notation. lIowever, until now, the Koskenniemi notation has had one clear advantage in that it was clear how t;o compile it into transducers, with all the consequent gains in etliciency and portability and with |;ire ability t;o construct lexical transducers as in (Karttunen, 1.994). This paper sets out to remedy (ha|;"
C96-1077,J92-3008,0,0.107912,"Missing"
C96-1077,C90-2064,0,0.0742903,"Missing"
C96-1077,E87-1003,0,\N,Missing
J00-4002,J90-3001,0,0.0511357,"are c o m m e n t e d on below. Like Pereira&apos;s a p p r o a c h , it avoids the n e e d for a free variable constraint, n o r does it n e e d the explicit recursion on the quantifier restriction i m p o s e d b y Lewin. We analyze quantified NPs at the QLF level as illustrated in the QLF for: (19) E v e r y m a n a g e r uses a computer. existsl(&e.pos(pres(use(e,every(.&gt;t,&gt;~(manager),ac.,t),.(com puter)))) We a s s u m e that e v e r y d e t e r m i n e r has its o w n equivalence, w h i c h resolves it as a quantifier: s o m e t i m e s this can be quite a complicated matter, as w i t h any (Alshawi 1990), w h i c h will resolve in different w a y s d e p e n d i n g on its linguistic context, b u t here w e a v o i d this complexity. 6 6 Separate equivalences might also make it easier to encode determiner-specific preferences, such as that of each for wide scope. A referee points out that the lack of any explicit ordering of application of equivalences makes one natural way of doing this unavailable. But I am not convinced that this would have been the right way in any case. These preferences are just that, not hard and fast rules, so we need to be able to permit all permutations where the co"
J00-4002,J94-4005,0,0.0220223,"ption in the fully general case, because it is quite conceivable that lexical disambiguation could require some contextual disambiguation first. Likewise, many PP attachment decisions have to be made on contextual grounds. There are several stategies that might be pursued. One is to adopt Pinkal&apos;s &quot;radical underspecification&quot; approach (Pinkal 1995) and use underspecified representations for all types of ambiguity, even syntactic ambiguity. The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs, either individually or in a &quot;packed&quot; structure (Alshawi and Carter 1994), with the resolution process as described here. Alternatively, I believe it is worth exploring the approach to disambiguation described in Pulman (2000), which would mesh nicely with the theory presented here. Efficiency. Extending coverage of linguistic constructs, and trying to achieve robustness or integrate with disambiguation schemes each pose the further problem of the efficiency of the HOU-based resolution process itself. While efficiency is acceptable for the short, simple sentences illustrated earlier, the computational properties of H O U mean that processing times increase in a hig"
J00-4002,P92-1005,0,0.364663,"contextual interpretation of pronouns, definites, ellipsis, focus, and quantifier scope. There is far more to say about each of these phenomena, of course, and the analyses here are by no means claimed to be definitive. The aim is merely to show that we can, to a first approximation, provide a reasonably fully worked out description of these phenomena in a truly bidirectional way. We then go on to compare the current approach with that of some other theories with similar aims: the &quot;standard&quot; version of quasi-logical form implemented in the Core Language Engine, as rationally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994); underspecified Discourse Representation Theory (Reyle 1993); and the &quot;glue language&quot; approach of Dalrymple et al. (1996). Finally, we discuss some of the semantic and logical issues raised by the approach described here, in particular the extent to which the theory meets the desiderata for accounts of underspecification outlined by van Eijck and Jaspars (1996), and the extent to which the theory supplies a methodologically satisfactory account of truth and interpretation for sentences involving contextually dependent constructs. 2. Contextual Interpretation The m"
J00-4002,P89-1030,0,0.105551,"Missing"
J00-4002,C82-1068,0,0.0158262,"interpretation, uses an intermediate quasi-logical form representation level. Using such a level of representation incurs an obligation to say what it means (&quot;no notation without denotation&quot;). We try to show how the theory presented here leads to a natural semantics for these quasi-logical forms, and indeed leads to a truth theory for contextually dependent interpretation that supports a natural consequence relation, and one appropriate for cases where interpretations are not fully specified. We relate this approach both to the classical tradition of formal linguistic semantics exemplified by Davidson (1972) and Montague (1974b) and more recent literature on the use of underspecification in semantics. The structure of the paper is as follows: In the next section we give an outline of the formalism and illustrate with the small fragment of English that has been implemented within this framework. We present analyses of the contextual interpretation of pronouns, definites, ellipsis, focus, and quantifier scope. There is far more to say about each of these phenomena, of course, and the analyses here are by no means claimed to be definitive. The aim is merely to show that we can, to a first approximat"
J00-4002,C96-1073,0,0.0218799,"Rest(~=~)=&gt;~(Ay.Pred(y, he)) ~ Rest(~)=~(Ay.Pred(y,y)) if binding_conditions_hold . . . . This equivalence is doing essentially the same job as Pereira&apos;s p r o n o u n abstraction schema in Pereira (1990). It will identify a p r o n o u n with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations. 3 For this case, and for many other types of restrictions currently handled by conditions, more elegant solutions are available using the &quot;sorted&quot; and &quot;colored&quot; versions of higher-order unification developed by Michael Kohlhase and colleagues (Gardent and Kohlhase 1996b, 1997; Gardent Kohlhase, and van Leusen 1996; Gardent, Kohlhase, and Konrad 1999). 503 Computational Linguistics Volume 26, Number 4 We illustrate this equivalence with the relevant instantiations for the following cases (in fact, the reflexive case is done with a separate equivalence differing only in that it mentions he-self instead of he, with associated differences in binding conditions): (3) Smith admires himself. QLF=exists1(Ae.pos(pres(like(e,smith,he-self)))) Rest= AQ.Q(smith) Pred=.Xx.Ay.exists1(Ae.pos(pres(like,e,x,y))) RLF=exists1(Ae.pos(pres(like(e,smith,smith)))) (4) Smith likes"
J00-4002,P96-1001,0,0.019988,"Rest(~=~)=&gt;~(Ay.Pred(y, he)) ~ Rest(~)=~(Ay.Pred(y,y)) if binding_conditions_hold . . . . This equivalence is doing essentially the same job as Pereira&apos;s p r o n o u n abstraction schema in Pereira (1990). It will identify a p r o n o u n with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations. 3 For this case, and for many other types of restrictions currently handled by conditions, more elegant solutions are available using the &quot;sorted&quot; and &quot;colored&quot; versions of higher-order unification developed by Michael Kohlhase and colleagues (Gardent and Kohlhase 1996b, 1997; Gardent Kohlhase, and van Leusen 1996; Gardent, Kohlhase, and Konrad 1999). 503 Computational Linguistics Volume 26, Number 4 We illustrate this equivalence with the relevant instantiations for the following cases (in fact, the reflexive case is done with a separate equivalence differing only in that it mentions he-self instead of he, with associated differences in binding conditions): (3) Smith admires himself. QLF=exists1(Ae.pos(pres(like(e,smith,he-self)))) Rest= AQ.Q(smith) Pred=.Xx.Ay.exists1(Ae.pos(pres(like,e,x,y))) RLF=exists1(Ae.pos(pres(like(e,smith,smith)))) (4) Smith likes"
J00-4002,P97-1051,0,0.0260071,"ously suggest candidates for parallel elements and the value of the Predicate that corresponds to the Ellipsis variable in the description of DSP above. Some sortal conditions within both the equivalence and the predicate parallel are necessary to make sure that the variables PolarityTenseEtc are appropriately instantiated, since there are many possibilities consistent with their type requirements. But no extralogical mechanisms are needed, apart from the definition of &quot;parallel,&quot; which is intended to correspond to the notion discussed in Dalrymple, Shieber, and Pereira (1991), Prfist (1992), Hobbs and Kehler (1997), and Gardent and Kohlhase (1997), among others. Parallelism may involve syntactic, semantic, pragmatic, and discourse components, depending on the construction involved. VP deletion, for example, requires the parallel element to be the subject of the preceding conjunct as well as being in the same domain of quantification as the remnant; whereas phrasal ellipsis like ... and John merely requires the antecedent to be in the same domain of quantification. Now given a sequence like: (15) Smith liked Sandy. Jones didn&apos;t. where the antecedent logical form and the QLF to be resolved are respectivel"
J00-4002,J87-1005,0,0.0737488,"een below, we need in any case something like the pair quantifier notation of Dalrymple, Shieber, and Pereira (1991), which would also solve this problem. With this addition we are able to produce both scopings for examples like: (20) Every manager in some company disappeared. This is a rather oversimplified treatment of quantifier scope, which we will refine a little shortly, but even as it stands the treatment has several advantages: • (21) in classic examples like: Every representative in a company saw most samples. only the available five relative scopings of the quantifiers are produced (Hobbs and Shieber 1987, 47), but without the need for a free variable constraint--the HOU algorithm will not produce any solutions in which a previously bound variable becomes free; • the equivalences are reversible, and thus the above sentences can be generated from scoped logical forms; • partial scopings are permitted (see Reyle [1996]) • scoping can be freely interleaved with other types of reference resolution; • unscoped or partially scoped forms are available for inference or for generation at every stage. 3.6 Comparison with Deductive Interpretation It is interesting to compare this analysis with that descr"
J00-4002,C94-1090,0,0.0169608,"take place to relate the RQLF for the interpreted sentence to a QLF that unambiguously expresses its contextualized meaning. This makes the task of expressing the output of some application system in a context-dependent way quite difficult: rather than being related to an RQLF, this output has to be related to a QLF that is sufficiently instantiated for a contextually unambiguous sentence to be generated from it. The resolution mechanism is not intended to be reversible, although by redefining resolution rules, reversibility is achievable to some extent within the limitations just discussed (Hurst 1994). A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt: it is that taken by Alshawi and Crouch (1992). This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume. Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only"
J00-4002,C90-3034,0,0.0170981,"such that either of the latter t w o relata can be incompletely specified w i t h o u t p r e v e n t i n g interpretation of the former. A n y analysis of focus should be able to capture this p h e n o m e n o n . The vital ingredient of the analysis here is the nondirectionality of inference p r o v i d e d b y higher-order unification, s u p p l e m e n t e d b y abduction. 3.5 Quantifier Scope We can i m p l e m e n t a d e d u c t i v e theory of quantifier scope using the conditional equivalence m e c h a n i s m . The version p r o p o s e d here c o m b i n e s a basic insight f r o m Lewin (1990) w i t h higher-order unification to give an analysis that has a strong resemblance to that p r o p o s e d in Pereira (1990, 1991), w i t h s o m e differences that are c o m m e n t e d on below. Like Pereira&apos;s a p p r o a c h , it avoids the n e e d for a free variable constraint, n o r does it n e e d the explicit recursion on the quantifier restriction i m p o s e d b y Lewin. We analyze quantified NPs at the QLF level as illustrated in the QLF for: (19) E v e r y m a n a g e r uses a computer. existsl(&e.pos(pres(use(e,every(.&gt;t,&gt;~(manager),ac.,t),.(com puter)))) We a s s u m e that e v"
J00-4002,P86-1037,0,0.143029,"nterpretation of variables is as for Prolog. Later, we extend the notion of inference involved in checking conditions beyond that provided by Prolog and the like to allow conditions to be &quot;abduced&quot; and added to the context if they cannot be proved directly, always provided that adding them to the context does not cause a contradiction. We assume some &quot;cost&quot; mechanism constrains this process. . Equivalences describe QLF or RLF patterns, typically containing variables. Determining whether an equivalence applies to a QLF or an RLF is done by higher-order unification (henceforth, HOU) (Huet 1975; Miller and Nadathur 1986; Pulman 1991; Dalrymple, Shieber, and Pereira 1991; Gawron 1992) of the logical form with the relevant pattern. Many of the contextual conditions require a higher-order equation to be solved. . The interpretation of a QLF is given via the RLFs it can be equivalent to with respect to given contexts. Given a fixed, fully-specified context, a QLF will generally be equivalent to a single RLF (unless the equivalences allow for several synonymous interpretations). In cases where the context does not resolve an ambiguity, the QLF will correspond to different RLFs depending on which assumptions are a"
J00-4002,J90-1001,0,0.466757,"on admissible solutions, as we have already been doing implicitly) Of course, this analysis of p r o n o u n reference will cover only the simplest possible cases of intersentential anaphora. Before going on to more complex cases, w e will also show h o w to deal with intrasentential anaphora, including reflexives, and binding of a p r o n o u n b y a quantifier. The relevant equivalence is: Pron-he-intra Rest(~=~)=&gt;~(Ay.Pred(y, he)) ~ Rest(~)=~(Ay.Pred(y,y)) if binding_conditions_hold . . . . This equivalence is doing essentially the same job as Pereira&apos;s p r o n o u n abstraction schema in Pereira (1990). It will identify a p r o n o u n with any term of type e elsewhere in the QLF, relying on the binding conditions to prevent impossible associations. 3 For this case, and for many other types of restrictions currently handled by conditions, more elegant solutions are available using the &quot;sorted&quot; and &quot;colored&quot; versions of higher-order unification developed by Michael Kohlhase and colleagues (Gardent and Kohlhase 1996b, 1997; Gardent Kohlhase, and van Leusen 1996; Gardent, Kohlhase, and Konrad 1999). 503 Computational Linguistics Volume 26, Number 4 We illustrate this equivalence with the relev"
J00-4002,E91-1002,0,0.19134,"is as for Prolog. Later, we extend the notion of inference involved in checking conditions beyond that provided by Prolog and the like to allow conditions to be &quot;abduced&quot; and added to the context if they cannot be proved directly, always provided that adding them to the context does not cause a contradiction. We assume some &quot;cost&quot; mechanism constrains this process. . Equivalences describe QLF or RLF patterns, typically containing variables. Determining whether an equivalence applies to a QLF or an RLF is done by higher-order unification (henceforth, HOU) (Huet 1975; Miller and Nadathur 1986; Pulman 1991; Dalrymple, Shieber, and Pereira 1991; Gawron 1992) of the logical form with the relevant pattern. Many of the contextual conditions require a higher-order equation to be solved. . The interpretation of a QLF is given via the RLFs it can be equivalent to with respect to given contexts. Given a fixed, fully-specified context, a QLF will generally be equivalent to a single RLF (unless the equivalences allow for several synonymous interpretations). In cases where the context does not resolve an ambiguity, the QLF will correspond to different RLFs depending on which assumptions are added to the c"
J00-4002,J96-3001,1,0.924669,"thing other than a trivial grammar, a given sentence will typically yield many QLFs. We will assume that syntactic and lexical disambiguation have taken place and that the only things still needed for a complete interpretation are the resolution of constructs like pronouns, definites, ellipsis, and so on. We return later to issues concerning robustness of linguistic coverage and to the interleaving of contextual disambiguation with syntactic and semantic processing. For concreteness, we are assuming here that QLFs are built using a simple unification grammar formalism of the type described in Pulman (1996), and that a chart parser and semantic head-driven generator are Pulman Bidirectional Contextual Resolution used for the analysis of sentences to QLFs and vice versa. But little of this detail is essential to our main aims: a wide range of grammatical formalisms and interpreters would be compatible with the basic assumptions of the contextual interpretation mechanism, assuming only that the same grammatical description is used in both the analysis and generation direction. What is required is that QLFs are, as here, expressed in a typed higher-order logic, augmented with constructs representin"
J00-4002,A92-1001,0,0.101752,"Missing"
J00-4002,E95-1001,0,0.0200127,"find complete grammatical analyses for every sentence, let alone full contextual interpretations, information extraction proceeds by reasoning from partial or underspecified representations that are in most logical respects the same kind of animal as the unresolved QLFs we have been talking about. Information extraction systems typically carry out such reasoning in a way that is, in Jerry Hobbs&apos; phrase, unhindered by theory. Developing a calculus for reasoning with QLFs is too large a task to be undertaken here. But the general outlines are reasonably clear, and we can adapt some of the UDRS (Reyle 1995) work to our own framework. Reyle points out that many of the inferences involving underspecified representations that we would like to capture rely on the assumption that whatever context disambiguates the premise also disambiguates the conclusion, even if we do not know what that context or disambiguation is. His example is: If the students get £10 then they buy books. The students get £10. They buy books. Our treatment of the interpretation of QLFs makes it a tautology that if one resolved form implies another, then the corresponding QLFs also do, given a fixed context. The other common pat"
J00-4002,P97-1052,0,\N,Missing
J07-1008,J88-2003,0,0.222816,"associated with their typical durations, not in the lexicon, but as part of our knowledge of the world. Egg represents durations as sets of “temporal granularities” such as {minutes}, {hours}, and {0.1 sec}. Mismatch of durations corresponds to empty intersections of these granularities. Egg develops enough of a mechanism for it to be clear how this information could be integrated into his previous account of reinterpretation. Egg also argues that the availability of an account of temporal duration eliminates the need for a separate aspectual category of punctuality, or the “point” class that Moens and Steedman (1988) added to Vendler’s original taxonomy. If we can account for the coercion in Joe sneezed for five minutes simply in terms of duration mismatch, we do not need to talk about coercion of a point category into something else. I am less convinced of this in the general case, however. Several accounts of multiple aspectual coercion, such as that exemplified in Moens and Steedman’s splendid example It took me two days to play the “Minute Waltz” in less than sixty seconds for more than an hour require a complex eventuality “play the Minute Waltz in less than sixty seconds” to be coerced to a point, a"
J87-3008,E87-1003,1,0.853849,"Missing"
J87-3008,C73-2019,0,0.819767,"Missing"
J87-3008,C86-1066,0,\N,Missing
J96-3001,C86-1071,0,0.0366816,"more) are frequently used in semiformal linguistic descriptions. In a context-free-based formalism they must actually be interpreted as a notation for a rule schema, rather than as part of the formalism itself: something like A -> B C* D is a shorthand for the infinite set of rules: A -> B D, A -> B C D, A -> B C C D, etc. While not essentially changing the weak generative capacity of a CFG, the use of Kleene operators does change the set of trees that can be assigned to sentences: N-ary branching trees can be generated directly. In some unification-based formalisms (e.g. Briscoe et al. 1987; Arnold et al. 1986) Kleene operators have been included. However, in the context of a typed unification formalism like ours, the exact interpretation of Kleene operators is not completely straightforward. Some examples will illustrate the problem. In a formalism like that in Arnold et al. (1986), grammarians write rules like the following, with the intent of capturing the fact that an Nbar can be preceded by an indefinite number of Adjective Phrases provided that (in French, for example) they agree in gender, etc., with the Nbar: iap:{agr=A} ==> [ .... adjp :{agr=A}*, nbar :{agr=A}] This is presumably intended t"
J96-3001,H91-1033,0,0.0264398,"Missing"
J96-3001,P92-1026,0,0.0537136,"Missing"
J96-3001,J89-4001,0,0.0283392,"ivalent to, or based on, unification grammars of the type exemplified by PATR (Shieber 1984) are very widely used in computational linguistics (Alshawi 1992; van Noord et al. 1990; Briscoe et al. 1987; Bobrow, Ingria, and Stallard 1991, etc.) A unification-based formalism has many well-known virtues: it is declarative, monotonic, reversible (in principle at least); it has a well-understood formal interpretation (Shieber 1986, Smolka 1992, Johnson 1988); and there exist well-understood and relatively efficient parsing and generation algorithms for grammars using such a formalism (Shieber 1988; Haas 1989; Alshawi 1992; Shieber et al. 1990, inter alia). However, a pure unification formalism is often thought to be a somewhat restricted grammatical formalism, especially when compared with the rich devices advocated by many grammarians. The recent literature (Pollard and Sag 1987, 1993, etc.) uses many devices that go beyond pure unification: set valued features; negation and disjunction; tests for membership; list operations like ""append"" and ""reverse""; multiple inheritance hierarchies; as well as Kleene operators like * or +1, which are familiar from linguistics textbooks although they have no"
J96-3001,C86-1016,0,0.025806,"s of LFG parsers that I am aware of, such checks are built into the parsing algorithm. However, it should instead be possible to achieve the same effect by compiling the unification part of an LFG grammar in such a way that completeness and coherence are checked via unifiability of two features: one going up, saying what a verb is looking for by way of arguments, and one coming down, saying what has actually been found. 6.1 Implementation The easiest way to implement this use of threading is by defining and using macros such as those given earlier for illustration. Some implementations (e.g., Karttunen 1986) build threading into the grammar compiler directly, but this can lead to inefficiency if features are threaded where they are never used. 7. Threading and Linear Precedence Threading can also be used as an efficient way of encoding linear precedence constraints. This has been most recently illustrated within the HPSG formalism by Engelkamp, Erbach, and Uskoreit (1992). Given a set of some partial ordering constraints and a domain within which they are to be enforced, the implementation of this as threading proceeds as follows. Firstly, given a set of constraints of the form a < c, b < d, etc."
J96-3001,J88-1004,0,0.165812,"ll be described satisfactorily in this manner. 4. Boolean Combinations of Feature Values Our formalism does not so far include Boolean combinations of feature values. The full range of such combinations, as is well known, can lead to very bad time and space behavior in processing. Ramsay (1990) shows how some instances of disjunction can be avoided, but there are nevertheless many occasions on which the natural analysis of some phenomenon is in terms of Boolean combinations of values. One extremely useful technique, although restricted to Boolean combinations of atomic values, is described by Mellish (1988). He gives an encoding of Boolean combinations of feature values (originally attributed to Colmerauer) in such a w a y that satisfiability is checked via unification. This technique is used in several systems (e.g. Alshawi 1992; the European Community's ALEP (Advanced Linguistic Engineering Platform) system; Alshawi et al. 1991). We describe it again here because we will need to know how it works in detail later on. 302 Pulman Unification Encodings Given a feature with values in some set of atoms, or p r o d u c t of sets of atoms, any Boolean combination of these can be represented by a term."
J96-3001,J81-4003,0,0.0448636,"in many cases using the Boolean combination would be a linguistically inaccurate solution. Having a definition like that just given implies that it is just an accident that there are no massaplur NPs, since they are a linguistically valid combination of features, according to the declaration. In this case, and similar ones, the description in terms of type inheritance would be regarded as capturing the facts in a more natural and linguistically motivated way. 6. Threading and Defaults The technique of gap threading is by now well known in the unification grammar literature. It originates with Pereira (1981) and has been used to implement wh-movement and other unbounded dependencies in several large grammars of English (Bobrow, Ingria, and Stallard 1991; Pulman 1992). The purpose of this section is to point to another use of the threading technique, which is to implement a rather simple, but very useful, notion of default: a notion that is, however, completely monotonic! Consider the following problem as an illustration. In an analysis of the English passive, we might want to treat the semantics in something like the following way: Joe was s e e n = exists(e,see(e,something,joe)) Joe was seen by"
J96-3001,J90-3004,0,0.318864,". This unification extends the number of daughters that rule 1 is looking for. Rule 3 terminates the recursion. The feature f l a t c o n j stops spurious nestings, if they are not wanted. In English, at least, this type of conjunction is the only construction for which a Kleene analysis is convincing, and they can all be described satisfactorily in this manner. 4. Boolean Combinations of Feature Values Our formalism does not so far include Boolean combinations of feature values. The full range of such combinations, as is well known, can lead to very bad time and space behavior in processing. Ramsay (1990) shows how some instances of disjunction can be avoided, but there are nevertheless many occasions on which the natural analysis of some phenomenon is in terms of Boolean combinations of values. One extremely useful technique, although restricted to Boolean combinations of atomic values, is described by Mellish (1988). He gives an encoding of Boolean combinations of feature values (originally attributed to Colmerauer) in such a w a y that satisfiability is checked via unification. This technique is used in several systems (e.g. Alshawi 1992; the European Community's ALEP (Advanced Linguistic E"
J96-3001,P84-1075,0,0.0440124,"idge Computer Laboratory This paper describes various techniques for enriching unification-based grammatical formalisms with notational devices that are compiled into categories and rules of a standard unification grammar. This enablesgrammarians to avail themselves of apparently richer notations that allow for the succinct and relatively elegant expression of grammatical facts, while still allowing for efficient processingfor the analysis or synthesis of sentences using such grammars. 1. Introduction Formalisms equivalent to, or based on, unification grammars of the type exemplified by PATR (Shieber 1984) are very widely used in computational linguistics (Alshawi 1992; van Noord et al. 1990; Briscoe et al. 1987; Bobrow, Ingria, and Stallard 1991, etc.) A unification-based formalism has many well-known virtues: it is declarative, monotonic, reversible (in principle at least); it has a well-understood formal interpretation (Shieber 1986, Smolka 1992, Johnson 1988); and there exist well-understood and relatively efficient parsing and generation algorithms for grammars using such a formalism (Shieber 1988; Haas 1989; Alshawi 1992; Shieber et al. 1990, inter alia). However, a pure unification forma"
J96-3001,C88-2128,0,0.0397855,"Formalisms equivalent to, or based on, unification grammars of the type exemplified by PATR (Shieber 1984) are very widely used in computational linguistics (Alshawi 1992; van Noord et al. 1990; Briscoe et al. 1987; Bobrow, Ingria, and Stallard 1991, etc.) A unification-based formalism has many well-known virtues: it is declarative, monotonic, reversible (in principle at least); it has a well-understood formal interpretation (Shieber 1986, Smolka 1992, Johnson 1988); and there exist well-understood and relatively efficient parsing and generation algorithms for grammars using such a formalism (Shieber 1988; Haas 1989; Alshawi 1992; Shieber et al. 1990, inter alia). However, a pure unification formalism is often thought to be a somewhat restricted grammatical formalism, especially when compared with the rich devices advocated by many grammarians. The recent literature (Pollard and Sag 1987, 1993, etc.) uses many devices that go beyond pure unification: set valued features; negation and disjunction; tests for membership; list operations like ""append"" and ""reverse""; multiple inheritance hierarchies; as well as Kleene operators like * or +1, which are familiar from linguistics textbooks although th"
P08-2028,E06-1027,0,0.334857,"appa scores of 3 Kilgarriff, A. (1995). BNC database and word frequency lists. www.kilgarriff.co.uk/bnc-readme.html 111 .40 (ALL-POL) and .74 (NON-NTR), or .48 (ALLPOL) and .83 (NON-NTR) without UNSURE cases. We used ANN-1’s data to adjust the Φntr coefficients of individual classifiers, and evaluated the system against both ANN-2 and ANN-3. The average scores between ANN-2 and ANN-3 are given in Table 1. Since even human polarity judgements become fuzzier near the neutral/non-neutral boundary due to differing personal degrees of sensitivity towards neutrality (cf. low (N) agreement in Ex. 2; Andreevskaia and Bergler (2006)), not all classification errors are equal for classifying a (+) case as (N) is more tolerable than classifying it as (-), for example. We therefore found it useful to characterise three distinct disagreement classes between human H and machine M encompassing FATAL (H (+) M(-) or H(-) M(+) ), GREEDY (H(N) M(-) or H(N) M(+) ), and LAZY (H(+) M(N) or H(-) M(N) ) cases. The classifiers generally mimic human judgements in that accuracy is much lower in the threeway classification task - a pattern concurring with past observations (cf. Esuli and Sebastiani (2006); Andreevskaia and Bergler (2006))."
P08-2028,E06-1025,0,0.0291173,"Missing"
P08-2028,P97-1023,0,0.0159138,"cy, and activity dimensions. Takamura et al. (2005) apply to words’ polarities a physical spin model inspired by the behaviour of electrons with a (+) or (-) direction, and an iterative termneighbourhood matrix which models magnetisation. Non-neutral adjectives were extracted from WordNet and assigned fuzzy sentiment category membership/centrality scores and tags in Andreevskaia and Bergler (2006). Corpus-based Methods. Lexicographic methods are necessarily confined within the underlying resources. Much greater coverage can be had with syntactic or co-occurrence patterns across large corpora. Hatzivassiloglou and McKeown (1997) clustered adjectives into (+) and (-) sets based on conjunction constructions, weighted similarity graphs, minimum-cuts, supervised learning, and clustering. A popular, more general unsupervised method was introduced in Turney and Littman (2003) which induces the polarity of a word from its Pointwise Mutual Information (PMI) or Latent Semantic Analysis (LSA) scores obtained from a web search engine against a few paradigmatic (+) and (-) seeds. Kaji and Kitsuregawa (2007) describe a method for harvesting sentiment words from non-neutral sentences extracted from Japanese web documents based on"
P08-2028,D07-1115,0,0.0128821,"derlying resources. Much greater coverage can be had with syntactic or co-occurrence patterns across large corpora. Hatzivassiloglou and McKeown (1997) clustered adjectives into (+) and (-) sets based on conjunction constructions, weighted similarity graphs, minimum-cuts, supervised learning, and clustering. A popular, more general unsupervised method was introduced in Turney and Littman (2003) which induces the polarity of a word from its Pointwise Mutual Information (PMI) or Latent Semantic Analysis (LSA) scores obtained from a web search engine against a few paradigmatic (+) and (-) seeds. Kaji and Kitsuregawa (2007) describe a method for harvesting sentiment words from non-neutral sentences extracted from Japanese web documents based on structural layout clues. Strong adjectival subjectivity clues were mined in Wiebe (2000) with a distributional similarity-based word clustering method seeded by hand-labelled annotation. 112 Riloff et al. (2003) mined subjective nouns from unannotated texts with two bootstrapping algorithms that exploit lexico-syntactic extraction patterns and manually-selected subjective seeds. 5 Conclusion In this study of unknown words in the domain of sentiment analysis, we presented"
P08-2028,W03-0404,0,0.0652228,"d from WordNet and assigned fuzzy sentiment category membership/centrality scores and tags in Andreevskaia and Bergler (2006). Corpus-based Methods. Lexicographic methods are necessarily confined within the underlying resources. Much greater coverage can be had with syntactic or co-occurrence patterns across large corpora. Hatzivassiloglou and McKeown (1997) clustered adjectives into (+) and (-) sets based on conjunction constructions, weighted similarity graphs, minimum-cuts, supervised learning, and clustering. A popular, more general unsupervised method was introduced in Turney and Littman (2003) which induces the polarity of a word from its Pointwise Mutual Information (PMI) or Latent Semantic Analysis (LSA) scores obtained from a web search engine against a few paradigmatic (+) and (-) seeds. Kaji and Kitsuregawa (2007) describe a method for harvesting sentiment words from non-neutral sentences extracted from Japanese web documents based on structural layout clues. Strong adjectival subjectivity clues were mined in Wiebe (2000) with a distributional similarity-based word clustering method seeded by hand-labelled annotation. 112 Riloff et al. (2003) mined subjective nouns from unanno"
P08-2028,kamps-etal-2004-using,0,\N,Missing
P08-2028,P05-1017,0,\N,Missing
R09-1048,C04-1018,0,0.0132299,"ions, and polarity conflicts. A relaxation labelling technique was used in [15] to classify product feature mentions by sequential analyses of words, features, and sentences with syntactic dependency, lexical, and collocational constraints. [10] extract opinions with fixed opinion frames which capture for a given entity an attribute and a sentiment expression with its HOLDER. 5.3 Sentiment Roles. The inventory of possible semantic roles specific to sentiment is unclear. Past proposals have targeted some of the most obvious roles encompassing opinion HOLDERs, SOURCEs, TARGETs, or EXPERIENCERs. [1] model the information filtering structures of opinions and facts with a supervised approach to identify the hierarchical structure of perspective and speech expressions using syntactic dominance features, and to recursively determine local and global parent-child relations amongst such exData set Scoring GS SNTC HUMAN COMPOS DIST SVM DIST+SVM GS PHR HUMAN COMPOS DIST SVM DIST+SVM ALL POL Acc k Table 5: Multi-entity scoring results NON NTR Acc k 62.07 52.20 56.44 50.04 54.12 .45 .28 .35 .28 .33 89.03 71.71 79.32 79.49 82.21 .78 .45 .59 .58 .64 61.63 48.70 51.42 52.74 52.92 .43 .24 .27 .25 .27"
R09-1048,W06-1651,0,0.0219307,"determine local and global parent-child relations amongst such exData set Scoring GS SNTC HUMAN COMPOS DIST SVM DIST+SVM GS PHR HUMAN COMPOS DIST SVM DIST+SVM ALL POL Acc k Table 5: Multi-entity scoring results NON NTR Acc k 62.07 52.20 56.44 50.04 54.12 .45 .28 .35 .28 .33 89.03 71.71 79.32 79.49 82.21 .78 .45 .59 .58 .64 61.63 48.70 51.42 52.74 52.92 .43 .24 .27 .25 .27 85.81 65.56 68.73 77.70 73.60 .70 .34 .40 .52 .48 56.44 50.04 54.12 51.42 52.74 52.92 pressions. However, only SOURCEs were targeted. A global Integer Linear Programming-driven constraintbased inference approach was used in [2] for joint extraction of sentiment expressions, SOURCEs, and their link relations using sequence tagging and relation classifiers with lexical, positional, and syntactic frame features. [7] extract HOLDERs and TOPICs using opinion verbs and adjectives, and FrameNet-driven semantic frame role labelling. In detecting HOLDERs, Maximum Entropy modelling with syntactic dependency features between sentiment expressions and candidate entities was used in [8]. [16], who highlight the insufficiency of automatic semantic role labelling in resolving SOURCEs and TARGETs, discuss the complexity involved in"
R09-1048,D08-1083,0,0.224709,"om mentions of people or organisations to concrete or even abstract objects, condition what a text is ultimately about. Besides the intrinsic value of entity scoring, the success of document- and sentence-level analysis is also decided by how accurately entities in them can be modelled. Deep entity analysis unfortunately presents the most difficult challenges, be they linguistic or computational. One of the most recent developments in the area - compositional semantics has shown potential for sentence- and expression-level analysis in both logic-oriented [11],[9] and machine learning-oriented [3] paradigms. Our goal in this paper is to further that avenue by extending it to entity-level sentiment analysis. Entity-level approaches have so far involved relatively shallow methods which usually presuppose some pre-given topic or entity of relevance to be classified or scored (§5.3). Other proposals have attempted specific semantic sentiment roles such as evident sentiment HOLDERs, SOURCEs, TARGETs, or EXPERIENCERs (§5.2). What characterises these approaches is that only a few specific entities in text are analysed while all others are left unanalysed. While shallow approaches can capture"
R09-1048,W06-0301,0,0.0794095,"sentiment roles or product features) are usually included per text region. In contrast, we wish to evaluate all entity markers in a given text region. To achieve that, a new multientity data set was compiled from a cross-genre pool of 24 documents’ dependency parses. Five annotators (three paid linguistics students, one of the authors, one volunteer) annotated 7904 entity markers as POS, NTR, or NEG (cf. Ex. 1). Cases displaying mixed sentiment or those infected with inescapable ambiguity were marked as ambiguous. In order to preclude misaligned annotations between annotators (cf. [20]: 3442; [7]: 6), we made a decision to confine ourselves to base nouns only (cf. Ex. 1). The data set contains two subsections. The first (GS PHR) contains 4765 entities from 1500 syntactic constituent phrases of differing lengths (from six documents) while the second (GS SNTC) encompasses 3139 entities from 500 full sentences (from 18 documents). Both subsets were further split into 4/5 training and 1/5 testing sections, yielding for training 2490 entities (GS SNTC) vs. 3877 entities (GS PHR) (§3.2). 649 and 888 entities are given for testing, respectively. For syntactic scoring, the SVM classifier comm"
R09-1048,N06-1026,0,0.0695351,"52.92 pressions. However, only SOURCEs were targeted. A global Integer Linear Programming-driven constraintbased inference approach was used in [2] for joint extraction of sentiment expressions, SOURCEs, and their link relations using sequence tagging and relation classifiers with lexical, positional, and syntactic frame features. [7] extract HOLDERs and TOPICs using opinion verbs and adjectives, and FrameNet-driven semantic frame role labelling. In detecting HOLDERs, Maximum Entropy modelling with syntactic dependency features between sentiment expressions and candidate entities was used in [8]. [16], who highlight the insufficiency of automatic semantic role labelling in resolving SOURCEs and TARGETs, discuss the complexity involved in the task ranging from attribution, multiple SOURCEs and TARGETs, semantic scope, referents, discourse structure, inference, and TARGET relations, amongst others. The interrelation between sentiment roles and discourse structures is discussed further in [18] who propose transitive opinion frames for linking TOPICs. The role of co-reference resolution is discussed in [19] alongside a TOPIC annotation scheme that links opinions based on topical co-refer"
R09-1048,D07-1114,0,0.00616817,"ntions based on neighbouring adjectives and sentential polarity frequencies. [4] propose a more complex approach targeting products’ parts and attributes with a holistic lexicon- and distance-based method that exploits local and global clause-, sentence-, and review-level evidence and patterns in disambiguating ambiguous words, irregular/idiomatic constructions, and polarity conflicts. A relaxation labelling technique was used in [15] to classify product feature mentions by sequential analyses of words, features, and sentences with syntactic dependency, lexical, and collocational constraints. [10] extract opinions with fixed opinion frames which capture for a given entity an attribute and a sentiment expression with its HOLDER. 5.3 Sentiment Roles. The inventory of possible semantic roles specific to sentiment is unclear. Past proposals have targeted some of the most obvious roles encompassing opinion HOLDERs, SOURCEs, TARGETs, or EXPERIENCERs. [1] model the information filtering structures of opinions and facts with a supervised approach to identify the hierarchical structure of perspective and speech expressions using syntactic dominance features, and to recursively determine local a"
R09-1048,H05-1043,0,0.0135545,"ked with proximity-based, heuristic, and supervised learning-based scorers. The product feature mining and summarisation system described in [5] classifies feature mentions based on neighbouring adjectives and sentential polarity frequencies. [4] propose a more complex approach targeting products’ parts and attributes with a holistic lexicon- and distance-based method that exploits local and global clause-, sentence-, and review-level evidence and patterns in disambiguating ambiguous words, irregular/idiomatic constructions, and polarity conflicts. A relaxation labelling technique was used in [15] to classify product feature mentions by sequential analyses of words, features, and sentences with syntactic dependency, lexical, and collocational constraints. [10] extract opinions with fixed opinion frames which capture for a given entity an attribute and a sentiment expression with its HOLDER. 5.3 Sentiment Roles. The inventory of possible semantic roles specific to sentiment is unclear. Past proposals have targeted some of the most obvious roles encompassing opinion HOLDERs, SOURCEs, TARGETs, or EXPERIENCERs. [1] model the information filtering structures of opinions and facts with a sup"
R09-1048,ruppenhofer-etal-2008-finding,0,0.0535642,":20:51 [London]29:18:53 Table 4: Human accuracy and inter-annotator agreement scores on the gold standard k ALL POL GS SNTC (3139) k NON NTR Acc ALL POL Acc NON NTR k ALL POL GS PHR (4765) k NON NTR Acc ALL POL Acc NON NTR Human-1 Human-2 Human-3 Human-4 Human-5 .50 .48 .34 .51 .40 .82 .77 .79 .80 .72 66.82 65.03 52.79 66.90 58.80 90.99 88.67 89.60 89.70 86.21 .49 .49 .33 .47 .36 .74 .71 .72 .66 .69 66.83 66.87 55.09 64.46 54.89 87.90 86.43 86.73 82.88 85.14 Avg .45 .78 62.07 89.03 .43 .70 61.63 85.81 tity markers (and any sentiment roles therein) are linked through a variety of complex means [16][18][6], taking discourse structure, Named Entities, semantic roles, and reported speech into account would be beneficial. Entity markers can be chained through anaphora/co-reference resolution which can lead to significant boosts [6]. The values for the weighting coefficients (§3.3) and the exploratory learning features for syntactic scoring (§3.2) can be optimised, and other scorers may be employed. 5 Related Work 5.1 Compositional Analysis. A few systems that exploit the compositional properties of sentiment in differing degrees have been proposed. The system closest to our framework is [9]"
R09-1048,C08-1101,0,0.0265739,"51 [London]29:18:53 Table 4: Human accuracy and inter-annotator agreement scores on the gold standard k ALL POL GS SNTC (3139) k NON NTR Acc ALL POL Acc NON NTR k ALL POL GS PHR (4765) k NON NTR Acc ALL POL Acc NON NTR Human-1 Human-2 Human-3 Human-4 Human-5 .50 .48 .34 .51 .40 .82 .77 .79 .80 .72 66.82 65.03 52.79 66.90 58.80 90.99 88.67 89.60 89.70 86.21 .49 .49 .33 .47 .36 .74 .71 .72 .66 .69 66.83 66.87 55.09 64.46 54.89 87.90 86.43 86.73 82.88 85.14 Avg .45 .78 62.07 89.03 .43 .70 61.63 85.81 tity markers (and any sentiment roles therein) are linked through a variety of complex means [16][18][6], taking discourse structure, Named Entities, semantic roles, and reported speech into account would be beneficial. Entity markers can be chained through anaphora/co-reference resolution which can lead to significant boosts [6]. The values for the weighting coefficients (§3.3) and the exploratory learning features for syntactic scoring (§3.2) can be optimised, and other scorers may be employed. 5 Related Work 5.1 Compositional Analysis. A few systems that exploit the compositional properties of sentiment in differing degrees have been proposed. The system closest to our framework is [9] who"
R09-1048,H05-2017,0,\N,Missing
R09-1048,stoyanov-cardie-2008-annotating,0,\N,Missing
R13-1036,J98-1004,0,0.22971,"Missing"
R13-1036,W04-3252,0,\N,Missing
S12-1011,D10-1115,0,0.0143673,"tion for disambiguating word meaning. 1 Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference. We propose a probabilistic model of the semantic representations for nouns and modifiers. The foundation of this model is a latent variable representation of noun and adjective semantics together with their comp"
S12-1011,W03-1022,0,0.212358,"|M| Figure 1: Plate diagram illustrating our model of noun and modifier semantic classes (designated N and M , respectively), a modifier-noun pair (m,n), and its context. N Parameterization and Inference Multi(ΨM Ni ) Multi(ΨnNi ) Multi(Ψm Mi ) Multi(ΨcNi ) 71 As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British"
S12-1011,J02-2003,0,0.426222,"erence Multi(ΨM Ni ) Multi(ΨnNi ) Multi(Ψm Mi ) Multi(ΨcNi ) 71 As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are"
S12-1011,P05-1004,0,0.123956,"valuations we test the degree to which adjective-noun relationships can be categorised. We analyse the effect of lexical context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning. 1 Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional prefer"
S12-1011,J10-4007,0,0.0302055,"Missing"
S12-1011,D11-1129,0,0.0350219,"Missing"
S12-1011,N09-1036,0,0.0334129,"re treated as a bag of words and 1 We evaluate this hypothesis as well as its inverse. 70 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 70–74, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2.2 αc Ψc αN αM ΨN ΨM |N| We use Gibbs sampling to estimate the distributions of N and M , integrating out the multinomial parameters Ψx (Griffiths and Steyvers, 2004). The Dirichlet parameters α are drawn independently from a Γ(1, 1) distribution, and are resampled using slice sampling at frequent intervals throughout the sampling process (Johnson and Goldwater, 2009). This “vague” prior encourages sparse draws from the Dirichlet distribution. The number of noun and adjective classes N and M was set to 50 each; other sizes (100,150) did not significantly alter results. |N| c N M n m k |D| Ψn Ψm |N| n α 3 m α include the words to the left and right of the noun, its siblings and governing verbs. We designate the vocabulary Vn for nouns, Vm for modifiers and Vc for context. We use zi to refer to the ith tuple in D and refer to variables within that tuple by subscripting them with i, e.g., ni and c3,i are the noun and the third context variable of zi . The lat"
S12-1011,J03-3005,0,0.414645,"d on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over every 10th iteration. 4 4.1 Evaluation Supersense Tagging Supersense taggin"
S12-1011,P08-1028,0,0.04565,"latent semantic representation for disambiguating word meaning. 1 Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference. We propose a probabilistic model of the semantic representations for nouns and modifiers. The foundation of this model is a latent variable representation of noun and adjective seman"
S12-1011,P10-1045,0,0.0241587,"Missing"
S12-1011,P93-1024,0,0.564443,"ulti(ΨnNi ) Multi(Ψm Mi ) Multi(ΨcNi ) 71 As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over ever"
S12-1011,P99-1014,0,0.0273177,"i ) Multi(ΨcNi ) 71 As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over every 10th iteration. 4 4"
S12-1021,W06-1201,0,0.023401,"s an idiosyncratic element to its meaning. 2 Definition from http://www.thefreedictionary.com We define lexicality as the degree to which idiosyncrasy contributes to a compound’s semantics. Inversely phrased, the compositionality of a compound can be defined as the degree to which its sense is related to the senses of its constituents.3 This graded representation follows Sp¨arck Jones (1985), who argued that “it is not possible to maintain a principled distinction between lexicalised and non-lexicalised compounds”. Some recent work also supports this view (Reddy et al., 2011; Bu et al., 2010; Baldwin, 2006). From a practical perspective, a real-valued representation of compositionality should help improve interpretation of compounds. This is especially true when factoring in the respective semantic contributions of its parts. 3.2 Context Generation According to the distributional hypothesis, the semantics of a lexical item can be expressed by its context. We apply this hypothesis to the problem of noun compound compositionality by using a generative model on compound context. Our model allows context to be generated by the compound itself or by either one of its constituents. By learning which e"
S12-1021,D10-1115,0,0.098579,"e thus able to proliferate infinitely. At the same time, semantic composition can take many different forms, making uniform interpretation of compounds impossible (Zanzotto et al., 2010). Most current work on MWEs focuses on interpreting compounds and sidesteps the task of determining whether a compound is compositional in the first place (Butnariu et al., 2010; Kim and Baldwin, 2008). Such methods, aimed at learning the semantics of compounds, can roughly be divided into two major strands of research. One group relies on data intensive methods to extract semantics vectors from large corpora (Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Giesbrecht, 2009). The focus of these approaches is to develop methods for composing the vectors of unigrams into a semantic vector representing a compound. Some of the work in this area touches on the issue of lexicality, as models learning distributional representations of MWEs ideally would first establish whether a given MWE is compositional or not (Mitchell and Lapata, 2010). The other group are knowledge intensive approaches collecting linguistic features (Kim and Baldwin, 2005; Korkontzelos and Manandhar, 2009). Tratz and Hovy (2010), for instance, train a class"
S12-1021,D07-1090,0,0.00961934,"ffect of parameter tuning decreases on larger data. 137 Working on problems related to non-unigram data, sparsity is a frequently encountered problem. As already explored in the previous section, this is also the case for our generative models of lexicality. It would be possible to use an even larger training corpus, but there are limitations as to what extent this is possible. The BNC, containing 100 million words, is already one of the largest corpora regularly used in Computational Linguistics. However, adding more data in an unsupervised sense is unlikely to significantly improve results (Brants et al., 2007). Alternatively, it would be possible to add specific training data that included the noun compounds from the evaluation data sets. This would, however, compromise the unsupervised nature of our approach, and it thus not an option either. In this paper, we will instead focus on extenuating the effects of data sparsity through other unsupervised means. For this purpose we investigate interpolating on a larger set of noun compounds. Kim and Baldwin (2007) observed that semantic similarity of verb-particle compounds correlates with their lexicality. We extend this observation for noun compounds,"
S12-1021,C10-1014,0,0.0441993,"unds by extrapolating their semantics from observations where the two nouns forming a compound are in an intransitive relationship. For example extracting the phrase ‘the family owns a car’ from the training data would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a classification option, considering it jointly with the actual compound interpretation. Next to the work on MWE interpretation there has been some work focused on determining lexicality in its own right (Reddy et al., 2011; Bu et al., 2010; Kim and Baldwin, 2007). One possibility is to exploit special properties of 133 lexical MWEs such as high statistical association of their constituents (Pedersen, 2011) or syntactic rigidity (Fazly et al., 2009; McCarthy et al., 2007). However, these approaches are limited in their applicability to compound nouns (Reddy et al., 2011). Another method is to compare the semantics of a compound and its constituents to decide compositionality. The approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods. Depending on the chosen representat"
S12-1021,S10-1007,0,0.058076,"Missing"
S12-1021,J09-1005,0,0.031615,"ata would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a classification option, considering it jointly with the actual compound interpretation. Next to the work on MWE interpretation there has been some work focused on determining lexicality in its own right (Reddy et al., 2011; Bu et al., 2010; Kim and Baldwin, 2007). One possibility is to exploit special properties of 133 lexical MWEs such as high statistical association of their constituents (Pedersen, 2011) or syntactic rigidity (Fazly et al., 2009; McCarthy et al., 2007). However, these approaches are limited in their applicability to compound nouns (Reddy et al., 2011). Another method is to compare the semantics of a compound and its constituents to decide compositionality. The approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods. Depending on the chosen representation of semantics these approaches can either be used for supervised classifiers or together with a distance metric comparing vector space representations of semantics. In a binary setting, a threshold would then"
S12-1021,I05-1082,0,0.0346288,"elies on data intensive methods to extract semantics vectors from large corpora (Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Giesbrecht, 2009). The focus of these approaches is to develop methods for composing the vectors of unigrams into a semantic vector representing a compound. Some of the work in this area touches on the issue of lexicality, as models learning distributional representations of MWEs ideally would first establish whether a given MWE is compositional or not (Mitchell and Lapata, 2010). The other group are knowledge intensive approaches collecting linguistic features (Kim and Baldwin, 2005; Korkontzelos and Manandhar, 2009). Tratz and Hovy (2010), for instance, train a classifier for noun compound interpretation on a large set of W ORD N ET and Thesaurus features. Combined approaches include Kim and Baldwin (2008), who interpret noun compounds by extrapolating their semantics from observations where the two nouns forming a compound are in an intransitive relationship. For example extracting the phrase ‘the family owns a car’ from the training data would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised clas"
S12-1021,P09-2017,0,0.551517,"methods to extract semantics vectors from large corpora (Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Giesbrecht, 2009). The focus of these approaches is to develop methods for composing the vectors of unigrams into a semantic vector representing a compound. Some of the work in this area touches on the issue of lexicality, as models learning distributional representations of MWEs ideally would first establish whether a given MWE is compositional or not (Mitchell and Lapata, 2010). The other group are knowledge intensive approaches collecting linguistic features (Kim and Baldwin, 2005; Korkontzelos and Manandhar, 2009). Tratz and Hovy (2010), for instance, train a classifier for noun compound interpretation on a large set of W ORD N ET and Thesaurus features. Combined approaches include Kim and Baldwin (2008), who interpret noun compounds by extrapolating their semantics from observations where the two nouns forming a compound are in an intransitive relationship. For example extracting the phrase ‘the family owns a car’ from the training data would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a cla"
S12-1021,N10-1089,0,0.0473838,"ion of a MWE, it needs to be established whether the expression should be treated as lexical (idiomatic) or as compositional. The final step, learning the semantics of the MWE, strongly depends on this decision. 1 Definition taken from Wikipedia, and clearly not recoverable if one only knows the meaning of the words ‘rat’ and ‘race’. The problem posed by MWEs is considered hard, but at the same time it is highly relevant and interesting. MWEs occur frequently in language and interpreting them correctly would directly improve results in a number of tasks in NLP such as translation and parsing (Korkontzelos and Manandhar, 2010). By extension this makes deciding the lexicality of MWEs an important challenge for various fields including machine translation, question answering and information retrieval. In this paper we discuss compositionality with respect to noun-noun compounds. Most Computational Linguistics literature treats compositionality as a binary problem, classifying compounds as either lexical or compositional. We show that this approach is too simplistic and argue for the real-valued treatment of compositionality. We propose two unsupervised models that learn compositionality rankings for compounds, placin"
S12-1021,P95-1007,0,0.0699465,"ther be used for supervised classifiers or together with a distance metric comparing vector space representations of semantics. In a binary setting, a threshold would then be applied to the result of that distance function (Korkontzelos and Manandhar, 2009). In a real-valued setting the distance metric itself can be used as a measure for compositionality (Reddy et al., 2011). Related to the vector space based models, some research focuses on improving the distance metrics used to compare induced semantics (Bu et al., 2010). 3 Methodology English noun-noun compounds are majority leftbranching (Lauer, 1995), with a head (the second element), modified by an attributive noun (first element). For example: Ground Floor — The floor of a building at or nearest ground level.2 In this paper, we will use the terms attributive noun (AN) and head noun (HN) to refer to the first and second noun in a noun compound. 3.1 Real-Valued Representation Lexicality of MWEs is frequently treated as a bi´ S´eaghdha, nary property (Tratz and Hovy, 2010; O 2007). We argue that lexicality should instead be treated as a graded property, as most compound semantics exhibit a mixture of compositional and lexical influences. F"
S12-1021,D07-1039,0,0.0653848,"that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a classification option, considering it jointly with the actual compound interpretation. Next to the work on MWE interpretation there has been some work focused on determining lexicality in its own right (Reddy et al., 2011; Bu et al., 2010; Kim and Baldwin, 2007). One possibility is to exploit special properties of 133 lexical MWEs such as high statistical association of their constituents (Pedersen, 2011) or syntactic rigidity (Fazly et al., 2009; McCarthy et al., 2007). However, these approaches are limited in their applicability to compound nouns (Reddy et al., 2011). Another method is to compare the semantics of a compound and its constituents to decide compositionality. The approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods. Depending on the chosen representation of semantics these approaches can either be used for supervised classifiers or together with a distance metric comparing vector space representations of semantics. In a binary setting, a threshold would then be applied to the result"
S12-1021,P07-3013,0,0.0362171,"Missing"
S12-1021,W11-1306,0,0.0282471,"family owns a car’ from the training data would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a classification option, considering it jointly with the actual compound interpretation. Next to the work on MWE interpretation there has been some work focused on determining lexicality in its own right (Reddy et al., 2011; Bu et al., 2010; Kim and Baldwin, 2007). One possibility is to exploit special properties of 133 lexical MWEs such as high statistical association of their constituents (Pedersen, 2011) or syntactic rigidity (Fazly et al., 2009; McCarthy et al., 2007). However, these approaches are limited in their applicability to compound nouns (Reddy et al., 2011). Another method is to compare the semantics of a compound and its constituents to decide compositionality. The approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods. Depending on the chosen representation of semantics these approaches can either be used for supervised classifiers or together with a distance metric comparing vector space representations of semantics. In"
S12-1021,I11-1024,0,0.516843,"interpret noun compounds by extrapolating their semantics from observations where the two nouns forming a compound are in an intransitive relationship. For example extracting the phrase ‘the family owns a car’ from the training data would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a classification option, considering it jointly with the actual compound interpretation. Next to the work on MWE interpretation there has been some work focused on determining lexicality in its own right (Reddy et al., 2011; Bu et al., 2010; Kim and Baldwin, 2007). One possibility is to exploit special properties of 133 lexical MWEs such as high statistical association of their constituents (Pedersen, 2011) or syntactic rigidity (Fazly et al., 2009; McCarthy et al., 2007). However, these approaches are limited in their applicability to compound nouns (Reddy et al., 2011). Another method is to compare the semantics of a compound and its constituents to decide compositionality. The approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods. Depending on the c"
S12-1021,P10-1070,0,0.141948,"rs from large corpora (Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Giesbrecht, 2009). The focus of these approaches is to develop methods for composing the vectors of unigrams into a semantic vector representing a compound. Some of the work in this area touches on the issue of lexicality, as models learning distributional representations of MWEs ideally would first establish whether a given MWE is compositional or not (Mitchell and Lapata, 2010). The other group are knowledge intensive approaches collecting linguistic features (Kim and Baldwin, 2005; Korkontzelos and Manandhar, 2009). Tratz and Hovy (2010), for instance, train a classifier for noun compound interpretation on a large set of W ORD N ET and Thesaurus features. Combined approaches include Kim and Baldwin (2008), who interpret noun compounds by extrapolating their semantics from observations where the two nouns forming a compound are in an intransitive relationship. For example extracting the phrase ‘the family owns a car’ from the training data would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a classification option, con"
S12-1021,C10-1142,0,0.124978,"Missing"
S12-1021,W09-2416,0,\N,Missing
W00-0740,C92-4186,0,0.0177383,"tion hypothesised by the move and fill operations as to be translated somehow into the gap threading notation which is also used by their formalism. No details are given of the results of this system, nor any empirical evaluation. This work shares m a n y of the goals of the approach we describe, in particular the use of explicit encoding of background knowledge of feature principles. The main difference is that the technique t h e y describe only hypothesises the context free backbone of the necessary rules, whereas in our approach the feature structures are also hypothesised simultaneously. Asker et al. (1992) also describe a m e t h o d for inducing new lexical entries when extending T h a l m a n n and Samuelsson (1995) describe a scheme which combines robust parsing and rule induction for unification grammars. T h e y use an LR parser, whose states and actions are augmented so as to try to recover from situations that in a standard LR parser would result in an error. The usual actions of shift, reduce, and accept are augmented by h y p o t h e s i s e d shift: shift a new item on to the stack even if no such action is specified in that state h y p o t h e s i s e d u n a r y r e d u c e : reduce"
W00-0740,P89-1013,0,0.113286,"of our learning strategy on the particular types of sentences in the training data. In a second experiment, we deleted the rules: nom_nom_mod syn nom: [mor=A]==> [nom: [mot=A] , rood: [gaps= [ng: [1 ,ng : []] ,of=nora, type=or (n, q) ] ]. 6 Related work The strong connections between proving and parsing axe well known (Shieber et al., 1995), so it is no surprise that we find related methods in both ILP and computational linguistics. In ILP the notion of inducing clauses to fix a failed proof, which is the topic of Section 2, is very old dating from the seminal work of Shapiro (1983). In NLP, Mellish (1989) presents a method for repairing failed parses in a relatively efficient way based on the fact that, after a failed parse, the information in the chart is sufficient for us to be able to determine what constituents would have allowed the parse to go through if they had been found. 6.1 R e l a t e d w o r k in I L P The use of abduction to repair proofs/paxses has been extensively researched in ILP as has the importance of abduction for multiple predicate learning. De Raedt (1992), for example, notes that &quot;Roughly speaking, combining abduction with single predicate-leaxning leads to multiple co"
W00-0740,W99-0708,0,0.0662136,"arse are represented and then examined to find appropriate rules. In CHILL these intermediate stages are states of a shift-reduce parser. 6.2 R e l a t e d w o r k in N L P Most work on grammar induction has taken place using formalisms in which categories are atomic: context-free grammars, categorial grammars, etc. Few attempts have been made at rule induction using a rich unification formalism. Two lines of work that are exceptions to this, and thus comparable to our own, are that of Osborne and colleagues; and the work of the SICS group using SRI&apos;s Core Language Engine and similar systems. Osborne (1999) argues (correctly) that the hypothesis space of grammars is sufficiently large that some form of bias is required. The current paper is concerned with methods for effecting what is known as declarative bias in the machine learning literature, i.e. hard constraints that reduce the size of the hypothesis space. Osborne, on the other hand, uses the Minimum Description Length (MDL) principle to effect a preferential (soft) bias towards smaller grammars. His approach is incremental and the induction of new rules is triggered by an unparsable sentence as follows: 1. Candidate rules are generated wh"
W00-0740,C00-1085,0,0.0275653,"hart after a failed parse to form the daughters of hypothesised rules. The mothers, though, are not found by abduction as in our case, also there is no subsequent generalisation step. Unlike us Osborne induces a probabilistic grammar. W h e n candidate rules are added, probabilities are renormalised and the n most likely parses are found. If annotated d a t a is being used, models that produce parses inconsistent with this d a t a are rejected. In (Osborne, 1999), the DCG is m a p p e d to a SCFG to compute probabilities, in very recent work a stochastic attribute-value g r a m m a r is used (Osborne, 2000). Giving the increasing sophistication of probabilistic linguistic models (for example, Collins (1997) has a statistical approach to learning gap-threading rules) a probabilistic extension of our work is a t t r a c t i v e - - i t will be interesting to see how far an integration of &apos;logical&apos; and statistical can go. After stage 4 we could reduce with 1 but this would not lead to an accepting state. Instead we perform a hypothesised shift at stage 5 followed by a hypothesised binary reduce with X VP Adv in stage 6. Next we reduce with rule 1 which instantiates X to VP and we have a complete pa"
W00-0740,P97-1003,0,\N,Missing
W00-0740,J81-4003,0,\N,Missing
W05-0202,W99-0613,0,0.0372423,"Missing"
W05-0202,W03-0210,0,0.013604,"d instead think of them either as aids to the pattern writing process – for example, frequently the decision trees that are learned are quite intuitive, and suggestive of useful patterns – or perhaps as complementary supporting assessment techniques to give extra confirmation. 6. Other work Several other groups are working on this problem, and we have learned from all of them. Systems which share properties with ours are C-Rater, developed by Leacock et al. (2003) at the Educational Testing Service(ETS), the IE-based system of Mitchell et al. (2003) at Intelligent Assessment Technologies, and Rosé et al. (2003) at Carnegie Mellon University. The four systems are being developed independently, yet it seems they share similar characteristics. Commercial and resource pressures currently make it impossible to try these different systems on the same data, and so performance comparisons are meaningless: this is a real hindrance to progress in this area. The field of automatic marking really needs a MUC-style competition to be able to develop and assess these techniques and systems in a controlled and objective way. 7. Current and Future Work The manually-engineered IE approach requires skill, much labour,"
W06-1662,I05-1055,0,0.245999,"Missing"
W06-1662,W01-1313,0,0.19609,"Missing"
W06-1662,P03-1069,0,\N,Missing
W07-1607,P06-4020,0,0.00656016,"tion. Despite this apparently idiosyncratic behaviour, we believe that prepositional choice is governed by a combination of several syntactic and semantic features. Contexts of occurrence can be represented by vectors; a machine learning algorithm trained on them can predict with some confidence, given a new occurrence of a context vector, whether a certain preposition is appropriate in that context or not. We consider the following macro-categories of features to be relevant: POS being modified; POS of the preposition’s complement; given a RASP-style grammatical relation output (GR; see e.g. Briscoe et al. 2006), what GRs the preposition occurs in; named entity (NE) information - whether the modified or complement items are NEs; WordNet information - to which of the WordNet lexicographer 45 Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 45–50, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics classes1 the modified and complement nouns and verbs belong; immediate context - POS tags of ±2 word window around the preposition. For example, given a sentence such as John drove to Cambridge, we would note that this occurrence of the preposition to modifies"
W07-1607,A00-2019,0,0.138335,"Missing"
W07-1607,J07-4004,0,\N,Missing
W07-1607,izumi-etal-2004-overview,0,\N,Missing
W08-2212,P98-1013,0,0.028901,"is syntactically valid combination. Similarly, an important component of reference resolution is the knowledge of what semantic category an entity falls under. For example, in ‘The crop can be used to produce ethanol. This can be used to power trucks or cars’, knowledge that ethanol is the kind of thing that can be subject of ‘power’, whereas ‘crop’ is not, is required to successfully resolve the reference of ‘this’. When considering division into semantic categories one’s immediate thought would be to take advantage of existing semantic resources (such as WordNet (Miller, 1995)) or FrameNet (Baker et al., 1998). For example, Clark and Weir (2002) calculate the probability of a noun sense appearing as a particular argument by using WordNet to generalise over the noun sense. However, even though WordNet has been extremely useful in numerous applications, many researchers have found that the fact that it is largely developed via the intuitions of lexicographers, rather than being empirically based, means that the semantic information often is poorly matched with word usage in a particular domain. Pantel and Lin (2002) and Phillips and Riloff (2002) have pointed out that WordNet often includes many rare"
W08-2212,C04-1180,0,0.029691,"ng domain specific verb senses (either in terms of verb classes or through the assignment of semantic types to the verb arguments) is applied as a proof of concept to the domain of financial news. We chose this domain since the WSJ section of the Penn Treebank II is already available in the form of predicate-argument structures, obtained according to the method described in Liakata and Pulman (2002). However, the same approach can apply to predicate-arg structures from non-treebank data such as the QLFs derived from LFG structures in Cahill et al. (2003) or semantic representations such as in Bos et al. (2004). The WSJ corpus consists of 2,454 articles with a total of 2,798 distinct verb predicates, 62 prepositional predicates and 221 copular predicates containing the verb to ‘be’. Here we are only dealing with the non-prepositional predicates. The latter follow an uneven distribution of occurrences; there is a minority of very frequent verbs whereas the majority are rather sparse. The problem with infrequent predicates is that the number of instances is often too small to allow for meaningful clustering of the verb-argument slots. To circumvent this, we pre-process predicates with low frequencies"
W08-2212,A97-1052,0,0.0601583,"ber are counted as arguments of the 1 arg1 is subject; arg2 is direct object; arg3 is indirect object, roughly 142 Liakata and Pulman less frequent semantically related predicate, so that the latter receives a count boost. For example, the words that appear as subjects of the verb ‘hit’ are also considered subjects of the verb ‘clobber’, which belongs to the same synset as ‘hit’ but is underrepresented in the corpus. This is making use of the knowledge that semantically similar verbs are similar in terms of subcategorisation (Korhonen and Preiss, 2003) and is in agreement with the approach in Briscoe and Carroll (1997) where the subcategorisation frames (SCFs) of representative verbs are merged together to form SCFs of the rest of the verbs belonging to the same semantic class. We understand that the above process may be indirectly adding false positives to the verb senses. It would be interesting in the future to examine the trade-off between boosting the counts of infrequent verbs and the addition of false positives. A second pre-processing stage was applied to the arguments of the 2,798 verb predicates. The idea underlying this process was to create a version of the predicates where obvious semantic grou"
W08-2212,J07-4004,0,0.0145586,"patterns. 3 Results and Evaluation To evaluate the semantic types assigned by the automatically derived classes as well as the transferability of the derived CPA-like patterns to unseen instances, we performed a pilot study where we applied the patterns to two randomly selected articles from the on-line versions of the WSJ and the FT from March 2008. We believe this to be a useful test for the validity of the patterns since the new articles are guaranteed to be distinct from the training WSJ data of the 90s, while still belonging to the same domain. We parsed the article using the CCG parser (Clark and Curran, 2007) and concentrated on its RASP option (Briscoe et al., 1997) output, consisting of dependency relations. Since our patterns concern the semantic typing of verb arguments, we focussed on the relations ncsubj (non-clausal subject), dobj (direct obj) and iobj (indirect obj) between a verb and the respective argument position. We ignored erroneous parses5 as well as copular predicates with the verb to ‘be’, since the CCG parser’s dependency relations did not maintain the connection between ‘be’ and the adjective or participle, making it clumsy to automatically link arguments in the way we need to."
W08-2212,J02-2003,0,0.131085,". Similarly, an important component of reference resolution is the knowledge of what semantic category an entity falls under. For example, in ‘The crop can be used to produce ethanol. This can be used to power trucks or cars’, knowledge that ethanol is the kind of thing that can be subject of ‘power’, whereas ‘crop’ is not, is required to successfully resolve the reference of ‘this’. When considering division into semantic categories one’s immediate thought would be to take advantage of existing semantic resources (such as WordNet (Miller, 1995)) or FrameNet (Baker et al., 1998). For example, Clark and Weir (2002) calculate the probability of a noun sense appearing as a particular argument by using WordNet to generalise over the noun sense. However, even though WordNet has been extremely useful in numerous applications, many researchers have found that the fact that it is largely developed via the intuitions of lexicographers, rather than being empirically based, means that the semantic information often is poorly matched with word usage in a particular domain. Pantel and Lin (2002) and Phillips and Riloff (2002) have pointed out that WordNet often includes many rare senses while missing out domainspec"
W08-2212,J05-1005,0,0.0301045,"xicosyntactic contexts, are aggregated to form intermediate clusters to which hierarchical clustering is applied for further generalisation. A very interesting aspect of this work is that concept-clusters have a dual nature, consisting both of words-terms (extension) and their lexico-syntactic contexts (intension). As is the case in our approach, cluster formation is twofold, by grouping together words according to the contexts they appear in but also by clustering contexts based on the words they share though this is mentioned as future work in Gamallo et al. (2007). However, in earlier work Gamallo et al. (2005) cluster together similar syntactic positions in Portuguese derived automatically and each cluster represents a semantic condition. Words-fillers of the common position are used to extensionally define the particular condition. Clusters are formed in two stages, where first the similarity between any two positions is calculated in terms of their common word fillers, the 20 most similar ones for each position are aggreggated and the intersection of common words kept as features. Next, basic clusters are agglomerated according to the amount Automatic Fine-Grained Semantic Classification for Doma"
W08-2212,P06-1044,0,0.116905,"clustering nouns to derive semantic classes. Work more directly comparable to ours includes Schulte im Walde (2003, 2006) who presents a method for clustering German verbs by linguistically motivated feature selection. Evaluation against a manually annotated gold standard showed that syntactic subcategorisation features were most informative whereas selectional preferences added noise to the clustering. However, the author concludes that there is no perfect choice of verb features and that some verbs can be distinguished on a coarse feature level while others require fine-grained information. Korhonen et al. (2006) also use syntactically motivated features to cluster together verbs from the biomedical domain and in more recent work (Sun et al., 2008) showed that rich syntactic information about both arguments and adjuncts of verbs constitute the best performing feature set for verb clustering. Gamallo et al. (2007) follow a similar approach to Pantel and Lin (2002) where an initial set of specific clusters, containing manually chosen terms representative of the domain as well as their lexicosyntactic contexts, are aggregated to form intermediate clusters to which hierarchical clustering is applied for f"
W08-2212,P03-1007,0,0.0247603,". Thus, words featuring as arguments of the most frequent synset member are counted as arguments of the 1 arg1 is subject; arg2 is direct object; arg3 is indirect object, roughly 142 Liakata and Pulman less frequent semantically related predicate, so that the latter receives a count boost. For example, the words that appear as subjects of the verb ‘hit’ are also considered subjects of the verb ‘clobber’, which belongs to the same synset as ‘hit’ but is underrepresented in the corpus. This is making use of the knowledge that semantically similar verbs are similar in terms of subcategorisation (Korhonen and Preiss, 2003) and is in agreement with the approach in Briscoe and Carroll (1997) where the subcategorisation frames (SCFs) of representative verbs are merged together to form SCFs of the rest of the verbs belonging to the same semantic class. We understand that the above process may be indirectly adding false positives to the verb senses. It would be interesting in the future to examine the trade-off between boosting the counts of infrequent verbs and the addition of false positives. A second pre-processing stage was applied to the arguments of the 2,798 verb predicates. The idea underlying this process w"
W08-2212,C02-1105,1,0.877323,"used to assign semantic types to arguments of verbs. We were pleasantly surprised to find almost perfect recall, and respectable precision figures. 2 Method The method of clustering together verb argument slots for obtaining domain specific verb senses (either in terms of verb classes or through the assignment of semantic types to the verb arguments) is applied as a proof of concept to the domain of financial news. We chose this domain since the WSJ section of the Penn Treebank II is already available in the form of predicate-argument structures, obtained according to the method described in Liakata and Pulman (2002). However, the same approach can apply to predicate-arg structures from non-treebank data such as the QLFs derived from LFG structures in Cahill et al. (2003) or semantic representations such as in Bos et al. (2004). The WSJ corpus consists of 2,454 articles with a total of 2,798 distinct verb predicates, 62 prepositional predicates and 221 copular predicates containing the verb to ‘be’. Here we are only dealing with the non-prepositional predicates. The latter follow an uneven distribution of occurrences; there is a minority of very frequent verbs whereas the majority are rather sparse. The p"
W08-2212,C02-1144,0,0.231079,"advantage of existing semantic resources (such as WordNet (Miller, 1995)) or FrameNet (Baker et al., 1998). For example, Clark and Weir (2002) calculate the probability of a noun sense appearing as a particular argument by using WordNet to generalise over the noun sense. However, even though WordNet has been extremely useful in numerous applications, many researchers have found that the fact that it is largely developed via the intuitions of lexicographers, rather than being empirically based, means that the semantic information often is poorly matched with word usage in a particular domain. Pantel and Lin (2002) and Phillips and Riloff (2002) have pointed out that WordNet often includes many rare senses while missing out domainspecific senses and terminology. Some authors, Kilgariff (1997) and Hanks and Pustejovsky (2004), among others, reject the basic idea shared by WordNet and FrameNet (as well as traditional dictionaries) that there is a fixed list of senses for many verbs, arguing that individual senses will often be domain specific and should be discovered empirically by examining the syntactic and semantic contexts they occur in. We are highly sympathetic to this view and in this work we assum"
W08-2212,P93-1024,0,0.15897,"uggest that the verb patterns provide reasonably full coverage of the domain, while we can assign informative fine-grained semantic types to arguments with a reasonable degree of precision. Of course, a larger evaluation would be desirable, as would some task-related measure of how much this semantic typing helps in accurate processing. We hope to do this in future work. 4 Related Work The literature on acquiring semantic classes of words is very extensive. It is mostly motivated by WSD and WSI where the aim is to discover or be able to differentiate between different senses of a target word. Pereira et al. (1993) describes a method for clustering words according to their distributions in particular syntactic contexts. Nouns for instance are classified according to their distribution as direct objects of verbs, where it is assumed that the classification of verbs and nouns co-varies. In our approach we also make this assumption and nouns are clustered indirectly by first grouping together the verb argument slots they fill. Clustering in both cases is probabilistic with the assumptions that members of the same cluster follow similar distributions or in our case a joint distribution. Phillips and Riloff"
W08-2212,W02-1017,0,0.0595792,"antic resources (such as WordNet (Miller, 1995)) or FrameNet (Baker et al., 1998). For example, Clark and Weir (2002) calculate the probability of a noun sense appearing as a particular argument by using WordNet to generalise over the noun sense. However, even though WordNet has been extremely useful in numerous applications, many researchers have found that the fact that it is largely developed via the intuitions of lexicographers, rather than being empirically based, means that the semantic information often is poorly matched with word usage in a particular domain. Pantel and Lin (2002) and Phillips and Riloff (2002) have pointed out that WordNet often includes many rare senses while missing out domainspecific senses and terminology. Some authors, Kilgariff (1997) and Hanks and Pustejovsky (2004), among others, reject the basic idea shared by WordNet and FrameNet (as well as traditional dictionaries) that there is a fixed list of senses for many verbs, arguing that individual senses will often be domain specific and should be discovered empirically by examining the syntactic and semantic contexts they occur in. We are highly sympathetic to this view and in this work we assume, as Hanks and Pustejovsky do,"
W08-2212,W04-1908,0,0.100684,"wing form when replacing class IDs with tentative semantic labels: [company_organisation] report [percentage_money_income_revenue_stock_share_asset] [proposition_stake_rate_percentage] However, this does not mean that a person cannot be the 1st argument of report; there is overlap between classes 1 and 6 (the major person class) and Figure 1 shows they are closely linked. Such proximity of classes is considered during pattern evaluation (Section 3). 148 Liakata and Pulman The patterns were stored in a MySQL database. They are partly modelled on the ‘Corpus Pattern Analysis’ model described in Pustejovsky et al. (2004). These are syntagmatic patterns representing a selection context for the predicate they include, which determines the sense of the latter although CPA Patterns as defined by Pustejovsky et al. (2004) and Rumshisky and Pustejovsky (2006) are in fact rather more detailed than our patterns. 3 Results and Evaluation To evaluate the semantic types assigned by the automatically derived classes as well as the transferability of the derived CPA-like patterns to unseen instances, we performed a pilot study where we applied the patterns to two randomly selected articles from the on-line versions of the"
W08-2212,rumshisky-pustejovsky-2006-inducing,0,0.013115,"not be the 1st argument of report; there is overlap between classes 1 and 6 (the major person class) and Figure 1 shows they are closely linked. Such proximity of classes is considered during pattern evaluation (Section 3). 148 Liakata and Pulman The patterns were stored in a MySQL database. They are partly modelled on the ‘Corpus Pattern Analysis’ model described in Pustejovsky et al. (2004). These are syntagmatic patterns representing a selection context for the predicate they include, which determines the sense of the latter although CPA Patterns as defined by Pustejovsky et al. (2004) and Rumshisky and Pustejovsky (2006) are in fact rather more detailed than our patterns. 3 Results and Evaluation To evaluate the semantic types assigned by the automatically derived classes as well as the transferability of the derived CPA-like patterns to unseen instances, we performed a pilot study where we applied the patterns to two randomly selected articles from the on-line versions of the WSJ and the FT from March 2008. We believe this to be a useful test for the validity of the patterns since the new articles are guaranteed to be distinct from the training WSJ data of the 90s, while still belonging to the same domain. W"
W08-2212,E03-1037,0,0.0574143,"Missing"
W08-2212,J06-2001,0,0.0357816,"Missing"
W08-2212,C04-1133,0,\N,Missing
W08-2212,C04-1027,1,\N,Missing
W08-2212,C98-1013,0,\N,Missing
W08-2212,P06-4020,0,\N,Missing
W09-3949,P06-2073,1,0.886233,"Missing"
W09-3949,P03-1054,0,\N,Missing
W09-3949,J00-3003,0,\N,Missing
W09-3951,N06-1047,0,0.0214887,"Missing"
W09-3951,J00-3003,0,0.897755,"Missing"
W09-3951,benedi-etal-2006-design,0,0.0174969,"Missing"
W10-2707,C96-1021,0,0.0608462,"ar constellations of entities and events relevant to the HWYD scenario, for example ‘argument at work between X and Y’, or ‘meeting with X about Y’. Processing is non-deterministic and so sentences will get many analyses. We use a ‘shortest path through the chart heuristic to select an interpretation. This is far from perfect, and we are currently working on a separate more motivated disambiguation module. The final stage of processing before the Dialogue Manager takes over is to perform reference resolution for pronouns and definite NPs. This module is based partly on the system described by Kennedy and Boguraev 1996, with the various weighting factors based on theirs, but designed so that the weights can be trained given appropriate data. Currently we are collecting such data and the present set of weights are taken from Kennedy and Boguraev but with additional salience given to the domain-specific named entity classes. Each referring NP gives rise to a discourse referent, and these are grouped into coreference classes based on grammatical, semantic, and salience properties. 7 Affective Dialogue Strategies Once the NLU and DM have a sufficiently instantiated template, which also records emotional value,"
W10-2707,webb-etal-2010-evaluating,0,0.0614409,"Missing"
W11-0114,D10-1115,0,0.579701,"Missing"
W11-0114,D08-1094,0,0.605185,"mathematical model of [3] can be implemented in a concrete setting which introduces a richer, not necessarily truth-theoretic, notion of natural language semantics which is closer to the ideas underlying standard distributional models of word meaning. We leave full evaluation to future work, in order to determine whether the following method in conjunction with word vectors built from large corpora leads to improved results on language processing tasks, such as computing sentence similarity and paraphrase evaluation. Nouns and Transitive Verbs. We take N to be a structured vector space, as in [4, 5]. The bases of N are annotated by ‘properties’ obtained by combining dependency relations with nouns, verbs and adjectives. For example, basis vectors might be associated with properties such as “arg-fluffy”, denoting the argument of the adjective fluffy, “subj-chase” denoting the subject of the verb chase, “obj-buy” denoting the object of the verb buy, and so on. We construct the vector for a noun by counting how many times in the corpus a word has been the argument of ‘fluffy’, the subject of ‘chase’, the object of ‘buy’, and so on. The framework in [3] offers no guidance as to what the sent"
W11-0114,P08-1028,0,0.801448,"ination of run and catch, such as: 1 2 chase = run + catch 3 3 Again, the meaning map ensures that these degrees propagate to the meaning of the whole sentence. For a worked out example see [3]. But neither of these examples provide a distributional sentence meaning. Here we take a first step towards a corpus-based distributional model, by attempting to recover a meaning for a sentence based on the meanings of the words derived from a corpus. But crucially this meaning goes beyond just composing the meanings of words using a vector operator, such as tensor product, summation or multiplication [8]. Our computation of sentence meaning treats some vectors as functions and others as 127 function arguments, according to how the words in the sentence are typed, and uses the syntactic structure as a guide to determine how the functions are applied to their arguments. The intuition behind this approach is that syntactic analysis guides semantic vector composition. The contribution of this paper is to introduce some concrete constructions for a compositional distributional model of meaning. These constructions demonstrate how the mathematical model of [3] can be implemented in a concrete setti"
W11-0114,J00-4006,0,\N,Missing
W11-0114,J98-1004,0,\N,Missing
W11-0149,N09-1003,0,0.0866524,"Missing"
W11-0149,C04-1180,0,0.0317287,"rarely co-occurred in a sentence, but the network shows a strong connection over student as well as over credit and work. 390 2.1 The Network Structure We build the network incrementally by parsing every sentence, translating it into a small network fragment and then mapping that fragment onto the main network generated from all previous sentences. Our translation of sentences from text to network is based on the one used in the ASKNet system (Harrington and Clark, 2007). It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004) and the semantic analysis tool Boxer (Bos et al., 2004), both of which are part of the C&C Toolkit1 . The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 manually annotated sentences of the WSJ. It is both robust and efficient. Boxer is designed to convert the CCG parsed text into a logical representation based on Discourse Representation Theory (DRT). This intermediate logical form representation presents an abstraction from syntactic details to semantic core information. For example, the syntactical forms progress of student and student’s progress have the same Boxer representation as well as the student wh"
W11-0149,J06-1003,0,0.185815,"man Oxford University Computing Laboratory {pia-ramona.wojtinnek,stephen.pulman}@comlab.ox.ac.uk Abstract We introduce a novel approach to measuring semantic relatedness of terms based on an automatically generated, large-scale semantic network. We present promising first results that indicate potential competitiveness with approaches based on manually created resources. 1 Introduction The quantification of semantic similarity and relatedness of terms is an important problem of lexical semantics. Its applications include word sense disambiguation, text summarization and information retrieval (Budanitsky and Hirst, 2006). Most approaches to measuring semantic relatedness fall into one of two categories. They either look at distributional properties based on corpora (Finkelstein et al., 2002; Agirre et al., 2009) or make use of pre-existing knowledge resources such as WordNet or Roget’s Thesaurus (Hughes and Ramage, 2007; Jarmasz, 2003). The latter approaches achieve good results, but they are inherently restricted in coverage and domain adaptation due to their reliance on costly manual acquisition of the resource. In addition, those methods that are based on hierarchical, taxonomically structured resources ar"
W11-0149,P04-1014,0,0.0395213,"text underlying the network in Fig. 2, dissertation and module rarely co-occurred in a sentence, but the network shows a strong connection over student as well as over credit and work. 390 2.1 The Network Structure We build the network incrementally by parsing every sentence, translating it into a small network fragment and then mapping that fragment onto the main network generated from all previous sentences. Our translation of sentences from text to network is based on the one used in the ASKNet system (Harrington and Clark, 2007). It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004) and the semantic analysis tool Boxer (Bos et al., 2004), both of which are part of the C&C Toolkit1 . The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 manually annotated sentences of the WSJ. It is both robust and efficient. Boxer is designed to convert the CCG parsed text into a logical representation based on Discourse Representation Theory (DRT). This intermediate logical form representation presents an abstraction from syntactic details to semantic core information. For example, the syntactical forms progress of student and student’s progress have"
W11-0149,C10-2041,0,0.569075,"k~v (y)k As spreading activation takes several factors into account, such as number of paths, length of paths, level of density and number of connections, this method leverages the full interconnected structure of the network. 3 Evaluation We evaluate our approach on the WordSimilarity-353 (Finkelstein et al., 2002) test collection, which is a commonly used gold standard for the semantic relatedness task. It provides average human judgments scores of the degree of relatedness for 353 word pairs. The collection contains classically similar word 2 The spreading activation algorithm is based on Harrington (2010) 392 Approach (Strube and Ponzetto, 2006) (Jarmasz, 2003) (Hughes and Ramage, 2007) (Agirre et al., 2009) (Finkelstein et al., 2002) (Harrington, 2010) (Agirre et al., 2009) (Agirre et al., 2009) (Gabrilovich and Markovitch, 2007) Network (all pairs) Network (&gt;100 freq: 293 pairs) Network (&gt;300 freq: 227 pairs) Wikipedia Roget’s WordNet WordNet Web corpus, LSA Sem. Network WordNet+gloss Web corpus Wikipedia Spearman 0.19-0.48 0.55 0.55 0.56 0.56 0.62 0.66 0.66 0.75 0.38 0.46 0.50 all pairs &gt;300 freq Similarity 0.19 (100 pairs) 0.50 (60 pairs) Relatedness 0.36 (250 pairs) 0.52 (171 pairs) Table"
W11-0149,D07-1061,0,0.227108,"etitiveness with approaches based on manually created resources. 1 Introduction The quantification of semantic similarity and relatedness of terms is an important problem of lexical semantics. Its applications include word sense disambiguation, text summarization and information retrieval (Budanitsky and Hirst, 2006). Most approaches to measuring semantic relatedness fall into one of two categories. They either look at distributional properties based on corpora (Finkelstein et al., 2002; Agirre et al., 2009) or make use of pre-existing knowledge resources such as WordNet or Roget’s Thesaurus (Hughes and Ramage, 2007; Jarmasz, 2003). The latter approaches achieve good results, but they are inherently restricted in coverage and domain adaptation due to their reliance on costly manual acquisition of the resource. In addition, those methods that are based on hierarchical, taxonomically structured resources are generally better suited for measuring semantic similarity than relatedness (Budanitsky and Hirst, 2006). In this paper, we introduce a novel technique that measures semantic relatedness based on an automatically generated semantic network. Terms are compared by the similarity of their contexts in the s"
W13-3513,D12-1050,0,0.101142,"by a number of models (Table 3). The most successful model (M1) does not apply any form of composition. Instead, the comparison of a sentence with a “landmark” sentence is simply based on disambiguated versions of the As our compositional method for the following tasks we use the multiplicative and additive models of Mitchell and Lapata (2008). Despite the simple nature of these models, there is a number of reasons that make them good candidates for demonstrating the main ideas of this paper. First, for better or worse “simple” does not necessarily mean “ineffective”. The comparative study of Blacoe and Lapata (2012) shows that for certain tasks these “baselines” perform equally well or even better than other more sophisticated models. And second, it is reasonable to expect that better compositional models would only work in favour of our arguments, and not the other way around. 2 The dataset will be available at http://www.cs.ox. ac.uk/activities/compdistmeaning/. 3 As a comparison, the Mitchell and Lapata (2008) dataset consists of 15 main verbs × 4 contexts × 2 landmarks = 120 sentence pairs, while the Grefenstette and Sadrzadeh (2011a) dataset has the same configuration and size with ours. 119 verbs a"
W13-3513,P08-1028,0,0.583266,"drzadeh Queen Mary Univ. of London School of Electr. Engineering and Computer Science Mile End Road London, E1 4NS, UK Stephen Pulman University of Oxford Dept of Computer Science Wolfson Bldg, Parks Road Oxford, OX1 3QD, UK stephen.pulman@cs.ox.ac.uk mehrs@eecs.qmul.ac.uk Abstract text constituents larger than words, i.e. for phrases and sentences. Given the complementary nature of those two semantic models, it is not surprising that considerable research activity has been dedicated on combining them into a single framework that would benefit from the best of both worlds in a unified manner: Mitchell and Lapata (2008) experiment with intransitive sentences, applying simple compositional models based on vector addition and point-wise multiplication in a disambiguation task; Baroni and Zamparelli (2010) and Guevara (2010) use regression models in order to build vectors for adjective-noun compounds; Erk and Padó (2008) work on transitive sentences using structured vector spaces; Socher et al. (2010, 2011, 2012) use neural networks to combine vectors following the grammatical structure; Grefenstette and Sadrzadeh (2011a,b) apply the categorical framework of Coecke et al. (2010) on the disambiguation task of Mi"
W13-3513,S10-1011,0,0.159708,"words corpus created from samples of written and spoken English. We perform word sense induction by following the generic algorithm of Schütze (1998), in which the senses of a word are represented by distinct clusters created by taking into account the various contexts in which this specific word occur in the corpus. For the actual clustering step we use a combination of hierarchical agglomerative clustering and the Cali´nski-Harabasz index (Cali´nski and Harabasz, 1974). The parameters of the models are fine-tuned on the noun set of S EMEVAL 2010 Word Sense Induction and Disambiguation task (Manandhar et al., 2010). Equipped with a disambiguated vector space, we use it on a verb disambiguation experiment, similar in style to that of Mitchell and Lapata (2008), but applied on a more linguistically motivated dataset, based on the work of Pickering and Frisson (2001). We find that the application 2 Composition in distributional models The transition from word meaning to sentence meaning, a task easily done by human subjects based on the rules of grammar, implies the existence of a composition operation applied to primitive text units in order to build compound ones. Various solutions have been proposed wit"
W13-3513,D08-1094,0,0.493933,"n words, i.e. for phrases and sentences. Given the complementary nature of those two semantic models, it is not surprising that considerable research activity has been dedicated on combining them into a single framework that would benefit from the best of both worlds in a unified manner: Mitchell and Lapata (2008) experiment with intransitive sentences, applying simple compositional models based on vector addition and point-wise multiplication in a disambiguation task; Baroni and Zamparelli (2010) and Guevara (2010) use regression models in order to build vectors for adjective-noun compounds; Erk and Padó (2008) work on transitive sentences using structured vector spaces; Socher et al. (2010, 2011, 2012) use neural networks to combine vectors following the grammatical structure; Grefenstette and Sadrzadeh (2011a,b) apply the categorical framework of Coecke et al. (2010) on the disambiguation task of Mitchell and Lapata (2008); and Kartsaklis et al. (2012) and Grefenstette et al. (2013) build upon previous implementations by adding specific algebraic operations and machine learning techniques to further improve the concrete abilities of the abstract categorical models. A common strand in all of the ab"
W13-3513,W13-0112,0,0.343182,"mpositional models based on vector addition and point-wise multiplication in a disambiguation task; Baroni and Zamparelli (2010) and Guevara (2010) use regression models in order to build vectors for adjective-noun compounds; Erk and Padó (2008) work on transitive sentences using structured vector spaces; Socher et al. (2010, 2011, 2012) use neural networks to combine vectors following the grammatical structure; Grefenstette and Sadrzadeh (2011a,b) apply the categorical framework of Coecke et al. (2010) on the disambiguation task of Mitchell and Lapata (2008); and Kartsaklis et al. (2012) and Grefenstette et al. (2013) build upon previous implementations by adding specific algebraic operations and machine learning techniques to further improve the concrete abilities of the abstract categorical models. A common strand in all of the above models is that they are based on “ambiguous” vector representations, where a polysemous word is represented by a single vector regardless of the number of its actual senses. For example, the word ‘bank’ has at least two meanings (financial institution and land alongside a river), both of which will be fused into a single vector representation. And, although it is generally t"
W13-3513,D11-1129,1,0.866158,"them into a single framework that would benefit from the best of both worlds in a unified manner: Mitchell and Lapata (2008) experiment with intransitive sentences, applying simple compositional models based on vector addition and point-wise multiplication in a disambiguation task; Baroni and Zamparelli (2010) and Guevara (2010) use regression models in order to build vectors for adjective-noun compounds; Erk and Padó (2008) work on transitive sentences using structured vector spaces; Socher et al. (2010, 2011, 2012) use neural networks to combine vectors following the grammatical structure; Grefenstette and Sadrzadeh (2011a,b) apply the categorical framework of Coecke et al. (2010) on the disambiguation task of Mitchell and Lapata (2008); and Kartsaklis et al. (2012) and Grefenstette et al. (2013) build upon previous implementations by adding specific algebraic operations and machine learning techniques to further improve the concrete abilities of the abstract categorical models. A common strand in all of the above models is that they are based on “ambiguous” vector representations, where a polysemous word is represented by a single vector regardless of the number of its actual senses. For example, the word ‘ba"
W13-3513,W11-2507,1,0.752566,"them into a single framework that would benefit from the best of both worlds in a unified manner: Mitchell and Lapata (2008) experiment with intransitive sentences, applying simple compositional models based on vector addition and point-wise multiplication in a disambiguation task; Baroni and Zamparelli (2010) and Guevara (2010) use regression models in order to build vectors for adjective-noun compounds; Erk and Padó (2008) work on transitive sentences using structured vector spaces; Socher et al. (2010, 2011, 2012) use neural networks to combine vectors following the grammatical structure; Grefenstette and Sadrzadeh (2011a,b) apply the categorical framework of Coecke et al. (2010) on the disambiguation task of Mitchell and Lapata (2008); and Kartsaklis et al. (2012) and Grefenstette et al. (2013) build upon previous implementations by adding specific algebraic operations and machine learning techniques to further improve the concrete abilities of the abstract categorical models. A common strand in all of the above models is that they are based on “ambiguous” vector representations, where a polysemous word is represented by a single vector regardless of the number of its actual senses. For example, the word ‘ba"
W13-3513,I11-1079,0,0.413975,"multiplication to tensors of higher rank. Regardless of their level of sophistication, most of the models which aim to apply compositionality on word vector representations fail to address the problem of handling the polysemous nature of words. Even more importantly, many of the models are evaluated on their ability to disambiguate the meaning of specific words, following an idea first introduced by Kintsch (2001) and later adopted by Mitchell and Lapata (2008) and others. For example, in this latter work the au115 showing promising results on the Mitchell and Lapata (2008) task. The work of Reddy et al. (2011) is closer to our research: the authors evaluate two word sense disambiguation approaches on the noun-noun compound similarity task introduced by Mitchell and Lapata (2010), using simple multiplicative and additive models for composition. The reported results are also promising, where at least one of their models performs better than the current practice of using ambiguous vector representations. Compared to both of the above works, the scope of the current paper is broader: it does not solely aim to demonstrate the positive effect of a “cleaner” vector space on the compositional process, but"
W13-3513,W10-2805,0,0.035798,"QD, UK stephen.pulman@cs.ox.ac.uk mehrs@eecs.qmul.ac.uk Abstract text constituents larger than words, i.e. for phrases and sentences. Given the complementary nature of those two semantic models, it is not surprising that considerable research activity has been dedicated on combining them into a single framework that would benefit from the best of both worlds in a unified manner: Mitchell and Lapata (2008) experiment with intransitive sentences, applying simple compositional models based on vector addition and point-wise multiplication in a disambiguation task; Baroni and Zamparelli (2010) and Guevara (2010) use regression models in order to build vectors for adjective-noun compounds; Erk and Padó (2008) work on transitive sentences using structured vector spaces; Socher et al. (2010, 2011, 2012) use neural networks to combine vectors following the grammatical structure; Grefenstette and Sadrzadeh (2011a,b) apply the categorical framework of Coecke et al. (2010) on the disambiguation task of Mitchell and Lapata (2008); and Kartsaklis et al. (2012) and Grefenstette et al. (2013) build upon previous implementations by adding specific algebraic operations and machine learning techniques to further i"
W13-3513,D07-1043,0,0.0124829,"Specifically, when using HAC one has to decide how to measure the distance between the clusters, which is the merging criterion applied in every iteration of the algorithm, as well as the measure between the data points, i.e. the individual vectors. Based on empirical tests we limit our options to two inter-cluster measures: complete-link and Ward’s methods. In the complete-link method the distance between two clusters X and Y is the distance between their two most remote elements: D(X, Y ) = max d(x, y) x∈X,y∈Y Table 2: Derived senses for word ‘keyboard’. called matching problem of F-score (Rosenberg and Hirschberg, 2007). Table 1 shows the results. Ward’s method in combination with correlation distance provided the highest V-measure, followed by the combination of complete-link with (again) correlation. Although a direct comparison of our models with the models participating in this task would not be quite sound (since these models were trained on a special corpus provided by the organizers, while our model was trained on the BNC), it is nevertheless enlightening to mention that the 0.14 V-measure places the Wardcorrelation model at the 4th rank among 28 systems for the noun set of the task, while at the same"
W13-3513,C12-2054,1,0.881372,"sentences, applying simple compositional models based on vector addition and point-wise multiplication in a disambiguation task; Baroni and Zamparelli (2010) and Guevara (2010) use regression models in order to build vectors for adjective-noun compounds; Erk and Padó (2008) work on transitive sentences using structured vector spaces; Socher et al. (2010, 2011, 2012) use neural networks to combine vectors following the grammatical structure; Grefenstette and Sadrzadeh (2011a,b) apply the categorical framework of Coecke et al. (2010) on the disambiguation task of Mitchell and Lapata (2008); and Kartsaklis et al. (2012) and Grefenstette et al. (2013) build upon previous implementations by adding specific algebraic operations and machine learning techniques to further improve the concrete abilities of the abstract categorical models. A common strand in all of the above models is that they are based on “ambiguous” vector representations, where a polysemous word is represented by a single vector regardless of the number of its actual senses. For example, the word ‘bank’ has at least two meanings (financial institution and land alongside a river), both of which will be fused into a single vector representation."
W13-3513,D12-1110,0,0.301337,"Missing"
W13-3513,W11-1102,0,0.0126364,"Missing"
W13-3513,D10-1115,0,\N,Missing
W13-3513,J98-1004,0,\N,Missing
W14-1806,W11-0101,0,0.0199603,"udes are what we term ‘solidarity’ attitudes, in that we assume that they were made in order to engender positive feelings in A. Engage comments have a very wide variety of forms and topics, which we will not be attempting to analyse in the initial rounds of the machine learning trials. Thank comments are all expressions of gratitude. • Doubt: “Why bold?” – M considers that something in the essay is of questionable value. Since expressions of uncertainty are often used as softeners rather than to express actual uncertainty, it seemed inappropriate to treat apparent uncertainty as a qualifier (Bunt, 2011) of attitudes. If we treat it as a qualifier, it suggests that M 7 Scheme design: Linguistic Act The third layer of the categorisation scheme identifies what we are calling the ‘linguistic act’ of the comment. The acts are distinguished principally 48 Instruction comments do not always have the imperative form is a repercussion of the informal conversational style of the comments. A sixth ‘dummy’ linguistic act category is assigned to all comments with attitude Engage, because we will not be attempting to analyse those. The linguistic act layer, then, categorises the comment’s form, while the"
