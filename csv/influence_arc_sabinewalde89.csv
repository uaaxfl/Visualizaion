2014.amta-researchers.21,2009.eamt-1.9,0,0.362469,"the high-quality lexical resource GermaNet. 2 Related Work Translating prepositions is an important problem in machine translation. So far, research has mostly been reported on rule-based systems. Gustavii (2005) uses bilingual features and selectional constraints to correct translations from a rule-based Swedish-English system; she reports a gain in accuracy for prepositions. Naskar and Bandyopadhyay (2006) outline a method to handle prepositions in an English-Bengali MT system: they use WordNet in combination with a bilingual example base for idiomatic PPs, but do not report any evaluation. Agirre et al. (2009) model Basque prepositions and grammatical case using syntactic-semantic features such as subcategorization triples for a rule-based system; they also report a gain in translation accuracy for prepositions. The approach of Shilon et al. (2012) is similar to the work of Agirre et al. (2009); however, Shilon’s system has a statistical component for ranking proposed translations, which leads to an improvement in BLEU for a small test set. Furthermore, Zollmann and Vogel (2011) use cluster information in syntactic SMT, although not specifically for translating prepositions. Huang and Knight (2006)"
2014.amta-researchers.21,E12-1068,1,0.871973,"ymbols and the source-side) remains the same. To keep the amount of generated rules manageable, we used a threshold of f ≥ 5 to select the rules for which to generate new PP rules and only kept generated rules with a translation probability of p ≥ 0.001 (“new rules”). Finally, we added both baseline and new rules (“BL+new”). 5 Experiments and Results We used a morphology-aware English-German translation system that first translates into a lemmatized representation, and then generates inflected forms based on morphological features predicted with a sequence model (e.g. Toutanova et al. (2008), Fraser et al. (2012)). This reduces morphological complexity of nominal phrases, and allows in particular to handle portmanteaus (combination of preposition and article: zur=zu+der: to the) which are split in pre-processing and merged in a post-processing step. Thus, during translation, prepositions occurring as portmanteaus are represented in the same way as non-portmanteau prepositions. Table 4 illustrates the processing steps. The lemmatized representation (first column) contains feature markup on nouns for the features number and gender, which are considered part of the stem. The information about gender is o"
2014.amta-researchers.21,W10-1734,1,0.820456,"ed entity category1 . A second benefit is that only nouns, for which we can expect to have either GermaNet coverage or a sound basis for feature extraction, are considered for clustering; “nonnouns” (such as typos or parse-errors which are often very low-frequency and thus likely to deteriorate clustering performance) are excluded from clustering. The second pre-processing step consists in compound handling: as German noun compounding is very productive and can lead to sparsity and coverage problems, we applied compound splitting to all nouns using a linguistically-informed compound splitter (Fritzinger and Fraser, 2010), which disambiguates competing SMOR analyses relying on corpus statistics. After pre-processing, noun class information is first computed for head nouns. Then, in a second step, compounds are added into classes based on their head noun. While this might introduce noise for a small number of non-compositional compounds, we assume that the gain in generalization is more important. 3.2 GermaNet GermaNet (Hamp and Feldweg, 1997; Kunze, 2000) is a lexical resource for German similar to the English WordNet (Fellbaum, 1998). It is a lexical-semantic taxonomy that groups words of the same concept int"
2014.amta-researchers.21,N04-1035,0,0.0525061,"tags and stems (second column). Based on the predicted morphological features and the lemma, inflected forms can be generated using a morphological resource (third column). Finally, after generating inflected forms, split instances of portmanteau prepositions are merged relying on a simple set of rules4 as illustrated in the example (zu+der → zur) in the third column of table 4. 5.1 Data We used 1.5 M sentences of parallel data (Europarl and news data from the 2009 WMT shared task), with the target-side part as language model data, to train a string-to-tree Moses system with GHKM extraction (Galley et al., 2004; Williams and Koehn, 2012). The tuning/test sets consist of 1025/1026 news sentences (from the 2009 WMT shared task). The German data was parsed with BitPar (Schmid, 2004). For generating inflected forms, we used the morphological tool SMOR (Schmid et al., 2004). For predicting the morphological features number, gender, case and strong/weak inflection, we trained one CRF for each of the four morphological features using the Wapiti toolkit (Lavergne et al., 2010). The tuples for modelling translation probabilities for rule generation and the context vectors for clustering were obtained from a"
2014.amta-researchers.21,2005.eamt-1.16,0,0.583506,"tional preferences as rigid annotation in the parse tree is not optimal, as there is no generally applicable optimal level of semantic information. With regard to resources, we found that cluster analyses based on simple window information are better at capturing selectional preferences, with superior performance to both (a) the clusters relying on syntactic features and (b) the classes induced from the high-quality lexical resource GermaNet. 2 Related Work Translating prepositions is an important problem in machine translation. So far, research has mostly been reported on rule-based systems. Gustavii (2005) uses bilingual features and selectional constraints to correct translations from a rule-based Swedish-English system; she reports a gain in accuracy for prepositions. Naskar and Bandyopadhyay (2006) outline a method to handle prepositions in an English-Bengali MT system: they use WordNet in combination with a bilingual example base for idiomatic PPs, but do not report any evaluation. Agirre et al. (2009) model Basque prepositions and grammatical case using syntactic-semantic features such as subcategorization triples for a rule-based system; they also report a gain in translation accuracy for"
2014.amta-researchers.21,W97-0802,0,0.0844244,"ding is very productive and can lead to sparsity and coverage problems, we applied compound splitting to all nouns using a linguistically-informed compound splitter (Fritzinger and Fraser, 2010), which disambiguates competing SMOR analyses relying on corpus statistics. After pre-processing, noun class information is first computed for head nouns. Then, in a second step, compounds are added into classes based on their head noun. While this might introduce noise for a small number of non-compositional compounds, we assume that the gain in generalization is more important. 3.2 GermaNet GermaNet (Hamp and Feldweg, 1997; Kunze, 2000) is a lexical resource for German similar to the English WordNet (Fellbaum, 1998). It is a lexical-semantic taxonomy that groups words of the same concept into synsets. For each head noun, we looked up the GermaNet class for a given hierarchical level, to determine the degree of generalization: GermaNet is graph-structured, and extracting the nouns at different levels results in more or less fine-grained sets of classes. We used noun classes from the levels 2, 3, 4, 5, counting from the top level2 . 1 Note, however, that our type-based annotation method does not take into account"
2014.amta-researchers.21,N06-1031,0,0.276473,"n. Agirre et al. (2009) model Basque prepositions and grammatical case using syntactic-semantic features such as subcategorization triples for a rule-based system; they also report a gain in translation accuracy for prepositions. The approach of Shilon et al. (2012) is similar to the work of Agirre et al. (2009); however, Shilon’s system has a statistical component for ranking proposed translations, which leads to an improvement in BLEU for a small test set. Furthermore, Zollmann and Vogel (2011) use cluster information in syntactic SMT, although not specifically for translating prepositions. Huang and Knight (2006) propose methods of relabeling syntax trees to improve statistical syntactic translation. Their annotation aims at making the used tag-set (based on the Penn Treebank) less general, assuming that it often fails to capture relevant grammatical distinctions and contexts that are crucial for translation. They distinguish between internal and external annotation. In the case of internal annotation, additional information about the node or its relatives that is otherwise not accessible to the respective node is annotated; this type of annotation consists of lexical and tag information. Their lexica"
2014.amta-researchers.21,kunze-2000-extension,0,0.0693447,"Missing"
2014.amta-researchers.21,P10-1052,0,0.184531,"Missing"
2014.amta-researchers.21,P08-1114,0,0.0219911,"at the same level of granularity. 6.2 Conclusion We started with the hypothesis that noun class information is useful to model selectional preferences in preposition translation rules. However, annotating semantic class information on NP/PP nodes of the parse trees in a string-to-tree system amounts to a hard constraint and our experiments indicate that this form of annotation leads to overly specific rules. We tried to compensate for this by making the non-annotated rules available and by adding new PP rules synthesized from monolingual data. However, previous work, such as e.g. the work of Marton and Resnik (2008), has shown that soft constraints often work better than hard constraints. It might therefore make sense to model selectional preferences through the use of feature functions which reward good choices, rather than markup in the string-to-tree grammar, but this would require extensive changes to the model and decoder. Another problem with our approach is that there is no generally applicable optimal level of selectional preferences. This is in line with semantic research on selectional preferences as verb subcategorization features (Schulte im Walde, 2006; Joanis et al., 2008): across subcatego"
2014.amta-researchers.21,W06-2113,0,0.469097,"that cluster analyses based on simple window information are better at capturing selectional preferences, with superior performance to both (a) the clusters relying on syntactic features and (b) the classes induced from the high-quality lexical resource GermaNet. 2 Related Work Translating prepositions is an important problem in machine translation. So far, research has mostly been reported on rule-based systems. Gustavii (2005) uses bilingual features and selectional constraints to correct translations from a rule-based Swedish-English system; she reports a gain in accuracy for prepositions. Naskar and Bandyopadhyay (2006) outline a method to handle prepositions in an English-Bengali MT system: they use WordNet in combination with a bilingual example base for idiomatic PPs, but do not report any evaluation. Agirre et al. (2009) model Basque prepositions and grammatical case using syntactic-semantic features such as subcategorization triples for a rule-based system; they also report a gain in translation accuracy for prepositions. The approach of Shilon et al. (2012) is similar to the work of Agirre et al. (2009); however, Shilon’s system has a statistical component for ranking proposed translations, which leads"
2014.amta-researchers.21,P06-1014,0,0.0297856,"erally acceptable semantic level of generalization in lexical resources. Because of this, the parse tree annotation is not flexible enough to take into account the varying needs of different contexts, as it always leads to rules of the same degree of specificity, and therefore cannot adapt to the respective contexts. With regard to resources, we found that none of the variants we considered was able to obtain noun class information that is optimal: WordNets in general are known to be very finegrained and contain many ambiguities, making it difficult to derive generally applicable noun groups (Navigli, 2006; Palmer et al., 2007). In contrast, window clusters might not contain the appropriate selectional preference information as they resemble topic clusters rather than a generalization over specific noun types. As opposed to the unstructured information used for the window clustering, the syntactic dependencies constitute the type of information that is needed to determine a valid preposition for a given context, i.e. the governing verb/noun or the noun in the PP. Thus, clusters learned from syntactic features were expected to better capture selectional preferences. However, these clusters faile"
2014.amta-researchers.21,C00-2094,0,0.743559,"s-related cluster contains persons (minister, chancellor) and other terms related to politics. In contrast, the classes assigned by GermNet resemble more a generalization over specific noun types as human beings (minister, chancellor) are grouped together and the remaining terms majority, opposition and dismissal are each in a separate group. Using syntactic features for clustering, in particular prepositions, aims at better capturing selectional preferences, and thus obtaining classes that provide salient information for the task of modeling the choice of prepositions in SMT; cf. for example Prescher et al. (2000), Erk et al. (2010), Joanis et al. (2008), Schulte im Walde (2006) and Schulte im Walde (2010) for more information. A major problem consists in finding a number of clusters that provides both (i) a good representation of the nouns and (ii) the optimal level of abstraction for our SMT-system. In our experiments, we varied the cluster sizes and used sets of 10 – 300 clusters. 4 Using Noun Class Information in SMT This section presents the basic enriched system and its variants extended with non-annotated baseline rules and new PP rules. In all experiments, a preposition is annotated on both its"
2014.amta-researchers.21,C04-1024,0,0.102844,"lly, after generating inflected forms, split instances of portmanteau prepositions are merged relying on a simple set of rules4 as illustrated in the example (zu+der → zur) in the third column of table 4. 5.1 Data We used 1.5 M sentences of parallel data (Europarl and news data from the 2009 WMT shared task), with the target-side part as language model data, to train a string-to-tree Moses system with GHKM extraction (Galley et al., 2004; Williams and Koehn, 2012). The tuning/test sets consist of 1025/1026 news sentences (from the 2009 WMT shared task). The German data was parsed with BitPar (Schmid, 2004). For generating inflected forms, we used the morphological tool SMOR (Schmid et al., 2004). For predicting the morphological features number, gender, case and strong/weak inflection, we trained one CRF for each of the four morphological features using the Wapiti toolkit (Lavergne et al., 2010). The tuples for modelling translation probabilities for rule generation and the context vectors for clustering were obtained from a combination of the web corpus SdeWaC (44M sentences, Faaß and Eckart (2013)) and the German part of the parallel data. 5.2 Results Table 5 presents the results of the syste"
2014.amta-researchers.21,schmid-etal-2004-smor,0,0.190229,"ii) clustering nouns on the basis of syntactic dependency information. Comparing these disjunctive methods should ensure a systematic assessment of integrating selectional preferences. 3.1 Pre-processing In order to obtain a consistent noun class annotation, we applied two pre-processing steps to the target-language data prior to computing noun classes using the three variants. In the first step, we attempt to resolve (possibly) inconsistent parsing decisions for word types tagged both as nouns and named entities. Only words recognized as nouns by the highcoverage morphological analyzer SMOR (Schmid et al., 2004) are considered as common nouns. The remaining instances are considered as named entities; they are classified into organization, location, person and a category for rest (Faruqui and Pad´o, 2010). Performing this preprocessing ensures that nouns are consistently labeled with the same noun class or named entity category1 . A second benefit is that only nouns, for which we can expect to have either GermaNet coverage or a sound basis for feature extraction, are considered for clustering; “nonnouns” (such as typos or parse-errors which are often very low-frequency and thus likely to deteriorate c"
2014.amta-researchers.21,J06-2001,1,0.928046,"Missing"
2014.amta-researchers.21,schulte-im-walde-2010-comparing,1,0.894257,"Missing"
2014.amta-researchers.21,W12-0514,0,0.224268,"lectional constraints to correct translations from a rule-based Swedish-English system; she reports a gain in accuracy for prepositions. Naskar and Bandyopadhyay (2006) outline a method to handle prepositions in an English-Bengali MT system: they use WordNet in combination with a bilingual example base for idiomatic PPs, but do not report any evaluation. Agirre et al. (2009) model Basque prepositions and grammatical case using syntactic-semantic features such as subcategorization triples for a rule-based system; they also report a gain in translation accuracy for prepositions. The approach of Shilon et al. (2012) is similar to the work of Agirre et al. (2009); however, Shilon’s system has a statistical component for ranking proposed translations, which leads to an improvement in BLEU for a small test set. Furthermore, Zollmann and Vogel (2011) use cluster information in syntactic SMT, although not specifically for translating prepositions. Huang and Knight (2006) propose methods of relabeling syntax trees to improve statistical syntactic translation. Their annotation aims at making the used tag-set (based on the Penn Treebank) less general, assuming that it often fails to capture relevant grammatical"
2014.amta-researchers.21,P08-1059,0,0.120768,"(other nodes, terminal symbols and the source-side) remains the same. To keep the amount of generated rules manageable, we used a threshold of f ≥ 5 to select the rules for which to generate new PP rules and only kept generated rules with a translation probability of p ≥ 0.001 (“new rules”). Finally, we added both baseline and new rules (“BL+new”). 5 Experiments and Results We used a morphology-aware English-German translation system that first translates into a lemmatized representation, and then generates inflected forms based on morphological features predicted with a sequence model (e.g. Toutanova et al. (2008), Fraser et al. (2012)). This reduces morphological complexity of nominal phrases, and allows in particular to handle portmanteaus (combination of preposition and article: zur=zu+der: to the) which are split in pre-processing and merged in a post-processing step. Thus, during translation, prepositions occurring as portmanteaus are represented in the same way as non-portmanteau prepositions. Table 4 illustrates the processing steps. The lemmatized representation (first column) contains feature markup on nouns for the features number and gender, which are considered part of the stem. The informa"
2014.amta-researchers.21,W12-3150,0,0.0434317,"nd column). Based on the predicted morphological features and the lemma, inflected forms can be generated using a morphological resource (third column). Finally, after generating inflected forms, split instances of portmanteau prepositions are merged relying on a simple set of rules4 as illustrated in the example (zu+der → zur) in the third column of table 4. 5.1 Data We used 1.5 M sentences of parallel data (Europarl and news data from the 2009 WMT shared task), with the target-side part as language model data, to train a string-to-tree Moses system with GHKM extraction (Galley et al., 2004; Williams and Koehn, 2012). The tuning/test sets consist of 1025/1026 news sentences (from the 2009 WMT shared task). The German data was parsed with BitPar (Schmid, 2004). For generating inflected forms, we used the morphological tool SMOR (Schmid et al., 2004). For predicting the morphological features number, gender, case and strong/weak inflection, we trained one CRF for each of the four morphological features using the Wapiti toolkit (Lavergne et al., 2010). The tuples for modelling translation probabilities for rule generation and the context vectors for clustering were obtained from a combination of the web corp"
2014.amta-researchers.21,P11-1001,0,0.0235711,"MT system: they use WordNet in combination with a bilingual example base for idiomatic PPs, but do not report any evaluation. Agirre et al. (2009) model Basque prepositions and grammatical case using syntactic-semantic features such as subcategorization triples for a rule-based system; they also report a gain in translation accuracy for prepositions. The approach of Shilon et al. (2012) is similar to the work of Agirre et al. (2009); however, Shilon’s system has a statistical component for ranking proposed translations, which leads to an improvement in BLEU for a small test set. Furthermore, Zollmann and Vogel (2011) use cluster information in syntactic SMT, although not specifically for translating prepositions. Huang and Knight (2006) propose methods of relabeling syntax trees to improve statistical syntactic translation. Their annotation aims at making the used tag-set (based on the Penn Treebank) less general, assuming that it often fails to capture relevant grammatical distinctions and contexts that are crucial for translation. They distinguish between internal and external annotation. In the case of internal annotation, additional information about the node or its relatives that is otherwise not acc"
2020.acl-main.258,W16-4702,0,0.3825,"ng-Treitler et al. (2008) use a contextual network; Bouamor et al. (2016) exploit language models; P´erez (2016) compares collocation networks. For standard term extraction, contrastive techniques represent one of the main strands of methodologies, by comparing a term candidate’s frequencies in a domain-specific and a general-language corpus (Ahmad et al., 1994; Rayson and Garside, 2000; Drouin, 2003; Kit and Liu, 2008; Bonin et al., 2010; Kochetkova, 2015; Lopes et al., 2016; Mykowiecka et al., 2018, i.a.). Recent approaches use word embeddings trained separately on contrastive corpora; e.g. Amjadian et al. (2016, 2018) concatenate general and domain-specific word embeddings and use them as input for classifiers, such as a multilayer perceptron. Similarly, Hazem and Morin (2017) and Liu et al. (2018) apply such a concatenation to represent a term in one language, as data enrichment pre-step for bilingual terminology extraction. In sum, approaches using contrastive corpora are popular in both automatic term extraction and term technicality prediction studies. The few approaches that use word embeddings as basis for a contrastive approach separately train word embeddings on general-language and domain c"
2020.acl-main.258,D16-1250,0,0.0315878,"SGNS vector spaces (Mikolov et al., 2013) for G EN and S PEC. Centering and Batch Normalization Across neural models we apply batch normalization (Ioffe and Szegedy, 2015), which normalizes the output of a preceding activation layer by subtracting the batch mean and then dividing by the batch standard deviation. This reduces the effect of inhomogeneous input data, in our case the different domain corpora. We further length-normalize and apply element-wise column mean-centering to the embeddings, which has proven to be beneficial as preprocessing step for rotational alignment of vector spaces (Artetxe et al., 2016; Schlechtweg et al., 2019) and as a general post-processing step for word embeddings (Mu and Viswanath, 2018). Note that the reason for the beneficial effect of centering is still unclear. Artetxe et al. (2016) provide an intuitive explanation that centering moves randomly similar embeddings further apart, while Mu and Viswanath (2018) consider centering as an operation making vectors “more isotropic”, i.e., more uniformly distributed across the directions in the space. Comparative Embeddings and Multi-Channel Model Simple vector concatenation does not incorporate any kind of comparison of th"
2020.acl-main.258,bonin-etal-2010-contrastive,0,0.0841097,"Missing"
2020.acl-main.258,L16-1366,0,0.0144551,"inology in medical, biomedical or health domains. Term familiarity refers to a user’s subjective understanding of term technicality. These studies typically rely on classical readability features such as frequency, term length, syllable count, the DaleChall readability formula and affixes (Zeng et al., 2005; Zeng-Treitler et al., 2008; Grabar et al., 2014; Vinod Vydiswaran et al., 2014). They further make use of domain-specific terminology attributes such as neo-classical word components, given that medical terminology is strongly influenced by Greek and Latin (Del´eger and Zweigenbaum, 2009; Bouamor et al., 2016). Besides the feature specification, the majority of studies exploits contrastive approaches. 2883 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2883–2889 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Contrastive approaches compare a term’s distribution in a domain and a reference corpus, for example a general-language corpus. Furthermore, for technicality prediction, often expert (medical) texts are compared against reference lay texts. Only a small number of studies relies on context-based approaches, e.g. Zeng-Treitler e"
2020.acl-main.258,W14-1202,0,0.011613,"y important across all specialized domains. Furthermore, term technicality prediction is important for a range of tasks such as automatic thesaurus 2 Related Work Existing studies on technicality predominantly focus on levels of familiarity or difficulty of terminology in medical, biomedical or health domains. Term familiarity refers to a user’s subjective understanding of term technicality. These studies typically rely on classical readability features such as frequency, term length, syllable count, the DaleChall readability formula and affixes (Zeng et al., 2005; Zeng-Treitler et al., 2008; Grabar et al., 2014; Vinod Vydiswaran et al., 2014). They further make use of domain-specific terminology attributes such as neo-classical word components, given that medical terminology is strongly influenced by Greek and Latin (Del´eger and Zweigenbaum, 2009; Bouamor et al., 2016). Besides the feature specification, the majority of studies exploits contrastive approaches. 2883 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2883–2889 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Contrastive approaches compare a term’s distribution in a domain"
2020.acl-main.258,I17-1069,0,0.131188,"tion, contrastive techniques represent one of the main strands of methodologies, by comparing a term candidate’s frequencies in a domain-specific and a general-language corpus (Ahmad et al., 1994; Rayson and Garside, 2000; Drouin, 2003; Kit and Liu, 2008; Bonin et al., 2010; Kochetkova, 2015; Lopes et al., 2016; Mykowiecka et al., 2018, i.a.). Recent approaches use word embeddings trained separately on contrastive corpora; e.g. Amjadian et al. (2016, 2018) concatenate general and domain-specific word embeddings and use them as input for classifiers, such as a multilayer perceptron. Similarly, Hazem and Morin (2017) and Liu et al. (2018) apply such a concatenation to represent a term in one language, as data enrichment pre-step for bilingual terminology extraction. In sum, approaches using contrastive corpora are popular in both automatic term extraction and term technicality prediction studies. The few approaches that use word embeddings as basis for a contrastive approach separately train word embeddings on general-language and domain corpora. In our work, we extend these methodologies by aligning vector spaces in order to more adequately represent meaning variation across corpora. 3 Definition of Tech"
2020.acl-main.258,C18-1242,0,0.0182356,"s represent one of the main strands of methodologies, by comparing a term candidate’s frequencies in a domain-specific and a general-language corpus (Ahmad et al., 1994; Rayson and Garside, 2000; Drouin, 2003; Kit and Liu, 2008; Bonin et al., 2010; Kochetkova, 2015; Lopes et al., 2016; Mykowiecka et al., 2018, i.a.). Recent approaches use word embeddings trained separately on contrastive corpora; e.g. Amjadian et al. (2016, 2018) concatenate general and domain-specific word embeddings and use them as input for classifiers, such as a multilayer perceptron. Similarly, Hazem and Morin (2017) and Liu et al. (2018) apply such a concatenation to represent a term in one language, as data enrichment pre-step for bilingual terminology extraction. In sum, approaches using contrastive corpora are popular in both automatic term extraction and term technicality prediction studies. The few approaches that use word embeddings as basis for a contrastive approach separately train word embeddings on general-language and domain corpora. In our work, we extend these methodologies by aligning vector spaces in order to more adequately represent meaning variation across corpora. 3 Definition of Technicality According to"
2020.acl-main.258,W09-3102,0,0.0428218,"Missing"
2020.acl-main.258,W00-0901,0,0.460856,"corpus, for example a general-language corpus. Furthermore, for technicality prediction, often expert (medical) texts are compared against reference lay texts. Only a small number of studies relies on context-based approaches, e.g. Zeng-Treitler et al. (2008) use a contextual network; Bouamor et al. (2016) exploit language models; P´erez (2016) compares collocation networks. For standard term extraction, contrastive techniques represent one of the main strands of methodologies, by comparing a term candidate’s frequencies in a domain-specific and a general-language corpus (Ahmad et al., 1994; Rayson and Garside, 2000; Drouin, 2003; Kit and Liu, 2008; Bonin et al., 2010; Kochetkova, 2015; Lopes et al., 2016; Mykowiecka et al., 2018, i.a.). Recent approaches use word embeddings trained separately on contrastive corpora; e.g. Amjadian et al. (2016, 2018) concatenate general and domain-specific word embeddings and use them as input for classifiers, such as a multilayer perceptron. Similarly, Hazem and Morin (2017) and Liu et al. (2018) apply such a concatenation to represent a term in one language, as data enrichment pre-step for bilingual terminology extraction. In sum, approaches using contrastive corpora a"
2020.acl-main.258,P19-1072,1,0.791617,"Missing"
2020.lrec-1.537,L16-1366,0,0.153053,"ion 4.), and the actual creation of the compound dataset (section 5.). After evaluating the interannotator agreement (section 6.), the final gold standard is described (section 7.). We present statistics and insights about the dataset (section 8.) and discuss the challenges of annotating difficulty (section 9.). 2. tution through simpler synonyms and providing an explanation (Elhadad, 2006; Kandula et al., 2010). Most studies focus on biomedical or medicals areas and the assessment of difficulty of domain-specific terminology. Approaches to evaluate familiarity prediction systems are diverse. Bouamor et al. (2016) rely on English Consumer Health Vocabulary that is included in the UMLS Metathesaurus (Zeng et al., 2007), whose vocabulary distinguishes between lay and specialized terms. Grabar et al. (2014) create a gold standard with manual annotations on a threeposition scale: understand − partly understand − don’t understand. Vydiswaran et al. (2014) perform a post-hoc evaluation of their presented models, letting a medical expert review a sample of 100 pairs, which were previously extracted as ’consumer’ and ’professional’ terms. ZengTreitler et al. (2008) measure a lay person’s familiarity with a ter"
2020.lrec-1.537,W14-5702,0,0.238845,"kipedia categories are manually filtered for categories which are contentwise too far away, as a further data cleaning step to maintain the topical focus of the corpora. Finally, all corpora are reduced to the size of the smallest corpus, which results in equally-sized corpora of 5.6 million tokens. The texts are tokenized, lemmatized and tagged with spaCy1 ; we applied lemma correction. 5. 5.1. Table 1 shows that more two-part compounds are extracted for the automotive domain than for DIY and cooking. This is in line with our observation that automotive is the most technical domain, and with Clouet and Daille (2014), that “[compounding] is particularly productive in specialized domains because of the necessity to denote the domain concepts in a very concise and precise way” (p. 11). 5.2. Since the set of retrieved compounds is too large to be annotated completely, we select a balanced subset. We consider the following compound characteristics as relevant for our task: • frequency of compound and components: How often do they occur in the respective domain-specific corpus as an independent unit (i.e. the components are not embedded within other words)? • productivity of the modifier and head: In how many"
2020.lrec-1.537,W14-1202,0,0.337434,"atistics and insights about the dataset (section 8.) and discuss the challenges of annotating difficulty (section 9.). 2. tution through simpler synonyms and providing an explanation (Elhadad, 2006; Kandula et al., 2010). Most studies focus on biomedical or medicals areas and the assessment of difficulty of domain-specific terminology. Approaches to evaluate familiarity prediction systems are diverse. Bouamor et al. (2016) rely on English Consumer Health Vocabulary that is included in the UMLS Metathesaurus (Zeng et al., 2007), whose vocabulary distinguishes between lay and specialized terms. Grabar et al. (2014) create a gold standard with manual annotations on a threeposition scale: understand − partly understand − don’t understand. Vydiswaran et al. (2014) perform a post-hoc evaluation of their presented models, letting a medical expert review a sample of 100 pairs, which were previously extracted as ’consumer’ and ’professional’ terms. ZengTreitler et al. (2008) measure a lay person’s familiarity with a term based on the percentage of annotators who identify the term correctly. 3. German Closed Noun Compounds Closed compounds consist of at least of two words, contracted together to form a compound"
2020.lrec-1.537,W17-1722,0,0.282165,"Missing"
2020.lrec-1.539,P14-1023,0,0.294071,"space models in some way, regarding the composite functions to combine the constituent vectors (Reddy et al., 2011b); or regarding the translations of compounds and constituents into multiple languages (Salehi et al., 2014); or regarding the contributions of modifiers and heads (Schulte im Walde et al., 2016); etc. What is still lacking, however, is a systematic assessment of the effect of vector-space reductions on the quality of predicting compositionality: Bullinaria and Levy (2012) explored the effect of Singular Value Decomposition (SVD) on semantics in vector spaces in general; and from Baroni et al. (2014b) and Levy et al. (2015) –among many others– we know that word embeddings provide a useful low-dimensional representation for vector spaces. But as to our knowledge, up to date only Salehi et al. (2015a) and Cordeiro et al. (2019) integrated vector-space reductions (in the form of word embeddings) into their computational prediction of noun compound compositionality, and Schulte im Walde et al. (2013) explored part-of-speech-based reductions in combination with frequency effects. Our contribution in this paper is to provide a systematic evaluation of vector-space reductions across kinds, i.e."
2020.lrec-1.539,W15-0903,1,0.832368,"is a crucial ingredient for lexicography and Natural Language Processing (NLP) applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. Compare, for example, the English noun compounds snowball –a ball consisting of snow, where clearly both constituents snow and ball contribute to the meaning of the compound– and butterfly –where the semantic contribution of the modifier noun butter is not obvious without knowing about the etymology of the compound. Studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) are examples of NLP applications that have integrated the prediction of multi-word compositionality into statistical machine translation. Accordingly, the field has witnessed a rich amount of computational approaches to automatically predict the degree of compositionality of noun compounds. These approaches typically represent compounds and their constituents within a vector space, and then compare the compound vectors with the constituent vectors as a proxy to the compounds’ degree of compositionality (Reddy et al., 2011b; Reddy et al., 2011a; Salehi and Cook, 2013"
2020.lrec-1.539,D14-1024,0,0.0191346,"pounds (and multi-word expressions in more general) is a crucial ingredient for lexicography and Natural Language Processing (NLP) applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. Compare, for example, the English noun compounds snowball –a ball consisting of snow, where clearly both constituents snow and ball contribute to the meaning of the compound– and butterfly –where the semantic contribution of the modifier noun butter is not obvious without knowing about the etymology of the compound. Studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) are examples of NLP applications that have integrated the prediction of multi-word compositionality into statistical machine translation. Accordingly, the field has witnessed a rich amount of computational approaches to automatically predict the degree of compositionality of noun compounds. These approaches typically represent compounds and their constituents within a vector space, and then compare the compound vectors with the constituent vectors as a proxy to the compounds’ degree of compositionality (Reddy et al., 2011b; R"
2020.lrec-1.539,Q15-1016,0,0.0351836,"regarding the composite functions to combine the constituent vectors (Reddy et al., 2011b); or regarding the translations of compounds and constituents into multiple languages (Salehi et al., 2014); or regarding the contributions of modifiers and heads (Schulte im Walde et al., 2016); etc. What is still lacking, however, is a systematic assessment of the effect of vector-space reductions on the quality of predicting compositionality: Bullinaria and Levy (2012) explored the effect of Singular Value Decomposition (SVD) on semantics in vector spaces in general; and from Baroni et al. (2014b) and Levy et al. (2015) –among many others– we know that word embeddings provide a useful low-dimensional representation for vector spaces. But as to our knowledge, up to date only Salehi et al. (2015a) and Cordeiro et al. (2019) integrated vector-space reductions (in the form of word embeddings) into their computational prediction of noun compound compositionality, and Schulte im Walde et al. (2013) explored part-of-speech-based reductions in combination with frequency effects. Our contribution in this paper is to provide a systematic evaluation of vector-space reductions across kinds, i.e., exploring part-of-speec"
2020.lrec-1.539,N13-1090,0,0.0210443,"gular Value Decomposition (SVD) to reduce the dimensionality of the whole matrix and the matrices containing only noun dimensions. With this PCA-using-SVD method, our matrix M was first decomposed into three matrices: M = U ΣW T (i.e., performing Singular Value Decomposition). Then, when reducing the number of dimensions to k, we sliced U to the first k rows, Σ to the top-left k × k matrix, and W T to the first k columns. Multiplying the three matrices provided a new matrix with less dimensions than previously in M . • W ORD 2V EC We trained a standard word2vec two-layer neural network model (Mikolov et al., 2013) on the ENCOW16 corpus with window size 10 to obtain 300-dimensional word vectors for our compounds and constituents. 4.2. 4.3. In order to zoom into specific strengths of individual vector space variants, we apply the variants to subsets of our compound targets according to the targets’ • • • • WORD1 use only the compound–modifier cosine score WORD2 use only the compound–head cosine score ADD add the compound–modifier and compound–head cosine scores MULT multiply the compound–modifier and compound– head cosine scores COMB add the compound–modifier, the compound–head and the multiplication of"
2020.lrec-1.539,I11-1079,0,0.155241,"kov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) are examples of NLP applications that have integrated the prediction of multi-word compositionality into statistical machine translation. Accordingly, the field has witnessed a rich amount of computational approaches to automatically predict the degree of compositionality of noun compounds. These approaches typically represent compounds and their constituents within a vector space, and then compare the compound vectors with the constituent vectors as a proxy to the compounds’ degree of compositionality (Reddy et al., 2011b; Reddy et al., 2011a; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014; Schulte im Walde et al., 2016; Cordeiro et al., 2019). Most of the approaches focus on English and German; most recently, Cordeiro et al. (2019) applied their framework to also French and Portuguese. All of the above-mentioned approaches explored variants of vector space models in some way, regarding the composite functions to combine the constituent vectors (Reddy et al., 2011b); or regarding the translations of compounds and constituents into multiple languages (Salehi et al., 2014); or regardi"
2020.lrec-1.539,I11-1024,0,0.231117,"kov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) are examples of NLP applications that have integrated the prediction of multi-word compositionality into statistical machine translation. Accordingly, the field has witnessed a rich amount of computational approaches to automatically predict the degree of compositionality of noun compounds. These approaches typically represent compounds and their constituents within a vector space, and then compare the compound vectors with the constituent vectors as a proxy to the compounds’ degree of compositionality (Reddy et al., 2011b; Reddy et al., 2011a; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014; Schulte im Walde et al., 2016; Cordeiro et al., 2019). Most of the approaches focus on English and German; most recently, Cordeiro et al. (2019) applied their framework to also French and Portuguese. All of the above-mentioned approaches explored variants of vector space models in some way, regarding the composite functions to combine the constituent vectors (Reddy et al., 2011b); or regarding the translations of compounds and constituents into multiple languages (Salehi et al., 2014); or regardi"
2020.lrec-1.539,S13-1039,0,0.0278899,"4), Cap et al. (2015), and Salehi et al. (2015b) are examples of NLP applications that have integrated the prediction of multi-word compositionality into statistical machine translation. Accordingly, the field has witnessed a rich amount of computational approaches to automatically predict the degree of compositionality of noun compounds. These approaches typically represent compounds and their constituents within a vector space, and then compare the compound vectors with the constituent vectors as a proxy to the compounds’ degree of compositionality (Reddy et al., 2011b; Reddy et al., 2011a; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014; Schulte im Walde et al., 2016; Cordeiro et al., 2019). Most of the approaches focus on English and German; most recently, Cordeiro et al. (2019) applied their framework to also French and Portuguese. All of the above-mentioned approaches explored variants of vector space models in some way, regarding the composite functions to combine the constituent vectors (Reddy et al., 2011b); or regarding the translations of compounds and constituents into multiple languages (Salehi et al., 2014); or regarding the contributions of modifiers and heads ("
2020.lrec-1.539,E14-1050,0,0.109746,"xamples of NLP applications that have integrated the prediction of multi-word compositionality into statistical machine translation. Accordingly, the field has witnessed a rich amount of computational approaches to automatically predict the degree of compositionality of noun compounds. These approaches typically represent compounds and their constituents within a vector space, and then compare the compound vectors with the constituent vectors as a proxy to the compounds’ degree of compositionality (Reddy et al., 2011b; Reddy et al., 2011a; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014; Schulte im Walde et al., 2016; Cordeiro et al., 2019). Most of the approaches focus on English and German; most recently, Cordeiro et al. (2019) applied their framework to also French and Portuguese. All of the above-mentioned approaches explored variants of vector space models in some way, regarding the composite functions to combine the constituent vectors (Reddy et al., 2011b); or regarding the translations of compounds and constituents into multiple languages (Salehi et al., 2014); or regarding the contributions of modifiers and heads (Schulte im Walde et al., 2016); etc. What is still l"
2020.lrec-1.539,N15-1099,0,0.0861798,"t for lexicography and Natural Language Processing (NLP) applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. Compare, for example, the English noun compounds snowball –a ball consisting of snow, where clearly both constituents snow and ball contribute to the meaning of the compound– and butterfly –where the semantic contribution of the modifier noun butter is not obvious without knowing about the etymology of the compound. Studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) are examples of NLP applications that have integrated the prediction of multi-word compositionality into statistical machine translation. Accordingly, the field has witnessed a rich amount of computational approaches to automatically predict the degree of compositionality of noun compounds. These approaches typically represent compounds and their constituents within a vector space, and then compare the compound vectors with the constituent vectors as a proxy to the compounds’ degree of compositionality (Reddy et al., 2011b; Reddy et al., 2011a; Salehi and Cook, 2013; Schulte im Walde et al."
2020.lrec-1.539,W15-0909,0,0.0949781,"t for lexicography and Natural Language Processing (NLP) applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. Compare, for example, the English noun compounds snowball –a ball consisting of snow, where clearly both constituents snow and ball contribute to the meaning of the compound– and butterfly –where the semantic contribution of the modifier noun butter is not obvious without knowing about the etymology of the compound. Studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) are examples of NLP applications that have integrated the prediction of multi-word compositionality into statistical machine translation. Accordingly, the field has witnessed a rich amount of computational approaches to automatically predict the degree of compositionality of noun compounds. These approaches typically represent compounds and their constituents within a vector space, and then compare the compound vectors with the constituent vectors as a proxy to the compounds’ degree of compositionality (Reddy et al., 2011b; Reddy et al., 2011a; Salehi and Cook, 2013; Schulte im Walde et al."
2020.lrec-1.539,schafer-bildhauer-2012-building,0,0.0409069,"Missing"
2020.lrec-1.539,S13-1038,1,0.807348,"Missing"
2020.lrec-1.539,S16-2020,1,0.678275,"Missing"
2020.lrec-1.539,W14-5709,1,0.87758,"Missing"
2020.lrec-1.540,W16-4702,0,0.0224576,"resents an important basis for further Natural Language Processing (NLP) tasks, such as thesaurus creation, automatic translation, and, in general, for domain knowledge acquisition and comprehension. Approaches for automatic term extraction can broadly be classified into four categories: linguistic (Justeson and Katz, 1995; Basili et al., 1997), statistical (Schäfer et al., 2015), hybrid (Frantzi et al., 1998; Maynard and Ananiadou, 1999) and machinelearning approaches (da Silva Conrado et al., 2013). Recently, word vector and deep learning approaches have emerged (Zadeh and Handschuh, 2014b; Amjadian et al., 2016; Wang et al., 2016). A niche of approaches is represented by graph-based statistical methods. In particular, we only find a handful of ideas for how to exploit the PageRank algorithm for automatic term extraction. The underlying motivation is that graph-based ranking algorithms –relying on graphs whose vertices represent the vocabulary of a corpus and whose edges represent some relationship between the words in the vocabulary– have the potential to inform about the relative importance of vertices in a graph, and that important vertices are likely to represent terms. For example, Mihalcea and"
2020.lrec-1.540,W97-0314,0,0.428924,"entations, Meaning Shifts 1. Introduction Terms are linguistic expressions which characterize a domain, i.e., words and phrases which are typical of a document or a corpus from a specific domain (in contrast to general language usage). The automatic recognition of terms represents an important basis for further Natural Language Processing (NLP) tasks, such as thesaurus creation, automatic translation, and, in general, for domain knowledge acquisition and comprehension. Approaches for automatic term extraction can broadly be classified into four categories: linguistic (Justeson and Katz, 1995; Basili et al., 1997), statistical (Schäfer et al., 2015), hybrid (Frantzi et al., 1998; Maynard and Ananiadou, 1999) and machinelearning approaches (da Silva Conrado et al., 2013). Recently, word vector and deep learning approaches have emerged (Zadeh and Handschuh, 2014b; Amjadian et al., 2016; Wang et al., 2016). A niche of approaches is represented by graph-based statistical methods. In particular, we only find a handful of ideas for how to exploit the PageRank algorithm for automatic term extraction. The underlying motivation is that graph-based ranking algorithms –relying on graphs whose vertices represent t"
2020.lrec-1.540,N13-2003,0,0.0319816,"t or a corpus from a specific domain (in contrast to general language usage). The automatic recognition of terms represents an important basis for further Natural Language Processing (NLP) tasks, such as thesaurus creation, automatic translation, and, in general, for domain knowledge acquisition and comprehension. Approaches for automatic term extraction can broadly be classified into four categories: linguistic (Justeson and Katz, 1995; Basili et al., 1997), statistical (Schäfer et al., 2015), hybrid (Frantzi et al., 1998; Maynard and Ananiadou, 1999) and machinelearning approaches (da Silva Conrado et al., 2013). Recently, word vector and deep learning approaches have emerged (Zadeh and Handschuh, 2014b; Amjadian et al., 2016; Wang et al., 2016). A niche of approaches is represented by graph-based statistical methods. In particular, we only find a handful of ideas for how to exploit the PageRank algorithm for automatic term extraction. The underlying motivation is that graph-based ranking algorithms –relying on graphs whose vertices represent the vocabulary of a corpus and whose edges represent some relationship between the words in the vocabulary– have the potential to inform about the relative impo"
2020.lrec-1.540,W11-1106,0,0.0605902,"Missing"
2020.lrec-1.540,S19-1001,1,0.827341,"traction. We go beyond standard cooccurrence and investigate the influence of measures of association strength (Evert, 2005) and first- vs. second-order co-occurrence (Rapp, 2002; Sahlgren, 2006; Schlechtweg et al., 2019) as basis for connecting vocabulary words in domain-specific and general-language graphs with varying types of edge weights. Our study is performed for the domain-specific ACL corpus and a do-it-yourself (DIY) corpus for English, and a domain-specific German cooking corpus. For the German term extraction we further integrate meaning shift values suggested in a previous study (Hätty et al., 2019) to create personalized representations and to thus distinguish between termhood strengths of ambiguous words across word senses. 2. Related Work The term extraction in Mihalcea and Tarau (2004) was probably the first approach to apply the PageRank algorithm. In a graph-based ranking model the terms extracted from the natural language text with a co-occurrence value within an n-sized window are represented as an undirected graph with nodes as single terms and co-occurrence values as unweighted edges. In their implementation the larger the text, the more terms were found. Personalized PageRank"
2020.lrec-1.540,W04-3252,0,0.722673,"et al., 2016; Wang et al., 2016). A niche of approaches is represented by graph-based statistical methods. In particular, we only find a handful of ideas for how to exploit the PageRank algorithm for automatic term extraction. The underlying motivation is that graph-based ranking algorithms –relying on graphs whose vertices represent the vocabulary of a corpus and whose edges represent some relationship between the words in the vocabulary– have the potential to inform about the relative importance of vertices in a graph, and that important vertices are likely to represent terms. For example, Mihalcea and Tarau (2004) used co-occurrence counts as edge weights in a graph; Khan et al. (2016) used embedding similarity for the same purpose. Zhanga et al. (2017) incorporated semantic relatedness via a personalized PageRank algorithm for extracting terms from the corpus. In this study, we present a comparison of vector space representations within a PageRank graph algorithm for automatic term extraction. We go beyond standard cooccurrence and investigate the influence of measures of association strength (Evert, 2005) and first- vs. second-order co-occurrence (Rapp, 2002; Sahlgren, 2006; Schlechtweg et al., 2019)"
2020.lrec-1.540,C02-1007,0,0.374797,"present terms. For example, Mihalcea and Tarau (2004) used co-occurrence counts as edge weights in a graph; Khan et al. (2016) used embedding similarity for the same purpose. Zhanga et al. (2017) incorporated semantic relatedness via a personalized PageRank algorithm for extracting terms from the corpus. In this study, we present a comparison of vector space representations within a PageRank graph algorithm for automatic term extraction. We go beyond standard cooccurrence and investigate the influence of measures of association strength (Evert, 2005) and first- vs. second-order co-occurrence (Rapp, 2002; Sahlgren, 2006; Schlechtweg et al., 2019) as basis for connecting vocabulary words in domain-specific and general-language graphs with varying types of edge weights. Our study is performed for the domain-specific ACL corpus and a do-it-yourself (DIY) corpus for English, and a domain-specific German cooking corpus. For the German term extraction we further integrate meaning shift values suggested in a previous study (Hätty et al., 2019) to create personalized representations and to thus distinguish between termhood strengths of ambiguous words across word senses. 2. Related Work The term extr"
2020.lrec-1.540,W19-4803,1,0.879958,"Missing"
2020.lrec-1.540,schulte-im-walde-2010-comparing,1,0.818864,"Missing"
2020.lrec-1.540,J98-1004,0,0.610303,"MI representations the co-occurrence counts in each matrix cell Mi,j are weighted by the local mutual information of target wi and context cj reflecting their degree of association (Evert, 2005). The values of the transformed matrix are association resulting from how matrix M is created. First-order Association: The matrix M is constructed between the nearby words within a n-sized window, as described above. Second-order Association The matrix M is constructed between context words of nearby words within a n-sized window. The idea for second-order co-occurrence vectors was first introduced by Schütze (1998) for word sense discrimination and has since then been extended and applied to a variety of tasks (Rapp, 2002; Sahlgren, 2006; Schulte im Walde, 2010; Zhuang et al., 2018; Schlechtweg et al., 2019). The basic idea is to represent a word w not by a vector of the counts of context words it directly co-occurs with, but instead by a count vector of the context words of the context words. The second-order co-occurrence provides the list of words which are contextually similar but not directly related within the corpus, i.e. context words sharing common list of words within a given threshold, theref"
2020.lrec-1.540,U16-1011,0,0.0132669,"sis for further Natural Language Processing (NLP) tasks, such as thesaurus creation, automatic translation, and, in general, for domain knowledge acquisition and comprehension. Approaches for automatic term extraction can broadly be classified into four categories: linguistic (Justeson and Katz, 1995; Basili et al., 1997), statistical (Schäfer et al., 2015), hybrid (Frantzi et al., 1998; Maynard and Ananiadou, 1999) and machinelearning approaches (da Silva Conrado et al., 2013). Recently, word vector and deep learning approaches have emerged (Zadeh and Handschuh, 2014b; Amjadian et al., 2016; Wang et al., 2016). A niche of approaches is represented by graph-based statistical methods. In particular, we only find a handful of ideas for how to exploit the PageRank algorithm for automatic term extraction. The underlying motivation is that graph-based ranking algorithms –relying on graphs whose vertices represent the vocabulary of a corpus and whose edges represent some relationship between the words in the vocabulary– have the potential to inform about the relative importance of vertices in a graph, and that important vertices are likely to represent terms. For example, Mihalcea and Tarau (2004) used co"
2020.lrec-1.540,W14-4807,0,0.0707358,"tic recognition of terms represents an important basis for further Natural Language Processing (NLP) tasks, such as thesaurus creation, automatic translation, and, in general, for domain knowledge acquisition and comprehension. Approaches for automatic term extraction can broadly be classified into four categories: linguistic (Justeson and Katz, 1995; Basili et al., 1997), statistical (Schäfer et al., 2015), hybrid (Frantzi et al., 1998; Maynard and Ananiadou, 1999) and machinelearning approaches (da Silva Conrado et al., 2013). Recently, word vector and deep learning approaches have emerged (Zadeh and Handschuh, 2014b; Amjadian et al., 2016; Wang et al., 2016). A niche of approaches is represented by graph-based statistical methods. In particular, we only find a handful of ideas for how to exploit the PageRank algorithm for automatic term extraction. The underlying motivation is that graph-based ranking algorithms –relying on graphs whose vertices represent the vocabulary of a corpus and whose edges represent some relationship between the words in the vocabulary– have the potential to inform about the relative importance of vertices in a graph, and that important vertices are likely to represent terms. Fo"
2020.lrec-1.540,zadeh-handschuh-2014-evaluation,0,0.424166,"tic recognition of terms represents an important basis for further Natural Language Processing (NLP) tasks, such as thesaurus creation, automatic translation, and, in general, for domain knowledge acquisition and comprehension. Approaches for automatic term extraction can broadly be classified into four categories: linguistic (Justeson and Katz, 1995; Basili et al., 1997), statistical (Schäfer et al., 2015), hybrid (Frantzi et al., 1998; Maynard and Ananiadou, 1999) and machinelearning approaches (da Silva Conrado et al., 2013). Recently, word vector and deep learning approaches have emerged (Zadeh and Handschuh, 2014b; Amjadian et al., 2016; Wang et al., 2016). A niche of approaches is represented by graph-based statistical methods. In particular, we only find a handful of ideas for how to exploit the PageRank algorithm for automatic term extraction. The underlying motivation is that graph-based ranking algorithms –relying on graphs whose vertices represent the vocabulary of a corpus and whose edges represent some relationship between the words in the vocabulary– have the potential to inform about the relative importance of vertices in a graph, and that important vertices are likely to represent terms. Fo"
2020.lrec-1.540,D18-1057,0,0.0236414,"of association (Evert, 2005). The values of the transformed matrix are association resulting from how matrix M is created. First-order Association: The matrix M is constructed between the nearby words within a n-sized window, as described above. Second-order Association The matrix M is constructed between context words of nearby words within a n-sized window. The idea for second-order co-occurrence vectors was first introduced by Schütze (1998) for word sense discrimination and has since then been extended and applied to a variety of tasks (Rapp, 2002; Sahlgren, 2006; Schulte im Walde, 2010; Zhuang et al., 2018; Schlechtweg et al., 2019). The basic idea is to represent a word w not by a vector of the counts of context words it directly co-occurs with, but instead by a count vector of the context words of the context words. The second-order co-occurrence provides the list of words which are contextually similar but not directly related within the corpus, i.e. context words sharing common list of words within a given threshold, therefore allowing to measure contextual similarity between higher order co-occurrences within the corpus. The second-order co-occurrence vectors are considered less sparse and"
2020.lrec-1.859,baroni-etal-2008-cleaneval,0,0.027994,"gchen, 2019). Over the years, there have been various attempts to clean corpora for both specific and general use in NLP with some contributions aiming to automate the process (Reynaert, 2006). In the field of machine translation, Imamura and Sumita (2002) present a method for cleaning bilingual corpora based on translation literality as measured by word-level and phrase-level correspondence in sentence pairs. As for more general applications, the special interest group of the Association for Computational Linguistics (ACL) on the Web as Corpus (ACL SIGWAC) released the shared task CLEANEVAL (Baroni et al., 2008), which aimed to clean web data for use as corpora in NLP. More recent efforts include Gra¨en et al. (2014) who cleaned the Europarl Corpus, a collection of the European Parliament’s debates. Similarly, Faaß and Eckart (2013) cleaned the German web corpus deWaC of the WaCky project (Baroni et al., 2009). Our work is close to that of Faaß and Eckart as we adopt a similar approach that requires several passes over the data with a measure to test the corpus quality. 6958 3. COHA The Corpus of Historical American English (COHA), developed by Brigham Young University, is a structured collection of"
2020.lrec-1.859,P04-3031,0,0.246734,"d to clean the annotated format first and then generate the dependent parts of the other formats using the cleaned corpus. Accordingly, the steps described in this section were performed on the annotated corpus. Annotated data CCOHA Clean data Pass 1 Pass 2 – – – – – – – – – – – – – – – – – – Evaluation – – – – – – – – – Cleaning Algorithm Adjust By the same is assied to Summer for the placid. Annotated Text Figure 2: Diagram of the annotated corpus clean-up. 4.1. Annotated Corpus clean-up The corpus clean-up was implemented using Python (Rossum, 1995) and the natural language toolkit (NLTK) (Bird and Loper, 2004). Specifically, the NLTK “Averaged Perceptron Tagger” was used to tag tokens, and NLTK “Punkt Sentence Tokenizer” was used to segment the data into sentences. The cleaning process, illustrated in Figure 2, was performed iteratively such that data were first cleaned and then manually evaluated. Based on the results of the evaluation, the cleaning algorithm would be updated and a new iteration would start where the original annotated corpus is cleaned and then evaluated. The cycle is repeated until the results of the evaluation reveal that no further improvements are needed. We explain the clean"
2020.lrec-1.859,W19-4706,0,0.0197824,"rs such as cultural changes and technological advances (Blank, 1999; Fromkin et al., 2018). The field of historical or diachronic linguistics is concerned with the study and analysis of language change over time. Over the past two decades, researchers have shown an increased interest in the various aspects of diachronic language change. This can be attributed to the advances in technology such as the digitization of historical texts, improved computational power and availability of large-scale historical corpora designed specifically for diachronic studies (Tahmasebi et al., 2018; Tang, 2018; Bowern, 2019). Large historical corpora first appeared a decade ago and quickly gained popularity because they allow researchers to test hypotheses using computational approaches that are only possible with corpora of such volume (Kutuzov et al., 2018; Dubossarsky et al., 2019; Perrone et al., 2019; Schlechtweg et al., 2019). The Corpus of Historical American English (COHA) (Davies, 2012) is a popular large-scale resource for studying lexical, syntactic and semantic change in English. Despite its many features and advantages, COHA is not without its limitations. These shortcomings, which include inconsiste"
2020.lrec-1.859,P19-1044,1,0.804653,"ave shown an increased interest in the various aspects of diachronic language change. This can be attributed to the advances in technology such as the digitization of historical texts, improved computational power and availability of large-scale historical corpora designed specifically for diachronic studies (Tahmasebi et al., 2018; Tang, 2018; Bowern, 2019). Large historical corpora first appeared a decade ago and quickly gained popularity because they allow researchers to test hypotheses using computational approaches that are only possible with corpora of such volume (Kutuzov et al., 2018; Dubossarsky et al., 2019; Perrone et al., 2019; Schlechtweg et al., 2019). The Corpus of Historical American English (COHA) (Davies, 2012) is a popular large-scale resource for studying lexical, syntactic and semantic change in English. Despite its many features and advantages, COHA is not without its limitations. These shortcomings, which include inconsistent lemmas and malformed tokens, can complicate certain tasks and increase the required time and effort to complete them. As a case in point, let us consider the original task for which we needed COHA. The task required sentence-level context extraction for a set o"
2020.lrec-1.859,E03-1076,0,0.109138,"ting corpus CCOHA offers more word tokens, less non-words, and less invalid tokens than the original COHA. While the annotated and linear text formats are available in CCOHA, the database format should be generated by interested parties. In conclusion, we discuss some of the possible improvements and steps that can be taken to further clean the corpus. First, malformed tokens that contain the pattern “P1X1 X2 ” may be cleaned using regular expressions. 6964 Second, malformed tokens that consist of one or more words could be cleaned using one of the many approaches for compound word splitting (Koehn and Knight, 2003; Norvig, 2009; Macherey et al., 2011). Third, if one wishes to use the more fine-grained POS tags of CLAWS7, it is feasible to extract tokens tagged using the coarse-grained tags and then retag them using CLAWS tagger or some heuristics. Last, by following the steps in Section 4.3. the database format of the clean corpus can be generated from the annotated data. 7. Acknowledgements We especially thank Mark Davies for his comments during the cleaning process. We would also like to thank our reviewers for their insightful feedback. The first and second authors were supported by the CRETA center"
2020.lrec-1.859,C18-1117,0,0.137912,"decades, researchers have shown an increased interest in the various aspects of diachronic language change. This can be attributed to the advances in technology such as the digitization of historical texts, improved computational power and availability of large-scale historical corpora designed specifically for diachronic studies (Tahmasebi et al., 2018; Tang, 2018; Bowern, 2019). Large historical corpora first appeared a decade ago and quickly gained popularity because they allow researchers to test hypotheses using computational approaches that are only possible with corpora of such volume (Kutuzov et al., 2018; Dubossarsky et al., 2019; Perrone et al., 2019; Schlechtweg et al., 2019). The Corpus of Historical American English (COHA) (Davies, 2012) is a popular large-scale resource for studying lexical, syntactic and semantic change in English. Despite its many features and advantages, COHA is not without its limitations. These shortcomings, which include inconsistent lemmas and malformed tokens, can complicate certain tasks and increase the required time and effort to complete them. As a case in point, let us consider the original task for which we needed COHA. The task required sentence-level cont"
2020.lrec-1.859,P11-1140,0,0.021557,"ens, less non-words, and less invalid tokens than the original COHA. While the annotated and linear text formats are available in CCOHA, the database format should be generated by interested parties. In conclusion, we discuss some of the possible improvements and steps that can be taken to further clean the corpus. First, malformed tokens that contain the pattern “P1X1 X2 ” may be cleaned using regular expressions. 6964 Second, malformed tokens that consist of one or more words could be cleaned using one of the many approaches for compound word splitting (Koehn and Knight, 2003; Norvig, 2009; Macherey et al., 2011). Third, if one wishes to use the more fine-grained POS tags of CLAWS7, it is feasible to extract tokens tagged using the coarse-grained tags and then retag them using CLAWS tagger or some heuristics. Last, by following the steps in Section 4.3. the database format of the clean corpus can be generated from the annotated data. 7. Acknowledgements We especially thank Mark Davies for his comments during the cleaning process. We would also like to thank our reviewers for their insightful feedback. The first and second authors were supported by the CRETA center funded by the German Ministry for Edu"
2020.lrec-1.859,H94-1020,0,0.454512,"read and split into sentences using NLTK Punkt sentence tokenizer. Next, all occurrences of the ‘NUL’ control character in the lemma field were replaced with the special string “<nul>”. Then, all tokens away from sentence boundaries where the lemma was either “<nul>” or “<temp>” were tagged and lemmatized given the full sentence as context. The only exception was the special token “@” which has a “<nul>” lemma. Similarly all tokens where the POS tag was “<nul>” were tagged and lemmatized in the same fashion. Considering that the NLTK “Averaged Perceptron Tagger” uses the Penn Treebank tagset (Marcus et al., 1994), the resulting POS tags were mapped to their CLAWS7 counterparts and appended with the special string “ <sub>” to help identify cleaned tokens. The mapping was manually created by the first author of this paper. In order to detect the malformed tokens around sentence boundaries, sentences were reconstructed using the NLTK segmentation results as a guide. Specifically, upon reading each token in the annotated file, it would be appended to a list of tokens that were not part of the previous NLTK sentence. This list or “partial sentence” was then compared to the current NLTK sentence and when th"
2020.lrec-1.859,W19-4707,0,0.0811786,"erest in the various aspects of diachronic language change. This can be attributed to the advances in technology such as the digitization of historical texts, improved computational power and availability of large-scale historical corpora designed specifically for diachronic studies (Tahmasebi et al., 2018; Tang, 2018; Bowern, 2019). Large historical corpora first appeared a decade ago and quickly gained popularity because they allow researchers to test hypotheses using computational approaches that are only possible with corpora of such volume (Kutuzov et al., 2018; Dubossarsky et al., 2019; Perrone et al., 2019; Schlechtweg et al., 2019). The Corpus of Historical American English (COHA) (Davies, 2012) is a popular large-scale resource for studying lexical, syntactic and semantic change in English. Despite its many features and advantages, COHA is not without its limitations. These shortcomings, which include inconsistent lemmas and malformed tokens, can complicate certain tasks and increase the required time and effort to complete them. As a case in point, let us consider the original task for which we needed COHA. The task required sentence-level context extraction for a set of target words, but wa"
2020.lrec-1.859,reynaert-2006-corpus,0,0.0714817,"next section, we describe the related work on data clean-up. Further, we give an overview of COHA and describe its features and limitations. Then, we discuss the approach taken to clean COHA and overcome its limitations in Section 4. The resulting clean corpus is presented and compared to the original corpus in Section 5. 2. Related Work Data clean-up is an essential yet time consuming process in research (Hill and Hengchen, 2019). Over the years, there have been various attempts to clean corpora for both specific and general use in NLP with some contributions aiming to automate the process (Reynaert, 2006). In the field of machine translation, Imamura and Sumita (2002) present a method for cleaning bilingual corpora based on translation literality as measured by word-level and phrase-level correspondence in sentence pairs. As for more general applications, the special interest group of the Association for Computational Linguistics (ACL) on the Web as Corpus (ACL SIGWAC) released the shared task CLEANEVAL (Baroni et al., 2008), which aimed to clean web data for use as corpora in NLP. More recent efforts include Gra¨en et al. (2014) who cleaned the Europarl Corpus, a collection of the European Pa"
2020.lrec-1.859,P19-1072,1,0.47303,"Missing"
2020.semeval-1.8,2020.lrec-1.859,1,0.876468,"Missing"
2020.semeval-1.8,P17-1042,0,0.0149686,"rs on the first model, as done by Ahmad et al. (2020). In this way, we expect to introduce considerably less noise to the vectors in the second corpus. Orthogonal Procrustes. SGNS is trained on each corpus separately, resulting in matrices A and B. To align them we follow Hamilton et al. (2016b) and calculate an orthogonally-constrained matrix W ∗ : W ∗ = arg min kBW − AkF (2) W ∈O(d) where the i-th row in matrices A and B correspond to the same word. Using W ∗ we get the aligned matrices AOP = A and B OP = BW ∗ . Prior to this alignment step we length-normalize and mean-center both matrices (Artetxe et al., 2017; Schlechtweg et al., 2019). Word Injection. The sentences of both corpora are shuffled into one joint corpus, but all occurrences of target words are substituted by the target word concatenated with a tag indicating the corpus it originated from (Ferrari et al., 2017; Schlechtweg et al., 2019). This leads to the creation of two vectors for each target word in one vector space, while non-target words receive only one vector encoding information from both corpora. This is very similar to Temporal Referencing (TR) (Dubossarsky et al., 2019), the difference being that with TR, the target-context"
2020.semeval-1.8,P19-1044,1,0.894486,"variety of proposed model architectures (Schlechtweg et al., 2020). An important component of high-performance LSC detection models is an alignment method to make semantic vector spaces comparable across time. In this paper we focus on a particular alignment method for type embeddings, Vector Initialization (VI), and how its performance interacts with vector dimensionality. We compare VI to two further state-of-the-art alignment methods, Orthogonal Procrustes (OP) and Word Injection (WI), which have shown high performance in previous studies (Hamilton et al., 2016b; Schlechtweg et al., 2019; Dubossarsky et al., 2019) and are also used in the top-ranking systems for Subtask 2. A systematic comparison of performance across dimensionalities d reveals that the optimal d of the models on the SemEval test data is lower than in standard choices, and that VI’s performance strongly depends on d, showing large drops for high dimensionalities. We demonstrate that this effect is correlated with the amount of frequency noise picked up by VI, i.e., the degree to which cosine distances between vectors reflect frequency differences between words rather than semantic differences. If properly tuned regarding dimensionaliti"
2020.semeval-1.8,D16-1229,0,0.106501,"multi-lingual evaluation framework to compare the variety of proposed model architectures (Schlechtweg et al., 2020). An important component of high-performance LSC detection models is an alignment method to make semantic vector spaces comparable across time. In this paper we focus on a particular alignment method for type embeddings, Vector Initialization (VI), and how its performance interacts with vector dimensionality. We compare VI to two further state-of-the-art alignment methods, Orthogonal Procrustes (OP) and Word Injection (WI), which have shown high performance in previous studies (Hamilton et al., 2016b; Schlechtweg et al., 2019; Dubossarsky et al., 2019) and are also used in the top-ranking systems for Subtask 2. A systematic comparison of performance across dimensionalities d reveals that the optimal d of the models on the SemEval test data is lower than in standard choices, and that VI’s performance strongly depends on d, showing large drops for high dimensionalities. We demonstrate that this effect is correlated with the amount of frequency noise picked up by VI, i.e., the degree to which cosine distances between vectors reflect frequency differences between words rather than semantic d"
2020.semeval-1.8,P16-1141,0,0.122502,"multi-lingual evaluation framework to compare the variety of proposed model architectures (Schlechtweg et al., 2020). An important component of high-performance LSC detection models is an alignment method to make semantic vector spaces comparable across time. In this paper we focus on a particular alignment method for type embeddings, Vector Initialization (VI), and how its performance interacts with vector dimensionality. We compare VI to two further state-of-the-art alignment methods, Orthogonal Procrustes (OP) and Word Injection (WI), which have shown high performance in previous studies (Hamilton et al., 2016b; Schlechtweg et al., 2019; Dubossarsky et al., 2019) and are also used in the top-ranking systems for Subtask 2. A systematic comparison of performance across dimensionalities d reveals that the optimal d of the models on the SemEval test data is lower than in standard choices, and that VI’s performance strongly depends on d, showing large drops for high dimensionalities. We demonstrate that this effect is correlated with the amount of frequency noise picked up by VI, i.e., the degree to which cosine distances between vectors reflect frequency differences between words rather than semantic d"
2020.semeval-1.8,2020.acl-main.258,1,0.773504,"Missing"
2020.semeval-1.8,P19-1379,0,0.161206,"ween dimensionality and noise: models with different susceptibilities to noise have typically been tested without varying the dimensionality (Hamilton et al., 2016b; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Shoemark et al., 2019). 3 System overview Most models in LSC detection combine three sub-systems: (i) creating semantic word representations, (ii) aligning them across corpora, and (iii) measuring differences between the aligned representations (Schlechtweg et al., 2019). Semantic representations can either be token-based, keeping one representation (e.g. a vector) per word use (Hu et al., 2019, e.g.), or type-based, collapsing information from different uses into one representation (Hamilton et al., 2016b, e.g.). The alignment step is needed mostly for vector space models, which might otherwise introduce arbitrary orthogonal transformations to the vector spaces they produce (Hamilton et al., 2016b). Our system focuses on the type-based Skip-gram Negative Sampling (SGNS) with Vector Initialisation alignment (VI) and Cosine Distance (CD). We chose this method due to its surprisingly good performance with d = 5 in a student shared-task project (Ahmad et al., 2020)1 and compare it to t"
2020.semeval-1.8,W14-2517,0,0.617006,"d no sub-sampling. Depending on corpus size we trained the model for either 5 (German, Swedish) or 30 epochs e (English, Latin).3 As we focus on the effect of dimensionality, each experiment was performed for each d ∈ {5, 10, 25, 50, 80, 150, 200, 250, 300, 350, 500, 750, 1000}. Prior to the shared task application we validated all models with these hyper-parameters on the German DURel dataset (Schlechtweg et al., 2018). 3.2 Alignment Vector Initialisation. In VI we first train the SGNS model on one corpus and then use these vectors to initialize the vectors for training on the second corpus (Kim et al., 2014). The motivation of this procedure is that if a word is used in similar contexts in both corpora, the second training step will not change the initial word vector much, while more different contexts will lead to a greater change of the vector. SGNS represents each word by two vectors, a word vector and a context vector. The former is modified when a word occurs as target w in a target-context pair (w, c), while the latter is modified when it occurs as context c. While Schlechtweg et al. (2019) only initialize the word vectors on the first model and context 1 These referenced results were achie"
2020.semeval-1.8,C18-1117,0,0.153603,"s for Subtask 2 and demonstrates that these can be outperformed if we optimize VI dimensionality. We demonstrate that differences in performance can largely be attributed to model-specific sources of noise, and we reveal a strong relationship between dimensionality and frequency-induced noise in VI alignment. Our results suggest that lexical semantic change models integrating vector space alignment should pay more attention to the role of the dimensionality parameter. 1 Introduction Lexical Semantic Change (LSC) Detection has drawn increasing attention in recent years (Tahmasebi et al., 2018; Kutuzov et al., 2018). SemEval-2020 Task 1 provides a multi-lingual evaluation framework to compare the variety of proposed model architectures (Schlechtweg et al., 2020). An important component of high-performance LSC detection models is an alignment method to make semantic vector spaces comparable across time. In this paper we focus on a particular alignment method for type embeddings, Vector Initialization (VI), and how its performance interacts with vector dimensionality. We compare VI to two further state-of-the-art alignment methods, Orthogonal Procrustes (OP) and Word Injection (WI), which have shown high p"
2020.semeval-1.8,Q15-1016,0,0.0497085,"rrences extracted from a corpus with a symmetric window. It represents each word w and each context c as a d-dimensional vector to solve X X arg max log σ(vc · vw ) + log σ(−vc · vw ), (1) θ (w,c)∈D0 (w,c)∈D where σ(x) = 1+e1−x , D is the set of all observed word-context pairs and D0 is the set of randomly generated negative samples (Mikolov et al., 2013a; Mikolov et al., 2013b; Goldberg and Levy, 2014). The optimized parameters θ are vwi and vci for i ∈ 1, ..., d. D0 is obtained by drawing k contexts from the empirical unigram distribution P (c) = #(c) |D |for each observation of (w, c), cf. Levy et al. (2015). After training, each word w is represented by its word vector vw . To keep our results comparable to previous research (Hamilton et al., 2016b; Schlechtweg et al., 2019) we chose common settings for most of the hyperparameters. We decided on a symmetrical context window of size 10, initial learning rate α of 0.025, number of negative samples k = 5 and no sub-sampling. Depending on corpus size we trained the model for either 5 (German, Swedish) or 30 epochs e (English, Latin).3 As we focus on the effect of dimensionality, each experiment was performed for each d ∈ {5, 10, 25, 50, 80, 150, 200"
2020.semeval-1.8,N18-2027,1,0.718909,"htweg et al., 2019) we chose common settings for most of the hyperparameters. We decided on a symmetrical context window of size 10, initial learning rate α of 0.025, number of negative samples k = 5 and no sub-sampling. Depending on corpus size we trained the model for either 5 (German, Swedish) or 30 epochs e (English, Latin).3 As we focus on the effect of dimensionality, each experiment was performed for each d ∈ {5, 10, 25, 50, 80, 150, 200, 250, 300, 350, 500, 750, 1000}. Prior to the shared task application we validated all models with these hyper-parameters on the German DURel dataset (Schlechtweg et al., 2018). 3.2 Alignment Vector Initialisation. In VI we first train the SGNS model on one corpus and then use these vectors to initialize the vectors for training on the second corpus (Kim et al., 2014). The motivation of this procedure is that if a word is used in similar contexts in both corpora, the second training step will not change the initial word vector much, while more different contexts will lead to a greater change of the vector. SGNS represents each word by two vectors, a word vector and a context vector. The former is modified when a word occurs as target w in a target-context pair (w, c"
2020.semeval-1.8,P19-1072,1,0.68902,"Missing"
2020.semeval-1.8,2020.semeval-1.1,1,0.862201,"Missing"
2020.semeval-1.8,D19-1007,0,0.577791,"(Online), December 12, 2020. representation method and alignment techniques. Consequently, a specific semantic representation learning algorithm (such as Skip-Gram with Negative Sampling) may have a different optimal dimensionality depending on the alignment technique it relies on. Up to now, previous research on LSC detection has not paid much attention to this relationship between dimensionality and noise: models with different susceptibilities to noise have typically been tested without varying the dimensionality (Hamilton et al., 2016b; Dubossarsky et al., 2019; Schlechtweg et al., 2019; Shoemark et al., 2019). 3 System overview Most models in LSC detection combine three sub-systems: (i) creating semantic word representations, (ii) aligning them across corpora, and (iii) measuring differences between the aligned representations (Schlechtweg et al., 2019). Semantic representations can either be token-based, keeping one representation (e.g. a vector) per word use (Hu et al., 2019, e.g.), or type-based, collapsing information from different uses into one representation (Hamilton et al., 2016b, e.g.). The alignment step is needed mostly for vector space models, which might otherwise introduce arbitrary"
2021.acl-long.543,D16-1250,0,0.0167507,"2020) and DIACR-Ita (Basile et al., 2020), we use the Skip-gram with Negative Sampling model (SGNS, Mikolov et al., 2013a,b) to create static word embeddings. SGNS is a shallow neural language model trained on pairs of word co-occurrences extracted from a corpus with a symmetric window. The optimized parameters can be interpreted as a semantic vector space that contains the word vectors for all words in the vocabulary. In our case, we obtain two separately trained vector spaces, one for each subcorpus (C1 and C2 ). Following standard practice, both spaces are length-normalized, mean-centered (Artetxe et al., 2016; Schlechtweg et al., 2019) and then aligned by applying Orthogonal Procrustes (OP), because columns from different vector spaces may not correspond to the same coordinate axes (Hamilton et al., 2016). The change between two time-specific embeddings is measured by calculating their Cosine Distance (CD) (Salton and McGill, 1983). The strength of SGNS+OP+CD has been shown in two recent shared tasks with this sub-system combination ranking among the best submissions (Arefyev and Zhikov, 2020; Kaiser et al., 2020b; P¨omsl and Lyapin, 2020; Praˇza´ k et al., 2020). 4.2 Token-based approach Bidirect"
2021.acl-long.543,N19-1423,0,0.00972412,"g Orthogonal Procrustes (OP), because columns from different vector spaces may not correspond to the same coordinate axes (Hamilton et al., 2016). The change between two time-specific embeddings is measured by calculating their Cosine Distance (CD) (Salton and McGill, 1983). The strength of SGNS+OP+CD has been shown in two recent shared tasks with this sub-system combination ranking among the best submissions (Arefyev and Zhikov, 2020; Kaiser et al., 2020b; P¨omsl and Lyapin, 2020; Praˇza´ k et al., 2020). 4.2 Token-based approach Bidirectional Encoder Representations from Transformers (BERT, Devlin et al., 2019) is a transformer-based neural language model designed to find contextualized representations for text by analyzing left and right contexts. The base version processes text in 12 different layers. In each layer, a contextualized token vector representation is created for every word. A layer, or a combination of multiple layers (we use the average), then serves as a representation for a token. For every target word we extract usages (i.e., sentences in which the word appears) by randomly sub-sampling up to 100 sentences from both subcorpora C1 and C2 .1 These are then fed into BERT to create co"
2021.acl-long.543,P19-1044,1,0.884283,"Missing"
2021.acl-long.543,D17-1118,0,0.0958213,"can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is,"
2021.acl-long.543,D19-1006,0,0.0250897,"ance (Dubossarsky et al., 2017; Schlechtweg and Schulte im Walde, 2020). 4 Models Type-based models generate a single vector for each word from a pre-defined vocabulary. In contrast, token-based models generate one vector for each usage of a word. While the former do not take into account that most words have multiple senses, the latter are able to capture this particular aspect and are thus presumably more suited for the task of LSCD (Martinc et al., 2020). Even though contextualized approaches have indeed significantly outperformed static approaches in several NLP tasks over the past years (Ethayarajh, 2019), the field of LSCD is still dominated by type-based models (Schlechtweg et al., 2020). Kutuzov and Giulianelli (2020) yet show that the performance of tokenbased models (especially ELMo) can be increased by fine-tuning on the target corpora. Laicher et al. (2020, 2021) drastically improve the performance of BERT by reducing the influence of target word morphology. In this paper, we compare both families of approaches for change discovery. 6986 4.1 Type-based approach Most type-based approaches in LSCD combine three sub-systems: (i) creating semantic word representations, (ii) aligning them ac"
2021.acl-long.543,Q16-1003,0,0.0215972,"ange. Furthermore, we provide an almost fully automated framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction of novel LSCD mode"
2021.acl-long.543,2020.acl-main.365,0,0.0284746,"Missing"
2021.acl-long.543,W11-2508,0,0.0169691,"et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotation process are Gulordava and Baroni (2011) and Cook et al. (2013). Gulordava and Baroni ask human annotators to rate 100 randomly sampled words on a 4-point scale from 0 (no change) to 3 (changed significantly), however without relating this to a data set. Cook et al. work closely with a professional lexicographer to inspect 20 lemmas predicted by their models plus 10 randomly selected ones. Gulordava and Baroni and Cook et al. evaluate their predictions on the (macro) lemma level. We, however, annotate our predictions on the (micro) usage level, enabling us to better control the criteria for annotation and their inter-subjectivity. I"
2021.acl-long.543,P16-1141,0,0.324875,"lished German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness o"
2021.acl-long.543,P19-1379,0,0.0131143,"en considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction of novel LSCD models, and on analyzing and evaluating existing models. Up to now, these preferences for development and analysis vs. app"
2021.acl-long.543,2021.eacl-main.10,1,0.752004,"Missing"
2021.acl-long.543,2020.semeval-1.8,1,0.859634,"C2 ). Following standard practice, both spaces are length-normalized, mean-centered (Artetxe et al., 2016; Schlechtweg et al., 2019) and then aligned by applying Orthogonal Procrustes (OP), because columns from different vector spaces may not correspond to the same coordinate axes (Hamilton et al., 2016). The change between two time-specific embeddings is measured by calculating their Cosine Distance (CD) (Salton and McGill, 1983). The strength of SGNS+OP+CD has been shown in two recent shared tasks with this sub-system combination ranking among the best submissions (Arefyev and Zhikov, 2020; Kaiser et al., 2020b; P¨omsl and Lyapin, 2020; Praˇza´ k et al., 2020). 4.2 Token-based approach Bidirectional Encoder Representations from Transformers (BERT, Devlin et al., 2019) is a transformer-based neural language model designed to find contextualized representations for text by analyzing left and right contexts. The base version processes text in 12 different layers. In each layer, a contextualized token vector representation is created for every word. A layer, or a combination of multiple layers (we use the average), then serves as a representation for a token. For every target word we extract usages (i."
2021.acl-long.543,C18-1117,0,0.0170949,"of existing models. In this paper, we propose a shift of focus from change detection to change discovery, i.e., discovering novel word senses over time from the full corpus vocabulary. By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks"
2021.acl-long.543,2021.eacl-srw.25,1,0.832016,"Missing"
2021.acl-long.543,W14-2517,0,0.181317,"r time from the full corpus vocabulary. By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a ver"
2021.acl-long.543,2020.semeval-1.14,0,0.0605088,"ate a single vector for each word from a pre-defined vocabulary. In contrast, token-based models generate one vector for each usage of a word. While the former do not take into account that most words have multiple senses, the latter are able to capture this particular aspect and are thus presumably more suited for the task of LSCD (Martinc et al., 2020). Even though contextualized approaches have indeed significantly outperformed static approaches in several NLP tasks over the past years (Ethayarajh, 2019), the field of LSCD is still dominated by type-based models (Schlechtweg et al., 2020). Kutuzov and Giulianelli (2020) yet show that the performance of tokenbased models (especially ELMo) can be increased by fine-tuning on the target corpora. Laicher et al. (2020, 2021) drastically improve the performance of BERT by reducing the influence of target word morphology. In this paper, we compare both families of approaches for change discovery. 6986 4.1 Type-based approach Most type-based approaches in LSCD combine three sub-systems: (i) creating semantic word representations, (ii) aligning them across corpora, and (iii) measuring differences between the aligned representations (Schlechtweg et al., 2019). Motivate"
2021.acl-long.543,2021.naacl-main.369,0,0.0262622,". ‘It is true that with the emergence of the manufactory, in contrast to the handicraft, traces of child labor are showing.’ (2) Sie wissen, daß wir f¨ur das Vieh mehr Futter aus eigenem Aufkommen brauchen. ‘They know that we need more feed from our own production for the cattle.’ The annotated data of a word is represented in a Word Usage Graph (WUG), where vertices represent word usages, and weights on edges represent 3 In a practical setting where predictions have to be generated only once, a much larger number may be chosen. Also, possibilities to scale up BERT performance can be applied (Montariol et al., 2021). 6988 C1 full C2 Figure 1: Word Usage Graph of German Aufkommen (left), subgraphs for first time period C1 (middle) and for second time period C2 (right). black/gray lines indicate high/low edge weights. the (median) semantic relatedness judgment of a pair of usages such as (1) and (2). The final WUGs are clustered with a variation of correlation clustering (Bansal et al., 2004; Schlechtweg et al., 2020) (see Figure 1, left) and split into two subgraphs representing nodes from subcorpora C1 and C2 , respectively (middle and right). Clusters are then interpreted as word senses and changes in c"
2021.acl-long.543,W19-4707,0,0.0189371,"elated Work State-of-the-art semantic change detection models are Vector Space Models (VSMs) (Schlechtweg et al., 2020). These can be divided into type-based (static) (Turney and Pantel, 2010) and token-based (contextualized) (Sch¨utze, 1998) approaches. For our study, we use both a static and a contextualized model. As mentioned above, previous work mostly focuses on creating data sets or developing, evaluating and analyzing models. A common approach for evaluation is to annotate target words selected from dictionaries in specific corpora (Tahmasebi and Risse, 2017; Schlechtweg et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotation process are Gulordava and Baroni (2011) and Co"
2021.acl-long.543,2020.semeval-1.21,0,0.0234657,"Missing"
2021.acl-long.543,2020.coling-main.90,0,0.0280782,"e detection models are Vector Space Models (VSMs) (Schlechtweg et al., 2020). These can be divided into type-based (static) (Turney and Pantel, 2010) and token-based (contextualized) (Sch¨utze, 1998) approaches. For our study, we use both a static and a contextualized model. As mentioned above, previous work mostly focuses on creating data sets or developing, evaluating and analyzing models. A common approach for evaluation is to annotate target words selected from dictionaries in specific corpora (Tahmasebi and Risse, 2017; Schlechtweg et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotation process are Gulordava and Baroni (2011) and Cook et al. (2013). Gulordava and Baroni ask huma"
2021.acl-long.543,N18-1044,0,0.0178523,"de an almost fully automated framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction of novel LSCD models, and on analyzing and"
2021.acl-long.543,P19-1072,1,0.778778,"Missing"
2021.acl-long.543,2020.semeval-1.1,1,0.439261,"using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction of novel LSCD models, and on analyzing and evaluating existing models. Up to now, these preferences for development and analysis vs. application represented a well-motivated choice, because the quality of state-of-the-art models had not been established yet, and because no tuning and testing data were available. But with recent advances in evalu"
2021.acl-long.543,N18-2027,1,0.910751,"Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction of novel LSCD models, and on analyzing and evaluating existing models. Up to now, these preferences for development and analysis vs. application represented a well-motivated choice, because the quality of state-of-the-art models had not been establishe"
2021.acl-long.543,2021.emnlp-main.567,1,0.783221,"Missing"
2021.acl-long.543,J98-1004,0,0.474416,"Missing"
2021.acl-long.543,D19-1007,0,0.0346962,"Missing"
2021.acl-long.543,tahmasebi-risse-2017-finding,0,0.0168705,". ©2021 Association for Computational Linguistics 2 Related Work State-of-the-art semantic change detection models are Vector Space Models (VSMs) (Schlechtweg et al., 2020). These can be divided into type-based (static) (Turney and Pantel, 2010) and token-based (contextualized) (Sch¨utze, 1998) approaches. For our study, we use both a static and a contextualized model. As mentioned above, previous work mostly focuses on creating data sets or developing, evaluating and analyzing models. A common approach for evaluation is to annotate target words selected from dictionaries in specific corpora (Tahmasebi and Risse, 2017; Schlechtweg et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotatio"
2021.acl-long.543,E17-1112,0,0.0248711,"tly focuses on creating data sets or developing, evaluating and analyzing models. A common approach for evaluation is to annotate target words selected from dictionaries in specific corpora (Tahmasebi and Risse, 2017; Schlechtweg et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotation process are Gulordava and Baroni (2011) and Cook et al. (2013). Gulordava and Baroni ask human annotators to rate 100 randomly sampled words on a 4-point scale from 0 (no change) to 3 (changed significantly), however without relating this to a data set. Cook et al. work closely with a professional lexicographer to inspect 20 lemmas predicted by their models plus 10 randomly selected ones. Gulordava an"
2021.acl-long.543,R19-1139,0,0.0224395,"a sets or developing, evaluating and analyzing models. A common approach for evaluation is to annotate target words selected from dictionaries in specific corpora (Tahmasebi and Risse, 2017; Schlechtweg et al., 2018; Perrone et al., 2019; Basile et al., 2020; Rodina and Kutuzov, 2020; Schlechtweg et al., 2020). Contrary to this, our goal is to find ‘undiscovered’ changing words and validate the predictions of our models by human annotators. Few studies focus on this task. Kim et al. (2014), Hamilton et al. (2016), Basile et al. (2016), Basile and Mcgillivray (2018), Takamura et al. (2017) and Tsakalidis et al. (2019) evaluate their approaches by validating the top ranked words through author intuitions or known historical data. The only approaches applying a systematic annotation process are Gulordava and Baroni (2011) and Cook et al. (2013). Gulordava and Baroni ask human annotators to rate 100 randomly sampled words on a 4-point scale from 0 (no change) to 3 (changed significantly), however without relating this to a data set. Cook et al. work closely with a professional lexicographer to inspect 20 lemmas predicted by their models plus 10 randomly selected ones. Gulordava and Baroni and Cook et al. eval"
2021.acl-long.543,2020.emnlp-main.682,0,0.0358132,"ted framework for both evaluation and discovery. 1 Introduction There has been considerable progress in Lexical Semantic Change Detection (LSCD) in recent years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021), with milestones such as the first approaches using neural language models (Kim et al., 2014; Kulkarni et al., 2015), the introduction of Orthogonal Procrustes alignment (Kulkarni et al., 2015; Hamilton et al., 2016), detecting sources of noise (Dubossarsky et al., 2017, 2019), the formulation of continuous models (Frermann and Lapata, 2016; Rosenfeld and Erk, 2018; Tsakalidis and Liakata, 2020), the first uses of contextualized embeddings (Hu et al., 2019; Giulianelli et al., 2020), the development of solid annotation and evaluation frameworks (Schlechtweg et al., 2018, 2019; Shoemark et al., 2019) and shared tasks (Basile et al., 2020; Schlechtweg et al., 2020). However, only a very limited amount of work applies the methods to discover novel instances of semantic change and to evaluate the usefulness of such discovered senses for external fields. That is, the majority of research focuses on the introduction of novel LSCD models, and on analyzing and evaluating existing models. Up"
2021.eacl-srw.25,2020.semeval-1.24,0,0.0500821,"Missing"
2021.eacl-srw.25,2020.semeval-1.4,0,0.0237961,"ance Related work Traditional approaches for LSC detection are typebased (Dubossarsky et al., 2019; Schlechtweg et al., 2019). This means that not every word occurrence is considered individually (token-based); instead, a general vector representation that summarizes every occurrence of a word (including polysemous words) is created. The results of SemEval2020 Task 1 and DIACR-Ita (Basile et al., 2020; Schlechtweg et al., 2020) demonstrated that overall type-based approaches (Asgari et al., 2020; Kaiser et al., 2020; Praˇza´ k et al., 2020) achieved better results than token-based approaches (Beck, 2020; Kutuzov and Giulianelli, 2020; Laicher et al., 2020). This is surprising, however, for two main reasons: (i) contextualized token-based approaches have significantly outperformed static type-based approaches in several NLP tasks over the past years (Ethayarajh, 2019). (ii) SemEval-2020 Task 1 and DIACR-Ita both include a subtask on binary change detection that requires to discover small sets of contextualized usages with the same sense. Typebased embeddings do not infer usage-based (or token-based) representations and are therefore not expected to be able to find such sets (Schlechtweg et al"
2021.eacl-srw.25,N19-1423,0,0.188567,"he past years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021). Recently, SemEval-2020 Task 1 and the Italian follow-up task DIACR-Ita provided a multi-lingual evaluation framework to compare the variety of proposed model architectures (Schlechtweg et al., 2020; Basile et al., 2020). Both tasks demonstrated that type-based embeddings outperform token-based embeddings. This is surprising given that contextualised token-based approaches have achieved significant improvements over the static type-based approaches in several NLP tasks over the past years (Peters et al., 2018; Devlin et al., 2019). In this study, we relate model results on LSC detection to results on the word sense disambiguation data set underlying SemEval-2020 Task 1. This allows us to test the performance of different methods more rigorously, and to thoroughly analyze results of clustering-based methods. We investigate the influence of a range of variables on clusterings of BERT vectors and show that its low performance Related work Traditional approaches for LSC detection are typebased (Dubossarsky et al., 2019; Schlechtweg et al., 2019). This means that not every word occurrence is considered individually (token-b"
2021.eacl-srw.25,W17-1202,0,0.0142805,"those two clusters whose merging maximizes a predefined criterion. We use Ward’s method, where clusters with the lowest loss of information are merged (Ward Jr, 1963). Following Giulianelli et al. (2020) and Martinc et al. (2020a), we estimate the number of clusters k with the Silhouette Method (Rousseeuw, 1987): we perform a cluster analysis for each 2 ≤ k ≤ 10 and calculate the silhouette index for each k. The number of clusters with the largest index is used for the final clustering. The Jensen-Shannon Distance (JSD) measures the difference between two probability distributions (Lin, 1991; Donoso and Sanchez, 2017). We convert two time specific clusterings into probability distribution P and Q and measure their distance JSD(P, Q) to obtain graded change values (Giulianelli et al., 2020; Kutuzov and Giulianelli, 2020). If P and Q are very similar, the JSD returns a value close to 0. If the distributions are very different, the JSD returns a value close to 1. Spearman’s Rank-Order Correlation Coefficient ρ measures the strength and the direction of the relationship between two variables (Bolboaca and J¨antschi, 2006) by correlating the rank order of two variables. Its values range from -1 to 1, where 1 de"
2021.eacl-srw.25,P19-1044,1,0.843352,"ovements over the static type-based approaches in several NLP tasks over the past years (Peters et al., 2018; Devlin et al., 2019). In this study, we relate model results on LSC detection to results on the word sense disambiguation data set underlying SemEval-2020 Task 1. This allows us to test the performance of different methods more rigorously, and to thoroughly analyze results of clustering-based methods. We investigate the influence of a range of variables on clusterings of BERT vectors and show that its low performance Related work Traditional approaches for LSC detection are typebased (Dubossarsky et al., 2019; Schlechtweg et al., 2019). This means that not every word occurrence is considered individually (token-based); instead, a general vector representation that summarizes every occurrence of a word (including polysemous words) is created. The results of SemEval2020 Task 1 and DIACR-Ita (Basile et al., 2020; Schlechtweg et al., 2020) demonstrated that overall type-based approaches (Asgari et al., 2020; Kaiser et al., 2020; Praˇza´ k et al., 2020) achieved better results than token-based approaches (Beck, 2020; Kutuzov and Giulianelli, 2020; Laicher et al., 2020). This is surprising, however, for"
2021.eacl-srw.25,D19-1006,0,0.0303503,"rizes every occurrence of a word (including polysemous words) is created. The results of SemEval2020 Task 1 and DIACR-Ita (Basile et al., 2020; Schlechtweg et al., 2020) demonstrated that overall type-based approaches (Asgari et al., 2020; Kaiser et al., 2020; Praˇza´ k et al., 2020) achieved better results than token-based approaches (Beck, 2020; Kutuzov and Giulianelli, 2020; Laicher et al., 2020). This is surprising, however, for two main reasons: (i) contextualized token-based approaches have significantly outperformed static type-based approaches in several NLP tasks over the past years (Ethayarajh, 2019). (ii) SemEval-2020 Task 1 and DIACR-Ita both include a subtask on binary change detection that requires to discover small sets of contextualized usages with the same sense. Typebased embeddings do not infer usage-based (or token-based) representations and are therefore not expected to be able to find such sets (Schlechtweg et al., 2020). Yet, they show better performance on binary change detection than clusterings of tokenbased embeddings (Kutuzov and Giulianelli, 2020). 3 Data and evaluation We utilize the annotated English, German and Swedish datasets (ENG, GER, SWE) underlying 192 Proceedi"
2021.eacl-srw.25,2020.acl-main.365,0,0.0663183,"Missing"
2021.eacl-srw.25,N18-1202,0,0.0498661,"easing attention in the past years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021). Recently, SemEval-2020 Task 1 and the Italian follow-up task DIACR-Ita provided a multi-lingual evaluation framework to compare the variety of proposed model architectures (Schlechtweg et al., 2020; Basile et al., 2020). Both tasks demonstrated that type-based embeddings outperform token-based embeddings. This is surprising given that contextualised token-based approaches have achieved significant improvements over the static type-based approaches in several NLP tasks over the past years (Peters et al., 2018; Devlin et al., 2019). In this study, we relate model results on LSC detection to results on the word sense disambiguation data set underlying SemEval-2020 Task 1. This allows us to test the performance of different methods more rigorously, and to thoroughly analyze results of clustering-based methods. We investigate the influence of a range of variables on clusterings of BERT vectors and show that its low performance Related work Traditional approaches for LSC detection are typebased (Dubossarsky et al., 2019; Schlechtweg et al., 2019). This means that not every word occurrence is considered"
2021.eacl-srw.25,2020.semeval-1.30,0,0.0611414,"Missing"
2021.eacl-srw.25,P19-1072,1,0.921379,"Missing"
2021.eacl-srw.25,2020.semeval-1.1,1,0.944414,"and show that its low performance is largely due to orthographic information on the target word, which is encoded even in the higher layers of BERT representations. By reducing the influence of orthography we considerably improve BERT’s performance. 1 2 Introduction Lexical Semantic Change (LSC) Detection has drawn increasing attention in the past years (Kutuzov et al., 2018; Tahmasebi et al., 2018; Hengchen et al., 2021). Recently, SemEval-2020 Task 1 and the Italian follow-up task DIACR-Ita provided a multi-lingual evaluation framework to compare the variety of proposed model architectures (Schlechtweg et al., 2020; Basile et al., 2020). Both tasks demonstrated that type-based embeddings outperform token-based embeddings. This is surprising given that contextualised token-based approaches have achieved significant improvements over the static type-based approaches in several NLP tasks over the past years (Peters et al., 2018; Devlin et al., 2019). In this study, we relate model results on LSC detection to results on the word sense disambiguation data set underlying SemEval-2020 Task 1. This allows us to test the performance of different methods more rigorously, and to thoroughly analyze results of clust"
2021.eacl-srw.25,N18-2027,1,0.902244,"rds, else 1.3 For proper names a sentence receives label 0, if no proper names are in the sentence, 1, if one proper name occurs, else 2.4 The hypothesis that proper names may influence the clustering was suggested in Martinc et al. (2020b). For corpora, a sentence is labeled 0, if it occurs in the first target corpus, else 1. Average measures Given two sets of token vectors V1 and V2 from t1 and t2 , Average Pairwise Distance (APD) is calculated by randomly picking n vectors from both sets, calculating their pairwise cosine distances d(x, y) where x ∈ V1 and y ∈ V2 and averaging over these. (Schlechtweg et al., 2018; Giulianelli et al., 2020). We determine n as the minimum size of V1 and V2 . APDOLD/NEW measure the average of pairwise distances within V1 and V2 , respectively. They are calculated as the average distance of max. 10, 000 randomly sampled unique combinations of vectors from either V1 or V2 . COS is calculated as the cosine distance of the respective mean vectors for V1 and V2 (Kutuzov and Giulianelli, 2020). 5 5.1 Results Clustering Because of the high computational load, we apply the clustering only to the ENG and the GER part of the SemEval data set. For this, we use BERT to create token"
2021.findings-acl.16,W11-2501,0,0.0164674,"WordNets, we extracted all noun–noun pairs with a hypernymy relation and removed duplicates, autohyponyms and space-separated multiword expressions. We also distinguish between compounds (which frequently represent hyponyms of their constituent heads, as in dog–lapdog) and non-compounds by applying a simple heuristic, i.e., categorising all hypernym–hyponym pairs as compounds if one is a substring of the other. We expected this subset to exhibit idiosynchratic behaviour in our prediction experiments. On the other hand, we rely on a number of benchmark datasets for hypernymy evaluation: BLESS (Baroni and Lenci, 2011) provides related concepts for 200 English concrete nouns connected through a semantic relation (hypernymy, co-hyponymy, meronymy, attribute, event) or a null-relation. The dataset by Lenci and Benotto (2012) contains a subset of BLESS relation pairs, as created for previous comparisons of hypernymy detection methods. A dataset similar to BLESS, EVALution, was induced from ConceptNet and WordNet (Santus et al., 2015). Its semantic relations include hypernymy, synonymy, antonymy and meronymy. For quality reasons, the pairs were filtered by automatic methods and crowd-sourcing to improve consist"
2021.findings-acl.16,I13-1095,0,0.017577,"and fish–cod, where the hyponym implies the hypernym, but not vice versa. From a cognitive perspective hypernymy is central to the organisation of the mental lexicon (Deese, 1965; Miller and Fellbaum, 1991; Murphy, 2003), next to further semantic relations such as synonymy, antonymy, etc. From a computational perspective hypernymy is central to solving a number of Natural Language Processing (NLP) tasks such as taxonomy creation (Hearst, 1998; Cimiano et al., 2004; Snow et al., 2006; Navigli and Ponzetto, 2012), textual entailment (Dagan et al., 2006; Clark et al., 2007) and text generation (Biran and McKeown, 2013). Accordingly, the field has witnessed active research on two subtasks involved in computational models of hypernymy (see Shwartz et al. (2017) for an extensive overview): hypernymy detection (i.e., distinguishing hypernymy from other semantic relations) and hypernymy prediction (i.e., determining While these unsupervised hypernymy prediction methods have been explored and compared extensively on a number of benchmark datasets (Shwartz et al., 2017), this study takes a novel perspective and performs a detailed analysis of whether and where the methods make similar or different decisions. Our p"
2021.findings-acl.16,W97-0802,0,0.230302,"ttps: //github.com/Thommy96/hyp-freq-comp. 186 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 186–192 August 1–6, 2021. ©2021 Association for Computational Linguistics 2.1 Gold Standard Datasets 2.2 Our study focuses on hypernymy between nouns and uses two types of gold standard resources for hypernymy relations. On the one hand, we rely on WordNets as classical large-scale taxonomies where hypernymy represents one of the core semantic relations for organisation: the English WordNet1 (Miller et al., 1990; Fellbaum, 1998), version 3, and the German GermaNet2 (Hamp and Feldweg, 1997; Kunze and Wagner, 1999; Lemnitzer and Kunze, 2007), version 11. From both WordNets, we extracted all noun–noun pairs with a hypernymy relation and removed duplicates, autohyponyms and space-separated multiword expressions. We also distinguish between compounds (which frequently represent hyponyms of their constituent heads, as in dog–lapdog) and non-compounds by applying a simple heuristic, i.e., categorising all hypernym–hyponym pairs as compounds if one is a substring of the other. We expected this subset to exhibit idiosynchratic behaviour in our prediction experiments. On the other hand,"
2021.findings-acl.16,W07-1409,0,0.0111075,"d a hyponym (subordinate), as in tree–oak and fish–cod, where the hyponym implies the hypernym, but not vice versa. From a cognitive perspective hypernymy is central to the organisation of the mental lexicon (Deese, 1965; Miller and Fellbaum, 1991; Murphy, 2003), next to further semantic relations such as synonymy, antonymy, etc. From a computational perspective hypernymy is central to solving a number of Natural Language Processing (NLP) tasks such as taxonomy creation (Hearst, 1998; Cimiano et al., 2004; Snow et al., 2006; Navigli and Ponzetto, 2012), textual entailment (Dagan et al., 2006; Clark et al., 2007) and text generation (Biran and McKeown, 2013). Accordingly, the field has witnessed active research on two subtasks involved in computational models of hypernymy (see Shwartz et al. (2017) for an extensive overview): hypernymy detection (i.e., distinguishing hypernymy from other semantic relations) and hypernymy prediction (i.e., determining While these unsupervised hypernymy prediction methods have been explored and compared extensively on a number of benchmark datasets (Shwartz et al., 2017), this study takes a novel perspective and performs a detailed analysis of whether and where the meth"
2021.findings-acl.16,D12-1050,0,0.045926,"40 98.79 70.38 90.15 BLESS EVALution LB Weeds 1,337 23.19 62.30 57.52 63.05 58.56 71.80 606 34.86 68.96 64.22 68.86 55.91 59.63 1,747 52.42 76.48 77.02 76.48 71.27 62.66 1,117 44.76 76.63 74.22 76.45 72.43 65.71 Table 1: Sizes of datasets and overall prediction results across datasets. Baselines: In comparison to the hypernymy methods we applied two baselines, cf. Zipf’s principles of least effort (Zipf, 1949): InvCL: An asymmetric precision method suggested by Lenci and Benotto (2012) that takes both feature inclusion as well as feature non-inclusion (originally suggested as ClarkDE (cde) by Clarke (2012)) into account. If invCL(x, y) &gt; invCL(y, x), then x is predicted as the hyponym and y as the hypernym, and vice versa. • Word Length: Given that hyponyms refer to more specific concepts than their hypernyms, and assuming that more specific concepts tend to have a longer word length, this baseline predicts the longer word in a word pair (as measured by the number of characters) as the hyponym. P cde(x, y) = invCL(x, y) = → → f ∈(− x ∩− y ) min(xf , yf ) P → f ∈− x xf • Word Frequency: Given that hyponyms refer to more specific concepts than their hypernyms and assuming that more specific conce"
2021.findings-acl.16,S12-1012,0,0.190404,"present hyponyms of their constituent heads, as in dog–lapdog) and non-compounds by applying a simple heuristic, i.e., categorising all hypernym–hyponym pairs as compounds if one is a substring of the other. We expected this subset to exhibit idiosynchratic behaviour in our prediction experiments. On the other hand, we rely on a number of benchmark datasets for hypernymy evaluation: BLESS (Baroni and Lenci, 2011) provides related concepts for 200 English concrete nouns connected through a semantic relation (hypernymy, co-hyponymy, meronymy, attribute, event) or a null-relation. The dataset by Lenci and Benotto (2012) contains a subset of BLESS relation pairs, as created for previous comparisons of hypernymy detection methods. A dataset similar to BLESS, EVALution, was induced from ConceptNet and WordNet (Santus et al., 2015). Its semantic relations include hypernymy, synonymy, antonymy and meronymy. For quality reasons, the pairs were filtered by automatic methods and crowd-sourcing to improve consistency and to determine prototypical pairs. Finally, we use the Weeds dataset (Weeds et al., 2004; Weeds and Weir, 2005) which contains word pairs related by hypernymy and co-hyponymy across word classes. From"
2021.findings-acl.16,E14-4008,1,0.905269,"han just Frequency? Demasking Unsupervised Hypernymy Prediction Methods Thomas Bott, Dominik Schlechtweg and Sabine Schulte im Walde Institute for Natural Language Processing, University of Stuttgart, Germany {bottts,schlecdk,schulte}@ims.uni-stuttgart.de Abstract which word in a pair of words is the hypernym and which is the hyponym). The target subtask of the current study is hypernymy prediction: we perform a comparative analysis of a class of approaches commonly refered to as unsupervised hypernymy methods (Weeds et al., 2004; Kotlerman et al., 2010; Clarke, 2012; Lenci and Benotto, 2012; Santus et al., 2014). These methods all rely on the distributional hypothesis (Harris, 1954; Firth, 1957) that words which are similar in meaning also occur in similar linguistic distributions. In this vein, they exploit asymmetries in distributional vector space representations, in order to contrast hypernym and hyponym vectors. This paper presents a comparison of unsupervised methods of hypernymy prediction (i.e., to predict which word in a pair of words such as fish–cod is the hypernym and which the hyponym). Most importantly, we demonstrate across datasets for English and for German that the predictions of th"
2021.findings-acl.16,W15-4208,0,0.0145934,"ted this subset to exhibit idiosynchratic behaviour in our prediction experiments. On the other hand, we rely on a number of benchmark datasets for hypernymy evaluation: BLESS (Baroni and Lenci, 2011) provides related concepts for 200 English concrete nouns connected through a semantic relation (hypernymy, co-hyponymy, meronymy, attribute, event) or a null-relation. The dataset by Lenci and Benotto (2012) contains a subset of BLESS relation pairs, as created for previous comparisons of hypernymy detection methods. A dataset similar to BLESS, EVALution, was induced from ConceptNet and WordNet (Santus et al., 2015). Its semantic relations include hypernymy, synonymy, antonymy and meronymy. For quality reasons, the pairs were filtered by automatic methods and crowd-sourcing to improve consistency and to determine prototypical pairs. Finally, we use the Weeds dataset (Weeds et al., 2004; Weeds and Weir, 2005) which contains word pairs related by hypernymy and co-hyponymy across word classes. From all four benchmark datasets we extracted all noun–noun pairs related by hypernymy. The first row in Table 1 shows the numbers of hypernymy pairs in the WordNets and in the benchmark datasets. 1 2 https://wordnet."
2021.findings-acl.16,P16-1226,0,0.0452839,"Missing"
2021.findings-acl.16,E17-1007,1,0.842348,"f the mental lexicon (Deese, 1965; Miller and Fellbaum, 1991; Murphy, 2003), next to further semantic relations such as synonymy, antonymy, etc. From a computational perspective hypernymy is central to solving a number of Natural Language Processing (NLP) tasks such as taxonomy creation (Hearst, 1998; Cimiano et al., 2004; Snow et al., 2006; Navigli and Ponzetto, 2012), textual entailment (Dagan et al., 2006; Clark et al., 2007) and text generation (Biran and McKeown, 2013). Accordingly, the field has witnessed active research on two subtasks involved in computational models of hypernymy (see Shwartz et al. (2017) for an extensive overview): hypernymy detection (i.e., distinguishing hypernymy from other semantic relations) and hypernymy prediction (i.e., determining While these unsupervised hypernymy prediction methods have been explored and compared extensively on a number of benchmark datasets (Shwartz et al., 2017), this study takes a novel perspective and performs a detailed analysis of whether and where the methods make similar or different decisions. Our prediction experiments on simplex and complex nouns in English and German WordNets and evaluation benchmarks show that most of the methods we in"
2021.findings-acl.16,P06-1101,0,0.0966522,"major paradigmatic semantic relation between two concepts, a hypernym (superordinate) and a hyponym (subordinate), as in tree–oak and fish–cod, where the hyponym implies the hypernym, but not vice versa. From a cognitive perspective hypernymy is central to the organisation of the mental lexicon (Deese, 1965; Miller and Fellbaum, 1991; Murphy, 2003), next to further semantic relations such as synonymy, antonymy, etc. From a computational perspective hypernymy is central to solving a number of Natural Language Processing (NLP) tasks such as taxonomy creation (Hearst, 1998; Cimiano et al., 2004; Snow et al., 2006; Navigli and Ponzetto, 2012), textual entailment (Dagan et al., 2006; Clark et al., 2007) and text generation (Biran and McKeown, 2013). Accordingly, the field has witnessed active research on two subtasks involved in computational models of hypernymy (see Shwartz et al. (2017) for an extensive overview): hypernymy detection (i.e., distinguishing hypernymy from other semantic relations) and hypernymy prediction (i.e., determining While these unsupervised hypernymy prediction methods have been explored and compared extensively on a number of benchmark datasets (Shwartz et al., 2017), this stud"
2021.findings-acl.16,J05-4002,0,0.128062,"hypernymy, co-hyponymy, meronymy, attribute, event) or a null-relation. The dataset by Lenci and Benotto (2012) contains a subset of BLESS relation pairs, as created for previous comparisons of hypernymy detection methods. A dataset similar to BLESS, EVALution, was induced from ConceptNet and WordNet (Santus et al., 2015). Its semantic relations include hypernymy, synonymy, antonymy and meronymy. For quality reasons, the pairs were filtered by automatic methods and crowd-sourcing to improve consistency and to determine prototypical pairs. Finally, we use the Weeds dataset (Weeds et al., 2004; Weeds and Weir, 2005) which contains word pairs related by hypernymy and co-hyponymy across word classes. From all four benchmark datasets we extracted all noun–noun pairs related by hypernymy. The first row in Table 1 shows the numbers of hypernymy pairs in the WordNets and in the benchmark datasets. 1 2 https://wordnet.princeton.edu https://uni-tuebingen.de/en/142806 Corpora and Vector Spaces We created our distributional vector spaces based on the WaCky3 corpora (Baroni et al., 2009) for English and for German. The English PukWaC corpus is the syntax-annotated version of ukWaC (Ferraresi et al., 2008) and conta"
2021.findings-acl.16,C04-1146,0,0.348398,"Missing"
2021.konvens-1.24,J90-1003,0,0.381999,"nce resolution; Guillaume et al. (2016) design ZombiLingo, a game for syntactic dependency annotation. We present a GWAP-style game implementation called W ORD G UESS1 where associations of a target word are offered to players in order to guess the target word. For example, associations such as winter, white, cold provide hints to players when guessing the target word snow. Our game is a web-based and mobile-based application whose aim is to learn and understand word-association and word-context relationships: previous research has shown that associations and corpus co-occurrence are related (Church and Hanks, 1990; de Deyne and Storms, 2008a; Schulte im Walde et al., 2008, i.a.); we plan to explore their connections and differences in more depth. In this vein, (i) we vary associations obtained from humans, and context-based words induced from corpus co-occurrence; (ii) we provide a multilingual gaming environment in order to understand the conditions across languages and relational patterns between native and second languages; and (iii) we offer the players to choose between levels of difficulty (i.e., providing more or less cues). The obtained data enables us to induce conditions and weights for word"
2021.konvens-1.24,C02-1113,0,0.0853929,"ul for many NLP purposes such as ontology induction and anaphora resolution. 1 Introduction Games-with-a-purpose (GWAP) offer enjoyable entertainment to players and at the same time allow researchers in Natural Language Processing (NLP) to collect and explore cognitive and (computational) linguistic facets of human-generated data. While the term GWAP has been coined by Von Ahn and Dabbish (2008), the underlying idea has been pursued across linguistic levels and across NLP purposes for much longer. To provide a few examples across research fields, the adventure and interactive fiction games by Gabsdil et al. (2002) rely on natural-language question-answering dialogues to explore inference systems with reference resolution, syntactic ambiguities, and scripted dialogues; Chamberlain et al. (2008) exploit collaborative work to identify relationships between words and phrases in web data; OntoGame (Siorpaes and Hepp, 2008) matches classes in an ontology with Wikipedia articles; Hladk´a et al. (2009) propose a gamified annotation approach for coreference resolution; Guillaume et al. (2016) design ZombiLingo, a game for syntactic dependency annotation. We present a GWAP-style game implementation called W ORD"
2021.konvens-1.24,C16-1286,0,0.0123894,"purposes for much longer. To provide a few examples across research fields, the adventure and interactive fiction games by Gabsdil et al. (2002) rely on natural-language question-answering dialogues to explore inference systems with reference resolution, syntactic ambiguities, and scripted dialogues; Chamberlain et al. (2008) exploit collaborative work to identify relationships between words and phrases in web data; OntoGame (Siorpaes and Hepp, 2008) matches classes in an ontology with Wikipedia articles; Hladk´a et al. (2009) propose a gamified annotation approach for coreference resolution; Guillaume et al. (2016) design ZombiLingo, a game for syntactic dependency annotation. We present a GWAP-style game implementation called W ORD G UESS1 where associations of a target word are offered to players in order to guess the target word. For example, associations such as winter, white, cold provide hints to players when guessing the target word snow. Our game is a web-based and mobile-based application whose aim is to learn and understand word-association and word-context relationships: previous research has shown that associations and corpus co-occurrence are related (Church and Hanks, 1990; de Deyne and St"
2021.konvens-1.24,P09-2053,0,0.0394785,"Missing"
2021.konvens-1.24,C02-1007,0,0.290666,"ne et al., 2019), among others. For many NLP purposes such as ontology induction and anaphora resolution, it is crucial to define and induce semantic relations between words or contexts, and according to the co-occurrence hypothesis (Miller, 1969; Spence and Owens, 1990) semantic association is related to the textual cooccurrence of stimulus-associate pairs. Therefore, a number of studies have exploited the connection between co-occurrence distributions and semantic relatedness, and used association norms as a test-bed for distributional models of semantic relatedness (Church and Hanks, 1990; Rapp, 2002; de Deyne and Storms, 2008a; Schulte im Walde et al., 2008, i.a.). Game Idea The aim of W ORD G UESS is to exploit a gamification environment in order to deepen the understanding of associative relatedness. Differently to previous approaches, we do not directly Figure 1: Sample page views of the game: (a) On the left you can see the decision page where the player can choose a predefined game, the number of targets and the difficulty level. (b) On the right you can see a game page at some point during the game where (top row: left to right) the player is currently working on the third out of f"
2021.starsem-1.23,Q16-1003,0,0.0196507,"erally falls within the area of Bayesian probabilistic modeling (Koch, 2007). More specifically, it is related to model-based graph clustering techniques, e.g., Latent Space models such as Gaussian Mixture Models (Hoff et al., 2002; Duda and Hart, 1973). These methods are common in the field of community detection (Abbe, 2017). Within computational linguistics our approach is most strongly related to generative probabilistic topic models, where words in documents are modeled as being drawn from a latent topic distribution (Steyvers and Griffiths, 2007). Topics are often interpreted as senses (Frermann and Lapata, 2016; Perrone et al., 2019). Another common, yet non-probabilistic, modeling approach for word senses is to group word uses expressing similar meanings into clusters based on contextual features (Sch¨utze, 1998; Biemann, 2006). As to our knowledge, only a small set of studies is concerned with the modeling of human-annotated WUGs (McCarthy et al., 2016; Schlechtweg et al., 2020, 2021). This research line is motivated by insights from lexical semantics that word senses are no discrete objects (Kilgarriff, 1997; Erk et al., 2013). Most important to note is the pioneering work of McCarthy et al. (201"
2021.starsem-1.23,W06-3812,0,0.116123,"Duda and Hart, 1973). These methods are common in the field of community detection (Abbe, 2017). Within computational linguistics our approach is most strongly related to generative probabilistic topic models, where words in documents are modeled as being drawn from a latent topic distribution (Steyvers and Griffiths, 2007). Topics are often interpreted as senses (Frermann and Lapata, 2016; Perrone et al., 2019). Another common, yet non-probabilistic, modeling approach for word senses is to group word uses expressing similar meanings into clusters based on contextual features (Sch¨utze, 1998; Biemann, 2006). As to our knowledge, only a small set of studies is concerned with the modeling of human-annotated WUGs (McCarthy et al., 2016; Schlechtweg et al., 2020, 2021). This research line is motivated by insights from lexical semantics that word senses are no discrete objects (Kilgarriff, 1997; Erk et al., 2013). Most important to note is the pioneering work of McCarthy et al. (2016) as the first to represent human-annotated word uses within graphs and then clustering the uses based on heuristics such as connected components and cliques. McCarthy et al. derived edge weights from human lexical substi"
2021.starsem-1.23,P08-2063,0,0.0652567,"connection with a global threshold to group vertices with high edge weights and developed an efficient iterative sampling strategy for edges to reduce annotation load. However, these approaches are ad-hoc clustering methods which do not provide a probabilistic model for WUGs. 3 Data Table 1: DURel relatedness scale (Schlechtweg et al., 2018). represent word uses and weights w ∈ W represent the semantic proximity of a pair of uses (u1 , u2 ) ∈ E (Schlechtweg and Schulte im Walde, submitted). In practice, semantic proximity can be measured by human annotator judgments on a scale of relatedness (Brown, 2008; Schlechtweg et al., 2018) or similarity (Erk et al., 2013). Humanannotated WUGs are often sparsely observed and noisy, i.e., only a small percentage of edges from the full graph are annotated, and annotators often show disagreements, e.g. for ambiguous uses, as can be seen in Figure 1. Recently, Schlechtweg et al. (2020, 2021) developed a large-scale multi-lingual resource of WUGs. Annotators were asked to judge the semantic relatedness of pairs of word uses (such as the two uses of grasp in (1) and (2)) according to the scale in Table 1.1 (1) He continued to grasp, between forefinger and th"
2021.starsem-1.23,J16-2003,0,0.281058,"mulation of the Weighted Stochastic Block Model, a generative model for random graphs popular in biology, physics and social sciences. By providing a probabilistic model of graded word meaning we aim to approach the slippery and yet widely used notion of word sense in a novel way. The proposed framework enables us to rigorously compare models of word senses with respect to their fit to the data. We perform extensive experiments and select the empirically most adequate model. 1 Introduction Word Usage Graphs (WUGs) are a relatively new model of graded word meaning in context (Erk et al., 2013; McCarthy et al., 2016; Schlechtweg et al., 2021). They represent word uses (i.e., words in context) within a weighted undirected graph, with edge weights reflecting the semantic proximity between uses. WUGs may be obtained via human annotation by presenting annotators with pairs of words uses and asking them for proximity judgments. The WUGs may then be clustered into sets of uses exhibiting high semantic proximity, in order to reflect traditional word sense distinctions (McCarthy et al., 2016), and to provide insight into key aspects of word meaning such as polysemy, vagueness, and lexical semantic change (Schlec"
2021.starsem-1.23,W19-4707,0,0.0131734,"a of Bayesian probabilistic modeling (Koch, 2007). More specifically, it is related to model-based graph clustering techniques, e.g., Latent Space models such as Gaussian Mixture Models (Hoff et al., 2002; Duda and Hart, 1973). These methods are common in the field of community detection (Abbe, 2017). Within computational linguistics our approach is most strongly related to generative probabilistic topic models, where words in documents are modeled as being drawn from a latent topic distribution (Steyvers and Griffiths, 2007). Topics are often interpreted as senses (Frermann and Lapata, 2016; Perrone et al., 2019). Another common, yet non-probabilistic, modeling approach for word senses is to group word uses expressing similar meanings into clusters based on contextual features (Sch¨utze, 1998; Biemann, 2006). As to our knowledge, only a small set of studies is concerned with the modeling of human-annotated WUGs (McCarthy et al., 2016; Schlechtweg et al., 2020, 2021). This research line is motivated by insights from lexical semantics that word senses are no discrete objects (Kilgarriff, 1997; Erk et al., 2013). Most important to note is the pioneering work of McCarthy et al. (2016) as the first to repr"
2021.starsem-1.23,2020.semeval-1.1,1,0.799905,", 2016; Schlechtweg et al., 2021). They represent word uses (i.e., words in context) within a weighted undirected graph, with edge weights reflecting the semantic proximity between uses. WUGs may be obtained via human annotation by presenting annotators with pairs of words uses and asking them for proximity judgments. The WUGs may then be clustered into sets of uses exhibiting high semantic proximity, in order to reflect traditional word sense distinctions (McCarthy et al., 2016), and to provide insight into key aspects of word meaning such as polysemy, vagueness, and lexical semantic change (Schlechtweg et al., 2020, 2021). We suggest to model WUGs with a Bayesian formulation of the Weighted Stochastic Block Model (WSBM), a generative model for random graphs popular in biology, physics and social sciences (Aicher et al., 2014; Peixoto, 2017). The basic assumption of WSBMs is that vertices belong to latent blocks (clusters), and that vertices in the same block are stochastically equivalent (i.e., they have edges drawn from the same distribution). Fitting the model is equivalent to determining the optimal latent block structure providing a clustering of word uses. By using a Bayesian probabilistic model of"
2021.starsem-1.23,N18-2027,1,0.852785,"espective target words and binarized them according to a threshold. This idea was recently modified and extended by Schlechtweg et al. (2020, 2021). Schlechtweg et al. used semantic proximity judgments to annotate edges. They applied correlation clustering (Bansal et al., 2004) in connection with a global threshold to group vertices with high edge weights and developed an efficient iterative sampling strategy for edges to reduce annotation load. However, these approaches are ad-hoc clustering methods which do not provide a probabilistic model for WUGs. 3 Data Table 1: DURel relatedness scale (Schlechtweg et al., 2018). represent word uses and weights w ∈ W represent the semantic proximity of a pair of uses (u1 , u2 ) ∈ E (Schlechtweg and Schulte im Walde, submitted). In practice, semantic proximity can be measured by human annotator judgments on a scale of relatedness (Brown, 2008; Schlechtweg et al., 2018) or similarity (Erk et al., 2013). Humanannotated WUGs are often sparsely observed and noisy, i.e., only a small percentage of edges from the full graph are annotated, and annotators often show disagreements, e.g. for ambiguous uses, as can be seen in Figure 1. Recently, Schlechtweg et al. (2020, 2021) d"
2021.starsem-1.23,2021.emnlp-main.567,1,0.725882,"d Stochastic Block Model, a generative model for random graphs popular in biology, physics and social sciences. By providing a probabilistic model of graded word meaning we aim to approach the slippery and yet widely used notion of word sense in a novel way. The proposed framework enables us to rigorously compare models of word senses with respect to their fit to the data. We perform extensive experiments and select the empirically most adequate model. 1 Introduction Word Usage Graphs (WUGs) are a relatively new model of graded word meaning in context (Erk et al., 2013; McCarthy et al., 2016; Schlechtweg et al., 2021). They represent word uses (i.e., words in context) within a weighted undirected graph, with edge weights reflecting the semantic proximity between uses. WUGs may be obtained via human annotation by presenting annotators with pairs of words uses and asking them for proximity judgments. The WUGs may then be clustered into sets of uses exhibiting high semantic proximity, in order to reflect traditional word sense distinctions (McCarthy et al., 2016), and to provide insight into key aspects of word meaning such as polysemy, vagueness, and lexical semantic change (Schlechtweg et al., 2020, 2021)."
2021.starsem-1.23,J98-1004,0,0.880279,"Missing"
2021.starsem-1.24,W14-1207,0,0.0682729,"Missing"
2021.starsem-1.24,2020.lrec-1.537,1,0.830408,"Missing"
2021.starsem-1.24,Q17-1010,0,0.0343167,"er productivity (domain) modifier productivity (general) head productivity (domain) head productivity (general) Tertiles and Ranges low mid high 3–4 4–8 8–444 0 0–17 17–53,569 1–14 14–55 55–665 0–101 103–588 590–4,976 1–14 14–61 62–1,157 0–119 119–786 786–8,293 Micro-F1 low high 0.773 0.722 0.779 0.722 0.863 0.658 0.884 0.661 0.802 0.652 0.812 0.693 Table 8: Ranges of selected properties across tertiles, and results on binary classification for extreme ‘low’ and ‘high’ tertiles when using all features (cf. All in Table 2 with Micro-F1=0.732). ing: word2vec (Mikolov et al., 2013) and fastText (Bojanowski et al., 2017).5 We use the word2vec model, because it is a standard model for natural language processing applications. The fastText model works on character n-grams and not on words, and Bojanowski et al. (2017) argues that it performs well on closed compounds. This model is particularly interesting for us because a compound embedding is learned partially from the same n-grams as the embeddings of its constituents. Thus, we implicitly have a representation of the constituents in the compound embedding, which we expect to be beneficial for our classification task. Inspecting some words and their nearest ne"
2021.starsem-1.24,bonin-etal-2010-contrastive,0,0.0594013,"Missing"
2021.starsem-1.24,L16-1366,0,0.0223248,"tion, a major strand of methodologies are contrastive techniques, where a term candidate’s distribution in a domain-specific text corpus is compared to the distribution in a reference corpus, for example a general-language corpus (Ahmad et al., 1994; Rayson and Garside, 2000; Drouin, 2003; Kit and Liu, 2008; Bonin et al., 2010; Kochetkova, 2015; Lopes et al., 2016; Mykowiecka et al., 2018, i.a.). Many term difficulty prediction studies rely on some variant of contrastive approaches, mostly frequency-based; notable exceptions are Zeng-Treitler et al. (2008), who apply a contextual network, and Bouamor et al. (2016), who use a likelihood ratio test based on two language models. Most studies fall into the medical, biomedical or health domain. They rely on classical readability features such as frequency, term length, syllable count, the Dale-Chall readability formula or affixes (Zeng et al., 2005; Zeng-Treitler et al., 2008; Vydiswaran et al., 2014; Grabar et al., 2014). Some features are tailored to the medical domain, for example relying on neo-classical word 1 Termhood refers to the degree to which a lexical unit can be considered a domain-specific concept (Kageura and Umino, 1996). components, since m"
2021.starsem-1.24,W14-5702,0,0.0710014,"Missing"
2021.starsem-1.24,W09-3102,0,0.0654895,"Missing"
2021.starsem-1.24,W14-4812,0,0.131141,"Missing"
2021.starsem-1.24,W14-1202,0,0.0197135,"2016; Mykowiecka et al., 2018, i.a.). Many term difficulty prediction studies rely on some variant of contrastive approaches, mostly frequency-based; notable exceptions are Zeng-Treitler et al. (2008), who apply a contextual network, and Bouamor et al. (2016), who use a likelihood ratio test based on two language models. Most studies fall into the medical, biomedical or health domain. They rely on classical readability features such as frequency, term length, syllable count, the Dale-Chall readability formula or affixes (Zeng et al., 2005; Zeng-Treitler et al., 2008; Vydiswaran et al., 2014; Grabar et al., 2014). Some features are tailored to the medical domain, for example relying on neo-classical word 1 Termhood refers to the degree to which a lexical unit can be considered a domain-specific concept (Kageura and Umino, 1996). components, since medical terminology is considered to be highly influenced by Greek and Latin (Del´eger and Zweigenbaum, 2009; Bouamor et al., 2016). As to our knowledge, there is no previous work that investigated term difficulty prediction for complex phrases. Regarding the more general task of automatic term extraction, a few studies included complex phrases and their cons"
2021.starsem-1.24,E17-4012,1,0.771859,"Missing"
2021.starsem-1.24,W18-4909,1,0.868072,"Missing"
2021.starsem-1.24,W00-0901,0,0.336295,"a decision tree classifier using manually designed features to characterize termhood and compound formation, and neural classifiers using word embeddings. 2 Related Work Term difficulty prediction (also referred to as term familiarity or term technicality prediction) can be seen as a subtask of automatic term extraction. For automatic term extraction, a major strand of methodologies are contrastive techniques, where a term candidate’s distribution in a domain-specific text corpus is compared to the distribution in a reference corpus, for example a general-language corpus (Ahmad et al., 1994; Rayson and Garside, 2000; Drouin, 2003; Kit and Liu, 2008; Bonin et al., 2010; Kochetkova, 2015; Lopes et al., 2016; Mykowiecka et al., 2018, i.a.). Many term difficulty prediction studies rely on some variant of contrastive approaches, mostly frequency-based; notable exceptions are Zeng-Treitler et al. (2008), who apply a contextual network, and Bouamor et al. (2016), who use a likelihood ratio test based on two language models. Most studies fall into the medical, biomedical or health domain. They rely on classical readability features such as frequency, term length, syllable count, the Dale-Chall readability formul"
2021.starsem-1.24,I11-1024,0,0.0448059,"s for the decision tree classification using all features. The classification models significantly outperform the respective baselines in the binary classification tasks, but in the four-class distinctions this only applies to the Automotive domain and across all domains (non-significant results are in italics). For the binary task, the results for Automotive are better than for Cooking and DIY. We assume that this divergence is due to a higher imbalance of class sizes across the domains, cf. figure 1. Note that we decided against a direct computation of compound–constituent compositionality (Reddy et al., 2011; Schulte im Walde et al., 2013, 2016) as a feature, because the compound dataset was balanced for frequency. It includes infrequent compounds for which word embeddings and compositionality measures would be imprecise. Method: Decision Trees. Decision tree classifiers (DTs) are supervised machine learning methods that are represented as tree structures. DTs were chosen for this task because they are easy to 4 Note that for all but one of these features we have a balanced set of compounds in the gold standard, see section 3.3. 255 Results by feature group. Having looked at the results when usin"
2021.starsem-1.24,S16-2020,1,0.903299,"Missing"
2021.starsem-1.24,S13-1038,1,0.787126,"Missing"
2021.starsem-1.24,W14-4814,0,0.039444,"Missing"
2021.starsem-1.24,W17-1722,0,0.0584491,"Missing"
2021.vardial-1.3,L18-1457,0,0.0642291,"Missing"
2021.vardial-1.3,rehbein-etal-2014-kiezdeutsch,0,0.138469,"syntactic features that can easily be obtained from standard part-of-speech (POS) taggers and lemmatisers. In this vein, we present three studies: morpho-syntactic variation in terms of part-of-speech unigram distributions (Study 1) and in terms of part-of-speech trigram distributions (Study 2); and lexical variation in the usage of nouns and verbs (Study 3). 2 conclusions were in agreement with studies by Freywald et al. (2011) who considered Kiezdeutsch a multi-ethnolect, and by Wiese (2013) who categorized Kiezdeutsch as an urban dialect. The introduction of a corpus of spoken Kiezdeutsch (Rehbein et al., 2014) led to research across linguistic levels. For example, te Velde (2017) investigated phonological form using syntax of verb-second constructions found in the German dialects Kiezdeutsch, Yiddish, Bavarian, Cimbrian, and colloquial German. More recently, the effects of English on Kiezdeutsch constructions were examined by Preseau (2018) who argued that it was necessary to reconsider Kiezdeutsch as a native dialect of German, given the role of English as a Lingua Franca (ELF) in urban Germany and the effect it has on Kiezdeutsch-speaking communities. Previous Work on Kiezdeutsch In the mid-1990s"
bott-schulte-im-walde-2014-optimizing,W03-1812,0,\N,Missing
bott-schulte-im-walde-2014-optimizing,W02-2001,0,\N,Missing
bott-schulte-im-walde-2014-optimizing,springorum-etal-2012-automatic,1,\N,Missing
bott-schulte-im-walde-2014-optimizing,W04-2118,1,\N,Missing
bott-schulte-im-walde-2014-optimizing,aldinger-2004-towards,0,\N,Missing
bott-schulte-im-walde-2014-optimizing,evert-2004-statistical,0,\N,Missing
C00-2105,P99-1035,0,0.0364807,"Missing"
C00-2105,E99-1016,0,0.0946008,"ts.  for part-of-speech tagging. Another HMM-based approach has been developed by Mats Rooth (Rooth, 1992). It integrates two HMMs; one of them models noun chunks internally, the other models the context of noun chunks. Abney's cascaded nitestate parser (Abney, 1996) also contains a processing step which recognises noun chunks and other types of chunks. Ramshaw and Marcus (Ramshaw and Marcus, 1995) successfully applied Eric Brill's transformation-based learning method to the chunking problem. Voutilainen's NPtool (Voutilainen, 1993) is based on his constraint-grammar system. Finally, Brants (Brants, 1999) described a German chunker which was implemented with cascaded Markov Models. In this paper, a probabilistic context-free parser is applied to the noun chunking task. The German grammar used in the experiments was semiautomatically extended with robustness rules in order to be able to process arbitrary input. The grammar parameters were trained on unlabelled data. A novel algorithm is used for noun chunk extraction. It maximises the probability of the chunk set. The following section introduces the grammar framework, followed by a description of the chunking algorithm in section 3, and the ex"
C00-2105,W98-1505,0,0.143589,"chunks. The robustness rules for the chunker are described in section 2.3. 2.1 (Head-Lexicalised) Probabilistic Context-Free Grammars A probabilistic context-free grammar (PCFG) is a context-free grammar which assigns a probability P to each context-free grammar rule in the rule set R. QThe probability of a parse tree T is de ned as r2R P (r)jrj , where jrj is the number of times rule r was applied to build T . The parameters of PCFGs can be learned from unparsed corpora using the Inside-Outside algorithm (Lari and Young, 1990). Head-lexicalised probabilistic context-free grammars (H-L PCFG) (Carroll and Rooth, 1998) extend the PCFG approach by incorporating information about the lexical head of constituents into the probabilistic model.1 Each node in a parse of a HL PCFG is labelled with a category and the lexical head of the category. A H-L PCFG rule looks like a PCFG rule in which one of the daughters has been marked as the head. The rule probabilities Prule (C ! jC ) are replaced by lexicalised rule probabilities Prule (C ! jC; h) where h is the lexical head of the mother constituent C . The probability of a rule therefore depends not only on the category of the mother node, but also on its lexical he"
C00-2105,P98-1035,0,0.0133887,"s de ned according to Abney's chunk style (Abney, 1991) who describes chunks as syntactic units which correspond in some way to prosodic patterns, containing a content word surrounded by some function word(s): all words from the beginning of the noun phrase to the head noun are included.3 The di erent kinds of noun chunks covered by our grammar are listed below and illustrated with examples:  a combination of a non-obligatory determiner, optional adjectives or cardinals and the noun 1 Other types of lexicalised PCFGs have been described in (Charniak, 1997), (Collins, 1997), (Goodman, 1997), (Chelba and Jelinek, 1998) and (Eisner and Satta, 1999). 2 The restricted corpora were extracted automatically from the Huge German Corpus (HGC), a collection of German newspapers as well as specialised magazines for industry, law, computer science. 3 As you will see below, there is one exception, noun chunks re ned by a proper name, which end with the name instead of the head noun. itself: (1) eine gute Idee a good idea (2) vielen Menschen for many people (3) deren kunstliche Stimme whose arti cial voice (4) elf Ladungen eleven cargos (5) Wasser water and prepositional phrases where the de nite article of the embedde"
C00-2105,A88-1019,0,0.110008,". of the country involved. `Leading economists with doubtable reputations are involved in guiding the country in times of bottlenecks.' A tool which identi es noun chunks is useful for term extraction (most technical terms are nouns or complex noun groups), for lexicographic purposes (see (Tapanainen and Jarvinen, 1998) on syntactically organised concordancing), and as index terms for information retrieval. Chunkers may also mark other types of chunks like verb groups, adverbial phrases or adjectival phrases. Several methods have been developed for noun chunking. Church's noun phrase tagger (Church, 1988), one of the rst noun chunkers, was based on a Hidden Markov Model (HMM) similar to those used Thanks to Mats Rooth and Uli Heid for many helpful comments.  for part-of-speech tagging. Another HMM-based approach has been developed by Mats Rooth (Rooth, 1992). It integrates two HMMs; one of them models noun chunks internally, the other models the context of noun chunks. Abney's cascaded nitestate parser (Abney, 1996) also contains a processing step which recognises noun chunks and other types of chunks. Ramshaw and Marcus (Ramshaw and Marcus, 1995) successfully applied Eric Brill's transformat"
C00-2105,P97-1003,0,0.058638,"oun chunk concept in the grammar is de ned according to Abney's chunk style (Abney, 1991) who describes chunks as syntactic units which correspond in some way to prosodic patterns, containing a content word surrounded by some function word(s): all words from the beginning of the noun phrase to the head noun are included.3 The di erent kinds of noun chunks covered by our grammar are listed below and illustrated with examples:  a combination of a non-obligatory determiner, optional adjectives or cardinals and the noun 1 Other types of lexicalised PCFGs have been described in (Charniak, 1997), (Collins, 1997), (Goodman, 1997), (Chelba and Jelinek, 1998) and (Eisner and Satta, 1999). 2 The restricted corpora were extracted automatically from the Huge German Corpus (HGC), a collection of German newspapers as well as specialised magazines for industry, law, computer science. 3 As you will see below, there is one exception, noun chunks re ned by a proper name, which end with the name instead of the head noun. itself: (1) eine gute Idee a good idea (2) vielen Menschen for many people (3) deren kunstliche Stimme whose arti cial voice (4) elf Ladungen eleven cargos (5) Wasser water and prepositional phr"
C00-2105,P99-1059,0,0.0135105,"hunk style (Abney, 1991) who describes chunks as syntactic units which correspond in some way to prosodic patterns, containing a content word surrounded by some function word(s): all words from the beginning of the noun phrase to the head noun are included.3 The di erent kinds of noun chunks covered by our grammar are listed below and illustrated with examples:  a combination of a non-obligatory determiner, optional adjectives or cardinals and the noun 1 Other types of lexicalised PCFGs have been described in (Charniak, 1997), (Collins, 1997), (Goodman, 1997), (Chelba and Jelinek, 1998) and (Eisner and Satta, 1999). 2 The restricted corpora were extracted automatically from the Huge German Corpus (HGC), a collection of German newspapers as well as specialised magazines for industry, law, computer science. 3 As you will see below, there is one exception, noun chunks re ned by a proper name, which end with the name instead of the head noun. itself: (1) eine gute Idee a good idea (2) vielen Menschen for many people (3) deren kunstliche Stimme whose arti cial voice (4) elf Ladungen eleven cargos (5) Wasser water and prepositional phrases where the de nite article of the embedded noun chunk is morphological"
C00-2105,P96-1024,0,0.0584566,"Missing"
C00-2105,1997.iwpt-1.13,0,0.0159286,"in the grammar is de ned according to Abney's chunk style (Abney, 1991) who describes chunks as syntactic units which correspond in some way to prosodic patterns, containing a content word surrounded by some function word(s): all words from the beginning of the noun phrase to the head noun are included.3 The di erent kinds of noun chunks covered by our grammar are listed below and illustrated with examples:  a combination of a non-obligatory determiner, optional adjectives or cardinals and the noun 1 Other types of lexicalised PCFGs have been described in (Charniak, 1997), (Collins, 1997), (Goodman, 1997), (Chelba and Jelinek, 1998) and (Eisner and Satta, 1999). 2 The restricted corpora were extracted automatically from the Huge German Corpus (HGC), a collection of German newspapers as well as specialised magazines for industry, law, computer science. 3 As you will see below, there is one exception, noun chunks re ned by a proper name, which end with the name instead of the head noun. itself: (1) eine gute Idee a good idea (2) vielen Menschen for many people (3) deren kunstliche Stimme whose arti cial voice (4) elf Ladungen eleven cargos (5) Wasser water and prepositional phrases where the de"
C00-2105,W95-0107,0,0.0866362,"developed for noun chunking. Church's noun phrase tagger (Church, 1988), one of the rst noun chunkers, was based on a Hidden Markov Model (HMM) similar to those used Thanks to Mats Rooth and Uli Heid for many helpful comments.  for part-of-speech tagging. Another HMM-based approach has been developed by Mats Rooth (Rooth, 1992). It integrates two HMMs; one of them models noun chunks internally, the other models the context of noun chunks. Abney's cascaded nitestate parser (Abney, 1996) also contains a processing step which recognises noun chunks and other types of chunks. Ramshaw and Marcus (Ramshaw and Marcus, 1995) successfully applied Eric Brill's transformation-based learning method to the chunking problem. Voutilainen's NPtool (Voutilainen, 1993) is based on his constraint-grammar system. Finally, Brants (Brants, 1999) described a German chunker which was implemented with cascaded Markov Models. In this paper, a probabilistic context-free parser is applied to the noun chunking task. The German grammar used in the experiments was semiautomatically extended with robustness rules in order to be able to process arbitrary input. The grammar parameters were trained on unlabelled data. A novel algorithm is"
C00-2105,W93-0306,0,0.0200504,"similar to those used Thanks to Mats Rooth and Uli Heid for many helpful comments.  for part-of-speech tagging. Another HMM-based approach has been developed by Mats Rooth (Rooth, 1992). It integrates two HMMs; one of them models noun chunks internally, the other models the context of noun chunks. Abney's cascaded nitestate parser (Abney, 1996) also contains a processing step which recognises noun chunks and other types of chunks. Ramshaw and Marcus (Ramshaw and Marcus, 1995) successfully applied Eric Brill's transformation-based learning method to the chunking problem. Voutilainen's NPtool (Voutilainen, 1993) is based on his constraint-grammar system. Finally, Brants (Brants, 1999) described a German chunker which was implemented with cascaded Markov Models. In this paper, a probabilistic context-free parser is applied to the noun chunking task. The German grammar used in the experiments was semiautomatically extended with robustness rules in order to be able to process arbitrary input. The grammar parameters were trained on unlabelled data. A novel algorithm is used for noun chunk extraction. It maximises the probability of the chunk set. The following section introduces the grammar framework, fo"
C00-2105,C98-1035,0,\N,Missing
C00-2108,W98-1505,0,0.108782,"Missing"
C00-2108,C96-1055,0,0.136597,"Missing"
C00-2108,P98-1112,0,0.270578,"Missing"
C00-2108,P99-1051,0,0.184713,"Missing"
C00-2108,W97-0209,0,0.0491742,"ation frame types they appeared with. These frequency counts then represented the syntactic description of the verbs. The next step was to rene the subcategorisation frame types by a preferential ordering on conceptual classes for the argument slots in the frames. The basis I could use for the selectional preferences was provided by the lexical heads in the frame tokens. For example, the nouns appearing in the direct object slot of the transitive frame for the verb drink included coee, milk, beer, indicating a conceptual class like beverage for this argument slot. I followed (Resnik, 1993)/(Resnik, 1997) who dened selectional preference as the amount of information a verb provides about its semantic argument classes. He utilised the WordNet taxonomy (Beckwith et al., 1991) for a probabilistic model capturing the co-occurrence behaviour of verbs and conceptual classes, where the conceptual classes were identied by WordNet synsets, sets of synonymous nouns within a semantic hierarchy. Referring to the above example, the three nouns coee, milk, beer are in three dierent synsets since they are not synonyms, but are all subordinated to the synset {beverage, drink, potable}. The goal in this"
C00-2108,E95-1016,0,0.0171936,"the verb P 4. c 2class f (c0s ) equals f (s), the frequency of the argument slot within a certain frame type, since summing over all possible classes within a subcategorisation frame slot equals the number of times the slot appeared 5. f (s): number of times the frame type appeared, since the frequency of a frame type equals the frequency of that frame with a certain slot marked The frequencies of a semantic class concerning an argument slot of a frame type (dependent or independent of a verb) were calculated by an approach slightly dierent to Resnik&apos;s, originally proposed by (Ribas, 1994)/(Ribas, 1995). For each noun appearing in a certain argument position its frequency was divided by the number of senses the noun was assigned by the WordNet hierarchy,1 to take account of the uncertainty about the sense of the noun. The fraction was allocated to each conceptual class in the hierarchy to which the noun belonged and accumulated upwards until a top node was reached. The result was a numerical distribution over the WordNet classes: 0 f (cs ) = X noun2cs f (noun) jsenses(noun)j (8) 1 For example, when considering the noun coee isolated from its context, we do not know whether we are talking ab"
C00-2108,E99-1007,0,0.215204,"Missing"
C00-2108,C98-1108,0,\N,Missing
C16-1254,D14-1151,0,0.0604566,"Missing"
C16-1254,P15-1144,0,0.258715,"create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings. The resulting word embeddings were used in similarity tasks, and to distinguish between antonyms and synonyms. Faruqui et al. (2015) improved word embeddings without relying on lexical resources, by applying ideas from sparse coding to transform dense word embeddings into sparse word embeddings. The dense vectors in their models can be transformed into sparse overcomplete vectors or sparse binary overcomplete vectors. They showed that the resulting vector representations were more similar to interpretable features in NLP and outperformed the original vector representations on several benchmark tasks. In this paper, we aim to improve word embeddings by reducing their noise. The hypothesis behind our approaches is that word"
C16-1254,J15-4004,0,0.0362333,"Z strongly differ from the word embeddings X; hence, the denoising is affected. However, the performance of the vectors Z∗ still outperforms the original vectors X, A and B after the denoising process. 3.3.1 Relatedness and Similarity Tasks For the relatedness task, we use two kinds of datasets: MEN (Bruni et al., 2014) consists of 3000 word pairs comprising 656 nouns, 57 adjectives and 38 verbs. The WordSim-353 relatedness dataset (Finkelstein et al., 2001) contains 252 word pairs. Concerning the similarity tasks, we evaluate the denoising vectors again on two kinds of datasets: SimLex-999 (Hill et al., 2015) contains 999 word pairs including 666 noun, 222 verb and 111 adjective pairs. The WordSim-353 similarity dataset consists of 203 word pairs. In addition, we evaluate our denoising vectors on the WordSim-353 dataset which contains 353 pairs for both similarity and relatedness relations. We calculate cosine similarity between the vectors of two words forming a test pair, and report the Spearman rank-order correlation coefficient ρ (Siegel and Castellan, 1988) against the respective gold standards of human ratings. 3.3.2 Synonymy We evaluate on 80 TOEFL (Test of English as a Foreign Language) sy"
C16-1254,D14-1181,0,0.00993301,"embeddings. The word denoising embeddings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings. 1 Introduction Word embeddings aim to represent words as low-dimensional dense vectors. In comparison to distributional count vectors, word embeddings address the problematic sparsity of word vectors and achieved impressive results in many NLP tasks such as sentiment analysis (e.g., Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.g., Lazaridou et al. (2013)). Moreover, word embeddings are attractive because they can be learned in an unsupervised fashion from unlabeled raw corpora. There are two main approaches to create word embeddings. The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013). The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices. In recent years, a number of a"
C16-1254,D13-1196,0,0.157921,"formation and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings. 1 Introduction Word embeddings aim to represent words as low-dimensional dense vectors. In comparison to distributional count vectors, word embeddings address the problematic sparsity of word vectors and achieved impressive results in many NLP tasks such as sentiment analysis (e.g., Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.g., Lazaridou et al. (2013)). Moreover, word embeddings are attractive because they can be learned in an unsupervised fashion from unlabeled raw corpora. There are two main approaches to create word embeddings. The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013). The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices. In recent years, a number of approaches have focused on improving word embeddings, often by integrating lexical resources. Fo"
C16-1254,N13-1090,0,0.485452,"rs. In comparison to distributional count vectors, word embeddings address the problematic sparsity of word vectors and achieved impressive results in many NLP tasks such as sentiment analysis (e.g., Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.g., Lazaridou et al. (2013)). Moreover, word embeddings are attractive because they can be learned in an unsupervised fashion from unlabeled raw corpora. There are two main approaches to create word embeddings. The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013). The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices. In recent years, a number of approaches have focused on improving word embeddings, often by integrating lexical resources. For example, Adel and Sch¨utze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Sk"
C16-1254,P16-2074,1,0.794306,"factorizing word-context co-occurrence matrices. In recent years, a number of approaches have focused on improving word embeddings, often by integrating lexical resources. For example, Adel and Sch¨utze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings. The resulting word embeddings were used in similarity tasks, and to distinguish between antonyms and synonyms. Faruqui et al. (2015) improved word embeddings without relying on lexical resources, by applying ideas from sparse coding to transform dense word embeddings into sparse word embeddings. The dense vectors in their models can be transformed into sparse overcomplete vectors or sparse binary overcomplete vectors. They showed that the resulting vector re"
C16-1254,D14-1162,0,0.0854031,"dings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings. 1 Introduction Word embeddings aim to represent words as low-dimensional dense vectors. In comparison to distributional count vectors, word embeddings address the problematic sparsity of word vectors and achieved impressive results in many NLP tasks such as sentiment analysis (e.g., Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.g., Lazaridou et al. (2013)). Moreover, word embeddings are attractive because they can be learned in an unsupervised fashion from unlabeled raw corpora. There are two main approaches to create word embeddings. The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013). The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices. In recent years, a number of approaches have focused on improving word embedding"
C16-1254,P15-2004,0,0.0137759,"approaches to create word embeddings. The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013). The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices. In recent years, a number of approaches have focused on improving word embeddings, often by integrating lexical resources. For example, Adel and Sch¨utze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings. The resulting word embeddings were used in similarity tasks, and to distinguish between antonyms and synonyms. Faruqui et al. (2015) improved word embeddings without relying on lexi"
C16-1254,schafer-bildhauer-2012-building,0,0.0608687,"Missing"
C18-1070,D16-1250,0,0.0269853,"ss of information, as they must reduce the effect of discriminant features which are domain-dependent. We argue in this paper that this leads to a decreased performance especially in cases where the similarity between the domains is low. Unlike previous approaches, in this paper, we propose a domain adaptation approach based on lessons learned from cross-lingual sentiment analysis (Barnes et al., 2018). This approach maintains the domaindependent features, while adapting them to the target domain. Following state-of-the-art approaches to create bilingual word embeddings (Mikolov et al., 2013; Artetxe et al., 2016; Artetxe et al., 2017), we learn to project a mapping from a source domain vector space to the target domain space, while jointly training a sentiment classifier for the source domain. We show that our proposed model (1) performs comparably to state-of-the-art models when domains are similar and (2) outperforms state-of-the-art models significantly on divergent domains. We report novel state-of-the-art results on 11 domain pairs. We also contribute a detailed error analysis and compare the effect of different projection lexicons. Our code is available at https://github.com/ jbarnesspain/domai"
C18-1070,P17-1042,0,0.0289435,"they must reduce the effect of discriminant features which are domain-dependent. We argue in this paper that this leads to a decreased performance especially in cases where the similarity between the domains is low. Unlike previous approaches, in this paper, we propose a domain adaptation approach based on lessons learned from cross-lingual sentiment analysis (Barnes et al., 2018). This approach maintains the domaindependent features, while adapting them to the target domain. Following state-of-the-art approaches to create bilingual word embeddings (Mikolov et al., 2013; Artetxe et al., 2016; Artetxe et al., 2017), we learn to project a mapping from a source domain vector space to the target domain space, while jointly training a sentiment classifier for the source domain. We show that our proposed model (1) performs comparably to state-of-the-art models when domains are similar and (2) outperforms state-of-the-art models significantly on divergent domains. We report novel state-of-the-art results on 11 domain pairs. We also contribute a detailed error analysis and compare the effect of different projection lexicons. Our code is available at https://github.com/ jbarnesspain/domain_blse. 2 Related Work"
C18-1070,P18-1231,1,0.872925,"ce to a latent hidden space. While pivot-based domain adaptation methods are well-motivated, they are often outperformed by autoencoder methods. However, both approaches to domain adaptation effectively lead to a loss of information, as they must reduce the effect of discriminant features which are domain-dependent. We argue in this paper that this leads to a decreased performance especially in cases where the similarity between the domains is low. Unlike previous approaches, in this paper, we propose a domain adaptation approach based on lessons learned from cross-lingual sentiment analysis (Barnes et al., 2018). This approach maintains the domaindependent features, while adapting them to the target domain. Following state-of-the-art approaches to create bilingual word embeddings (Mikolov et al., 2013; Artetxe et al., 2016; Artetxe et al., 2017), we learn to project a mapping from a source domain vector space to the target domain space, while jointly training a sentiment classifier for the source domain. We show that our proposed model (1) performs comparably to state-of-the-art models when domains are similar and (2) outperforms state-of-the-art models significantly on divergent domains. We report n"
C18-1070,W06-1615,0,0.8895,". We show that our proposed model (1) performs comparably to state-of-the-art models when domains are similar and (2) outperforms state-of-the-art models significantly on divergent domains. We report novel state-of-the-art results on 11 domain pairs. We also contribute a detailed error analysis and compare the effect of different projection lexicons. Our code is available at https://github.com/ jbarnesspain/domain_blse. 2 Related Work and Motivation Domain adaptation is an omnipresent challenge in natural language processing. It has been applied for many tasks, such as part-of-speech tagging (Blitzer et al., 2006; Daume III, 2007), parsing (Blitzer et al., 2006; Finkel and Manning, 2009; McClosky et al., 2010), or named entity recognition (Daume III, 2007; Guo et al., 2009; Yu and Jiang, 2015). In the following, we limit our review to adaptation techniques which have been applied to sentiment analysis. 2.1 Pivot-based Approaches Blitzer et al. (2006) propose structural correspondence learning (S CL), which introduces the concept of pivots. These are features that behave in the same way for discriminative learning for both domains, e. g., good or terrible for sentiment analysis. The intuition is that n"
C18-1070,P07-1056,0,0.819641,"Licence details: http:// creativecommons.org/licenses/by/4.0/ 818 Proceedings of the 27th International Conference on Computational Linguistics, pages 818–830 Santa Fe, New Mexico, USA, August 20-26, 2018. 1 Introduction One of the main limitations of current approaches to sentiment analysis is that they are sensitive to differences in domain. This leads to classifiers that, after training, perform poorly on new domains (Pang and Lee, 2008; Deriu et al., 2017). Domain adaptation techniques provide a solution to reduce the discrepancy and enable models to perform well across multiple domains (Blitzer et al., 2007). The two main approaches to domain adaptation for sentiment analysis are pivot-based methods (Blitzer et al., 2007; Pan et al., 2010; Yu and Jiang, 2016), which augment the feature space with domain-independent features learned on unsupervised data, and autoencoder approaches (Glorot et al., 2011; Chen et al., 2012), which seek to create a good general mapping from a sentence to a latent hidden space. While pivot-based domain adaptation methods are well-motivated, they are often outperformed by autoencoder methods. However, both approaches to domain adaptation effectively lead to a loss of in"
C18-1070,P14-1058,0,0.0633112,"ges of being less interpretable, requiring long training times, and only utilizing a small amount of the original feature space. 2.3 Domain Specific Word Representations A third approach is to create word representations that provide useful features for multiple domains. He et al. (2011) propose a joint sentiment-topic model which uses pivots to change the topic-word Dirichlet priors. Bollegala et al. (2015) create domain-specific embeddings for pivots and non-pivots with the constraint that the pivot representations are similar across domains. The work that is most similar to ours is that of Bollegala et al. (2014). Their method learns to predict differences in word distributions across domains by learning to project lower-dimensional SVD representations of documents across domains. Unlike our work, however, they learn the projection step separately from the classification. They also only learn to project the features that the two domains have in common, which implies discarding information useful for classification. These approaches, however, perform worse than M SDA and N SCL. 3 Projecting Representations Our approach is motivated by previous success in learning to project embeddings across languages"
C18-1070,P15-1071,0,0.0257084,"significant gain in speed, as well as the ability to include more features from the original representations. Autoencoder models perform better than earlier S CL models (excluding N SCL), but have the disadvantages of being less interpretable, requiring long training times, and only utilizing a small amount of the original feature space. 2.3 Domain Specific Word Representations A third approach is to create word representations that provide useful features for multiple domains. He et al. (2011) propose a joint sentiment-topic model which uses pivots to change the topic-word Dirichlet priors. Bollegala et al. (2015) create domain-specific embeddings for pivots and non-pivots with the constraint that the pivot representations are similar across domains. The work that is most similar to ours is that of Bollegala et al. (2014). Their method learns to predict differences in word distributions across domains by learning to project lower-dimensional SVD representations of documents across domains. Unlike our work, however, they learn the projection step separately from the classification. They also only learn to project the features that the two domains have in common, which implies discarding information usef"
C18-1070,P07-1033,0,0.468904,"Missing"
C18-1070,W17-1103,0,0.033437,"Missing"
C18-1070,E14-1049,0,0.0350047,"CL or M SDA. In fact, M SDA performs very poorly on the minority negative class, with error rates reaching 98 percent. N SCL almost always favors a single class, with error rates as high as 60.4 on negative and 70.5 on positive. 5.3 Choice of Projection Lexicon Given that the choice of projection lexicon is one of the key parameters in the B LSE model, we experiment with three approaches to creating a projection lexicon and observe their effect on the books to SemEval 2013 setup. The Most Frequent Source Words are a common source of projection lexicon in the multilingual embedding literature (Faruqui and Dyer, 2014; Lazaridou et al., 2015). For our experiment, we take the 20,000 most frequent tokens from the Brown corpus (Francis and Kuˇcera, 1979). The hypothesis behind using a general corpus is that a large general lexicon will provide more supervision than a smaller task-specific lexicon. This should contribute to learning accurate projection matrices M and M 0 . Sentiment Lexicons often contain domain-independent words that convey sentiment. In our model, using a sentiment lexicon as a translation dictionary is equivalent to the use of pivots in other frameworks, as these are usually domain independ"
C18-1070,N09-1068,0,0.0333024,"he-art models when domains are similar and (2) outperforms state-of-the-art models significantly on divergent domains. We report novel state-of-the-art results on 11 domain pairs. We also contribute a detailed error analysis and compare the effect of different projection lexicons. Our code is available at https://github.com/ jbarnesspain/domain_blse. 2 Related Work and Motivation Domain adaptation is an omnipresent challenge in natural language processing. It has been applied for many tasks, such as part-of-speech tagging (Blitzer et al., 2006; Daume III, 2007), parsing (Blitzer et al., 2006; Finkel and Manning, 2009; McClosky et al., 2010), or named entity recognition (Daume III, 2007; Guo et al., 2009; Yu and Jiang, 2015). In the following, we limit our review to adaptation techniques which have been applied to sentiment analysis. 2.1 Pivot-based Approaches Blitzer et al. (2006) propose structural correspondence learning (S CL), which introduces the concept of pivots. These are features that behave in the same way for discriminative learning for both domains, e. g., good or terrible for sentiment analysis. The intuition is that non-pivot domain-dependent features, e. g., well-written for the book domain"
C18-1070,N09-1032,0,0.0710411,"on divergent domains. We report novel state-of-the-art results on 11 domain pairs. We also contribute a detailed error analysis and compare the effect of different projection lexicons. Our code is available at https://github.com/ jbarnesspain/domain_blse. 2 Related Work and Motivation Domain adaptation is an omnipresent challenge in natural language processing. It has been applied for many tasks, such as part-of-speech tagging (Blitzer et al., 2006; Daume III, 2007), parsing (Blitzer et al., 2006; Finkel and Manning, 2009; McClosky et al., 2010), or named entity recognition (Daume III, 2007; Guo et al., 2009; Yu and Jiang, 2015). In the following, we limit our review to adaptation techniques which have been applied to sentiment analysis. 2.1 Pivot-based Approaches Blitzer et al. (2006) propose structural correspondence learning (S CL), which introduces the concept of pivots. These are features that behave in the same way for discriminative learning for both domains, e. g., good or terrible for sentiment analysis. The intuition is that non-pivot domain-dependent features, e. g., well-written for the book domain or reliable for electronics, which are highly correlated to a pivot should be treated t"
C18-1070,P11-1013,0,0.0161035,"transformations which are performed in closed-form, with the non-linearity being applied afterwards. This leads to a significant gain in speed, as well as the ability to include more features from the original representations. Autoencoder models perform better than earlier S CL models (excluding N SCL), but have the disadvantages of being less interpretable, requiring long training times, and only utilizing a small amount of the original feature space. 2.3 Domain Specific Word Representations A third approach is to create word representations that provide useful features for multiple domains. He et al. (2011) propose a joint sentiment-topic model which uses pivots to change the topic-word Dirichlet priors. Bollegala et al. (2015) create domain-specific embeddings for pivots and non-pivots with the constraint that the pivot representations are similar across domains. The work that is most similar to ours is that of Bollegala et al. (2014). Their method learns to predict differences in word distributions across domains by learning to project lower-dimensional SVD representations of documents across domains. Unlike our work, however, they learn the projection step separately from the classification."
C18-1070,P15-1027,0,0.0315887,"SDA performs very poorly on the minority negative class, with error rates reaching 98 percent. N SCL almost always favors a single class, with error rates as high as 60.4 on negative and 70.5 on positive. 5.3 Choice of Projection Lexicon Given that the choice of projection lexicon is one of the key parameters in the B LSE model, we experiment with three approaches to creating a projection lexicon and observe their effect on the books to SemEval 2013 setup. The Most Frequent Source Words are a common source of projection lexicon in the multilingual embedding literature (Faruqui and Dyer, 2014; Lazaridou et al., 2015). For our experiment, we take the 20,000 most frequent tokens from the Brown corpus (Francis and Kuˇcera, 1979). The hypothesis behind using a general corpus is that a large general lexicon will provide more supervision than a smaller task-specific lexicon. This should contribute to learning accurate projection matrices M and M 0 . Sentiment Lexicons often contain domain-independent words that convey sentiment. In our model, using a sentiment lexicon as a translation dictionary is equivalent to the use of pivots in other frameworks, as these are usually domain independent words with are good p"
C18-1070,S13-2052,0,0.0444366,"constructing a lexicon that maps concepts from one domain to those of another, i. e. “read” in the books domain and “watch” for movies. In order to create a mapping from both original vector spaces S and T to shared sentiment-informed bi-domain spaces z and ˆ z, we employ two linear projection matrices, M and M 0 . During training, for each translation pair in L, we first look up their associated vectors, project them through their associated projection matrix and finally minimize the mean squared error of the two projected vectors. This is very similar to the approach taken by Mikolov et al. (2013), but includes an additional target projection matrix. The projection quality is ensured by minimizing the mean squared error1 n MSE = 1X (zi − ˆ zi )2 , n (1) i=1 where zi = Ssi · M is the dot product of the embedding for source word si and the source projection matrix and ˆ zi = Tti · M 0 is the same for the target word ti and target matrix M 0 . The intuition for including this second matrix is that a single projection matrix does not support the transfer of sentiment information from the source domain to the target domain. Although this term is degenerate by itself, when coupled with the s"
C18-1070,S16-1001,0,0.0337518,"arget domain (Ziser and Reichart, 2017). We take the unlabeled data from each domain to create the domain embeddings for our method, as well as to train the domain independent representations for the N SCL and M SDA methods. In order to create embeddings for the Amazon corpora, we concatenate all of the unlabeled data from all domains. The statistics for this corpus are given in Table 1. 822 4.1.2 SemEval Corpora Sentiment analysis of Twitter data is common nowadays, with several popular shared tasks organized on the topic (Nakov et al., 2013; Villena-Rom´an et al., 2013; Basile et al., 2014; Nakov et al., 2016, i. a.). In order to evaluate how well domain adaptation techniques perform on large domain gaps, we also use the message polarity classification corpora provided by the organizers of SemEval 2013 and 2016 (Nakov et al., 2013; Nakov et al., 2016). We will refer to these as S13 and S16, respectively. These contain tweets which have been annotated for positive, negative, and neutral sentiment. We remove neutral tweets, giving us a binary setup which allows compatibility with the Amazon corpora. The statistics for these corpora are given in Table 1. 4.2 Embeddings For B LSE, we create mono-domai"
C18-1070,P15-2028,0,0.0202908,"ins. We report novel state-of-the-art results on 11 domain pairs. We also contribute a detailed error analysis and compare the effect of different projection lexicons. Our code is available at https://github.com/ jbarnesspain/domain_blse. 2 Related Work and Motivation Domain adaptation is an omnipresent challenge in natural language processing. It has been applied for many tasks, such as part-of-speech tagging (Blitzer et al., 2006; Daume III, 2007), parsing (Blitzer et al., 2006; Finkel and Manning, 2009; McClosky et al., 2010), or named entity recognition (Daume III, 2007; Guo et al., 2009; Yu and Jiang, 2015). In the following, we limit our review to adaptation techniques which have been applied to sentiment analysis. 2.1 Pivot-based Approaches Blitzer et al. (2006) propose structural correspondence learning (S CL), which introduces the concept of pivots. These are features that behave in the same way for discriminative learning for both domains, e. g., good or terrible for sentiment analysis. The intuition is that non-pivot domain-dependent features, e. g., well-written for the book domain or reliable for electronics, which are highly correlated to a pivot should be treated the same by a sentimen"
C18-1070,D16-1023,0,0.148162,"830 Santa Fe, New Mexico, USA, August 20-26, 2018. 1 Introduction One of the main limitations of current approaches to sentiment analysis is that they are sensitive to differences in domain. This leads to classifiers that, after training, perform poorly on new domains (Pang and Lee, 2008; Deriu et al., 2017). Domain adaptation techniques provide a solution to reduce the discrepancy and enable models to perform well across multiple domains (Blitzer et al., 2007). The two main approaches to domain adaptation for sentiment analysis are pivot-based methods (Blitzer et al., 2007; Pan et al., 2010; Yu and Jiang, 2016), which augment the feature space with domain-independent features learned on unsupervised data, and autoencoder approaches (Glorot et al., 2011; Chen et al., 2012), which seek to create a good general mapping from a sentence to a latent hidden space. While pivot-based domain adaptation methods are well-motivated, they are often outperformed by autoencoder methods. However, both approaches to domain adaptation effectively lead to a loss of information, as they must reduce the effect of discriminant features which are domain-dependent. We argue in this paper that this leads to a decreased perfo"
C18-1070,K17-1040,0,0.210985,"Section 4.3. 4.1 Datasets 4.1.1 Amazon Corpora In order to evaluate our proposed method, we use the corpus collected by Blitzer et al. (2007), which consists of Amazon product reviews from four domains: books (B), DVD (D), electronics (E), and kitchen (K). Each subcorpus contains a balanced labeled subset, with 1000 positive and 1000 negative reviews, as well as a much larger set of unlabeled reviews. We use the standard split of 1600 reviews from each domain as training data and the remaining 400 reviews as validation data. For testing, we use all of the 2000 reviews from the target domain (Ziser and Reichart, 2017). We take the unlabeled data from each domain to create the domain embeddings for our method, as well as to train the domain independent representations for the N SCL and M SDA methods. In order to create embeddings for the Amazon corpora, we concatenate all of the unlabeled data from all domains. The statistics for this corpus are given in Table 1. 822 4.1.2 SemEval Corpora Sentiment analysis of Twitter data is common nowadays, with several popular shared tasks organized on the topic (Nakov et al., 2013; Villena-Rom´an et al., 2013; Basile et al., 2014; Nakov et al., 2016, i. a.). In order to"
C18-1070,N10-1004,0,\N,Missing
D07-1018,alsina-etal-2002-catcg,1,0.719581,"of features across the three classes. Level morph func uni bi sem all Explanation morphological (derivational) properties syntactic function uni-gram distribution bi-gram distribution distributional cues of semantic properties combination of the 5 linguistic levels # Features 2 4 24 50 18 10.3 Table 1: Linguistic levels as feature sets. tion from the adjective database. Syntactic and semantic features encode distributional properties of adjectives. Syntactic features comprise three subtypes: (i) the syntactic function (level func) of the adjective, as assigned by a shallow Constraint Grammar (Alsina et al., 2002), distinguishing the modifier (pre-nominal or post-nominal) and predicative functions; (ii) a unigram distribution (level uni), independently encoding the parts of speech (POS) of the words preceding and following the adjective, respectively; and (iii) a bigram distribution (level bi), the POS bigram around the target adjective, considering only the 50 most frequent bigrams to avoid sparse features. Semantic features (level sem) expand syntactic features with heterogeneous shallow cues of semantic properties. Table 2 lists the semantic properties encoded in the features, as well as the number"
D07-1018,C04-1161,1,0.924213,"Missing"
D07-1018,W05-1009,1,0.884155,"Missing"
D07-1018,brants-2000-inter,0,0.233811,"Missing"
D07-1018,C00-1023,0,0.374331,"es (similar to determiners, like viele ‘many’), referential adjectives (heutige, ‘of today’), qualitative adjectives (equivalent to basic adjectives), classificatory adjectives (equivalent to object adjectives), and adjectives of origin (Stuttgarter, ‘from Stuttgart’). In a recent paper, Yallop et al. (2005) reported experiments on the acquisition of syntactic subcategorisation patterns for English adjectives. Apart from the above research with a classificatory flavour, other lines of research exploited lexical relations among adjectives for Word Sense Disambiguation (Justeson and Katz, 1995; Chao and Dyer, 2000). Work by Lapata (2001), contrary to the studies mentioned so far, focused on the meaning of adjective-noun combinations, not on that of adjectives alone. 8 Conclusion This paper has presented an architecture for the semantic classification of Catalan adjectives that explicitly includes polysemous classes. The focus of the architecture was on two issues: (i) finding an appropriate set of linguistic features, and (ii) defining an adequate architecture for the task. The investigation and comparison of features at various linguistic levels has shown that morphology plays a major role for the targ"
D07-1018,P93-1023,0,0.424175,"Missing"
D07-1018,P97-1023,0,0.543866,"Missing"
D07-1018,C00-1044,0,0.291835,"Missing"
D07-1018,H05-1124,0,0.0650012,"and as object in each of the binary decisions, it was deemed polysemous (BO). The motivation behind this approach was that polysemous adjectives should exhibit properties of all the classes involved. As a result, positive decisions on each binary classification can be made by the algorithm, which can be viewed as implicit polysemous assignments. This classification architecture is very popular in Machine Learning for multi-label problems, cf. (Schapire and Singer, 2000; Ghamrawi and McCallum, 2005), and has also been applied to NLP problems such as entity extraction and noun-phrase chunking (McDonald et al., 2005). The remainder of this section describes other methodological aspects of our experiments. 4.1 Classifier: Decision Trees As classifier for the binary decisions we chose Decision Trees, one of the most widely used Machine Learning techniques for supervised experiments (Witten and Frank, 2005). Decision Trees provide a transparent representation of the decisions made by the algorithm, and thus facilitate the inspection of results and the error analysis. The experiments were carried out with the freely available Weka software package. The particular algorithm chosen, Weka’s J48, is the latest op"
D07-1018,J01-3003,0,0.122499,"-the-art Machine Learning architecture for Multi-label Classification (Schapire and Singer, 2000; Ghamrawi and McCallum, 2005) and an Ensemble Classifier (Dietterich, 2002) with b) the definition of features at various levels of linguistic description. A proper treatment of polysemy is essential in the area of lexical acquisition, since polysemy repreToni Badia GLiCom Universitat Pompeu Fabra 08003 Barcelona toni.badia@upf.edu sents a pervasive phenomenon in natural language. However, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem (cf. Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes). There are a few exceptions to this tradition, such as Pereira et al. (1993), Rooth et al. (1999), Korhonen et al. (2003), who used soft clustering methods for multiple assignment to verb semantic classes. Our work addresses the lack of methodology in modelling a polysemous classification. We implement a multi-label classification architecture to handle polysemy. This paper concentrates on the classification of Catalan adjectives, but the general nature of the architec"
D07-1018,P93-1024,0,0.101669,"at various levels of linguistic description. A proper treatment of polysemy is essential in the area of lexical acquisition, since polysemy repreToni Badia GLiCom Universitat Pompeu Fabra 08003 Barcelona toni.badia@upf.edu sents a pervasive phenomenon in natural language. However, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem (cf. Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes). There are a few exceptions to this tradition, such as Pereira et al. (1993), Rooth et al. (1999), Korhonen et al. (2003), who used soft clustering methods for multiple assignment to verb semantic classes. Our work addresses the lack of methodology in modelling a polysemous classification. We implement a multi-label classification architecture to handle polysemy. This paper concentrates on the classification of Catalan adjectives, but the general nature of the architecture should allow related tasks to profit from our insights. As target classification for the experiments, a set of 210 Catalan adjectives was manually classified by experts into three simple and three p"
D07-1018,P97-1007,0,0.0156307,"ention in the Machine Learning community in the last decade (Dietterich, 2002). When building an ensemble classifier, several class proposals for each item are obtained, and one of them is chosen on the basis of majority voting, weighted voting, or more sophisticated decision methods. It has been shown that in most cases, the accuracy of the ensemble classifier is higher than the best individual classifier (Freund and Schapire, 1996; Dietterich, 2000; Breiman, 2001). Within NLP, ensemble classifiers have been applied, for instance, to genus term disambiguation in machinereadable dictionaries (Rigau et al., 1997), using a majority voting scheme upon several heuristics, and to part of speech tagging, by combining the class predictions of different algorithms (van Halteren et Levels morph+func+uni+bi+sem+all func+uni+bi+sem morph+func+sem+all bl all Full Ac. 84.0 ±0.06 81.5 ±0.04 72.4 ±0.03 51.0 ±0.0 62.3 ±2.3 Part. Ac. 95.7 ±0.02 95.9 ±0.01 89.3 ±0.02 65.2 ±0.0 90.7 ±1.6 Table 7: Results for ensemble classifier. al., 1998). The main reason for the general success of ensemble classifiers is that they gloss over the biases introduced by the individual systems. We implemented an ensemble classifier by usi"
D07-1018,P99-1014,0,0.0505561,"inguistic description. A proper treatment of polysemy is essential in the area of lexical acquisition, since polysemy repreToni Badia GLiCom Universitat Pompeu Fabra 08003 Barcelona toni.badia@upf.edu sents a pervasive phenomenon in natural language. However, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem (cf. Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes). There are a few exceptions to this tradition, such as Pereira et al. (1993), Rooth et al. (1999), Korhonen et al. (2003), who used soft clustering methods for multiple assignment to verb semantic classes. Our work addresses the lack of methodology in modelling a polysemous classification. We implement a multi-label classification architecture to handle polysemy. This paper concentrates on the classification of Catalan adjectives, but the general nature of the architecture should allow related tasks to profit from our insights. As target classification for the experiments, a set of 210 Catalan adjectives was manually classified by experts into three simple and three polysemous semantic cl"
D07-1018,J95-1001,0,0.496279,"een quantitative adjectives (similar to determiners, like viele ‘many’), referential adjectives (heutige, ‘of today’), qualitative adjectives (equivalent to basic adjectives), classificatory adjectives (equivalent to object adjectives), and adjectives of origin (Stuttgarter, ‘from Stuttgart’). In a recent paper, Yallop et al. (2005) reported experiments on the acquisition of syntactic subcategorisation patterns for English adjectives. Apart from the above research with a classificatory flavour, other lines of research exploited lexical relations among adjectives for Word Sense Disambiguation (Justeson and Katz, 1995; Chao and Dyer, 2000). Work by Lapata (2001), contrary to the studies mentioned so far, focused on the meaning of adjective-noun combinations, not on that of adjectives alone. 8 Conclusion This paper has presented an architecture for the semantic classification of Catalan adjectives that explicitly includes polysemous classes. The focus of the architecture was on two issues: (i) finding an appropriate set of linguistic features, and (ii) defining an adequate architecture for the task. The investigation and comparison of features at various linguistic levels has shown that morphology plays a m"
D07-1018,W03-0410,0,0.213867,"itecture for Multi-label Classification (Schapire and Singer, 2000; Ghamrawi and McCallum, 2005) and an Ensemble Classifier (Dietterich, 2002) with b) the definition of features at various levels of linguistic description. A proper treatment of polysemy is essential in the area of lexical acquisition, since polysemy repreToni Badia GLiCom Universitat Pompeu Fabra 08003 Barcelona toni.badia@upf.edu sents a pervasive phenomenon in natural language. However, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem (cf. Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes). There are a few exceptions to this tradition, such as Pereira et al. (1993), Rooth et al. (1999), Korhonen et al. (2003), who used soft clustering methods for multiple assignment to verb semantic classes. Our work addresses the lack of methodology in modelling a polysemous classification. We implement a multi-label classification architecture to handle polysemy. This paper concentrates on the classification of Catalan adjectives, but the general nature of the architecture should allow related tasks"
D07-1018,P03-1009,0,0.202383,". A proper treatment of polysemy is essential in the area of lexical acquisition, since polysemy repreToni Badia GLiCom Universitat Pompeu Fabra 08003 Barcelona toni.badia@upf.edu sents a pervasive phenomenon in natural language. However, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem (cf. Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes). There are a few exceptions to this tradition, such as Pereira et al. (1993), Rooth et al. (1999), Korhonen et al. (2003), who used soft clustering methods for multiple assignment to verb semantic classes. Our work addresses the lack of methodology in modelling a polysemous classification. We implement a multi-label classification architecture to handle polysemy. This paper concentrates on the classification of Catalan adjectives, but the general nature of the architecture should allow related tasks to profit from our insights. As target classification for the experiments, a set of 210 Catalan adjectives was manually classified by experts into three simple and three polysemous semantic classes. We deliberately d"
D07-1018,P98-1081,0,0.0824816,"Missing"
D07-1018,N01-1009,0,0.132463,"ke viele ‘many’), referential adjectives (heutige, ‘of today’), qualitative adjectives (equivalent to basic adjectives), classificatory adjectives (equivalent to object adjectives), and adjectives of origin (Stuttgarter, ‘from Stuttgart’). In a recent paper, Yallop et al. (2005) reported experiments on the acquisition of syntactic subcategorisation patterns for English adjectives. Apart from the above research with a classificatory flavour, other lines of research exploited lexical relations among adjectives for Word Sense Disambiguation (Justeson and Katz, 1995; Chao and Dyer, 2000). Work by Lapata (2001), contrary to the studies mentioned so far, focused on the meaning of adjective-noun combinations, not on that of adjectives alone. 8 Conclusion This paper has presented an architecture for the semantic classification of Catalan adjectives that explicitly includes polysemous classes. The focus of the architecture was on two issues: (i) finding an appropriate set of linguistic features, and (ii) defining an adequate architecture for the task. The investigation and comparison of features at various linguistic levels has shown that morphology plays a major role for the target classification, desp"
D07-1018,J93-2004,0,0.0461699,"Missing"
D07-1018,P05-1076,0,0.15006,"to automatically identify adjectival scales from corpora. Coordination information was used in Bohnet et al. (2002) for a classification task similar to the task we pursue, using a bootstrapping approach. The authors, however, pursued a classification that is not purely semantic, between quantitative adjectives (similar to determiners, like viele ‘many’), referential adjectives (heutige, ‘of today’), qualitative adjectives (equivalent to basic adjectives), classificatory adjectives (equivalent to object adjectives), and adjectives of origin (Stuttgarter, ‘from Stuttgart’). In a recent paper, Yallop et al. (2005) reported experiments on the acquisition of syntactic subcategorisation patterns for English adjectives. Apart from the above research with a classificatory flavour, other lines of research exploited lexical relations among adjectives for Word Sense Disambiguation (Justeson and Katz, 1995; Chao and Dyer, 2000). Work by Lapata (2001), contrary to the studies mentioned so far, focused on the meaning of adjective-noun combinations, not on that of adjectives alone. 8 Conclusion This paper has presented an architecture for the semantic classification of Catalan adjectives that explicitly includes p"
D07-1018,C98-1078,0,\N,Missing
D13-1115,Q13-1005,0,0.0162268,"Missing"
D13-1115,W11-2503,0,0.0199222,"rarily. Ideally, each cluster should have a common object or clear visual attribute, and words are express in terms of these visual commonalities. 3 http://simplecv.org 1149 We also compute GIST vectors (Oliva and Torralba, 2001) for every image using LearGIST (Douze et al., 2009). Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall “gist” of the whole image. It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality. Finally, as with the SURF features, we clustered the GIST representations for our images into 500 clusters, and represented words as membership in the clusters, forming the GIST Clusters modality. 4 Model Definition Our experiments are based on the multimodal extension of Latent Dirichlet Allocation developed by Andrews et al. (2009). Previously LDA has been successfully"
D13-1115,P12-1015,0,0.646619,"nguage where semantic representations of words are extended to include perceptual information. The underlying hypothesis is that the meanings of words are explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the “meaning of words is entirely given by other words” (Bruni et al., 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on w"
D13-1115,D10-1124,0,0.0736166,"Missing"
D13-1115,P10-1126,0,0.109391,"tung Universit¨at Stuttgart schulte@ims.uni-stuttgart.de Abstract The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual fe"
D13-1115,N10-1011,0,0.267596,"tung Universit¨at Stuttgart schulte@ims.uni-stuttgart.de Abstract The language grounding problem has come in many different flavors with just as many different approaches. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual fe"
D13-1115,Q13-1003,0,0.0207816,"r concatenation. Bruni et al. (2012b) show that the contextual visual words (i.e. the visual features around an object, rather than of the object itself) are even more useful at times, suggesting the plausibility of a sort of distributional hypothesis for images. More recently, Silberer et al. (2013) show that visual attribute classifiers, which have been immensely successful in object recognition (Farhadi et al., 2009), act as excellent substitutes for feature norms. Other work on modeling the meanings of verbs using video recognition has also begun showing great promise (Mathe et al., 2008; Regneri et al., 2013). The Computer Vision community has also benefited greatly from efforts to unify the two modalities. To name a few examples, Rohrbach et al. (2010) and Socher et al. (2013) show how semantic information from text can be used to improve zero-shot classification (i.e., classifying never-before-seen objects), and Motwani and Mooney (2012) show that verb clusters can be used to improve activity recognition in videos. 3 Data Our experiments use several existing and new data sets for each of our modalities. We employ a large web corpus and a large set of association norms. We also introduce two new"
D13-1115,D12-1137,1,0.691409,"semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are direc"
D13-1115,schulte-im-walde-etal-2012-association,1,0.738339,"lity For our Text modality, we use deWaC, a large German web corpus created by the WaCKy group (Baroni et al., 2009) containing approximately 1.7B word tokens. We filtered the corpus by: removing words with unprintable characters or encoding troubles; removing all stopwords; removing word types with a total frequency of less than 500; and removing documents with a length shorter than 100. The resulting corpus has 1,038,883 documents consisting of 75,678 word types and 466M word tokens. 3.2 Cognitive Modalities Association Norms (AN) is a collection of association norms collected by Schulte im Walde et al. (2012). In association norm experiments, subjects are presented with a cue word and asked to list the first few words that come to mind. With enough subjects and responses, association norms can provide a common and detailed view of the meaning components of cue words. After removing responses given only once in the entire study, the data set contains a total of 95,214 cue-response pairs for 1,012 nouns and 5,716 response types. Feature Norms (FN) is our new collection of feature norms for a group of 569 German nouns. We 1148 present subjects on Amazon Mechanical Turk with a cue noun and ask them to"
D13-1115,D12-1130,0,0.749809,"n devoted to multimodal or “grounded” models of language where semantic representations of words are extended to include perceptual information. The underlying hypothesis is that the meanings of words are explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the “meaning of words is entirely given by other words” (Bruni et al., 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional mo"
D13-1115,P13-1056,0,0.476413,"ords are extended to include perceptual information. The underlying hypothesis is that the meanings of words are explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient for a complete understanding of language. Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (Andrews et al., 2009; Steyvers, 2010; Feng and Lapata, 2010b; Bruni et al., 2011; Silberer and Lapata, 2012; Johns and Jones, 2012; Bruni et al., 2012a; Bruni et al., 2012b; Silberer et al., 2013). Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the “meaning of words is entirely given by other words” (Bruni et al., 2012b). In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was originally developed by 1146 Proceedings of"
D13-1115,P11-1096,0,0.00943226,"hes. Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning (Kate and Mooney, 2007). Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions (Chen and Mooney, 2011) or robot commands (Tellex et al., 2011; Matuszek et al., 2012). Some efforts have tackled tasks such as automatic image caption generation (Feng and Lapata, 2010a; Ordonez et al., 2011), text illustration (Joshi et al., 2006), or automatic location identification of Twitter users (Eisenstein et al., 2010; Wing and Baldridge, 2011; Roller et al., 2012). Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images."
D17-1022,E12-1004,0,0.526533,"ional inclusion hypothesis (Geffet and Dagan, 2005; ZhitomirskyGeffet and Dagan, 2009), or the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). These measures assign scores to semantic relation pairs, and hypernymy scores are expected to be higher than those of other relation pairs. Typically, Average Precision (AP) (Kotlerman et al., 2010) is applied to rank and distinguish between the predicted relations. Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (LR), to predict hypernymy. Across approaches, Shwartz et al. (2017) demonstrated that there is no single unsupervised measure which consistently deals well with discriminating hypernymy from other semantic relations. Furthermore, Levy et al. (2015) showed that supervised methods memorize prototypical hypernyms instead of learning a relation between two words. Approaches of hypernymy-specific embeddings utilize neural models to learn vector representations for"
D17-1022,W11-2501,0,0.135005,"BLESS Lenci&Benotto Weeds Hypernymy vs. other relations meronymy attribute antonymy synonymy other relations meronymy coordination attribute event other relations antonymy synonymy coordination Baseline 0.353 0.675 0.651 0.55 0.657 0.051 0.76 0.537 0.74 0.779 0.382 0.624 0.725 0.441 HyperScore 0.538 0.811 0.800 0.743 0.793 0.454 0.913 0.888 0.918 0.620 0.574 0.696 0.751 0.850 Table 2: AP results of HyperScore in comparison to state-of-the-art measures. Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2016). The evaluation was performed on four semantic relation datasets: BLESS (Baroni and Lenci, 2011), W EEDS (Weeds et al., 2004), EVAL UTION (Santus et al., 2015), and L ENCI &B ENOTTO (Benotto, 2015). Table 1 describes the detail of these datasets in terms of the semantic relations and the number of instances. The Average Precision (AP) ranking measure is used to evaluate the performance of the measures. Unsupervised Hypernymy Detection and Directionality In this section, we assess our model on two experimental setups: i) a ranking retrieval setup that expects hypernymy pairs to have a higher similarity score than instances from other semantic relations; ii) a classification setup that req"
D17-1022,W09-0215,0,0.543267,"e-extracted. The resulting term embeddings are fed to an SVM classifier to predict hypernymy. However, this model only learns term pairs without considering their contexts, leading to a lack of generalization for term embeddings. Tuan et al. (2016) introduced a dynamic weighting neural network to learn term embeddings that encode information about hypernymy and also about their contexts, considering all words between a hypernym and its Related Work Unsupervised hypernymy measures: A variety of directional measures for unsupervised hypernymy detection (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; 1 www.ims.uni-stuttgart.de/data/hypervec 234 hyponym in a sentence. The proposed model is trained on a set of hypernym relations extracted from WordNet (Miller, 1995). The embeddings are applied as features to detect hypernymy, using an SVM classifier. Tuan et al. (2016) handles the drawback of the approach by Yu et al. (2015), considering the contextual information between two terms; however the method still is not able to determine the directionality of a hypernym pair. Vendrov et al. (2016) proposed a method to encode order into learned distributed representations,"
D17-1022,N13-1073,0,0.0279829,"the representations, a mapping function between a source language (German, Italian) and our English HyperVec space is learned, by relying on the least-squares error method from previous work using cross-lingual data (Mikolov et al., 2013a) and different modalities (Lazaridou et al., 2015). To learn a mapping function between two languages, a one-to-one correspondence (word translations) between two sets of vectors is required. We obtained these translations by using the parallel Europarl3 V7 corpus for German–English and Italian–English. Word alignment counts were extracted using fast align (Dyer et al., 2013). We then assigned each source word to the English word with the maximum number of alignments in the parallel corpus. We could match 25,547 pairs for DE→EN and 47,475 pairs for IT→EN. Taking the aligned subset of both spaces, we assume that X is the matrix obtained by concatenating all source vectors, and likewise Y is the matrix obtained by concatenating all corresponding English elements. Applying the `2-regularized leastsquares error objective can be described using the following equation: Table 5: Results (ρ) of HyperScore and state-ofthe-art measures and word embedding models on graded le"
D17-1022,P05-1014,0,0.378666,"Missing"
D17-1022,N16-1018,0,0.0686701,"Missing"
D17-1022,P15-2020,0,0.675742,"Missing"
D17-1022,P15-1027,0,0.0164543,"he languagespecific representations are obtained using the same hyper-parameter settings as for our English SGNS model (cf. Section 4.1). As corpus resource we relied on Wikipedia dumps2 . Note that we do not use any additional resource, such as the German or Italian WordNet, to tune the embeddings for hypernymy detection. Based on the representations, a mapping function between a source language (German, Italian) and our English HyperVec space is learned, by relying on the least-squares error method from previous work using cross-lingual data (Mikolov et al., 2013a) and different modalities (Lazaridou et al., 2015). To learn a mapping function between two languages, a one-to-one correspondence (word translations) between two sets of vectors is required. We obtained these translations by using the parallel Europarl3 V7 corpus for German–English and Italian–English. Word alignment counts were extracted using fast align (Dyer et al., 2013). We then assigned each source word to the English word with the maximum number of alignments in the parallel corpus. We could match 25,547 pairs for DE→EN and 47,475 pairs for IT→EN. Taking the aligned subset of both spaces, we assume that X is the matrix obtained by con"
D17-1022,P16-2074,1,0.775424,"r relations. For example, the hypernym pair animal–frog will be assigned a higher cosine score than the co-hyponymy pair eagle–frog. Secondly, the embeddings are learned to capture the distributional hierarchy between hyponym and hypernym, as an indicator to differentiate between hypernym and hyponym. For example, given a hyponym–hypernym pair (p, q), we can exploit the Euclidean norms of ~q and p~ to differentiate between the two words, such that the Euclidean norm of the hypernym ~q is larger than the Euclidean norm of the hyponym p~. Inspired by the distributional lexical contrast model in Nguyen et al. (2016) for distinguishing antonymy from synonymy, this paper proposes two objective functions to learn hierarchical embeddings for hypernymy. Before moving Learning Hierarchical Embeddings Our approach makes use of a set of hypernyms which could be obtained from either exploiting the transitivity of the hypernymy relation (Fallucchi and Zanzotto, 2011) or lexical databases, to learn hierarchical embeddings. We rely on WordNet, a large lexical database of English (Fellbaum, 1998), and extract all hypernym–hyponym pairs for nouns and for verbs, including both direct and indirect hypernymy, e.g., anima"
D17-1022,S12-1012,0,0.570854,"d be closer to the vector of bird than to the vector of animal. We evaluate our HyperVec model on both unsupervised and supervised hypernymy detection and directionality tasks. In addition, we apply the model to the task of graded lexical entailment (Vuli´c et al., 2016), and we assess the capability of HyperVec on generalizing hypernymy by mapping to German and Italian. Results on benchmark datasets of hypernymy show that the hierarchical embeddings outperform state-of-the-art measures and previous embedding models. Furthermore, the implementation of our models is made publicly available.1 2 Lenci and Benotto, 2012) all rely on some variation of the distributional inclusion hypothesis: If u is a semantically narrower term than v, then a significant number of salient distributional features of u is expected to be included in the feature vector of v as well. In addition, Santus et al. (2014) proposed the distributional informativeness hypothesis, that hypernyms tend to be less informative than hyponyms, and that they occur in more general contexts than their hyponyms. All of these approaches represent words as vectors in distributional semantic models (Turney and Pantel, 2010), relying on the distributiona"
D17-1022,D14-1162,0,0.0901717,"utional semantic models (Turney and Pantel, 2010), relying on the distributional hypothesis (Harris, 1954; Firth, 1957). For evaluation, these directional models use the AP measure to assess the proportion of hypernyms at the top of a score-sorted list. In a different vein, Kiela et al. (2015) introduced three unsupervised methods drawn from visual properties of images to determine a concept’s generality in hypernymy tasks. Supervised hypernymy methods: The studies in this area are based on word embeddings which represent words as low-dimensional and realvalued vectors (Mikolov et al., 2013b; Pennington et al., 2014). Each hypernymy pair is encoded by some combination of the two word vectors, such as concatenation (Baroni et al., 2012) or difference (Roller et al., 2014; Weeds et al., 2014). Hypernymy is distinguished from other relations by using a classification approach, such as SVM or LR. Because word embeddings are trained for similar and symmetric vectors, it is however unclear whether the supervised methods do actually learn the asymmetry in hypernymy (Levy et al., 2015). Hypernymy-specific embeddings: These approaches are closest to our work. Yu et al. (2015) proposed a dynamic distance-margin mod"
D17-1022,N15-1098,0,0.270569,"rank and distinguish between the predicted relations. Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (LR), to predict hypernymy. Across approaches, Shwartz et al. (2017) demonstrated that there is no single unsupervised measure which consistently deals well with discriminating hypernymy from other semantic relations. Furthermore, Levy et al. (2015) showed that supervised methods memorize prototypical hypernyms instead of learning a relation between two words. Approaches of hypernymy-specific embeddings utilize neural models to learn vector representations for hypernymy. Yu et al. (2015) proposed a supervised method to learn term embeddings for hypernymy identification, based on pre-extracted hypernymy pairs. Recently, Tuan et al. (2016) proposed a dynamic weighting neural model to learn term embeddings in which the model encodes not only the information of hypernyms vs. hyponyms, but also their contextual information. The performance of"
D17-1022,E14-1054,0,0.105106,"tionality Kim Anh Nguyen, Maximilian K¨oper, Sabine Schulte im Walde, Ngoc Thang Vu Institut f¨ur Maschinelle Sprachverarbeitung Universit¨at Stuttgart Pfaffenwaldring 5B, 70569 Stuttgart, Germany {nguyenkh,koepermn,schulte,thangvu}@ims.uni-stuttgart.de Abstract tions. Distributional count approaches make use of either directionally unsupervised measures or of supervised classification methods. Unsupervised measures exploit the distributional inclusion hypothesis (Geffet and Dagan, 2005; ZhitomirskyGeffet and Dagan, 2009), or the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). These measures assign scores to semantic relation pairs, and hypernymy scores are expected to be higher than those of other relation pairs. Typically, Average Precision (AP) (Kotlerman et al., 2010) is applied to rank and distinguish between the predicted relations. Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (LR), to predict"
D17-1022,C14-1097,0,0.379172,"Missing"
D17-1022,E14-4008,1,0.931991,"y Detection and Directionality Kim Anh Nguyen, Maximilian K¨oper, Sabine Schulte im Walde, Ngoc Thang Vu Institut f¨ur Maschinelle Sprachverarbeitung Universit¨at Stuttgart Pfaffenwaldring 5B, 70569 Stuttgart, Germany {nguyenkh,koepermn,schulte,thangvu}@ims.uni-stuttgart.de Abstract tions. Distributional count approaches make use of either directionally unsupervised measures or of supervised classification methods. Unsupervised measures exploit the distributional inclusion hypothesis (Geffet and Dagan, 2005; ZhitomirskyGeffet and Dagan, 2009), or the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). These measures assign scores to semantic relation pairs, and hypernymy scores are expected to be higher than those of other relation pairs. Typically, Average Precision (AP) (Kotlerman et al., 2010) is applied to rank and distinguish between the predicted relations. Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (L"
D17-1022,W15-4208,0,0.125309,"ttribute antonymy synonymy other relations meronymy coordination attribute event other relations antonymy synonymy coordination Baseline 0.353 0.675 0.651 0.55 0.657 0.051 0.76 0.537 0.74 0.779 0.382 0.624 0.725 0.441 HyperScore 0.538 0.811 0.800 0.743 0.793 0.454 0.913 0.888 0.918 0.620 0.574 0.696 0.751 0.850 Table 2: AP results of HyperScore in comparison to state-of-the-art measures. Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2016). The evaluation was performed on four semantic relation datasets: BLESS (Baroni and Lenci, 2011), W EEDS (Weeds et al., 2004), EVAL UTION (Santus et al., 2015), and L ENCI &B ENOTTO (Benotto, 2015). Table 1 describes the detail of these datasets in terms of the semantic relations and the number of instances. The Average Precision (AP) ranking measure is used to evaluate the performance of the measures. Unsupervised Hypernymy Detection and Directionality In this section, we assess our model on two experimental setups: i) a ranking retrieval setup that expects hypernymy pairs to have a higher similarity score than instances from other semantic relations; ii) a classification setup that requires both hypernymy detection and directionality. 4.2.1 Rankin"
D17-1022,C14-1212,0,0.773944,"an, 2005; ZhitomirskyGeffet and Dagan, 2009), or the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). These measures assign scores to semantic relation pairs, and hypernymy scores are expected to be higher than those of other relation pairs. Typically, Average Precision (AP) (Kotlerman et al., 2010) is applied to rank and distinguish between the predicted relations. Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (LR), to predict hypernymy. Across approaches, Shwartz et al. (2017) demonstrated that there is no single unsupervised measure which consistently deals well with discriminating hypernymy from other semantic relations. Furthermore, Levy et al. (2015) showed that supervised methods memorize prototypical hypernyms instead of learning a relation between two words. Approaches of hypernymy-specific embeddings utilize neural models to learn vector representations for hypernymy. Yu et al. (2015) proposed a su"
D17-1022,schafer-bildhauer-2012-building,0,0.0253683,"Missing"
D17-1022,W03-1011,0,0.657699,"on the taxonomic relation data which is pre-extracted. The resulting term embeddings are fed to an SVM classifier to predict hypernymy. However, this model only learns term pairs without considering their contexts, leading to a lack of generalization for term embeddings. Tuan et al. (2016) introduced a dynamic weighting neural network to learn term embeddings that encode information about hypernymy and also about their contexts, considering all words between a hypernym and its Related Work Unsupervised hypernymy measures: A variety of directional measures for unsupervised hypernymy detection (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; 1 www.ims.uni-stuttgart.de/data/hypervec 234 hyponym in a sentence. The proposed model is trained on a set of hypernym relations extracted from WordNet (Miller, 1995). The embeddings are applied as features to detect hypernymy, using an SVM classifier. Tuan et al. (2016) handles the drawback of the approach by Yu et al. (2015), considering the contextual information between two terms; however the method still is not able to determine the directionality of a hypernym pair. Vendrov et al. (2016) proposed a method to encode order into le"
D17-1022,W14-5814,1,0.879094,"Missing"
D17-1022,C04-1146,0,0.643677,"Missing"
D17-1022,E17-1007,0,0.814922,"to semantic relation pairs, and hypernymy scores are expected to be higher than those of other relation pairs. Typically, Average Precision (AP) (Kotlerman et al., 2010) is applied to rank and distinguish between the predicted relations. Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (LR), to predict hypernymy. Across approaches, Shwartz et al. (2017) demonstrated that there is no single unsupervised measure which consistently deals well with discriminating hypernymy from other semantic relations. Furthermore, Levy et al. (2015) showed that supervised methods memorize prototypical hypernyms instead of learning a relation between two words. Approaches of hypernymy-specific embeddings utilize neural models to learn vector representations for hypernymy. Yu et al. (2015) proposed a supervised method to learn term embeddings for hypernymy identification, based on pre-extracted hypernymy pairs. Recently, Tuan et al. (2016) proposed a dynamic wei"
D17-1022,P06-1101,0,0.0488304,"asures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment. 1 Introduction Hypernymy represents a major semantic relation and a key organization principle of semantic memory (Miller and Fellbaum, 1991; Murphy, 2002). It is an asymmetric relation between two terms, a hypernym (superordinate) and a hyponym (subordiate), as in animal–bird and flower–rose, where the hyponym necessarily implies the hypernym, but not vice versa. From a computational point of view, automatic hypernymy detection is useful for NLP tasks such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011), recognizing textual entailment (Dagan et al., 2013), and text generation (Biran and McKeown, 2013), among many others. Two families of approaches to identify and discriminate hypernyms are predominent in NLP, both of them relying on word vector representa233 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 233–243 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics In this paper, we propose a novel neural model HyperVec to learn hierarchical embeddings that (i) discriminate hypernymy f"
D17-1022,J09-3004,0,0.0317116,"Missing"
D17-1022,D16-1039,0,0.42042,"Across approaches, Shwartz et al. (2017) demonstrated that there is no single unsupervised measure which consistently deals well with discriminating hypernymy from other semantic relations. Furthermore, Levy et al. (2015) showed that supervised methods memorize prototypical hypernyms instead of learning a relation between two words. Approaches of hypernymy-specific embeddings utilize neural models to learn vector representations for hypernymy. Yu et al. (2015) proposed a supervised method to learn term embeddings for hypernymy identification, based on pre-extracted hypernymy pairs. Recently, Tuan et al. (2016) proposed a dynamic weighting neural model to learn term embeddings in which the model encodes not only the information of hypernyms vs. hyponyms, but also their contextual information. The performance of this family of models is typically evaluated by using an SVM to discriminate hypernymy from other relations. We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order"
D17-1022,I13-1095,0,\N,Missing
D19-1477,W17-5202,1,0.843619,"tenated and fed into a classifier. 0 ∈ Rd+d ×c , where c is a model parameter, and a layer W2 ∈ Rc×o , where o is the number of output classes. The final prediction is computed as follows, where σ is a ReLU function (Nair and Hinton, 2010): yˆ = softmax (W2 (σ (W1 (lks)))) 3.2 (1) Linguistic Module The linguistic module is implemented using a recurrent neural network, concretely an LSTM (Hochreiter and Schmidhuber, 1997). Since LSTMs have become ubiquitous in NLP, we omit a detailed description of the inner workings of the model here and refer readers to Tai et al. (2015); Tang et al. (2016); Barnes et al. (2017) for overviews. We use a bidirectional LSTM (B I LSTM) (Graves, 2012), whose final states are concatenated in order to obtain the representation of the input text. 3.3 Social Module The goal of the social module is to return author representations which encode homophily relations among users, i.e., which assign similar vectors to users who are socially related. We model social relations using graphs G = (V, E), where V is the set of nodes representing individuals and E the set of edges representing relations among them. We use vi ∈ V to refer to a node in the social graph, and eij ∈ E to denot"
D19-1477,D14-1179,0,0.0326514,"Missing"
D19-1477,C18-1156,0,0.154726,"o make a prediction. We apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods. 1 Introduction The idea that extra-linguistic information about speakers can help language understanding has recently gained traction in NLP. Several studies have successfully exploited social information to classify user-generated language in downstream tasks such as sentiment analysis (Yang and Eisenstein, 2017), abusive speech identification (Mishra et al., 2018) and sarcasm detection (Hazarika et al., 2018; Wallace et al., 2016). The underlying goal is to capture the sociological phenomenon of homophily (McPherson et al., 2001) – i.e., people’s tendency to group together with others they share ideas, beliefs, and practices with – and to exploit it jointly with linguistic information to obtain richer text representations. In this paper, we advance ∗ Research conducted when the author was at the University of Amsterdam. this line of research. In particular, we address a common shortcoming in current models by using state-of-the-art graph neural networks to encode and leverage homophily relations."
D19-1477,P15-1073,0,0.0294632,"valuate its performance against commonly used models for user representation. • We show that exploiting social information leads to improvements in two tasks (stance and hate speech detection) and that our model significantly outperforms competing alternatives. • We perform an extended error analysis, in which we show the robustness across tasks of user representations based on social graphs, and the superiority of dynamic representations over static ones. 2 Related Work Several strands of research have explored different social features to create user representations for NLP in social media. Hovy (2015) and Hovy and Fornaciari (2018) focus on demographic information (age and gender), while Bamman et al. (2014) and Hovy and Purschke (2018) exploit geographic location to account for regional variation. Demographic and geographic information, however, need to be made explicit by users and thus are often not available or not reliable. To address this drawback, other studies have aimed at extracting user information by just observing users’ behaviour on social platforms. To tackle sarcasm detection on Reddit, Kolchinski and Potts (2018) assign to each user a random embedding that is updated durin"
D19-1477,D18-1469,0,0.0263307,"ads to improvements in two tasks (stance and hate speech detection) and that our model significantly outperforms competing alternatives. • We perform an extended error analysis, in which we show the robustness across tasks of user representations based on social graphs, and the superiority of dynamic representations over static ones. 2 Related Work Several strands of research have explored different social features to create user representations for NLP in social media. Hovy (2015) and Hovy and Fornaciari (2018) focus on demographic information (age and gender), while Bamman et al. (2014) and Hovy and Purschke (2018) exploit geographic location to account for regional variation. Demographic and geographic information, however, need to be made explicit by users and thus are often not available or not reliable. To address this drawback, other studies have aimed at extracting user information by just observing users’ behaviour on social platforms. To tackle sarcasm detection on Reddit, Kolchinski and Potts (2018) assign to each user a random embedding that is updated during the training phase, with the goal of learning individualised patterns of sarcasm usage. Wallace et al. (2016) and Hazarika et al. (2018)"
D19-1477,D18-1140,0,0.101488,"rent social features to create user representations for NLP in social media. Hovy (2015) and Hovy and Fornaciari (2018) focus on demographic information (age and gender), while Bamman et al. (2014) and Hovy and Purschke (2018) exploit geographic location to account for regional variation. Demographic and geographic information, however, need to be made explicit by users and thus are often not available or not reliable. To address this drawback, other studies have aimed at extracting user information by just observing users’ behaviour on social platforms. To tackle sarcasm detection on Reddit, Kolchinski and Potts (2018) assign to each user a random embedding that is updated during the training phase, with the goal of learning individualised patterns of sarcasm usage. Wallace et al. (2016) and Hazarika et al. (2018) address the same task, using ParagraphVector (Le and Mikolov, 2014) to condense all the past comments/posts of a user into a low dimensional vector, which is taken to capture their interests and opinions. All these studies use the concatenation of author and post embeddings for the final prediction, showing that adding author information leads to significant improvements. While the approaches disc"
D19-1477,C18-1093,1,0.938598,"nd combines it with linguistic information to make a prediction. We apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods. 1 Introduction The idea that extra-linguistic information about speakers can help language understanding has recently gained traction in NLP. Several studies have successfully exploited social information to classify user-generated language in downstream tasks such as sentiment analysis (Yang and Eisenstein, 2017), abusive speech identification (Mishra et al., 2018) and sarcasm detection (Hazarika et al., 2018; Wallace et al., 2016). The underlying goal is to capture the sociological phenomenon of homophily (McPherson et al., 2001) – i.e., people’s tendency to group together with others they share ideas, beliefs, and practices with – and to exploit it jointly with linguistic information to obtain richer text representations. In this paper, we advance ∗ Research conducted when the author was at the University of Amsterdam. this line of research. In particular, we address a common shortcoming in current models by using state-of-the-art graph neural network"
D19-1477,N19-1221,1,0.816547,"line of work has focused on leveraging the social connections of users in Twitter data. This methodology relies on creating a social graph where users are nodes connected to each other by their retweeting, mentioning, or following behaviour. Techniques such as Line (Tang et al., 2015), Node2Vec (Grover and Leskovec, 2016) or Graph Convolutional Networks (GCNs, Kipf and Welling, 2017) are then used to learn low-dimensional embeddings for each user, which have been shown to be beneficial in different downstream tasks when combined with textual information. For example, Mishra et al. (2018) and Mishra et al. (2019) use the concatenation strategy mentioned above for abusive language detection; Yang et al. (2016) optimise social and linguistic representations with two distinct scoring functions to perform entity linking; while Yang and Eisenstein (2017) use an ensemble learning setup for sentiment analysis, where the final prediction is given by the weighted combination of several classifiers, each exploring the social graph independently. Methods like Line, Node2Vec and GCNs create user representations by aggregating the information coming from their connections in the social graph, without making any di"
D19-1477,S16-1003,0,0.123827,"Missing"
D19-1477,D16-1244,0,0.0952175,"Missing"
D19-1477,D14-1162,0,0.103455,"vector size (200) and epochs (20). For the GAT encoder, we experiment with values 10, 15, 20, 25, 30, 50 for the size of the hidden layer; for the number of heads, we explore values 1, 2, 3, 4. We keep the number of hops equal to 1 and the alpha value for the Leaky ReLU of the attention heads equal to 0.2 across all the settings.5 Since our focus is on social information, we keep the hyperparameters of the linguistic module and the classifier fixed across all the settings. Namely, the BiLSTM has depth of 1, the hidden layer has 50 units, and uses 200-d GloVe embeddings pretrained on Twitter (Pennington et al., 2014). For the classifier, we set the dimensionality of the non-linear layer to 50. 4 We also experimented with updating author embeddings during training, but did not observe any difference in the results. 5 We implement PV using the Gensim library: https://radimrehurek.com/gensim/models/ doc2vec.html. For N2V, we use the implementation at: https://github.com/aditya-grover/ node2vec. For GAT, the implementation at: https://github.com/Diego999/pyGAT. 4710 4.3 Tasks and Datasets We test all the models on three Twitter datasets annotated for different tasks. For all datasets we tokenise and lowercase"
D19-1477,S17-2088,0,0.0569418,"Missing"
D19-1477,S15-2078,0,0.0194674,"the models using the same metrics used for the optimization process (see Section 4.4). In Table 2 we report the results, that we compute as the average of ten runs with random parameter initialization.9 We use the unpaired Welch’s t test to check for statistically significant difference between models. Tasks The results show that social information helps improve the performance on Stance and Hate Speech detection, while it has no effect for Sentiment Analysis. While this result contrasts with the one reported by Yang and Eisenstein (2017), who use a previous version of the Sentiment dataset (Rosenthal et al., 2015), it is not 9 Frequency LING LING+random LING+PV LING+N2V LING+GAT Sentiment 0.332 0.676 0.657 0.671 0.672 0.666 Stance 0.397 0.569 0.571 0.601∗ 0.629∗ 0.640∗† Hate 0.057 0.624 0.600 0.667∗ 0.656∗ 0.674∗† Table 2: Results for all the models on the three datasets in our experiment. Marked with ∗ are the results which significantly improve over LING and LING+random (p < 0.05, also for the following results);  indicates a significant improvement over LING+PV; † a significant improvement over LING+N2V. surprising given the analysis made in the previous section regarding the amount of homophily"
D19-1477,P15-1150,0,0.102031,"Missing"
D19-1477,C16-1311,0,0.0202065,"wo modules are concatenated and fed into a classifier. 0 ∈ Rd+d ×c , where c is a model parameter, and a layer W2 ∈ Rc×o , where o is the number of output classes. The final prediction is computed as follows, where σ is a ReLU function (Nair and Hinton, 2010): yˆ = softmax (W2 (σ (W1 (lks)))) 3.2 (1) Linguistic Module The linguistic module is implemented using a recurrent neural network, concretely an LSTM (Hochreiter and Schmidhuber, 1997). Since LSTMs have become ubiquitous in NLP, we omit a detailed description of the inner workings of the model here and refer readers to Tai et al. (2015); Tang et al. (2016); Barnes et al. (2017) for overviews. We use a bidirectional LSTM (B I LSTM) (Graves, 2012), whose final states are concatenated in order to obtain the representation of the input text. 3.3 Social Module The goal of the social module is to return author representations which encode homophily relations among users, i.e., which assign similar vectors to users who are socially related. We model social relations using graphs G = (V, E), where V is the set of nodes representing individuals and E the set of edges representing relations among them. We use vi ∈ V to refer to a node in the social graph"
D19-1477,K16-1017,0,0.29446,"apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods. 1 Introduction The idea that extra-linguistic information about speakers can help language understanding has recently gained traction in NLP. Several studies have successfully exploited social information to classify user-generated language in downstream tasks such as sentiment analysis (Yang and Eisenstein, 2017), abusive speech identification (Mishra et al., 2018) and sarcasm detection (Hazarika et al., 2018; Wallace et al., 2016). The underlying goal is to capture the sociological phenomenon of homophily (McPherson et al., 2001) – i.e., people’s tendency to group together with others they share ideas, beliefs, and practices with – and to exploit it jointly with linguistic information to obtain richer text representations. In this paper, we advance ∗ Research conducted when the author was at the University of Amsterdam. this line of research. In particular, we address a common shortcoming in current models by using state-of-the-art graph neural networks to encode and leverage homophily relations. Most current models re"
D19-1477,D16-1152,0,0.0199429,"ology relies on creating a social graph where users are nodes connected to each other by their retweeting, mentioning, or following behaviour. Techniques such as Line (Tang et al., 2015), Node2Vec (Grover and Leskovec, 2016) or Graph Convolutional Networks (GCNs, Kipf and Welling, 2017) are then used to learn low-dimensional embeddings for each user, which have been shown to be beneficial in different downstream tasks when combined with textual information. For example, Mishra et al. (2018) and Mishra et al. (2019) use the concatenation strategy mentioned above for abusive language detection; Yang et al. (2016) optimise social and linguistic representations with two distinct scoring functions to perform entity linking; while Yang and Eisenstein (2017) use an ensemble learning setup for sentiment analysis, where the final prediction is given by the weighted combination of several classifiers, each exploring the social graph independently. Methods like Line, Node2Vec and GCNs create user representations by aggregating the information coming from their connections in the social graph, without making any distinction among them. In contrast, we use Graph Attention Networks (GATs, Velickovic et al., 2018)"
D19-1477,Q17-1021,0,0.199647,"on given the most relevant connections for a target task, and combines it with linguistic information to make a prediction. We apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods. 1 Introduction The idea that extra-linguistic information about speakers can help language understanding has recently gained traction in NLP. Several studies have successfully exploited social information to classify user-generated language in downstream tasks such as sentiment analysis (Yang and Eisenstein, 2017), abusive speech identification (Mishra et al., 2018) and sarcasm detection (Hazarika et al., 2018; Wallace et al., 2016). The underlying goal is to capture the sociological phenomenon of homophily (McPherson et al., 2001) – i.e., people’s tendency to group together with others they share ideas, beliefs, and practices with – and to exploit it jointly with linguistic information to obtain richer text representations. In this paper, we advance ∗ Research conducted when the author was at the University of Amsterdam. this line of research. In particular, we address a common shortcoming in current"
E03-1037,C96-1055,0,0.417423,"e levels of verb description, purely syntactic frame types, prepositional phrase information and selectional preferences. In contrast to previous approaches concentrating on the sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification. 1 Introduction The verb is central to the meaning and the structure of a sentence, and lexical verb information represents the core in supporting NLP-tasks such as word sense disambiguation (Dorr and Jones, 1996; Prescher et al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im W"
E03-1037,P98-1112,0,0.043709,"onal preferences. In contrast to previous approaches concentrating on the sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification. 1 Introduction The verb is central to the meaning and the structure of a sentence, and lexical verb information represents the core in supporting NLP-tasks such as word sense disambiguation (Dorr and Jones, 1996; Prescher et al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Joanis, 2002). Computational approaches on verb classification whic"
E03-1037,kunze-2000-extension,0,0.0386273,"realisations for a specific verb-frame-slot combination in form of lexical heads. For example, the most prominent nominal argument heads for the verb verfolgen 'to follow' in the accusative NP slot of the transitive frame type 'rm.' (the considered frame slot is underlined) are Ziel 'goal', Strategie 'strategy', Politik 'policy'. Obviously, we would run into a sparse data problem if we tried to incorporate selectional preferences on the nominal level into the verb descriptions. We need a generalisation of the selectional preference definition, for which we use the noun hierarchy in GennaNet (Kunze, 2000), the German pendant of the semantic ontology WordNet (Fellbaum, 1998). The hierarchy is realised as synsets, sets of synonymous nouns, which are organised into multiple inheritance hypernym relationships. A noun may appear in several synsets, according to its number of senses. For each nominal argument in a verb317 frame-slot combination, the joint frequency is split over the different senses of the noun and propagated upwards the hierarchy. In case of multiple hypernym synsets, the frequency is split, such that the sum of frequencies over the disjoint top synsets equals the total joint frequ"
E03-1037,C00-2094,0,0.0397767,"iption, purely syntactic frame types, prepositional phrase information and selectional preferences. In contrast to previous approaches concentrating on the sparse data problem, we present evidence for a linguistically defined limit on the usefulness of features which is driven by the idiosyncratic properties of the verbs and the specific attributes of the desired verb classification. 1 Introduction The verb is central to the meaning and the structure of a sentence, and lexical verb information represents the core in supporting NLP-tasks such as word sense disambiguation (Dorr and Jones, 1996; Prescher et al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im Walde, 2000; Merlo and St"
E03-1037,P02-1029,1,0.924793,"dure in multivariate data analysis. It is designed to uncover an inherent natural structure of data objects, and the induced equivalence classes provide a means to generalise over the objects. We perform clustering by the k-Means algorithm (Forgy, 1965), an unsupervised hard clustering method assigning is data objects to k clusters. 2 Initial verb clusters are iteratively re-organised by assigning each verb to its closest cluster and re-calculating cluster centroids until no further changes take place. The clustering methodology in this work is based on parameter investigations in (Schulte im Walde and Brew, 2002): the clustering input is obtained from a hierarchical analysis on the German verbs (Ward's amalgamation method), the number of clusters being the number of manual classes; similarity measure is performed by the skew divergence, a variant of the Kullback-Leibler divergence. The 168 verbs are associated with probabilistic frame descriptions on various levels of verb information, and assigned to starting clusters by hierarchical clustering. The k-Means algorithm is then allowed to run until no further changes take place, and the resulting clusters are evaluated and interpreted against the manual"
E03-1037,C00-2108,1,0.947369,"6; Prescher et al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has established an extensive manual classification for English verbs; computational approaches adopt the linguistic hypothesis that verb meaning components to a certain extent determine verb behaviour as basis for automatically inducing semantic verb classes from corpusbased features (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Joanis, 2002). Computational approaches on verb classification which take advantage of corpus-based and knowledge-based verb information offered by available tools and resources such as statistical parsers and semantic ontologies, suffer from severe problems to encode and benefit from the information, especially with respect to selectional preferences, cf. Schulte im Walde (2000); Joanis (2002). This paper presents clustering experiments on German verbs which explore the relevance of features on three levels of verb description, purely syntactic frame types, prepos"
E03-1037,C98-1108,0,\N,Missing
E03-1037,J01-3003,0,\N,Missing
E14-4008,J10-4006,1,0.410132,"c Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). 38 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38–42, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we introduce SLQS, a new entropy-based distributional measure that aims to identify hypernyms by providing a distributional characterization of their semantic generality. We assess it with two tasks: (i.) the identification of the broader term in hyponym-hypernym pairs (directionality task); (ii.) the discrimination of hypernymy among other semantic relations (detectio"
E14-4008,J07-2002,0,0.0337174,"Distributional Semantic Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). 38 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38–42, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we introduce SLQS, a new entropy-based distributional measure that aims to identify hypernyms by providing a distributional characterization of their semantic generality. We assess it with two tasks: (i.) the identification of the broader term in hyponym-hypernym pairs (directionality task); (ii.) the discrimination of hypernymy among other sem"
E14-4008,W09-0215,0,0.851838,"Missing"
E14-4008,P05-1014,0,0.914436,"Missing"
E14-4008,W03-1011,0,0.848579,"we propose SLQS, which measures the semantic generality of a word by the entropy of its statistically most prominent contexts. Related work The problem of identifying asymmetric relations like hypernymy has so far been addressed in distributional semantics only in a limited way (Kotlerman et al., 2010) or treated through semisupervised approaches, such as pattern-based approaches (Hearst, 1992). The few works that have attempted a completely unsupervised approach to the identification of hypernymy in corpora have mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: , = SLQS: A new entropy-based measure For every term wi we identify the N most associated contexts c (where N is a parameter empirically set to 50)1. The association strength has been calculated with Local Mutual Information (LMI; Evert, 2005). For each"
E14-4008,C04-1146,0,0.70813,"h measures the semantic generality of a word by the entropy of its statistically most prominent contexts. Related work The problem of identifying asymmetric relations like hypernymy has so far been addressed in distributional semantics only in a limited way (Kotlerman et al., 2010) or treated through semisupervised approaches, such as pattern-based approaches (Hearst, 1992). The few works that have attempted a completely unsupervised approach to the identification of hypernymy in corpora have mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: , = SLQS: A new entropy-based measure For every term wi we identify the N most associated contexts c (where N is a parameter empirically set to 50)1. The association strength has been calculated with Local Mutual Information (LMI; Evert, 2005). For each selected context c,"
E14-4008,S12-1012,1,0.739596,"Missing"
E14-4008,C92-2082,0,\N,Missing
E14-4008,W11-2501,1,\N,Missing
E14-4008,W09-0200,0,\N,Missing
E17-1008,N13-1090,0,0.289181,"els and pattern-based models. These distributional semantic models (DSMs) offer a means to represent meaning vectors of words or word pairs, and to determine their semantic relatedness (Turney and Pantel, 2010). In co-occurrence models, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in particular contexts. When using word embeddings, these models rely on neural methods to represent words as low-dimensional vectors. To create the word embeddings, the models either make use of neural-based techniques, such as the skipgram model (Mikolov et al., 2013), or use matrix factorization (Pennington et al., 2014) that builds word embeddings by factorizing word-context cooccurrence matrices. In comparison to standard co-occurrence vector representations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.g., Vu et al. (2016)), and antonym-synonym distinction (e.g., Nguyen et al. (2016)). In pattern-based models, vector representations make use of lexico-syntactic surface patterns to distinguish be"
E17-1008,P16-2074,1,0.942688,"o create the word embeddings, the models either make use of neural-based techniques, such as the skipgram model (Mikolov et al., 2013), or use matrix factorization (Pennington et al., 2014) that builds word embeddings by factorizing word-context cooccurrence matrices. In comparison to standard co-occurrence vector representations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.g., Vu et al. (2016)), and antonym-synonym distinction (e.g., Nguyen et al. (2016)). In pattern-based models, vector representations make use of lexico-syntactic surface patterns to distinguish between the relations of word pairs. For example, Justeson and Katz (1991) suggested that adjectival opposites co-occur with each other in specific linear sequences, such as between X and Y. Hearst (1992) determined surface patterns, e.g., X such as Y, to identify nominal hypernyms. Lin et al. (2003) proposed two textual patterns indicating semantic incompatibility, from X to Y and either X or Y, to distinguish opposites from semantically similar Distinguishing between antonyms and s"
E17-1008,E12-1004,0,0.0310582,"s then fed into a logistic regression layer whose target is the class label associated with the pair (x, y). Finally, the pair (x, y) is predicted as positive (i.e., antonymous) word pair if the probability of the prediction for ~vxy is larger than 0.5. 4 Baseline Models To compare AntSynNET with baseline models for pattern-based classification of antonyms and synonyms, we introduce two pattern-based baseline methods: the distributional method (Section 4.1), and the distributed method (Section 4.2). 4.1 3.3.2 Combined AntSynNET Inspired by the supervised distributional concatenation method in Baroni et al. (2012) and the integrated path-based and distributional method for hypernymy detection in Shwartz et al. (2016), we Distributional Baseline As a first baseline, we apply the approach by Roth and Schulte im Walde (2014), henceforth R&SiW. They used a vector space model to represent pairs of words by a combination of standard lexico80 5 syntactic patterns and discourse markers. In addition to the patterns, the discourse markers added information to express discourse relations, which in turn may indicate the specific semantic relation between the two words in a word pair. For example, contrast relation"
E17-1008,N15-1100,0,0.059795,"t the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Instead of taking into account all words in a window of a certain size for feature extraction, the authors experimented with only words of a certain part-of-speech, and restricted distributions. Santus et al. (2014) proposed a different method to distinguish antonyms from synonyms by identifying the most salient dimensions of meaning in vector representations and reporting a new average-precision-based distributional measure and an entropy-based measure. Ono et al. (2015) trained supervised word embeddings for the task of identifying antonymy. They proposed two models to learn word embeddings: the first model relied on thesaurus information; the second model made use of distributional information and thesaurus information. More recently, Nguyen et al. (2016) proposed two methods to distinguish antonyms from synonyms: in the first method, the authors improved the quality of weighted feature vectors by strengthening those features that are most salient in the vectors, and by putting less emphasis on those that are of minor importance when distinguishing degrees"
E17-1008,D14-1162,0,0.0912449,"emantic models (DSMs) offer a means to represent meaning vectors of words or word pairs, and to determine their semantic relatedness (Turney and Pantel, 2010). In co-occurrence models, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in particular contexts. When using word embeddings, these models rely on neural methods to represent words as low-dimensional vectors. To create the word embeddings, the models either make use of neural-based techniques, such as the skipgram model (Mikolov et al., 2013), or use matrix factorization (Pennington et al., 2014) that builds word embeddings by factorizing word-context cooccurrence matrices. In comparison to standard co-occurrence vector representations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.g., Vu et al. (2016)), and antonym-synonym distinction (e.g., Nguyen et al. (2016)). In pattern-based models, vector representations make use of lexico-syntactic surface patterns to distinguish between the relations of word pairs. For example, Justeso"
E17-1008,P14-2086,1,0.701568,"Missing"
E17-1008,C92-2082,0,0.564987,"ntations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.g., Vu et al. (2016)), and antonym-synonym distinction (e.g., Nguyen et al. (2016)). In pattern-based models, vector representations make use of lexico-syntactic surface patterns to distinguish between the relations of word pairs. For example, Justeson and Katz (1991) suggested that adjectival opposites co-occur with each other in specific linear sequences, such as between X and Y. Hearst (1992) determined surface patterns, e.g., X such as Y, to identify nominal hypernyms. Lin et al. (2003) proposed two textual patterns indicating semantic incompatibility, from X to Y and either X or Y, to distinguish opposites from semantically similar Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations. In this paper, we present a novel neural network model AntSynNET that expl"
E17-1008,E14-4008,1,0.844498,"ssigning signs to the entries in the cooccurrence matrix on which latent semantic analysis operates, such that synonyms would tend to have positive cosine similarities, and antonyms would tend to have negative cosine similarities. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Instead of taking into account all words in a window of a certain size for feature extraction, the authors experimented with only words of a certain part-of-speech, and restricted distributions. Santus et al. (2014) proposed a different method to distinguish antonyms from synonyms by identifying the most salient dimensions of meaning in vector representations and reporting a new average-precision-based distributional measure and an entropy-based measure. Ono et al. (2015) trained supervised word embeddings for the task of identifying antonymy. They proposed two models to learn word embeddings: the first model relied on thesaurus information; the second model made use of distributional information and thesaurus information. More recently, Nguyen et al. (2016) proposed two methods to distinguish antonyms f"
E17-1008,P82-1020,0,0.778913,"Missing"
E17-1008,I13-1056,1,0.844685,"istribution of patterns, Schwartz et al. used patterns to learn word embeddings. Vector representation methods: Yih et al. (2012) introduced a new vector representation where antonyms lie on opposite sides of a sphere. They derived this representation with the incorporation of a thesaurus and latent semantic analhttps://github.com/nguyenkh/AntSynNET 77 3.1 ysis, by assigning signs to the entries in the cooccurrence matrix on which latent semantic analysis operates, such that synonyms would tend to have positive cosine similarities, and antonyms would tend to have negative cosine similarities. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Instead of taking into account all words in a window of a certain size for feature extraction, the authors experimented with only words of a certain part-of-speech, and restricted distributions. Santus et al. (2014) proposed a different method to distinguish antonyms from synonyms by identifying the most salient dimensions of meaning in vector representations and reporting a new average-precision-based distributional measure and an entropy-base"
E17-1008,J91-1001,0,0.38196,", 2014) that builds word embeddings by factorizing word-context cooccurrence matrices. In comparison to standard co-occurrence vector representations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.g., Vu et al. (2016)), and antonym-synonym distinction (e.g., Nguyen et al. (2016)). In pattern-based models, vector representations make use of lexico-syntactic surface patterns to distinguish between the relations of word pairs. For example, Justeson and Katz (1991) suggested that adjectival opposites co-occur with each other in specific linear sequences, such as between X and Y. Hearst (1992) determined surface patterns, e.g., X such as Y, to identify nominal hypernyms. Lin et al. (2003) proposed two textual patterns indicating semantic incompatibility, from X to Y and either X or Y, to distinguish opposites from semantically similar Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have p"
E17-1008,K15-1026,0,0.22757,"contexts, which makes it challenging to automatically distinguish between them. Two families of approaches to differentiate between antonyms and synonyms are predominent 76 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 76–85, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics words. Roth and Schulte im Walde (2014) proposed a method that combined patterns with discourse markers for classifying paradigmatic relations including antonymy, synonymy, and hypernymy. Recently, Schwartz et al. (2015) used two prominent patterns from Lin et al. (2003) to learn word embeddings that distinguished antonyms from similar words in determining degrees of similarity and word analogy. In this paper, we present a novel patternbased neural method AntSynNET to distinguish antonyms from synonyms. We hypothesize that antonymous word pairs co-occur with each other in lexico-syntactic patterns within a sentence more often than would be expected by synonymous pairs. This hypothesis is inspired by corpus-based studies on antonymy and synonymy. Among others, Charles and Miller (1989) suggested that adjectiva"
E17-1008,P16-1226,0,0.10629,"Missing"
E17-1008,N16-1065,1,0.89008,"Missing"
E17-1008,D12-1111,0,0.184336,"Missing"
E17-2086,P98-1012,0,0.0942963,"et al., 2016), containing a total of 400 German particle verbs Evaluation We evaluate our models on various semantic tasks: general predictions of semantic similarity, and specific tasks regarding complex German verbs, 1 https://radimrehurek.com/gensim/ models/hdpmodel.html 2 https://github.com/rscarberry-wa/ clodhopper 3 https://github.com/perdisci/jbirch 537 c-Means baseline with single-sense embeddings. (using every possible threshold within a range of [0.01, 0.99] to determine the memberships, and reporting the one providing the highest score). As evaluation measure we relied on B-Cubed (Bagga and Baldwin, 1998) and report f-score between the soft extension of precision and recall. Table 3 presents the results. Overall, C HIN R EST P works best, and C HIN W HISP and the BIRCH variants work similarly well. NPMSSGR is worst. A manual inspection revealed that NP-MSSGR assigns many verbs to multiple clusters, resulting in too large and fuzzy clusters. across 11 particle types. The results are presented in Table 2. C HIN W HISP performs significantly better than the baseline, while most other models are performing equally to or even inferior to the baseline. Model NP-MSSGR ChinRestP ChinWhisp HDP x-Means("
E17-2086,W11-2501,0,0.0111823,"on semantic relation identification (for nouns only) and semantic relatedness between sentences, and Iacobacci et al. (2015) who applied multi-sense embeddings to word and relational similarity. Introduction In recent years, a considerable number of semantic tasks and datasets have been developed, in order to evaluate the semantic quality of computational models. These tasks include general predictions of semantic similarity (e.g., relying on WordSim353 (Finkelstein et al., 2001) or SimLex-999 (Hill et al., 2015)); more specific predictions of semantic relation types (e.g., relying on BLESS (Baroni and Lenci, 2011) or the SemRel database (Scheible and Schulte im Walde, 2014)); predicting the degree of compositionality for complex nouns and verbs; etc. Computational semantic models predominantly make use of the distributional hypothesis in some way or the other, assuming that words with similar distributions have related meanings (Harris, 1954; Firth, 1957). Distributional models thus offer a means to represent meaning vectors of words, and to determine their semantic relatedness (Turney and Pantel, 2010). In this paper, we compare and extend approaches to obtain multi-sense embeddings, in order to model"
E17-2086,P14-1023,0,0.105012,"Missing"
E17-2086,J15-4004,0,0.024298,"ness tasks. Among the few exceptions are Li and Jurafsky (2015) who evaluated multisense embeddings on semantic relation identification (for nouns only) and semantic relatedness between sentences, and Iacobacci et al. (2015) who applied multi-sense embeddings to word and relational similarity. Introduction In recent years, a considerable number of semantic tasks and datasets have been developed, in order to evaluate the semantic quality of computational models. These tasks include general predictions of semantic similarity (e.g., relying on WordSim353 (Finkelstein et al., 2001) or SimLex-999 (Hill et al., 2015)); more specific predictions of semantic relation types (e.g., relying on BLESS (Baroni and Lenci, 2011) or the SemRel database (Scheible and Schulte im Walde, 2014)); predicting the degree of compositionality for complex nouns and verbs; etc. Computational semantic models predominantly make use of the distributional hypothesis in some way or the other, assuming that words with similar distributions have related meanings (Harris, 1954; Firth, 1957). Distributional models thus offer a means to represent meaning vectors of words, and to determine their semantic relatedness (Turney and Pantel, 20"
E17-2086,W06-3812,0,0.009378,"ferent types of context features: (a) all nouns in the sentence (NN), and (b) all words in a symmetrical window of size 10, weighted by the exponential decay function ( W 10E XP ), cf. Iacobacci et al. (2016). (2) Successive learning of single-sense representations and sense disambiguation This class of approaches also relies on skip-grams but learns senses only in a later stage. Pelevina et al. (2016) introduced a non-parametric method that computes a graph relying on cosine-based nearest neighbors, after learning single-sense representations. The graph-clustering algorithm Chinese Whispers (Biemann, 2006) identifies senses in the graph, to induce multi-sense embeddings by applying a composition function to word senses. We refer to this approach as C HIN W HISP. For the actual clustering, we compare nonparametric flat and hierarchical methods. As for HDP, we cluster verb tokens separately, and then mark each verb token with a tag corresponding to a cluster number. The number of clusters containing a specific verb type corresponds to its number of senses. For flat clustering, we use X -M EANS (Pelleg and Moore, 2000), which extends the standard hard k-means clustering approach into a nonparametr"
E17-2086,P15-1010,0,0.0949224,"nal semantic models that break down word type vectors to word sense vectors, have predominantly be applied to Word Sense Disambiguation/Discrimination or (Crosslingual) Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010; Jurgens and Klapaftis, 2013). As to our knowledge, there is little work on DSMs that distinguishes between word senses and addresses various semantic relatedness tasks. Among the few exceptions are Li and Jurafsky (2015) who evaluated multisense embeddings on semantic relation identification (for nouns only) and semantic relatedness between sentences, and Iacobacci et al. (2015) who applied multi-sense embeddings to word and relational similarity. Introduction In recent years, a considerable number of semantic tasks and datasets have been developed, in order to evaluate the semantic quality of computational models. These tasks include general predictions of semantic similarity (e.g., relying on WordSim353 (Finkelstein et al., 2001) or SimLex-999 (Hill et al., 2015)); more specific predictions of semantic relation types (e.g., relying on BLESS (Baroni and Lenci, 2011) or the SemRel database (Scheible and Schulte im Walde, 2014)); predicting the degree of compositional"
E17-2086,C10-1011,0,0.0175997,"; detection of non-literal language usage. The goal of the evaluation is to explore whether the distinction of verb senses in our multi-sense embedding models leads to an improvement of model predictions across semantic tasks. Corpus & Target Verbs As corpus resource for our target verbs as well as for the experimental setup, we use DECOW14AX, a German web corpus containing 12 billion tokens (Sch¨afer and Bildhauer, 2012; Sch¨afer, 2015). The corpus sentences were morphologically annotated and parsed using SMOR (Faaß et al., 2010), MarMoT (M¨uller et al., 2013) and the MATE dependency parser (Bohnet, 2010). Based on the morphological annotation, we extracted the lemmas of all verb types from the corpus with frequencies &gt;100 (regarding base verbs) and &gt;200 (regarding complex verbs), and all their sentence contexts. The total selection of German verb types contains 11 869 lemmas, including 6 998 complex verbs. Similarity Traditionally, distributional word representations are predominantly evaluated on their ability to predict the degree of similarity for word pairs in existing benchmarks. The predicted degrees of similarity are compared against human similarity ratings. For our German targets, we"
E17-2086,P16-1085,0,0.0238932,"represent each verb token by a vector: We look up the individual vector representations of the verb’s context words, and create the verb token vector as the average vector of these context words, ignoring the target verb. This simple kind of phrase/sentence representation has been shown to work well on a variety of tasks (e.g., Milajevs et al. (2014), Hill et al. (2016)). In addition, it allows us to compare different types of context features: (a) all nouns in the sentence (NN), and (b) all words in a symmetrical window of size 10, weighted by the exponential decay function ( W 10E XP ), cf. Iacobacci et al. (2016). (2) Successive learning of single-sense representations and sense disambiguation This class of approaches also relies on skip-grams but learns senses only in a later stage. Pelevina et al. (2016) introduced a non-parametric method that computes a graph relying on cosine-based nearest neighbors, after learning single-sense representations. The graph-clustering algorithm Chinese Whispers (Biemann, 2006) identifies senses in the graph, to induce multi-sense embeddings by applying a composition function to word senses. We refer to this approach as C HIN W HISP. For the actual clustering, we comp"
E17-2086,W16-5318,1,0.836438,"Compositionality Addressing the compositionality of complex words is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. In this evaluation, we predict the degree of compositionality of German complex verbs, i.e., the degree of relatedness between a complex verb and its corresponding base verb (such as abnehmen–nehmen ’take over–take’, and anfangen–fangen ’begin–catch’). The predictions are evaluated against an existing dataset of human ratings on compositionality (Bott et al., 2016), containing a total of 400 German particle verbs Evaluation We evaluate our models on various semantic tasks: general predictions of semantic similarity, and specific tasks regarding complex German verbs, 1 https://radimrehurek.com/gensim/ models/hdpmodel.html 2 https://github.com/rscarberry-wa/ clodhopper 3 https://github.com/perdisci/jbirch 537 c-Means baseline with single-sense embeddings. (using every possible threshold within a range of [0.01, 0.99] to determine the memberships, and reporting the one providing the highest score). As evaluation measure we relied on B-Cubed (Bagga and Bald"
E17-2086,S13-2049,0,0.0292152,"various semantic tasks: semantic classification; predicting compositionality; and detecting non-literal language usage. While there is no overall best model, all models significantly outperform a word2vec single-sense skip baseline, thus demonstrating the need to distinguish between word senses in a distributional semantic model. 1 In contrast, distributional semantic models that break down word type vectors to word sense vectors, have predominantly be applied to Word Sense Disambiguation/Discrimination or (Crosslingual) Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010; Jurgens and Klapaftis, 2013). As to our knowledge, there is little work on DSMs that distinguishes between word senses and addresses various semantic relatedness tasks. Among the few exceptions are Li and Jurafsky (2015) who evaluated multisense embeddings on semantic relation identification (for nouns only) and semantic relatedness between sentences, and Iacobacci et al. (2015) who applied multi-sense embeddings to word and relational similarity. Introduction In recent years, a considerable number of semantic tasks and datasets have been developed, in order to evaluate the semantic quality of computational models. These"
E17-2086,W98-1114,0,0.0267419,"ans(w10Exp) BIRCH(NN) BIRCH(w10Exp) Baseline Prediction .20 .30 .32 .19 .19 .26 .28 .26 .26 Table 2: Results for predicting compositionality. Model NP-MSSGR ChinRestP ChinWhisp HDP x-Means(NN) x-Means(w10Exp) BIRCH(NN) BIRCH(w10Exp) Baseline Semantic Verb Classification Semantic verb classifications are of great interest to NLP, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). We target the semantic classification of German complex verbs by applying hard clustering to multi-sense embeddings, rather than using soft clustering. Focusing on particle verbs across three particles (ab, an, auf ), we aim to obtain cluster analyses that resemble existing manual sense classifications based on formal semantic definitions (Kliche, 2011; Lechler and Roßdeutscher, 2009; Springorum, 2011). All"
E17-2086,D07-1091,0,0.0122407,"Table 2: Results for predicting compositionality. Model NP-MSSGR ChinRestP ChinWhisp HDP x-Means(NN) x-Means(w10Exp) BIRCH(NN) BIRCH(w10Exp) Baseline Semantic Verb Classification Semantic verb classifications are of great interest to NLP, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). We target the semantic classification of German complex verbs by applying hard clustering to multi-sense embeddings, rather than using soft clustering. Focusing on particle verbs across three particles (ab, an, auf ), we aim to obtain cluster analyses that resemble existing manual sense classifications based on formal semantic definitions (Kliche, 2011; Lechler and Roßdeutscher, 2009; Springorum, 2011). All datasets represent fuzzy gold standards. The ab classification contains 205 particle verbs"
E17-2086,C96-1055,0,0.414897,"ferior to the baseline. Model NP-MSSGR ChinRestP ChinWhisp HDP x-Means(NN) x-Means(w10Exp) BIRCH(NN) BIRCH(w10Exp) Baseline Prediction .20 .30 .32 .19 .19 .26 .28 .26 .26 Table 2: Results for predicting compositionality. Model NP-MSSGR ChinRestP ChinWhisp HDP x-Means(NN) x-Means(w10Exp) BIRCH(NN) BIRCH(w10Exp) Baseline Semantic Verb Classification Semantic verb classifications are of great interest to NLP, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). We target the semantic classification of German complex verbs by applying hard clustering to multi-sense embeddings, rather than using soft clustering. Focusing on particle verbs across three particles (ab, an, auf ), we aim to obtain cluster analyses that resemble existing manual sense classifications based on formal semantic de"
E17-2086,P05-1005,0,0.0365547,". Model NP-MSSGR ChinRestP ChinWhisp HDP x-Means(NN) x-Means(w10Exp) BIRCH(NN) BIRCH(w10Exp) Baseline Prediction .20 .30 .32 .19 .19 .26 .28 .26 .26 Table 2: Results for predicting compositionality. Model NP-MSSGR ChinRestP ChinWhisp HDP x-Means(NN) x-Means(w10Exp) BIRCH(NN) BIRCH(w10Exp) Baseline Semantic Verb Classification Semantic verb classifications are of great interest to NLP, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). We target the semantic classification of German complex verbs by applying hard clustering to multi-sense embeddings, rather than using soft clustering. Focusing on particle verbs across three particles (ab, an, auf ), we aim to obtain cluster analyses that resemble existing manual sense classifications based on formal semantic definitions (Kliche, 2011;"
E17-2086,W15-0105,1,0.878084,"Missing"
E17-2086,N16-1039,1,0.682878,"Missing"
E17-2086,N13-1090,0,0.0175032,"ned in a twostage procedure: In a first stage, a corpus is automatically sense-annotated by appending a sense index to every word token (e.g., apple1 , apple2 , etc.). In a second stage, standard techniques are applied to learn single-sense representations for the annotated senses in the corpus. Since the annotations distinguish between senses, the “singlesense” representations effectively represent multisense embeddings. For example, Iacobacci et al. (2015) perform the first step by using an off-theshelf word sense disambiguation tool, and the second step by applying Mikolov’s word2vec tool (Mikolov et al., 2013b; Mikolov et al., 2013a). We investigate several variants regarding the automatic corpus sense annotation. (i) Rather than applying an off-the-shelf WSD tool, we apply the topic-based sense learning method from (Lau et al., 2012), the Hierarchical Dirichlet process (HDP) (Teh et al., 2004). The 536 3 Experiments i.e. semantic classification; prediction of compositionality; detection of non-literal language usage. The goal of the evaluation is to explore whether the distinction of verb senses in our multi-sense embedding models leads to an improvement of model predictions across semantic tasks"
E17-2086,E12-1060,0,0.0248357,"e-sense representations for the annotated senses in the corpus. Since the annotations distinguish between senses, the “singlesense” representations effectively represent multisense embeddings. For example, Iacobacci et al. (2015) perform the first step by using an off-theshelf word sense disambiguation tool, and the second step by applying Mikolov’s word2vec tool (Mikolov et al., 2013b; Mikolov et al., 2013a). We investigate several variants regarding the automatic corpus sense annotation. (i) Rather than applying an off-the-shelf WSD tool, we apply the topic-based sense learning method from (Lau et al., 2012), the Hierarchical Dirichlet process (HDP) (Teh et al., 2004). The 536 3 Experiments i.e. semantic classification; prediction of compositionality; detection of non-literal language usage. The goal of the evaluation is to explore whether the distinction of verb senses in our multi-sense embedding models leads to an improvement of model predictions across semantic tasks. Corpus & Target Verbs As corpus resource for our target verbs as well as for the experimental setup, we use DECOW14AX, a German web corpus containing 12 billion tokens (Sch¨afer and Bildhauer, 2012; Sch¨afer, 2015). The corpus s"
E17-2086,D14-1079,0,0.0186511,"se Restaurant Process (C HIN R EST P), cf. Li and Jurafsky (2015). (ii) As an alternative to the topic model, we apply different clustering algorithms, which not only allows more flexibility in the sense classification technique but also regarding the verb features: we represent each verb token by a vector: We look up the individual vector representations of the verb’s context words, and create the verb token vector as the average vector of these context words, ignoring the target verb. This simple kind of phrase/sentence representation has been shown to work well on a variety of tasks (e.g., Milajevs et al. (2014), Hill et al. (2016)). In addition, it allows us to compare different types of context features: (a) all nouns in the sentence (NN), and (b) all words in a symmetrical window of size 10, weighted by the exponential decay function ( W 10E XP ), cf. Iacobacci et al. (2016). (2) Successive learning of single-sense representations and sense disambiguation This class of approaches also relies on skip-grams but learns senses only in a later stage. Pelevina et al. (2016) introduced a non-parametric method that computes a graph relying on cosine-based nearest neighbors, after learning single-sense rep"
E17-2086,D13-1032,0,0.0433275,"Missing"
E17-2086,D14-1113,0,0.038579,"for the respective sentence and that topic. We implemented and applied several variants of state-of-the-art methods for obtaining multi-sense embeddings. In this paper, we restrict the selection to models that perform unsupervised and nonparametric sense learning, i.e., methods that learn potentially different numbers of senses per word, using only a corpus but no sense inventory. (1) Joint learning of sense representations and application of sense disambiguation From this advanced family of multi-sense embedding induction, we applied the non-parametric multiplesense skip-grams (NP-MSSG), cf. Neelakantan et al. (2014), and skip-grams extended by the Chinese Restaurant Process (C HIN R EST P), cf. Li and Jurafsky (2015). (ii) As an alternative to the topic model, we apply different clustering algorithms, which not only allows more flexibility in the sense classification technique but also regarding the verb features: we represent each verb token by a vector: We look up the individual vector representations of the verb’s context words, and create the verb token vector as the average vector of these context words, ignoring the target verb. This simple kind of phrase/sentence representation has been shown to w"
E17-2086,Q15-1016,0,0.0818526,"Missing"
E17-2086,W16-1620,0,0.0124801,"noring the target verb. This simple kind of phrase/sentence representation has been shown to work well on a variety of tasks (e.g., Milajevs et al. (2014), Hill et al. (2016)). In addition, it allows us to compare different types of context features: (a) all nouns in the sentence (NN), and (b) all words in a symmetrical window of size 10, weighted by the exponential decay function ( W 10E XP ), cf. Iacobacci et al. (2016). (2) Successive learning of single-sense representations and sense disambiguation This class of approaches also relies on skip-grams but learns senses only in a later stage. Pelevina et al. (2016) introduced a non-parametric method that computes a graph relying on cosine-based nearest neighbors, after learning single-sense representations. The graph-clustering algorithm Chinese Whispers (Biemann, 2006) identifies senses in the graph, to induce multi-sense embeddings by applying a composition function to word senses. We refer to this approach as C HIN W HISP. For the actual clustering, we compare nonparametric flat and hierarchical methods. As for HDP, we cluster verb tokens separately, and then mark each verb token with a tag corresponding to a cluster number. The number of clusters co"
E17-2086,D15-1200,0,0.0824444,"rd2vec single-sense skip baseline, thus demonstrating the need to distinguish between word senses in a distributional semantic model. 1 In contrast, distributional semantic models that break down word type vectors to word sense vectors, have predominantly be applied to Word Sense Disambiguation/Discrimination or (Crosslingual) Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010; Jurgens and Klapaftis, 2013). As to our knowledge, there is little work on DSMs that distinguishes between word senses and addresses various semantic relatedness tasks. Among the few exceptions are Li and Jurafsky (2015) who evaluated multisense embeddings on semantic relation identification (for nouns only) and semantic relatedness between sentences, and Iacobacci et al. (2015) who applied multi-sense embeddings to word and relational similarity. Introduction In recent years, a considerable number of semantic tasks and datasets have been developed, in order to evaluate the semantic quality of computational models. These tasks include general predictions of semantic similarity (e.g., relying on WordSim353 (Finkelstein et al., 2001) or SimLex-999 (Hill et al., 2015)); more specific predictions of semantic rela"
E17-2086,D14-1162,0,0.0751907,"Missing"
E17-2086,S07-1009,0,0.107262,"Missing"
E17-2086,J07-4005,0,0.0199702,"tP ChinWhisp HDP x-Means(NN) x-Means(w10Exp) BIRCH(NN) BIRCH(w10Exp) Baseline Prediction .20 .30 .32 .19 .19 .26 .28 .26 .26 Table 2: Results for predicting compositionality. Model NP-MSSGR ChinRestP ChinWhisp HDP x-Means(NN) x-Means(w10Exp) BIRCH(NN) BIRCH(w10Exp) Baseline Semantic Verb Classification Semantic verb classifications are of great interest to NLP, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). We target the semantic classification of German complex verbs by applying hard clustering to multi-sense embeddings, rather than using soft clustering. Focusing on particle verbs across three particles (ab, an, auf ), we aim to obtain cluster analyses that resemble existing manual sense classifications based on formal semantic definitions (Kliche, 2011; Lechler and Roßdeutsche"
E17-2086,C00-2094,0,0.0265279,".19 .19 .26 .28 .26 .26 Table 2: Results for predicting compositionality. Model NP-MSSGR ChinRestP ChinWhisp HDP x-Means(NN) x-Means(w10Exp) BIRCH(NN) BIRCH(w10Exp) Baseline Semantic Verb Classification Semantic verb classifications are of great interest to NLP, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). We target the semantic classification of German complex verbs by applying hard clustering to multi-sense embeddings, rather than using soft clustering. Focusing on particle verbs across three particles (ab, an, auf ), we aim to obtain cluster analyses that resemble existing manual sense classifications based on formal semantic definitions (Kliche, 2011; Lechler and Roßdeutscher, 2009; Springorum, 2011). All datasets represent fuzzy gold standards. The ab classification conta"
E17-2086,S10-1002,0,0.0254411,"the model variants on various semantic tasks: semantic classification; predicting compositionality; and detecting non-literal language usage. While there is no overall best model, all models significantly outperform a word2vec single-sense skip baseline, thus demonstrating the need to distinguish between word senses in a distributional semantic model. 1 In contrast, distributional semantic models that break down word type vectors to word sense vectors, have predominantly be applied to Word Sense Disambiguation/Discrimination or (Crosslingual) Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010; Jurgens and Klapaftis, 2013). As to our knowledge, there is little work on DSMs that distinguishes between word senses and addresses various semantic relatedness tasks. Among the few exceptions are Li and Jurafsky (2015) who evaluated multisense embeddings on semantic relation identification (for nouns only) and semantic relatedness between sentences, and Iacobacci et al. (2015) who applied multi-sense embeddings to word and relational similarity. Introduction In recent years, a considerable number of semantic tasks and datasets have been developed, in order to evaluate the semantic quality"
E17-2086,schafer-bildhauer-2012-building,0,0.0857957,"Missing"
E17-2086,W14-5814,1,0.818677,"Missing"
E17-2086,P03-1002,0,0.0204162,"tP ChinWhisp HDP x-Means(NN) x-Means(w10Exp) BIRCH(NN) BIRCH(w10Exp) Baseline Semantic Verb Classification Semantic verb classifications are of great interest to NLP, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). We target the semantic classification of German complex verbs by applying hard clustering to multi-sense embeddings, rather than using soft clustering. Focusing on particle verbs across three particles (ab, an, auf ), we aim to obtain cluster analyses that resemble existing manual sense classifications based on formal semantic definitions (Kliche, 2011; Lechler and Roßdeutscher, 2009; Springorum, 2011). All datasets represent fuzzy gold standards. The ab classification contains 205 particle verbs in 9 classes; the an classification contains 188 particle verbs in 8 clas"
E17-2086,2014.amta-researchers.21,1,0.703957,"redicting compositionality. Model NP-MSSGR ChinRestP ChinWhisp HDP x-Means(NN) x-Means(w10Exp) BIRCH(NN) BIRCH(w10Exp) Baseline Semantic Verb Classification Semantic verb classifications are of great interest to NLP, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). We target the semantic classification of German complex verbs by applying hard clustering to multi-sense embeddings, rather than using soft clustering. Focusing on particle verbs across three particles (ab, an, auf ), we aim to obtain cluster analyses that resemble existing manual sense classifications based on formal semantic definitions (Kliche, 2011; Lechler and Roßdeutscher, 2009; Springorum, 2011). All datasets represent fuzzy gold standards. The ab classification contains 205 particle verbs in 9 classes; the an c"
E17-2099,2012.eamt-1.42,1,0.863105,"on enriched with features relevant for support verb constructions and verbal inflection. We show that the components targeting the different linguistic levels are complementary, but also that applying only verbal pre-ordering can introduce problems on the morpho-lexical level; our Syntax Different syntactic structures in source and target language are problematic as they are hard to capture by word alignment, and long-distance reorderings are typically also disfavoured in phrasebased SMT. Hierarchical systems can bridge gaps up to a certain length, possibly enhanced by explicit modeling, e.g. Braune et al. (2012). An alternative method, especially for phrasebased systems, is source-side reordering: in a preprocessing step, the source-side data is arranged such that it corresponds to the target-side structure. This improves the alignment and does not require long-distance reordering during decoding, see e.g. Collins et al. (2005) and Gojun and Fraser (2012). Lexicon Problems on the lexical level are diverse and include word sense disambiguation, selectional preferences and the translation of multi-word structures. Many approaches rely on rich source-side features to provide more context for decoding, e"
E17-2099,W15-0903,1,0.843872,"es for source context: Standard Features on the source-side comprise part-of-speech tags and lemmas within the phrase and a context window (5 for tags, 3 for word/lemma). Information across larger gaps is captured by dependency relations such as verbobject pairs or verb-subject pairs, cf. columns 4 and 5 in table 1. On the target-side, lemmas and part-of-speech tags for the current phrase are given. Support Verb Constructions are formed by a verb and a predicative noun, e.g. make a contribution. Typically, the verb does not contribute its full meaning, and thus cannot be translated literally. Cap et al. (2015) improved German-English phrasebased SMT by annotating support verb status on source-side verbs, which essentially divides verbs into two groups: “non-literal use” in a support verb construction, and “literal use” otherwise. The set of support verb constructions consists of highly associated noun+verb tuples. Cap et al. (2015) opted for a hard annotation by adding markup. Instead, we add a classifier feature and compare two variants: (i) setting the feature to a binary support verb status (yes/no) for a fixed set of tuples (using a log-likelihood threshold of 1000, as in Cap et al. (2015)). Th"
E17-2099,loaiciga-etal-2014-english,0,0.0605651,"Missing"
E17-2099,W16-2203,1,0.84877,"ime) and target-side generation of nominal inflection (post-processing). For (ii), we focus on source-side reordering and investigate whether introducing German clause ordering in the English data entails new problems: while in “regular” English verbs and their arguments are close to each other, they can be separated by large distances in the German-structured English. Reordering improves translation quality, but separating the verb from its arguments has also negative consequences. First, the agreement in number between verbs and subjects is impaired because subjects and verbs are separated (Ramm and Fraser, 2016). Second, there can be a negative effect on the lexical level, for example when translating multiword expressions. Consider the phrase to cut interest rates: if the parts occur close to each other, there is enough context to translate cut into senken (‘to decrease’). However, with too large a gap between cut and interest rates, it becomes difficult to disambiguate cut, leading to the wrong translation schneiden (’to cut with a knife’). 2 Morpho-Syntactic Modeling This section outlines the pre- and post-processing steps for morpho-syntactic modeling. during translation. To re-inflect the stemme"
E17-2099,schmid-etal-2004-smor,0,0.0241496,"Missing"
E17-2099,D07-1007,0,0.0606299,"alternative method, especially for phrasebased systems, is source-side reordering: in a preprocessing step, the source-side data is arranged such that it corresponds to the target-side structure. This improves the alignment and does not require long-distance reordering during decoding, see e.g. Collins et al. (2005) and Gojun and Fraser (2012). Lexicon Problems on the lexical level are diverse and include word sense disambiguation, selectional preferences and the translation of multi-word structures. Many approaches rely on rich source-side features to provide more context for decoding, e.g. Carpuat and Wu (2007), Jeong et al. (2010), Tamchyna et al. (2014), Tamchyna et al. (2016). 625 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 625–630, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics that the ground was permanently frozen in the current crisis , the us federal reserve and the european central bank cut interest rates dass der boden ständig gefroren war in der aktuellen krise senken die us-notenbank und die europäische zentralbank die zinssätze that the ground permanently fr"
E17-2099,D13-1174,0,0.0186888,"l features with features geared towards verbal inflection. 1 Morphology Inflection is one of the main problems when translating into a morphologically rich language. It is subject to local restrictions such as agreement in nominal phrases, but also depends on sentence-level interactions, such as verb-subject agreement, or the realization of grammatical case. Target-side morphology can be modeled through computation of inflectional features and generation of inflected forms (Toutanova et al., 2008; Fraser et al., 2012), by means of synthetic phrases to provide the full set of word inflections (Chahuneau et al., 2013), or by introducing agreement restrictions for consistent inflection (Williams and Koehn, 2011). Introduction and Motivation Many of the errors occurring in SMT can be attributed to problems on three linguistic levels: morphological richness, structural differences between source and target language, and lexical choice. Often, these categories are intertwined: for example, the syntactic function of an argument can be expressed on the morphological level by grammatical case (e.g. in German), or on the syntactic level through word ordering (such as SVO in English). This paper addresses problems"
E17-2099,P05-1022,0,0.0418345,"del is based on 4.592.139 parallel sentences; and 45M sentences (News14+parallel data) are used to train a 5-gram language model. We use NewsTest’13 (3000 sentences) and News Test’14 (3003 sentences) for tuning and testing. The linguistic processing for inflection prediction includes parsing (Schmid, 2004) and morphological analysis/generation (Schmid et al., 2004). To predict the features for nominal inflection, CRF sequence models (Lavergne et al., 2010) are trained on the target-side of the parallel data. The reordering rules from Gojun and Fraser (2012) are applied to parsed English data (Charniak and Johnson, 2005). We use a version of Moses with the integrated discriminative classifier VowpalWabbit (Tamchyna et al., 2014)2 . Training examples are extracted from the parallel data based on phrase-table entries. In order to keep the amount of training examples manageable, the phrase-table is reduced with sigtestfiltering with the setting -l a+e -n 30.3 We run 50 training iterations and apply early-stopping on the development set to identify the optimal model. 627 2 3 github.com/moses-smt/mosesdecoder/tree/master/vw All experiments use sigtest-filtered phrase-tables. 19.45 VW-1 pos/lem 19.81* VW-2 pos/lem/"
E17-2099,P05-1066,0,0.0738678,"n source and target language are problematic as they are hard to capture by word alignment, and long-distance reorderings are typically also disfavoured in phrasebased SMT. Hierarchical systems can bridge gaps up to a certain length, possibly enhanced by explicit modeling, e.g. Braune et al. (2012). An alternative method, especially for phrasebased systems, is source-side reordering: in a preprocessing step, the source-side data is arranged such that it corresponds to the target-side structure. This improves the alignment and does not require long-distance reordering during decoding, see e.g. Collins et al. (2005) and Gojun and Fraser (2012). Lexicon Problems on the lexical level are diverse and include word sense disambiguation, selectional preferences and the translation of multi-word structures. Many approaches rely on rich source-side features to provide more context for decoding, e.g. Carpuat and Wu (2007), Jeong et al. (2010), Tamchyna et al. (2014), Tamchyna et al. (2016). 625 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 625–630, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Lin"
E17-2099,E12-1068,1,0.91806,"scriminative classifier can overcome these problems, in particular when enriching standard lexical features with features geared towards verbal inflection. 1 Morphology Inflection is one of the main problems when translating into a morphologically rich language. It is subject to local restrictions such as agreement in nominal phrases, but also depends on sentence-level interactions, such as verb-subject agreement, or the realization of grammatical case. Target-side morphology can be modeled through computation of inflectional features and generation of inflected forms (Toutanova et al., 2008; Fraser et al., 2012), by means of synthetic phrases to provide the full set of word inflections (Chahuneau et al., 2013), or by introducing agreement restrictions for consistent inflection (Williams and Koehn, 2011). Introduction and Motivation Many of the errors occurring in SMT can be attributed to problems on three linguistic levels: morphological richness, structural differences between source and target language, and lexical choice. Often, these categories are intertwined: for example, the syntactic function of an argument can be expressed on the morphological level by grammatical case (e.g. in German), or o"
E17-2099,E12-1074,1,0.934706,"ge are problematic as they are hard to capture by word alignment, and long-distance reorderings are typically also disfavoured in phrasebased SMT. Hierarchical systems can bridge gaps up to a certain length, possibly enhanced by explicit modeling, e.g. Braune et al. (2012). An alternative method, especially for phrasebased systems, is source-side reordering: in a preprocessing step, the source-side data is arranged such that it corresponds to the target-side structure. This improves the alignment and does not require long-distance reordering during decoding, see e.g. Collins et al. (2005) and Gojun and Fraser (2012). Lexicon Problems on the lexical level are diverse and include word sense disambiguation, selectional preferences and the translation of multi-word structures. Many approaches rely on rich source-side features to provide more context for decoding, e.g. Carpuat and Wu (2007), Jeong et al. (2010), Tamchyna et al. (2014), Tamchyna et al. (2016). 625 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 625–630, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics that the ground was"
E17-2099,2010.amta-papers.33,0,0.026038,"pecially for phrasebased systems, is source-side reordering: in a preprocessing step, the source-side data is arranged such that it corresponds to the target-side structure. This improves the alignment and does not require long-distance reordering during decoding, see e.g. Collins et al. (2005) and Gojun and Fraser (2012). Lexicon Problems on the lexical level are diverse and include word sense disambiguation, selectional preferences and the translation of multi-word structures. Many approaches rely on rich source-side features to provide more context for decoding, e.g. Carpuat and Wu (2007), Jeong et al. (2010), Tamchyna et al. (2014), Tamchyna et al. (2016). 625 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 625–630, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics that the ground was permanently frozen in the current crisis , the us federal reserve and the european central bank cut interest rates dass der boden ständig gefroren war in der aktuellen krise senken die us-notenbank und die europäische zentralbank die zinssätze that the ground permanently frozen was in the curre"
E17-2099,P10-1052,0,0.10306,"Missing"
E17-2099,C04-1024,0,0.0114703,"rrect auxiliary (sein: ’to be’ vs. haben: ’to have’) for German present/past perfect. 4 Experiments and Results This section presents the results of combining the strategies for the three linguistic levels. Data and Resources All systems are built using the Moses phrase-based framework. The translation model is based on 4.592.139 parallel sentences; and 45M sentences (News14+parallel data) are used to train a 5-gram language model. We use NewsTest’13 (3000 sentences) and News Test’14 (3003 sentences) for tuning and testing. The linguistic processing for inflection prediction includes parsing (Schmid, 2004) and morphological analysis/generation (Schmid et al., 2004). To predict the features for nominal inflection, CRF sequence models (Lavergne et al., 2010) are trained on the target-side of the parallel data. The reordering rules from Gojun and Fraser (2012) are applied to parsed English data (Charniak and Johnson, 2005). We use a version of Moses with the integrated discriminative classifier VowpalWabbit (Tamchyna et al., 2014)2 . Training examples are extracted from the parallel data based on phrase-table entries. In order to keep the amount of training examples manageable, the phrase-table is"
E17-2099,P16-1161,1,0.854072,"ide reordering: in a preprocessing step, the source-side data is arranged such that it corresponds to the target-side structure. This improves the alignment and does not require long-distance reordering during decoding, see e.g. Collins et al. (2005) and Gojun and Fraser (2012). Lexicon Problems on the lexical level are diverse and include word sense disambiguation, selectional preferences and the translation of multi-word structures. Many approaches rely on rich source-side features to provide more context for decoding, e.g. Carpuat and Wu (2007), Jeong et al. (2010), Tamchyna et al. (2014), Tamchyna et al. (2016). 625 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 625–630, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics that the ground was permanently frozen in the current crisis , the us federal reserve and the european central bank cut interest rates dass der boden ständig gefroren war in der aktuellen krise senken die us-notenbank und die europäische zentralbank die zinssätze that the ground permanently frozen was in the current crisis , cut the us federal reserve and the e"
E17-2099,P08-1059,0,0.0267052,"and lexical level. A discriminative classifier can overcome these problems, in particular when enriching standard lexical features with features geared towards verbal inflection. 1 Morphology Inflection is one of the main problems when translating into a morphologically rich language. It is subject to local restrictions such as agreement in nominal phrases, but also depends on sentence-level interactions, such as verb-subject agreement, or the realization of grammatical case. Target-side morphology can be modeled through computation of inflectional features and generation of inflected forms (Toutanova et al., 2008; Fraser et al., 2012), by means of synthetic phrases to provide the full set of word inflections (Chahuneau et al., 2013), or by introducing agreement restrictions for consistent inflection (Williams and Koehn, 2011). Introduction and Motivation Many of the errors occurring in SMT can be attributed to problems on three linguistic levels: morphological richness, structural differences between source and target language, and lexical choice. Often, these categories are intertwined: for example, the syntactic function of an argument can be expressed on the morphological level by grammatical case"
E17-2099,W11-2126,0,0.0180105,"the main problems when translating into a morphologically rich language. It is subject to local restrictions such as agreement in nominal phrases, but also depends on sentence-level interactions, such as verb-subject agreement, or the realization of grammatical case. Target-side morphology can be modeled through computation of inflectional features and generation of inflected forms (Toutanova et al., 2008; Fraser et al., 2012), by means of synthetic phrases to provide the full set of word inflections (Chahuneau et al., 2013), or by introducing agreement restrictions for consistent inflection (Williams and Koehn, 2011). Introduction and Motivation Many of the errors occurring in SMT can be attributed to problems on three linguistic levels: morphological richness, structural differences between source and target language, and lexical choice. Often, these categories are intertwined: for example, the syntactic function of an argument can be expressed on the morphological level by grammatical case (e.g. in German), or on the syntactic level through word ordering (such as SVO in English). This paper addresses problems across the three linguistic levels by combining established approaches which were previously st"
E17-4012,J90-1003,0,0.332552,"Missing"
E17-4012,N13-2003,0,0.259996,"extent an expression is a term, i.e to which extent it is related to domain-specific concepts (Kagueura and Umino, 1996). Among a large number of measures, association measures like Pointwise Mutual Information (PMI) (Church and Hanks, 1989) are used to determine unithood whereas termdocument measures like tf-idf (Salton and McGill, 1986) are used to determine termhood. Such measures use distinctive characteristics of terms on how they and their components are distributed within a domain or across domains. We address term extraction as a machine learning classification problem (c.f. da Silva Conrado et al., 2013). Most importantly, we focus on the interpretability of a trained classifier to understand the contributions of feature classes to the decision process. For this task, we use random forests to automatically detect the best features. These features are used to build simple decision tree classifiers. For the classification, we use features based on numeric measures which are computed from occurrences of term candidates, its components and derived symbolic information like POS tags. We call these distributional features. The advantage of relying on such features is that they are simple to compute"
E17-4012,P06-2084,0,0.0369991,"valid terms as our gold standard terms. We cleaned the corpus by applying a language detection tool (langdetect2 ) to each sentence, in order to remove sentences which are too noisy. A drawback of the corpus is that about 42,000 sentences could not be connected to a document. Thus, if no document was found for a certain term, its term-document measures were set to a default value outside of a feature’s range, or to an extreme value. Related Work There are several studies investigating linguistic and numeric features, machine learning or a combination of both to extract collocations or terms. Pecina and Schlesinger (2006) combined 82 association measures to extract Czech bigrams and tested various classifiers. The combination of measures was highly superior to using the best single measure. Ramisch et al. (2010) introduced the mwetoolkit which identifies multi-word expressions from different domains. The tool provides a candidate extraction step in advance, descriptive features (e.g. capitalisation, prefixes) and association measures can be used to train a classifier. The latter ones are extended for multi-word expressions of indefinite length and only comprise measures which do not depend on a contingency tab"
E17-4012,ramisch-etal-2010-mwetoolkit,0,0.0519259,"corpus is that about 42,000 sentences could not be connected to a document. Thus, if no document was found for a certain term, its term-document measures were set to a default value outside of a feature’s range, or to an extreme value. Related Work There are several studies investigating linguistic and numeric features, machine learning or a combination of both to extract collocations or terms. Pecina and Schlesinger (2006) combined 82 association measures to extract Czech bigrams and tested various classifiers. The combination of measures was highly superior to using the best single measure. Ramisch et al. (2010) introduced the mwetoolkit which identifies multi-word expressions from different domains. The tool provides a candidate extraction step in advance, descriptive features (e.g. capitalisation, prefixes) and association measures can be used to train a classifier. The latter ones are extended for multi-word expressions of indefinite length and only comprise measures which do not depend on a contingency table. Karan et al. (2012) extract bigram and trigram collocations for Croatian by relying on association measures, frequency counts, POS-tags and semantic similarities of all word pairs in an n-gr"
E17-4012,P06-1099,0,0.0692251,"Missing"
E17-4012,W14-4807,0,0.304544,"mputational Linguistics 3 Our feature classes are motivated and defined in Section 4. In Section 5, we investigate the design of our models with a subsequent presentation of experiments and evaluation results in Section 6. In Section 7, we present a second experiment with term candidates which share a component to further explore their contribution to termhood. 2 Data and Classification Method 3.1 Corpus and Gold Standard The underlying data set for the experiments is the ACL RD-TEC 1.01 , a corpus designed for the evaluation of terminology extraction in the area of Computational Linguistics (Zadeh and Handschuh, 2014). It extends ACL ARC, an automatically segmented and POS-tagged corpus of 10,922 ACL publications from 1965 to 2006. ACL RDTEC adds a manual annotation of 22,044 valid terms and 61,758 non-terms. The term annotations are further refined with a labeling of terminology terms which are defined as means to accomplish a practical task, like methods, systems and algorithms used in Computational Linguistics. We take the valid terms as our gold standard terms. We cleaned the corpus by applying a language detection tool (langdetect2 ) to each sentence, in order to remove sentences which are too noisy."
E17-4012,zhang-etal-2008-comparative,0,0.0436358,"e tool provides a candidate extraction step in advance, descriptive features (e.g. capitalisation, prefixes) and association measures can be used to train a classifier. The latter ones are extended for multi-word expressions of indefinite length and only comprise measures which do not depend on a contingency table. Karan et al. (2012) extract bigram and trigram collocations for Croatian by relying on association measures, frequency counts, POS-tags and semantic similarities of all word pairs in an n-gram. They found that POS-tags, the semantic features and PMI work best. With regard to terms, Zhang et al. (2008) compare different measures (e.g. tf-idf) for both single- and multi-word term extraction and use a voting algorithm to predict the rank of a term. They emphasize the importance of considering unigram terms and the choice of the corpus. Foo and Merkel (2010) use RIPPER (Cohen, 1995), a rule induction learning system to extract unigram and bigram terms, by using both linguistic and numeric features. They show that the design of the ratio of positive and negative examples while training governs the output rules. Da Silva Conrado et al. (2013) investigate features for the classification of Brazil"
E17-4012,W15-3603,0,\N,Missing
E17-4012,karan-etal-2012-evaluation,0,\N,Missing
H05-1077,W04-3205,0,0.0503985,"discrimination (Sch¨utze, 1998), text indexing (Deerwester et al., 1990), and summarisation (Barzilay et al., 2002). Different applications incorporate different semantic verb relations, varying with respect to their demands. To date, limited effort has been spent on specifying the range of verb-verb relations. Morris and Hirst (2004) perform a study on lexical semantic relations which ensure text cohesion. Their relations are not specific to verb-verb pairs, but include e.g. descriptive noun-adjective pairs (such as professors/brilliant), or stereotypical relations (such as homeless/drunk). Chklovski and Pantel (2004) address the automatic acquisition of verb-verb pairs and their relations from the web. They define syntagmatic patterns to cover strength, enablement and temporal relations in addition to synonymy and antonymy, but they do not perform an exhaustive study. We suggest that an analysis of human verb-verb associations may identify the range of semantic relations which are crucial in NLP applications. We present a preparatory study where the lexical semantic taxonymy GermaNet (Kunze, 2000; Kunze, 2004) is checked on the types of classical semantic verb relations1 in our data; verb-verb pairs not c"
H05-1077,kunze-2000-extension,0,0.324697,"ive pairs (such as professors/brilliant), or stereotypical relations (such as homeless/drunk). Chklovski and Pantel (2004) address the automatic acquisition of verb-verb pairs and their relations from the web. They define syntagmatic patterns to cover strength, enablement and temporal relations in addition to synonymy and antonymy, but they do not perform an exhaustive study. We suggest that an analysis of human verb-verb associations may identify the range of semantic relations which are crucial in NLP applications. We present a preparatory study where the lexical semantic taxonymy GermaNet (Kunze, 2000; Kunze, 2004) is checked on the types of classical semantic verb relations1 in our data; verb-verb pairs not covered by GermaNet can help to detect missing links in the taxonomy, and provide an empirical basis for defining non-classical relations. 1 We follow Morris and Hirst (2004) and refer to the paradigmatic WordNet relations as the ”classical” relations. 612 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 612–619, Vancouver, October 2005. 2005 Association for Computational Linguistics Second, in d"
H05-1077,P98-2127,0,0.0173321,"re as those concepts spontaneously called to mind by a stimulus word. In the current investigation, we assume that these evoked concepts reflect highly salient linguistic and conceptual features of the stimulus word. Given this assumption, identifying the types of information provided by speakers and distinguishing and quantifying the relationships between stimulus and response can serve a number of purposes for NLP applications. First, the notion of semantic verb relations is crucial for many NLP tasks and applications such as verb clustering (Pereira et al., 1993; Merlo and Stevenson, 2001; Lin, 1998; Schulte im Walde, 2003), thesaurus extraction (Lin, 1999; McCarthy et al., 2003), word sense discrimination (Sch¨utze, 1998), text indexing (Deerwester et al., 1990), and summarisation (Barzilay et al., 2002). Different applications incorporate different semantic verb relations, varying with respect to their demands. To date, limited effort has been spent on specifying the range of verb-verb relations. Morris and Hirst (2004) perform a study on lexical semantic relations which ensure text cohesion. Their relations are not specific to verb-verb pairs, but include e.g. descriptive noun-adjecti"
H05-1077,P99-1041,0,0.0600318,"mulus word. In the current investigation, we assume that these evoked concepts reflect highly salient linguistic and conceptual features of the stimulus word. Given this assumption, identifying the types of information provided by speakers and distinguishing and quantifying the relationships between stimulus and response can serve a number of purposes for NLP applications. First, the notion of semantic verb relations is crucial for many NLP tasks and applications such as verb clustering (Pereira et al., 1993; Merlo and Stevenson, 2001; Lin, 1998; Schulte im Walde, 2003), thesaurus extraction (Lin, 1999; McCarthy et al., 2003), word sense discrimination (Sch¨utze, 1998), text indexing (Deerwester et al., 1990), and summarisation (Barzilay et al., 2002). Different applications incorporate different semantic verb relations, varying with respect to their demands. To date, limited effort has been spent on specifying the range of verb-verb relations. Morris and Hirst (2004) perform a study on lexical semantic relations which ensure text cohesion. Their relations are not specific to verb-verb pairs, but include e.g. descriptive noun-adjective pairs (such as professors/brilliant), or stereotypical"
H05-1077,W03-1810,0,0.14713,"In the current investigation, we assume that these evoked concepts reflect highly salient linguistic and conceptual features of the stimulus word. Given this assumption, identifying the types of information provided by speakers and distinguishing and quantifying the relationships between stimulus and response can serve a number of purposes for NLP applications. First, the notion of semantic verb relations is crucial for many NLP tasks and applications such as verb clustering (Pereira et al., 1993; Merlo and Stevenson, 2001; Lin, 1998; Schulte im Walde, 2003), thesaurus extraction (Lin, 1999; McCarthy et al., 2003), word sense discrimination (Sch¨utze, 1998), text indexing (Deerwester et al., 1990), and summarisation (Barzilay et al., 2002). Different applications incorporate different semantic verb relations, varying with respect to their demands. To date, limited effort has been spent on specifying the range of verb-verb relations. Morris and Hirst (2004) perform a study on lexical semantic relations which ensure text cohesion. Their relations are not specific to verb-verb pairs, but include e.g. descriptive noun-adjective pairs (such as professors/brilliant), or stereotypical relations (such as homel"
H05-1077,J01-3003,0,0.0677501,"fine semantic associates here as those concepts spontaneously called to mind by a stimulus word. In the current investigation, we assume that these evoked concepts reflect highly salient linguistic and conceptual features of the stimulus word. Given this assumption, identifying the types of information provided by speakers and distinguishing and quantifying the relationships between stimulus and response can serve a number of purposes for NLP applications. First, the notion of semantic verb relations is crucial for many NLP tasks and applications such as verb clustering (Pereira et al., 1993; Merlo and Stevenson, 2001; Lin, 1998; Schulte im Walde, 2003), thesaurus extraction (Lin, 1999; McCarthy et al., 2003), word sense discrimination (Sch¨utze, 1998), text indexing (Deerwester et al., 1990), and summarisation (Barzilay et al., 2002). Different applications incorporate different semantic verb relations, varying with respect to their demands. To date, limited effort has been spent on specifying the range of verb-verb relations. Morris and Hirst (2004) perform a study on lexical semantic relations which ensure text cohesion. Their relations are not specific to verb-verb pairs, but include e.g. descriptive n"
H05-1077,W04-2607,0,0.0464197,"LP applications. First, the notion of semantic verb relations is crucial for many NLP tasks and applications such as verb clustering (Pereira et al., 1993; Merlo and Stevenson, 2001; Lin, 1998; Schulte im Walde, 2003), thesaurus extraction (Lin, 1999; McCarthy et al., 2003), word sense discrimination (Sch¨utze, 1998), text indexing (Deerwester et al., 1990), and summarisation (Barzilay et al., 2002). Different applications incorporate different semantic verb relations, varying with respect to their demands. To date, limited effort has been spent on specifying the range of verb-verb relations. Morris and Hirst (2004) perform a study on lexical semantic relations which ensure text cohesion. Their relations are not specific to verb-verb pairs, but include e.g. descriptive noun-adjective pairs (such as professors/brilliant), or stereotypical relations (such as homeless/drunk). Chklovski and Pantel (2004) address the automatic acquisition of verb-verb pairs and their relations from the web. They define syntagmatic patterns to cover strength, enablement and temporal relations in addition to synonymy and antonymy, but they do not perform an exhaustive study. We suggest that an analysis of human verb-verb associ"
H05-1077,P93-1024,0,0.747874,"web experiment. We define semantic associates here as those concepts spontaneously called to mind by a stimulus word. In the current investigation, we assume that these evoked concepts reflect highly salient linguistic and conceptual features of the stimulus word. Given this assumption, identifying the types of information provided by speakers and distinguishing and quantifying the relationships between stimulus and response can serve a number of purposes for NLP applications. First, the notion of semantic verb relations is crucial for many NLP tasks and applications such as verb clustering (Pereira et al., 1993; Merlo and Stevenson, 2001; Lin, 1998; Schulte im Walde, 2003), thesaurus extraction (Lin, 1999; McCarthy et al., 2003), word sense discrimination (Sch¨utze, 1998), text indexing (Deerwester et al., 1990), and summarisation (Barzilay et al., 2002). Different applications incorporate different semantic verb relations, varying with respect to their demands. To date, limited effort has been spent on specifying the range of verb-verb relations. Morris and Hirst (2004) perform a study on lexical semantic relations which ensure text cohesion. Their relations are not specific to verb-verb pairs, but"
H05-1077,J06-2001,1,\N,Missing
H05-1077,J98-1004,0,\N,Missing
H05-1077,C98-2122,0,\N,Missing
I13-1056,C10-1011,0,0.0554005,"Missing"
I13-1056,J12-3005,1,0.887982,"Missing"
I13-1056,P98-2127,0,0.522644,"Missing"
I13-1056,J06-1003,0,0.189223,"Missing"
I13-1056,J07-4005,0,0.0454138,"Missing"
I13-1056,E06-1016,0,0.0300391,"Missing"
I13-1056,P08-1118,0,0.0334833,"Missing"
I13-1056,P97-1067,0,0.188296,"Missing"
I13-1056,N10-1013,0,0.0685923,"Missing"
I13-1056,P10-2017,0,0.061044,"Missing"
I13-1056,W09-1109,0,0.041163,"Missing"
I13-1056,C08-1114,0,0.0902855,"Missing"
I13-1056,D12-1111,0,0.190464,"Missing"
I13-1056,P90-1034,0,0.788504,"Missing"
I13-1056,J91-1001,0,\N,Missing
I13-1056,J13-3004,0,\N,Missing
I13-1056,faass-etal-2010-design,0,\N,Missing
I13-1056,C98-2122,0,\N,Missing
I13-1072,C10-1011,0,0.036672,"aCky group. The corpus cleaning had focused mainly on removing duplicates from the deWaC, and on disregarding sentences that were syntactically illformed (relying on a parsability index provided by a standard dependency parser (Schiehlen, 2003)). The sdeWaC contains approx. 880 million words. In this paper, we focus on one specific feature set that is expected to provide salient properties towards preposition meaning, i.e., the nouns that are subcategorised by the prepositions. This dependency information was extracted from a parsed version of the sdeWaC using Bohnet’s MATE dependency parser (Bohnet, 2010). So each preposition was associated with a feature vector over its Cluster Analyses The pipeline in our framework is as follows. 1. The prepositions are associated with a distributional feature set. 2. The vector space of prepositions is hardclustered using Self-Organising Maps. 633 5, and the mean distance prep2cluster(c) was 10, then p would not be assigned to c for t = 0.05, 0.1 . . . , 0.5 but for t = 0.6, . . . , 0.95. In this way, we created 19 different soft cluster analyses St (C) for each hard clustering C, one for each t. With low values of t, few prepositions (i.e., only those that"
I13-1072,S12-1023,1,0.910775,"Missing"
I13-1072,J12-3005,1,0.907764,"Missing"
I13-1072,P10-2017,0,0.112314,"Missing"
I13-1072,W09-1109,0,0.0550215,"Missing"
I13-1072,D07-1043,0,0.867057,"aim to investigate prototypical spatial properties of polysemous objects. More specifically, this paper is part of a larger framework that systematically explores the vector spatial properties of German prepositions, a notoriously polysemous closed word class. Relying on Self-Organising Maps (SOMs, cf. Kohonen (2001)) and preposition-dependent nouns as vector-space features, we present a methodology to identify the degree of polysemy of the prepositions. For this task, the methodology applies two cluster evaluation metrics, the Silhouette Value (Kaufman and Rousseeuw, 1990) and the V-Measure (Rosenberg and Hirschberg, 2007), to hard vs. soft cluster analyses based on the SelfOrganising Maps. Since we start out with a hard clustering, a sub-task is concerned with transferring the SOM hard clusters to soft clusters. Similarly, the original V-Measure applies to hard clusters only, so a second sub-task is concerned with defining a Fuzzy V-Measure that applies to soft clusters. Our main hypothesis is that polysemous prepositions are outliers, and thus represent either (i) singletons or (ii) marginals of the clusters within a cluster analysis. The paper is organised as follows. After introducing our preposition data i"
I13-1072,saint-dizier-2006-prepnet,0,0.135279,"emantics has gone beyond a specific choice of prepositions (such as spatial prepositions), towards a systematic classification of preposition senses. In recent years, computational research on prepositions has been enforced, mainly driven by the ACL Special Interest Group on Semantics (ACL-SIGSEM). The SIG has organised a series of workshops on prepositions, and a special issue in the Computational Linguistics journal (Baldwin et al., 2009). Related work across languages includes The Preposition Project for English prepositions (Litkowski and Hargraves, 2005), PrepNet for French prepositions (Saint-Dizier, 2006), and a German project on the role of preposition senses in determiner omission in prepositional phrases (Kiss et al., 2010). The latter is most closely related to the present work, as it is also aimed at German. Their focus however is on manual classifications and corpus annotation, in contrast to our automatic classification approach. As in many other languages, German prepositions are notoriously ambiguous, e.g. note the quite distinct senses of the German preposition nach in nach drei Stunden/Berlin/Meinung ’after three hours/to Berlin/according to’, referring to a temporal, directional, a"
I13-1072,E03-1087,0,0.0279402,"etup of experiments focuses rather on the methodology towards polysemy detection, and is thus restricted to one algorithm (SOMs) and one feature set (nouns). 3.1 Preposition Corpus Features The distributional features for the German prepositions were induced from the sdeWaC corpus (Faaß and Eckart, 2013), a cleaned version of the German web corpus deWaC created by the WaCky group. The corpus cleaning had focused mainly on removing duplicates from the deWaC, and on disregarding sentences that were syntactically illformed (relying on a parsability index provided by a standard dependency parser (Schiehlen, 2003)). The sdeWaC contains approx. 880 million words. In this paper, we focus on one specific feature set that is expected to provide salient properties towards preposition meaning, i.e., the nouns that are subcategorised by the prepositions. This dependency information was extracted from a parsed version of the sdeWaC using Bohnet’s MATE dependency parser (Bohnet, 2010). So each preposition was associated with a feature vector over its Cluster Analyses The pipeline in our framework is as follows. 1. The prepositions are associated with a distributional feature set. 2. The vector space of preposit"
I13-1072,W02-1107,0,0.0441843,"uns from the corpus which co-occurred with the largest number of prepositions. 3.2 Hard Clustering For hard-clustering the German prepositions, we relied on the Self-Organising Maps (SOMs) artificial neural networks provided by the kohonen library of the R Project for Statistical Computing1 . We expected SOMs to be especially useful for this task, as they create typology-preserving maps, and should thus provide a suitable model to look into the spatial properties of polysemous vectors. Furthermore, SOMs have successfully been applied to semantic classification before (Ontrup and Ritter, 2001; Kanzaki et al., 2002; Guida, 2007). We created SOM maps with k clusters, for 2 ≤ k ≤ 47, where 47 represents the total number of prepositions. For each k, we initiated twodimensional spacings for all possible hexagonal grids. For example, we trained four SOM maps with 30 clusters, using a 30×1 grid, a 15×2 grid, a 10×3 grid, and a 6×5 grid. The distance measure used in the maps was Euclidean Distance, which is the only option for SOMs in R. 3.3 (2) Preposition-based softening: For each preposition p within a hard cluster analysis C, we calculated the mean distance cluster2prep(p) over all cluster centroids zc to"
I13-1072,J98-1004,0,0.651112,"Missing"
I13-1072,W03-0410,0,0.216814,"ion measures to decide about the quality of a cluster analysis. On the other hand, our methodology relies on evaluation metrics to identify polysemous prepositions, so the measures are crucial to perform this work. There is a large body of research regarding the question of how to compare and evaluate two cluster analyses. For example, with respect to the specific task of semantic classification, Schulte im Walde (2003), compared a range of evaluation measures. Related work in this area partly adopted the suggested measures, and in addition relied on Purity or Accuracy (Korhonen et al., 2003; Stevenson and Joanis, 2003). In more general terms, there is an ongoing discussion about cluster comparison, mainly in the field of Machine Learning, but also elsewhere. Recent examples include Meila (2007), Rosenberg and Hirschberg (2007), and Vinh and Bailey (2010). These approaches all concentrate on evaluations relying on the entropy between two cluster analyses, in order to compare them. Entropy is an information-theoretic measure of uncertainty; in our context, entropy measures how uncertain a clustering is, given the information provided by a gold standard, and vice versa. a(oi ) = 1 |cA |− 1 o b(oi ) = mincB 6=c"
I13-1072,C10-2064,0,0.0579541,"Missing"
I13-1072,P03-1009,0,0.204702,"We thus needed evaluation measures to decide about the quality of a cluster analysis. On the other hand, our methodology relies on evaluation metrics to identify polysemous prepositions, so the measures are crucial to perform this work. There is a large body of research regarding the question of how to compare and evaluate two cluster analyses. For example, with respect to the specific task of semantic classification, Schulte im Walde (2003), compared a range of evaluation measures. Related work in this area partly adopted the suggested measures, and in addition relied on Purity or Accuracy (Korhonen et al., 2003; Stevenson and Joanis, 2003). In more general terms, there is an ongoing discussion about cluster comparison, mainly in the field of Machine Learning, but also elsewhere. Recent examples include Meila (2007), Rosenberg and Hirschberg (2007), and Vinh and Bailey (2010). These approaches all concentrate on evaluations relying on the entropy between two cluster analyses, in order to compare them. Entropy is an information-theoretic measure of uncertainty; in our context, entropy measures how uncertain a clustering is, given the information provided by a gold standard, and vice versa. a(oi ) = 1"
I13-1072,J06-2001,1,\N,Missing
ivanova-etal-2008-evaluating,schulte-im-walde-2002-subcategorisation,1,\N,Missing
ivanova-etal-2008-evaluating,heid-weller-2008-tools,1,\N,Missing
ivanova-etal-2008-evaluating,C00-2105,1,\N,Missing
ivanova-etal-2008-evaluating,E99-1016,0,\N,Missing
ivanova-etal-2008-evaluating,E06-2001,1,\N,Missing
ivanova-etal-2008-evaluating,evert-2004-statistical,0,\N,Missing
J06-2001,P98-1013,0,0.0529087,"Missing"
J06-2001,W98-1505,0,0.0608763,"Missing"
J06-2001,C96-1055,0,0.0632771,". 2000). On the one hand, verb classes reduce redundancy in verb descriptions since they encode the common properties of verbs. On the other hand, verb classes can predict and refine properties of a verb that received insufficient empirical evidence, with reference to verbs in the same class: Under this criterion, a verb classification is especially useful for the pervasive problem of data sparseness in NLP, where little or no knowledge is provided for rare events. For example, the English verb classification by Levin (1993) has been used in NLP applications such as word sense disambiguation (Dorr and Jones 1996), machine translation (Dorr 1997), document classification (Klavans and Kan 1998), and subcategorization acquisition (Korhonen 2002). To my knowledge, no comparable German verb classification is available so far; therefore, such a classification would provide a principled basis for filling a gap in available lexical knowledge. ¨ ∗ Department of Computational Linguistics, Saarbrucken, Germany. E-mail: schulte@coli.uni-sb.de. Submission received: 1 September 2003; revised submission received: 5 September 2005; accepted for publication: 10 November 2005. © 2006 Association for Computational Lingu"
J06-2001,W97-0802,0,0.110177,"tions in the form of lexical heads, with reference to a specific verb–frame–slot combination. Obviously, we would run into a sparse data problem if we tried to incorporate selectional preferences into the verb descriptions at such a specific level. We are provided with detailed information at the nominal level, but we need a generalization of the selectional preference definition. A widely used resource for selectional preference information is the semantic ¨ ontology WordNet (Miller et al. 1990; Fellbaum 1998); the University of Tubingen has developed the German version of WordNet, GermaNet (Hamp and Feldweg 1997; Kunze 2000). The hierarchy is realized by means of synsets, sets of synonymous nouns, which are organized by multiple inheritance hyponym/hypernym relationships. A noun can appear in several synsets, according to its number of senses. The German noun hierarchy in GermaNet is utilized for the generalization of selectional preferences: For each noun in a verb–frame–slot combination, the joint frequency is divided over the different senses of the noun and propagated up the hierarchy. In case of multiple hypernym 167 Computational Linguistics Volume 32, Number 2 synsets, the frequency is divided"
J06-2001,P93-1023,0,0.452308,"ture vectors. The cosine measure can be applied to frequency and probability values. For a detailed description of hierarchical clustering techniques and an intuitive interpretation of the similarity measures, the reader is referred to, for example, Kaufman and Rousseeuw (1990). There is no agreed standard method for evaluating clustering experiments and results, but a variety of evaluation measures from diverse areas such as theoretical statistics, machine vision, and Web-page clustering are generally applicable. We used the following two measures for the evaluation: (1) Hatzivassiloglou and McKeown (1993) define and evaluate a cluster analysis of adjectives, based on common cluster membership of object pairs in the clustering C and the manual classification M. Recall and precision numbers are calculated in the standard way, with true positives the number of common pairs in M and C, false positives the number of pairs in C, but not M, and false negatives the number of pairs in M, but not C. We use the f -score pairF (as harmonic mean between recall and precision), which provides an easy to understand 170 Schulte im Walde Induction of German Semantic Verb Classes Table 2 Data similarity measures"
J06-2001,P98-1112,0,0.0315263,"nce they encode the common properties of verbs. On the other hand, verb classes can predict and refine properties of a verb that received insufficient empirical evidence, with reference to verbs in the same class: Under this criterion, a verb classification is especially useful for the pervasive problem of data sparseness in NLP, where little or no knowledge is provided for rare events. For example, the English verb classification by Levin (1993) has been used in NLP applications such as word sense disambiguation (Dorr and Jones 1996), machine translation (Dorr 1997), document classification (Klavans and Kan 1998), and subcategorization acquisition (Korhonen 2002). To my knowledge, no comparable German verb classification is available so far; therefore, such a classification would provide a principled basis for filling a gap in available lexical knowledge. ¨ ∗ Department of Computational Linguistics, Saarbrucken, Germany. E-mail: schulte@coli.uni-sb.de. Submission received: 1 September 2003; revised submission received: 5 September 2005; accepted for publication: 10 November 2005. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 2 How can we obtain a semantic"
J06-2001,P03-1009,0,0.792882,"Missing"
J06-2001,kunze-2000-extension,0,0.138816,"xical heads, with reference to a specific verb–frame–slot combination. Obviously, we would run into a sparse data problem if we tried to incorporate selectional preferences into the verb descriptions at such a specific level. We are provided with detailed information at the nominal level, but we need a generalization of the selectional preference definition. A widely used resource for selectional preference information is the semantic ¨ ontology WordNet (Miller et al. 1990; Fellbaum 1998); the University of Tubingen has developed the German version of WordNet, GermaNet (Hamp and Feldweg 1997; Kunze 2000). The hierarchy is realized by means of synsets, sets of synonymous nouns, which are organized by multiple inheritance hyponym/hypernym relationships. A noun can appear in several synsets, according to its number of senses. The German noun hierarchy in GermaNet is utilized for the generalization of selectional preferences: For each noun in a verb–frame–slot combination, the joint frequency is divided over the different senses of the noun and propagated up the hierarchy. In case of multiple hypernym 167 Computational Linguistics Volume 32, Number 2 synsets, the frequency is divided again. The s"
J06-2001,P99-1051,0,0.0798985,"elationship is not perfect, we can make this prediction: If we induce a verb classification on the basis of verb features describing verb behavior, then the resulting behavior classification should agree with a semantic classification to a certain extent (yet to be determined). The aim of this work is to utilize this prediction for the automatic acquisition of German semantic verb classes. The verb behavior itself is commonly captured by the diathesis alternation of verbs: alternative constructions at the syntax–semantics interface that express the same or a similar conceptual idea of a verb (Lapata 1999; Schulte im Walde 2000; McCarthy 2001; Merlo and Stevenson 2001; Joanis 2002). Consider example (1), where the most common alternations of the Manner of Motion with a Vehicle verb fahren ‘drive’ are illustrated. The conceptual participants are a vehicle, a driver, a passenger, and a direction. In (a), the vehicle is expressed as the subject in a transitive verb construction, with a prepositional phrase indicating the direction. In (b), the driver is expressed as the subject in a transitive verb construction, with a prepositional phrase indicating the direction. In (c), the driver is expressed"
J06-2001,J04-1003,0,0.0242343,"off-the-shelf such as FrameNet (Baker, Fillmore, and Lowe 1998; Fontenelle 2003) and PropBank (Palmer, Gildea, and Kingsbury 2005). Instead, the automatic construction of semantic classes typically benefits from a longstanding linguistic hypothesis that asserts a tight connection between the lexical meaning of a verb and its behavior: To a certain extent, the lexical meaning of a verb determines its behavior, particularly with respect to the choice of its arguments (Pinker 1989; Levin 1993; Dorr and Jones 1996; Siegel and McKeown 2000; Merlo and Stevenson 2001; Schulte im Walde and Brew 2002; Lapata and Brew 2004). Even though the meaning–behavior relationship is not perfect, we can make this prediction: If we induce a verb classification on the basis of verb features describing verb behavior, then the resulting behavior classification should agree with a semantic classification to a certain extent (yet to be determined). The aim of this work is to utilize this prediction for the automatic acquisition of German semantic verb classes. The verb behavior itself is commonly captured by the diathesis alternation of verbs: alternative constructions at the syntax–semantics interface that express the same or a"
J06-2001,J98-2002,0,0.0894239,"pproaches, a combination that has not yet been applied. (4) Variations in the existing feature description are especially relevant for the choice of selectional preferences. The experiment results demonstrated that the 15 conceptual GermaNet top levels are not sufficient for all verbs. For example, the verbs t¨oten and unterrichten require a finer version of selectional preferences in order to be distinguished. It is worthwhile either to find a more appropriate level of selectional preferences in WordNet or to apply a more sophisticated approach towards selectional preferences such as that of Li and Abe (1998), in order to determine a more flexible choice of selectional preferences. (5) With respect to a large-scale classification of verbs, it will be interesting to apply classification techniques to the verb data. This would require more data manually labeled with classes in order to train a classifier. But the resulting classifier might abstract better than k-means over the different requirements of the verb classes with respect to the feature description. (6) As an extension of the existing clustering, a soft clustering algorithm will be applied to the German verbs. Soft clustering enables us to"
J06-2001,J01-3003,0,0.888736,"e semantically annotated and provide semantic information off-the-shelf such as FrameNet (Baker, Fillmore, and Lowe 1998; Fontenelle 2003) and PropBank (Palmer, Gildea, and Kingsbury 2005). Instead, the automatic construction of semantic classes typically benefits from a longstanding linguistic hypothesis that asserts a tight connection between the lexical meaning of a verb and its behavior: To a certain extent, the lexical meaning of a verb determines its behavior, particularly with respect to the choice of its arguments (Pinker 1989; Levin 1993; Dorr and Jones 1996; Siegel and McKeown 2000; Merlo and Stevenson 2001; Schulte im Walde and Brew 2002; Lapata and Brew 2004). Even though the meaning–behavior relationship is not perfect, we can make this prediction: If we induce a verb classification on the basis of verb features describing verb behavior, then the resulting behavior classification should agree with a semantic classification to a certain extent (yet to be determined). The aim of this work is to utilize this prediction for the automatic acquisition of German semantic verb classes. The verb behavior itself is commonly captured by the diathesis alternation of verbs: alternative constructions at th"
J06-2001,P02-1027,0,0.378651,"Missing"
J06-2001,J05-1004,0,0.209833,"Missing"
J06-2001,P93-1024,0,0.888699,"Missing"
J06-2001,P99-1014,0,0.743364,"Missing"
J06-2001,C00-2108,1,0.897373,"Missing"
J06-2001,schulte-im-walde-2002-subcategorisation,1,0.90362,"Missing"
J06-2001,E03-1037,1,0.877196,"Missing"
J06-2001,P02-1029,1,0.832484,"Missing"
J06-2001,H05-1077,1,0.850194,"Missing"
J06-2001,J00-4004,0,0.170731,"classes? Few resources are semantically annotated and provide semantic information off-the-shelf such as FrameNet (Baker, Fillmore, and Lowe 1998; Fontenelle 2003) and PropBank (Palmer, Gildea, and Kingsbury 2005). Instead, the automatic construction of semantic classes typically benefits from a longstanding linguistic hypothesis that asserts a tight connection between the lexical meaning of a verb and its behavior: To a certain extent, the lexical meaning of a verb determines its behavior, particularly with respect to the choice of its arguments (Pinker 1989; Levin 1993; Dorr and Jones 1996; Siegel and McKeown 2000; Merlo and Stevenson 2001; Schulte im Walde and Brew 2002; Lapata and Brew 2004). Even though the meaning–behavior relationship is not perfect, we can make this prediction: If we induce a verb classification on the basis of verb features describing verb behavior, then the resulting behavior classification should agree with a semantic classification to a certain extent (yet to be determined). The aim of this work is to utilize this prediction for the automatic acquisition of German semantic verb classes. The verb behavior itself is commonly captured by the diathesis alternation of verbs: alter"
J06-2001,W03-0410,0,0.30358,"Missing"
J06-2001,C02-1146,0,0.0366515,"Missing"
J06-2001,A00-2018,0,\N,Missing
J06-2001,E95-1016,0,\N,Missing
J06-2001,P97-1003,0,\N,Missing
J06-2001,J02-2003,0,\N,Missing
J06-2001,C94-1042,0,\N,Missing
J06-2001,C96-2204,0,\N,Missing
J06-2001,E03-1034,0,\N,Missing
J06-2001,W02-1108,0,\N,Missing
J06-2001,C00-2094,0,\N,Missing
J06-2001,A97-1052,0,\N,Missing
J06-2001,P99-1059,0,\N,Missing
J06-2001,J93-2002,0,\N,Missing
J06-2001,W02-1016,1,\N,Missing
J06-2001,C98-1108,0,\N,Missing
J06-2001,C98-1013,0,\N,Missing
J06-2001,P93-1032,0,\N,Missing
J06-2001,P90-1034,0,\N,Missing
J06-2001,P99-1035,0,\N,Missing
J06-2001,P95-1037,0,\N,Missing
J06-2001,P87-1027,0,\N,Missing
J06-2001,P03-1068,0,\N,Missing
J06-2001,W99-0632,0,\N,Missing
J06-2001,W99-0504,0,\N,Missing
J12-3005,W04-3221,0,0.0126322,"ent semantic classes in this article. The semantic properties of adjectives can also be exploited in advanced NLP tasks and applications such as Question Answering, Dialog Systems, Natural Language Generation, or Information Extraction. For instance, from a sentence like This maimai is round and sweet, we can quite safely infer that the (invented) object maimai is a physical object, probably edible. This type of process could be exploited in, for instance, Information Extraction and ontology population, although to our knowledge this possibility has received but little attention (Malouf 2000; Almuhareb and Poesio 2004). As for polysemy, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem, by biasing the experimental material to include monosemous words only, or by choosing an approach that ignores polysemy (Hindle 1990; Merlo and Stevenson 2001; Schulte im Walde 2006; Joanis, Stevenson, and James 2008). There are a few exceptions to this tradition, such as Pereira, Tishby, and Lee (1993), Rooth et al. (1999), and Korhonen, Krymolowski, and Marx (2003), who used soft clustering methods for multiple assignment to verb semantic classes (see Section 4.5). The"
J12-3005,A00-2006,0,0.574496,"0), has thus focused on scalar adjectives, that is, adjectives like good and bad, which can be translated into values that can be ordered along a scale. These adjectives typically enter into antonymy relations (the semantic relation between good and bad), and in fact antonymy is the main organizing criterion for adjectives in WordNet (Miller 1998), the most widely used semantic resource in NLP. However, when examining a large scale lexicon, it becomes immediately apparent that there are many other types of adjectives that do not easily ﬁt in a scale-based or antonymy-based view of adjectives (Alonge et al. 2000). Some examples are pulmonary, former, and foldable. It is not clear, for instance, whether it makes sense to ask for an antonym of pulmonary, or to establish a “foldability” scale for foldable. These adjectives need a different treatment, and they are treated in terms of different semantic classes in this article. The semantic properties of adjectives can also be exploited in advanced NLP tasks and applications such as Question Answering, Dialog Systems, Natural Language Generation, or Information Extraction. For instance, from a sentence like This maimai is round and sweet, we can quite safe"
J12-3005,alsina-etal-2002-catcg,1,0.550013,"Missing"
J12-3005,W06-2911,0,0.0639609,"Missing"
J12-3005,J08-4004,0,0.09134,"Missing"
J12-3005,C04-1161,1,0.862482,"Missing"
J12-3005,brants-2000-inter,0,0.011141,"ticles on speciﬁcally this topic: Carvalho and Ranchhod (2003) used adjective classes similar to the ones explored here to disambiguate between nominal and adjectival readings in Portuguese. Adjective information, manually coded, served to establish constraints in a ﬁnite-state transducer part-of-speech tagger. Actually, POS tagging was also the initial motivation for the present research, as adjective–noun and adjective–verb (participle) ambiguities cause most difﬁculties to both humans and machines in languages such as English, German, and Catalan (Marcus, Santorini, and Marcinkiewicz 1993; Brants 2000; Boleda 2007). Bohnet, Klatt, and Wanner (2002) also has similar goals to the present research, as it is aimed at automatically classifying German adjectives. However, the classiﬁcation 577 Computational Linguistics Volume 38, Number 3 used is not purely semantic, polysemy is not taken into account, and the evidence and techniques used are more limited than the ones used here. Other research on adjectives within computational linguistics is oriented toward different goals than ours. Yallop, Korhonen, and Briscoe (2005) tackle syntactic, not semantic classiﬁcation, akin to the acquisition of s"
J12-3005,E09-1013,0,0.070879,"Missing"
J12-3005,C00-1023,0,0.0490761,"at automatically classifying German adjectives. However, the classiﬁcation 577 Computational Linguistics Volume 38, Number 3 used is not purely semantic, polysemy is not taken into account, and the evidence and techniques used are more limited than the ones used here. Other research on adjectives within computational linguistics is oriented toward different goals than ours. Yallop, Korhonen, and Briscoe (2005) tackle syntactic, not semantic classiﬁcation, akin to the acquisition of subcategorization frames for verbs. Another relevant line of research pursues WSD. Justeson and Katz (1995) and Chao and Dyer (2000) showed that adjectives are a very useful cue for disambiguating the sense of the nouns they modify. Adjective classes could be further exploited in WSD in at least two respects: (1) to establish an inventory of adjective senses (if polysemous instances are correctly detected; this is where sense induction and our own work ﬁts in), and (2) to exploit class-based properties for the disambiguation, similar to related work on verb classes (Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention, however, is Opinion Mini"
J12-3005,P10-1018,0,0.0531777,"Missing"
J12-3005,C96-1055,0,0.140226,"t of the work reported in this article was done while the ﬁrst author was a postdoctoral scholar at U. Polit`ecnica de Catalunya and a visiting researcher at U. Stuttgart. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 1998). This article tackles precisely this task, that is, the semantic classiﬁcation of adjectives, for Catalan. We aim at automatically inducing the semantic class for an adjective given its linguistic properties, as extracted from corpora and other resources. The acquisition of semantic classes has been widely studied for verbs (Dorr and Jones 1996; McCarthy 2000; Korhonen, Krymolowski, and Marx 2003; Lapata and Brew 2004; Schulte im Walde 2006; Joanis, Stevenson, and James 2008) and, to a lesser extent, for nouns (Hindle 1990; Pereira, Tishby, and Lee 1993), but, with very few exceptions (Bohnet, Klatt, and Wanner 2002; Carvalho and Ranchhod 2003), not for adjectives. Furthermore, we cannot rely on a well-established classiﬁcation for adjectives. The classes themselves are subject to experimentation. We will test two different classiﬁcations, analyzing the empirical properties of the classes and the problems in their deﬁnition. Another"
J12-3005,P10-2017,0,0.0693823,"Missing"
J12-3005,P93-1023,0,0.405696,"similar to related work on verb classes (Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention, however, is Opinion Mining and Sentiment Analysis (Pang and Lee 2008), as adjectives are known to convey much of the evaluative and subjective information in language (Wiebe et al. 2004). The typical goal of this kind of study has been to identify subjective adjectives and their orientation (positive, neutral, negative). This type of research, from pioneering work by Hatzivassiloglou and colleages (Hatzivassiloglou and McKeown 1993, 1997; Hatzivassiloglou and Wiebe 2000) to current research (de Marneffe, Manning, and Potts 2010), has thus focused on scalar adjectives, that is, adjectives like good and bad, which can be translated into values that can be ordered along a scale. These adjectives typically enter into antonymy relations (the semantic relation between good and bad), and in fact antonymy is the main organizing criterion for adjectives in WordNet (Miller 1998), the most widely used semantic resource in NLP. However, when examining a large scale lexicon, it becomes immediately apparent that there are many other"
J12-3005,P97-1023,0,0.562892,"Missing"
J12-3005,C00-1044,0,0.0159898,"(Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention, however, is Opinion Mining and Sentiment Analysis (Pang and Lee 2008), as adjectives are known to convey much of the evaluative and subjective information in language (Wiebe et al. 2004). The typical goal of this kind of study has been to identify subjective adjectives and their orientation (positive, neutral, negative). This type of research, from pioneering work by Hatzivassiloglou and colleages (Hatzivassiloglou and McKeown 1993, 1997; Hatzivassiloglou and Wiebe 2000) to current research (de Marneffe, Manning, and Potts 2010), has thus focused on scalar adjectives, that is, adjectives like good and bad, which can be translated into values that can be ordered along a scale. These adjectives typically enter into antonymy relations (the semantic relation between good and bad), and in fact antonymy is the main organizing criterion for adjectives in WordNet (Miller 1998), the most widely used semantic resource in NLP. However, when examining a large scale lexicon, it becomes immediately apparent that there are many other types of adjectives that do not easily ﬁ"
J12-3005,P90-1034,0,0.837972,"for Computational Linguistics Computational Linguistics Volume 38, Number 3 1998). This article tackles precisely this task, that is, the semantic classiﬁcation of adjectives, for Catalan. We aim at automatically inducing the semantic class for an adjective given its linguistic properties, as extracted from corpora and other resources. The acquisition of semantic classes has been widely studied for verbs (Dorr and Jones 1996; McCarthy 2000; Korhonen, Krymolowski, and Marx 2003; Lapata and Brew 2004; Schulte im Walde 2006; Joanis, Stevenson, and James 2008) and, to a lesser extent, for nouns (Hindle 1990; Pereira, Tishby, and Lee 1993), but, with very few exceptions (Bohnet, Klatt, and Wanner 2002; Carvalho and Ranchhod 2003), not for adjectives. Furthermore, we cannot rely on a well-established classiﬁcation for adjectives. The classes themselves are subject to experimentation. We will test two different classiﬁcations, analyzing the empirical properties of the classes and the problems in their deﬁnition. Another signiﬁcant challenge is posed by polysemy, or the fact that one and the same adjective can have multiple senses. Different senses may fall into different classes, such that it is no"
J12-3005,J95-1001,0,0.159979,"sent research, as it is aimed at automatically classifying German adjectives. However, the classiﬁcation 577 Computational Linguistics Volume 38, Number 3 used is not purely semantic, polysemy is not taken into account, and the evidence and techniques used are more limited than the ones used here. Other research on adjectives within computational linguistics is oriented toward different goals than ours. Yallop, Korhonen, and Briscoe (2005) tackle syntactic, not semantic classiﬁcation, akin to the acquisition of subcategorization frames for verbs. Another relevant line of research pursues WSD. Justeson and Katz (1995) and Chao and Dyer (2000) showed that adjectives are a very useful cue for disambiguating the sense of the nouns they modify. Adjective classes could be further exploited in WSD in at least two respects: (1) to establish an inventory of adjective senses (if polysemous instances are correctly detected; this is where sense induction and our own work ﬁts in), and (2) to exploit class-based properties for the disambiguation, similar to related work on verb classes (Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention,"
J12-3005,P05-1005,0,0.0610871,"mes for verbs. Another relevant line of research pursues WSD. Justeson and Katz (1995) and Chao and Dyer (2000) showed that adjectives are a very useful cue for disambiguating the sense of the nouns they modify. Adjective classes could be further exploited in WSD in at least two respects: (1) to establish an inventory of adjective senses (if polysemous instances are correctly detected; this is where sense induction and our own work ﬁts in), and (2) to exploit class-based properties for the disambiguation, similar to related work on verb classes (Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention, however, is Opinion Mining and Sentiment Analysis (Pang and Lee 2008), as adjectives are known to convey much of the evaluative and subjective information in language (Wiebe et al. 2004). The typical goal of this kind of study has been to identify subjective adjectives and their orientation (positive, neutral, negative). This type of research, from pioneering work by Hatzivassiloglou and colleages (Hatzivassiloglou and McKeown 1993, 1997; Hatzivassiloglou and Wiebe 2000) to current research (de Marneffe, Manning, and Potts 2010),"
J12-3005,P03-1009,0,0.522368,"Missing"
J12-3005,N01-1009,0,0.248894,"Missing"
J12-3005,J04-1003,0,0.0606417,"a postdoctoral scholar at U. Polit`ecnica de Catalunya and a visiting researcher at U. Stuttgart. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 1998). This article tackles precisely this task, that is, the semantic classiﬁcation of adjectives, for Catalan. We aim at automatically inducing the semantic class for an adjective given its linguistic properties, as extracted from corpora and other resources. The acquisition of semantic classes has been widely studied for verbs (Dorr and Jones 1996; McCarthy 2000; Korhonen, Krymolowski, and Marx 2003; Lapata and Brew 2004; Schulte im Walde 2006; Joanis, Stevenson, and James 2008) and, to a lesser extent, for nouns (Hindle 1990; Pereira, Tishby, and Lee 1993), but, with very few exceptions (Bohnet, Klatt, and Wanner 2002; Carvalho and Ranchhod 2003), not for adjectives. Furthermore, we cannot rely on a well-established classiﬁcation for adjectives. The classes themselves are subject to experimentation. We will test two different classiﬁcations, analyzing the empirical properties of the classes and the problems in their deﬁnition. Another signiﬁcant challenge is posed by polysemy, or the fact that one and the sa"
J12-3005,P00-1012,0,0.0126806,"rms of different semantic classes in this article. The semantic properties of adjectives can also be exploited in advanced NLP tasks and applications such as Question Answering, Dialog Systems, Natural Language Generation, or Information Extraction. For instance, from a sentence like This maimai is round and sweet, we can quite safely infer that the (invented) object maimai is a physical object, probably edible. This type of process could be exploited in, for instance, Information Extraction and ontology population, although to our knowledge this possibility has received but little attention (Malouf 2000; Almuhareb and Poesio 2004). As for polysemy, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem, by biasing the experimental material to include monosemous words only, or by choosing an approach that ignores polysemy (Hindle 1990; Merlo and Stevenson 2001; Schulte im Walde 2006; Joanis, Stevenson, and James 2008). There are a few exceptions to this tradition, such as Pereira, Tishby, and Lee (1993), Rooth et al. (1999), and Korhonen, Krymolowski, and Marx (2003), who used soft clustering methods for multiple assignment to verb semantic cl"
J12-3005,J93-2004,0,0.0422735,"Missing"
J12-3005,A00-2034,0,0.0327602,"d in this article was done while the ﬁrst author was a postdoctoral scholar at U. Polit`ecnica de Catalunya and a visiting researcher at U. Stuttgart. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 1998). This article tackles precisely this task, that is, the semantic classiﬁcation of adjectives, for Catalan. We aim at automatically inducing the semantic class for an adjective given its linguistic properties, as extracted from corpora and other resources. The acquisition of semantic classes has been widely studied for verbs (Dorr and Jones 1996; McCarthy 2000; Korhonen, Krymolowski, and Marx 2003; Lapata and Brew 2004; Schulte im Walde 2006; Joanis, Stevenson, and James 2008) and, to a lesser extent, for nouns (Hindle 1990; Pereira, Tishby, and Lee 1993), but, with very few exceptions (Bohnet, Klatt, and Wanner 2002; Carvalho and Ranchhod 2003), not for adjectives. Furthermore, we cannot rely on a well-established classiﬁcation for adjectives. The classes themselves are subject to experimentation. We will test two different classiﬁcations, analyzing the empirical properties of the classes and the problems in their deﬁnition. Another signiﬁcant cha"
J12-3005,P04-1036,0,0.0788851,"Missing"
J12-3005,H05-1124,0,0.0246268,"Missing"
J12-3005,J01-3003,0,0.799967,"s round and sweet, we can quite safely infer that the (invented) object maimai is a physical object, probably edible. This type of process could be exploited in, for instance, Information Extraction and ontology population, although to our knowledge this possibility has received but little attention (Malouf 2000; Almuhareb and Poesio 2004). As for polysemy, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem, by biasing the experimental material to include monosemous words only, or by choosing an approach that ignores polysemy (Hindle 1990; Merlo and Stevenson 2001; Schulte im Walde 2006; Joanis, Stevenson, and James 2008). There are a few exceptions to this tradition, such as Pereira, Tishby, and Lee (1993), Rooth et al. (1999), and Korhonen, Krymolowski, and Marx (2003), who used soft clustering methods for multiple assignment to verb semantic classes (see Section 4.5). There is very little related work in empirical computational semantics in modeling regular polysemy. A pioneering piece of research is Buitelaar (1998), which tried to account for regular polysemy with the CoreLex resource. CoreLex, building on the Generative Lexicon theory (Pustejovsk"
J12-3005,P93-1024,0,0.443207,"Missing"
J12-3005,W05-0311,0,0.0336628,"in Section 3, including syntactic and semantic characteristics. The judges had a moderate degree of agreement, comparable to that obtained in other tasks on semantics or discourse, inter-annotator scores ranging between κ = 0.54 and 0.64 (see Artstein and Poesio [2008] for a discussion of agreement measures for computational linguistics). For comparison, V´eronis (1998) reported a mean pair-wise weighted κ = 0.43 for a word sense tagging task in French; and Merlo and Stevenson (2001) obtained κ = 0.53–0.66 for the task of classifying English verbs as unergative, unaccusative, or object-drop. Poesio and Artstein (2005) report κ values of 0.63–0.66 (0.45–0.50 if a trivial category is dropped) for the tagging of anaphoric relations. Our judges reported difﬁculties in tagging particular kinds of adjectives, such as deverbal adjectives. This issue will be retaken in Section 4.5. No intensional adjectives were identiﬁed in the data by the judges, and only one intensional-qualitative adjective was identiﬁed. Two intensional lemmata were manually added to be able to minimally track the class. This is clearly insufﬁcient for a quantitative approach, however, so the intensional class is dropped in the alternative cl"
J12-3005,C00-2094,0,0.200395,"Missing"
J12-3005,P99-1014,0,0.16525,"ce, Information Extraction and ontology population, although to our knowledge this possibility has received but little attention (Malouf 2000; Almuhareb and Poesio 2004). As for polysemy, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem, by biasing the experimental material to include monosemous words only, or by choosing an approach that ignores polysemy (Hindle 1990; Merlo and Stevenson 2001; Schulte im Walde 2006; Joanis, Stevenson, and James 2008). There are a few exceptions to this tradition, such as Pereira, Tishby, and Lee (1993), Rooth et al. (1999), and Korhonen, Krymolowski, and Marx (2003), who used soft clustering methods for multiple assignment to verb semantic classes (see Section 4.5). There is very little related work in empirical computational semantics in modeling regular polysemy. A pioneering piece of research is Buitelaar (1998), which tried to account for regular polysemy with the CoreLex resource. CoreLex, building on the Generative Lexicon theory (Pustejovsky 1995), groups WordNet senses into 39 “basic 578 Boleda, Schulte im Walde, and Badia Modeling Regular Polysemy in Catalan Adjectives types” (broad ontological categor"
J12-3005,sanroma-boleda-2010-database,1,0.793345,"Missing"
J12-3005,J06-2001,1,0.948191,"Missing"
J12-3005,J98-1004,0,0.616799,"Missing"
J12-3005,J04-3002,0,0.0440612,"d in WSD in at least two respects: (1) to establish an inventory of adjective senses (if polysemous instances are correctly detected; this is where sense induction and our own work ﬁts in), and (2) to exploit class-based properties for the disambiguation, similar to related work on verb classes (Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention, however, is Opinion Mining and Sentiment Analysis (Pang and Lee 2008), as adjectives are known to convey much of the evaluative and subjective information in language (Wiebe et al. 2004). The typical goal of this kind of study has been to identify subjective adjectives and their orientation (positive, neutral, negative). This type of research, from pioneering work by Hatzivassiloglou and colleages (Hatzivassiloglou and McKeown 1993, 1997; Hatzivassiloglou and Wiebe 2000) to current research (de Marneffe, Manning, and Potts 2010), has thus focused on scalar adjectives, that is, adjectives like good and bad, which can be translated into values that can be ordered along a scale. These adjectives typically enter into antonymy relations (the semantic relation between good and bad)"
J12-3005,P05-1076,0,0.0708525,"Missing"
J12-3005,D07-1018,1,\N,Missing
J12-3005,J13-3008,0,\N,Missing
J12-3005,W11-0128,0,\N,Missing
K17-1036,W09-2402,0,0.0825866,"Missing"
K17-1036,D16-1229,0,0.138991,"ering (Shutova et al., 2013), among others. As to our knowledge, no previous work has explicitly exploited the idea of generalization (via hypernymy models) in metaphor detection yet. Related Work There is a number of recent approaches to trace semantic change via distributional methods. This includes mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a similarity metric (such as cosine) in a semantic vector space (Gulordava and Baroni, 2011; Kim et al., 2014; Xu and Kemp, 2015; Eger and Mehler, 2016; Hellrich and Hahn, 2016; Hamilton et al., 2016a,b) and (ii), word sense induction models (WSI) inferring for each word a probability distribution over different word senses (or topics) in turn modeled as a distribution over words (Wang and Mccallum, 2006; Bamman and Crane, 2011; Wijaya and Yeniterzi, 2011; Lau et al., 2012; Mihalcea and Nastase, 2012; Frermann and Lapata, 2016). Most of the similarity models seem to be limited to quantify the degree of overall change rather than being able to qualify different types of semantic change.2 Similarity metrics, in particular, were shown not to distinguish well between words on different levels"
K17-1036,E06-1042,0,0.0254003,"us approaches to language change exploit the notion of entropy. Juola (2003) describes language change on a very general level by computing the relative entropy (or KL-divergence) of language stages, i.e. intuitively speaking, measuring how well later stages of English encode a prior stage. Kisselew et al. (2016) are interested in the diachronic properties of conversion using— among other measures—a word entropy measure. Finally, research on synchronic metaphor identification has applied a wide range of approaches, including binary classification relying on standard distributional similarity (Birke and Sarkar, 2006), text cohesion measures (Li and Sporleder, 2009), classification relying on abstractness cues (Turney et al., 2011; K¨oper and Schulte im Walde, 2016) or cross-lingual information (Tsvetkov et al., 2014), and soft clustering (Shutova et al., 2013), among others. As to our knowledge, no previous work has explicitly exploited the idea of generalization (via hypernymy models) in metaphor detection yet. Related Work There is a number of recent approaches to trace semantic change via distributional methods. This includes mainly (i), semantic similarity models assuming one sense for each word and t"
K17-1036,P16-1141,0,0.0717024,"Missing"
K17-1036,E12-1060,0,0.256208,"hods. This includes mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a similarity metric (such as cosine) in a semantic vector space (Gulordava and Baroni, 2011; Kim et al., 2014; Xu and Kemp, 2015; Eger and Mehler, 2016; Hellrich and Hahn, 2016; Hamilton et al., 2016a,b) and (ii), word sense induction models (WSI) inferring for each word a probability distribution over different word senses (or topics) in turn modeled as a distribution over words (Wang and Mccallum, 2006; Bamman and Crane, 2011; Wijaya and Yeniterzi, 2011; Lau et al., 2012; Mihalcea and Nastase, 2012; Frermann and Lapata, 2016). Most of the similarity models seem to be limited to quantify the degree of overall change rather than being able to qualify different types of semantic change.2 Similarity metrics, in particular, were shown not to distinguish well between words on different levels of the semantic hierarchy (Shwartz et al., 2016). Thus, we cannot expect diachronic similarity models to reflect changes in the semantic generality of a word over time, which was described to be a central effect of semantic change (cf. Bybee, 2015, p. 197). Additionally, they"
K17-1036,D09-1033,0,0.0201518,"on of entropy. Juola (2003) describes language change on a very general level by computing the relative entropy (or KL-divergence) of language stages, i.e. intuitively speaking, measuring how well later stages of English encode a prior stage. Kisselew et al. (2016) are interested in the diachronic properties of conversion using— among other measures—a word entropy measure. Finally, research on synchronic metaphor identification has applied a wide range of approaches, including binary classification relying on standard distributional similarity (Birke and Sarkar, 2006), text cohesion measures (Li and Sporleder, 2009), classification relying on abstractness cues (Turney et al., 2011; K¨oper and Schulte im Walde, 2016) or cross-lingual information (Tsvetkov et al., 2014), and soft clustering (Shutova et al., 2013), among others. As to our knowledge, no previous work has explicitly exploited the idea of generalization (via hypernymy models) in metaphor detection yet. Related Work There is a number of recent approaches to trace semantic change via distributional methods. This includes mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a simil"
K17-1036,P14-1024,0,0.025241,"intuitively speaking, measuring how well later stages of English encode a prior stage. Kisselew et al. (2016) are interested in the diachronic properties of conversion using— among other measures—a word entropy measure. Finally, research on synchronic metaphor identification has applied a wide range of approaches, including binary classification relying on standard distributional similarity (Birke and Sarkar, 2006), text cohesion measures (Li and Sporleder, 2009), classification relying on abstractness cues (Turney et al., 2011; K¨oper and Schulte im Walde, 2016) or cross-lingual information (Tsvetkov et al., 2014), and soft clustering (Shutova et al., 2013), among others. As to our knowledge, no previous work has explicitly exploited the idea of generalization (via hypernymy models) in metaphor detection yet. Related Work There is a number of recent approaches to trace semantic change via distributional methods. This includes mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a similarity metric (such as cosine) in a semantic vector space (Gulordava and Baroni, 2011; Kim et al., 2014; Xu and Kemp, 2015; Eger and Mehler, 2016; Hellrich"
K17-1036,P12-2051,0,0.0570291,"s mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a similarity metric (such as cosine) in a semantic vector space (Gulordava and Baroni, 2011; Kim et al., 2014; Xu and Kemp, 2015; Eger and Mehler, 2016; Hellrich and Hahn, 2016; Hamilton et al., 2016a,b) and (ii), word sense induction models (WSI) inferring for each word a probability distribution over different word senses (or topics) in turn modeled as a distribution over words (Wang and Mccallum, 2006; Bamman and Crane, 2011; Wijaya and Yeniterzi, 2011; Lau et al., 2012; Mihalcea and Nastase, 2012; Frermann and Lapata, 2016). Most of the similarity models seem to be limited to quantify the degree of overall change rather than being able to qualify different types of semantic change.2 Similarity metrics, in particular, were shown not to distinguish well between words on different levels of the semantic hierarchy (Shwartz et al., 2016). Thus, we cannot expect diachronic similarity models to reflect changes in the semantic generality of a word over time, which was described to be a central effect of semantic change (cf. Bybee, 2015, p. 197). Additionally, they often pose the problem of ve"
K17-1036,D11-1063,0,0.0234796,"l level by computing the relative entropy (or KL-divergence) of language stages, i.e. intuitively speaking, measuring how well later stages of English encode a prior stage. Kisselew et al. (2016) are interested in the diachronic properties of conversion using— among other measures—a word entropy measure. Finally, research on synchronic metaphor identification has applied a wide range of approaches, including binary classification relying on standard distributional similarity (Birke and Sarkar, 2006), text cohesion measures (Li and Sporleder, 2009), classification relying on abstractness cues (Turney et al., 2011; K¨oper and Schulte im Walde, 2016) or cross-lingual information (Tsvetkov et al., 2014), and soft clustering (Shutova et al., 2013), among others. As to our knowledge, no previous work has explicitly exploited the idea of generalization (via hypernymy models) in metaphor detection yet. Related Work There is a number of recent approaches to trace semantic change via distributional methods. This includes mainly (i), semantic similarity models assuming one sense for each word and then measuring its spatial displacement by a similarity metric (such as cosine) in a semantic vector space (Gulordav"
K17-1036,E14-1054,0,0.0128119,"bly abstract) radically’ (MB ) as in (2). recall that metaphor involves a mapping between two different domains (as introduced in Lakoff and Johnson 1980) in contrast to other types of meaning change, which is why we would expect a relatively strong effect on the contextual distribution here. Moreover, not only the range of a word’s meanings influences the range of contexts it occurs in, but also the particular nature of the individual meanings has an influence. As research in hypernymy detection shows, words at different levels of semantic generality have different distributional properties (Rimell, 2014; Santus et al., 2014; Shwartz et al., 2016). According to the distributional informativeness hypothesis, semantically more general words are less informative than special words as they occur in more general contexts (Rimell, 2014; Santus et al., 2014). Hence, differences in semantic generality of source and target concept should be reflected by their contextual distribution.6 Such differences occur particularly with taxonomic meaning changes like generalization and specialization, but also with metaphoric change, as it often results in the emergence of more abstract meanings of a word. Consid"
K17-1036,W03-1011,0,0.0549213,"Missing"
K17-1036,W09-0214,0,0.0798965,"arning (CoNLL 2017), pages 354–367, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics metaphoric change test set for German. Section 7 illustrates how the measures’ predictions shall be evaluated. The results are presented and discussed in Section 8. Section 9 will then conclude and give a short outlook to further research objectives. 2 to detect meaning changes where no new senses can be induced as, e.g., in grammaticalization. Moreover, some models require elaborate training (e.g., Frermann and Lapata, 2016). Apart from similarity and WSI models, Sagi et al. (2009) measure semantic broadening and narrowing of words (shifting upwards and downwards in the semantic taxonomy respectively) via semantic density calculated as the average cosine of its context word vectors. Just as word entropy, semantic density is based on the measurement of linguistic context dispersion (see Section 3.1). However, this method is only applied in a case study with very limited scope in terms of the number of phenomena covered and there is no verification of the test items via annotation. Hence, it remains to be shown that the method can generally distinguish broadening and narr"
K17-1036,E14-4008,1,0.765644,"radically’ (MB ) as in (2). recall that metaphor involves a mapping between two different domains (as introduced in Lakoff and Johnson 1980) in contrast to other types of meaning change, which is why we would expect a relatively strong effect on the contextual distribution here. Moreover, not only the range of a word’s meanings influences the range of contexts it occurs in, but also the particular nature of the individual meanings has an influence. As research in hypernymy detection shows, words at different levels of semantic generality have different distributional properties (Rimell, 2014; Santus et al., 2014; Shwartz et al., 2016). According to the distributional informativeness hypothesis, semantically more general words are less informative than special words as they occur in more general contexts (Rimell, 2014; Santus et al., 2014). Hence, differences in semantic generality of source and target concept should be reflected by their contextual distribution.6 Such differences occur particularly with taxonomic meaning changes like generalization and specialization, but also with metaphoric change, as it often results in the emergence of more abstract meanings of a word. Consider, e.g., the develop"
K17-1036,J13-3003,0,\N,Missing
K17-1036,W09-0215,0,\N,Missing
K17-1036,P09-1002,0,\N,Missing
K17-1036,P09-2018,0,\N,Missing
K17-1036,Q16-1003,0,\N,Missing
K17-1036,N16-1039,1,\N,Missing
K17-1036,P16-2009,0,\N,Missing
K17-1036,W11-2508,0,\N,Missing
K17-1036,W16-2015,0,\N,Missing
K17-1036,C16-1262,0,\N,Missing
koper-schulte-im-walde-2014-rank,C10-1011,0,\N,Missing
koper-schulte-im-walde-2014-rank,W10-1827,0,\N,Missing
koper-schulte-im-walde-2014-rank,N09-3017,0,\N,Missing
koper-schulte-im-walde-2014-rank,S07-1005,0,\N,Missing
koper-schulte-im-walde-2014-rank,C10-2052,0,\N,Missing
koper-schulte-im-walde-2014-rank,S12-1023,0,\N,Missing
koper-schulte-im-walde-2014-rank,J98-1004,0,\N,Missing
koper-schulte-im-walde-2014-rank,P10-2017,0,\N,Missing
koper-schulte-im-walde-2014-rank,N10-1013,0,\N,Missing
koper-schulte-im-walde-2014-rank,I13-1072,1,\N,Missing
koper-schulte-im-walde-2014-rank,saint-dizier-2006-prepnet,0,\N,Missing
koper-schulte-im-walde-2014-rank,D07-1043,0,\N,Missing
L16-1191,N09-1003,0,0.0459216,"t files with the gold standard and/or automatic class assignments, relying on two commaseparated columns per line: hword, classi. After initialising the tool, it computes a two-dimensional map of the high-dimensional object data. The dimensionality reduction is performed by t-Stochastic Neighbour Embedding (t-SNE), see van der Maaten and Hinton (2008). TSNE is based on element-wise distances using the standard distance measure cosine. Figure 1 illustrates the resulting main view with the two-dimensional map. In the following paragraphs, we describe the options of the visualisation tool. 1 See Agirre et al. (2009) and Bullinaria and Levy (2007; 2012), among others, for systematic comparisons of co-occurrence features on various semantic relatedness tasks. 1202 B B C C A A D D E E Figure 1: Verbs are mapped into the visual representation A according to the similarity of their high-dimensional context vectors. Verbs of the gold standard class Weather are selected in C and highlighted in A. Navigation bar B lets users switch data sets as well as load, store and recompute visual mappings. With D precomputed clusterings can be highlighted and E offers details if vectors are compared. Main Window [A]: The ma"
L16-1191,W12-0203,0,0.0282197,"alysis tool based on 9,000 Ph.D. theses for investigating shared ideas and interdisciplinary collaboration between academic departments. There is comparable little work with respect to visualisation in lexical semantics: Kievit-Kylar and Jones (2012) present a tool that allows to identify relations between words in a graph structure, where words are visualised as nodes, and word similarities are shown as directed edges. In contrast to our proposed tool, the actual position of a word is however not determined by its underlying feature distribution, and can even be changed using click and drag. Heylen et al. (2012) used Google Chart Tools together with multidimensional scaling to obtain a two-dimensional visualisation based on the occurrences of Dutch nouns. 5. Conclusion We presented a novel interactive tool to visualise and explore high-dimensional distributional features in lexical semantic classification. The tool provides a visual overview that approximates the relation of objects, and in addition offers a trust-view for the reliable interpretation of distances if one specific element is in focus. Our tool is especially useful when explicit feature information and classification assignments are ava"
L16-1191,W14-3110,0,0.0837294,"Missing"
L16-1191,W14-3111,0,0.026568,"al. (1999) developed a tool to analyse large amounts of documents. There is also a rich tradition of visualisation 2 Empirical evidence showed that k = 5 neighbours is a good choice. 1204 Figure 4: Standard view, with distances based on T-SNE. The verbs highlighted in red belong to the gold standard class Position: liegen ‘to lie‘, sitzen ‘to sit’, stehen ‘to stand’. Figure 5: Trust-View with respect to the position verb liegen ‘to lie’. approaches with respect to topic models, such as Sievert and Shirley (2014) who make use of Latent Dirichlet allocation (LDA) to visualise topics, as well as Smith et al. (2014) who focus on hierarchical topic models. Topical similarity is also 1205 an important aspect in Chuang et al. (2012), who developed a visual analysis tool based on 9,000 Ph.D. theses for investigating shared ideas and interdisciplinary collaboration between academic departments. There is comparable little work with respect to visualisation in lexical semantics: Kievit-Kylar and Jones (2012) present a tool that allows to identify relations between words in a graph structure, where words are visualised as nodes, and word similarities are shown as directed edges. In contrast to our proposed tool,"
L16-1362,W15-0903,1,0.799043,"morphemes (Taft and Forster, 1975; Taft, 2004), or whether they can be accessed both ways: as whole forms and componentially (dual route models, cf. Caramazza et al. (1988), Baayen and Schreuder (1999)). From a computational point of view, addressing the compositionality of noun compounds (and multi-word expressions in more general) is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014); Weller et al. (2014); Cap et al. (2015); and Salehi et al. (2015) have integrated the prediction of compositionality into Statistical Machine Translation. Accordingly, research across languages has aimed for predicting the compositionality of noun compounds automatically. For example, Reddy et al. (2011) predicted 1 See Fleischer and Barz (2012) for a detailed overview and Klos (2011) for a recent detailed exploration. the compositionality of 90 English noun–noun compounds via distributional information. Similarly, Schulte im Walde et al. (2013) assessed various types of distributional features to predict the compositionality of 24"
L16-1362,D14-1024,0,0.0901731,"rth, 1983)), whether they are decomposed into their morphemes (Taft and Forster, 1975; Taft, 2004), or whether they can be accessed both ways: as whole forms and componentially (dual route models, cf. Caramazza et al. (1988), Baayen and Schreuder (1999)). From a computational point of view, addressing the compositionality of noun compounds (and multi-word expressions in more general) is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014); Weller et al. (2014); Cap et al. (2015); and Salehi et al. (2015) have integrated the prediction of compositionality into Statistical Machine Translation. Accordingly, research across languages has aimed for predicting the compositionality of noun compounds automatically. For example, Reddy et al. (2011) predicted 1 See Fleischer and Barz (2012) for a detailed overview and Klos (2011) for a recent detailed exploration. the compositionality of 90 English noun–noun compounds via distributional information. Similarly, Schulte im Walde et al. (2013) assessed various types of distributional featu"
L16-1362,dima-etal-2014-tell,0,0.0297534,"Lauer (1995), and also with a set of 35 seman´ tic relations introduced by Moldovan and Girju (2003). O S´eaghdha (2007) relied on a set of nine semantic relations suggested by Levi (1978), and designed and evaluated a set of relations that took over four of Levi’s relations (BE, HAVE, IN, ABOUT) and added two relations refering to event participants (ACTOR, INST(rument)) that replaced the relations MAKE, CAUSE, FOR, FROM, USE. The relation LEX refers to lexicalised compounds where no relation can be assigned. A set of 1,443 English noun compounds was annotated with his modified relation set. Dima et al. (2014) worked on German noun compounds and suggested to combine paraphraseand property-based relation annotation. ´ We decided to apply the relation set suggested by O S´eaghdha (2007) to our German noun compounds, for two reasons: (i) He had evaluated his annotation relations and annotation scheme, and (ii) his dataset had a similar size as ours, so we could aim for comparing results across languages. The three authors of this paper who are native speakers of German annotated the compounds with his semantic relations. We used three rounds for the annotation, with discussions in between. If disagree"
L16-1362,W97-0802,0,0.239331,"es relevant for the gold standard: • corpus frequencies of the compounds and the constituents (i.e., modifiers and heads), relying on decow; Consequently, we created a gold standard with a focus on two-part noun-noun compounds including • productivity of the constituents (modifiers and heads), i.e., how many compound types contained a specific modifier or head constituent; • compounds and constituents from various frequency ranges; • compounds and constituents from various productivity ranges; • number of senses of the constituents (modifiers and heads) and the compounds, relying on GermaNet (Hamp and Feldweg, 1997; Kunze, 2000). • compounds and constituents with various numbers of senses; and • compounds with various semantic relations. Optimally, the compound targets in the gold standard should be balanced according to all of the above criteria, to include a similar number of compounds and constituents across the conditions. The following section will describe details of the creation process, and to what extent we achieved such a balance. 3. Creation of the Gold Standard We rely on one of the currently largest corpora for German to induce our new gold standard of German noun–noun compounds Gh ost-NN:"
L16-1362,kunze-2000-extension,0,0.139754,"standard: • corpus frequencies of the compounds and the constituents (i.e., modifiers and heads), relying on decow; Consequently, we created a gold standard with a focus on two-part noun-noun compounds including • productivity of the constituents (modifiers and heads), i.e., how many compound types contained a specific modifier or head constituent; • compounds and constituents from various frequency ranges; • compounds and constituents from various productivity ranges; • number of senses of the constituents (modifiers and heads) and the compounds, relying on GermaNet (Hamp and Feldweg, 1997; Kunze, 2000). • compounds and constituents with various numbers of senses; and • compounds with various semantic relations. Optimally, the compound targets in the gold standard should be balanced according to all of the above criteria, to include a similar number of compounds and constituents across the conditions. The following section will describe details of the creation process, and to what extent we achieved such a balance. 3. Creation of the Gold Standard We rely on one of the currently largest corpora for German to induce our new gold standard of German noun–noun compounds Gh ost-NN: the web corpus"
L16-1362,I11-1024,0,0.201662,"ionality of noun compounds (and multi-word expressions in more general) is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014); Weller et al. (2014); Cap et al. (2015); and Salehi et al. (2015) have integrated the prediction of compositionality into Statistical Machine Translation. Accordingly, research across languages has aimed for predicting the compositionality of noun compounds automatically. For example, Reddy et al. (2011) predicted 1 See Fleischer and Barz (2012) for a detailed overview and Klos (2011) for a recent detailed exploration. the compositionality of 90 English noun–noun compounds via distributional information. Similarly, Schulte im Walde et al. (2013) assessed various types of distributional features to predict the compositionality of 244 German noun– noun compounds. Salehi and Cook (2013) and Salehi et al. (2014) explored multi-lingual dictionaries and distributional evidence to predict the compositionality of German and English noun–noun compounds. Evaluating predictions of compositionality requi"
L16-1362,S13-1039,0,0.20513,"integrated the prediction of compositionality into Statistical Machine Translation. Accordingly, research across languages has aimed for predicting the compositionality of noun compounds automatically. For example, Reddy et al. (2011) predicted 1 See Fleischer and Barz (2012) for a detailed overview and Klos (2011) for a recent detailed exploration. the compositionality of 90 English noun–noun compounds via distributional information. Similarly, Schulte im Walde et al. (2013) assessed various types of distributional features to predict the compositionality of 244 German noun– noun compounds. Salehi and Cook (2013) and Salehi et al. (2014) explored multi-lingual dictionaries and distributional evidence to predict the compositionality of German and English noun–noun compounds. Evaluating predictions of compositionality requires a gold standard of compositionality ratings, if the evaluation is not extrinsic. So in parallel to the emergence of computational systems predicting compositionality, there has also been an increase of gold standards to evaluate the predictions. Regarding noun compounds, Reddy et al. (2011) used heuristics about hypernymy and definitions in WordNet to induce 90 English noun–noun c"
L16-1362,E14-1050,0,0.436479,"of compositionality into Statistical Machine Translation. Accordingly, research across languages has aimed for predicting the compositionality of noun compounds automatically. For example, Reddy et al. (2011) predicted 1 See Fleischer and Barz (2012) for a detailed overview and Klos (2011) for a recent detailed exploration. the compositionality of 90 English noun–noun compounds via distributional information. Similarly, Schulte im Walde et al. (2013) assessed various types of distributional features to predict the compositionality of 244 German noun– noun compounds. Salehi and Cook (2013) and Salehi et al. (2014) explored multi-lingual dictionaries and distributional evidence to predict the compositionality of German and English noun–noun compounds. Evaluating predictions of compositionality requires a gold standard of compositionality ratings, if the evaluation is not extrinsic. So in parallel to the emergence of computational systems predicting compositionality, there has also been an increase of gold standards to evaluate the predictions. Regarding noun compounds, Reddy et al. (2011) used heuristics about hypernymy and definitions in WordNet to induce 90 English noun–noun compounds. Schulte im Wald"
L16-1362,W15-0909,0,0.23464,"rster, 1975; Taft, 2004), or whether they can be accessed both ways: as whole forms and componentially (dual route models, cf. Caramazza et al. (1988), Baayen and Schreuder (1999)). From a computational point of view, addressing the compositionality of noun compounds (and multi-word expressions in more general) is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014); Weller et al. (2014); Cap et al. (2015); and Salehi et al. (2015) have integrated the prediction of compositionality into Statistical Machine Translation. Accordingly, research across languages has aimed for predicting the compositionality of noun compounds automatically. For example, Reddy et al. (2011) predicted 1 See Fleischer and Barz (2012) for a detailed overview and Klos (2011) for a recent detailed exploration. the compositionality of 90 English noun–noun compounds via distributional information. Similarly, Schulte im Walde et al. (2013) assessed various types of distributional features to predict the compositionality of 244 German noun– noun compou"
L16-1362,schafer-bildhauer-2012-building,0,0.0574664,"Missing"
L16-1362,S13-1038,1,0.831166,"Missing"
L16-1362,W14-5709,1,0.922311,"Missing"
L16-1413,P14-1023,0,0.0432156,"ases with each iteration, it can be observed that in some cases the next paradigm word decreases the correlation for the test data. 15 20 25 Iteration 30 Distributional Information In order to apply the algorithm explained in Section 3., word representations were required. Since the training process considers every possible word of the vocabulary when selecting the next paradigm word, it is especially useful to work with low-dimensional word representations. Recent work by Mikolov et al. (2013) introduced an efficient way to learn low dimensional but reliable word representations. In addition Baroni et al. (2014) and K¨oper et al. (2015) compared these representations across a variety of tasks, showing superior performance to traditional count vector space approaches. We used the word2vec toolkit3 and applied it to a lemmatised version of the DECOW14AX German web corpus (Sch¨afer and Bildhauer, 2012; Sch¨afer, 2015). This corpus contains about 20 billion tokens. We ignored words that occurred less than 100 times in the corpus. In addition we tuned the hyper-parameter settings on two German word correlation tasks: Gur350 (Zesch and Gurevych, 2006) and Gur65 (Gurevych, 2005). These tasks compare distrib"
L16-1413,I05-1067,0,0.117727,"Missing"
L16-1413,W15-0105,1,0.877149,"Missing"
L16-1413,schafer-bildhauer-2012-building,0,0.143758,"Missing"
L16-1413,D11-1063,0,0.174282,"ere is a number of resources that focuses on a single rating type only: Brysbaert et al. (2014) collected 40.000 abstractness ratings for English. The MRC Psycholinguistic Database1 contains roughly eight thousand abstractness ratings. However all of these resources cover only a small proportion of a language due to the fact that human annotators are required to obtain ratings. Especially the German resources contain usually less than 3000 words. In addition most resources focus on nouns only and therefore lack other word classes such as verbs or adjectives. A notable exception is the work by Turney et al. (2011), who used a method that overcome these limitations by applying an algorithm from Turney and Littman (2003). This algorithm uses distributional semantics (vector representations of words) and learns to assign a rating score to unseen words based on other known labelled training instances. Using this method, they were able to learn abstractness ratings for 114 501 English words. In this work we apply the same method as in Turney et al. (2011) and: (i) we learn ratings for German, (ii) we learn four dif1 http://www.psych.rl.ac.uk/ ferent rating types, (iii) recent advances in the field of word r"
L16-1413,1996.amta-1.36,0,0.632596,"Missing"
L16-1413,W06-1104,0,0.0844879,"Missing"
N16-1039,W02-2001,0,0.175018,"’s context, and classified word senses in a given context as either literal or metaphorical. Their targets were adjective–noun combinations and verbs. Tsvetkov et al. (2014) presented a language-independent approach to metaphor identification. They used affective ratings, WordNet categories and vector-space word representations to train a metaphor-detecting classifier on English samples, and then applied it to a different target language using bilingual dictionaries. Computational research on particle verbs was initially concerned with the automatic acquisition of particle verbs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005). 354 Afterwards, the main focus has been on modelling the degree of compositionality of particle verbs as based on distributional features (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). All these approaches were type-based, and predicting the compositionality was mainly concerned with PV–BV similarity, not taking the contribution of the particle into account. In cases where the particle semantics was respected (such as Bannard (2005)), the results were disappointing because modelling particle senses is still an unsolved problem. Regarding Ge"
N16-1039,W03-1812,0,0.0837178,"tification. They used affective ratings, WordNet categories and vector-space word representations to train a metaphor-detecting classifier on English samples, and then applied it to a different target language using bilingual dictionaries. Computational research on particle verbs was initially concerned with the automatic acquisition of particle verbs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005). 354 Afterwards, the main focus has been on modelling the degree of compositionality of particle verbs as based on distributional features (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). All these approaches were type-based, and predicting the compositionality was mainly concerned with PV–BV similarity, not taking the contribution of the particle into account. In cases where the particle semantics was respected (such as Bannard (2005)), the results were disappointing because modelling particle senses is still an unsolved problem. Regarding German particle verbs, there has also been a focus on modelling PV compositionality (Kühner and Schulte im Walde, 2010; Bott and Schulte im Walde, 2014; Bott and Schulte im Walde, 2015). As in English, the approaches were a"
N16-1039,E06-1042,0,0.565383,"identification and computational models of German particle verbs (Section 2), before we introduce our dataset 353 Proceedings of NAACL-HLT 2016, pages 353–362, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics on German particle verbs (Section 3), the particle verb features (Section 4), and the experiments, results and analyses (Section 5). 2 Related Work Previous work relevant to this paper includes research on identifying non-literal language usage, and computational work on (German) particle verb meaning. Identification of non-literal language usage: Birke and Sarkar (2006), Birke and Sarkar (2007), Li and Sporleder (2009) and Sporleder and Li (2009) performed binary token-based classifications for English datasets, relying on various contextual indicators. Birke & Sarkar exploited seed sets of literal vs. non-literal sentences, and used distributional similarity to classify English verbs. Li & Sporleder defined two models of text cohesion (a cohesion chain and a cohesion graph) to classify V+NP and V+PP combinations. Shutova et al. (2013) performed both metaphor identification and interpretion (by paraphrasing), focusing on English verbs. She relied on a seed s"
N16-1039,W07-0104,0,0.306587,"ational models of German particle verbs (Section 2), before we introduce our dataset 353 Proceedings of NAACL-HLT 2016, pages 353–362, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics on German particle verbs (Section 3), the particle verb features (Section 4), and the experiments, results and analyses (Section 5). 2 Related Work Previous work relevant to this paper includes research on identifying non-literal language usage, and computational work on (German) particle verb meaning. Identification of non-literal language usage: Birke and Sarkar (2006), Birke and Sarkar (2007), Li and Sporleder (2009) and Sporleder and Li (2009) performed binary token-based classifications for English datasets, relying on various contextual indicators. Birke & Sarkar exploited seed sets of literal vs. non-literal sentences, and used distributional similarity to classify English verbs. Li & Sporleder defined two models of text cohesion (a cohesion chain and a cohesion graph) to classify V+NP and V+PP combinations. Shutova et al. (2013) performed both metaphor identification and interpretion (by paraphrasing), focusing on English verbs. She relied on a seed set of annotated metaphors"
N16-1039,C10-1011,0,0.0900157,"Missing"
N16-1039,bott-schulte-im-walde-2014-optimizing,1,0.926235,"Missing"
N16-1039,W15-0104,1,0.912106,"Missing"
N16-1039,P01-1025,0,0.104097,"up of sentences for a given target verb as a separate learning problem, while we learn one classifier across different verbs. Our method 4 (AC Ratings) can be considered a German re-implementation of the approach by Turney et al. (2011). In comparison to the results of previous work, our approach can safely be considered state-of-the-art. 5.2 PV-Specific Experiments 5.2.1 Incorporating Standard Measures of Multiword Idiomaticity One traditional line of research to identify typebased multiword collocations or idiomatic expressions relies on the association strength between the multiword parts (Evert and Krenn, 2001; Krenn and Evert, 2001; Stevenson et al., 2004): The stronger the association between the parts of a multiword expression (as determined by raw frequency, some variant of mutual information, etc.), the stronger the collocation/idiomaticity of the combination of the parts. Based on this assumption, we calculated the association strength between PVs and their contextual subjects/objects, using local mutual information (LMI), cf. Evert (2005). The LMI scores were based on type-based frequency counts in the DECOW14AX corpus and added as features to the respective contexts, assuming that large LMI"
N16-1039,W06-3506,0,0.041534,"ications for English datasets, relying on various contextual indicators. Birke & Sarkar exploited seed sets of literal vs. non-literal sentences, and used distributional similarity to classify English verbs. Li & Sporleder defined two models of text cohesion (a cohesion chain and a cohesion graph) to classify V+NP and V+PP combinations. Shutova et al. (2013) performed both metaphor identification and interpretion (by paraphrasing), focusing on English verbs. She relied on a seed set of annotated metaphors and standard verb and noun clustering, to classify literal vs. metaphorical verb senses. Gedigian et al. (2006) also predicted metaphorical meanings of English verb tokens, however heavily relying on manual rather than unsupervised data (i.e. labeled sentences and PropBank annotation) and a maximum entropy classifier. Turney et al. (2011) assumed that metaphorical word usage is correlated with the degree of abstractness of the word’s context, and classified word senses in a given context as either literal or metaphorical. Their targets were adjective–noun combinations and verbs. Tsvetkov et al. (2014) presented a language-independent approach to metaphor identification. They used affective ratings, Wor"
N16-1039,L16-1413,1,0.787135,"Missing"
N16-1039,Q15-1016,0,0.0614514,"the context of the first, literal sentence fits well to the BV meaning, but the context of the second, non-literal sentence does not. The distributional fit of the BV in the literal context should therefore be high, but the distributional fit of the BV in the non-literal context should be low. 1. (lit.) “Der Ton der Gitarre klingt aus.” The tone of the guitar fades. 2. (non-lit.) “Den Abend lassen wir mit Wein ausklingen.” We end the evening with wine. To measure the distributional fit of PVs and BVs to PV contexts, we created 400-dimensional word representations using the hyperwords toolkit (Levy et al., 2015) and the DECOW14AX corpus. We relied on a symmetrical window of size 3 and applied positive pointwise mutual (PPMI) feature weighting together with singular value decomposition (SVD). Based on the word representations, we calculated cosine similarities between the PVs and their contexts, and likewise between the respective BVs and the PV contexts. The contexts we used were the same seven dimensions we used for the affective ratings (cf. Section 4.2). For example, regarding the sentence “Die Katze springt auf den Tisch” (The cat jumps on the table), we calculated the distributional similarity b"
N16-1039,D09-1033,0,0.126501,"particle verbs (Section 2), before we introduce our dataset 353 Proceedings of NAACL-HLT 2016, pages 353–362, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics on German particle verbs (Section 3), the particle verb features (Section 4), and the experiments, results and analyses (Section 5). 2 Related Work Previous work relevant to this paper includes research on identifying non-literal language usage, and computational work on (German) particle verb meaning. Identification of non-literal language usage: Birke and Sarkar (2006), Birke and Sarkar (2007), Li and Sporleder (2009) and Sporleder and Li (2009) performed binary token-based classifications for English datasets, relying on various contextual indicators. Birke & Sarkar exploited seed sets of literal vs. non-literal sentences, and used distributional similarity to classify English verbs. Li & Sporleder defined two models of text cohesion (a cohesion chain and a cohesion graph) to classify V+NP and V+PP combinations. Shutova et al. (2013) performed both metaphor identification and interpretion (by paraphrasing), focusing on English verbs. She relied on a seed set of annotated metaphors and standard verb and no"
N16-1039,W03-1810,0,0.184675,"proach to metaphor identification. They used affective ratings, WordNet categories and vector-space word representations to train a metaphor-detecting classifier on English samples, and then applied it to a different target language using bilingual dictionaries. Computational research on particle verbs was initially concerned with the automatic acquisition of particle verbs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005). 354 Afterwards, the main focus has been on modelling the degree of compositionality of particle verbs as based on distributional features (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). All these approaches were type-based, and predicting the compositionality was mainly concerned with PV–BV similarity, not taking the contribution of the particle into account. In cases where the particle semantics was respected (such as Bannard (2005)), the results were disappointing because modelling particle senses is still an unsolved problem. Regarding German particle verbs, there has also been a focus on modelling PV compositionality (Kühner and Schulte im Walde, 2010; Bott and Schulte im Walde, 2014; Bott and Schulte im Walde, 2015). As in English,"
N16-1039,D13-1032,0,0.093789,"Missing"
N16-1039,schafer-bildhauer-2012-building,0,0.376551,"Missing"
N16-1039,E09-1086,0,0.0639759,"before we introduce our dataset 353 Proceedings of NAACL-HLT 2016, pages 353–362, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics on German particle verbs (Section 3), the particle verb features (Section 4), and the experiments, results and analyses (Section 5). 2 Related Work Previous work relevant to this paper includes research on identifying non-literal language usage, and computational work on (German) particle verb meaning. Identification of non-literal language usage: Birke and Sarkar (2006), Birke and Sarkar (2007), Li and Sporleder (2009) and Sporleder and Li (2009) performed binary token-based classifications for English datasets, relying on various contextual indicators. Birke & Sarkar exploited seed sets of literal vs. non-literal sentences, and used distributional similarity to classify English verbs. Li & Sporleder defined two models of text cohesion (a cohesion chain and a cohesion graph) to classify V+NP and V+PP combinations. Shutova et al. (2013) performed both metaphor identification and interpretion (by paraphrasing), focusing on English verbs. She relied on a seed set of annotated metaphors and standard verb and noun clustering, to classify l"
N16-1039,springorum-etal-2012-automatic,1,0.855939,"In cases where the particle semantics was respected (such as Bannard (2005)), the results were disappointing because modelling particle senses is still an unsolved problem. Regarding German particle verbs, there has also been a focus on modelling PV compositionality (Kühner and Schulte im Walde, 2010; Bott and Schulte im Walde, 2014; Bott and Schulte im Walde, 2015). As in English, the approaches were all type-based and mainly concerned with PV–BV similarity. Another line of research categorized particle meanings by relating formal semantic definitions to automatic classifications (Rüd, 2012; Springorum et al., 2012). Furthermore, Springorum et al. (2013b) recently provided a corpusbased study on regular meaning shift conditions for German particle verbs. 3 Particle Verb Dataset We selected 165 particle verbs across 10 particles, based on previous experiments and datasets that incorporated German particle verbs with regular meaning shifts, various degrees of ambiguity, and across frequency ranges (Springorum et al., 2013b; Springorum et al., 2013a; Bott and Schulte im Walde, 2015). For the 165 PVs, we randomly extracted 50 sentences from DECOW14AX, a German web corpus containing 12 billion tokens (Schäfer"
N16-1039,W13-0120,1,0.934742,"ing Literal and Non-Literal Usage of German Particle Verbs Maximilian Köper Sabine Schulte im Walde Institut für Maschinelle Sprachverarbeitung Universität Stuttgart, Germany {maximilian.koeper,schulte}@ims.uni-stuttgart.de Abstract (BV) such as lachen (smile/laugh) and a verb particle such as an. German PVs are highly productive (Springorum et al., 2013b; Springorum et al., 2013a), and the particles are notoriously ambiguous (Lechler and Roßdeutscher, 2009; Haselbach, 2011; Springorum, 2011). Furthermore, the particles often trigger (regular) meaning shifts when they combine with base verbs (Springorum et al., 2013b), so the resulting PVs represent frequent cases of non-literal meaning. The contributions of this paper are as follows: This paper provides a binary, token-based classification of German particle verbs (PVs) into literal vs. non-literal usage. A random forest improving standard features (e.g., bagof-words; affective ratings) with PV-specific information and abstraction over common nouns significantly outperforms the majority baseline. In addition, PV-specific classification experiments demonstrate the role of shared particle semantics and semantically related base verbs in PV meaning shifts."
N16-1039,W04-0401,0,0.0528573,"eparate learning problem, while we learn one classifier across different verbs. Our method 4 (AC Ratings) can be considered a German re-implementation of the approach by Turney et al. (2011). In comparison to the results of previous work, our approach can safely be considered state-of-the-art. 5.2 PV-Specific Experiments 5.2.1 Incorporating Standard Measures of Multiword Idiomaticity One traditional line of research to identify typebased multiword collocations or idiomatic expressions relies on the association strength between the multiword parts (Evert and Krenn, 2001; Krenn and Evert, 2001; Stevenson et al., 2004): The stronger the association between the parts of a multiword expression (as determined by raw frequency, some variant of mutual information, etc.), the stronger the collocation/idiomaticity of the combination of the parts. Based on this assumption, we calculated the association strength between PVs and their contextual subjects/objects, using local mutual information (LMI), cf. Evert (2005). The LMI scores were based on type-based frequency counts in the DECOW14AX corpus and added as features to the respective contexts, assuming that large LMI scores indicate non-literal PV usage. 5.2.2 Non"
N16-1039,P14-1024,0,0.588314,"ted metaphors and standard verb and noun clustering, to classify literal vs. metaphorical verb senses. Gedigian et al. (2006) also predicted metaphorical meanings of English verb tokens, however heavily relying on manual rather than unsupervised data (i.e. labeled sentences and PropBank annotation) and a maximum entropy classifier. Turney et al. (2011) assumed that metaphorical word usage is correlated with the degree of abstractness of the word’s context, and classified word senses in a given context as either literal or metaphorical. Their targets were adjective–noun combinations and verbs. Tsvetkov et al. (2014) presented a language-independent approach to metaphor identification. They used affective ratings, WordNet categories and vector-space word representations to train a metaphor-detecting classifier on English samples, and then applied it to a different target language using bilingual dictionaries. Computational research on particle verbs was initially concerned with the automatic acquisition of particle verbs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005). 354 Afterwards, the main focus has been on modelling the degree of compositionality of particle verbs a"
N16-1039,D11-1063,0,0.634004,"two models of text cohesion (a cohesion chain and a cohesion graph) to classify V+NP and V+PP combinations. Shutova et al. (2013) performed both metaphor identification and interpretion (by paraphrasing), focusing on English verbs. She relied on a seed set of annotated metaphors and standard verb and noun clustering, to classify literal vs. metaphorical verb senses. Gedigian et al. (2006) also predicted metaphorical meanings of English verb tokens, however heavily relying on manual rather than unsupervised data (i.e. labeled sentences and PropBank annotation) and a maximum entropy classifier. Turney et al. (2011) assumed that metaphorical word usage is correlated with the degree of abstractness of the word’s context, and classified word senses in a given context as either literal or metaphorical. Their targets were adjective–noun combinations and verbs. Tsvetkov et al. (2014) presented a language-independent approach to metaphor identification. They used affective ratings, WordNet categories and vector-space word representations to train a metaphor-detecting classifier on English samples, and then applied it to a different target language using bilingual dictionaries. Computational research on particl"
N18-2024,N13-1090,0,0.027775,"model of meaning shifts for German particle verbs. We define our task from the perspective of an analogy, comparing a BV pair with a PV pair, cf. Figure 1. A BV–PV model of regular meaning shifts expects (i) semantic coherence In a similar vein, a rich tradition on computational work on analogies focuses on finding a relational analogy in multiple choices as required by the SAT Scholastic Aptitude Test (Turney, 2006, 2012; Speer et al., 2017). While the SAT questions provide a limited set of possible answers, more recent attention has been spent on open vocabulary tasks of the form A:B::C:? (Mikolov et al., 2013; Levy and Goldberg, 2014). The contribution of our analogy model is twofold: (i) it makes a step forward from handselected manual datasets of meaning shifts to larger-scale automatic classification; and (ii) it aims to deepen the linguistic insights into complex verb meaning shifts. While we focus on German particle verbs, we expect our explorations to be applicable also to other types of meaning shifts or languages. Most importantly, we show that (a) there are variants of (ir)regular meaning shifts that go beyond what was found in corpus-based explorations; (b) generalisation via classificat"
N18-2024,W97-0802,0,0.164281,"ilarity model containing four cosine values. Figure 3 looks into cosine values across combinations of meaning shift categories. Figure 3 (a) shows box plots for BV-PV pairs in the two compositional categories vs. the meaning-shifted categories. It illustrates that BV-PV combinations Generalisation Models Classes and clusters are powerful techniques to generalise over unseen or infrequent events. We therefore extended the basic similarity model by adding class label features for the four involved verbs. We compared three different classifications. (1) We used the 15 verb classes from GermaNet (Hamp and Feldweg, 1997; Kunze, 2000). For particle verbs not covered by GermaNet, we used the existing verbs as a seed set and applied a nearest-prototype (centroid) classifier to all other BVs and PVs, with a centroid for each of the 15 classes. Thus we were able to assign class labels to all verbs in our dataset. (2) For three out of our four particle types (ab, an, auf ), we found existing manual semantic classifications with 9, 8 and 11 classes, respectively (Lechler and Roßdeutscher, 2009; Kliche, 2011; Springorum, 2011). To obtain class labels for all verbs, we applied the same 152 nearest-centroid technique"
N18-2024,schafer-bildhauer-2012-building,0,0.0313194,"Missing"
N18-2024,N16-1039,1,0.925157,"Missing"
N18-2024,W17-1903,1,0.896145,"Missing"
N18-2024,kunze-2000-extension,0,0.0485305,"four cosine values. Figure 3 looks into cosine values across combinations of meaning shift categories. Figure 3 (a) shows box plots for BV-PV pairs in the two compositional categories vs. the meaning-shifted categories. It illustrates that BV-PV combinations Generalisation Models Classes and clusters are powerful techniques to generalise over unseen or infrequent events. We therefore extended the basic similarity model by adding class label features for the four involved verbs. We compared three different classifications. (1) We used the 15 verb classes from GermaNet (Hamp and Feldweg, 1997; Kunze, 2000). For particle verbs not covered by GermaNet, we used the existing verbs as a seed set and applied a nearest-prototype (centroid) classifier to all other BVs and PVs, with a centroid for each of the 15 classes. Thus we were able to assign class labels to all verbs in our dataset. (2) For three out of our four particle types (ab, an, auf ), we found existing manual semantic classifications with 9, 8 and 11 classes, respectively (Lechler and Roßdeutscher, 2009; Kliche, 2011; Springorum, 2011). To obtain class labels for all verbs, we applied the same 152 nearest-centroid technique as for the Ger"
N18-2024,W13-0120,1,0.903731,"een four types of shifts, with verb classes boosting the performance, and affective features for abstractness, emotion and sentiment representing the most salient indicators. 1 BV2 donnern sim(BV2 ,PV2 ) sim(BV1 ,BV2 ) BV1 Introduction brummen PV2 aufdonnern sim(PV1 ,PV2 ) sim(BV1 ,PV1 ) PV1 aufbrummen Figure 1: Analogy model applied to BV–PV shifts. German particle verbs are complex verb structures such as anstrahlen ‘to beam/smile at’ that combine a prefix particle (an) with a base verb (strahlen ‘to beam’). They are highly ambiguous, and they often trigger meaning shifts of the base verbs (Springorum et al., 2013; K¨oper and Schulte im Walde, 2016). More specifically, Springorum et al. (2013) presented a manual corpus exploration suggesting regular mechanisms in meaning shifts from base verbs (BVs) to particle verbs (PVs) that apply across a semantically coherent set of BVs. For example, the two sound BVs brummen ’to hum’ and donnern ’to rumble’ both describe a displeasing loud noise. Combining them with the particle auf, the PVs aufbrummen and aufdonnern are near-synonyms in one of their senses, roughly meaning ’to forcefully assign a task’. In a similar vein, Morgan (1997) used schematic diagrams to"
N18-2024,P14-1024,0,0.0201253,"ning shift often involves a change in emotion and/or sentiment. For example, while the BV servieren ‘to serve’ is perceived as rather neutral or slightly positive, the PV abservieren ‘to dump sb.’ has a clearly negative meaning and correlates with the emotion sadness. On the other hand, the BV motzen ‘to grumble’ is associated with a negative sentiment and the emotion anger, while its PV aufmotzen ‘to shine up, soup up’ indicates a positive change. In a slightly different vein, non-literal word usage often correlates with the degree of abstractness of the word’s contexts (Turney et al., 2011; Tsvetkov et al., 2014; K¨oper and Schulte im Walde, 2016). For example, the PV abschminken with the BV schminken ‘to put on make-up’ has a literal, very concrete meaning (‘to remove makeup’) and also a shifted, very abstract non-literal meaning (‘to forget about something’). We enriched the basic similarity model by integrating affective information from human-created lexicons. Since affective datasets are typically small-scale and mostly exist for English, we applied a cross-lingual approach (Smith et al., 2017) to learn a linear transformation that aligns monolingual vectors from two languages in a single vector"
N18-2024,W14-1618,0,0.0204686,"ts for German particle verbs. We define our task from the perspective of an analogy, comparing a BV pair with a PV pair, cf. Figure 1. A BV–PV model of regular meaning shifts expects (i) semantic coherence In a similar vein, a rich tradition on computational work on analogies focuses on finding a relational analogy in multiple choices as required by the SAT Scholastic Aptitude Test (Turney, 2006, 2012; Speer et al., 2017). While the SAT questions provide a limited set of possible answers, more recent attention has been spent on open vocabulary tasks of the form A:B::C:? (Mikolov et al., 2013; Levy and Goldberg, 2014). The contribution of our analogy model is twofold: (i) it makes a step forward from handselected manual datasets of meaning shifts to larger-scale automatic classification; and (ii) it aims to deepen the linguistic insights into complex verb meaning shifts. While we focus on German particle verbs, we expect our explorations to be applicable also to other types of meaning shifts or languages. Most importantly, we show that (a) there are variants of (ir)regular meaning shifts that go beyond what was found in corpus-based explorations; (b) generalisation via classification boosts the strengths o"
N18-2024,J06-3003,0,0.0824466,"task’. In a similar vein, Morgan (1997) used schematic diagrams to illustrate meaning shifts of English complex verbs with the particle out. The goal of this work is to provide a computational model of meaning shifts for German particle verbs. We define our task from the perspective of an analogy, comparing a BV pair with a PV pair, cf. Figure 1. A BV–PV model of regular meaning shifts expects (i) semantic coherence In a similar vein, a rich tradition on computational work on analogies focuses on finding a relational analogy in multiple choices as required by the SAT Scholastic Aptitude Test (Turney, 2006, 2012; Speer et al., 2017). While the SAT questions provide a limited set of possible answers, more recent attention has been spent on open vocabulary tasks of the form A:B::C:? (Mikolov et al., 2013; Levy and Goldberg, 2014). The contribution of our analogy model is twofold: (i) it makes a step forward from handselected manual datasets of meaning shifts to larger-scale automatic classification; and (ii) it aims to deepen the linguistic insights into complex verb meaning shifts. While we focus on German particle verbs, we expect our explorations to be applicable also to other types of meaning"
N18-2024,D11-1063,0,0.0779768,"ct Models A BV–PV meaning shift often involves a change in emotion and/or sentiment. For example, while the BV servieren ‘to serve’ is perceived as rather neutral or slightly positive, the PV abservieren ‘to dump sb.’ has a clearly negative meaning and correlates with the emotion sadness. On the other hand, the BV motzen ‘to grumble’ is associated with a negative sentiment and the emotion anger, while its PV aufmotzen ‘to shine up, soup up’ indicates a positive change. In a slightly different vein, non-literal word usage often correlates with the degree of abstractness of the word’s contexts (Turney et al., 2011; Tsvetkov et al., 2014; K¨oper and Schulte im Walde, 2016). For example, the PV abschminken with the BV schminken ‘to put on make-up’ has a literal, very concrete meaning (‘to remove makeup’) and also a shifted, very abstract non-literal meaning (‘to forget about something’). We enriched the basic similarity model by integrating affective information from human-created lexicons. Since affective datasets are typically small-scale and mostly exist for English, we applied a cross-lingual approach (Smith et al., 2017) to learn a linear transformation that aligns monolingual vectors from two langu"
N18-2027,E12-1060,0,0.086885,"verview). Within this set, our paper is most related to work focusing on graded polysemy annotation. Most prominently, Soares da Silva (1992) is interested in the question of whether the theoretical distinction between polysemy and homonymy can be experimentally verified; Brown (2008) wants to know how finegrained word senses are, and Erk et al. (2009, 2013) examine whether we should adopt a graded notion of word meaning. In contrast, there is little work on annotation with a focus on semantic change, despite the growing interest and modeling efforts in the field of semantic change detection. Lau et al. (2012) and Cook et al. (2014) aim at verifying the semantic developments of their targets by a quasi-annotation procedure of dictionary entries, however without reporting inter-annotator agreement or other measures of reliability. Gulordava and Baroni (2011) ask annotators for their intuitions about changes but without direct relation to language data. Bamman and Crane (2011) exploit aligned translated texts as source of word senses and conduct a very limited annotation study on Latin texts from different time periods. Schlechtweg et al. (2017) propose a small-scale annotation of metaphoric change,"
N18-2027,P08-2063,0,0.834466,"e meaning changes with high interannotator agreement. The resulting test set for German comprises ratings from five annotators for the relatedness of 1,320 use pairs across 22 target words. 1 2 Related Work A large number of studies has been performed on synchronic word sense annotation (see Ide and Pustejovsky, 2017 for an overview). Within this set, our paper is most related to work focusing on graded polysemy annotation. Most prominently, Soares da Silva (1992) is interested in the question of whether the theoretical distinction between polysemy and homonymy can be experimentally verified; Brown (2008) wants to know how finegrained word senses are, and Erk et al. (2009, 2013) examine whether we should adopt a graded notion of word meaning. In contrast, there is little work on annotation with a focus on semantic change, despite the growing interest and modeling efforts in the field of semantic change detection. Lau et al. (2012) and Cook et al. (2014) aim at verifying the semantic developments of their targets by a quasi-annotation procedure of dictionary entries, however without reporting inter-annotator agreement or other measures of reliability. Gulordava and Baroni (2011) ask annotators"
N18-2027,C14-1154,0,0.204373,"set, our paper is most related to work focusing on graded polysemy annotation. Most prominently, Soares da Silva (1992) is interested in the question of whether the theoretical distinction between polysemy and homonymy can be experimentally verified; Brown (2008) wants to know how finegrained word senses are, and Erk et al. (2009, 2013) examine whether we should adopt a graded notion of word meaning. In contrast, there is little work on annotation with a focus on semantic change, despite the growing interest and modeling efforts in the field of semantic change detection. Lau et al. (2012) and Cook et al. (2014) aim at verifying the semantic developments of their targets by a quasi-annotation procedure of dictionary entries, however without reporting inter-annotator agreement or other measures of reliability. Gulordava and Baroni (2011) ask annotators for their intuitions about changes but without direct relation to language data. Bamman and Crane (2011) exploit aligned translated texts as source of word senses and conduct a very limited annotation study on Latin texts from different time periods. Schlechtweg et al. (2017) propose a small-scale annotation of metaphoric change, but altogether there is"
N18-2027,K17-1036,1,0.848899,"and modeling efforts in the field of semantic change detection. Lau et al. (2012) and Cook et al. (2014) aim at verifying the semantic developments of their targets by a quasi-annotation procedure of dictionary entries, however without reporting inter-annotator agreement or other measures of reliability. Gulordava and Baroni (2011) ask annotators for their intuitions about changes but without direct relation to language data. Bamman and Crane (2011) exploit aligned translated texts as source of word senses and conduct a very limited annotation study on Latin texts from different time periods. Schlechtweg et al. (2017) propose a small-scale annotation of metaphoric change, but altogether there is no standard test set across languages that goes beyond a few hand-selected examples. Introduction We see an increasing interest in the automatic detection of semantic change in computational linguistics (Hamilton et al., 2016; Frermann and Lapata, 2016; Schlechtweg et al., 2017, i.a.), motivated by expected performance improvements of practical NLP applications, or theoretical interest in language or cultural change. However, a major obstacle in the computational modeling of semantic change is evaluation (Lau et al"
N18-2027,D17-1118,0,0.0663508,"but altogether there is no standard test set across languages that goes beyond a few hand-selected examples. Introduction We see an increasing interest in the automatic detection of semantic change in computational linguistics (Hamilton et al., 2016; Frermann and Lapata, 2016; Schlechtweg et al., 2017, i.a.), motivated by expected performance improvements of practical NLP applications, or theoretical interest in language or cultural change. However, a major obstacle in the computational modeling of semantic change is evaluation (Lau et al., 2012; Cook et al., 2014; Frermann and Lapata, 2016; Dubossarsky et al., 2017). Most importantly, there is no reliable test set of semantic change for any language that goes beyond a small set of hand-selected targets. We counteract this lack of resources by extending a framework of synchronic polysemy annotation to the annotation of Diachronic Usage Relatedness (DURel). DURel has a strong theoretical basis and at the same time makes use of established synchronic procedures that rely on the intuitive notion of semantic relatedness. The annotations distinguish between innovative and reductive meaning change with high inter-annotator agreement. DURel is languageindependen"
N18-2027,P09-1002,0,0.098778,"ing test set for German comprises ratings from five annotators for the relatedness of 1,320 use pairs across 22 target words. 1 2 Related Work A large number of studies has been performed on synchronic word sense annotation (see Ide and Pustejovsky, 2017 for an overview). Within this set, our paper is most related to work focusing on graded polysemy annotation. Most prominently, Soares da Silva (1992) is interested in the question of whether the theoretical distinction between polysemy and homonymy can be experimentally verified; Brown (2008) wants to know how finegrained word senses are, and Erk et al. (2009, 2013) examine whether we should adopt a graded notion of word meaning. In contrast, there is little work on annotation with a focus on semantic change, despite the growing interest and modeling efforts in the field of semantic change detection. Lau et al. (2012) and Cook et al. (2014) aim at verifying the semantic developments of their targets by a quasi-annotation procedure of dictionary entries, however without reporting inter-annotator agreement or other measures of reliability. Gulordava and Baroni (2011) ask annotators for their intuitions about changes but without direct relation to la"
N18-2027,J13-3003,0,0.31494,"h and Mervis, 1975), Blank develops criteria to decide whether word uses are related by polysemy. He defines a continuum of semantic proximity with polysemy located between identity and homonymy, as depicted in Table 1. x Identity   Context Variance   Polysemy Homonymy Table 1: Continuum of semantic proximity (cf. Blank, 1997, p. 418). 3 While it is difficult to directly apply these criteria to practical annotation tasks, we exploit the scale of semantic proximity indirectly, as previously done by synchronic research on polysemy applying similar scales (Soares da Silva, 1992; Brown, 2008; Erk et al., 2013). Especially Erk et al.’s in-depth study validates an annotation framework relying on a scale of semantic proximity, revealing high inter-annotator agreement and strong correlation with traditional single-sense annotation as well as annotation of multiple lexical paraphrases. For our study, we decided to adopt 2 t1 : EARLIER t2 : LATER Figure 1: Two-dimensional use spaces (Tuggy, 1993; Zlatev, 2003) in two time periods with a target word w undergoing innovative meaning change. Dots represent uses of w. Spatial proximity of two uses means high relatedness. There are a number of other, more comp"
N18-2027,Q16-1003,0,0.244538,") ask annotators for their intuitions about changes but without direct relation to language data. Bamman and Crane (2011) exploit aligned translated texts as source of word senses and conduct a very limited annotation study on Latin texts from different time periods. Schlechtweg et al. (2017) propose a small-scale annotation of metaphoric change, but altogether there is no standard test set across languages that goes beyond a few hand-selected examples. Introduction We see an increasing interest in the automatic detection of semantic change in computational linguistics (Hamilton et al., 2016; Frermann and Lapata, 2016; Schlechtweg et al., 2017, i.a.), motivated by expected performance improvements of practical NLP applications, or theoretical interest in language or cultural change. However, a major obstacle in the computational modeling of semantic change is evaluation (Lau et al., 2012; Cook et al., 2014; Frermann and Lapata, 2016; Dubossarsky et al., 2017). Most importantly, there is no reliable test set of semantic change for any language that goes beyond a small set of hand-selected targets. We counteract this lack of resources by extending a framework of synchronic polysemy annotation to the annotati"
N18-2027,W11-2508,0,0.373756,"an be experimentally verified; Brown (2008) wants to know how finegrained word senses are, and Erk et al. (2009, 2013) examine whether we should adopt a graded notion of word meaning. In contrast, there is little work on annotation with a focus on semantic change, despite the growing interest and modeling efforts in the field of semantic change detection. Lau et al. (2012) and Cook et al. (2014) aim at verifying the semantic developments of their targets by a quasi-annotation procedure of dictionary entries, however without reporting inter-annotator agreement or other measures of reliability. Gulordava and Baroni (2011) ask annotators for their intuitions about changes but without direct relation to language data. Bamman and Crane (2011) exploit aligned translated texts as source of word senses and conduct a very limited annotation study on Latin texts from different time periods. Schlechtweg et al. (2017) propose a small-scale annotation of metaphoric change, but altogether there is no standard test set across languages that goes beyond a few hand-selected examples. Introduction We see an increasing interest in the automatic detection of semantic change in computational linguistics (Hamilton et al., 2016; F"
N18-2027,D16-1229,0,0.206362,"Missing"
N18-2032,J15-4004,0,0.453705,", gender, number, and tense, thus differing strongly from Western European languages. We introduce two novel datasets for Vietnamese: a dataset of lexical contrast pairs ViCon to distinguish between similarity (synonymy) and dissimilarity (antonymy), and a dataset of semantic relation pairs ViSim-400 to reflect the continuum between similarity and relatedness. The two datasets are publicly available.1 Moreover, we verify our novel datasets through standard and neural co-occurrence models, in order to show that we obtain a similar behaviour as for the corresponding English datasets SimLex-999 (Hill et al., 2015), and the lexical contrast dataset (henceforth LexCon), cf. Nguyen et al. (2016a). We present two novel datasets for the lowresource language Vietnamese to assess models of semantic similarity: ViCon comprises pairs of synonyms and antonyms across word classes, thus offering data to distinguish between similarity and dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets. 1 Introduction C"
N18-2032,P12-1015,0,0.0300536,"al., 2006), and machine translation (He et al., 2008; Marton et al., 2009). In order to evaluate these models, gold standard resources with word pairs have to be collected (typically across semantic relations such as synonymy, hypernymy, antonymy, co-hyponymy, meronomy, etc.) and annotated for their degree of similarity via human judgements. The most prominent examples of gold standard similarity resources for English are the Rubenstein & Goodenough (RG) dataset (Rubenstein and Goodenough, 1965), the TOEFL test questions (Landauer and Dumais, 1997), WordSim353 (Finkelstein et al., 2001), MEN (Bruni et al., 2012), SimLex-999 (Hill et al., 2015), and the lexical contrast datasets by (Nguyen et al., 2016a, 2017). For other languages, resource examples are the translation of the RG dataset to German (Gurevych, 2005), the German dataset of paradigmatic relations (Scheible and Schulte im Walde, 2 Related Work Over the years a number of datasets have been collected for studying and evaluating semantic similarity and semantic relatedness. For English, Rubenstein and Goodenough (1965) presented a small dataset (RG) of 65 noun pairs. For each pair, the degree of similarity in meaning was provided by 15 raters."
N18-2032,J06-1003,0,0.187743,"nt two novel datasets for the lowresource language Vietnamese to assess models of semantic similarity: ViCon comprises pairs of synonyms and antonyms across word classes, thus offering data to distinguish between similarity and dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets. 1 Introduction Computational models that distinguish between semantic similarity and semantic relatedness (Budanitsky and Hirst, 2006) are important for many NLP applications, such as the automatic generation of dictionaries, thesauri, and ontologies (Biemann, 2005; Cimiano et al., 2005; Li et al., 2006), and machine translation (He et al., 2008; Marton et al., 2009). In order to evaluate these models, gold standard resources with word pairs have to be collected (typically across semantic relations such as synonymy, hypernymy, antonymy, co-hyponymy, meronomy, etc.) and annotated for their degree of similarity via human judgements. The most prominent examples of gold standard similarity resources for English are the Rubenstei"
N18-2032,P06-1129,0,0.126557,"Missing"
N18-2032,D09-1040,0,0.0267108,"ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets. 1 Introduction Computational models that distinguish between semantic similarity and semantic relatedness (Budanitsky and Hirst, 2006) are important for many NLP applications, such as the automatic generation of dictionaries, thesauri, and ontologies (Biemann, 2005; Cimiano et al., 2005; Li et al., 2006), and machine translation (He et al., 2008; Marton et al., 2009). In order to evaluate these models, gold standard resources with word pairs have to be collected (typically across semantic relations such as synonymy, hypernymy, antonymy, co-hyponymy, meronomy, etc.) and annotated for their degree of similarity via human judgements. The most prominent examples of gold standard similarity resources for English are the Rubenstein & Goodenough (RG) dataset (Rubenstein and Goodenough, 1965), the TOEFL test questions (Landauer and Dumais, 1997), WordSim353 (Finkelstein et al., 2001), MEN (Bruni et al., 2012), SimLex-999 (Hill et al., 2015), and the lexical contr"
N18-2032,goldhahn-etal-2012-building,0,0.0182433,"E model (Nguyen et al., 2016a), and the mLCM model (Pham et al., 2015). Both the dLCE and the mLCM models integrated lexical contrast information into the basic Skipgram model to train word embeddings for distinguishing antonyms from synonyms, and for reflecting degrees of similarity. The three models were trained with 300 dimensions, a window size of 5 words, and 10 negative samples. Regarding the corpora, we relied on Vietnamese corpora with a total of ≈145 million tokens, including the Vietnamese Wikipedia,9 VNESEcorpus and VNTQcorpus,10 and the Leipzig Corpora Collection for Vietnamese11 (Goldhahn et al., 2012). For word segmentation and POS tagging, we used the opensource toolkit UETnlp12 (Nguyen and Le, 2016). The antonym and synonym pairs to train the 9 https://dumps.wikimedia.org/viwiki/latest/ http://viet.jnlp.org/ download-du-lieu-tu-vung-corpus 11 http://wortschatz.uni-leipzig.de/en/download 12 https://github.com/phongnt570/UETnlp 10 202 dLCE and mLCM models were extracted from VWN consisting of 49,458 antonymous pairs and 338,714 synonymous pairs. All pairs which appeared in ViSim-400 were excluded from this set. Table 2 shows Spearman’s correlations ρ, comparing the scores of the three mode"
N18-2032,W04-2607,0,0.065819,"of synonymy and co-hyponymy. For example, in Vietnamese “đội” (team) and “nhóm” (group) represents a synonym pair; “ô_tô” (car) and “xe_đạp” (bike) is a co-hyponymy pair. More specifically, words in the pair “ô_tô” (car) and “xe_đạp” (bike) share several features such as physical (e.g. bánh_xe / wheels) and functional (e.g. vận_tải / transport), so that the two Vietnamese words are interchangeable regarding the kinds of transportation. The concept of semantic relatedness is broader and holds for relations such as meronymy, antonymy, functional association, and other “nonclassical relations” (Morris and Hirst, 2004). For example, “ô_tô” (car) and “xăng_dầu” (petrol) represent a meronym pair. In contrast to similarity, this meronym pair expresses a clearly functional relationship; the words are strongly associated with each other but not similar. Empirical studies have shown that the predictions of distributional models as well as humans are strongly related to the part-of-speech (POS) category of the learned concepts. Among others, Gentner (2006) showed that verb concepts are harder to learn by children than noun concepts. Distinguishing antonymy from synonymy is one of the most difficult challenges. Whi"
N18-2032,I05-1067,0,0.306203,"such as synonymy, hypernymy, antonymy, co-hyponymy, meronomy, etc.) and annotated for their degree of similarity via human judgements. The most prominent examples of gold standard similarity resources for English are the Rubenstein & Goodenough (RG) dataset (Rubenstein and Goodenough, 1965), the TOEFL test questions (Landauer and Dumais, 1997), WordSim353 (Finkelstein et al., 2001), MEN (Bruni et al., 2012), SimLex-999 (Hill et al., 2015), and the lexical contrast datasets by (Nguyen et al., 2016a, 2017). For other languages, resource examples are the translation of the RG dataset to German (Gurevych, 2005), the German dataset of paradigmatic relations (Scheible and Schulte im Walde, 2 Related Work Over the years a number of datasets have been collected for studying and evaluating semantic similarity and semantic relatedness. For English, Rubenstein and Goodenough (1965) presented a small dataset (RG) of 65 noun pairs. For each pair, the degree of similarity in meaning was provided by 15 raters. The RG dataset is assumed to reflect similarity rather than relatedness. Finkelstein et al. (2001) created a set of 353 English nounnoun pairs (WordSim-353)2 , where each pair was rated by 16 subjects ac"
N18-2032,E17-1008,1,0.799014,"Missing"
N18-2032,P16-2074,1,0.652555,"guages. We introduce two novel datasets for Vietnamese: a dataset of lexical contrast pairs ViCon to distinguish between similarity (synonymy) and dissimilarity (antonymy), and a dataset of semantic relation pairs ViSim-400 to reflect the continuum between similarity and relatedness. The two datasets are publicly available.1 Moreover, we verify our novel datasets through standard and neural co-occurrence models, in order to show that we obtain a similar behaviour as for the corresponding English datasets SimLex-999 (Hill et al., 2015), and the lexical contrast dataset (henceforth LexCon), cf. Nguyen et al. (2016a). We present two novel datasets for the lowresource language Vietnamese to assess models of semantic similarity: ViCon comprises pairs of synonyms and antonyms across word classes, thus offering data to distinguish between similarity and dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets. 1 Introduction Computational models that distinguish between semantic similarity and semantic r"
N18-2032,D08-1011,0,0.0334893,"nd dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets. 1 Introduction Computational models that distinguish between semantic similarity and semantic relatedness (Budanitsky and Hirst, 2006) are important for many NLP applications, such as the automatic generation of dictionaries, thesauri, and ontologies (Biemann, 2005; Cimiano et al., 2005; Li et al., 2006), and machine translation (He et al., 2008; Marton et al., 2009). In order to evaluate these models, gold standard resources with word pairs have to be collected (typically across semantic relations such as synonymy, hypernymy, antonymy, co-hyponymy, meronomy, etc.) and annotated for their degree of similarity via human judgements. The most prominent examples of gold standard similarity resources for English are the Rubenstein & Goodenough (RG) dataset (Rubenstein and Goodenough, 1965), the TOEFL test questions (Landauer and Dumais, 1997), WordSim353 (Finkelstein et al., 2001), MEN (Bruni et al., 2012), SimLex-999 (Hill et al., 2015),"
N18-2032,2016.gwc-1.38,0,0.125361,"guages. We introduce two novel datasets for Vietnamese: a dataset of lexical contrast pairs ViCon to distinguish between similarity (synonymy) and dissimilarity (antonymy), and a dataset of semantic relation pairs ViSim-400 to reflect the continuum between similarity and relatedness. The two datasets are publicly available.1 Moreover, we verify our novel datasets through standard and neural co-occurrence models, in order to show that we obtain a similar behaviour as for the corresponding English datasets SimLex-999 (Hill et al., 2015), and the lexical contrast dataset (henceforth LexCon), cf. Nguyen et al. (2016a). We present two novel datasets for the lowresource language Vietnamese to assess models of semantic similarity: ViCon comprises pairs of synonyms and antonyms across word classes, thus offering data to distinguish between similarity and dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets. 1 Introduction Computational models that distinguish between semantic similarity and semantic r"
N18-2032,D07-1042,0,0.0971335,"Missing"
N18-2032,P15-2004,0,0.0246537,"ility of the annotated dataset. Furthermore, the box plots in Figure 1 present the distributions of all rated pairs in terms of the fine-grained semantic relations across word classes. They reveal that –across word classes– synonym pairs are clearly rated as the most similar words, and antonym as well as unrelated pairs 4.1 Verification of ViSim-400 We adopt a comparison of neural models on SimLex-999 as suggested by Nguyen et al. (2016a). They applied three models, a Skip-gram model with negative sampling SGNS (Mikolov et al., 2013), the dLCE model (Nguyen et al., 2016a), and the mLCM model (Pham et al., 2015). Both the dLCE and the mLCM models integrated lexical contrast information into the basic Skipgram model to train word embeddings for distinguishing antonyms from synonyms, and for reflecting degrees of similarity. The three models were trained with 300 dimensions, a window size of 5 words, and 10 negative samples. Regarding the corpora, we relied on Vietnamese corpora with a total of ≈145 million tokens, including the Vietnamese Wikipedia,9 VNESEcorpus and VNTQcorpus,10 and the Leipzig Corpora Collection for Vietnamese11 (Goldhahn et al., 2012). For word segmentation and POS tagging, we used"
N18-2032,D10-1114,0,0.0294189,"ms of six relations: the five extracted relations from VCL and VWN, 8 Annotation of ViSim-400 3.5 Agreement in ViSim-400 We analysed the ratings of the ViSim-400 annotators with two different inter-annotator agreement (IAA) measures, Krippendorff’s alpha coefficient (Krippendorff, 2004), and the average standard deviation (STD) of all pairs across word classes. The first IAA measure, IAA-pairwise, computes the average pairwise Spearman’s ρ correlation between any two raters. This IAA measure has been a common choice in previous data collections in distributional semantics (Pad´o et al., 2007; Reisinger and Mooney, 2010; Hill et al., 2015). http://viet.wordnet.vn/wnms/ 201 ANT COHYPO HOLO HYPE SYN 6 6 6 4 4 4 2 2 2 0 0 UNREL 0 Adj Noun Verb Figure 1: Distribution of scored pairs in ViSim-400 across parts-of-speech and semantic relations. IAA-Mean ρ IAA-Pairwise ρ Krippendorff’s α STD All Noun Verb Adjective 0.86 0.79 0.78 0.87 0.86 0.76 0.76 0.87 0.86 0.78 0.78 0.90 0.78 0.75 0.86 0.82 are clearly rated as the most dissimilar words. Hypernymy, co-hyponymy and holonymy are in between, but rather similar than dissimilar. 4 Verification of Datasets In this section, we verify our novel datasets ViCon and ViSim-4"
N18-2032,W14-5814,1,0.906451,"Missing"
N18-2032,J17-4004,0,0.0591779,"Missing"
N18-2052,2014.amta-researchers.5,0,0.0216302,"m specificity, term granularity and subtermhood. 1 Introduction Terms are linguistic units which characterize a specific topic domain, and their identification is relevant for a number of NLP tasks, such as information retrieval and automatic translation. Not only the automatic extraction of terms is a challenging task, but also their manual definition and identification: while we find a range of gold standard corpora for the evaluation of term extraction systems for English (Kim et al., 2003; Bernier-Colborne and Drouin, 2014; Zadeh and Schumann, 2016) and to a lesser extent also for German (Arcan et al., 2014; Arcan, 2017; H¨atty et al., 2017), these benchmark datasets vary hugely in terms of granularity of term definition, topic and thematic focus. All datasets have in common that they have been annotated by domain experts and/or by terminologists, which is considered a necessary requirement for term evaluation (Castellv´ı, 1999; Gouws et al., 2007). However, Estop`a (2001) shows that even experts with different perspectives on terminology (e.g., terminologists, domain experts, translators and documentalists) vary significantly in their annotation of terms. Moreover, although individual studies d"
N18-2052,L16-1294,0,0.103097,"n inter-annotator agreement offer insights into differences in term specificity, term granularity and subtermhood. 1 Introduction Terms are linguistic units which characterize a specific topic domain, and their identification is relevant for a number of NLP tasks, such as information retrieval and automatic translation. Not only the automatic extraction of terms is a challenging task, but also their manual definition and identification: while we find a range of gold standard corpora for the evaluation of term extraction systems for English (Kim et al., 2003; Bernier-Colborne and Drouin, 2014; Zadeh and Schumann, 2016) and to a lesser extent also for German (Arcan et al., 2014; Arcan, 2017; H¨atty et al., 2017), these benchmark datasets vary hugely in terms of granularity of term definition, topic and thematic focus. All datasets have in common that they have been annotated by domain experts and/or by terminologists, which is considered a necessary requirement for term evaluation (Castellv´ı, 1999; Gouws et al., 2007). However, Estop`a (2001) shows that even experts with different perspectives on terminology (e.g., terminologists, domain experts, translators and documentalists) vary significantly in their a"
N18-2052,W17-7002,1,0.787118,"Missing"
N18-2052,W16-4706,0,0.0971534,"Missing"
N18-2052,P13-4001,0,0.0331632,"Missing"
N18-4002,W03-1809,0,0.011869,"ntence by answering the question: ”What is the usage of the PV in the sentence on a 6-point scale ranging from clearly literal (0) to clearly non-literal (5) language usage?” In case of multiple PVs in the same sentence, the information of which PV to evaluate was provided for the annotators. Although we use binary division of PVs in this study, it was reasonable to collect evaluations on a larger than binary scale because of the following reasons: first, it is a well-known fact that multi-word expressions do not fall into the binary classes of compositional vs. non-compositional expressions (Bannard et al., 2003), and second, it was important to create a dataset that would be applicable to multiple tasks. Thus our dataset can be used to investigate the degrees of compositionality of PVs in the future. The agreement among 3 annotators on all 6 categories is fair (Fleiss’ κ = 0.36). A binary distinction based on the average sentence scores into literal (average ≤ 2.4) and non-literal (average ≥ 2.5) resulted in substantial agreement (κ = 0.73). Our experiments below use the binary-class setting, disregarding all cases of disagreement. This final dataset2 includes 1,490 sentences: 1,102 non-literal and 3"
N18-4002,E06-1042,0,0.0218571,"t to each other, and the particles are homonymous with adpositions. In addition, as illustrated in examples (1a) vs. (1b), the same PV type can be used in literal vs. non-literal language. (1) a. Ta astu-s kaks sammu tagasi. he step-PST.3 SG two step.PRT back ‘He took two steps back.’ b. Ta astu-s ameti-st tagasi. he step-PST.3 SG job-ELA back ‘He resigned from his job.’ Given that the automatic detection of nonliteral expressions (including metaphors and idioms) is critical for many NLP tasks, the last decade has seen an increase in research on distinguishing literal vs. non-literal meaning (Birke and Sarkar, 2006, 2007; Sporleder and Li, 2009; Turney et al., 2011; Shutova et al., 2013; Tsvetkov et al., 2014; K¨oper and Schulte im Walde, 2016). 2 Related Work The compositionality of Estonian PVs has been under discussion in the theoretical literature for 9 Proceedings of NAACL-HLT 2018: Student Research Workshop, pages 9–16 c New Orleans, Louisiana, June 2 - 4, 2018. 2017 Association for Computational Linguistics decades but still lacks a comprehensive study. Tragel and Veismann (2008) studied six verbal particles and their aspectual meanings, and described how horizontal and vertical dimensions are re"
N18-4002,W07-0104,0,0.0571144,"Missing"
N18-4002,D11-1063,0,0.537988,"adpositions. In addition, as illustrated in examples (1a) vs. (1b), the same PV type can be used in literal vs. non-literal language. (1) a. Ta astu-s kaks sammu tagasi. he step-PST.3 SG two step.PRT back ‘He took two steps back.’ b. Ta astu-s ameti-st tagasi. he step-PST.3 SG job-ELA back ‘He resigned from his job.’ Given that the automatic detection of nonliteral expressions (including metaphors and idioms) is critical for many NLP tasks, the last decade has seen an increase in research on distinguishing literal vs. non-literal meaning (Birke and Sarkar, 2006, 2007; Sporleder and Li, 2009; Turney et al., 2011; Shutova et al., 2013; Tsvetkov et al., 2014; K¨oper and Schulte im Walde, 2016). 2 Related Work The compositionality of Estonian PVs has been under discussion in the theoretical literature for 9 Proceedings of NAACL-HLT 2018: Student Research Workshop, pages 9–16 c New Orleans, Louisiana, June 2 - 4, 2018. 2017 Association for Computational Linguistics decades but still lacks a comprehensive study. Tragel and Veismann (2008) studied six verbal particles and their aspectual meanings, and described how horizontal and vertical dimensions are represented. Veismann and Sahkai (2016) investigated"
N18-4002,kaalep-muischnek-2002-using,0,0.0904678,"iterature for 9 Proceedings of NAACL-HLT 2018: Student Research Workshop, pages 9–16 c New Orleans, Louisiana, June 2 - 4, 2018. 2017 Association for Computational Linguistics decades but still lacks a comprehensive study. Tragel and Veismann (2008) studied six verbal particles and their aspectual meanings, and described how horizontal and vertical dimensions are represented. Veismann and Sahkai (2016) investigated the prosody of Estonian PVs, finding PVs expressing perfectivity the most problematic to classify. Recent computational studies on Estonian PVs involve their automatic acquisition (Kaalep and Muischnek, 2002; Uiboaed, 2010; Aedmaa, 2014), and predicting their degrees of compositionality (Aedmaa, 2017). Muischnek et al. (2013) investigated the role of Estonian PVs in computational syntax, focusing on Constraint Grammar. Most research on automatically detecting non-literal language has been done on English and German (as mentioned above), and elaborated on general indicators to identify non-literal language. Our work is the first attempt to automatically distinguish literal and non-literal usage of Estonian PVs, and to specify on theory- and language-specific features. 3 The resulting set of senten"
N18-4002,L16-1413,1,0.883108,"Missing"
N18-4002,N16-1039,1,0.919737,"Missing"
N18-4002,E09-1086,0,0.015152,"cles are homonymous with adpositions. In addition, as illustrated in examples (1a) vs. (1b), the same PV type can be used in literal vs. non-literal language. (1) a. Ta astu-s kaks sammu tagasi. he step-PST.3 SG two step.PRT back ‘He took two steps back.’ b. Ta astu-s ameti-st tagasi. he step-PST.3 SG job-ELA back ‘He resigned from his job.’ Given that the automatic detection of nonliteral expressions (including metaphors and idioms) is critical for many NLP tasks, the last decade has seen an increase in research on distinguishing literal vs. non-literal meaning (Birke and Sarkar, 2006, 2007; Sporleder and Li, 2009; Turney et al., 2011; Shutova et al., 2013; Tsvetkov et al., 2014; K¨oper and Schulte im Walde, 2016). 2 Related Work The compositionality of Estonian PVs has been under discussion in the theoretical literature for 9 Proceedings of NAACL-HLT 2018: Student Research Workshop, pages 9–16 c New Orleans, Louisiana, June 2 - 4, 2018. 2017 Association for Computational Linguistics decades but still lacks a comprehensive study. Tragel and Veismann (2008) studied six verbal particles and their aspectual meanings, and described how horizontal and vertical dimensions are represented. Veismann and Sahkai"
N18-4002,P14-1024,0,0.362875,"ators for Detecting Non-Literal Usage of Estonian Particle Verbs Eleri Aedmaa1 , Maximilian K¨oper2 and Sabine Schulte im Walde2 1 Institute of Estonian and General Linguistics, University of Tartu, Estonia 2 Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart, Germany eleri.aedmaa@ut.ee {maximilian.koeper,schulte}@ims.uni-stuttgart.de Abstract Most research up to date has, however, focused on resource-rich languages (mainly English and German), and elaborated on general indicators – such as contextual abstractness – to identify non-literal language. As to our knowledge, only Tsvetkov et al. (2014) and K¨oper and Schulte im Walde (2016) explored language-specific features. The aim of this work is to automatically predict literal vs. non-literal language usage for a very frequent type of multi-word expression in a low-resource language, i.e., Estonian. The predicate is the center of grammatical and usually semantic structure of the sentence, and it determines the meaning and the form of its arguments, cf. Erelt et al. (1993). Hence, the surrounding words (i.e., the context), their meanings and grammatical forms could help to decide whether the PV should be classified as compositional or"
P02-1029,C96-1055,0,0.157719,"nts. The theoretical foundation has been established in extensive work on semantic verb classes such as (Levin, 1993) for English and (Vázquez et al., 2000) for Spanish: each verb class contains verbs which are similar in their meaning and in their syntactic properties. From a practical point of view, a verb classification supports Natural Language Processing tasks, since it provides a principled basis for filling gaps in available lexical knowledge. For example, the English verb classification has been used for applications such as machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998). Various attempts have been made to infer conveniently observable morpho-syntactic and semantic properties for English verb classes (Dorr and Jones, 1996; Lapata, 1999; Stevenson and Merlo, 1999; Schulte im Walde, 2000; McCarthy, 2001). To our knowledge this is the first work to obtain German verb classes automatically. We used a robust statistical parser (Schmid, 2000) to acquire purely syntactic subcategorisation information for verbs. The information was provided in form of probability distributions over verb frames for each verb. There"
P02-1029,P98-1112,0,0.417224,"in extensive work on semantic verb classes such as (Levin, 1993) for English and (Vázquez et al., 2000) for Spanish: each verb class contains verbs which are similar in their meaning and in their syntactic properties. From a practical point of view, a verb classification supports Natural Language Processing tasks, since it provides a principled basis for filling gaps in available lexical knowledge. For example, the English verb classification has been used for applications such as machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998). Various attempts have been made to infer conveniently observable morpho-syntactic and semantic properties for English verb classes (Dorr and Jones, 1996; Lapata, 1999; Stevenson and Merlo, 1999; Schulte im Walde, 2000; McCarthy, 2001). To our knowledge this is the first work to obtain German verb classes automatically. We used a robust statistical parser (Schmid, 2000) to acquire purely syntactic subcategorisation information for verbs. The information was provided in form of probability distributions over verb frames for each verb. There were two conditions: the first with relatively coarse"
P02-1029,P99-1051,0,0.216635,"aning and in their syntactic properties. From a practical point of view, a verb classification supports Natural Language Processing tasks, since it provides a principled basis for filling gaps in available lexical knowledge. For example, the English verb classification has been used for applications such as machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998). Various attempts have been made to infer conveniently observable morpho-syntactic and semantic properties for English verb classes (Dorr and Jones, 1996; Lapata, 1999; Stevenson and Merlo, 1999; Schulte im Walde, 2000; McCarthy, 2001). To our knowledge this is the first work to obtain German verb classes automatically. We used a robust statistical parser (Schmid, 2000) to acquire purely syntactic subcategorisation information for verbs. The information was provided in form of probability distributions over verb frames for each verb. There were two conditions: the first with relatively coarse syntactic verb subcategorisation frames, the second a more delicate classification subdividing the verb frames of the first condition using prepositional phrase inform"
P02-1029,C00-2108,1,0.895871,"tical point of view, a verb classification supports Natural Language Processing tasks, since it provides a principled basis for filling gaps in available lexical knowledge. For example, the English verb classification has been used for applications such as machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998). Various attempts have been made to infer conveniently observable morpho-syntactic and semantic properties for English verb classes (Dorr and Jones, 1996; Lapata, 1999; Stevenson and Merlo, 1999; Schulte im Walde, 2000; McCarthy, 2001). To our knowledge this is the first work to obtain German verb classes automatically. We used a robust statistical parser (Schmid, 2000) to acquire purely syntactic subcategorisation information for verbs. The information was provided in form of probability distributions over verb frames for each verb. There were two conditions: the first with relatively coarse syntactic verb subcategorisation frames, the second a more delicate classification subdividing the verb frames of the first condition using prepositional phrase information (case plus preposition). In both conditions v"
P02-1029,schulte-im-walde-2002-subcategorisation,1,0.716294,"long term goal is to support the development of high-quality and large-scale lexical resources. 2 Syntactic Descriptors for Verb Frames The syntactic subcategorisation frames for German verbs were obtained by unsupervised learning in a statistical grammar framework (Schulte im Walde et al., 2001): a German context-free grammar containing frame-predicting grammar rules and information about lexical heads was trained on 25 million words of a large German newspaper corpus. The lexicalised version of the probabilistic grammar served as source for syntactic descriptors for verb frames (Schulte im Walde, 2002b). The verb frame types contain at most three arguments. Possible arguments in the frames are nominative (n), dative (d) and accusative (a) noun phrases, reflexive pronouns (r), prepositional phrases (p), expletive es (x), non-finite clauses (i), finite clauses (s-2 for verb second clauses, s-dass for dass-clauses, s-ob for ob-clauses, s-w for indirect wh-questions), and copula constructions (k). For example, subcategorising a direct (accusative case) object and a non-finite clause would be represented by nai. We defined a total of 38 subcategorisation frame types, according to the verb subca"
P02-1029,E99-1007,0,0.2346,"heir syntactic properties. From a practical point of view, a verb classification supports Natural Language Processing tasks, since it provides a principled basis for filling gaps in available lexical knowledge. For example, the English verb classification has been used for applications such as machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998). Various attempts have been made to infer conveniently observable morpho-syntactic and semantic properties for English verb classes (Dorr and Jones, 1996; Lapata, 1999; Stevenson and Merlo, 1999; Schulte im Walde, 2000; McCarthy, 2001). To our knowledge this is the first work to obtain German verb classes automatically. We used a robust statistical parser (Schmid, 2000) to acquire purely syntactic subcategorisation information for verbs. The information was provided in form of probability distributions over verb frames for each verb. There were two conditions: the first with relatively coarse syntactic verb subcategorisation frames, the second a more delicate classification subdividing the verb frames of the first condition using prepositional phrase information (case plus prepositio"
P02-1029,C00-2094,0,0.0429675,"ns for generalising over these objects. In our case, clustering is realised on verbs: the data objects are represented by verbs, and the data features for describing the objects are realised by a probability distribution over syntactic verb frame descriptions. Clustering is applicable to a variety of areas in Natural Language Processing, e.g. by utilising class type descriptions such as in machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998), or by applying clusters for smoothing such as in machine translation (Prescher et al., 2000), or probabilistic grammars (Riezler et al., 2000). We performed clustering by the k-Means algorithm as proposed by (Forgy, 1965), which is an unsupervised hard clustering method assigning data  objects to exactly clusters. Initial verb clusters are iteratively re-organised by assigning each verb to its closest cluster (centroid) and re-calculating cluster centroids until no further changes take place. One parameter of the clustering process is the distance measure used. Standard choices include the cosine, Euclidean distance, Manhattan metric, and variants of the Kullback-Leibler (KL) diverg"
P02-1029,M95-1005,0,0.00967065,"od related to hierarchical clustering (Schulte im Walde, 2000). 5 Clustering Evaluation The task of evaluating the result of a cluster analysis against the known gold standard of hand-constructed verb classes requires us to assess the similarity between two sets of equivalence relations. As noted by (Strehl et al., 2000), it is useful to have an evaluation measure that does not depend on the choice of similarity measure or on the original dimensionality of the input data, since that allows meaningful comparison of results for which these parameters vary. This is similar to the perspective of (Vilain et al., 1995), who present, in the context of the MUC co-reference evaluation scheme, a model-theoretic measure of the similarity between equivalence classes. Strehl et al. consider a clustering that partitions   objects (  ) into clusters;  the  clusters  of are the sets for which     . 1 We also tried various transformations and variations of the probabilities, such as frequencies and binarisation, but none proved as effective as the probabilities. We call the cluster result  and the desired goldstandard  . For measuring the quality of an indi-  vidual cluster, the cluster"
P02-1029,P00-1061,0,0.0111788,", clustering is realised on verbs: the data objects are represented by verbs, and the data features for describing the objects are realised by a probability distribution over syntactic verb frame descriptions. Clustering is applicable to a variety of areas in Natural Language Processing, e.g. by utilising class type descriptions such as in machine translation (Dorr, 1997), word sense disambiguation (Dorr and Jones, 1996), and document classification (Klavans and Kan, 1998), or by applying clusters for smoothing such as in machine translation (Prescher et al., 2000), or probabilistic grammars (Riezler et al., 2000). We performed clustering by the k-Means algorithm as proposed by (Forgy, 1965), which is an unsupervised hard clustering method assigning data  objects to exactly clusters. Initial verb clusters are iteratively re-organised by assigning each verb to its closest cluster (centroid) and re-calculating cluster centroids until no further changes take place. One parameter of the clustering process is the distance measure used. Standard choices include the cosine, Euclidean distance, Manhattan metric, and variants of the Kullback-Leibler (KL) divergence. We concentrated on two variants of KL in Equ"
P02-1029,C98-1108,0,\N,Missing
P08-1057,E03-1034,0,0.499721,"clustering approach that previously incorporated selectional preferences as verb features. However, her model was not soft-clustering, and she only used a simple approach to represent selectional preferences by WordNet’s top-level concepts, instead of making use of the whole hierarchy and more sophisticated methods, as in the current paper. Last but not least, there are other models of selectional preferences than the MDL model we used in our paper. Most such models also rely on the 503 WordNet hierarchy (Resnik, 1997; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002). Brockmann and Lapata (2003) compared some of the models against human judgements on the acceptability of sentences, and demonstrated that the models were significantly correlated with human ratings, and that no model performed best; rather, the different methods are suited for different argument relations. 5 Summary and Outlook This paper presented an innovative, complex approach to semantic verb classes that relies on selectional preferences as verb properties. The probabilistic verb class model underlying the semantic classes was trained by a combination of the EM algorithm and the MDL principle, providing soft cluste"
P08-1057,W98-1505,0,0.236617,"Missing"
P08-1057,C00-1028,0,0.558881,"knowledge, Schulte im Walde (2006) is the only hard-clustering approach that previously incorporated selectional preferences as verb features. However, her model was not soft-clustering, and she only used a simple approach to represent selectional preferences by WordNet’s top-level concepts, instead of making use of the whole hierarchy and more sophisticated methods, as in the current paper. Last but not least, there are other models of selectional preferences than the MDL model we used in our paper. Most such models also rely on the 503 WordNet hierarchy (Resnik, 1997; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002). Brockmann and Lapata (2003) compared some of the models against human judgements on the acceptability of sentences, and demonstrated that the models were significantly correlated with human ratings, and that no model performed best; rather, the different methods are suited for different argument relations. 5 Summary and Outlook This paper presented an innovative, complex approach to semantic verb classes that relies on selectional preferences as verb properties. The probabilistic verb class model underlying the semantic classes was trained by a combination of the EM al"
P08-1057,J02-2003,0,0.262808,"2006) is the only hard-clustering approach that previously incorporated selectional preferences as verb features. However, her model was not soft-clustering, and she only used a simple approach to represent selectional preferences by WordNet’s top-level concepts, instead of making use of the whole hierarchy and more sophisticated methods, as in the current paper. Last but not least, there are other models of selectional preferences than the MDL model we used in our paper. Most such models also rely on the 503 WordNet hierarchy (Resnik, 1997; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002). Brockmann and Lapata (2003) compared some of the models against human judgements on the acceptability of sentences, and demonstrated that the models were significantly correlated with human ratings, and that no model performed best; rather, the different methods are suited for different argument relations. 5 Summary and Outlook This paper presented an innovative, complex approach to semantic verb classes that relies on selectional preferences as verb properties. The probabilistic verb class model underlying the semantic classes was trained by a combination of the EM algorithm and the MDL pri"
P08-1057,C96-1055,0,0.0284114,"s that generalise over verbs according to their semantic properties. Intuitive examples of such classifications are the M OTION WITH A V EHICLE class, including verbs such as drive, fly, row, etc., or the B REAK A S OLID S URFACE WITH AN I NSTRUMENT class, including verbs such as break, crush, fracture, smash, etc. Semantic verb classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Up to now, such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007), document classification (Klavans and Kan, 1998), and in statistical lexical acquisition in general (Rooth et al., 1999; Merlo and Stevenson, 2001; Korhonen, 2002; Schulte im Walde, 2006). Given that the creation of semantic verb classifications is not an end task in itself, but depends on the application scenario of the classification, we find various approaches to an automatic induction of semantic verb classifications. For example, Siegel and McKeown (2000) used several machine learning algorithms"
P08-1057,P98-1112,0,0.0215797,"CLE class, including verbs such as drive, fly, row, etc., or the B REAK A S OLID S URFACE WITH AN I NSTRUMENT class, including verbs such as break, crush, fracture, smash, etc. Semantic verb classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Up to now, such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007), document classification (Klavans and Kan, 1998), and in statistical lexical acquisition in general (Rooth et al., 1999; Merlo and Stevenson, 2001; Korhonen, 2002; Schulte im Walde, 2006). Given that the creation of semantic verb classifications is not an end task in itself, but depends on the application scenario of the classification, we find various approaches to an automatic induction of semantic verb classifications. For example, Siegel and McKeown (2000) used several machine learning algorithms to perform an automatic aspectual classification of English verbs into event and stative verbs. Merlo and Stevenson (2001) presented an automa"
P08-1057,D07-1091,0,0.0101779,"such classifications are the M OTION WITH A V EHICLE class, including verbs such as drive, fly, row, etc., or the B REAK A S OLID S URFACE WITH AN I NSTRUMENT class, including verbs such as break, crush, fracture, smash, etc. Semantic verb classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Up to now, such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007), document classification (Klavans and Kan, 1998), and in statistical lexical acquisition in general (Rooth et al., 1999; Merlo and Stevenson, 2001; Korhonen, 2002; Schulte im Walde, 2006). Given that the creation of semantic verb classifications is not an end task in itself, but depends on the application scenario of the classification, we find various approaches to an automatic induction of semantic verb classifications. For example, Siegel and McKeown (2000) used several machine learning algorithms to perform an automatic aspectual classification of English verbs into event and stative verb"
P08-1057,P05-1005,0,0.022284,"verbs according to their semantic properties. Intuitive examples of such classifications are the M OTION WITH A V EHICLE class, including verbs such as drive, fly, row, etc., or the B REAK A S OLID S URFACE WITH AN I NSTRUMENT class, including verbs such as break, crush, fracture, smash, etc. Semantic verb classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Up to now, such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007), document classification (Klavans and Kan, 1998), and in statistical lexical acquisition in general (Rooth et al., 1999; Merlo and Stevenson, 2001; Korhonen, 2002; Schulte im Walde, 2006). Given that the creation of semantic verb classifications is not an end task in itself, but depends on the application scenario of the classification, we find various approaches to an automatic induction of semantic verb classifications. For example, Siegel and McKeown (2000) used several machine learning algorithms to perform an automatic a"
P08-1057,P03-1009,0,0.437238,"oaches to an automatic induction of semantic verb classifications. For example, Siegel and McKeown (2000) used several machine learning algorithms to perform an automatic aspectual classification of English verbs into event and stative verbs. Merlo and Stevenson (2001) presented an automatic classification of three types of English intransitive verbs, based on argument structure and heuristics to thematic relations. Pereira et al. (1993) and Rooth et al. (1999) relied on the ExpectationMaximisation algorithm to induce soft clusters of verbs, based on the verbs’ direct object nouns. Similarly, Korhonen et al. (2003) relied on the Information Bottleneck (Tishby et al., 1999) and subcategorisation frame types to induce soft verb clusters. This paper presents an innovative, complex approach to semantic verb classes that relies on selectional preferences as verb properties. The underlying linguistic assumption for this verb class model is that verbs which agree on their selectional preferences belong to a common semantic class. The model is implemented as a softclustering approach, in order to capture the polysemy of the verbs. The training procedure uses the Expectation-Maximisation (EM) algorithm (Baum, 19"
P08-1057,J98-2002,0,0.35937,"e data length, with the model length defined as the number of bits needed to encode the model and its parameters, and the data length defined as the number of bits required to encode the training data with the given model. According to coding theory, an optimal encoding uses −log2 p bits, on average, to encode data whose probability is p. Usually, the model length increases and the data length decreases as more parameters are added to a model. The MDL principle finds a compromise between the size of the model and the accuracy of the data description. Our selectional preference model relies on Li and Abe (1998), applying the MDL principle to determine selectional preferences of verbs and their arguments, by means of a concept hierarchy ordered by hypernym/hyponym relations. Given a set of nouns within a specific argument slot as a sample, the approach finds the cut3 in a concept hierarchy which minimises the sum of encoding both the model and the data. The model length (ML) is defined as k ∗ log2 |S|, ML = 2 with k the number of concepts in the partial hierarchy between the top concept and the concepts in the cut, and |S |the sample size, i.e., the total frequency of the data set. The data length (D"
P08-1057,P93-1024,0,0.94928,"e, 2006). Given that the creation of semantic verb classifications is not an end task in itself, but depends on the application scenario of the classification, we find various approaches to an automatic induction of semantic verb classifications. For example, Siegel and McKeown (2000) used several machine learning algorithms to perform an automatic aspectual classification of English verbs into event and stative verbs. Merlo and Stevenson (2001) presented an automatic classification of three types of English intransitive verbs, based on argument structure and heuristics to thematic relations. Pereira et al. (1993) and Rooth et al. (1999) relied on the ExpectationMaximisation algorithm to induce soft clusters of verbs, based on the verbs’ direct object nouns. Similarly, Korhonen et al. (2003) relied on the Information Bottleneck (Tishby et al., 1999) and subcategorisation frame types to induce soft verb clusters. This paper presents an innovative, complex approach to semantic verb classes that relies on selectional preferences as verb properties. The underlying linguistic assumption for this verb class model is that verbs which agree on their selectional preferences belong to a common semantic class. Th"
P08-1057,C00-2094,0,0.0238335,"Intuitive examples of such classifications are the M OTION WITH A V EHICLE class, including verbs such as drive, fly, row, etc., or the B REAK A S OLID S URFACE WITH AN I NSTRUMENT class, including verbs such as break, crush, fracture, smash, etc. Semantic verb classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Up to now, such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007), document classification (Klavans and Kan, 1998), and in statistical lexical acquisition in general (Rooth et al., 1999; Merlo and Stevenson, 2001; Korhonen, 2002; Schulte im Walde, 2006). Given that the creation of semantic verb classifications is not an end task in itself, but depends on the application scenario of the classification, we find various approaches to an automatic induction of semantic verb classifications. For example, Siegel and McKeown (2000) used several machine learning algorithms to perform an automatic aspectual classification of English verbs int"
P08-1057,W97-0209,0,0.237752,"ation to animacy. To the best of our knowledge, Schulte im Walde (2006) is the only hard-clustering approach that previously incorporated selectional preferences as verb features. However, her model was not soft-clustering, and she only used a simple approach to represent selectional preferences by WordNet’s top-level concepts, instead of making use of the whole hierarchy and more sophisticated methods, as in the current paper. Last but not least, there are other models of selectional preferences than the MDL model we used in our paper. Most such models also rely on the 503 WordNet hierarchy (Resnik, 1997; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002). Brockmann and Lapata (2003) compared some of the models against human judgements on the acceptability of sentences, and demonstrated that the models were significantly correlated with human ratings, and that no model performed best; rather, the different methods are suited for different argument relations. 5 Summary and Outlook This paper presented an innovative, complex approach to semantic verb classes that relies on selectional preferences as verb properties. The probabilistic verb class model underlying the semant"
P08-1057,P99-1014,0,0.788603,"OLID S URFACE WITH AN I NSTRUMENT class, including verbs such as break, crush, fracture, smash, etc. Semantic verb classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Up to now, such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007), document classification (Klavans and Kan, 1998), and in statistical lexical acquisition in general (Rooth et al., 1999; Merlo and Stevenson, 2001; Korhonen, 2002; Schulte im Walde, 2006). Given that the creation of semantic verb classifications is not an end task in itself, but depends on the application scenario of the classification, we find various approaches to an automatic induction of semantic verb classifications. For example, Siegel and McKeown (2000) used several machine learning algorithms to perform an automatic aspectual classification of English verbs into event and stative verbs. Merlo and Stevenson (2001) presented an automatic classification of three types of English intransitive verbs, based"
P08-1057,J06-2001,1,0.94211,"rush, fracture, smash, etc. Semantic verb classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Up to now, such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007), document classification (Klavans and Kan, 1998), and in statistical lexical acquisition in general (Rooth et al., 1999; Merlo and Stevenson, 2001; Korhonen, 2002; Schulte im Walde, 2006). Given that the creation of semantic verb classifications is not an end task in itself, but depends on the application scenario of the classification, we find various approaches to an automatic induction of semantic verb classifications. For example, Siegel and McKeown (2000) used several machine learning algorithms to perform an automatic aspectual classification of English verbs into event and stative verbs. Merlo and Stevenson (2001) presented an automatic classification of three types of English intransitive verbs, based on argument structure and heuristics to thematic relations. Pereira"
P08-1057,J00-4004,0,0.00912264,"pplications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007), document classification (Klavans and Kan, 1998), and in statistical lexical acquisition in general (Rooth et al., 1999; Merlo and Stevenson, 2001; Korhonen, 2002; Schulte im Walde, 2006). Given that the creation of semantic verb classifications is not an end task in itself, but depends on the application scenario of the classification, we find various approaches to an automatic induction of semantic verb classifications. For example, Siegel and McKeown (2000) used several machine learning algorithms to perform an automatic aspectual classification of English verbs into event and stative verbs. Merlo and Stevenson (2001) presented an automatic classification of three types of English intransitive verbs, based on argument structure and heuristics to thematic relations. Pereira et al. (1993) and Rooth et al. (1999) relied on the ExpectationMaximisation algorithm to induce soft clusters of verbs, based on the verbs’ direct object nouns. Similarly, Korhonen et al. (2003) relied on the Information Bottleneck (Tishby et al., 1999) and subcategorisation f"
P08-1057,D07-1107,0,0.0451065,"Missing"
P08-1057,J01-3003,0,\N,Missing
P13-1058,2012.eamt-1.6,0,0.155104,"Missing"
P13-1058,C10-1011,0,0.0114686,"is identified, and via the alignment, the equivalent German verb is obtained. Similarly, candidates for noun-nounGen structures are identified by extracting and aligning English noun-of-noun phrases. 5 We train four CRFs on data prepared as shown in section 3. The corpora used for the extraction of subcategorization tuples were Europarl and German newspaper data (200 million words). We choose this particular data combination in order to provide data that matches the training data, as well as to add new data of the test set’s domain (news). The German part of Europarl was dependencyparsed with Bohnet (2010), and subcategorization information was extracted as described in Scheible et al. (2013); the newspaper data (HGC - Huge German Corpus) was parsed with Schmid (2000), and subcategorization information was extracted as described in Schulte im Walde (2002b). 5.2 We report results of two types of systems (table 5): first, a regular translation system built on surface forms (i.e., normal text) and second, four inflection prediction systems. The first inflection prediction system (1) uses a simple case prediction model, whereas the remaining systems are enriched with (2) subcategorization informati"
P13-1058,A97-1052,0,0.219587,"t gloss influence the political stability Table 2: Overview of the inflection process: the stem markup is highlighted in the SMT output. discourse. On the one hand, this has led to a range of manually or semi-automatically developed lexical resources focusing on verb information, such as the Levin classes (Levin, 1993), VerbNet (Kipper Schuler, 2006), FrameNet4 (Fillmore et al., 2003), and PropBank (Palmer et al., 2005). On the other hand, we find automatic approaches to the induction of verb subcategorization information at the syntax-semantics interface for a large number of languages, e.g. Briscoe and Carroll (1997) for English; Sarkar and Zeman (2000) for Czech; Schulte im Walde (2002a) for German; Messiant (2008) for French. This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction (Surdeanu et al., 2003; Venturi1 et al., 2009), parsing (Carroll et al., 1998; Carroll and Fang, 2004) and word sense disambiguation (Kohomban and Lee, 2005; McCarthy et al., 2007). 4.1 EP HGC Both V-SUBJ 454,350 712,717 1,089,492 V-OBJAcc 332,847 329,830 607,541 V-OBJDat 53,711 160,377 206,764 Table 3: Number of verb-noun types extracted from Europarl (EP) and newspaper d"
P13-1058,2012.eamt-1.35,0,0.0170999,"l. (2012) has access to lexical information, it is limited to a certain window size and has no direct information about the relation of verb–noun pairs occurring in the sentence. Using a window of a limited size is particularly problematic for German, as there can be large gaps between the verb and its subcategorized nouns; introducing information about the relation of verbs and nouns helps to bridge such gaps. Furthermore, that model was not able to make effective use of source-side features. One of the objectives of using an inflection prediction model is morphologically well-formed output. Kirchhoff et al. (2012) evaluated user reactions to different error types in machine translation and came to the result that morphological 3.1 Translation pipeline Stemmed representation/feature markup We first parse the German side of the parallel training data with BitPar (Schmid, 2004). This maps each surface form appearing in normal text to a stem and morphological features (case, gender, number). We use this representation to create the stemmed representation for training the translation model. With the exception of stem-markup (discussed below), all morphological features are removed from the stemmed represent"
P13-1058,P05-1005,0,0.0268511,"3), and PropBank (Palmer et al., 2005). On the other hand, we find automatic approaches to the induction of verb subcategorization information at the syntax-semantics interface for a large number of languages, e.g. Briscoe and Carroll (1997) for English; Sarkar and Zeman (2000) for Czech; Schulte im Walde (2002a) for German; Messiant (2008) for French. This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction (Surdeanu et al., 2003; Venturi1 et al., 2009), parsing (Carroll et al., 1998; Carroll and Fang, 2004) and word sense disambiguation (Kohomban and Lee, 2005; McCarthy et al., 2007). 4.1 EP HGC Both V-SUBJ 454,350 712,717 1,089,492 V-OBJAcc 332,847 329,830 607,541 V-OBJDat 53,711 160,377 206,764 Table 3: Number of verb-noun types extracted from Europarl (EP) and newspaper data (HGC). that modify nouns (noun–noun modification case prediction). Typically, these NP modifiers are genitive NPs. To this end, we integrate nounnounGen tuples with their respective frequencies. These preferences for a certain function (i.e. subject, object or modifier) are passed on to the system at the level of nouns and integrated into the CRF through the derived probabil"
P13-1058,W98-1114,0,0.089596,"Levin, 1993), VerbNet (Kipper Schuler, 2006), FrameNet4 (Fillmore et al., 2003), and PropBank (Palmer et al., 2005). On the other hand, we find automatic approaches to the induction of verb subcategorization information at the syntax-semantics interface for a large number of languages, e.g. Briscoe and Carroll (1997) for English; Sarkar and Zeman (2000) for Czech; Schulte im Walde (2002a) for German; Messiant (2008) for French. This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction (Surdeanu et al., 2003; Venturi1 et al., 2009), parsing (Carroll et al., 1998; Carroll and Fang, 2004) and word sense disambiguation (Kohomban and Lee, 2005; McCarthy et al., 2007). 4.1 EP HGC Both V-SUBJ 454,350 712,717 1,089,492 V-OBJAcc 332,847 329,830 607,541 V-OBJDat 53,711 160,377 206,764 Table 3: Number of verb-noun types extracted from Europarl (EP) and newspaper data (HGC). that modify nouns (noun–noun modification case prediction). Typically, these NP modifiers are genitive NPs. To this end, we integrate nounnounGen tuples with their respective frequencies. These preferences for a certain function (i.e. subject, object or modifier) are passed on to the system"
P13-1058,P10-1052,0,0.0584832,"Missing"
P13-1058,C10-1081,0,0.048035,"Missing"
P13-1058,E12-1068,1,0.642165,"3 Previous work Previous work has already introduced the idea of generating inflected forms as a post-processing step for a translation system that has been stripped of (most) target-language-specific features. Toutanova et al. (2008) and Jeong et al. (2010) built translation systems that predict inflected word forms based on a large array of morphological and syntactic features, obtained from both source and target side. Kholy and Habash (2012) and Green and DeNero (2012) work on English to Arabic translation and model gender, number and definiteness, focusing primarily on improving fluency. Fraser et al. (2012) used a phrase-based system to transfer stems and generated inflected forms based on the stems and their morphological features. For case prediction, they trained a CRF with access to lemmas and POS-tags within a given window. We re-implemented the system by Fraser et al. as a hierarchical machine translation system using a string-to-tree setup. In contrast to the flat phrase-based setting of Fraser et al. (2012), syntactic trees on the SMT output allow us to work with verb–noun structures, which are relevant for case prediction. While the CRF used for case prediction in Fraser et al. (2012) h"
P13-1058,J07-4005,0,0.0160654,"et al., 2005). On the other hand, we find automatic approaches to the induction of verb subcategorization information at the syntax-semantics interface for a large number of languages, e.g. Briscoe and Carroll (1997) for English; Sarkar and Zeman (2000) for Czech; Schulte im Walde (2002a) for German; Messiant (2008) for French. This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction (Surdeanu et al., 2003; Venturi1 et al., 2009), parsing (Carroll et al., 1998; Carroll and Fang, 2004) and word sense disambiguation (Kohomban and Lee, 2005; McCarthy et al., 2007). 4.1 EP HGC Both V-SUBJ 454,350 712,717 1,089,492 V-OBJAcc 332,847 329,830 607,541 V-OBJDat 53,711 160,377 206,764 Table 3: Number of verb-noun types extracted from Europarl (EP) and newspaper data (HGC). that modify nouns (noun–noun modification case prediction). Typically, these NP modifiers are genitive NPs. To this end, we integrate nounnounGen tuples with their respective frequencies. These preferences for a certain function (i.e. subject, object or modifier) are passed on to the system at the level of nouns and integrated into the CRF through the derived probabilities. The tuples and tr"
P13-1058,P08-3010,0,0.0138294,"ghted in the SMT output. discourse. On the one hand, this has led to a range of manually or semi-automatically developed lexical resources focusing on verb information, such as the Levin classes (Levin, 1993), VerbNet (Kipper Schuler, 2006), FrameNet4 (Fillmore et al., 2003), and PropBank (Palmer et al., 2005). On the other hand, we find automatic approaches to the induction of verb subcategorization information at the syntax-semantics interface for a large number of languages, e.g. Briscoe and Carroll (1997) for English; Sarkar and Zeman (2000) for Czech; Schulte im Walde (2002a) for German; Messiant (2008) for French. This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction (Surdeanu et al., 2003; Venturi1 et al., 2009), parsing (Carroll et al., 1998; Carroll and Fang, 2004) and word sense disambiguation (Kohomban and Lee, 2005; McCarthy et al., 2007). 4.1 EP HGC Both V-SUBJ 454,350 712,717 1,089,492 V-OBJAcc 332,847 329,830 607,541 V-OBJDat 53,711 160,377 206,764 Table 3: Number of verb-noun types extracted from Europarl (EP) and newspaper data (HGC). that modify nouns (noun–noun modification case prediction). Typically, these NP modifiers"
P13-1058,N04-1035,0,0.0744853,"data and annotating it with the respective features, and then adding this new data set to the original training data. As this method comes with its own problems, such as transferring the morphological annotation to not necessarily isomorphically translated text, we do not use translated data as part of the training data. Instead, we limit the power of the CRF model through experimenting with the removal of features, until we had a system that was robust to this problem. We use a hierarchical translation system. Instead of translating phrases, a hierarchical system extracts translation rules (Galley et al., 2004) which allow the decoder to provide a tree spanning over the translated sentence. In order to avoid sparsity during rule extraction, we use a string-to-tree setup, where only the target-side part of the data is parsed. Translation rules are of the following form: [X]1 allows [X]2 [X]1 allows [X]2 → [NP]1 [NP]2 erlaubt → [NP]1 erlaubt [NP]2 This example illustrates how rules can cover the different word ordering possibilities in German. PP nodes are annotated with their respective case, as well as with the lemma of the preposition they contain. In our experiments, this enriched annotation has s"
P13-1058,P12-1016,0,0.0289141,"ubsections 3.1 to 3.4, we present the simple version of the inflection prediction system; our new features are described in sections 4.2 and 4.3. 3 Previous work Previous work has already introduced the idea of generating inflected forms as a post-processing step for a translation system that has been stripped of (most) target-language-specific features. Toutanova et al. (2008) and Jeong et al. (2010) built translation systems that predict inflected word forms based on a large array of morphological and syntactic features, obtained from both source and target side. Kholy and Habash (2012) and Green and DeNero (2012) work on English to Arabic translation and model gender, number and definiteness, focusing primarily on improving fluency. Fraser et al. (2012) used a phrase-based system to transfer stems and generated inflected forms based on the stems and their morphological features. For case prediction, they trained a CRF with access to lemmas and POS-tags within a given window. We re-implemented the system by Fraser et al. as a hierarchical machine translation system using a string-to-tree setup. In contrast to the flat phrase-based setting of Fraser et al. (2012), syntactic trees on the SMT output allow"
P13-1058,J05-1004,0,0.0242722,"SMT output beeinflussen<VVFIN> d<ART> politisch<ADJ> Stabilit¨ at<NN><Fem><Sg> predicted features – Fem.Acc.Sg.St Fem.Acc.Sg.Wk Fem.Acc.Sg.Wk inflected forms beeinflussen die politische Stabilit¨at gloss influence the political stability Table 2: Overview of the inflection process: the stem markup is highlighted in the SMT output. discourse. On the one hand, this has led to a range of manually or semi-automatically developed lexical resources focusing on verb information, such as the Levin classes (Levin, 1993), VerbNet (Kipper Schuler, 2006), FrameNet4 (Fillmore et al., 2003), and PropBank (Palmer et al., 2005). On the other hand, we find automatic approaches to the induction of verb subcategorization information at the syntax-semantics interface for a large number of languages, e.g. Briscoe and Carroll (1997) for English; Sarkar and Zeman (2000) for Czech; Schulte im Walde (2002a) for German; Messiant (2008) for French. This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction (Surdeanu et al., 2003; Venturi1 et al., 2009), parsing (Carroll et al., 1998; Carroll and Fang, 2004) and word sense disambiguation (Kohomban and Lee, 2005; McCarthy et al"
P13-1058,N06-1031,0,0.0140266,"ows [X]2 → [NP]1 [NP]2 erlaubt → [NP]1 erlaubt [NP]2 This example illustrates how rules can cover the different word ordering possibilities in German. PP nodes are annotated with their respective case, as well as with the lemma of the preposition they contain. In our experiments, this enriched annotation has small improvements over the simpler setting with only head categories (details omitted). This outcome, in particular that adding the lemma of the preposition to the PP node helps to improve translation quality, has been observed before in tree restructuring work for improving translation (Huang and Knight, 2006). 3.3 Feature prediction and generation of inflected forms In this section we discuss our focus, which is prediction of case, but also the prediction of number, gender and strong/weak adjectival inflection. The latter feature is German-specific; its values2 (strong/weak) depend on the combination of the other features, as well as on the type of determiner (e.g. definite/indefinite/none). Morphological features are predicted on four separate CRF models, one for each feature. The models for case, number and gender are independent of another, whereas the model for adjectival inflection requires i"
P13-1058,2001.mtsummit-papers.68,0,0.0404221,"tion (cf. section 4.2), (3) source-side features (cf. section 4.3), and (4) both source-side features and subcategorization information. In (2) and (4), the subcategorization information was included using tuples obtained from source-side dependencies11 . The simple prediction system corresponds to that presented in section 3; for all inflection prediction systems, the same SMT output and models for number, gender and strong/weak inflection were used; thus the only difference with the simple prediction system is the model for case prediction. We present three types of evaluation: BLEU scores (Papineni et al., 2001), prediction accuracy on clean data and a manual evaluation of the best system in section 5.3. Table 5 gives results in case-insensitive BLEU. While the inflection prediction systems (1-4) are significantly12 better than the surface-form system (0), the different versions of the inflection systems are not distinguishable in terms of BLEU; however, our manual evaluation shows that the new features have a positive impact on translation quality. Experiments and evaluation In this section, we present experiments using different feature combinations. We also present a manual evaluation of our best"
P13-1058,2010.amta-papers.33,0,0.0775772,"In the first step, English input is translated to German stems. In the second step, morphological features are predicted and inflected forms are generated based on the word stems and the morphological features. In subsections 3.1 to 3.4, we present the simple version of the inflection prediction system; our new features are described in sections 4.2 and 4.3. 3 Previous work Previous work has already introduced the idea of generating inflected forms as a post-processing step for a translation system that has been stripped of (most) target-language-specific features. Toutanova et al. (2008) and Jeong et al. (2010) built translation systems that predict inflected word forms based on a large array of morphological and syntactic features, obtained from both source and target side. Kholy and Habash (2012) and Green and DeNero (2012) work on English to Arabic translation and model gender, number and definiteness, focusing primarily on improving fluency. Fraser et al. (2012) used a phrase-based system to transfer stems and generated inflected forms based on the stems and their morphological features. For case prediction, they trained a CRF with access to lemmas and POS-tags within a given window. We re-imple"
P13-1058,C00-2100,0,0.0661059,"y Table 2: Overview of the inflection process: the stem markup is highlighted in the SMT output. discourse. On the one hand, this has led to a range of manually or semi-automatically developed lexical resources focusing on verb information, such as the Levin classes (Levin, 1993), VerbNet (Kipper Schuler, 2006), FrameNet4 (Fillmore et al., 2003), and PropBank (Palmer et al., 2005). On the other hand, we find automatic approaches to the induction of verb subcategorization information at the syntax-semantics interface for a large number of languages, e.g. Briscoe and Carroll (1997) for English; Sarkar and Zeman (2000) for Czech; Schulte im Walde (2002a) for German; Messiant (2008) for French. This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction (Surdeanu et al., 2003; Venturi1 et al., 2009), parsing (Carroll et al., 1998; Carroll and Fang, 2004) and word sense disambiguation (Kohomban and Lee, 2005; McCarthy et al., 2007). 4.1 EP HGC Both V-SUBJ 454,350 712,717 1,089,492 V-OBJAcc 332,847 329,830 607,541 V-OBJDat 53,711 160,377 206,764 Table 3: Number of verb-noun types extracted from Europarl (EP) and newspaper data (HGC). that modify nouns (noun–no"
P13-1058,2009.eamt-1.30,0,0.049735,"Missing"
P13-1058,schmid-etal-2004-smor,0,0.0401568,"the markup (number and gender on nouns) in the stemmed output of the SMT system is part of the input to the respective feature prediction. For gender and number, the values given on the stems of the nouns are then propagated over the phrase. While the case of prepositional phrases is determined by the case annotation on prepositions, the case of nominal phrases is computed only based on the respective contexts. After predicting all morphological features, the information required to generate inflected forms is complete: based on the stems and the features, we use the morphological tool SMOR (Schmid et al., 2004) for the generation of inflected forms. One general problem with feature-prediction is that the ill-formed SMT output is not well represented by the training data which consists of wellformed sentences. This problem was also mentioned by Stymne and Cancedda (2011) and Kholy and Habash (2012). They deal with this problem by translating the training data and annotating it with the respective features, and then adding this new data set to the original training data. As this method comes with its own problems, such as transferring the morphological annotation to not necessarily isomorphically tran"
P13-1058,N09-2004,0,0.100737,"Missing"
P13-1058,C04-1024,0,0.0332844,"ps between the verb and its subcategorized nouns; introducing information about the relation of verbs and nouns helps to bridge such gaps. Furthermore, that model was not able to make effective use of source-side features. One of the objectives of using an inflection prediction model is morphologically well-formed output. Kirchhoff et al. (2012) evaluated user reactions to different error types in machine translation and came to the result that morphological 3.1 Translation pipeline Stemmed representation/feature markup We first parse the German side of the parallel training data with BitPar (Schmid, 2004). This maps each surface form appearing in normal text to a stem and morphological features (case, gender, number). We use this representation to create the stemmed representation for training the translation model. With the exception of stem-markup (discussed below), all morphological features are removed from the stemmed representation. The stem markup is used as part of the input to the feature prediction; the basic idea is that the given feature values are picked up by the prediction model and then propagated over the phrase. Nouns, as the head of NPs and PPs, are annotated with gender and"
P13-1058,schulte-im-walde-2002-subcategorisation,1,0.776383,"the stem markup is highlighted in the SMT output. discourse. On the one hand, this has led to a range of manually or semi-automatically developed lexical resources focusing on verb information, such as the Levin classes (Levin, 1993), VerbNet (Kipper Schuler, 2006), FrameNet4 (Fillmore et al., 2003), and PropBank (Palmer et al., 2005). On the other hand, we find automatic approaches to the induction of verb subcategorization information at the syntax-semantics interface for a large number of languages, e.g. Briscoe and Carroll (1997) for English; Sarkar and Zeman (2000) for Czech; Schulte im Walde (2002a) for German; Messiant (2008) for French. This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction (Surdeanu et al., 2003; Venturi1 et al., 2009), parsing (Carroll et al., 1998; Carroll and Fang, 2004) and word sense disambiguation (Kohomban and Lee, 2005; McCarthy et al., 2007). 4.1 EP HGC Both V-SUBJ 454,350 712,717 1,089,492 V-OBJAcc 332,847 329,830 607,541 V-OBJDat 53,711 160,377 206,764 Table 3: Number of verb-noun types extracted from Europarl (EP) and newspaper data (HGC). that modify nouns (noun–noun modification case prediction)."
P13-1058,W11-2129,0,0.153639,"of prepositional phrases is determined by the case annotation on prepositions, the case of nominal phrases is computed only based on the respective contexts. After predicting all morphological features, the information required to generate inflected forms is complete: based on the stems and the features, we use the morphological tool SMOR (Schmid et al., 2004) for the generation of inflected forms. One general problem with feature-prediction is that the ill-formed SMT output is not well represented by the training data which consists of wellformed sentences. This problem was also mentioned by Stymne and Cancedda (2011) and Kholy and Habash (2012). They deal with this problem by translating the training data and annotating it with the respective features, and then adding this new data set to the original training data. As this method comes with its own problems, such as transferring the morphological annotation to not necessarily isomorphically translated text, we do not use translated data as part of the training data. Instead, we limit the power of the CRF model through experimenting with the removal of features, until we had a system that was robust to this problem. We use a hierarchical translation syste"
P13-1058,P03-1002,0,0.0301945,"ocusing on verb information, such as the Levin classes (Levin, 1993), VerbNet (Kipper Schuler, 2006), FrameNet4 (Fillmore et al., 2003), and PropBank (Palmer et al., 2005). On the other hand, we find automatic approaches to the induction of verb subcategorization information at the syntax-semantics interface for a large number of languages, e.g. Briscoe and Carroll (1997) for English; Sarkar and Zeman (2000) for Czech; Schulte im Walde (2002a) for German; Messiant (2008) for French. This basic kind of verb knowledge has been shown to be useful in many NLP tasks such as information extraction (Surdeanu et al., 2003; Venturi1 et al., 2009), parsing (Carroll et al., 1998; Carroll and Fang, 2004) and word sense disambiguation (Kohomban and Lee, 2005; McCarthy et al., 2007). 4.1 EP HGC Both V-SUBJ 454,350 712,717 1,089,492 V-OBJAcc 332,847 329,830 607,541 V-OBJDat 53,711 160,377 206,764 Table 3: Number of verb-noun types extracted from Europarl (EP) and newspaper data (HGC). that modify nouns (noun–noun modification case prediction). Typically, these NP modifiers are genitive NPs. To this end, we integrate nounnounGen tuples with their respective frequencies. These preferences for a certain function (i.e. s"
P13-1058,P08-1059,0,0.367113,"o-step translation process. In the first step, English input is translated to German stems. In the second step, morphological features are predicted and inflected forms are generated based on the word stems and the morphological features. In subsections 3.1 to 3.4, we present the simple version of the inflection prediction system; our new features are described in sections 4.2 and 4.3. 3 Previous work Previous work has already introduced the idea of generating inflected forms as a post-processing step for a translation system that has been stripped of (most) target-language-specific features. Toutanova et al. (2008) and Jeong et al. (2010) built translation systems that predict inflected word forms based on a large array of morphological and syntactic features, obtained from both source and target side. Kholy and Habash (2012) and Green and DeNero (2012) work on English to Arabic translation and model gender, number and definiteness, focusing primarily on improving fluency. Fraser et al. (2012) used a phrase-based system to transfer stems and generated inflected forms based on the stems and their morphological features. For case prediction, they trained a CRF with access to lemmas and POS-tags within a g"
P13-1058,P02-1040,0,\N,Missing
P13-1058,P11-2121,0,\N,Missing
P13-1058,W12-3150,0,\N,Missing
P13-1058,W08-0308,0,\N,Missing
P14-2086,J06-1005,0,0.25431,"524 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–530, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics based relation extraction to verbs, distinguishing five non-disjoint relations (similarity, strength, antonymy, enablement, happens-before). Pantel and Pennacchiotti (2006) developed Espresso, a weakly-supervised system that exploits patterns in large-scale web data to distinguish between five noun-noun relations (hypernymy, meronymy, succession, reaction, production). Similarly to Girju et al. (2006), they used generic patterns, but relied on a bootstrapping cycle combined with reliability measures, rather than manual resources. Whereas each of the aforementioned approaches considers only one word class and clearly disjoint categories, we distinguish between paradigmatic relations that can be distributionally very similar and propose a unified framework for nouns, verbs and adjectives. Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability"
P14-2086,W97-0802,0,0.0962232,"instances (cf. Yeh, 2000) revealed that only two differences in results are significant. We hypothesize that one reason for this outcome might be that both models cover complementary sets of instances. To verify this hypothesis, we apply a combined model, which is based on a weighted linear combination of distances computed by the two individual models.5 As displayed in Table 3, this combined model yields further improvements Development Set and Hyperparameters We select the hyperparameters of our model using an independent development set, which we extract from the lexical resource GermaNet (Hamp and Feldweg, 1997). For each considered word category, we extract instances of synonymy, antonymy and hypernymy. In total, 1502 instances are identified, with 64 of them overlapping with the evaluation data set described in Section 3. Note though that the development set is not used for evaluation but only to select the following hyperparameters. We experimented with different vector values (absolute frequency, log frequency, pointwise mutual information (PMI)), distance measures (cosine, euclidean) and normalization schemes. In contrast to S&K, who did not observe any improvements using PMI, we found it to per"
P14-2086,P13-2013,0,0.0208346,"les of discourse relations/markers. information to distinguish between paradigmatic relations. Our approach is motivated by linguistic studies that indicated a connection between discourse relations and lexical relations of words occurring in the respective discourse segments: Murphy et al. (2009) have shown, for example, that antonyms frequently serve as indicators for contrast relations in English and Swedish. More generally, pairs of word tokens have been identified as strong features for classifying discourse relations when no explicit discourse markers are available (Pitler et al., 2009; Biran and McKeown, 2013). Whereas word pairs have frequently been used as features for disambiguating discourse relations, to the best of our knowledge, our approach is novel in that we are the first to apply discourse relations as features for classifying lexical relations. One reason for this might be that discourse relations in general are only available in manually annotated corpora. Previous work has shown, however, that such relations can be classified reliably given the presence of explicit discourse markers.1 We hence rely on such markers as proxies for discourse relations (for examples, cf. Table 2). 4.1 Mod"
P14-2086,D13-1167,0,0.0316553,"Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) extended this approach to induce vector representations that can capture multiple relations. Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) proposed to distinguish between the three relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations i"
P14-2086,C92-2082,0,0.372261,"than manual resources. Whereas each of the aforementioned approaches considers only one word class and clearly disjoint categories, we distinguish between paradigmatic relations that can be distributionally very similar and propose a unified framework for nouns, verbs and adjectives. Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used pattern"
P14-2086,P04-1087,0,0.0350868,"acted from text. Each option comes with its own shortcomings: knowledge bases, on the one hand, are typically developed for a single language or domain, meaning that they might not generalize well; word patterns, on the other hand, are noisy and can be sparse for infrequent word pairs. In this paper, we propose to strike a balance between availability and restrictedness by making use of discourse markers. This approach has several advantages: markers are frequently found across genres (Webber, 2009), they exist in many languages (Jucker and Yiv, 1998), and capture various semantic properties (Hutchinson, 2004). We implement discourse markers within a vector space model that aims to distinguish between the three paradigmatic relations synonymy, antonymy and hypernymy in German and in English, across the three word classes of nouns, verbs, adjectives. We examine the performance of discourse markers as vector space dimensions in isolation and also explore their contribution in combination with lexical patterns. Distinguishing between paradigmatic relations such as synonymy, antonymy and hypernymy is an important prerequisite in a range of NLP applications. In this paper, we explore discourse relations"
P14-2086,W04-3205,0,0.0359404,"e relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Girju et al. (2003) applied a single pattern to distinguish pairs of nouns that are in a causal relationship from those that are not, and Girju et al. (2006) extended the work towards part–whole relations, applying a supervised, knowledge-intensive approach. Chklovski and Pantel (2004) were the first to apply pattern3 Baseline Model and Data Set The task addressed in this work is to distinguish between synonymy, antonymy and hypernymy. As a starting point, we build on the approach and data set used by Schulte im Walde and K¨oper (2013, henceforth just S&K). In their work, frequency statistics over automatically acquired co-occurrence patterns were found to be good indicators for the paradigmatic relation that holds between two given words of the same word class. They further experimented with refinements of the vector space model, for example, by only considering patterns o"
P14-2086,S12-1012,0,0.0231113,"y disjoint categories, we distinguish between paradigmatic relations that can be distributionally very similar and propose a unified framework for nouns, verbs and adjectives. Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘e"
P14-2086,P08-1118,0,0.173762,"Missing"
P14-2086,J02-2001,0,0.0169077,"erms of distributional similarity (Sch¨utze, 1992; Turney and Pantel, 2010), hence perform below their potential when inferring the type of relation that holds between two words. This distinction is crucial, however, in a range of tasks: in sentiment analysis, for example, words of the same and opposing polarity need to be distinguished; in textual entailment, systems further need to identify hypernymy because of directional inference requirements. 2 Related Work As mentioned above, there is a rich tradition of research on identifying a single paradigmatic relations. Work on synonyms includes Edmonds and Hirst (2002), who employed a co-occurrence network and second-order co-occurrence, and Curran (2003), who explored word-based and syntaxbased co-occurrence for thesaurus construction. 524 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–530, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics based relation extraction to verbs, distinguishing five non-disjoint relations (similarity, strength, antonymy, enablement, happens-before). Pantel and Pennacchiotti (2006) developed Espresso, a weakly-supervised"
P14-2086,D08-1103,0,0.158947,"identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) e"
P14-2086,D08-1094,0,0.0233038,"Missing"
P14-2086,J13-3004,0,0.106162,"on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) extended this approach to induce vector representations that can capture multiple relations. Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) pr"
P14-2086,N03-1011,0,0.0556205,"ge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) proposed to distinguish between the three relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Girju et al. (2003) applied a single pattern to distinguish pairs of nouns that are in a causal relationship from those that are not, and Girju et al. (2006) extended the work towards part–whole relations, applying a supervised, knowledge-intensive approach. Chklovski and Pantel (2004) were the first to apply pattern3 Baseline Model and Data Set The task addressed in this work is to distinguish between synonymy, antonymy and hypernymy. As a starting point, we build on the approach and data set used by Schulte im Walde and K¨oper (2013, henceforth just S&K). In their work, frequency statistics over automatically"
P14-2086,J06-3003,0,0.0247675,"ies to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) extended this approach to induce vector representations that can capture multiple relations. Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) proposed to distinguish between the three relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Girju et al. (2003) appli"
P14-2086,P06-1015,0,0.391085,"k on synonyms includes Edmonds and Hirst (2002), who employed a co-occurrence network and second-order co-occurrence, and Curran (2003), who explored word-based and syntaxbased co-occurrence for thesaurus construction. 524 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–530, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics based relation extraction to verbs, distinguishing five non-disjoint relations (similarity, strength, antonymy, enablement, happens-before). Pantel and Pennacchiotti (2006) developed Espresso, a weakly-supervised system that exploits patterns in large-scale web data to distinguish between five noun-noun relations (hypernymy, meronymy, succession, reaction, production). Similarly to Girju et al. (2006), they used generic patterns, but relied on a bootstrapping cycle combined with reliability measures, rather than manual resources. Whereas each of the aforementioned approaches considers only one word class and clearly disjoint categories, we distinguish between paradigmatic relations that can be distributionally very similar and propose a unified framework for nou"
P14-2086,P06-2111,0,0.0319444,"Missing"
P14-2086,P09-1076,0,0.125534,"tly relied on manually created knowledge sources, or lexico-syntactic patterns that can be automatically extracted from text. Each option comes with its own shortcomings: knowledge bases, on the one hand, are typically developed for a single language or domain, meaning that they might not generalize well; word patterns, on the other hand, are noisy and can be sparse for infrequent word pairs. In this paper, we propose to strike a balance between availability and restrictedness by making use of discourse markers. This approach has several advantages: markers are frequently found across genres (Webber, 2009), they exist in many languages (Jucker and Yiv, 1998), and capture various semantic properties (Hutchinson, 2004). We implement discourse markers within a vector space model that aims to distinguish between the three paradigmatic relations synonymy, antonymy and hypernymy in German and in English, across the three word classes of nouns, verbs, adjectives. We examine the performance of discourse markers as vector space dimensions in isolation and also explore their contribution in combination with lexical patterns. Distinguishing between paradigmatic relations such as synonymy, antonymy and hyp"
P14-2086,D08-1020,0,0.0942524,"Missing"
P14-2086,C04-1146,0,0.106838,"Missing"
P14-2086,P09-1077,0,0.0256296,". . . Table 2: Examples of discourse relations/markers. information to distinguish between paradigmatic relations. Our approach is motivated by linguistic studies that indicated a connection between discourse relations and lexical relations of words occurring in the respective discourse segments: Murphy et al. (2009) have shown, for example, that antonyms frequently serve as indicators for contrast relations in English and Swedish. More generally, pairs of word tokens have been identified as strong features for classifying discourse relations when no explicit discourse markers are available (Pitler et al., 2009; Biran and McKeown, 2013). Whereas word pairs have frequently been used as features for disambiguating discourse relations, to the best of our knowledge, our approach is novel in that we are the first to apply discourse relations as features for classifying lexical relations. One reason for this might be that discourse relations in general are only available in manually annotated corpora. Previous work has shown, however, that such relations can be classified reliably given the presence of explicit discourse markers.1 We hence rely on such markers as proxies for discourse relations (for examp"
P14-2086,prasad-etal-2008-penn,0,0.1444,"Missing"
P14-2086,C00-2137,0,0.196532,"Missing"
P14-2086,D12-1111,0,0.301787,"Missing"
P14-2086,E14-4008,1,0.771115,"tinguish between paradigmatic relations that can be distributionally very similar and propose a unified framework for nouns, verbs and adjectives. Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-p"
P14-2086,schafer-bildhauer-2012-building,0,0.0413909,"Missing"
P16-2042,P93-1023,0,0.451347,"s predicting ambiguity. 1 Introduction In the last decades, an impressive number of semantic classifications has been developed, both regarding manual lexicographic and/or cognitive classifications such as WordNet (Fellbaum, 1998), FrameNet (Fillmore et al., 2003), VerbNet (Kipper Schuler, 2006) and PrepNet/The Preposition Project (Litkowski and Hargraves, 2005; SaintDizier, 2005), as well as regarding computational classifications for nouns (Hindle, 1990; Pereira et al., 1993; Snow et al., 2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). 2 2.1 Experiments Pr"
P16-2042,P90-1034,0,0.81167,"ategorised nouns) are the most successful, and that (ii) soft clustering approaches are required for the task but reveal quite different attitudes towards predicting ambiguity. 1 Introduction In the last decades, an impressive number of semantic classifications has been developed, both regarding manual lexicographic and/or cognitive classifications such as WordNet (Fellbaum, 1998), FrameNet (Fillmore et al., 2003), VerbNet (Kipper Schuler, 2006) and PrepNet/The Preposition Project (Litkowski and Hargraves, 2005; SaintDizier, 2005), as well as regarding computational classifications for nouns (Hindle, 1990; Pereira et al., 1993; Snow et al., 2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine"
P16-2042,C10-1011,0,0.0443367,"ociation for Computational Linguistics lokal modal temporal kausal distributiv final urheber konditional ersatz restriktiv partitiv kopulativ Class ’local’ ’modal’ ’temporal’ ’causal’ ’distributive’ ’final’ ’creator’ ’conditional’ ’replacement’ ’restrictive’ ’partitive’ ’copulative’ Size 27 24 21 5 6 4 3 3 2 2 2 2 (3) 2nd-order syntactic co-occurrence: adjectives that modify nouns subcategorised by the prepositions, and adverbs that modify verbs subcategorising the prepositions. The dependency information was extracted from a parsed version of the SdeWaC using Bohnet’s MATE dependency parser (Bohnet, 2010; Scheible et al., 2013). All but the CBOW features were weighted according to positive pointwise mutual information. Table 1: Preposition classes. 2.3 classes that contained more than one preposition, and deleted prepositions that appeared <10,000 times in our web corpus containing 880 million words (cf. Section 2.2). This selection process resulted in 12 semantic classes covering between 2 and 27 prepositions each (cf. Table 1), and a more fine-grained version that sub-divided the three largest classes ’local’, ’modal’ and ’temporal’ into 6/10/7 sub-classes, respectively, and resulted in a t"
P16-2042,J12-3005,1,0.841623,"on In the last decades, an impressive number of semantic classifications has been developed, both regarding manual lexicographic and/or cognitive classifications such as WordNet (Fellbaum, 1998), FrameNet (Fillmore et al., 2003), VerbNet (Kipper Schuler, 2006) and PrepNet/The Preposition Project (Litkowski and Hargraves, 2005; SaintDizier, 2005), as well as regarding computational classifications for nouns (Hindle, 1990; Pereira et al., 1993; Snow et al., 2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). 2 2.1 Experiments Preposition Data In the"
P16-2042,C10-2052,0,0.0255268,"ons, comparably little effort in computational semantics has gone beyond a specific choice of prepositions (such as spatial prepositions), towards a systematic classification of preposition senses, as in The Preposition Project (Litkowski and Hargraves, 2005). Distributional approaches towards preposition meaning and sense distinction have only recently started to explore salient preposition features, but with few exceptions (such as Baldwin (2006)) these approaches focused on token-based classification of preposition senses (Ye and Baldwin, 2006; O’Hara and Wiebe, 2009; Tratz and Hovy, 2009; Hovy et al., 2010; Hovy et al., 2011). This paper addresses an automatic classification of preposition types in German, comparing various clustering approaches. We aim for an unsupervised setting that does not require predefined expensive resources, such as a token-based annotation of preposition senses. Our task is challenging, because (i) prepositions are notoriously ambiguous, (ii) the interpretation of out-of-context preposition type classification is more difficult than context-embedded token interpretation, (iii) there are no established lexical resources for type-based semantic classification other than"
P16-2042,P11-2056,0,0.0520067,"tle effort in computational semantics has gone beyond a specific choice of prepositions (such as spatial prepositions), towards a systematic classification of preposition senses, as in The Preposition Project (Litkowski and Hargraves, 2005). Distributional approaches towards preposition meaning and sense distinction have only recently started to explore salient preposition features, but with few exceptions (such as Baldwin (2006)) these approaches focused on token-based classification of preposition senses (Ye and Baldwin, 2006; O’Hara and Wiebe, 2009; Tratz and Hovy, 2009; Hovy et al., 2010; Hovy et al., 2011). This paper addresses an automatic classification of preposition types in German, comparing various clustering approaches. We aim for an unsupervised setting that does not require predefined expensive resources, such as a token-based annotation of preposition senses. Our task is challenging, because (i) prepositions are notoriously ambiguous, (ii) the interpretation of out-of-context preposition type classification is more difficult than context-embedded token interpretation, (iii) there are no established lexical resources for type-based semantic classification other than for English, and (i"
P16-2042,W98-1114,0,0.480686,"g computational classifications for nouns (Hindle, 1990; Pereira et al., 1993; Snow et al., 2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). 2 2.1 Experiments Preposition Data In the absence of any large-scale semantic hierarchical type classification, the German grammar book by Helbig and Buscha (1998) represents our gold standard. We selected those preposition 256 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 256–263, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Lingu"
P16-2042,S13-2049,0,0.0231231,"igns monosemous prepositions to only one cluster, and prepositions with three senses to a random integer in [1, 3]. Evaluation We chose the fuzzy extension of B-Cubed (Bagga and Baldwin, 1998) as evaluation measure, because it is (a) a pair-wise evaluation, which is considered as most suitable for soft clustering evaluations, and (b) distinguishes between homogeneity and completeness of a clustering, and thus resembles an evaluation by precision and recall. Amig´o et al. (2009) demonstrated the strengths of B-Cubed, and a similar version has been used in SemEval 2013 for Word Sense Induction (Jurgens and Klapaftis, 2013). Pair-wise precision P determines the homogeneity of a cluster analysis, by calculating for each individual preposition p the amount of prepositions p0 in the same cluster c that also belong to the same gold-standard class g, cf. Equation (1). Pair-wise recall R determines the completeness of a cluster analysis, by calculating for each individual preposition p the amount of prepositions p0 in the same gold-standard class g that also belong to the same cluster c, cf. Equation (2). The overall B-Cubed precision and recall scores are the averages over all preposition-wise scores. We combined pre"
P16-2042,D07-1091,0,0.178072,"2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). 2 2.1 Experiments Preposition Data In the absence of any large-scale semantic hierarchical type classification, the German grammar book by Helbig and Buscha (1998) represents our gold standard. We selected those preposition 256 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 256–263, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics lokal modal temporal kausal distributiv final urheber konditional ersatz restriktiv p"
P16-2042,C96-1055,0,0.645171,"Project (Litkowski and Hargraves, 2005; SaintDizier, 2005), as well as regarding computational classifications for nouns (Hindle, 1990; Pereira et al., 1993; Snow et al., 2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). 2 2.1 Experiments Preposition Data In the absence of any large-scale semantic hierarchical type classification, the German grammar book by Helbig and Buscha (1998) represents our gold standard. We selected those preposition 256 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 256–263,"
P16-2042,P05-1005,0,0.569387,"Hargraves, 2005; SaintDizier, 2005), as well as regarding computational classifications for nouns (Hindle, 1990; Pereira et al., 1993; Snow et al., 2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). 2 2.1 Experiments Preposition Data In the absence of any large-scale semantic hierarchical type classification, the German grammar book by Helbig and Buscha (1998) represents our gold standard. We selected those preposition 256 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 256–263, c Berlin, Germany, Augu"
P16-2042,koper-schulte-im-walde-2014-rank,1,0.896756,"Missing"
P16-2042,D07-1043,0,0.0309883,"sults in the previous section demonstrate the success of the type-based clustering, we were interested in two specific questions: (i) Where do the differences in the quality of the cluster analyses come from? (ii) Do the best cluster analyses present linguistically reliable and useful semantic classes? From a quantitative point of view, both questions have been addressed by the evaluation measure, fuzzy B-Cubed, which we chose for reasons outlined in Section 2.4. One should keep in mind, however, that there is an ongoing discussion about cluster comparison and cluster evaluation (Meila, 2007; Rosenberg and Hirschberg, 2007; Vinh and Bailey, 2010; Utt et al., 2014), which demonstrates uncertainty about an optimal measure, and which concerns us, expecially regarding the linguistic aspects of soft clustering. In the following, we therefore provide qualitative analyses and discussions of the cluster approaches and analyses. Optimal k: While fuzzy B-Cubed determined the numbers of clusters [15, 19] as optimal for the soft-clustering approaches, we also looked into the NMF cluster analysis with k = 32, with NMF as the best approach and 32 as the number of gold standard classes. The clusters are, again, very similar i"
P16-2042,P03-1009,0,0.617187,"quired for the task but reveal quite different attitudes towards predicting ambiguity. 1 Introduction In the last decades, an impressive number of semantic classifications has been developed, both regarding manual lexicographic and/or cognitive classifications such as WordNet (Fellbaum, 1998), FrameNet (Fillmore et al., 2003), VerbNet (Kipper Schuler, 2006) and PrepNet/The Preposition Project (Litkowski and Hargraves, 2005; SaintDizier, 2005), as well as regarding computational classifications for nouns (Hindle, 1990; Pereira et al., 1993; Snow et al., 2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information ex"
P16-2042,J07-4005,0,0.0137311,"izier, 2005), as well as regarding computational classifications for nouns (Hindle, 1990; Pereira et al., 1993; Snow et al., 2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). 2 2.1 Experiments Preposition Data In the absence of any large-scale semantic hierarchical type classification, the German grammar book by Helbig and Buscha (1998) represents our gold standard. We selected those preposition 256 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 256–263, c Berlin, Germany, August 7-12, 2016. 2016 Asso"
P16-2042,J06-2001,1,0.888652,"te different attitudes towards predicting ambiguity. 1 Introduction In the last decades, an impressive number of semantic classifications has been developed, both regarding manual lexicographic and/or cognitive classifications such as WordNet (Fellbaum, 1998), FrameNet (Fillmore et al., 2003), VerbNet (Kipper Schuler, 2006) and PrepNet/The Preposition Project (Litkowski and Hargraves, 2005; SaintDizier, 2005), as well as regarding computational classifications for nouns (Hindle, 1990; Pereira et al., 1993; Snow et al., 2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al."
P16-2042,P06-1101,0,0.0439859,"ssful, and that (ii) soft clustering approaches are required for the task but reveal quite different attitudes towards predicting ambiguity. 1 Introduction In the last decades, an impressive number of semantic classifications has been developed, both regarding manual lexicographic and/or cognitive classifications such as WordNet (Fellbaum, 1998), FrameNet (Fillmore et al., 2003), VerbNet (Kipper Schuler, 2006) and PrepNet/The Preposition Project (Litkowski and Hargraves, 2005; SaintDizier, 2005), as well as regarding computational classifications for nouns (Hindle, 1990; Pereira et al., 1993; Snow et al., 2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn"
P16-2042,I13-1072,1,0.832665,"rds model (CBOW) using negative sampling with K=15 (Mikolov et al., 2013); (2) direct syntactic dependency: we compare the most salient preposition-related dependencies: preposition-subcategorised nouns (nouns-dep, e.g., in Buch ‘in book’), prepositionsubcategorising nouns (nouns-gov, e.g., Buch von ‘book by’), and prepositionsubcategorising verbs (verbs-gov, e.g., reisen nach ‘to travel to’); 1 While we also conducted experiments using the coarsegrained class distribution in Table 1, the experiments in this paper focus on the fine-grained inventory. 2 The gold standard was previously used in Springorum et al. (2013) and in K¨oper and Schulte im Walde (2014). 3 257 k-Means/prep directly provides binary membership. preposition–cluster membership matrix with values ∈ [0, 1]. We transfered the real membership values to binary membership by applying a threshold t to decide about the cluster membership, again with t = 0.05, 0.1, 0.15, . . . , 0.95. For each clustering approach and for each number of clusters k we then identified the best threshold. 2.4 each preposition was assigned to n clusters, with n a random number between 1 and the number of gold-standard classes for that specific preposition. Note that t"
P16-2042,J09-2002,0,0.0760907,"Missing"
P16-2042,P03-1002,0,0.365272,"im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). 2 2.1 Experiments Preposition Data In the absence of any large-scale semantic hierarchical type classification, the German grammar book by Helbig and Buscha (1998) represents our gold standard. We selected those preposition 256 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 256–263, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics lokal modal temporal kausal distributiv final urheber konditional ersatz restriktiv partitiv kopulativ Class ’local’ ’modal’ ’temporal’ ’causal’ ’distributive"
P16-2042,P93-1024,0,0.802851,"ns) are the most successful, and that (ii) soft clustering approaches are required for the task but reveal quite different attitudes towards predicting ambiguity. 1 Introduction In the last decades, an impressive number of semantic classifications has been developed, both regarding manual lexicographic and/or cognitive classifications such as WordNet (Fellbaum, 1998), FrameNet (Fillmore et al., 2003), VerbNet (Kipper Schuler, 2006) and PrepNet/The Preposition Project (Litkowski and Hargraves, 2005; SaintDizier, 2005), as well as regarding computational classifications for nouns (Hindle, 1990; Pereira et al., 1993; Snow et al., 2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher"
P16-2042,N09-3017,0,0.0162273,"ct Regarding prepositions, comparably little effort in computational semantics has gone beyond a specific choice of prepositions (such as spatial prepositions), towards a systematic classification of preposition senses, as in The Preposition Project (Litkowski and Hargraves, 2005). Distributional approaches towards preposition meaning and sense distinction have only recently started to explore salient preposition features, but with few exceptions (such as Baldwin (2006)) these approaches focused on token-based classification of preposition senses (Ye and Baldwin, 2006; O’Hara and Wiebe, 2009; Tratz and Hovy, 2009; Hovy et al., 2010; Hovy et al., 2011). This paper addresses an automatic classification of preposition types in German, comparing various clustering approaches. We aim for an unsupervised setting that does not require predefined expensive resources, such as a token-based annotation of preposition senses. Our task is challenging, because (i) prepositions are notoriously ambiguous, (ii) the interpretation of out-of-context preposition type classification is more difficult than context-embedded token interpretation, (iii) there are no established lexical resources for type-based semantic classi"
P16-2042,C00-2094,0,0.624189,"al., 1993; Snow et al., 2006), verbs (Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). 2 2.1 Experiments Preposition Data In the absence of any large-scale semantic hierarchical type classification, the German grammar book by Helbig and Buscha (1998) represents our gold standard. We selected those preposition 256 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 256–263, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics lokal modal temporal kausal distributiv final urheber konditio"
P16-2042,utt-etal-2014-fuzzy,1,0.895191,"Missing"
P16-2042,2014.amta-researchers.21,1,0.728708,"d Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006) and adjectives (Hatzivassiloglou and McKeown, 1993; Boleda et al., 2012). Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009). 2 2.1 Experiments Preposition Data In the absence of any large-scale semantic hierarchical type classification, the German grammar book by Helbig and Buscha (1998) represents our gold standard. We selected those preposition 256 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 256–263, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics lokal modal temporal kausal distributiv final urheber konditional ersatz restriktiv partitiv kopulativ Clas"
P16-2042,P98-1012,0,\N,Missing
P16-2042,C98-1012,0,\N,Missing
P16-2042,P99-1014,0,\N,Missing
P16-2042,J01-3003,0,\N,Missing
P16-2074,D14-1151,0,0.0570947,"Missing"
P16-2074,J06-1003,0,0.16654,"Lyons, 1977). From a computational point of view, distinguishing between antonymy and synonymy is important for NLP applications such as Machine Translation and Textual Entailment, which go beyond a general notion of semantic relatedness and require to identify specific semantic relations. However, due to interchangeable substitution, antonyms and synonyms often occur in similar contexts, which makes it challenging to automatically distinguish between them. Distributional semantic models (DSMs) offer a means to represent meaning vectors of words and to determine their semantic “relatedness” (Budanitsky and Hirst, 2006; Turney and Pantel, 2010). 454 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 454–459, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics information (LMI) (Evert, 2005) to determine the original strengths of the word features. Our score weightSA (w, f ) subsequently defines the weights of a target word w and a feature f : posed thesaurus-based word embeddings to capture antonyms. They proposed two models: the WE-T model that trains word embeddings on thesaurus information; and the WE-TD model that incorporated"
P16-2074,P15-2004,0,0.459613,"the 54th Annual Meeting of the Association for Computational Linguistics, pages 454–459, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics information (LMI) (Evert, 2005) to determine the original strengths of the word features. Our score weightSA (w, f ) subsequently defines the weights of a target word w and a feature f : posed thesaurus-based word embeddings to capture antonyms. They proposed two models: the WE-T model that trains word embeddings on thesaurus information; and the WE-TD model that incorporated distributional information into the WET model. Pham et al. (2015) introduced the multitask lexical contrast model (mLCM) by incorporating WordNet into a skip-gram model to optimize semantic vectors to predict contexts. Their model outperformed standard skip-gram models with negative sampling on both general semantic tasks and distinguishing antonyms from synonyms. In this paper, we propose two approaches that make use of lexical contrast information in distributional semantic space and word embeddings for antonym–synonym distinction. Firstly, we incorporate lexical contrast into distributional vectors and strengthen those word features that are most salient"
P16-2074,P14-2086,1,0.917612,"Missing"
P16-2074,J15-4004,0,0.192303,"information, we used WordNet (Miller, 1995) and Wordnik1 to collect antonyms and synonyms, obtaining a total of 363,309 synonym and 38,423 antonym pairs. 3.2 3.3 The second experiment evaluates the performance of our dLCE model on both antonym–synonym distinction and a word similarity task. The similarity task requires to predict the degree of similarity for word pairs, and the ranked list of predictions is evaluated against a gold standard of human ratings, relying on the Spearman rank-order correlation coefficient ρ (Siegel and Castellan, 1988). In this paper, we use the SimLex-999 dataset (Hill et al., 2015) to evaluate word embedding models on predicting similarities. The resource contains 999 word pairs (666 noun, 222 verb and 111 adjective pairs) and was explicitly built to test models on capturing similarity rather than relatedness or association. Table 2 shows that our dLCE model outperforms both SGNS and mLCM, proving that the lexical contrast information has a positive effect on predicting similarity. Distinguishing antonyms from synonyms The first experiment evaluates our lexical contrast vectors by applying the vector representations with the improved weightSA scores to the task of disti"
P16-2074,E14-4008,1,0.905876,"riples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns ‘from X to Y’ or ‘either X or Y’ significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations). Lately, antonym–synonym distinction has also been a focus of word embedding models. For example, Adel and Sch¨utze (2014) integrated coreference chains extracted from large corpora into a skip-gram model to create word embeddings that identified antonyms. Ono et al. (2015) proWe propose a novel vector repre"
P16-2074,Y14-1018,0,0.235039,"riples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns ‘from X to Y’ or ‘either X or Y’ significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations). Lately, antonym–synonym distinction has also been a focus of word embedding models. For example, Adel and Sch¨utze (2014) integrated coreference chains extracted from large corpora into a skip-gram model to create word embeddings that identified antonyms. Ono et al. (2015) proWe propose a novel vector repre"
P16-2074,schafer-bildhauer-2012-building,0,0.0865315,"Missing"
P16-2074,N13-1090,0,0.462808,"h negative sampling on both general semantic tasks and distinguishing antonyms from synonyms. In this paper, we propose two approaches that make use of lexical contrast information in distributional semantic space and word embeddings for antonym–synonym distinction. Firstly, we incorporate lexical contrast into distributional vectors and strengthen those word features that are most salient for determining word similarities, assuming that feature overlap in synonyms is stronger than feature overlap in antonyms. Secondly, we propose a novel extension of a skip-gram model with negative sampling (Mikolov et al., 2013b) that integrates the lexical contrast information into the objective function. The proposed model optimizes the semantic vectors to predict degrees of word similarity and also to distinguish antonyms from synonyms. The improved word embeddings outperform state-of-the-art models on antonym– synonym distinction and a word similarity task. 2 P 1 weightSA (w, f ) = #(w,u) sim(w, u) P P u∈W (f )∩S(w) 1 0 − #(w0 ,v) v∈W (f )∩S(w0 ) sim(w , v) w0 ∈A(w) The new weightSA scores of a target word w and a feature f exploit the differences between the average similarities of synonyms to the target word ("
P16-2074,I13-1056,1,0.890467,"allenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies. For example, Lin et al. (2003) used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns ‘from X to Y’ or ‘either X or Y’ significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations). Lately, antonym–synonym distinction has also been a focus of word embedding models. For example, Adel and Sch¨utze (2014) integrated core"
P16-2074,J13-3004,0,0.0815755,"th synonyms (such as formal–conventional) and antonyms (such as formal–informal) as related words and cannot sufficiently distinguish between the two relations. In recent years, a number of distributional approaches have accepted the challenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies. For example, Lin et al. (2003) used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns ‘from X to Y’ or ‘either X or Y’ significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-b"
P16-2074,N15-1100,0,0.186051,"l by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations). Lately, antonym–synonym distinction has also been a focus of word embedding models. For example, Adel and Sch¨utze (2014) integrated coreference chains extracted from large corpora into a skip-gram model to create word embeddings that identified antonyms. Ono et al. (2015) proWe propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66–0.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex999, and on distingui"
P18-1231,agerri-etal-2014-ixa,0,0.0281299,"d Basque (EU). ments, we take 70 percent of the data for training, 20 percent for testing and the remaining 10 percent are used as development data for tuning. 4.2 Monolingual Word Embeddings For B LSE, A RTETXE, and M T, we require monolingual vector spaces for each of our languages. For English, we use the publicly available GoogleNews vectors4 . For Spanish, Catalan, and Basque, we train skip-gram embeddings using the Word2Vec toolkit4 with 300 dimensions, subsampling of 10−4 , window of 5, negative sampling of 15 based on a 2016 Wikipedia corpus5 (sentence-split, tokenized with IXA pipes (Agerri et al., 2014) and lowercased). The statistics of the Wikipedia corpora are given in Table 2. 4.3 Bilingual Lexicon For B LSE, A RTETXE, and BARISTA, we also require a bilingual lexicon. We use the sentiment lexicon from Hu and Liu (2004) (to which we refer in the following as Bing Liu) and its translation into each target language. We translate the lexicon using Google Translate and exclude multi-word expressions.6 This leaves a dictionary of 5700 translations in Spanish, 5271 in Catalan, and 4577 in Basque. We set aside ten percent of the translation pairs as a development set in order to check that the d"
P18-1231,P15-1040,0,0.0175784,"l sentiment analysis typically exploit machine translation based methods or multilingual models. Machine translation (M T) can provide a way to transfer sentiment information from a resource-rich to resourcepoor languages (Mihalcea et al., 2007; Balahur and Turchi, 2014). However, M T-based methods require large parallel corpora to train the translation system, which are often not available for underresourced languages. Examples of multilingual methods that have been applied to cross-lingual sentiment analysis include domain adaptation methods (Prettenhofer and Stein, 2011), delexicalization (Almeida et al., 2015), and bilingual word embeddings (Mikolov et al., 2013; Hermann and Blunsom, 2014; Artetxe et al., 2016). These approaches however do not incorporate enough sentiment information to perform well cross-lingually, as we will show later. We propose a novel approach to incorporate sentiment information in a model, which does not have these disadvantages. Bilingual Sentiment Embeddings (B LSE) are embeddings that are jointly optimized to represent both (a) semantic information in the source and target languages, which are bound to each other through a small bilingual dictionary, and (b) sentiment in"
P18-1231,D16-1250,0,0.508022,"e translation (M T) can provide a way to transfer sentiment information from a resource-rich to resourcepoor languages (Mihalcea et al., 2007; Balahur and Turchi, 2014). However, M T-based methods require large parallel corpora to train the translation system, which are often not available for underresourced languages. Examples of multilingual methods that have been applied to cross-lingual sentiment analysis include domain adaptation methods (Prettenhofer and Stein, 2011), delexicalization (Almeida et al., 2015), and bilingual word embeddings (Mikolov et al., 2013; Hermann and Blunsom, 2014; Artetxe et al., 2016). These approaches however do not incorporate enough sentiment information to perform well cross-lingually, as we will show later. We propose a novel approach to incorporate sentiment information in a model, which does not have these disadvantages. Bilingual Sentiment Embeddings (B LSE) are embeddings that are jointly optimized to represent both (a) semantic information in the source and target languages, which are bound to each other through a small bilingual dictionary, and (b) sentiment information, which is annotated on the source language only. We only need three resources: (i) a comparab"
P18-1231,P17-1042,0,0.164081,"Missing"
P18-1231,C10-1004,0,0.0300373,"nal Linguistics 2 Related Work Machine Translation: Early work in cross-lingual sentiment analysis found that machine translation (M T) had reached a point of maturity that enabled the transfer of sentiment across languages. Researchers translated sentiment lexicons (Mihalcea et al., 2007; Meng et al., 2012) or annotated corpora and used word alignments to project sentiment annotation and create target-language annotated corpora (Banea et al., 2008; Duh et al., 2011; Demirtas and Pechenizkiy, 2013; Balahur and Turchi, 2014). Several approaches included a multi-view representation of the data (Banea et al., 2010; Xiao and Guo, 2012) or co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) to improve over a naive implementation of machine translation, where only the translated data is used. There are also approaches which only require parallel data (Meng et al., 2012; Zhou et al., 2016; Rasooli et al., 2017), instead of machine translation. All of these approaches, however, require large amounts of parallel data or an existing high quality translation tool, which are not always available. A notable exception is the approach proposed by Chen et al. (2016), an adversarial deep averaging network, which"
P18-1231,D08-1014,0,0.215816,"al Meeting of the Association for Computational Linguistics (Long Papers), pages 2483–2493 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work Machine Translation: Early work in cross-lingual sentiment analysis found that machine translation (M T) had reached a point of maturity that enabled the transfer of sentiment across languages. Researchers translated sentiment lexicons (Mihalcea et al., 2007; Meng et al., 2012) or annotated corpora and used word alignments to project sentiment annotation and create target-language annotated corpora (Banea et al., 2008; Duh et al., 2011; Demirtas and Pechenizkiy, 2013; Balahur and Turchi, 2014). Several approaches included a multi-view representation of the data (Banea et al., 2010; Xiao and Guo, 2012) or co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) to improve over a naive implementation of machine translation, where only the translated data is used. There are also approaches which only require parallel data (Meng et al., 2012; Zhou et al., 2016; Rasooli et al., 2017), instead of machine translation. All of these approaches, however, require large amounts of parallel data or an existing high qual"
P18-1231,L18-1104,1,0.839911,"t . As in the training procedure, for each sentence, we take the word embeddings from the target embeddings T and average them to ai ∈ Rd . We then project this vector to the joint bilingual space ˆ zi = ai · M 0 . Finally, we pass ˆ zi through a softmax layer P to get our prediction yˆi = softmax(ˆ zi · P ). 4 4.1 Datasets and Resources OpeNER and MultiBooked To evaluate our proposed model, we conduct experiments using four benchmark datasets and three bilingual combinations. We use the OpeNER English and Spanish datasets (Agerri et al., 2013) and the MultiBooked Catalan and Basque datasets (Barnes et al., 2018). All datasets contain hotel reviews which are annotated for aspect-level sentiment analysis. The labels include Strong Negative (−−), Negative (−), Positive (+), and Strong Positive (++). We map the aspect-level annotations to sentence level by taking the most common label and remove instances of mixed polarity. We also create a binary setup by combining the strong and weak classes. This gives us a total of six experiments. The details of the sentence-level datasets are summarized in Table 1. For each of the experi2486 5 Experiments 5.1 Figure 2: Binary and four class macro F1 on Spanish (ES)"
P18-1231,Q18-1039,0,0.0618566,"Missing"
P18-1231,P11-2075,0,0.0254505,"sociation for Computational Linguistics (Long Papers), pages 2483–2493 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work Machine Translation: Early work in cross-lingual sentiment analysis found that machine translation (M T) had reached a point of maturity that enabled the transfer of sentiment across languages. Researchers translated sentiment lexicons (Mihalcea et al., 2007; Meng et al., 2012) or annotated corpora and used word alignments to project sentiment annotation and create target-language annotated corpora (Banea et al., 2008; Duh et al., 2011; Demirtas and Pechenizkiy, 2013; Balahur and Turchi, 2014). Several approaches included a multi-view representation of the data (Banea et al., 2010; Xiao and Guo, 2012) or co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) to improve over a naive implementation of machine translation, where only the translated data is used. There are also approaches which only require parallel data (Meng et al., 2012; Zhou et al., 2016; Rasooli et al., 2017), instead of machine translation. All of these approaches, however, require large amounts of parallel data or an existing high quality translation to"
P18-1231,N15-1157,0,0.186226,"requiring the projection to be orthogonal, thereby preserving the monolingual quality of the original word vectors. Given source embeddings S, target embeddings T , and a bilingual lexicon L, Artetxe et al. (2016) learn a projection matrix W by minimizing the square of Euclidean distances X arg min ||S 0 W − T 0 ||2F , (1) W i where S 0 ∈ S and T 0 ∈ T are the word embedding matrices for the tokens in the bilingual lexicon L. This is solved using the Moore-Penrose pseudoinverse S 0+ = (S 0T S 0 )−1 S 0T as W = S 0+ T 0 , which can be computed using SVD. We refer to this approach as A RTETXE. Gouws and Søgaard (2015) propose a method to create a pseudo-bilingual corpus with a small taskspecific bilingual lexicon, which can then be used to train bilingual embeddings (BARISTA). This approach requires a monolingual corpus in both the source and target languages and a set of translation pairs. The source and target corpora are concatenated and then every word is randomly kept or replaced by its translation with a probability of 0.5. Any kind of word embedding algorithm can be trained with this pseudo-bilingual corpus to create bilingual word embeddings. These last techniques have the advantage of requiring re"
P18-1231,P14-1006,0,0.109653,"multilingual models. Machine translation (M T) can provide a way to transfer sentiment information from a resource-rich to resourcepoor languages (Mihalcea et al., 2007; Balahur and Turchi, 2014). However, M T-based methods require large parallel corpora to train the translation system, which are often not available for underresourced languages. Examples of multilingual methods that have been applied to cross-lingual sentiment analysis include domain adaptation methods (Prettenhofer and Stein, 2011), delexicalization (Almeida et al., 2015), and bilingual word embeddings (Mikolov et al., 2013; Hermann and Blunsom, 2014; Artetxe et al., 2016). These approaches however do not incorporate enough sentiment information to perform well cross-lingually, as we will show later. We propose a novel approach to incorporate sentiment information in a model, which does not have these disadvantages. Bilingual Sentiment Embeddings (B LSE) are embeddings that are jointly optimized to represent both (a) semantic information in the source and target languages, which are bound to each other through a small bilingual dictionary, and (b) sentiment information, which is annotated on the source language only. We only need three re"
P18-1231,P15-1162,0,0.0208872,"Classification We add a second training objective to optimize the projected source vectors to predict the sentiment of source phrases. This inevitably changes the projection characteristics of the matrix M , and consequently M 0 and encourages M 0 to learn to predict sentiment without any training examples in the target language. To train M to predict sentiment, we require a source-language corpus Csource = {(x1 , y1 ), (x2 , y2 ), . . . , (xi , yi )} where each sentence xi is associated with a label yi . For classification, we use a two-layer feedforward averaging network, loosely following Iyyer et al. (2015)3 . For a sentence xi we take the word embeddings from the source embedding S and average them to ai ∈ Rd . We then project this vector to the joint bilingual space zi = ai · M . Finally, we pass zi through a softmax layer P to get our prediction yˆi = softmax(zi · P ). To train our model to predict sentiment, we minimize the cross-entropy error of our predictions H=− n X yi log yˆi − (1 − yi ) log(1 − yˆi ) . (3) i=1 3.3 Joint Learning In order to jointly train both the projection component and the sentiment component, we combine the two loss functions to optimize the parameter 1 We omit para"
P18-1231,P15-1027,0,0.0494919,"using the most frequent words as translation pairs is an effective approach, for sentiment analysis, this does not seem to help. Using a translated sentiment lexicon, even if it is small, gives better results. 9 http://www.meta-share.org The translation took approximately one hour. We can extrapolate that hand translating a sentiment lexicon the size of the Bing Liu lexicon would take no more than 5 hours. 10 BLSE No M&apos; translation translation source F1 source F1 target F1 target F1 1.0 Cosine Similarity Research into projection techniques for bilingual word embeddings (Mikolov et al., 2013; Lazaridou et al., 2015; Artetxe et al., 2016) often uses a lexicon of the most frequent 8–10 thousand words in English and their translations as training data. We test this approach by taking the 10,000 wordto-word translations from the Apertium Englishto-Spanish dictionary9 . We also use the Google Translate API to translate the NRC hashtag sentiment lexicon (Mohammad et al., 2013) and keep the 22,984 word-to-word translations. We perform the same experiment as above and vary the amount of training data from 0, 100, 300, 600, 1000, 3000, 6000, 10,000 up to 20,000 training pairs. Finally, we compile a small hand tr"
P18-1231,P11-1015,0,0.126172,"proach requires a monolingual corpus in both the source and target languages and a set of translation pairs. The source and target corpora are concatenated and then every word is randomly kept or replaced by its translation with a probability of 0.5. Any kind of word embedding algorithm can be trained with this pseudo-bilingual corpus to create bilingual word embeddings. These last techniques have the advantage of requiring relatively little parallel training data while taking advantage of larger amounts of monolingual data. However, they are not optimized for sentiment. Sentiment Embeddings: Maas et al. (2011) first explored the idea of incorporating sentiment information into semantic word vectors. They proposed a topic modeling approach similar to latent Dirichlet allocation in order to collect the semantic information in their word vectors. To incorporate the sentiment information, they included a second objective whereby they maximize the probability of the sentiment label for each word in a labeled document. Tang et al. (2014) exploit distantly annotated tweets to create Twitter sentiment embeddings. To incorporate distributional information about tokens, they use a hinge loss and maximize the"
P18-1231,P12-1060,0,0.0609445,"bilingual sentiment space. Our implementation is publicly available at https://github.com/jbarnesspain/blse. 2483 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2483–2493 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work Machine Translation: Early work in cross-lingual sentiment analysis found that machine translation (M T) had reached a point of maturity that enabled the transfer of sentiment across languages. Researchers translated sentiment lexicons (Mihalcea et al., 2007; Meng et al., 2012) or annotated corpora and used word alignments to project sentiment annotation and create target-language annotated corpora (Banea et al., 2008; Duh et al., 2011; Demirtas and Pechenizkiy, 2013; Balahur and Turchi, 2014). Several approaches included a multi-view representation of the data (Banea et al., 2010; Xiao and Guo, 2012) or co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) to improve over a naive implementation of machine translation, where only the translated data is used. There are also approaches which only require parallel data (Meng et al., 2012; Zhou et al., 2016; Rasooli e"
P18-1231,P07-1123,0,0.839981,"sentiment analysis are motivated by the lack of training data in the vast majority of languages. Even languages spoken by several million people, such as Catalan, often have few resources available to perform sentiment analysis in specific domains. We therefore aim to harness the knowledge previously collected in resource-rich languages. Previous approaches for cross-lingual sentiment analysis typically exploit machine translation based methods or multilingual models. Machine translation (M T) can provide a way to transfer sentiment information from a resource-rich to resourcepoor languages (Mihalcea et al., 2007; Balahur and Turchi, 2014). However, M T-based methods require large parallel corpora to train the translation system, which are often not available for underresourced languages. Examples of multilingual methods that have been applied to cross-lingual sentiment analysis include domain adaptation methods (Prettenhofer and Stein, 2011), delexicalization (Almeida et al., 2015), and bilingual word embeddings (Mikolov et al., 2013; Hermann and Blunsom, 2014; Artetxe et al., 2016). These approaches however do not incorporate enough sentiment information to perform well cross-lingually, as we will s"
P18-1231,P09-1027,0,0.280041,"y work in cross-lingual sentiment analysis found that machine translation (M T) had reached a point of maturity that enabled the transfer of sentiment across languages. Researchers translated sentiment lexicons (Mihalcea et al., 2007; Meng et al., 2012) or annotated corpora and used word alignments to project sentiment annotation and create target-language annotated corpora (Banea et al., 2008; Duh et al., 2011; Demirtas and Pechenizkiy, 2013; Balahur and Turchi, 2014). Several approaches included a multi-view representation of the data (Banea et al., 2010; Xiao and Guo, 2012) or co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) to improve over a naive implementation of machine translation, where only the translated data is used. There are also approaches which only require parallel data (Meng et al., 2012; Zhou et al., 2016; Rasooli et al., 2017), instead of machine translation. All of these approaches, however, require large amounts of parallel data or an existing high quality translation tool, which are not always available. A notable exception is the approach proposed by Chen et al. (2016), an adversarial deep averaging network, which trains a joint feature extractor for two langu"
P18-1231,W10-3111,0,0.038225,"number of mistakes (an average of 71 per model on binary and 167 on 4-class), it is mainly due to its prevalence. M T performed the best on the test examples which according to the annotation require a correct understanding of the vocabulary (81 F1 on binary /54 F1 on 4-class), with B LSE (79/48) slightly worse. A RTETXE (70/35) and BARISTA (67/41) perform significantly worse. This suggests that B LSE is better A RTETXE and BARISTA at transferring sentiment of the most important sentiment bearing words. Negation: Negation is a well-studied phenomenon in sentiment analysis (Pang et al., 2002; Wiegand et al., 2010; Zhu et al., 2014; Reitan et al., 2015). Therefore, we are interested in how these four models perform on phrases that include the negation of a key element, for example “In general, this hotel isn’t bad”. We would like our models to recognize that the combination of two negative elements “isn’t” and “bad” lead to a Positive label. Given the simple classification strategy, all models perform relatively well on phrases with negation (all reach nearly 60 F1 in the binary setting). However, while B LSE performs the best on negation in the binary setting (82.9 F1 ), it has more problems with nega"
P18-1231,C12-1174,0,0.683476,"lated Work Machine Translation: Early work in cross-lingual sentiment analysis found that machine translation (M T) had reached a point of maturity that enabled the transfer of sentiment across languages. Researchers translated sentiment lexicons (Mihalcea et al., 2007; Meng et al., 2012) or annotated corpora and used word alignments to project sentiment annotation and create target-language annotated corpora (Banea et al., 2008; Duh et al., 2011; Demirtas and Pechenizkiy, 2013; Balahur and Turchi, 2014). Several approaches included a multi-view representation of the data (Banea et al., 2010; Xiao and Guo, 2012) or co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) to improve over a naive implementation of machine translation, where only the translated data is used. There are also approaches which only require parallel data (Meng et al., 2012; Zhou et al., 2016; Rasooli et al., 2017), instead of machine translation. All of these approaches, however, require large amounts of parallel data or an existing high quality translation tool, which are not always available. A notable exception is the approach proposed by Chen et al. (2016), an adversarial deep averaging network, which trains a joint featu"
P18-1231,C00-2137,0,0.0403147,".5 **60.0 38.1 *42.5 R **80.1 **73.0 **72.7 *43.4 38.1 37.4 F1 **74.6 **72.9 **69.3 *41.2 35.9 30.0 In Figure 2, we report the results of all four methods. Our method outperforms the other projection methods (the baselines A RTETXE and BARISTA) on four of the six experiments substantially. It performs only slightly worse than the more resourcecostly upper bounds (M T and M ONO). This is especially noticeable for the binary classification task, where B LSE performs nearly as well as machine translation and significantly better than the other methods. We perform approximate randomization tests (Yeh, 2000) with 10,000 runs and highlight the results that are statistically significant (**p < 0.01, *p < 0.05) in Table 3. In more detail, we see that M T generally performs better than the projection methods (79–69 F1 on binary, 52–44 on 4-class). B LSE (75–69 on binary, 41–30 on 4-class) has the best performance of the projection methods and is comparable with M T on the binary setup, with no significant difference on binary Basque. A RTETXE (67–46 on binary, 35–21 on 4-class) and BARISTA (61– 55 on binary, 40–34 on 4-class) are significantly worse than B LSE on all experiments except Catalan and Ba"
P18-1231,S13-2053,0,0.0422246,"ng Liu lexicon would take no more than 5 hours. 10 BLSE No M&apos; translation translation source F1 source F1 target F1 target F1 1.0 Cosine Similarity Research into projection techniques for bilingual word embeddings (Mikolov et al., 2013; Lazaridou et al., 2015; Artetxe et al., 2016) often uses a lexicon of the most frequent 8–10 thousand words in English and their translations as training data. We test this approach by taking the 10,000 wordto-word translations from the Apertium Englishto-Spanish dictionary9 . We also use the Google Translate API to translate the NRC hashtag sentiment lexicon (Mohammad et al., 2013) and keep the 22,984 word-to-word translations. We perform the same experiment as above and vary the amount of training data from 0, 100, 300, 600, 1000, 3000, 6000, 10,000 up to 20,000 training pairs. Finally, we compile a small hand translated dictionary of 200 pairs, which we then expand using target language morphological information, finally giving us 657 translation pairs10 . The macro F1 score for the Bing Liu dictionary climbs constantly with the increasing translation pairs. Both the Apertium and NRC dictionaries perform worse than the translated lexicon by Bing Liu, while the expande"
P18-1231,W02-1011,0,0.0239736,"s the largest total number of mistakes (an average of 71 per model on binary and 167 on 4-class), it is mainly due to its prevalence. M T performed the best on the test examples which according to the annotation require a correct understanding of the vocabulary (81 F1 on binary /54 F1 on 4-class), with B LSE (79/48) slightly worse. A RTETXE (70/35) and BARISTA (67/41) perform significantly worse. This suggests that B LSE is better A RTETXE and BARISTA at transferring sentiment of the most important sentiment bearing words. Negation: Negation is a well-studied phenomenon in sentiment analysis (Pang et al., 2002; Wiegand et al., 2010; Zhu et al., 2014; Reitan et al., 2015). Therefore, we are interested in how these four models perform on phrases that include the negation of a key element, for example “In general, this hotel isn’t bad”. We would like our models to recognize that the combination of two negative elements “isn’t” and “bad” lead to a Positive label. Given the simple classification strategy, all models perform relatively well on phrases with negation (all reach nearly 60 F1 in the binary setting). However, while B LSE performs the best on negation in the binary setting (82.9 F1 ), it has m"
P18-1231,P15-1042,0,0.0548822,"sentiment information, they included a second objective whereby they maximize the probability of the sentiment label for each word in a labeled document. Tang et al. (2014) exploit distantly annotated tweets to create Twitter sentiment embeddings. To incorporate distributional information about tokens, they use a hinge loss and maximize the likelihood of a true n-gram over a corrupted n-gram. They include a second objective where they classify the polarity of the tweet given the true n-gram. While these techniques have proven useful, they are not easily transferred to a cross-lingual setting. Zhou et al. (2015) create bilingual sentiment embeddings by translating all source data to the 2484 target language and vice versa. This requires the existence of a machine translation system, which is a prohibitive assumption for many under-resourced languages, especially if it must be open and freely accessible. This motivates approaches which can use smaller amounts of parallel data to achieve similar results. 3 n 1X MSE = (zi − ˆ zi )2 , n (2) i=1 Model In order to project not only semantic similarity and relatedness but also sentiment information to our target language, we propose a new model, namely Bilin"
P18-1231,P14-1029,0,0.0235702,"n average of 71 per model on binary and 167 on 4-class), it is mainly due to its prevalence. M T performed the best on the test examples which according to the annotation require a correct understanding of the vocabulary (81 F1 on binary /54 F1 on 4-class), with B LSE (79/48) slightly worse. A RTETXE (70/35) and BARISTA (67/41) perform significantly worse. This suggests that B LSE is better A RTETXE and BARISTA at transferring sentiment of the most important sentiment bearing words. Negation: Negation is a well-studied phenomenon in sentiment analysis (Pang et al., 2002; Wiegand et al., 2010; Zhu et al., 2014; Reitan et al., 2015). Therefore, we are interested in how these four models perform on phrases that include the negation of a key element, for example “In general, this hotel isn’t bad”. We would like our models to recognize that the combination of two negative elements “isn’t” and “bad” lead to a Positive label. Given the simple classification strategy, all models perform relatively well on phrases with negation (all reach nearly 60 F1 in the binary setting). However, while B LSE performs the best on negation in the binary setting (82.9 F1 ), it has more problems with negation in the 4-clas"
P18-1231,W15-2914,0,0.0526238,"Missing"
P18-1231,P14-1146,0,0.0717978,"latively little parallel training data while taking advantage of larger amounts of monolingual data. However, they are not optimized for sentiment. Sentiment Embeddings: Maas et al. (2011) first explored the idea of incorporating sentiment information into semantic word vectors. They proposed a topic modeling approach similar to latent Dirichlet allocation in order to collect the semantic information in their word vectors. To incorporate the sentiment information, they included a second objective whereby they maximize the probability of the sentiment label for each word in a labeled document. Tang et al. (2014) exploit distantly annotated tweets to create Twitter sentiment embeddings. To incorporate distributional information about tokens, they use a hinge loss and maximize the likelihood of a true n-gram over a corrupted n-gram. They include a second objective where they classify the polarity of the tweet given the true n-gram. While these techniques have proven useful, they are not easily transferred to a cross-lingual setting. Zhou et al. (2015) create bilingual sentiment embeddings by translating all source data to the 2484 target language and vice versa. This requires the existence of a machine"
P19-1072,C14-1154,0,0.104448,"2016; Hamilton et al., 2016a,b; Hellrich and Hahn, 2016; Rosenfeld and Erk, 2018). LSC is typically measured by the cosine distance (or some alternative metric) between the two vectors, or by differences in contextual dispersion between the two vectors (Kisselew et al., 2016; Schlechtweg et al., 2017). (ii) Diachronic topic models infer a probability distribution for each word over different word senses (or topics), which are in turn modeled as a distribution over words (Wang and McCallum, 2006; Bamman and Crane, 2011; Wijaya and Yeniterzi, 2011; Lau et al., 2012; Mihalcea and Nastase, 2012; Cook et al., 2014; Frermann and Lapata, 2016). LSC of a word is measured by calculating a novelty score for its senses based on their frequency of use. (iii) Clustering models assign all uses of a word into sense clusters based on some contextual property (Mitra et al., 2015). Word sense clustering models are similar to topic models in that they map uses to senses. Accordingly, LSC of a word is measured similarly as in (ii). For an overview on diachronic LSC detection, see Tahmasebi et al. (2018). Evaluation. Existing evaluation procedures for LSC detection can be distinguished into evaluation on (i) empirical"
P19-1072,P17-1042,0,0.0243392,"ntic representation inferred for a target word w and time period t consists of a Kdimensional distribution over word senses φt and a V -dimensional distribution over the vocabulary ψ t,k for each word sense k, where K is a predefined number of senses for target word w. SCAN places parametrized logistic normal priors on φt and ψ t,k in order to encourage a smooth change of parameters, where the extent of change is controlled through the precision parameter K φ , which is learned during training. j Following standard practice we length-normalize and mean-center A and B in a pre-processing step (Artetxe et al., 2017), and constrain W to be orthogonal, which preserves distances within each time period. Under this constraint, minimizing the squared Euclidean distance becomes equivalent to maximizing the dot product when finding the optimal rotational alignment (Hamilton et al., 2016b; Artetxe et al., 2017). The optimal solution for this 736 Although ψ t,k may change over time for word sense k, senses are intended to remain thematically consistent as controlled by word precision parameter K ψ . This allows comparison of the topic distribution across time periods. For each target word w we infer a SCAN model"
P19-1072,cook-stevenson-2010-automatically,0,0.0325258,"valuation. Existing evaluation procedures for LSC detection can be distinguished into evaluation on (i) empirically observed data, and (ii) synthetic data or related tasks. (i) includes case studies of individual words (Sagi et al., 2009; Jatowt and Duh, 2014; Hamilton et al., 2016a), stand-alone comparison of a few hand-selected words (Wijaya and Yeniterzi, 2011; Hamilton et al., 2016b; Del Tredici and Fern´andez, 2017), comparison of hand-selected changing vs. semantically stable words (Lau et al., 2012; Cook et al., 2014), and post-hoc evaluation of the predictions of the presented models (Cook and Stevenson, 2010; Kulkarni et al., 2015; Del Tredici et al., 2016; Eger and Mehler, 2016; Ferrari et al., 2017). Schlechtweg et al. (2017) propose a small-scale annotation of diachronic metaphoric change. Synthetic evaluation procedures (ii) include studies that simulate LSC (Cook and Stevenson, 2010; Kulkarni et al., 2015; Rosenfeld and Erk, 2018), evaluate sense assignments in WordNet (Mitra et al., 2015; Frermann and Lapata, 2016), identify text creation dates, (Mihalcea and Nastase, 2012; Frermann and Lapata, 2016), or predict the log-likelihood of textual data (Frermann and Lapata, 2016). Overall, the va"
P19-1072,L16-1690,0,0.0235666,"Missing"
P19-1072,P18-1073,0,0.029735,"V > , where B > DA = U ΣV > is the SVD of B > DA. Hence, A and B are aligned by ACI ∗j = A∗j for all cj ∈ Va ∩ Vb , CI B∗j = B∗j for all cj ∈ Va ∩ Vb , AOP = A, where X∗j denotes the jth column of X. B OP = BW ∗ , where A and B correspond to their preprocessed versions. We also experiment with two variants: OP− omits mean-centering (Hamilton et al., 2016b), which is potentially harmful as a better solution may be found after mean-centering. OP+ corresponds to OP with additional pre- and postprocessing steps and has been shown to improve performance in research on bilingual lexicon induction (Artetxe et al., 2018a,b). We apply all OP variants only to the low-dimensional matrices. Shared Random Vectors (SRV). RI offers an elegant way to align count-based vector spaces and reduce their dimensionality at the same time (Basile et al., 2015). Instead of multiplying count matrices A and B each by a separate random matrix RA and RB they may be multiplied both by the same random matrix R representing them in the same low-dimensional random space. Hence, A and B are aligned by ASVR = AR, Vector Initialization (VI). In VI we first learn AVI using standard SGNS and then initialize the SGNS model for learning B V"
P19-1072,W17-6804,1,0.893969,"Missing"
P19-1072,P13-2140,0,0.0317579,"reover, for the diachronic task, synthetic datasets are used which do not reflect actual diachronic changes. Synchronic LSC Detection. We use the term synchronic LSC to refer to NLP research areas with a focus on how the meanings of words vary across domains or communities of speakers. Synchronic LSC per se is not widely researched; for meaning shifts across domains, there is strongly related research which is concerned with domainspecific word sense disambiguation (Maynard and Ananiadou, 1998; Chen and Al-Mubaid, 2006; Taghipour and Ng, 2015; Daille et al., 2016) or term ambiguity detection (Baldwin et al., 2013; Wang et al., 2013). The only notable work for explicitly measuring across domain meaning shifts is Ferrari et al. (2017), which is based on semantic vector spaces and cosine distance. Synchronic LSC across communities has been investigated as meaning variation in online communities, leverag3 Task and Data Our study makes use of the evaluation framework proposed in Schlechtweg et al. (2018), where diachronic LSC detection is defined as a comparison between word uses in two time-specific corpora. We further applied the framework to create an analogous synchronic LSC dataset that compares word"
P19-1072,W17-1202,0,0.0539523,"rotational alignment (Hamilton et al., 2016b; Artetxe et al., 2017). The optimal solution for this 736 Although ψ t,k may change over time for word sense k, senses are intended to remain thematically consistent as controlled by word precision parameter K ψ . This allows comparison of the topic distribution across time periods. For each target word w we infer a SCAN model for two time periods a and b and take φaw and φbw as the respective semantic representations. Jensen-Shannon Distance (JSD). JSD computes the distance between two probability distributions φx , φy of words wx , wy (Lin, 1991; Donoso and Sanchez, 2017). It is the symmetrized square root of the Kullback-Leibler divergence: 5 where M = (φx +φy )/2. JSD is high if φx and φy assign different probabilities to the same events. r JSD(φx ||φy ) = LSC Detection Measures LSC detection measures predict a degree of LSC from two time-specific semantic representations of a word w. They either capture the contextual similarity (Section 5.1) or changes in the contextual dispersion (Section 5.2) of w’s representations.6 5.1 5.2 Dispersion Measures Frequency Difference (FD). The logtransformed relative frequency of a word w for a corpus C is defined by Simil"
P19-1072,drouin-2004-detection,0,0.0805493,"ting models which consistently show superior performance. Meanwhile, the detection of lexical sense divergences across time-specific corpora is not the only possible application of LSC detection models. In more general terms, they have the potential to detect sense divergences between corpora of any type, not necessarily time-specific ones. We acknowledge this observation and further explore a synchronic LSC detection task: identifying domain-specific changes of word senses in comparison to general-language usage, which is addressed, e.g., in term identification and automatic term extraction (Drouin, 2004; P´erez, 2016; H¨atty and Schulte im Walde, 2018), and in determining social and dialectal language variations (Del Tredici and Fern´andez, 2017; Hovy and Purschke, 2018).2 For addressing the synchronic LSC task, we present a recent sense-specific term dataset (H¨atty et al., 2019) that we created analogously to the existing diachronic dataset, and we show that the diachronic models can be successfully applied to the synchronic task as well. This two-fold evaluation assures robustness and reproducibility of our model comparisons under various conditions. We perform an interdisciplinary large-"
P19-1072,P19-1044,1,0.560499,"Hence, the negative correlation in C OOK propagates to the final results. This is supported by the fact that the only measure not normalized by corpus size (HD) has a positive correlation. As these findings show, the dispersion measures are strongly influenced by frequency and very sensitive to different corpus sizes. Control Condition. As we saw, dispersion measures are sensitive to frequency. Similar obser9 We see the same tendency for WI against random indexing with a shared random space (SRV), but instead variable results for count and PPMI alignment (CI). This contradicts the findings in Dubossarsky et al. (2019), using, however, a different task and synthetic data. 10 JSD was not included here, as it was only applied to SCAN and its performance thus strongly depends on the underlying meaning representation. 739 Dataset DURel SURel vations have been made for other LSC measures (Dubossarsky et al., 2017). In order to test for this influence within our datasets we follow Dubossarsky et al. (2017) in adding a control condition to the experiments for which sentences are randomly shuffled across corpora (time periods). For each target word we merge all sentences from the two corpora Ca and Cb containing it"
P19-1072,P14-1023,0,0.078307,"#(c) |D |for each observation of (w,c), cf. Levy et al. (2015). SGNS and PPMI representations are highly related in that the cells of the implicitly factorized matrix M are PPMI values shifted by the constant k (Levy and Goldberg, 2014). Hence, SGNS and PPMI share the hyperparameter k. The final SGNS matrix is given by M SGNS = W, where the ith row of M SGNS corresponds to wi ’s d-dimensional semantic representation. As in RI we apply subsampling with a threshold t. SGNS with particular parameter configurations has shown to outperform transformed count-based techniques on a variety of tasks (Baroni et al., 2014; Levy et al., 2015). Ud Σpd , where p is an eigenvalue weighting parameter (Levy et al., 2015). The ith row of M SVD corresponds to wi ’s d-dimensional representation. Random Indexing (RI). RI is a dimensionality reduction technique based on the JohnsonLindenstrauss lemma according to which points in a vector space can be mapped into a randomly selected subspace under approximate preservation of the distances between points, if the subspace has a sufficiently high dimensionality (Johnson and Lindenstrauss, 1984; Sahlgren, 2004). We reduce the dimensionality of a count-based matrix M by multip"
P19-1072,D17-1118,0,0.34105,"lourishing new field within NLP (Frermann and Lapata, 2016; Hamilton et al., 2016b; Schlechtweg et al., 2017, i.a.).1 Yet, it is hard to compare the performances of the various models, and optimal parameter choices remain unclear, because up to now most models have been compared on different evaluation tasks and data. Presently, we do not know which model performs best under which conditions, and if more complex model architectures gain performance benefits over simpler models. This situation hinders advances in the field and favors unfelicitous drawings of statistical laws of diachronic LSC (Dubossarsky et al., 2017). In this study, we provide the first large-scale evaluation of an extensive number of approaches. 1 An example for diachronic LSC is the German noun Vorwort (Paul, 2002), which was mainly used in the meaning of ‘preposition’ before ≈1800. Then Vorwort rapidly acquired a new meaning ‘preface’, which after 1850 has nearly exclusively been used. 2 An example for domain-specific synchronic LSC is the German noun Form. In general-language use, Form means ‘shape’/‘form’, while in the cooking domain the predominant meaning is the domain-specific ‘baking tin’. 732 Proceedings of the 57th Annual Meeti"
P19-1072,W09-0214,0,0.0758404,"on their frequency of use. (iii) Clustering models assign all uses of a word into sense clusters based on some contextual property (Mitra et al., 2015). Word sense clustering models are similar to topic models in that they map uses to senses. Accordingly, LSC of a word is measured similarly as in (ii). For an overview on diachronic LSC detection, see Tahmasebi et al. (2018). Evaluation. Existing evaluation procedures for LSC detection can be distinguished into evaluation on (i) empirically observed data, and (ii) synthetic data or related tasks. (i) includes case studies of individual words (Sagi et al., 2009; Jatowt and Duh, 2014; Hamilton et al., 2016a), stand-alone comparison of a few hand-selected words (Wijaya and Yeniterzi, 2011; Hamilton et al., 2016b; Del Tredici and Fern´andez, 2017), comparison of hand-selected changing vs. semantically stable words (Lau et al., 2012; Cook et al., 2014), and post-hoc evaluation of the predictions of the presented models (Cook and Stevenson, 2010; Kulkarni et al., 2015; Del Tredici et al., 2016; Eger and Mehler, 2016; Ferrari et al., 2017). Schlechtweg et al. (2017) propose a small-scale annotation of diachronic metaphoric change. Synthetic evaluation pro"
P19-1072,E14-4008,1,0.843606,"First the cosine similarity of ~x, ~y with each vector in the union of the sets of their k nearest neighbors Nk (~x) and Nk (~y ) is computed and represented as a vector s whose entries are given by s(j) = cos(~x, ~zj ) DKL (φx ||M ) + DKL (φy ||M ) , 2 where |CT |is the number of types in corpus C. The TD of two vectors ~x and ~y in two corpora X and Y is the absolute difference in T: ∀~zj ∈ Nk (~x) ∪ Nk (~y ). T D(~x, X, ~y , Y ) = |T (~x, X) − T (~y , Y )|. LND is then computed as cosine distance between the two vectors: Entropy Difference (HD). HD relies on vector entropy as suggested by Santus et al. (2014). The entropy of a non-zero word vector w ~ is defined by LN D(~x, ~y ) = CD(s~x , s~y ). LND does not require matrix alignment, because it measures the distances to the nearest neighbors in each space separately. It was claimed to capture changes in paradigmatic rather than syntagmatic relations between words (Hamilton et al., 2016a). V H(w) ~ =− w ~i X P i=1 ~j j=1 w log P w ~i ~j j=1 w . VH is based on Shannon’s entropy (Shannon, 1948), which measures the unpredictability of w’s 6 Find an overview of which measure was applied to which representation type in Appendix A. 737 Dataset DURel SUR"
poesio-etal-2002-acquiring,J98-2001,1,\N,Missing
poesio-etal-2002-acquiring,W97-1301,1,\N,Missing
poesio-etal-2002-acquiring,E99-1001,0,\N,Missing
poesio-etal-2002-acquiring,P00-1051,0,\N,Missing
poesio-etal-2002-acquiring,P00-1023,0,\N,Missing
poesio-etal-2002-acquiring,J93-2002,0,\N,Missing
poesio-etal-2002-acquiring,J92-4003,0,\N,Missing
poesio-etal-2002-acquiring,P93-1032,0,\N,Missing
poesio-etal-2002-acquiring,J94-4002,0,\N,Missing
poesio-etal-2002-acquiring,P97-1072,1,\N,Missing
poesio-etal-2002-acquiring,J01-4003,0,\N,Missing
poesio-etal-2002-acquiring,P99-1008,0,\N,Missing
poesio-etal-2002-acquiring,J00-4003,1,\N,Missing
poesio-etal-2002-acquiring,P98-2143,0,\N,Missing
poesio-etal-2002-acquiring,C98-2138,0,\N,Missing
poesio-etal-2010-babyexp,J07-2002,0,\N,Missing
poesio-etal-2010-babyexp,P98-2127,0,\N,Missing
poesio-etal-2010-babyexp,C98-2122,0,\N,Missing
roth-schulte-im-walde-2008-corpus,poesio-etal-2002-acquiring,1,\N,Missing
roth-schulte-im-walde-2008-corpus,N07-1025,0,\N,Missing
roth-schulte-im-walde-2008-corpus,J07-2002,0,\N,Missing
roth-schulte-im-walde-2008-corpus,J06-2001,1,\N,Missing
roth-schulte-im-walde-2008-corpus,J98-1004,0,\N,Missing
roth-schulte-im-walde-2008-corpus,P98-2127,0,\N,Missing
roth-schulte-im-walde-2008-corpus,C98-2122,0,\N,Missing
S13-1038,N09-1003,0,0.0959637,"vector space models are all nouns (i.e., the compound nouns, the modifier nouns, and the head nouns), our hypothesis is that adjectives and verbs are expected to provide salient distributional properties, as adjective/verb meaning and noun meaning are in a strong interdependent relationship. Even more, we expect adjectives and verbs that are syntactically bound to the nouns under consideration (syntax-based, i.e., attributive adjectives and subcategorising verbs) to outperform those that “just” appear in the window contexts of the nouns (window-based). In order to investigate this first 1 See Agirre et al. (2009) and Bullinaria and Levy (2007; 2012), among others, for systematic comparisons of cooccurrence features on various semantic relatedness tasks. 255 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 255–265, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics hypothesis, we compare window-based and syntaxbased distributional features across parts-of-speech. Concerning a more specific aspect of compound meaning, we are interested in the contributions of the modifier noun"
S13-1038,D10-1115,0,0.0308106,"lding, 2009) and the modifier properties, inferential processing and world knowledge (Gagn´e and Spalding, 2011) were taken into account. However, also in psycholinguistic studies that explore the semantic role of modifiers and heads in noun compounds there is no agreement about which constituent properties are inherited by the compound. 4 Related Work Most computational approaches to model the meaning or compositionality of compounds have been performed for English, including work on particle verbs (McCarthy et al., 2003; Bannard, 2005; Cook and Stevenson, 2006); adjective-noun combinations (Baroni and Zamparelli, 2010; Boleda et al., 2013); and noun-noun compounds (Reddy et 263 al., 2011b; Reddy et al., 2011a). Most closely related to our work is Reddy et al. (2011b), who relied on window-based distributional models to predict the compositionality of English noun-noun compounds. Their gold standard also comprised compound–constituent ratings as well as compound whole ratings, but the resources had been cleaned more extensively, and they reached ρ = 0.714. Concerning vector space explorations and semantic relatedness in more general terms, Bullinaria and Levy (2007; 2012) also systematically assessed a rang"
S13-1038,W06-1207,0,0.114599,"lation between the modifier and the head (Gagn´e and Spalding, 2009) and the modifier properties, inferential processing and world knowledge (Gagn´e and Spalding, 2011) were taken into account. However, also in psycholinguistic studies that explore the semantic role of modifiers and heads in noun compounds there is no agreement about which constituent properties are inherited by the compound. 4 Related Work Most computational approaches to model the meaning or compositionality of compounds have been performed for English, including work on particle verbs (McCarthy et al., 2003; Bannard, 2005; Cook and Stevenson, 2006); adjective-noun combinations (Baroni and Zamparelli, 2010; Boleda et al., 2013); and noun-noun compounds (Reddy et 263 al., 2011b; Reddy et al., 2011a). Most closely related to our work is Reddy et al. (2011b), who relied on window-based distributional models to predict the compositionality of English noun-noun compounds. Their gold standard also comprised compound–constituent ratings as well as compound whole ratings, but the resources had been cleaned more extensively, and they reached ρ = 0.714. Concerning vector space explorations and semantic relatedness in more general terms, Bullinaria"
S13-1038,W03-1810,0,0.450217,"arious factors such as the semantic relation between the modifier and the head (Gagn´e and Spalding, 2009) and the modifier properties, inferential processing and world knowledge (Gagn´e and Spalding, 2011) were taken into account. However, also in psycholinguistic studies that explore the semantic role of modifiers and heads in noun compounds there is no agreement about which constituent properties are inherited by the compound. 4 Related Work Most computational approaches to model the meaning or compositionality of compounds have been performed for English, including work on particle verbs (McCarthy et al., 2003; Bannard, 2005; Cook and Stevenson, 2006); adjective-noun combinations (Baroni and Zamparelli, 2010; Boleda et al., 2013); and noun-noun compounds (Reddy et 263 al., 2011b; Reddy et al., 2011a). Most closely related to our work is Reddy et al. (2011b), who relied on window-based distributional models to predict the compositionality of English noun-noun compounds. Their gold standard also comprised compound–constituent ratings as well as compound whole ratings, but the resources had been cleaned more extensively, and they reached ρ = 0.714. Concerning vector space explorations and semantic rel"
S13-1038,I11-1079,0,0.367119,"compounds are not particularly skewed to any area within the range.4 Figure 2 again shows the mean ratings for the compounds as a whole as well as for the compound– constituent pairs, but in this case only the compound whole ratings were sorted, and the compound– constituent ratings were plotted against the compound whole ratings. According to the plot, the compound–modifier ratings (red) seem to correlate better with the compound whole ratings than the compound–head ratings (yellow) do. This intuition will be confirmed in Section 3.1. 4 www.mturk.com 257 The illustration idea was taken from Reddy et al. (2011b). Figure 2: Compounds ratings sorted by whole ratings. 3 Vector Space Models (VSMs) The goal of our vector space models is to identify distributional features that are salient to predict the degree of compositionality of the compounds, by relying on the similarities between the compound and constituent properties. In all our vector space experiments, we used cooccurrence frequency counts as induced from German web corpora, and calculated local mutual information (LMI)5 values (Evert, 2005), to instantiate the empirical properties of our target nouns with regard to the various corpus-based fe"
S13-1038,I11-1024,0,0.619371,"compounds are not particularly skewed to any area within the range.4 Figure 2 again shows the mean ratings for the compounds as a whole as well as for the compound– constituent pairs, but in this case only the compound whole ratings were sorted, and the compound– constituent ratings were plotted against the compound whole ratings. According to the plot, the compound–modifier ratings (red) seem to correlate better with the compound whole ratings than the compound–head ratings (yellow) do. This intuition will be confirmed in Section 3.1. 4 www.mturk.com 257 The illustration idea was taken from Reddy et al. (2011b). Figure 2: Compounds ratings sorted by whole ratings. 3 Vector Space Models (VSMs) The goal of our vector space models is to identify distributional features that are salient to predict the degree of compositionality of the compounds, by relying on the similarities between the compound and constituent properties. In all our vector space experiments, we used cooccurrence frequency counts as induced from German web corpora, and calculated local mutual information (LMI)5 values (Evert, 2005), to instantiate the empirical properties of our target nouns with regard to the various corpus-based fe"
S13-1038,W13-1005,1,0.787647,"d. For each of the compounds we calculated the rating mean and the standard deviation. We refer to this second set as our compound whole ratings. Table 1 presents example mean ratings for the compound–constituent ratings as well as for the compound whole ratings, accompanied by the standard deviations. We selected two examples each for five categories of mean ratings: the compound– constituent ratings were (1) high or (2) mid or (3) low with regard to both constituents; the compound– constituent ratings were (4) low with regard to the modifier but high with regard to the head; (5) vice versa. Roller et al. (2013) performed a thorough 3 Figure 1: Distribution of compound ratings. analysis of the two sets of ratings, and assessed their reliability from several perspectives. Figure 1 shows how the mean ratings for the compounds as a whole, for the compound–modifier pairs as well as for the compound–head pairs are distributed over the range [1, 7]: For each set, we independently sorted the 244 values and plotted them. The purpose of the figure is to illustrate that the ratings for our 244 noun-noun compounds are not particularly skewed to any area within the range.4 Figure 2 again shows the mean ratings f"
S13-1038,E03-1087,0,0.0397999,"e arithmetic operations strengthen the predictions, and multiplication reached an upper bound of ρ = 0.7829, thus outperforming not only the head-only but also the modifier-only upper bound. 3.2 German Web Corpora Most of our experiments rely on the sdeWaC corpus (Faaß et al., 2010), a cleaned version of the German web corpus deWaC created by the WaCky group (Baroni et al., 2009). The corpus cleaning had focused mainly on removing duplicates from the deWaC, and on disregarding sentences that were syntactically illformed (relying on a parsability index provided by a standard dependency parser (Schiehlen, 2003)). The sdeWaC contains approx. 880 million words and can be downloaded from http://wacky.sslmit. unibo.it/. While the sdeWaC is an attractive corpus choice because it is a web corpus with a reasonable size, and yet has been cleaned and parsed (so that we can induce syntax-based distributional features), it has one serious drawback for a window-based approach (and, in general, for corpus work going beyond the sentence border): The sentences in the corpus have been sorted alphabetically, so going be259 yond the sentence border is likely to entering a sentence that did not originally precede or f"
S13-1038,J90-1003,0,\N,Missing
S13-1038,faass-etal-2010-design,0,\N,Missing
S14-1020,W09-3207,0,0.0700477,"Missing"
S14-1020,J10-4006,0,0.615986,"s. syntagmatic relations. 1 Introduction Distributional takes on the representation and acquisition of word meaning rely on the assumption that words with similar meaning tend to occur in similar contexts: this assumption, known as distributional hypothesis, has been first proposed by Harris (1954). Distributional Semantic Models (henceforth, DSMs) are computational models that operationalize the distributional hypothesis; they produce semantic representations for words in the form of distributional vectors recording patterns of co-occurrence in large samples of language data (Sahlgren, 2006; Baroni and Lenci, 2010; Turney and Pantel, 2010). Comparison between distributional vectors allows the identification of shared contexts as an empirical correlate of 160 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 160–170, Dublin, Ireland, August 23-24 2014. labels syntagmatic and paradigmatic to characterize different types of semantic relations, and we will use the labels first-order and second-order to characterize corpus-based models with respect to the kind of co-occurrence information they encode. We will refer to collocation lists and termdocument DSMs"
S14-1020,W11-2501,0,0.0505922,"different semantic relations, namely: synonyms (SYN), 436 triples (example of a consistent prime and target: frigid–cold); antonyms (ANT): 135 triples (e.g., hot–cold); cohyponyms (COH): 159 triples (e.g., table–chair); forward phrasal associates (FPA): 144 triples (e.g., help–wanted); back2 3.2 Evaluated Parameters DSMs evaluated in this paper belong to the class of bag-of-words models. We defined a large vocabulary of target words (27522 lemma types) containing all the items from the evaluated datasets as well as items from other state-of-the-art evaluation studies (Baroni and Lenci, 2010; Baroni and Lenci, 2011). Context words were filtered by partof-speech (nouns, verbs, adjectives, and adverbs). Distributional models were built using the UCS toolkit3 and the wordspace package for R4 . The following parameters have been evaluated: • Source corpus (abbreviated as corpus in plots 1-4): We compiled DSMs from three corpora often used in DSM evaluation studies and that 3 4 The dataset is available at http://spp.montana.edu/ 162 http://www.collocations.de/software.html http://r-forge.r-project.org/projects/wordspace/ differ in both size and quality: British National Corpus5 , ukWaC, and WaCkypedia EN6 . •"
S14-1020,D10-1115,0,0.0421147,"matic Paradigmatic Paradigmatic Paradigmatic GEK FPA BPA SYN COH ANT Unreduced Min Max Mean 54.8 98.4 86.6 41.0 98.0 82.3 49.4 97.7 83.8 54.8 98.4 86.6 49.0 100.0 92.6 69.6 100.0 94.2 Reduced Min Max Mean 48.0 97.0 80.8 43.0 98.6 82.1 41.6 98.9 83.9 57.3 99.0 88.2 54.3 100.0 94.0 57.8 100.0 94.3 Table 2: Distribution of Accuracy alization encoded in the reduced dimensions) is irrelevant to other tasks, but crucial for modeling the relations in the GEK dataset. This interpretation is consistent with the detrimental effect of SVD in tasks involving vector composition reported in the literature (Baroni and Zamparelli, 2010). 5.1 Importance of Parameters To obtain further insights into DSM performance we explore the effect of specific model parameters, comparing syntagmatic vs. paradigmatic relations and reduced vs. unreduced runs. In order to establish a ranking of the parameters according to their importance wrt. model performance, we use a feature ablation approach. The ablation value for a given parameter is the proportion of variance (R2 ) explained by this parameter together with all its interactions, corresponding to the reduction in adjusted R2 of the linear model fit if the parameter were left out. In ot"
S14-1020,W13-2608,1,0.935349,"Summing up, in both Rapp (2002) and Sahlgren (2006) it is claimed that second-order models perform poorly in predicting syntagmatic relations. However, neither of those studies involves datasets containing exclusively syntagmatic relations, as the evaluation focuses either on paradigmatic relations (TOEFL multiple choice test, antonymy test) or on resources containing both types of relations (thesauri, association norms). 3 ward phrasal associates (BPA): 89 triples (e.g., wanted–help). The second priming dataset is the Generalized Event Knowledge dataset (henceforth GEK), already evaluated in Lapesa and Evert (2013): a collection of 402 triples (target, consistent prime, inconsistent prime) from three priming studies conducted to demonstrate that event knowledge is responsible for facilitation of the processing of words that denote events and their participants. The first study was conducted by Ferretti et al. (2001), who found that verbs facilitate the processing of nouns denoting prototypical participants in the depicted event and of adjectives denoting features of prototypical participants. The study covered five thematic relations: agent (e.g., pay–customer), patient, feature of the patient, instrume"
S14-1020,S12-1012,0,0.0190868,"lection methodology. Section 5 presents the results of our evaluation study. Section 6 summarizes main findings and sketches ongoing and future work. 2 Previous Work In this section we discuss previous work relevant to the distributional modeling of paradigmatic and syntagmatic relations. For space constraints, we focus only on two studies (Rapp, 2002; Sahlgren, 2006) in which the two classes of relations are compared at a global level, and not on studies that are concerned with specific semantic relations, e.g., synonymy (Edmonds and Hirst, 2002; Curran, 2003), hypernymy (Weeds et al., 2004; Lenci and Benotto, 2012) or syntagmatic predicate preferences (McCarthy and Carroll, 2003; Erk et al., 2010), etc. In previous studies, the comparison of syntagmatic and paradigmatic relations has been implemented in terms of an opposition between different classes of corpus-based models: term-context models (words as targets, documents or context regions as features) vs. bag-of-words models (words as targets and features) in Sahlgren (2006); collocation lists vs. bag-of-words models in Rapp (2002). Given the high terminological variation in the literature, in this paper we will adopt the 1 Term-document models encod"
S14-1020,J03-4004,0,0.0554907,"uation study. Section 6 summarizes main findings and sketches ongoing and future work. 2 Previous Work In this section we discuss previous work relevant to the distributional modeling of paradigmatic and syntagmatic relations. For space constraints, we focus only on two studies (Rapp, 2002; Sahlgren, 2006) in which the two classes of relations are compared at a global level, and not on studies that are concerned with specific semantic relations, e.g., synonymy (Edmonds and Hirst, 2002; Curran, 2003), hypernymy (Weeds et al., 2004; Lenci and Benotto, 2012) or syntagmatic predicate preferences (McCarthy and Carroll, 2003; Erk et al., 2010), etc. In previous studies, the comparison of syntagmatic and paradigmatic relations has been implemented in terms of an opposition between different classes of corpus-based models: term-context models (words as targets, documents or context regions as features) vs. bag-of-words models (words as targets and features) in Sahlgren (2006); collocation lists vs. bag-of-words models in Rapp (2002). Given the high terminological variation in the literature, in this paper we will adopt the 1 Term-document models encode first-order information because dot products between row vector"
S14-1020,J02-2001,0,0.0634824,"k, datasets and evaluated parameters. Section 4 introduces our model selection methodology. Section 5 presents the results of our evaluation study. Section 6 summarizes main findings and sketches ongoing and future work. 2 Previous Work In this section we discuss previous work relevant to the distributional modeling of paradigmatic and syntagmatic relations. For space constraints, we focus only on two studies (Rapp, 2002; Sahlgren, 2006) in which the two classes of relations are compared at a global level, and not on studies that are concerned with specific semantic relations, e.g., synonymy (Edmonds and Hirst, 2002; Curran, 2003), hypernymy (Weeds et al., 2004; Lenci and Benotto, 2012) or syntagmatic predicate preferences (McCarthy and Carroll, 2003; Erk et al., 2010), etc. In previous studies, the comparison of syntagmatic and paradigmatic relations has been implemented in terms of an opposition between different classes of corpus-based models: term-context models (words as targets, documents or context regions as features) vs. bag-of-words models (words as targets and features) in Sahlgren (2006); collocation lists vs. bag-of-words models in Rapp (2002). Given the high terminological variation in the"
S14-1020,J10-4007,0,0.0759452,"Missing"
S14-1020,P04-1003,0,0.0323045,"group synonyms with antonyms and cohyponyms from SPP as paradigmatic relations, and the entire GEK dataset with backward and forward phrasal associates from SPP as syntagmatic relations. Experimental Setting 3.1 Evaluation Task and Data In this study, bag-of-words DSMs are evaluated on two datasets containing experimental items from two priming studies. Each item is a word triple (target, consistent prime, inconsistent prime) with a particular semantic relation between target and consistent prime. Following previous work on modeling priming effects as a comparison between prime-target pairs (McDonald and Brew, 2004; Pad´o and Lapata, 2007; Herda˘gdelen et al., 2009), we evaluate our models in a classification task. The goal is to identify the consistent prime on the basis of its distributional relatedness to the target: if a particular DSM (i.e., a certain parameter combination) is sensitive to a specific relation (or group of relations), we expect the consistent primes to be closer to the target in semantic space than the inconsistent ones. The first dataset is derived from the Semantic Priming Project (SPP) (Hutchison et al., 2013). To the best of our knowledge, our study represents the first evaluati"
S14-1020,J07-2002,0,0.252789,"Missing"
S14-1020,C02-1007,0,0.720429,"they are also called relations in absentia (Sahlgren, 2006) because paradigmatically related words do not co-occur. Examples of paradigmatic relations are synonyms (e.g., frigid–cold) and antonyms (e.g., cold–hot). Syntagmatic relations hold between words that cooccur (relations in praesentia) and therefore exhibit a similar distribution across contexts. Typical examples of syntagmatic relations are phrasal associates (e.g., help–wanted) and syntactic collocations (e.g., dog–bark). Distributional modeling has already tackled the issue of paradigmatic and syntagmatic relations (Sahlgren, 2006; Rapp, 2002). Key contributions of the present work are the scope of its evaluation (in terms of semantic relations and model parameters) and the new perspective on paradigmatic vs. syntagmatic models provided by our results. Concerning the scope of the evaluation, this is the first study in which the comparison involves such a wide range of semantic relations (paradigmatic: synonyms, antonyms and co-hyponyms; syntagmatic: syntactic collocations, backward and forward phrasal associates). Moreover, our evaluation covers a large number of DSM parameters: source corpus, size and direction of the context wind"
S14-1020,I13-1056,1,0.831603,"g experiments. The leading theme of our study is a comparison between syntagmatic and paradigmatic relations in terms of the aspects of distributional similarity that characterize them. Our results show that second-order DSMs are capable of capturing both syntagmatic and paradigmatic relations, if parameters are properly tuned. Size of the co-occurrence window as well as parameters connected to dimensionality reduction play a key role in adapting DSMs to particular relations. Even if we do not address the more specific task of distinguishing between relations (e.g., synonyms vs. antonyms; see Scheible et al. (2013) and references therein), we believe that such applications may benefit from our detailed analyses on the effects of DSM parameters. Ongoing and future work is concerned with the expansion of the evaluation setting to other classes of models (first-order models, dependency-based second-order models) and parameters (e.g., dimensionality reduction with Random Indexing). corpus win score transf r.dim d.sk acc best ukwac 16 s-ll log 900 50 96.0 97.0 ukwac 8 z-sc root 900 0 93.0 98.6 ukwac 8 z-sc root 900 0 95.5 98.9 ukwac 4 s-ll log 900 50 96.3 99.0 ukwac 4 s-ll log 900 50 98.7 100 wacky 8 s-ll lo"
S14-1020,J98-1004,0,0.596471,"Missing"
S14-1020,C04-1146,0,0.103187,"Missing"
S14-1022,aldinger-2004-towards,0,0.544583,"verbs have been studied from the theoretical perspective and, to a more limited extend, from the aspect of the computational predictability of the degree of semantic compositionality (the transparency of their meaning with respect to the meaning of the base verb and the particle) and the semantic classifiabilty of PVs. For English, there is work on the automatic extraction of PVs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005) and the determination of compositionality (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). To the best of our knowledge Aldinger (2004) is the first work that studies German PVs from a corpus based perspective, with an emphasis on the syntactic behavior and syntactic change. Schulte im Walde (2004), Schulte im Walde (2005) and Schulte im Walde (2006b) present preliminary distributional studies to explore salient features at the syntax-semantics interface that determine the semantic nearest neighbours of German PVs. Relying on the insights of those studies, Schulte im Walde (2006b) and Hartmann (2008) describe experiments which model the subcategorization transfer of German PVs with respect to their BVs in order to strengthen"
S14-1022,P03-1009,0,0.0425272,"be found: Er hatte an der Wand angelauscht und wusste Bescheid. (‘He had listened at the wall and knew everything.’) 182 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 182–192, Dublin, Ireland, August 23-24 2014. relies on syntax and the gold standard reflects semantic regularities. tactic subcategorization frames tend to be good predictors for the semantics of verbs in general: verbs that are similar in meaning also tend to have similar subcategorization frames and selectional preferences (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006a; Joanis et al., 2008). But, as we will show below, PV-BV pairs tend to have a special behavior with respect to their subcategorization, even if their meanings are closely related. Because we are interested in pairs of PVs and their BVs, we thus have to look at pairs of subcategorization preferences, and rely on the concept of syntactic transfer. We use syntactic transfer as a technical term here, which we define as regular changes in subcategorization frames by PVs and corresponding BVs, e.g., the incorporation or addition of complements of PVs in comparison to their"
S14-1022,W02-2001,0,0.133464,"nd the semantic classes of PVs. There is only one more point to make: the classes shown in (2), could actually be seen as reflecting different meanings of the particle an itself. 3 Related Work Particle verbs have been studied from the theoretical perspective and, to a more limited extend, from the aspect of the computational predictability of the degree of semantic compositionality (the transparency of their meaning with respect to the meaning of the base verb and the particle) and the semantic classifiabilty of PVs. For English, there is work on the automatic extraction of PVs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005) and the determination of compositionality (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). To the best of our knowledge Aldinger (2004) is the first work that studies German PVs from a corpus based perspective, with an emphasis on the syntactic behavior and syntactic change. Schulte im Walde (2004), Schulte im Walde (2005) and Schulte im Walde (2006b) present preliminary distributional studies to explore salient features at the syntax-semantics interface that determine the semantic nearest neighbours of German PVs. Relying on the insights of t"
S14-1022,W03-1812,0,0.349659,"fferent meanings of the particle an itself. 3 Related Work Particle verbs have been studied from the theoretical perspective and, to a more limited extend, from the aspect of the computational predictability of the degree of semantic compositionality (the transparency of their meaning with respect to the meaning of the base verb and the particle) and the semantic classifiabilty of PVs. For English, there is work on the automatic extraction of PVs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005) and the determination of compositionality (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). To the best of our knowledge Aldinger (2004) is the first work that studies German PVs from a corpus based perspective, with an emphasis on the syntactic behavior and syntactic change. Schulte im Walde (2004), Schulte im Walde (2005) and Schulte im Walde (2006b) present preliminary distributional studies to explore salient features at the syntax-semantics interface that determine the semantic nearest neighbours of German PVs. Relying on the insights of those studies, Schulte im Walde (2006b) and Hartmann (2008) describe experiments which model the subcategorization transfer o"
S14-1022,W03-1810,0,0.673213,"e seen as reflecting different meanings of the particle an itself. 3 Related Work Particle verbs have been studied from the theoretical perspective and, to a more limited extend, from the aspect of the computational predictability of the degree of semantic compositionality (the transparency of their meaning with respect to the meaning of the base verb and the particle) and the semantic classifiabilty of PVs. For English, there is work on the automatic extraction of PVs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005) and the determination of compositionality (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). To the best of our knowledge Aldinger (2004) is the first work that studies German PVs from a corpus based perspective, with an emphasis on the syntactic behavior and syntactic change. Schulte im Walde (2004), Schulte im Walde (2005) and Schulte im Walde (2006b) present preliminary distributional studies to explore salient features at the syntax-semantics interface that determine the semantic nearest neighbours of German PVs. Relying on the insights of those studies, Schulte im Walde (2006b) and Hartmann (2008) describe experiments which model the subcat"
S14-1022,P99-1014,0,0.121009,"pt for PP-modifiers) in the representation of subcat frames. We did not use information on subjects, because in German all verbs have subjects, which may be implicit in the case of subordinate clauses. We found that for this reason that with the representation of subjects in the extracted features no relevant information was 4.4 Clustering Methods For the clustering experiments we used two different clustering algorithms: K-means and Latent Semantic Classes (LSC). K-means is a standard flat, hard-clustering algorithm; we used the Weka implementation (Witten and Frank, 2005). LSC (Rooth, 1998; Rooth et al., 1999) is a two-dimensional soft-clustering algorithm which learns three probability distributions: one for the clusters, and one for the output probabilities of each element and for each feature type with regard to a cluster. The latter two (elements and features) correspond to the two dimensions of the clustering. In our case the elements are the PV-BV pairs, and the features are normalized counts of the subcategorization frames. 4.5 Evaluation Our feature vectors are a combination of the feature vector for the BV and the feature vector for the PV of each PV-BV pair. Since the length of each vecto"
S14-1022,C00-2108,1,0.550857,"le, senentence like the following could be found: Er hatte an der Wand angelauscht und wusste Bescheid. (‘He had listened at the wall and knew everything.’) 182 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 182–192, Dublin, Ireland, August 23-24 2014. relies on syntax and the gold standard reflects semantic regularities. tactic subcategorization frames tend to be good predictors for the semantics of verbs in general: verbs that are similar in meaning also tend to have similar subcategorization frames and selectional preferences (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006a; Joanis et al., 2008). But, as we will show below, PV-BV pairs tend to have a special behavior with respect to their subcategorization, even if their meanings are closely related. Because we are interested in pairs of PVs and their BVs, we thus have to look at pairs of subcategorization preferences, and rely on the concept of syntactic transfer. We use syntactic transfer as a technical term here, which we define as regular changes in subcategorization frames by PVs and corresponding BVs, e.g., the incorporation or addi"
S14-1022,W04-2118,1,0.811335,"compositionality (the transparency of their meaning with respect to the meaning of the base verb and the particle) and the semantic classifiabilty of PVs. For English, there is work on the automatic extraction of PVs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005) and the determination of compositionality (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). To the best of our knowledge Aldinger (2004) is the first work that studies German PVs from a corpus based perspective, with an emphasis on the syntactic behavior and syntactic change. Schulte im Walde (2004), Schulte im Walde (2005) and Schulte im Walde (2006b) present preliminary distributional studies to explore salient features at the syntax-semantics interface that determine the semantic nearest neighbours of German PVs. Relying on the insights of those studies, Schulte im Walde (2006b) and Hartmann (2008) describe experiments which model the subcategorization transfer of German PVs with respect to their BVs in order to strengthen PV-BV distributional similarity. The main goal for them is to use transfer information in order to predict the degree of semantic compositionality of PVs. K¨uhner a"
S14-1022,J06-2001,1,0.921044,"elauscht und wusste Bescheid. (‘He had listened at the wall and knew everything.’) 182 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 182–192, Dublin, Ireland, August 23-24 2014. relies on syntax and the gold standard reflects semantic regularities. tactic subcategorization frames tend to be good predictors for the semantics of verbs in general: verbs that are similar in meaning also tend to have similar subcategorization frames and selectional preferences (Schulte im Walde, 2000; Merlo and Stevenson, 2001; Korhonen et al., 2003; Schulte im Walde, 2006a; Joanis et al., 2008). But, as we will show below, PV-BV pairs tend to have a special behavior with respect to their subcategorization, even if their meanings are closely related. Because we are interested in pairs of PVs and their BVs, we thus have to look at pairs of subcategorization preferences, and rely on the concept of syntactic transfer. We use syntactic transfer as a technical term here, which we define as regular changes in subcategorization frames by PVs and corresponding BVs, e.g., the incorporation or addition of complements of PVs in comparison to their BVs (Stiebels, 1996; L¨u"
S14-1022,springorum-etal-2012-automatic,1,0.806961,"hat the gold standard used here is a valid representation of human language intuition. Most importantly, the annotators did not use syntactic criteria tion in the form of dependency arc labels (concatenated with the head nouns) does not yield satisfactory results, putting the syntactic transfer problem in evidence, again. They conclude that an incorporation of syntactic transfer information between BVs and PVs could possibly improve the results. Based on a theoretical study (Springorum, 2011), which explains particle meanings in terms of Discourse Representation Theory (Kamp and Reyle, 1993), Springorum et al. (2012) show that four classes of PVs with the particle an can be classified automatically. They take a supervised approach using decision trees. The use of decision trees also allows them to manually inspect and analyze the decisions made by the classifier. As predictive features they use the head nouns of objects, generalized classes of these nouns and PP types. The approach we take here is not fully comparable to any of the former approaches, since we try to derive a semantic classification BV-PP pairs in an unsupervised manner and we only use syntactic features, stemming from corpus instances of"
S14-1022,J01-3003,0,\N,Missing
S16-2010,C10-1011,0,0.0229373,"pment empty - emptiness religion - religious sport - sporty dispensable - indispensable write - rewrite familiar - unfamiliar Inst. 227 295 874 103 330 687 294 422 155 172 1,897 215 652 207 454 151 136 178 Table 3: English dataset (Lazaridou et al., 2013). 2.2 Word Embedding Vectors We relied on the German and English COW web corpora2 (Schäfer and Bildhauer, 2012) to obtain vector representations. The corpora contain 20 billion words and 9 billion words, respectively. We parsed the corpora using state-of-the-art pipelines integrating the MarMoT tagger and the MATE parser (Müller et al., 2013; Bohnet, 2010), and induced window co-occurrences for all corpus lemma–POS pairs and co-occurring nouns, verbs and adjectives in a 5-lemma window. We then created 400-dimensional word representations using the hyperwords toolkit (Levy et al., 2015), with context distribution smoothing of 0.75 and positive point-wise mutual information weighting together with singular value decomposition. The resulting vector space models contain approximately 460 000 lemmas for German and 240 000 lemmas for English. 2.1 Derivation Datasets We created a new collection of German particle verb derivations1 relying on the same"
S16-2010,D13-1032,0,0.0232703,"agonally equip - equipment empty - emptiness religion - religious sport - sporty dispensable - indispensable write - rewrite familiar - unfamiliar Inst. 227 295 874 103 330 687 294 422 155 172 1,897 215 652 207 454 151 136 178 Table 3: English dataset (Lazaridou et al., 2013). 2.2 Word Embedding Vectors We relied on the German and English COW web corpora2 (Schäfer and Bildhauer, 2012) to obtain vector representations. The corpora contain 20 billion words and 9 billion words, respectively. We parsed the corpora using state-of-the-art pipelines integrating the MarMoT tagger and the MATE parser (Müller et al., 2013; Bohnet, 2010), and induced window co-occurrences for all corpus lemma–POS pairs and co-occurring nouns, verbs and adjectives in a 5-lemma window. We then created 400-dimensional word representations using the hyperwords toolkit (Levy et al., 2015), with context distribution smoothing of 0.75 and positive point-wise mutual information weighting together with singular value decomposition. The resulting vector space models contain approximately 460 000 lemmas for German and 240 000 lemmas for English. 2.1 Derivation Datasets We created a new collection of German particle verb derivations1 relyi"
S16-2010,schafer-bildhauer-2012-building,0,0.0190159,"ss -ly -ment -ness -ous -y inreunbelieve - believable doctor - doctoral repeat - repeater use - useful algorithm - algorithmic erupt - eruption drama - dramatist accessible - accessibility cannibal - cannibalize word - wordless diagonal - diagonally equip - equipment empty - emptiness religion - religious sport - sporty dispensable - indispensable write - rewrite familiar - unfamiliar Inst. 227 295 874 103 330 687 294 422 155 172 1,897 215 652 207 454 151 136 178 Table 3: English dataset (Lazaridou et al., 2013). 2.2 Word Embedding Vectors We relied on the German and English COW web corpora2 (Schäfer and Bildhauer, 2012) to obtain vector representations. The corpora contain 20 billion words and 9 billion words, respectively. We parsed the corpora using state-of-the-art pipelines integrating the MarMoT tagger and the MATE parser (Müller et al., 2013; Bohnet, 2010), and induced window co-occurrences for all corpus lemma–POS pairs and co-occurring nouns, verbs and adjectives in a 5-lemma window. We then created 400-dimensional word representations using the hyperwords toolkit (Levy et al., 2015), with context distribution smoothing of 0.75 and positive point-wise mutual information weighting together with singul"
S16-2010,W15-0108,1,0.844025,"erns (e.g., use → use + f ul ) as the result of a compositional process, where base term and affix are combined. We exploit such models for German particle verbs (PVs), and focus on the task of learning a mapping function between base verbs and particle verbs. Our models apply particle-verb motivated training-space restrictions relying on nearest neighbors, as well as recent advances from zeroshot-learning. The models improve the mapping between base terms and derived terms for a new PV derivation dataset, and also across existing derivation datasets for German and English. The experiments by Kisselew et al. (2015) were performed over six derivational patterns for German (cf. Table 1), including particle verbs (PVs) with two different particle prefixes (an and durch), which were particularly difficult to predict. PVs such as anfangen (to start) are compositions of a base verb (BV) such as fangen (to catch) and a verb particle such as an. Predicting PV meaning is challenging because German PVs are highly productive (Springorum et al., 2013b; Springorum et al., 2013a), and the particles are notoriously ambiguous (Lechler and Roßdeutscher, 2009; Haselbach, 2011; Kliche, 2011; Springorum, 2011). Furthermore"
S16-2010,P13-1149,0,0.139149,"o predict. PVs such as anfangen (to start) are compositions of a base verb (BV) such as fangen (to catch) and a verb particle such as an. Predicting PV meaning is challenging because German PVs are highly productive (Springorum et al., 2013b; Springorum et al., 2013a), and the particles are notoriously ambiguous (Lechler and Roßdeutscher, 2009; Haselbach, 2011; Kliche, 2011; Springorum, 2011). Furthermore, the particles often trigger meaning shifts when they combine with base verbs (Springorum et al., 2013b), so the resulting PVs represent frequent cases of non-literal meaning. 1 Introduction Lazaridou et al. (2013) were the first to apply distributional semantic models (DSMs) to the task of deriving the meaning of morphologically complex words from their parts. They relied on high-dimensional vector representations to model the derived term (e.g., useful) as a result of a compositional process that combines the meanings of the base term (e.g., to use) and the affix (e.g., ful). For evaluation, they compared the predicted vector of the complex word with the original, corpus-based vector. More recently, Kisselew et al. (2015) put the task of modeling derivation into the perspective of zero-shot-learning:"
S16-2010,W13-0120,1,0.633061,"he mapping between base terms and derived terms for a new PV derivation dataset, and also across existing derivation datasets for German and English. The experiments by Kisselew et al. (2015) were performed over six derivational patterns for German (cf. Table 1), including particle verbs (PVs) with two different particle prefixes (an and durch), which were particularly difficult to predict. PVs such as anfangen (to start) are compositions of a base verb (BV) such as fangen (to catch) and a verb particle such as an. Predicting PV meaning is challenging because German PVs are highly productive (Springorum et al., 2013b; Springorum et al., 2013a), and the particles are notoriously ambiguous (Lechler and Roßdeutscher, 2009; Haselbach, 2011; Kliche, 2011; Springorum, 2011). Furthermore, the particles often trigger meaning shifts when they combine with base verbs (Springorum et al., 2013b), so the resulting PVs represent frequent cases of non-literal meaning. 1 Introduction Lazaridou et al. (2013) were the first to apply distributional semantic models (DSMs) to the task of deriving the meaning of morphologically complex words from their parts. They relied on high-dimensional vector representations to model the"
S16-2010,P15-1027,0,0.0327905,"hinelle Sprachverarbeitung, Universität Stuttgart Pfaffenwaldring 5B, 70569 Stuttgart, Germany {koepermn,schulte,kisselmx,pado}@ims.uni-stuttgart.de Abstract vector was computed, a nearest neighbor search was applied to validate if the prediction corresponded to the derived term. In zero-shotlearning the task is to predict novel values, i.e., values that were never seen in training. More formally, zero-shot-learning trains a classifier f : X → Y that predicts novel values for Y (Palatucci et al., 2009). It is often applied across vector spaces, such as different domains (Mikolov et al., 2013; Lazaridou et al., 2015). Recent models in distributional semantics consider derivational patterns (e.g., use → use + f ul ) as the result of a compositional process, where base term and affix are combined. We exploit such models for German particle verbs (PVs), and focus on the task of learning a mapping function between base verbs and particle verbs. Our models apply particle-verb motivated training-space restrictions relying on nearest neighbors, as well as recent advances from zeroshot-learning. The models improve the mapping between base terms and derived terms for a new PV derivation dataset, and also across ex"
S16-2010,W14-1618,0,0.0952037,"Missing"
S16-2010,P13-1118,1,0.791565,"Missing"
S16-2010,Q15-1016,0,0.0230701,"t (Lazaridou et al., 2013). 2.2 Word Embedding Vectors We relied on the German and English COW web corpora2 (Schäfer and Bildhauer, 2012) to obtain vector representations. The corpora contain 20 billion words and 9 billion words, respectively. We parsed the corpora using state-of-the-art pipelines integrating the MarMoT tagger and the MATE parser (Müller et al., 2013; Bohnet, 2010), and induced window co-occurrences for all corpus lemma–POS pairs and co-occurring nouns, verbs and adjectives in a 5-lemma window. We then created 400-dimensional word representations using the hyperwords toolkit (Levy et al., 2015), with context distribution smoothing of 0.75 and positive point-wise mutual information weighting together with singular value decomposition. The resulting vector space models contain approximately 460 000 lemmas for German and 240 000 lemmas for English. 2.1 Derivation Datasets We created a new collection of German particle verb derivations1 relying on the same resource as Kisselew et al. (2015), the semiautomatic derivational lexicon for German DErivBase (Zeller et al., 2013). From DErivBase, we induced all pairs of base verbs and particle verbs across seven different particles. Nonexisting"
S16-2020,W13-0601,0,0.0604654,"Missing"
S16-2020,briscoe-carroll-2002-robust,0,0.0124993,"ational Cor4 The translations of the example compounds are hair washing, hair dress, hair care, floral glory, and colour glory. 5 In fact, the annotation was performed for a superset of 1,208 compounds, but we only took into account 868 compounds with perfect agreement, i.e. IAA=1. 151 Language DE EN Dataset #Compounds Gh OST-NN/S Gh OST-NN/XL VD HB R EDDY OS 180 868 244 90 396 Annotation Frequency/Productivity Ambiguity DECOW GermaNet DECOW GermaNet DECOW GermaNet ENCOW WordNet ENCOW WordNet Relations Levi (7) Levi (7) – Levi (9) Levi (6) Table 1: Noun-noun compound datasets. pus using RASP (Briscoe and Carroll, 2002), and applied a simple heuristics to induce compound candidates: He used all sequences of two or more common nouns that were preceded or followed by sentence boundaries or by words not representing common nouns. Of these compound candidates, a random selection of 2,000 instances was used ´ S´eaghdha, 2007) and for relation annotation (O classification experiments. The final gold standard is a subset of these compounds, containing 1,443 noun-noun compounds. We refer to this dataset as OS. Both English compound datasets were enriched with frequencies and productivities, based on the ENCOW14AX 6"
S16-2020,W15-0903,1,0.778974,"s that both semantically transparent and semantically opaque compounds show morphological constituency; in addition, the semantic transparency of the head constituent was found to play a significant role. From a computational point of view, addressing the compositionality of noun compounds (and multi-word expressions in more general) is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) have integrated the prediction of multi-word compositionality into statistical machine translation. Computational approaches to automatically predict the compositionality of noun compounds have mostly been realised as vector space models, and can be subdivided into two subfields: (i) approaches that aim to predict the meaning of a compound by composite functions, relying on the vectors of the constituents (e.g., Mitchell and Lapata (2010), Coecke et al. (2011), Baroni et al. (2014), and Hermann (2014)); and (ii) approaches that aim to predict the degree of compositi"
S16-2020,D14-1024,0,0.160522,"(OO). Libben et al. confirmed Zwitserlood’s analyses that both semantically transparent and semantically opaque compounds show morphological constituency; in addition, the semantic transparency of the head constituent was found to play a significant role. From a computational point of view, addressing the compositionality of noun compounds (and multi-word expressions in more general) is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) have integrated the prediction of multi-word compositionality into statistical machine translation. Computational approaches to automatically predict the compositionality of noun compounds have mostly been realised as vector space models, and can be subdivided into two subfields: (i) approaches that aim to predict the meaning of a compound by composite functions, relying on the vectors of the constituents (e.g., Mitchell and Lapata (2010), Coecke et al. (2011), Baroni et al. (2014), and Hermann (2014)); and (ii) approaches th"
S16-2020,kunze-2000-extension,0,0.0490413,"requency-based factors, i.e., the frequencies of the compounds and their constituents (van Jaarsveld and Rattink, 1988; Janssen et al., 2008); • corpus frequencies of the compounds and the constituents (i.e., modifiers and heads), relying on DECOW14AX; • productivity of the constituents i.e., how many compound types contained a specific modifier/head constituent; • the productivity (morphological family size), i.e., the number of compounds that share a constituent (de Jong et al., 2002); and • number of senses of the compounds and the constituents, relying on GermaNet (Hamp and Feldweg, 1997; Kunze, 2000). • semantic variables as the relationship between compound modifier and head: a teapot is a pot FOR tea; a snowball is a ball MADE OF snow (Gagn´e and Spalding, 2009; Ji et al., 2011). From the set of compound candidates we extracted a random subset that was balanced3 for In addition, we were interested in the effect of ambiguity (of both the modifiers and the heads) regarding the compositionality of the compounds. Our explorations required gold standards of compounds that were annotated with all these compound and constituent properties. Since most previous work on computational predictions"
S16-2020,W97-0802,0,0.132794,"resentation, such as • frequency-based factors, i.e., the frequencies of the compounds and their constituents (van Jaarsveld and Rattink, 1988; Janssen et al., 2008); • corpus frequencies of the compounds and the constituents (i.e., modifiers and heads), relying on DECOW14AX; • productivity of the constituents i.e., how many compound types contained a specific modifier/head constituent; • the productivity (morphological family size), i.e., the number of compounds that share a constituent (de Jong et al., 2002); and • number of senses of the compounds and the constituents, relying on GermaNet (Hamp and Feldweg, 1997; Kunze, 2000). • semantic variables as the relationship between compound modifier and head: a teapot is a pot FOR tea; a snowball is a ball MADE OF snow (Gagn´e and Spalding, 2009; Ji et al., 2011). From the set of compound candidates we extracted a random subset that was balanced3 for In addition, we were interested in the effect of ambiguity (of both the modifiers and the heads) regarding the compositionality of the compounds. Our explorations required gold standards of compounds that were annotated with all these compound and constituent properties. Since most previous work on computationa"
S16-2020,L16-1362,1,0.626581,"Missing"
S16-2020,I11-1024,0,0.283217,"cal machine translation. Computational approaches to automatically predict the compositionality of noun compounds have mostly been realised as vector space models, and can be subdivided into two subfields: (i) approaches that aim to predict the meaning of a compound by composite functions, relying on the vectors of the constituents (e.g., Mitchell and Lapata (2010), Coecke et al. (2011), Baroni et al. (2014), and Hermann (2014)); and (ii) approaches that aim to predict the degree of compositionality of a compound, typically by comparing the compound vectors with the constituent vectors (e.g., Reddy et al. (2011), Salehi and Cook (2013), Schulte im Walde et al. (2013), Salehi et al. (2014; 2015a)). In line with subfield (ii), this paper aims to distinguish the contributions of modifier and head properties when predicting the compositionality of English and German nounnoun compounds in a vector space model. In this paper, we explore the role of constituent properties in English and German noun-noun compounds (corpus frequencies of the compounds and their constituents; productivity and ambiguity of the constituents; and semantic relations between the constituents), when predicting the degrees of composi"
S16-2020,S13-1039,0,0.118891,"on. Computational approaches to automatically predict the compositionality of noun compounds have mostly been realised as vector space models, and can be subdivided into two subfields: (i) approaches that aim to predict the meaning of a compound by composite functions, relying on the vectors of the constituents (e.g., Mitchell and Lapata (2010), Coecke et al. (2011), Baroni et al. (2014), and Hermann (2014)); and (ii) approaches that aim to predict the degree of compositionality of a compound, typically by comparing the compound vectors with the constituent vectors (e.g., Reddy et al. (2011), Salehi and Cook (2013), Schulte im Walde et al. (2013), Salehi et al. (2014; 2015a)). In line with subfield (ii), this paper aims to distinguish the contributions of modifier and head properties when predicting the compositionality of English and German nounnoun compounds in a vector space model. In this paper, we explore the role of constituent properties in English and German noun-noun compounds (corpus frequencies of the compounds and their constituents; productivity and ambiguity of the constituents; and semantic relations between the constituents), when predicting the degrees of compositionality of the compoun"
S16-2020,E14-1050,0,0.435538,"e compositionality of noun compounds have mostly been realised as vector space models, and can be subdivided into two subfields: (i) approaches that aim to predict the meaning of a compound by composite functions, relying on the vectors of the constituents (e.g., Mitchell and Lapata (2010), Coecke et al. (2011), Baroni et al. (2014), and Hermann (2014)); and (ii) approaches that aim to predict the degree of compositionality of a compound, typically by comparing the compound vectors with the constituent vectors (e.g., Reddy et al. (2011), Salehi and Cook (2013), Schulte im Walde et al. (2013), Salehi et al. (2014; 2015a)). In line with subfield (ii), this paper aims to distinguish the contributions of modifier and head properties when predicting the compositionality of English and German nounnoun compounds in a vector space model. In this paper, we explore the role of constituent properties in English and German noun-noun compounds (corpus frequencies of the compounds and their constituents; productivity and ambiguity of the constituents; and semantic relations between the constituents), when predicting the degrees of compositionality of the compounds within a vector space model. The results demonstra"
S16-2020,N15-1099,0,0.0457677,"y transparent and semantically opaque compounds show morphological constituency; in addition, the semantic transparency of the head constituent was found to play a significant role. From a computational point of view, addressing the compositionality of noun compounds (and multi-word expressions in more general) is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) have integrated the prediction of multi-word compositionality into statistical machine translation. Computational approaches to automatically predict the compositionality of noun compounds have mostly been realised as vector space models, and can be subdivided into two subfields: (i) approaches that aim to predict the meaning of a compound by composite functions, relying on the vectors of the constituents (e.g., Mitchell and Lapata (2010), Coecke et al. (2011), Baroni et al. (2014), and Hermann (2014)); and (ii) approaches that aim to predict the degree of compositionality of a compound, ty"
S16-2020,W14-5709,1,0.914327,"Missing"
S16-2020,W15-0909,0,0.106506,"y transparent and semantically opaque compounds show morphological constituency; in addition, the semantic transparency of the head constituent was found to play a significant role. From a computational point of view, addressing the compositionality of noun compounds (and multi-word expressions in more general) is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) have integrated the prediction of multi-word compositionality into statistical machine translation. Computational approaches to automatically predict the compositionality of noun compounds have mostly been realised as vector space models, and can be subdivided into two subfields: (i) approaches that aim to predict the meaning of a compound by composite functions, relying on the vectors of the constituents (e.g., Mitchell and Lapata (2010), Coecke et al. (2011), Baroni et al. (2014), and Hermann (2014)); and (ii) approaches that aim to predict the degree of compositionality of a compound, ty"
S16-2020,schafer-bildhauer-2012-building,0,0.0835741,"Missing"
S16-2020,S13-1038,1,0.738063,"Missing"
S16-2020,J14-3005,0,\N,Missing
S16-2020,2014.lilt-9.5,0,\N,Missing
S18-2003,W03-1812,0,0.0393451,"classes (Lechler and Roßdeutscher, 2009; Haselbach, 2011; Kliche, 2011; Springorum, 2011). Corpus studies and annotations demonstrated the potential of German PVs to appear in non-literal language usage, and to trigger meaning shifts (Springorum et al., 2013; K¨oper and Schulte im Walde, 2016b). Regarding computational models, the majority of existing approaches to PV meaning addressed the automatic prediction of German PV compositionality (Salehi et al., 2014; Bott and Schulte im Walde, 2015; K¨oper and Schulte im Walde, 2017b), in a similar vein as computational approaches for English PVs (Baldwin et al., 2003; Bannard, 2005; McCarthy et al., 2003; Kim and Baldwin, 2007; Salehi and Cook, 2013; Salehi et al., 2014). Only few approaches to German and English PVs have included the meaning contributions of the particles into the prediction of PV meaning (Bannard, 2005; Cook and Stevenson, 2006; K¨oper et al., 2016). Overall, we are faced with a variety of interdisciplinary approaches to identifying and modelling the meaning components and the composite meanings of German PVs. Current and future research activities are however hindered by a lack of resources that go beyond PV–BV compositionality and can"
S18-2003,E06-1042,0,0.0271096,"ordNet when annotating a total of 1,650 French and German metaphor instances. Similarly, Shutova and Teufel (2010) used the source and target domains from the MML but relied only on a subset of the domains, which they then extended for their annotation purposes. As to our knowledge, there is no previous dataset on meaning shifts of complex verbs, other than a smaller-scale collection developed in parallel by ourselves, which however focuses on analogies in meaning shifts rather than source–target domains (K¨oper and Schulte im Walde, 2018). Some datasets include non-literal meanings of verbs (Birke and Sarkar, 2006; Turney et al., 2011; Shutova et al., 2013; K¨oper and Schulte im Walde, 2016b), and the MML-based meaning shift annotations by L¨onneker-Rodman (2008) and Shutova and Teufel (2010) also include verbs but are less targetspecific than our work. In addition, while both L¨onneker-Rodman (2008) and Shutova and Teufel (2010) asked their annotators to label words in their corpus data, we follow a different strategy and ask our participants to generate sentences according to domain-specific target senses. compared word-based and syntax-based distributional models in the prediction of German PVs. K¨o"
S18-2003,bott-schulte-im-walde-2014-optimizing,1,0.892637,"Missing"
S18-2003,W15-0104,1,0.905457,"Missing"
S18-2003,W17-1708,1,0.883775,"Missing"
S18-2003,W06-1207,0,0.252217,"m Walde, 2016b). Regarding computational models, the majority of existing approaches to PV meaning addressed the automatic prediction of German PV compositionality (Salehi et al., 2014; Bott and Schulte im Walde, 2015; K¨oper and Schulte im Walde, 2017b), in a similar vein as computational approaches for English PVs (Baldwin et al., 2003; Bannard, 2005; McCarthy et al., 2003; Kim and Baldwin, 2007; Salehi and Cook, 2013; Salehi et al., 2014). Only few approaches to German and English PVs have included the meaning contributions of the particles into the prediction of PV meaning (Bannard, 2005; Cook and Stevenson, 2006; K¨oper et al., 2016). Overall, we are faced with a variety of interdisciplinary approaches to identifying and modelling the meaning components and the composite meanings of German PVs. Current and future research activities are however hindered by a lack of resources that go beyond PV–BV compositionality and can serve as gold standards for assessing (i) the meaning contributions of the notoriously ambiguous particles, and (ii) meaning shifts of PVs in comparison to their BVs. This paper presents a collection to assess meaning components in German complex verbs, which frequently undergo meani"
S18-2003,L16-1413,1,0.905757,"Missing"
S18-2003,N16-1039,1,0.843828,"Missing"
S18-2003,E17-2086,1,0.897271,"Missing"
S18-2003,S13-1039,0,0.145483,", 2011). Corpus studies and annotations demonstrated the potential of German PVs to appear in non-literal language usage, and to trigger meaning shifts (Springorum et al., 2013; K¨oper and Schulte im Walde, 2016b). Regarding computational models, the majority of existing approaches to PV meaning addressed the automatic prediction of German PV compositionality (Salehi et al., 2014; Bott and Schulte im Walde, 2015; K¨oper and Schulte im Walde, 2017b), in a similar vein as computational approaches for English PVs (Baldwin et al., 2003; Bannard, 2005; McCarthy et al., 2003; Kim and Baldwin, 2007; Salehi and Cook, 2013; Salehi et al., 2014). Only few approaches to German and English PVs have included the meaning contributions of the particles into the prediction of PV meaning (Bannard, 2005; Cook and Stevenson, 2006; K¨oper et al., 2016). Overall, we are faced with a variety of interdisciplinary approaches to identifying and modelling the meaning components and the composite meanings of German PVs. Current and future research activities are however hindered by a lack of resources that go beyond PV–BV compositionality and can serve as gold standards for assessing (i) the meaning contributions of the notoriou"
S18-2003,W17-1728,1,0.876056,"Missing"
S18-2003,E14-1050,0,0.112215,"research interest. For example, a series of formal-semantic analyses manually classified German PVs (with particles ab, an, auf, nach) into soft semantic classes (Lechler and Roßdeutscher, 2009; Haselbach, 2011; Kliche, 2011; Springorum, 2011). Corpus studies and annotations demonstrated the potential of German PVs to appear in non-literal language usage, and to trigger meaning shifts (Springorum et al., 2013; K¨oper and Schulte im Walde, 2016b). Regarding computational models, the majority of existing approaches to PV meaning addressed the automatic prediction of German PV compositionality (Salehi et al., 2014; Bott and Schulte im Walde, 2015; K¨oper and Schulte im Walde, 2017b), in a similar vein as computational approaches for English PVs (Baldwin et al., 2003; Bannard, 2005; McCarthy et al., 2003; Kim and Baldwin, 2007; Salehi and Cook, 2013; Salehi et al., 2014). Only few approaches to German and English PVs have included the meaning contributions of the particles into the prediction of PV meaning (Bannard, 2005; Cook and Stevenson, 2006; K¨oper et al., 2016). Overall, we are faced with a variety of interdisciplinary approaches to identifying and modelling the meaning components and the composi"
S18-2003,N18-2024,1,0.886784,"Missing"
S18-2003,S16-2010,1,0.904796,"Missing"
S18-2003,shutova-teufel-2010-metaphor,0,0.201847,"er and Schulte im Walde (2010), Bott and Schulte im Walde (2017) and K¨oper and Schulte im Walde (2017a) used unsupervised (soft) clustering and multi-sense embeddings to determine the degree of compositionality of German PVs. Salehi and Cook (2013) and Salehi et al. (2014) relied on translations into multiple languages in order to predict the degree of compositionality for English PVs. Bott and Schulte im Walde (2014) and Bott and Schulte im Walde (2015) explored and 23 who relied on the MML next to EuroWordNet when annotating a total of 1,650 French and German metaphor instances. Similarly, Shutova and Teufel (2010) used the source and target domains from the MML but relied only on a subset of the domains, which they then extended for their annotation purposes. As to our knowledge, there is no previous dataset on meaning shifts of complex verbs, other than a smaller-scale collection developed in parallel by ourselves, which however focuses on analogies in meaning shifts rather than source–target domains (K¨oper and Schulte im Walde, 2018). Some datasets include non-literal meanings of verbs (Birke and Sarkar, 2006; Turney et al., 2011; Shutova et al., 2013; K¨oper and Schulte im Walde, 2016b), and the MM"
S18-2003,W13-0120,1,0.937463,"uttgart.de Abstract With PVs representing a large and challenging class in the lexicon, their meaning components and their mechanisms of compositionality have received a considerable amount of interdisciplinary research interest. For example, a series of formal-semantic analyses manually classified German PVs (with particles ab, an, auf, nach) into soft semantic classes (Lechler and Roßdeutscher, 2009; Haselbach, 2011; Kliche, 2011; Springorum, 2011). Corpus studies and annotations demonstrated the potential of German PVs to appear in non-literal language usage, and to trigger meaning shifts (Springorum et al., 2013; K¨oper and Schulte im Walde, 2016b). Regarding computational models, the majority of existing approaches to PV meaning addressed the automatic prediction of German PV compositionality (Salehi et al., 2014; Bott and Schulte im Walde, 2015; K¨oper and Schulte im Walde, 2017b), in a similar vein as computational approaches for English PVs (Baldwin et al., 2003; Bannard, 2005; McCarthy et al., 2003; Kim and Baldwin, 2007; Salehi and Cook, 2013; Salehi et al., 2014). Only few approaches to German and English PVs have included the meaning contributions of the particles into the prediction of PV me"
S18-2003,D11-1063,0,0.206098,"total of 1,650 French and German metaphor instances. Similarly, Shutova and Teufel (2010) used the source and target domains from the MML but relied only on a subset of the domains, which they then extended for their annotation purposes. As to our knowledge, there is no previous dataset on meaning shifts of complex verbs, other than a smaller-scale collection developed in parallel by ourselves, which however focuses on analogies in meaning shifts rather than source–target domains (K¨oper and Schulte im Walde, 2018). Some datasets include non-literal meanings of verbs (Birke and Sarkar, 2006; Turney et al., 2011; Shutova et al., 2013; K¨oper and Schulte im Walde, 2016b), and the MML-based meaning shift annotations by L¨onneker-Rodman (2008) and Shutova and Teufel (2010) also include verbs but are less targetspecific than our work. In addition, while both L¨onneker-Rodman (2008) and Shutova and Teufel (2010) asked their annotators to label words in their corpus data, we follow a different strategy and ask our participants to generate sentences according to domain-specific target senses. compared word-based and syntax-based distributional models in the prediction of German PVs. K¨oper and Schulte im Wa"
S18-2003,W03-1810,0,0.28845,"009; Haselbach, 2011; Kliche, 2011; Springorum, 2011). Corpus studies and annotations demonstrated the potential of German PVs to appear in non-literal language usage, and to trigger meaning shifts (Springorum et al., 2013; K¨oper and Schulte im Walde, 2016b). Regarding computational models, the majority of existing approaches to PV meaning addressed the automatic prediction of German PV compositionality (Salehi et al., 2014; Bott and Schulte im Walde, 2015; K¨oper and Schulte im Walde, 2017b), in a similar vein as computational approaches for English PVs (Baldwin et al., 2003; Bannard, 2005; McCarthy et al., 2003; Kim and Baldwin, 2007; Salehi and Cook, 2013; Salehi et al., 2014). Only few approaches to German and English PVs have included the meaning contributions of the particles into the prediction of PV meaning (Bannard, 2005; Cook and Stevenson, 2006; K¨oper et al., 2016). Overall, we are faced with a variety of interdisciplinary approaches to identifying and modelling the meaning components and the composite meanings of German PVs. Current and future research activities are however hindered by a lack of resources that go beyond PV–BV compositionality and can serve as gold standards for assessing"
S18-2008,W17-7101,1,0.831474,"Missing"
S18-2008,N16-1039,1,0.914827,"Missing"
S18-2008,W17-6910,1,0.32057,"ystem (Barsalou and Wiemer-Hastings, 2005; Glenberg and Kaschak, 2002; Hill et al., 2014; Pecher et al., 2011). According to this account, concrete concepts have a direct referent in the real world, while abstract concepts have to activate a series of concrete concepts that provide the necessary situational context required to successfully process their meanings (Barsalou, 1999). These interdisciplinary outcomes are not fully supported by recent computational studies showing different contextual patterns for concrete and abstract words in text compared to the literature (Bhaskar et al., 2017; Frassinelli et al., 2017). It is becoming clear, however, that the inclusion of information regarding the concreteness of words plays a key role in the automatic identification of non-literal language usage (Turney et al., 2011; Across disciplines, researchers are eager to gain insight into empirical features of abstract vs. concrete concepts. In this work, we provide a detailed characterisation of the distributional nature of abstract and concrete words across 16,620 English nouns, verbs and adjectives. Specifically, we investigate the following questions: (1) What is the distribution of concreteness in the contexts"
S18-2008,W17-1903,1,0.870955,"Missing"
S18-2008,schafer-bildhauer-2012-building,0,0.0750069,"Missing"
S18-2008,D11-1063,0,0.0861657,"tract concepts have to activate a series of concrete concepts that provide the necessary situational context required to successfully process their meanings (Barsalou, 1999). These interdisciplinary outcomes are not fully supported by recent computational studies showing different contextual patterns for concrete and abstract words in text compared to the literature (Bhaskar et al., 2017; Frassinelli et al., 2017). It is becoming clear, however, that the inclusion of information regarding the concreteness of words plays a key role in the automatic identification of non-literal language usage (Turney et al., 2011; Across disciplines, researchers are eager to gain insight into empirical features of abstract vs. concrete concepts. In this work, we provide a detailed characterisation of the distributional nature of abstract and concrete words across 16,620 English nouns, verbs and adjectives. Specifically, we investigate the following questions: (1) What is the distribution of concreteness in the contexts of concrete and abstract target words? (2) What are the differences between concrete and abstract words in terms of contextual semantic diversity? (3) How does the entropy of concrete and abstract word"
S19-1001,P16-1141,0,0.261594,"Missing"
S19-1001,E17-4012,1,0.868975,"Missing"
schulte-im-walde-2002-subcategorisation,W98-1505,0,\N,Missing
schulte-im-walde-2002-subcategorisation,C94-1042,0,\N,Missing
schulte-im-walde-2002-subcategorisation,A97-1052,0,\N,Missing
schulte-im-walde-2002-subcategorisation,J93-2002,0,\N,Missing
schulte-im-walde-2002-subcategorisation,P93-1032,0,\N,Missing
schulte-im-walde-2006-human,H05-1077,1,\N,Missing
schulte-im-walde-2006-human,C00-2108,1,\N,Missing
schulte-im-walde-2006-human,J06-2001,1,\N,Missing
schulte-im-walde-2006-human,P04-2007,0,\N,Missing
schulte-im-walde-2006-human,P03-1009,0,\N,Missing
schulte-im-walde-2006-human,P03-1068,0,\N,Missing
schulte-im-walde-2006-human,P93-1023,0,\N,Missing
schulte-im-walde-2006-human,kunze-2000-extension,0,\N,Missing
schulte-im-walde-2010-comparing,D07-1039,0,\N,Missing
schulte-im-walde-2010-comparing,J98-2002,0,\N,Missing
schulte-im-walde-2010-comparing,J02-2003,0,\N,Missing
schulte-im-walde-2010-comparing,C00-1028,0,\N,Missing
schulte-im-walde-2010-comparing,E03-1034,0,\N,Missing
schulte-im-walde-2010-comparing,E03-1087,0,\N,Missing
schulte-im-walde-2010-comparing,E06-2001,0,\N,Missing
schulte-im-walde-2010-comparing,P07-1028,0,\N,Missing
schulte-im-walde-2010-comparing,P99-1014,0,\N,Missing
schulte-im-walde-2010-comparing,P09-2019,0,\N,Missing
schulte-im-walde-2010-comparing,P08-1057,1,\N,Missing
schulte-im-walde-2010-comparing,P93-1023,0,\N,Missing
schulte-im-walde-2010-comparing,J03-4004,0,\N,Missing
schulte-im-walde-2010-comparing,P93-1024,0,\N,Missing
schulte-im-walde-etal-2012-association,I11-1079,0,\N,Missing
schulte-im-walde-etal-2012-association,J90-1003,0,\N,Missing
schulte-im-walde-etal-2012-association,E12-1004,0,\N,Missing
schulte-im-walde-etal-2012-association,E06-2001,0,\N,Missing
schulte-im-walde-etal-2012-association,C02-1007,0,\N,Missing
schulte-im-walde-etal-2012-association,W06-2506,1,\N,Missing
springorum-etal-2012-automatic,D07-1039,0,\N,Missing
springorum-etal-2012-automatic,W06-1207,0,\N,Missing
springorum-etal-2012-automatic,faass-etal-2010-design,0,\N,Missing
springorum-etal-2012-automatic,E03-1087,0,\N,Missing
springorum-etal-2012-automatic,W03-1808,0,\N,Missing
springorum-etal-2012-automatic,W06-2109,0,\N,Missing
springorum-etal-2012-automatic,W03-1810,0,\N,Missing
springorum-etal-2012-automatic,W03-1812,0,\N,Missing
springorum-etal-2012-automatic,P06-2064,0,\N,Missing
springorum-etal-2012-automatic,U08-1008,0,\N,Missing
springorum-etal-2012-automatic,W04-2118,1,\N,Missing
springorum-etal-2012-automatic,kunze-2000-extension,0,\N,Missing
springorum-etal-2012-automatic,aldinger-2004-towards,0,\N,Missing
utt-etal-2014-fuzzy,J06-2001,1,\N,Missing
utt-etal-2014-fuzzy,J01-3003,0,\N,Missing
utt-etal-2014-fuzzy,I13-1072,1,\N,Missing
utt-etal-2014-fuzzy,D07-1043,0,\N,Missing
W02-1016,W98-1505,0,0.0225794,"Missing"
W02-1016,1997.mtsummit-workshop.2,0,0.0624191,"Missing"
W02-1016,C96-1055,0,0.0164073,"Missing"
W02-1016,W99-0632,1,0.900684,"Missing"
W02-1016,P99-1051,0,0.154179,"Missing"
W02-1016,A00-2034,0,0.0219475,"Missing"
W02-1016,J01-3003,0,0.151964,"Missing"
W02-1016,P02-1029,1,0.742129,"Missing"
W02-1016,C00-2108,1,0.923673,"Missing"
W02-1016,schulte-im-walde-2002-subcategorisation,1,0.881087,"Missing"
W04-2118,J06-2001,1,\N,Missing
W04-2118,W03-1808,0,\N,Missing
W04-2118,W03-1812,0,\N,Missing
W04-2118,W02-2001,0,\N,Missing
W05-1009,alsina-etal-2002-catcg,1,0.759722,"as will be shown throughout the paper, and can be exploited in low-level NLP tasks (POStagging), and also in more demanding tasks, such as paraphrase detection and generation (e.g. exploiting the relationship tangible → can be touched, or deformació nasal → deformity affecting the nose). 2.2 Gold standard To perform the experiments, we built a set of annotated data based on this classification (gold standard from now on). We extracted the lemmata and data for the gold standard from a 16.5 million word Catalan corpus (Rafel, 1994), lemmatised, POS-tagged and shallow parsed with the CatCG tool (Alsina et al., 2002). The shallow parser gives information on the syntactic function of each word (subject, object, etc.), not on phrase structure. 186 lemmata were randomly chosen among all 2564 adjectives occuring more than 25 times in the corpus. 86 of the 186 lemmata were classified by 3 human judges into each of the classes (basic, object, event).2 In case of polysemy affecting the class as2 The 3 human judges were PhD students with training in linguistics, one of which had done research on adjectives. As it was defined, the level of training in linguistics needed for the signment, the judges were instructed"
W05-1009,C04-1161,1,0.913074,"pful to overcome the ceiling reached with morphology. 1 Introduction This paper fits into a broader effort addressing the automatic acquisition of semantic classes for Catalan adjectives. So far, no established standard of such semantic classes is available in theoretical or empirical linguistic research. Our aim is to reach a classification that is empirically adequate and theoretically sound, and we use computational techniques as a means to explore large amounts of data which would be impossible to explore by hand to help us define and characterise the classification. In previous research (Boleda et al., 2004), we developed a three-way classification according to generally accepted adjective properties (see Section 2), Decision trees are appropriate for our task, to test and compare sets of features, based on our gold standard. They are also known for their easy interpretation, by reading feature combinations off the tree paths. This property will help us get insight into relevant characteristics of our adjective classes, and in the error analysis. The paper is structured as follows: Section 2 presents the adjective classification and the gold standard used for the experiments. Sections 3 and 4 exp"
W05-1009,J96-2004,0,0.0307312,", ‘related to a point’ (usually, a point in time), as in això va ser un esdeveniment puntual, ‘this was a once-occuring event’. This is the meaning we would expect from the derivation punt (‘point’) + al, and is an object meaning. In this case, the judge should assign the adjective to two classes, primary basic, secondary object. Compositional meanings are thus those corresponding to active morphological processes, and can be predicted from the meaning of the noun and the derivation with the suffix (be it denominal, deverbal or participial). The judges had an acceptable 0.74 mean κ agreement (Carletta, 1996) for the assignment of the primary class, but a meaningless 0.21 for the secondary class (they did not even agree on which lemmata were polysemous). As a reaction to the low agreement about polysemy, we incorporated polysemy information from a Catalan dictionary (DLC, 1993). This information was incorporated only in addition to the gathered gold standard: In many cases the dictionary only lists the compositional sense. We added it as a second reading if our judges considered the noncompositional one as most frequent. One of the authors of the paper classified the remaining 100 lemmata accordin"
W05-1009,P93-1023,0,0.481006,"Missing"
W05-1009,P03-1009,0,0.160549,"Missing"
W05-1009,J01-3003,0,0.0900152,"Missing"
W05-1009,P02-1029,1,0.894544,"Missing"
W06-2506,P99-1014,0,0.0200323,"veldt, 1990) were based on word associations and were used to identify word senses. An enourmous number of approaches in computational linguistics can be found on the SENSEVAL webpage (SENSEVAL, ), which hosts a word sense disambiguation competition. We applied Latent Semantic Clusters (LSC) to our association data. The LSC algorithm is an instance of the Expectation-Maximisation algorithm (Baum, 1972) for unsupervised training based on unannotated data, and has been applied to model the selectional dependency between two sets of words participating in a grammatical relationship (Rooth, 1998; Rooth et al., 1999). The resulting cluster analysis defines two-dimensional soft clusters which are able to generalise over hidden data. LSC training learns three probability distributions, one for the probabilities of the clusters, and one for each tuple input item and each cluster (i.e., a probability distribution for the target nouns and each cluster, and one for the associations and each cluster), thus the two dimensions. We use an implementation of the LSC algorithm as provided by Helmut Schmid. The LSC output depends not only on the distributional input, but also on the number of clusters to model. As a ru"
W06-2910,P03-1068,0,0.0700921,"Missing"
W06-2910,P04-2007,0,0.035608,"Missing"
W06-2910,kunze-2000-extension,0,0.0156282,"ew divergence, a smoothed variant of the KullbackLeibler divergence (Lee, 2001). The goal of these experiments was not to explore the optimal feature combination; thus, we rely on previous experiments and parameter settings, cf. Schulte im Walde (2003). Our claim is that the hierarchical verb classes and their underlying features (i.e. the verb associations) represent a useful basis for a theoryindependent semantic classification of the German verbs. To support this claim, we validated the assoc-classes against standard approaches to semantic verb classes, i.e. GermaNet as the German WordNet (Kunze, 2000), and the German counterpart of FrameNet in the Salsa project (Erk et al., 2003). Details of the validation can be found in (Schulte im Walde, 2006); the main issues are as follows. We did not directly compare the assoc-classes against the GermaNet/FrameNet classes, since not all of our 330 experiments verbs were covered by the two resources. Instead, we replicated the above cluster experiment for a reduced number of verbs: We extracted those classes from the resources which contain association verbs; light verbs, nonassociation verbs, other classes as well as singletons were disregarded. This"
W06-2910,P98-2127,0,0.117491,"Missing"
W06-2910,W03-1810,0,0.0577786,"Missing"
W06-2910,P93-1024,0,0.37391,"Missing"
W06-2910,P99-1014,0,0.0520685,"Missing"
W06-2910,H05-1077,1,0.761394,"erform previous clustering results? By applying the feature choices to GermaNet and FrameNet, we address the question whether the same types of features are salient for different types of semantic verb classes? In what follows, the paper presents the association data in Section 2 and the association-based classes in Section 3. In Section 4, we compare the associations with corpus-based feature types, and in Section 5 we apply the insights to induce semantic verb classes. 2 Verb Association Data We obtained human associations to German verbs from native speakers in a web experiment (Schulte im Walde and Melinger, 2005). 330 verbs were selected for the experiment (henceforth: experiment verbs), from different semantic categories, and dif70 ferent corpus frequency bands. Participants were given 55 verbs each, and had 30 seconds per verb to type as many associations as they could. 299 native German speakers participated in the experiment, between 44 and 54 for each verb. In total, we collected 81,373 associations from 16,445 trials; each trial elicited an average of 5.16 responses with a range of 0-16. All data sets were pre-processed in the following way: For each target verb, we quantified over all responses"
W06-2910,P93-1023,0,0.148112,"Missing"
W06-2910,E03-1040,0,0.0465962,"Missing"
W06-2910,P03-1009,0,0.134741,"sive manual classifications, Since the target classification determines the similarity and dissimilarity of the verbs, the verb feature selection should model the similarity of interest. For example, Merlo and Stevenson (2001) classify 60 English verbs which alternate between an intransitive and a transitive usage, and assign them to three verb classes, according to the semantic role assignment in the frames; their verb features are chosen such that they model the syntactic frame alternation proportions and also heuristics for semantic role assignment. In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient. The verb features need to relate to a behavioural component (modelling the syntax-semantics interplay), but the set of features which potentially influence the behaviour is large, ranging from structural syntactic descriptions and argument role fillers to adverbial adjuncts. In addition, it is not clear how fine-grained the features should be; for example, how much information is covered by low-level window co-oc"
W06-2910,schulte-im-walde-2006-human,1,0.716381,"e combination; thus, we rely on previous experiments and parameter settings, cf. Schulte im Walde (2003). Our claim is that the hierarchical verb classes and their underlying features (i.e. the verb associations) represent a useful basis for a theoryindependent semantic classification of the German verbs. To support this claim, we validated the assoc-classes against standard approaches to semantic verb classes, i.e. GermaNet as the German WordNet (Kunze, 2000), and the German counterpart of FrameNet in the Salsa project (Erk et al., 2003). Details of the validation can be found in (Schulte im Walde, 2006); the main issues are as follows. We did not directly compare the assoc-classes against the GermaNet/FrameNet classes, since not all of our 330 experiments verbs were covered by the two resources. Instead, we replicated the above cluster experiment for a reduced number of verbs: We extracted those classes from the resources which contain association verbs; light verbs, nonassociation verbs, other classes as well as singletons were disregarded. This left us with 33 classes from GermaNet, and 38 classes from FrameNet. These remaining classifications are polysemous: The 33 GermaNet classes contai"
W06-2910,W03-0410,0,0.110969,"ions, Since the target classification determines the similarity and dissimilarity of the verbs, the verb feature selection should model the similarity of interest. For example, Merlo and Stevenson (2001) classify 60 English verbs which alternate between an intransitive and a transitive usage, and assign them to three verb classes, according to the semantic role assignment in the frames; their verb features are chosen such that they model the syntactic frame alternation proportions and also heuristics for semantic role assignment. In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient. The verb features need to relate to a behavioural component (modelling the syntax-semantics interplay), but the set of features which potentially influence the behaviour is large, ranging from structural syntactic descriptions and argument role fillers to adverbial adjuncts. In addition, it is not clear how fine-grained the features should be; for example, how much information is covered by low-level window co-occurrence vs. higher-order sy"
W06-2910,J06-2001,1,\N,Missing
W06-2910,J01-3003,0,\N,Missing
W06-2910,C98-2122,0,\N,Missing
W13-0120,C10-1011,0,0.033604,"lar shifts in meanings across multiple verbs. 3 Corpus-based Acquisition of Base and Particle Verb Groups with a Meaning Shift Our strategy to identify meaning shifts in BV–PV transfer is as follows. We searched our corpus for examples of base verbs and particle verbs, concentrating on one specific particle at a time. As corpus data, we rely on the SD E WAC corpus (Faaß et al., 2010), a cleaned version of the German web corpus DE WAC created by the WAC KY group (Baroni et al., 2009). The SD E WAC contains approximately 880 million tokens and has been parsed by Bohnet’s MATE dependency parser (Bohnet, 2010). The information we used for our search was effectively verb subcategorization information that had been extracted and quantified automatically from the corpus parses. That is, for each verb (including BVs as well as PVs), we have quantitative information about how often the verb appeared with a specific subcategorization frame, and how often and which nominal complements appeared within the frames. In a first step, we searched the subcategorization database for all occurrences of particle verbs with a specific particle (such as auf ), the particle verbs’ subcategorization frames, and the nom"
W13-0120,S12-1023,1,0.900231,"Missing"
W13-0120,kunze-2000-extension,0,0.124382,"y the nominal complements of the verbs (Schulte im Walde, 2010), to be useful indicators of the kinds of connotations we so far collected manually. For example, concerning meaning shift class 4 above, strong adjectival modifiers of both Feuer ‘fire’ and Flamme ‘flame’ are ewig ‘eternal’ lodernd ‘blazing’, and offen ‘open’, while strong adjectival modifiers of both Konflikt ‘conflict’ and Diskussion ‘discussion’ are aktuell ‘current’, politisch ‘political’, and weit ‘wide’. (iii) Instead of subjective definitions of conceptual generalizations over nominal complements, one could apply GermaNet (Kunze, 2000), the German pendant to WordNet (Fellbaum, 1998). For example, both Feuer ‘fire’ and Flamme ‘flame’ are generalized to Ereignis ‘event’ by GermaNet on level 3 (starting from the top node level) and to Ph¨anomen ‘phenomenon’ on level 4, while both Konflikt ‘conflict’ and Diskussion ‘discussion’ are generalized to Kommunikation ‘communication’ and Gespr¨ach ‘conversation’ on levels 3 and 4, respectively. (iv) A simple way to enlarge meaning shift classes is by looking up synonyms of the base and/or particle verbs in dictionaries. For example, Bulitta and Bulitta (2003) defines aufdr¨angen, aufn¨"
W13-0120,N10-1013,0,0.169802,"respectively. (iv) A simple way to enlarge meaning shift classes is by looking up synonyms of the base and/or particle verbs in dictionaries. For example, Bulitta and Bulitta (2003) defines aufdr¨angen, aufn¨otigen, and aufoktroyieren as near-synonyms to aufzw¨angen, so we could check whether these three particle verbs fall into meaning shift class 3. A long-term goal of our work is to extend it toward a more automatically driven identification of meaning shifts in particle verbs. Three examples of approaches that are potentially useful to complement our corpus-based search are the following: Reisinger and Mooney (2010) presented a multiprototype vector-space model that discriminates multiple senses of a word by clustering contexts, an idea adopted from Sch¨utze (1998). We could reduce their “contexts” to the crucial information about the BV and PV properties we identified, i.e., subcategorization frame types and concept properties, possibly refined by further meaning aspects as suggested above. The framework would then allow us to determine the similarity between the “contexts” of base verbs and particle verbs, and thus to identify the semantically coherent groups of base verbs as well as literal meanings o"
W13-0120,schulte-im-walde-2010-comparing,1,0.885157,"Missing"
W13-0120,J98-1004,0,0.508997,"Missing"
W13-0120,D11-1063,0,0.168392,"Missing"
W13-0120,faass-etal-2010-design,0,\N,Missing
W13-0120,E06-1042,0,\N,Missing
W13-1005,D10-1115,0,0.110785,"Missing"
W13-1005,W06-1207,0,0.157719,"Missing"
W13-1005,W13-0112,0,0.0162671,"t Austin roller@cs.utexas.edu ‡ Institut f¨ur Maschinelle Sprachverarbeitung Universit¨at Stuttgart {schulte,scheible}@ims.uni-stuttgart.de Abstract 2003; Bannard, 2005; Cook and Stevenson, 2006); adjective-noun combinations (Baroni and Zamparelli, 2010; Boleda et al., 2013); and noun-noun compounds (Reddy et al., 2011b; Reddy et al., 2011a). Others have aimed at predicting the compositionality of phrases and sentences of arbitrary type and length, either by focusing on the learning approach (Socher et al., 2011); by integrating symbolic models into distributional models (Coecke et al., 2011; Grefenstette et al., 2013); or by exploring the arithmetic operations to predict compositionality by the meaning of the parts (Widdows, 2008; Mitchell and Lapata, 2010). Human ratings are an important source for evaluating computational models that predict compositionality, but like many data sets of human semantic judgements, are often fraught with uncertainty and noise. However, despite their importance, to our knowledge there has been no extensive look at the effects of cleansing methods on human rating data. This paper assesses two standard cleansing approaches on two sets of compositionality ratings for German nou"
W13-1005,W03-1810,0,0.786282,"Missing"
W13-1005,I11-1079,0,0.0786528,"second type of noise (Type II noise: unreliability), occurs when a subject is consistently unreliable or uncooperative. This may happen if the subject misunderstands the task, or if a subject simply wishes to complete the task as quickly as possible. Judgements collected via crowdsourcing are especially prone to this second kind of noise, when compared to traditional penand-paper experiments, since participants aim to maximize their hourly wage.2 In this paper, we apply two standard cleansing methods (Ben-Gal, 2005; Maletic and Marcus, 2010), that have been used on similar rating data before (Reddy et al., 2011b), on two data sets of compositionality ratings of German noun-noun compounds. We aim to address two main points. The first is to assess the cleansing approaches in their ability to produce compositionality ratings of higher quality and consistency, while facing a reduction of data mass in the cleansing process. In particular, we look at the effects of removing outlier judgements resulting from uncertainty (Type I noise) and dropping unreliable subjects (Type II noise). The second issue is to assess the overall reliability of our two rating data sets: Are they clean enough to be used as gold"
W13-1005,I11-1024,0,0.403998,"second type of noise (Type II noise: unreliability), occurs when a subject is consistently unreliable or uncooperative. This may happen if the subject misunderstands the task, or if a subject simply wishes to complete the task as quickly as possible. Judgements collected via crowdsourcing are especially prone to this second kind of noise, when compared to traditional penand-paper experiments, since participants aim to maximize their hourly wage.2 In this paper, we apply two standard cleansing methods (Ben-Gal, 2005; Maletic and Marcus, 2010), that have been used on similar rating data before (Reddy et al., 2011b), on two data sets of compositionality ratings of German noun-noun compounds. We aim to address two main points. The first is to assess the cleansing approaches in their ability to produce compositionality ratings of higher quality and consistency, while facing a reduction of data mass in the cleansing process. In particular, we look at the effects of removing outlier judgements resulting from uncertainty (Type I noise) and dropping unreliable subjects (Type II noise). The second issue is to assess the overall reliability of our two rating data sets: Are they clean enough to be used as gold"
W13-1005,schulte-im-walde-etal-2012-association,1,0.852534,"ation norms. Association norms have a long tradition in psycholinguistic research to investigate semantic memory, making use of the implicit notion that associates reflect meaning components of words (Deese, 1965; Miller, 1969; Clark, 1971; Nelson et al., 1998; Nelson et al., 2000; McNamara, 2005; de Deyne and Storms, 2008). They are collected by presenting a stimulus word to a subject and collecting the first words that come to mind. We rely on association norms that were collected for our compounds and constituents via both a large scale web experiment and Amazon Mechanical Turk (Schulte im Walde et al., 2012) (unpublished). The resulting combined data set contains 85,049/34,560 stimulus-association tokens/types for the compound and constituent stimuli. Table 3 gives examples of associations from the data set for some stimuli. The guiding intuition behind comparing our rating data sets with association norms is that a compound which is compositional with respect to a constituent should have similar associations as its constituent (Schulte im Walde et al., 2012). To measure the correlation of the rating data with the association norms, we first compute the Jaccard similarity that measures the overla"
W14-0818,D13-1115,1,0.865511,"Missing"
W14-0818,schulte-im-walde-etal-2012-association,1,0.914638,"nd Schulte im Walde, 2013). In this paper, we present a new resource of feature norms for a set of 572 concrete, depictable German nouns. More specifically, these nouns include 244 noun-noun compounds and their corresponding constituents. For example, we include features for ‘Schneeball‘ (‘snowball’), ‘Schnee‘ (‘snow’), and ‘Ball‘ (‘ball’). Table 1 presents the most prominent features of this example compound and its constituents. Our collection complements existing data sets for the same targets, including compositionality ratings (von der Heide and Borgwaldt, 2009); associations (Schulte im Walde et al., 2012; Schulte im Walde and Borgwaldt, 2014); and images (Roller and Schulte im Walde, 2013). The remainder of this paper details the collection process of the feature norms, discusses two forms of cleansing and normalization we employed, and performs quantitative and qualitative analyses. We find that the normalization procedures improve quality in terms of feature tokens per feature type, that the normalized feature norms have a desirable distribution of features per cue, and that the feature norms are useful in semantic models to predict compositionality. 2 Feature Norm Collection We employ Amaz"
W14-0818,D12-1130,0,0.092055,"ng their responses. Feature norms have been widely used in psycholinguistic research on conceptual representations in semantic memory. Prominent collections have been pursued by McRae et al. (2005) for living vs. non-living basic-level concepts; by Vinson and Vigliocco (2008) for objects and events; and by Wu and Barsalou (2009) for noun and noun phrase objects. In recent years, feature norms have also acted as a loose proxy for perceptual information in data-intensive computational models of semantic tasks, in order to bridge the gap between language and the real world (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). In this paper, we present a new resource of feature norms for a set of 572 concrete, depictable German nouns. More specifically, these nouns include 244 noun-noun compounds and their corresponding constituents. For example, we include features for ‘Schneeball‘ (‘snowball’), ‘Schnee‘ (‘snow’), and ‘Ball‘ (‘ball’). Table 1 presents the most prominent features of this example compound and its constituents. Our collection complements existing data sets for the same targets, including compositionality ratings (von der Heide and Borgwaldt, 2009); associations (S"
W14-5701,aldinger-2004-towards,0,0.0415775,"studied from the theoretical perspective and, to a more limited extent, from the aspect of the computational identifiability, predictability of the degree of semantic compositionality (the transparency of their meaning with respect to the meaning of the base verb and the particle) and the semantic classifiabilty of PVs. For English, there is work on the automatic extraction of PVs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005) and the determination of compositionality (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). To the best of our knowledge Aldinger (2004) is the first work that studies German PVs from a corpus based perspective, with an emphasis on the syntactic behavior and syntactic change. Schulte im Walde (2004; 2005; 2006) presents several preliminary distri2 butional studies to explore salient features at the syntax-semantics interface that determine the semantic nearest neighbours of German PVs. Relying on the insights of those studies, Schulte im Walde (2006) and Hartmann (2008) present preliminary experiments on modelling the subcategorization transfer of German PVs with respect to their BVs, in order to strengthen PV-BV distributiona"
W14-5701,W02-2001,0,0.12569,"aluation. Section 4 presents the results which are then discussed in section 5. Section 6 concludes the paper with some final remarks and outlook on future work. 2 Related Work Particle verbs have been studied from the theoretical perspective and, to a more limited extent, from the aspect of the computational identifiability, predictability of the degree of semantic compositionality (the transparency of their meaning with respect to the meaning of the base verb and the particle) and the semantic classifiabilty of PVs. For English, there is work on the automatic extraction of PVs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005) and the determination of compositionality (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). To the best of our knowledge Aldinger (2004) is the first work that studies German PVs from a corpus based perspective, with an emphasis on the syntactic behavior and syntactic change. Schulte im Walde (2004; 2005; 2006) presents several preliminary distri2 butional studies to explore salient features at the syntax-semantics interface that determine the semantic nearest neighbours of German PVs. Relying on the insights of those studies, Schulte im Walde"
W14-5701,W03-1812,0,0.0233979,"nd outlook on future work. 2 Related Work Particle verbs have been studied from the theoretical perspective and, to a more limited extent, from the aspect of the computational identifiability, predictability of the degree of semantic compositionality (the transparency of their meaning with respect to the meaning of the base verb and the particle) and the semantic classifiabilty of PVs. For English, there is work on the automatic extraction of PVs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005) and the determination of compositionality (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). To the best of our knowledge Aldinger (2004) is the first work that studies German PVs from a corpus based perspective, with an emphasis on the syntactic behavior and syntactic change. Schulte im Walde (2004; 2005; 2006) presents several preliminary distri2 butional studies to explore salient features at the syntax-semantics interface that determine the semantic nearest neighbours of German PVs. Relying on the insights of those studies, Schulte im Walde (2006) and Hartmann (2008) present preliminary experiments on modelling the subcategorization transfer of German PVs with re"
W14-5701,C10-1011,0,0.0272465,"e also excluded clausal complements, because they could not be properly represented by our vector extraction method. To get an idea of the lower bound of the outcome values, we used a select-1 baseline. This baseline was obtained by calculating the expected precision and recall for the case that for each subcategorization slot a matching slot from the corresponding other verb is assigned randomly. As training data we used a lemmatized and tagged version of the SDeWaC corpus (Faaß and Eckart, 2013), a corpus of nearly 885 million words. The corpus was processed with the Mate dependency parser (Bohnet, 2010). The output of this parser represents the syntactic complements of the verbs as labelled arcs. In the case of nominal objects the nominal heads could be directly read of the dependent nodes and the syntactic relation of the arc labels. In the case of PP-complements we read the nominal heads of the nominal node which depends on the preposition which in turn depends on the verb. For the extraction of features we could rely on the database compiled by (Scheible et al., 2013). 4 Particle an auf Typical frames for the BV NPnom +NPacc +PP-an NPnom +PP-zu/in/ nach/auf NPnom +NPacc +PP-mit Typical fr"
W14-5701,bott-schulte-im-walde-2014-optimizing,1,0.673523,"Missing"
W14-5701,S14-1022,1,0.705338,"Missing"
W14-5701,W03-1810,0,0.0473802,"th some final remarks and outlook on future work. 2 Related Work Particle verbs have been studied from the theoretical perspective and, to a more limited extent, from the aspect of the computational identifiability, predictability of the degree of semantic compositionality (the transparency of their meaning with respect to the meaning of the base verb and the particle) and the semantic classifiabilty of PVs. For English, there is work on the automatic extraction of PVs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005) and the determination of compositionality (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). To the best of our knowledge Aldinger (2004) is the first work that studies German PVs from a corpus based perspective, with an emphasis on the syntactic behavior and syntactic change. Schulte im Walde (2004; 2005; 2006) presents several preliminary distri2 butional studies to explore salient features at the syntax-semantics interface that determine the semantic nearest neighbours of German PVs. Relying on the insights of those studies, Schulte im Walde (2006) and Hartmann (2008) present preliminary experiments on modelling the subcategorization transfer"
W14-5701,W04-2118,1,0.728112,"compositionality (the transparency of their meaning with respect to the meaning of the base verb and the particle) and the semantic classifiabilty of PVs. For English, there is work on the automatic extraction of PVs from corpora (Baldwin and Villavicencio, 2002; Baldwin, 2005; Villavicencio, 2005) and the determination of compositionality (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005). To the best of our knowledge Aldinger (2004) is the first work that studies German PVs from a corpus based perspective, with an emphasis on the syntactic behavior and syntactic change. Schulte im Walde (2004; 2005; 2006) presents several preliminary distri2 butional studies to explore salient features at the syntax-semantics interface that determine the semantic nearest neighbours of German PVs. Relying on the insights of those studies, Schulte im Walde (2006) and Hartmann (2008) present preliminary experiments on modelling the subcategorization transfer of German PVs with respect to their BVs, in order to strengthen PV-BV distributional similarity. The main goal for them is to use transfer information in order to predict the degree of semantic compositionality of PVs. K¨uhner and Schulte im Wald"
W14-5701,springorum-etal-2012-automatic,1,0.844157,"transfer problem in evidence, the problem which we address here. They conclude that an incorporation of syntactic transfer information between BVs and PVs could possibly improve the results. In Bott and Schulte im Walde (2014a) we present a method to assess PV compositionality without recurring to any syntactic features, but we assume that the results of this method could be improved if additional syntactic transfer information was incorporated. Based on a theoretical study (Springorum, 2011) which explains particle meanings in terms of Discourse Representation Theory (Kamp and Reyle, 1993), Springorum et al. (2012) show that four classes of PVs with the particle an can be classified automatically. They take a supervised approach using decision trees. The use of decision trees also allows them to manually inspect and analyze the decisions made by the classifier. As predictive features they use the head nouns of objects, generalized classes of these nouns and PP types. In Bott and Schulte im Walde (2014b) we present an experiment to classify semantic classes of PVs, based on subcategorization information stemming from both the BV and the BV of each BV-PV pair. In this work we use the same gold standard we"
W14-5709,I08-1033,0,0.0159169,"ey (2000) use a parser for context-sensitive disambiguation, and Fritzinger and Fraser (2010) use corpus frequencies to find the best split for each compound. Other approaches use a two-step word alignment process: first, word alignment is performed on a split representation of the compounding language. Then, all former compound parts for which there is no aligned counterpart in the non-compounding language are merged back to the compound again. Finally, word alignment is re-run on this representation. See Koehn and Knight (2003) for experiments on German, DeNeefe et al. (2008) for Arabic and Bai et al. (2008) for Chinese. This blocks non-compositional compounds from being split if they are translated as one simplex English word in the training data (e.g. Heckensch¨utze, lit. ’hedge|shooter’; ’sniper’) and aligned correctly. However, cases like J¨agerzaun, ’lattice fence’ are not covered. In the present work, we identify compounds with a morphological analyser, disambiguated with corpus frequencies. Moreover, we restrict splitting to compositional compounds using distributional semantics. We are not aware of any previous work that takes semantics into account for compound splitting in SMT. 2.2 Dist"
W14-5709,bott-schulte-im-walde-2014-optimizing,1,0.806874,"Missing"
W14-5709,N12-1047,0,0.0416781,"This section gives an overview on the technical details of the SMT system and our data sets. Compound splitting is applied to all source-language data, i.e. the parallel data used to train the model, as well as the input for parameter tuning and testing.2 Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems (Koehn et al., 2007). We use it with default settings to train a translation model and we do so separately for each of the different compound splittings. Word alignment is performed using GIZA++ (Och and Ney, 2003). Feature weights are tuned using Batch-Mira (Cherry and Foster, 2012) with ’-safe-hope’ until convergence. Training Data Our parallel training data contains the Europarl corpus (version 4, cf. Koehn (2005)) and also newspaper texts, overall ca. 1.5 million sentences3 (roughly 44 million words). In addition, we 1 Baum|schule: ‘tree|school’ (tree nursery) Compounds not contained in the parallel data are always split, as they cannot be translated otherwise. 3 Data from the shared task of the EACL 2009 workshop on statistical machine translation: www.statmt.org/wmt09 2 84 use an English corpus of roughly 227 million words (including the English part of the parallel"
W14-5709,W06-1207,0,0.0846585,"w a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest. Among many other tasks, distributional semantic information has been utilised to determine the degree of compositionality (or: semantic transparency) of various types of compounds, most notably regarding noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013), Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), K¨uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)). Typically, these approaches rely on co-occurrence information from a corpus (either referring to bagsof-words, or focusing on target-specific types of features), and compare the distributional features of the compounds with those of the constituents, in order to predict the degree of compositionality of the 82 SMT Preprocessing Holzzaun Jägerzaun Input Holz 0.311 Holz wooden Zaun 0.825 Zaun fence Jäger 0.015 Zaun 0.725 Step 1: Identify Component Words Step 2: Predict Similarity Scores Jägerzaun Step"
W14-5709,2008.amta-papers.7,0,0.050281,"require disambiguation: Nießen and Ney (2000) use a parser for context-sensitive disambiguation, and Fritzinger and Fraser (2010) use corpus frequencies to find the best split for each compound. Other approaches use a two-step word alignment process: first, word alignment is performed on a split representation of the compounding language. Then, all former compound parts for which there is no aligned counterpart in the non-compounding language are merged back to the compound again. Finally, word alignment is re-run on this representation. See Koehn and Knight (2003) for experiments on German, DeNeefe et al. (2008) for Arabic and Bai et al. (2008) for Chinese. This blocks non-compositional compounds from being split if they are translated as one simplex English word in the training data (e.g. Heckensch¨utze, lit. ’hedge|shooter’; ’sniper’) and aligned correctly. However, cases like J¨agerzaun, ’lattice fence’ are not covered. In the present work, we identify compounds with a morphological analyser, disambiguated with corpus frequencies. Moreover, we restrict splitting to compositional compounds using distributional semantics. We are not aware of any previous work that takes semantics into account for co"
W14-5709,W10-1734,1,0.961425,"tituents J¨ager (’hunter’) and Zaun (’fence’). Here, an erroneous splitting of the compound can lead to wrong generalizations or translation pairs, such as J¨ager → lattice, in the absence of other evidence about how to translate J¨ager. When splitting compounds for SMT, two important factors should thus be considered: (1) whether a compound is compositional and should be split, and if so (2) how the compound should be split. Most previous approaches mainly focused on the second task, how to split a compound, e.g. using frequency statistics (Koehn and Knight, 2003) or a rule-based morphology (Fritzinger and Fraser, 2010), and all of them showed improved SMT quality for compound splitting. The decision about whether the compound is compositional and should be split at all has not received much attention in the past. In this work, we examine the effect of only splitting compositional compounds, in contrast to splitting all compounds. To this end, we combine (A) an approach relying on the distributional similarity between compounds and their constituents, to predict the degree of compositionality and thus to determine whether to split the compound with (B) a combination of morphological and frequency-based featu"
W14-5709,W11-2123,0,0.0163357,"s (version 4, cf. Koehn (2005)) and also newspaper texts, overall ca. 1.5 million sentences3 (roughly 44 million words). In addition, we 1 Baum|schule: ‘tree|school’ (tree nursery) Compounds not contained in the parallel data are always split, as they cannot be translated otherwise. 3 Data from the shared task of the EACL 2009 workshop on statistical machine translation: www.statmt.org/wmt09 2 84 use an English corpus of roughly 227 million words (including the English part of the parallel data) to build a target-side 5-gram language model with SRILM (Stolcke, 2002) in combination with KENLM (Heafield, 2011). For parameter tuning, we use 1,025 sentences of news data. Standard Test set 1,026 sentences of news data (test set from the 2009 WMT Shared Task): this set is to measure the translation quality on a standard SMT test and make it comparable to other work. Noun/Verb Test set As our main focus lies on sentences containing compounds, we created a second test set which is rich in compounds. From the combined 2008-2013 Shared Task test sets, we extracted all sentences containing at least one noun compound for which we have compound-constituent similarity scores. Moreover, we excluded sentences co"
W14-5709,E03-1076,0,0.418448,"’) cannot be represented by the meanings of its constituents J¨ager (’hunter’) and Zaun (’fence’). Here, an erroneous splitting of the compound can lead to wrong generalizations or translation pairs, such as J¨ager → lattice, in the absence of other evidence about how to translate J¨ager. When splitting compounds for SMT, two important factors should thus be considered: (1) whether a compound is compositional and should be split, and if so (2) how the compound should be split. Most previous approaches mainly focused on the second task, how to split a compound, e.g. using frequency statistics (Koehn and Knight, 2003) or a rule-based morphology (Fritzinger and Fraser, 2010), and all of them showed improved SMT quality for compound splitting. The decision about whether the compound is compositional and should be split at all has not received much attention in the past. In this work, we examine the effect of only splitting compositional compounds, in contrast to splitting all compounds. To this end, we combine (A) an approach relying on the distributional similarity between compounds and their constituents, to predict the degree of compositionality and thus to determine whether to split the compound with (B)"
W14-5709,2005.mtsummit-papers.11,0,0.0339958,"ata, i.e. the parallel data used to train the model, as well as the input for parameter tuning and testing.2 Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems (Koehn et al., 2007). We use it with default settings to train a translation model and we do so separately for each of the different compound splittings. Word alignment is performed using GIZA++ (Och and Ney, 2003). Feature weights are tuned using Batch-Mira (Cherry and Foster, 2012) with ’-safe-hope’ until convergence. Training Data Our parallel training data contains the Europarl corpus (version 4, cf. Koehn (2005)) and also newspaper texts, overall ca. 1.5 million sentences3 (roughly 44 million words). In addition, we 1 Baum|schule: ‘tree|school’ (tree nursery) Compounds not contained in the parallel data are always split, as they cannot be translated otherwise. 3 Data from the shared task of the EACL 2009 workshop on statistical machine translation: www.statmt.org/wmt09 2 84 use an English corpus of roughly 227 million words (including the English part of the parallel data) to build a target-side 5-gram language model with SRILM (Stolcke, 2002) in combination with KENLM (Heafield, 2011). For parameter"
W14-5709,W03-1810,0,0.218336,"1957; Harris, 1968) that ”you shall know a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest. Among many other tasks, distributional semantic information has been utilised to determine the degree of compositionality (or: semantic transparency) of various types of compounds, most notably regarding noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013), Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), K¨uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)). Typically, these approaches rely on co-occurrence information from a corpus (either referring to bagsof-words, or focusing on target-specific types of features), and compare the distributional features of the compounds with those of the constituents, in order to predict the degree of compositionality of the 82 SMT Preprocessing Holzzaun Jägerzaun Input Holz 0.311 Holz wooden Zaun 0.825 Zaun fence Jäger 0.015 Zaun 0.725 Step 1: Identify Component Words Step"
W14-5709,C00-2162,0,0.663192,"approaches. All lines of research improved translation performance due to compound splitting. In Koehn and Knight (2003), compounds are split through the identification of substrings from a corpus. The splitting is performed without linguistic knowledge (except for the insertion of the filler letters “(e)s”), which necessarily leads to many erroneous splittings. Multiple possible splitting options are disambiguated using the frequencies of the substrings. Starting from Koehn and Knight (2003), Stymne (2008) covers more morphological transformations and imposes POS constraints on the subwords. Nießen and Ney (2000) and Fritzinger and Fraser (2010) perform compound splitting by relying on morphological analysers to identify suitable split points. This has the advantage of returning only linguistically motivated splitting options, but the analyses are often ambiguous and require disambiguation: Nießen and Ney (2000) use a parser for context-sensitive disambiguation, and Fritzinger and Fraser (2010) use corpus frequencies to find the best split for each compound. Other approaches use a two-step word alignment process: first, word alignment is performed on a split representation of the compounding language."
W14-5709,J03-1002,0,0.00610431,"potentially infinite number of new forms. 4 Experimental Setting This section gives an overview on the technical details of the SMT system and our data sets. Compound splitting is applied to all source-language data, i.e. the parallel data used to train the model, as well as the input for parameter tuning and testing.2 Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems (Koehn et al., 2007). We use it with default settings to train a translation model and we do so separately for each of the different compound splittings. Word alignment is performed using GIZA++ (Och and Ney, 2003). Feature weights are tuned using Batch-Mira (Cherry and Foster, 2012) with ’-safe-hope’ until convergence. Training Data Our parallel training data contains the Europarl corpus (version 4, cf. Koehn (2005)) and also newspaper texts, overall ca. 1.5 million sentences3 (roughly 44 million words). In addition, we 1 Baum|schule: ‘tree|school’ (tree nursery) Compounds not contained in the parallel data are always split, as they cannot be translated otherwise. 3 Data from the shared task of the EACL 2009 workshop on statistical machine translation: www.statmt.org/wmt09 2 84 use an English corpus of"
W14-5709,P02-1040,0,0.0997756,"Missing"
W14-5709,I11-1024,0,0.334105,"t of lexical semantic research over the past 20 years. Based on the distributional hypothesis (Firth, 1957; Harris, 1968) that ”you shall know a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest. Among many other tasks, distributional semantic information has been utilised to determine the degree of compositionality (or: semantic transparency) of various types of compounds, most notably regarding noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013), Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), K¨uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)). Typically, these approaches rely on co-occurrence information from a corpus (either referring to bagsof-words, or focusing on target-specific types of features), and compare the distributional features of the compounds with those of the constituents, in order to predict the degree of compositionality of the 82 SMT Preprocessing Holzzaun Jägerzaun Input Ho"
W14-5709,E14-1050,0,0.246781,". Based on the distributional hypothesis (Firth, 1957; Harris, 1968) that ”you shall know a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest. Among many other tasks, distributional semantic information has been utilised to determine the degree of compositionality (or: semantic transparency) of various types of compounds, most notably regarding noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013), Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), K¨uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)). Typically, these approaches rely on co-occurrence information from a corpus (either referring to bagsof-words, or focusing on target-specific types of features), and compare the distributional features of the compounds with those of the constituents, in order to predict the degree of compositionality of the 82 SMT Preprocessing Holzzaun Jägerzaun Input Holz 0.311 Holz wooden Zaun 0.825 Zaun fence Jäger 0.015"
W14-5709,schafer-bildhauer-2012-building,0,0.0426085,"Missing"
W14-5709,schmid-etal-2004-smor,0,0.115596,", whereas anfangen ‘begin’ is more opaque with respect to fangen ‘catch’. In contrast, einsetzen has both transparent (e.g. ‘insert’) and opaque (e.g. ‘begin’) verb senses with respect to setzen ‘put/sit (down)’. The high degree of ambiguity makes particle verbs a challenge for NLP. Moreover, particle and base verb can occur separately (er f¨angt an: ‘he begins’) or in one word (dass er anf¨angt: ‘that he begins’), depending on the clausal type. This makes consistent treatment of particle verbs difficult. 3.2 Identification of Component Parts We use the rule-based morphological analyser SMOR (Schmid et al., 2004) to identify compounds and their constituents in our parallel training data (cf. Section 4). It relies on a large lexicon of word lemmas and feature rules for productive morphological processes in German, i.e., compounding, derivation and 83 inflection. In this paper, we will not consider splitting into derivational affixes (as needed for, e.g., Arabic and Turkish), but instead identify simplex words that may also occur independently. Moreover, we only keep noun compounds and particle verbs consisting of two constituents. The resulting set consists of 93,299 noun compound types and 3,689 parti"
W14-5709,S13-1038,1,0.724142,"Missing"
W14-5709,P07-2045,0,\N,Missing
W14-5814,W11-2501,0,0.110731,"ed similarity judgements from 51 subjects on 65 noun pairs, a seminal study which was later replicated by Miller and Charles (1991), and Resnik (1995). Finkelstein et al. (2002) created a set of 353 English noun-noun pairs rated by 16 subjects according to their semantic relatedness on a scale from 0 to 10. For German, Gurevych (2005) replicated Rubenstein and Goodenough’s experiments by translating the original 65 word pairs into German. In later work, she used the same experimental setup to increase the number of word pairs to 350 (Gurevych, 2006). The dataset most similar to ours is BLESS (Baroni and Lenci, 2011), a freely available dataset that includes 200 distinct English concrete nouns as target concepts, equally divided between living and nonliving entities, and grouped into 17 broad classes such as bird, fruit. For each target concept, BLESS contains several relata, connected to it through a semantic relation (hypernymy, co-hyponymy, meronymy, attribute, event), or through a null-relation. BLESS thus includes two paradigmatic relations (hypernymy, co-hyponymy) but does not focus on paradigmatic relations. Furthermore, it is restricted to concrete nouns, rather than working across word classes. 3"
W14-5814,I05-1067,0,0.175332,"ase, our results will be directly relevant for assessing, developing, and maintaining this resource. 2 Related work Over the years a number of datasets have been made available for studying and evaluating semantic relatedness. For English, Rubenstein and Goodenough (1965) obtained similarity judgements from 51 subjects on 65 noun pairs, a seminal study which was later replicated by Miller and Charles (1991), and Resnik (1995). Finkelstein et al. (2002) created a set of 353 English noun-noun pairs rated by 16 subjects according to their semantic relatedness on a scale from 0 to 10. For German, Gurevych (2005) replicated Rubenstein and Goodenough’s experiments by translating the original 65 word pairs into German. In later work, she used the same experimental setup to increase the number of word pairs to 350 (Gurevych, 2006). The dataset most similar to ours is BLESS (Baroni and Lenci, 2011), a freely available dataset that includes 200 distinct English concrete nouns as target concepts, equally divided between living and nonliving entities, and grouped into 17 broad classes such as bird, fruit. For each target concept, BLESS contains several relata, connected to it through a semantic relation (hyp"
W14-5814,W97-0802,0,0.0950824,"lexical-semantic word net for German, from which the set of target words was drawn (4.1). We then describe the selection of target words from GermaNet, which used a stratified sampling approach (4.2). Finally, we introduce the platform used to implement the experiments, Amazon Mechanical Turk (4.3). 4.1 Target source: GermaNet GermaNet is a lexical-semantic word net that aims to relate German nouns, verbs, and adjectives semantically. GermaNet has been modelled along the lines of the Princeton WordNet for English (Miller et al., 1990; Fellbaum, 1998) and shares its general design principles (Hamp and Feldweg, 1997; Kunze and Wagner, 1999; Lemnitzer and Kunze, 2007). For example, lexical units denoting the same concept are grouped into synonym sets (‘synsets’). These are in turn interlinked via conceptual-semantic relations (such as hypernymy) and lexical relations (such as antonymy). For each of the major word classes, the databases further take a number of semantic categories into consideration, expressed via top-level nodes in the semantic network (such as ‘Artefakt/artifact’, ‘Geschehen/event’, ‘Gef¨uhl/feeling’). However, in contrast to WordNet, GermaNet also includes so-called ‘artificial concepts"
W15-0104,W03-1812,0,0.241951,"(2011), Bullinaria and butional semantics tasks (Joanis et al. (2008), O Levy (2012), among others). The variants of our fine-grained syntactic approach are able to predict PV compositionality, but even though our model is (a) theoretically well-grounded, (b) supported by sophisticated generalization methods and (c) successful, a conceptually much simpler bag-of-words approach to the distributional representation of PVs cannot be outperformed. 2 Related Work & Motivation The problem of predicting degrees of PV compositionality is not new and has been addressed previously, mainly for English (Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005). For German, Schulte im Walde (2005) explored salient features at the syntax-semantics interface that determined the semantic nearest neighbors of German PVs. Relying on the insights of this study, K¨uhner and Schulte im Walde (2010) used unsupervised clustering to determine the degree of compositionality of German PVs. They hypothesized that compositional PVs tend to occur more often in the same clusters with their corresponding BVs than opaque PVs. Their approach relied on nominal complement heads in two modes, (1) with and (2) without explicit referen"
W15-0104,C10-1011,0,0.0180079,"less likely to result in sparse representations. For this reason, we apply a series of generalization techniques utilizing a lexical taxonomy and Topic Models, as well as SVD as a dimensionality reduction technique. 3 Experiments 3.1 Syntactic Slot Correspondence In order to build a model of syntactic transfer to predict PV compositionality, a pre-processing step determined a measure of syntactic slot correspondence. We selected the 5 most common subcategorization frames of each PV and each BV induced from dependency parses of the German web corpus SdeWaC containing approx. 880 million words (Bohnet, 2010; Faaß and Eckart, 2013). From these 5 most probable verb frames, we used all noun and prepositional phrase complement slots with nominal heads, except for adjuncts. Each PV slot was compared against each BV slot, by measuring the cosine between the vectors containing the complement heads as dimensions, and head counts1 within the slots as values. E.g. (see examples (3) and (4)), we found the nouns Licht and Taschenlampe (among others) both as instrumental PP (DAT-mit)2 of anleuchten and as subject (SBJ) of leuchten, and the cosine of this slot correspondence over all nouns was 0.9898. 3.2 Syn"
W15-0104,bott-schulte-im-walde-2014-optimizing,1,0.552063,"Missing"
W15-0104,S14-1022,1,0.906916,"Missing"
W15-0104,P13-4006,0,0.0184226,"GermaNet (GN) is the German version of WordNet (Hamp and Feldweg, 1997). We use the nth topmost taxonomy levels in the GermaNet hierarchy as generalizations of head nouns. In the case of multiple inheritance the counts of a subordinate node are distributed over the superordinated nodes. 2. LDA: We use the MALLET tool (McCallum, 2002) to create LDA topic generalizations for the head ´ S´eaghdha (2010). While LDA is usually applied over text documents, we nouns, in a similar way as O consider as document the set of noun heads in the same subcategorization slot. 3. SVD: We use the DISSECT tool (Dinu et al., 2013) to apply dimensionality reduction to the vectors of complement head nouns. 1 2 We used Local Mutual Information (LMI) (Evert, 2005). PP slots are marked with case and preposition. 36 3.4 Evaluation We evaluated our models against three gold standards (GS). Each of them contains PVs across different particles and was annotated by humans for the degree of compositionality: GS1: A gold standard collected by Hartmann (2008), consisting of 99 randomly selected PVs across 11 particles, balanced over 8 frequency ranges and judged by 4 experts on a scale from 0 to 10. GS2: A gold standard of 354 rand"
W15-0104,D11-1051,0,0.0205469,"-grained syntactic transfer information which is not accessible within a window-based distributional approach, while it should preserve an essential part of the information contained in context windows, since the head nouns within subcategorization frames typically appear in the local context. To compensate for the inevitable data sparseness, we employ the lexical taxonomy GermaNet (Hamp and Feldweg, 1997), Topic Models (Blei et al., 2003) and Singular Value Decomposition (SVD) to generalize over individual complement heads. All of them have proven effective in other distri´ S´eaghdha (2010), Guo and Diab (2011), Bullinaria and butional semantics tasks (Joanis et al. (2008), O Levy (2012), among others). The variants of our fine-grained syntactic approach are able to predict PV compositionality, but even though our model is (a) theoretically well-grounded, (b) supported by sophisticated generalization methods and (c) successful, a conceptually much simpler bag-of-words approach to the distributional representation of PVs cannot be outperformed. 2 Related Work & Motivation The problem of predicting degrees of PV compositionality is not new and has been addressed previously, mainly for English (Baldwin"
W15-0104,W97-0802,0,0.0729336,"xamples (3) and (4) above) would allow us to predict strong PV compositionality, even though the distributional similarity of identical complement types (e.g., the subjects) is low. Our novel approach exploits fine-grained syntactic transfer information which is not accessible within a window-based distributional approach, while it should preserve an essential part of the information contained in context windows, since the head nouns within subcategorization frames typically appear in the local context. To compensate for the inevitable data sparseness, we employ the lexical taxonomy GermaNet (Hamp and Feldweg, 1997), Topic Models (Blei et al., 2003) and Singular Value Decomposition (SVD) to generalize over individual complement heads. All of them have proven effective in other distri´ S´eaghdha (2010), Guo and Diab (2011), Bullinaria and butional semantics tasks (Joanis et al. (2008), O Levy (2012), among others). The variants of our fine-grained syntactic approach are able to predict PV compositionality, but even though our model is (a) theoretically well-grounded, (b) supported by sophisticated generalization methods and (c) successful, a conceptually much simpler bag-of-words approach to the distribut"
W15-0104,W03-1810,0,0.498806,"d butional semantics tasks (Joanis et al. (2008), O Levy (2012), among others). The variants of our fine-grained syntactic approach are able to predict PV compositionality, but even though our model is (a) theoretically well-grounded, (b) supported by sophisticated generalization methods and (c) successful, a conceptually much simpler bag-of-words approach to the distributional representation of PVs cannot be outperformed. 2 Related Work & Motivation The problem of predicting degrees of PV compositionality is not new and has been addressed previously, mainly for English (Baldwin et al., 2003; McCarthy et al., 2003; Bannard, 2005). For German, Schulte im Walde (2005) explored salient features at the syntax-semantics interface that determined the semantic nearest neighbors of German PVs. Relying on the insights of this study, K¨uhner and Schulte im Walde (2010) used unsupervised clustering to determine the degree of compositionality of German PVs. They hypothesized that compositional PVs tend to occur more often in the same clusters with their corresponding BVs than opaque PVs. Their approach relied on nominal complement heads in two modes, (1) with and (2) without explicit reference to the syntactic fun"
W15-0104,P10-1045,0,0.0389043,"Missing"
W15-0104,J01-3003,0,\N,Missing
W15-0105,P14-1023,0,0.538808,"vector spaces). However, increasing attention is being devoted to low-dimensional continuous word vector representations. Unlike count vectors, these continuous vectors are the result of supervised training of context-predicting models (predict vector spaces).1 Mikolov et al. (2013) reported that a predict vector space trained with a simplified neural language model (cf. Bengio et al. (2003)) seemingly encodes syntactic and semantic properties, which can be recovered directly from the space through linear translations, to solve analogies such as −−→ −−→ −−−→ −−−−→ king − man = queen − woman. Baroni et al. (2014) presented experiments where predict vectors outperform count vectors on several semantic benchmarks involving semantic relatedness, word clustering, and selectional preferences. Several open questions regarding predict vectors remain. In this paper, we focus on two shortcomings of previous analyses. First, the analogies in the “syntactic” and “semantic” benchmark datasets by Mikolov et al. (2013) in fact cover mostly morpho-syntactic relations – even in the semantic category. Consequently, it is still unknown to what extent predict vector spaces encode deep semantic relatedness, such as parad"
W15-0105,W11-2501,0,0.0612051,"to the WaCKy corpora (Baroni et al., 2009) –, antonyms, synonyms, and hypernyms were collected in an experiment hosted on Amazon Mechanical Turk. We constructed analogy questions by selecting only those target-response pairs that were submitted by at least four out of ten turkers. Then, we exhaustively combined all pairs for each word class and relation type.3 The resulting English dataset contains 7 516 analogies; the German dataset contains 2 462 analogies. In the same way, we created an analogy dataset with 10 000 unique analogy questions from the hypernymy and meronymy relations in BLESS (Baroni and Lenci, 2011), by randomly picking semantic relation pairs. BLESS is available only for English, but we included it in Sem-Para as it is a popular semantic benchmark. Overall, the Sem-Para dataset constitutes a deep semantic challenge, containing very specific, domainrelated and potentially low-frequent semantic details that are difficult to solve even for humans. For example, the tasks include antonyms such as biblical:secular::deaf:hearing or screech:whisper::ink:erase; hypernyms such as groove:dance::maze:puzzle; and synonyms such as skyline:horizon::rumor:gossip. The general semantic dataset (Sem-Gen)"
W15-0105,E14-1049,0,0.0213118,"of deep semantic analogies has been performed so far. Second, it remains unclear whether comparable performance can be achieved for a wider range of relations in morphologically rich languages, as most previous work on predict vectors worked with English data. A notable exception is Zuanovi´c et al. (2014), who achieved strong performance for superlative and country-capital analogies in Croatian. Wolf et al. (2013) learned mappings of predict vectors between English, Hebrew, and Arabic, but provided no deeper insight into the model’s capabilities on a direct evaluation of semantic relations. Faruqui and Dyer (2014) trained predict vectors using two languages, but evaluated only in English. We present a systematic exploration of morpho-syntactic and semantic relatedness in English and the morphologically richer language German. We show detailed results of the continuous bag-of-words model (CBOW) by Mikolov et al. (2013), which we apply to equivalent morpho-syntactic tasks for both 1 The terminology follows Baroni et al. (2014). 40 Proceedings of the 11th International Conference on Computational Semantics, pages 40–45, c London, UK, April 15-17 2015. 2015 Association for Computational Linguistics languag"
W15-0105,I05-1067,0,0.326797,"nge, containing very specific, domainrelated and potentially low-frequent semantic details that are difficult to solve even for humans. For example, the tasks include antonyms such as biblical:secular::deaf:hearing or screech:whisper::ink:erase; hypernyms such as groove:dance::maze:puzzle; and synonyms such as skyline:horizon::rumor:gossip. The general semantic dataset (Sem-Gen) does not require to solve analogies but to predict the degree of semantic relatedness between word pairs. It contains three semantic benchmarks: 1. RG (Rubenstein and Goodenough, 1965) and its German equivalent Gur65 (Gurevych, 2005). 2. WordSim353 (Finkelstein et al., 2001) and its translation into German WordSim280 by Schmidt et al. (2011): As Schmidt et al. did not re-rate the German relation pairs after translation (which we considered necessary due to potential meaning shifts), we collected new ratings for the German pairs from 10 subjects, applying the same conditions as the original WordSim353 collection task. To ensure identical size for both languages, we reduced the English data to the common 280 pairs. 2 The new datasets are available at http://www.ims.uni-stuttgart.de/data/analogies/. Regarding hypernymy and m"
W15-0105,W14-1618,0,0.0272223,"ct models are the standard CBOW and SKIP-gram models, trained with the word2vec toolkit (Mikolov et al., 2013). We use negative sampling with 15 negative samples, 400 dimensions, a symmetrical window of size 2, subsampling with p = 10−5 , and a frequency threshold of 50 to filter out rare words. Our count model is a standard bag-of-words model with positive point-wise mutual information weighting and dimensionality reduction through singular value decomposition. The dimensionality and the window size were set identical to the predict vectors. We solve analogy tasks with the 3C OS M UL method (Levy and Goldberg, 2014), and similarity tasks with cosine similarity. For the Google, TOEFL, and Sem-Para tasks, we report accuracy; for RG and WordSim we report Spearman’s rank-order correlation coefficient ρ. 3.2 Results Table 1 compares the word-based (W) and lemma-based (L) results of the English (EN) and the German (DE) predict vs. count models. We first confirm previous insight (Baroni et al., 2014) that the predict models (CBOW; SKIP) in most cases outperform the count models (BOW). Second, we also confirm that the SKIP-gram model outperforms CBOW only on Google-Sem (Mikolov et al., 2013). Third, we find that"
W15-0105,W14-1619,0,0.0641451,"antic relatedness, word clustering, and selectional preferences. Several open questions regarding predict vectors remain. In this paper, we focus on two shortcomings of previous analyses. First, the analogies in the “syntactic” and “semantic” benchmark datasets by Mikolov et al. (2013) in fact cover mostly morpho-syntactic relations – even in the semantic category. Consequently, it is still unknown to what extent predict vector spaces encode deep semantic relatedness, such as paradigmatic relations. Rei and Briscoe (2014) offered some insight by testing hypernymy relations through similarity; Melamud et al. (2014) investigated synonymy, hypernymy, and co-hyponymy relations. However, no systematic evaluation of deep semantic analogies has been performed so far. Second, it remains unclear whether comparable performance can be achieved for a wider range of relations in morphologically rich languages, as most previous work on predict vectors worked with English data. A notable exception is Zuanovi´c et al. (2014), who achieved strong performance for superlative and country-capital analogies in Croatian. Wolf et al. (2013) learned mappings of predict vectors between English, Hebrew, and Arabic, but provided"
W15-0105,N13-1090,0,0.785568,"nd semantic relations. Our results show that (i) morphological complexity causes a drop in accuracy, and (ii) continuous representations lack the ability to solve analogies of paradigmatic relations. 1 Introduction Until recently, the majority of research on semantic spaces concentrated on vector spaces relying on context counts (count vector spaces). However, increasing attention is being devoted to low-dimensional continuous word vector representations. Unlike count vectors, these continuous vectors are the result of supervised training of context-predicting models (predict vector spaces).1 Mikolov et al. (2013) reported that a predict vector space trained with a simplified neural language model (cf. Bengio et al. (2003)) seemingly encodes syntactic and semantic properties, which can be recovered directly from the space through linear translations, to solve analogies such as −−→ −−→ −−−→ −−−−→ king − man = queen − woman. Baroni et al. (2014) presented experiments where predict vectors outperform count vectors on several semantic benchmarks involving semantic relatedness, word clustering, and selectional preferences. Several open questions regarding predict vectors remain. In this paper, we focus on t"
W15-0105,D07-1060,0,0.0165767,"P BOW CBOW SKIP BOW CBOW SKIP BOW CBOW SKIP BOW CBOW SKIP BOW 68.8 68.3 42.4 43.5 71.8 71.8 45.9 45.9 39.5 40.3 27.3 28.9 81.9 47.1 48.4 31.8 80.5 47.4 47.1 31.5 57.9 29.3 31.0 23.7 77.9 80.5 75.6 73.3 77.8 78.6 73.3 75.7 77.8 66.4 58.9 64.7 19.3 18.4 14.7 15.1 16.4 15.9 14.4 13.8 15.6 15.8 14.8 14.9 96.2 90.0 69.0 69.4 96.2 87.5 68.3 68.5 72.2 66.2 54.4 55.8 Table 1: Results (ρ for Sem-Gen, accuracy for others) by task category across models. 3. 80 TOEFL (Test of English as a Foreign Language) questions by Landauer and Dumais (1997) for English, and 426 questions from a similar collection by Mohammad et al. (2007) for German. Each semantic similarity question is multiple choice, with four alternatives for a given stem. Unlike the original English TOEFL data, the German dataset also contains phrases, which we disregarded. 2.2 Corpora We obtain vectors using the COW web corpora ENCOW14 for English and DECOW12 for German (Sch¨afer and Bildhauer, 2012). The corpora contain lemma and part-of-speech annotations. In addition, we applied some basic pre-processing: we removed non-alphanumeric tokens and sentences with fewer than four words, and we lowercased all tokens. In order to limit effects of corpus size,"
W15-0105,W14-1608,0,0.00709017,"nts where predict vectors outperform count vectors on several semantic benchmarks involving semantic relatedness, word clustering, and selectional preferences. Several open questions regarding predict vectors remain. In this paper, we focus on two shortcomings of previous analyses. First, the analogies in the “syntactic” and “semantic” benchmark datasets by Mikolov et al. (2013) in fact cover mostly morpho-syntactic relations – even in the semantic category. Consequently, it is still unknown to what extent predict vector spaces encode deep semantic relatedness, such as paradigmatic relations. Rei and Briscoe (2014) offered some insight by testing hypernymy relations through similarity; Melamud et al. (2014) investigated synonymy, hypernymy, and co-hyponymy relations. However, no systematic evaluation of deep semantic analogies has been performed so far. Second, it remains unclear whether comparable performance can be achieved for a wider range of relations in morphologically rich languages, as most previous work on predict vectors worked with English data. A notable exception is Zuanovi´c et al. (2014), who achieved strong performance for superlative and country-capital analogies in Croatian. Wolf et al"
W15-0105,schafer-bildhauer-2012-building,0,0.0379249,"Missing"
W15-0105,W14-5814,1,0.907781,"Missing"
W15-0903,W11-0815,0,0.0160317,"ring measures of association strength within multiword expressions, in order to distinguish literal from collocational interpretations, is probably (Evert, 2005). Addressing the compositionality of multiword expressions is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its parts, and what the expression means. Examples of applications that have profited from integrating the semantics of multiword expressions are Part-ofSpeech Tagging (Constant and Sigogne, 2011), Parsing (Wehrli, 2014), Information Retrieval (Acosta et al., 2011), and SMT (Carpuat and Diab, 2010; Weller et al., 2014), see below for details. 3 http://multiword.sourceforge.net 21 MWEs in SMT: Previous work regarding multiword expressions in SMTcan be divided into static approaches, where the training data is modified in order to facilitate a standard SMT system to learn suitable MWE translations and dynamic approaches where the modification takes place in the phrase table of the SMT system. Static approaches include (Lambert and Banchs, 2005), who first extract bilingual – English and Spanish – MWEs based on parsed data and then merge them into “super-t"
W15-0903,W02-2001,0,0.0825391,"ons have been a recurrent focus of attention within theoretical, cognitive, and in the last decade also within computational linguistics: The workshops on multiword expressions attached to major CL conferences3 celebrated their 10th anniversary in 2014, and the SIGLEX-MWE has initiated three special issues in NLP journals. After initial approaches mainly focused on characterising the computationally challenging properties of multiword expressions (such as Sag et al. (2002) and Villavicencio et al. (2005)) and automatically identifying various types of multiword expressions in corpora (such as Baldwin and Villavicencio (2002), Villavicencio (2003) and Bannard (2007) who extracted English particle verbs), the focus of interest moved towards deeper semantic models of specific types of multiword expressions and towards integrating multiword expressions into applications. Compositionality of MWEs: A wide range of semantic approaches has been concerned with distinguishing degrees of compositionality within multiword expressions, addressing • noun compounds (Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013)), • particle verbs (McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (200"
W15-0903,W07-1101,0,0.0188376,"ical, cognitive, and in the last decade also within computational linguistics: The workshops on multiword expressions attached to major CL conferences3 celebrated their 10th anniversary in 2014, and the SIGLEX-MWE has initiated three special issues in NLP journals. After initial approaches mainly focused on characterising the computationally challenging properties of multiword expressions (such as Sag et al. (2002) and Villavicencio et al. (2005)) and automatically identifying various types of multiword expressions in corpora (such as Baldwin and Villavicencio (2002), Villavicencio (2003) and Bannard (2007) who extracted English particle verbs), the focus of interest moved towards deeper semantic models of specific types of multiword expressions and towards integrating multiword expressions into applications. Compositionality of MWEs: A wide range of semantic approaches has been concerned with distinguishing degrees of compositionality within multiword expressions, addressing • noun compounds (Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013)), • particle verbs (McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), Ramisch et al. (2008)), • light-/supp"
W15-0903,C10-1011,0,0.0325758,"rb-Object Pair Extraction To obtain a set of SVCs, we first extract verb-object pairs from dependency-parsed data. In a second step, all of these potential SVCs are scored and ranked by association measures. The SVC candidates with the highest association scores constitute the set of SVCs to be marked in both the parallel training data as well 22 as in the data to be translated. For extracting the SVC candidates, we follow the extraction method outlined in Scheible et al. (2013) who describe a set of guidelines to induce the complete set of argument and adjunct phrases from dependency-parses (Bohnet, 2010). While in this study we focus on verb-object pairs, our extraction method allows for an easy extension to also cover other types of SVCs, such as preposition+noun+verb triples. The example given in Table 4 illustrates the need for parsed data when working with German: due to the flexible word order already illustrated in Section 2, verb and object are often not adjacent, but allow for the insertion of several phrases ([the giant planet]SU BJ [for the development [of the solar system]P P ]P P ) or sub-ordinate clauses between them. Furthermore, parsed data allows for an extraction of verb-obje"
W15-0903,N10-1029,0,0.745573,"SVC. In this paper, we will show that we can achieve improved translation quality by marking the verbs that occur within V+NP SVCs. The marking distinguishes the SVC verbs from independent occurrences of the verb and thus enables the SMT system to learn different translations for the different kinds of occurrences. We focus on German SVCs, which are particularly challenging due to the morphological richness and the relatively free word 19 Proceedings of NAACL-HLT 2015, pages 19–28, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics order in German. While Carpuat and Diab (2010) included some English SVCs into their pilot study on evaluating MWEs through SMT, to our knowledge there is no other previous work on SVCs in the context of SMT. 2 SVCs in Statistical Machine Translation Default translation: In SMT, translations are “learned” from parallel data. Out of a set of possible translations derived from that data, the SMT decoder selects the most probable one. Today, most SMT systems translate whole phrases instead of single words, which allows to take some context of the word into account. Moreover, a language model and an reodering model are consulted in order to p"
W15-0903,N12-1047,0,0.0222857,"riments Baseline Exp1000 Exp500 Exp350 Exp250 BLEU tuning1 20.43 21.04 21.08 20.86 20.85 BLEU tuning2 20.49 21.01 21.01 20.89 20.84 System Reference Baseline Exp1000 Exp500 Exp350 Exp250 Table 8: Number of sentences produced by the systems, which contain at least one full verb. Table 7: BLEU scores on the 2014 testset. 5.2 System Details We used the Moses toolkit (Koehn et al., 2007) to train standard phrase-based systems with default configurations. We trained an English 5-gram language model using KenLM (Heafield, 2011). For tuning the feature weights, we applied batch-mira with -safe-hope (Cherry and Foster, 2012). In order to ensure stable tuning, we performed two subsequent tuning procedures with identical starting conditions and report on results for both of them. 6 Evaluation In order to evaluate the translation quality of our systems in comparison to each other and also to a baseline without any markup, we performed a standard MT evaluation using the BLEU metric. In addition, we also performed a semi-automatic evaluation with a focus on verb translations. 6.1 Automatic MT Evaluation It is common practise to evaluate the performance of an SMT system by comparing its output to one (or more) human re"
W15-0903,D14-1024,0,0.520382,"anslations and dynamic approaches where the modification takes place in the phrase table of the SMT system. Static approaches include (Lambert and Banchs, 2005), who first extract bilingual – English and Spanish – MWEs based on parsed data and then merge them into “super-tokens”, which later is treated as a unit by the SMT system. Similarly, Carpuat and Diab (2010) merge parts of English MWEs extracted from lexica into larger units in order to improve English to Arabic SMT. In addition, they increase the maximal phrase size from 5 in conventional systems to 10 words per phrase. More recently, Cholakov and Kordoni (2014) described a static approach to handle English phrasal verbs – extracted from lexical ressources – for translation into Bulgarian, where the particles are usually not separated from the verbs. While static approaches have shown to improve translation quality, they do not allow for contextdependent decisions on how to translate MWEs. Instead of modifying MWEs in the training data, dynamic approaches handle MWEs directly in the phrase table of the SMT system. Ren et al. (2009) present an approach to handle bilingual Chinese English MWEs. These are extracted from domainspecific parallel text and"
W15-0903,W11-0809,0,0.0303154,"Fazly and Stevenson (2008), Evert (2009)) The most prominent approach exploring measures of association strength within multiword expressions, in order to distinguish literal from collocational interpretations, is probably (Evert, 2005). Addressing the compositionality of multiword expressions is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its parts, and what the expression means. Examples of applications that have profited from integrating the semantics of multiword expressions are Part-ofSpeech Tagging (Constant and Sigogne, 2011), Parsing (Wehrli, 2014), Information Retrieval (Acosta et al., 2011), and SMT (Carpuat and Diab, 2010; Weller et al., 2014), see below for details. 3 http://multiword.sourceforge.net 21 MWEs in SMT: Previous work regarding multiword expressions in SMTcan be divided into static approaches, where the training data is modified in order to facilitate a standard SMT system to learn suitable MWE translations and dynamic approaches where the modification takes place in the phrase table of the SMT system. Static approaches include (Lambert and Banchs, 2005), who first extract bilingual – English and"
W15-0903,W06-1207,0,0.0234877,"nd Villavicencio (2002), Villavicencio (2003) and Bannard (2007) who extracted English particle verbs), the focus of interest moved towards deeper semantic models of specific types of multiword expressions and towards integrating multiword expressions into applications. Compositionality of MWEs: A wide range of semantic approaches has been concerned with distinguishing degrees of compositionality within multiword expressions, addressing • noun compounds (Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013)), • particle verbs (McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), Ramisch et al. (2008)), • light-/support-verb constructions (Bannard (2007), Fazly et al. (2007), Fazly et al. (2009)), • various MWE types (Lin (1999), Katz and Giesbrecht (2006), Fazly and Stevenson (2008), Evert (2009)) The most prominent approach exploring measures of association strength within multiword expressions, in order to distinguish literal from collocational interpretations, is probably (Evert, 2005). Addressing the compositionality of multiword expressions is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole"
W15-0903,J09-1005,0,0.0368296,"moved towards deeper semantic models of specific types of multiword expressions and towards integrating multiword expressions into applications. Compositionality of MWEs: A wide range of semantic approaches has been concerned with distinguishing degrees of compositionality within multiword expressions, addressing • noun compounds (Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013)), • particle verbs (McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), Ramisch et al. (2008)), • light-/support-verb constructions (Bannard (2007), Fazly et al. (2007), Fazly et al. (2009)), • various MWE types (Lin (1999), Katz and Giesbrecht (2006), Fazly and Stevenson (2008), Evert (2009)) The most prominent approach exploring measures of association strength within multiword expressions, in order to distinguish literal from collocational interpretations, is probably (Evert, 2005). Addressing the compositionality of multiword expressions is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its parts, and what the expression means. Examples of applications that have profited from integrating the"
W15-0903,W11-2123,0,0.0162936,"testing the most recent test set of 2014 (∼3,000 sentences each). 4 www.statmt.org Experiments Baseline Exp1000 Exp500 Exp350 Exp250 BLEU tuning1 20.43 21.04 21.08 20.86 20.85 BLEU tuning2 20.49 21.01 21.01 20.89 20.84 System Reference Baseline Exp1000 Exp500 Exp350 Exp250 Table 8: Number of sentences produced by the systems, which contain at least one full verb. Table 7: BLEU scores on the 2014 testset. 5.2 System Details We used the Moses toolkit (Koehn et al., 2007) to train standard phrase-based systems with default configurations. We trained an English 5-gram language model using KenLM (Heafield, 2011). For tuning the feature weights, we applied batch-mira with -safe-hope (Cherry and Foster, 2012). In order to ensure stable tuning, we performed two subsequent tuning procedures with identical starting conditions and report on results for both of them. 6 Evaluation In order to evaluate the translation quality of our systems in comparison to each other and also to a baseline without any markup, we performed a standard MT evaluation using the BLEU metric. In addition, we also performed a semi-automatic evaluation with a focus on verb translations. 6.1 Automatic MT Evaluation It is common practi"
W15-0903,W06-1203,0,0.112124,"Missing"
W15-0903,2005.mtsummit-papers.11,0,0.250273,"y occur between the components of the SVC “Beitrag leisten”. Note that some SVCs allow for more intervening words than others. In Table 3, the comparison of the average distance between the verb and the noun within the two SVCs “Beitrag leisten” and “Rechnung tragen” shows considerable differences. SVC Beitrag leisten to make a contribution Rechnung tragen to account for distance 5.44 2.62 Table 3: Average distance of SVC components. The mean distances are derived from 3,549 occurrences of the SVC “Beitrag leisten” and 1,868 occurrences of the SVC “Rechnung tragen” within the Europarl corpus (Koehn, 2005). They were calculated by substracting the lower position in the sentence from the higher position for either the noun or the verb. Whenever the verb and the noun occurred directly adjacent, the score yields “1”. Methodology: In order to enable the SMT system to distinguish occurrences of a verb within an SVC from independent occurrences, we add a special markup to the verbs occurring in an SVC. By introducing this markup, the translations for independent verbs with a literal meaning are separated from those of verbs occurring in an SVC context. Thus, the SMT system can learn the SVC-translati"
W15-0903,2005.mtsummit-posters.11,0,0.41515,"rd expressions are Part-ofSpeech Tagging (Constant and Sigogne, 2011), Parsing (Wehrli, 2014), Information Retrieval (Acosta et al., 2011), and SMT (Carpuat and Diab, 2010; Weller et al., 2014), see below for details. 3 http://multiword.sourceforge.net 21 MWEs in SMT: Previous work regarding multiword expressions in SMTcan be divided into static approaches, where the training data is modified in order to facilitate a standard SMT system to learn suitable MWE translations and dynamic approaches where the modification takes place in the phrase table of the SMT system. Static approaches include (Lambert and Banchs, 2005), who first extract bilingual – English and Spanish – MWEs based on parsed data and then merge them into “super-tokens”, which later is treated as a unit by the SMT system. Similarly, Carpuat and Diab (2010) merge parts of English MWEs extracted from lexica into larger units in order to improve English to Arabic SMT. In addition, they increase the maximal phrase size from 5 in conventional systems to 10 words per phrase. More recently, Cholakov and Kordoni (2014) described a static approach to handle English phrasal verbs – extracted from lexical ressources – for translation into Bulgarian, wh"
W15-0903,P99-1041,0,0.146889,"ecific types of multiword expressions and towards integrating multiword expressions into applications. Compositionality of MWEs: A wide range of semantic approaches has been concerned with distinguishing degrees of compositionality within multiword expressions, addressing • noun compounds (Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013)), • particle verbs (McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), Ramisch et al. (2008)), • light-/support-verb constructions (Bannard (2007), Fazly et al. (2007), Fazly et al. (2009)), • various MWE types (Lin (1999), Katz and Giesbrecht (2006), Fazly and Stevenson (2008), Evert (2009)) The most prominent approach exploring measures of association strength within multiword expressions, in order to distinguish literal from collocational interpretations, is probably (Evert, 2005). Addressing the compositionality of multiword expressions is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its parts, and what the expression means. Examples of applications that have profited from integrating the semantics of multiword expression"
W15-0903,W03-1810,0,0.457764,"xpressions in corpora (such as Baldwin and Villavicencio (2002), Villavicencio (2003) and Bannard (2007) who extracted English particle verbs), the focus of interest moved towards deeper semantic models of specific types of multiword expressions and towards integrating multiword expressions into applications. Compositionality of MWEs: A wide range of semantic approaches has been concerned with distinguishing degrees of compositionality within multiword expressions, addressing • noun compounds (Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013)), • particle verbs (McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), Ramisch et al. (2008)), • light-/support-verb constructions (Bannard (2007), Fazly et al. (2007), Fazly et al. (2009)), • various MWE types (Lin (1999), Katz and Giesbrecht (2006), Fazly and Stevenson (2008), Evert (2009)) The most prominent approach exploring measures of association strength within multiword expressions, in order to distinguish literal from collocational interpretations, is probably (Evert, 2005). Addressing the compositionality of multiword expressions is a crucial ingredient for lexicography and NLP applications, to know whether"
W15-0903,2001.mtsummit-papers.68,0,0.0458189,"g procedures with identical starting conditions and report on results for both of them. 6 Evaluation In order to evaluate the translation quality of our systems in comparison to each other and also to a baseline without any markup, we performed a standard MT evaluation using the BLEU metric. In addition, we also performed a semi-automatic evaluation with a focus on verb translations. 6.1 Automatic MT Evaluation It is common practise to evaluate the performance of an SMT system by comparing its output to one (or more) human reference translations. We follow this line and calculate BLEU scores (Papineni et al., 2001) for each of our systems. Our testset is taken from the 2014 shared task on statistical machine translation (∼ 3,000 words). We tested all BLEU scores for statistical significance using pairwise bootstrap resampling with sample size 1,000 and a p-value of 0.055 . Results are givenin Table 7. Compared to the baseline, we found that all of our systems containing verb markup for SVC verbs lead to a significant improvement in terms of BLEU. The fact that all investigated sets of automatically identified SVCs improve the translation quality in the same magnitude shows that no manual filtering 5 # s"
W15-0903,W08-2107,0,0.0624375,"llavicencio (2003) and Bannard (2007) who extracted English particle verbs), the focus of interest moved towards deeper semantic models of specific types of multiword expressions and towards integrating multiword expressions into applications. Compositionality of MWEs: A wide range of semantic approaches has been concerned with distinguishing degrees of compositionality within multiword expressions, addressing • noun compounds (Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013)), • particle verbs (McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), Ramisch et al. (2008)), • light-/support-verb constructions (Bannard (2007), Fazly et al. (2007), Fazly et al. (2009)), • various MWE types (Lin (1999), Katz and Giesbrecht (2006), Fazly and Stevenson (2008), Evert (2009)) The most prominent approach exploring measures of association strength within multiword expressions, in order to distinguish literal from collocational interpretations, is probably (Evert, 2005). Addressing the compositionality of multiword expressions is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its parts,"
W15-0903,I11-1024,0,0.445807,"t al. (2005)) and automatically identifying various types of multiword expressions in corpora (such as Baldwin and Villavicencio (2002), Villavicencio (2003) and Bannard (2007) who extracted English particle verbs), the focus of interest moved towards deeper semantic models of specific types of multiword expressions and towards integrating multiword expressions into applications. Compositionality of MWEs: A wide range of semantic approaches has been concerned with distinguishing degrees of compositionality within multiword expressions, addressing • noun compounds (Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013)), • particle verbs (McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), Ramisch et al. (2008)), • light-/support-verb constructions (Bannard (2007), Fazly et al. (2007), Fazly et al. (2009)), • various MWE types (Lin (1999), Katz and Giesbrecht (2006), Fazly and Stevenson (2008), Evert (2009)) The most prominent approach exploring measures of association strength within multiword expressions, in order to distinguish literal from collocational interpretations, is probably (Evert, 2005). Addressing the compositionality of multiword expressions is a"
W15-0903,W09-2907,0,0.181414,"Missing"
W15-0903,S13-1038,1,0.780239,"Missing"
W15-0903,W03-1808,0,0.0340077,"attention within theoretical, cognitive, and in the last decade also within computational linguistics: The workshops on multiword expressions attached to major CL conferences3 celebrated their 10th anniversary in 2014, and the SIGLEX-MWE has initiated three special issues in NLP journals. After initial approaches mainly focused on characterising the computationally challenging properties of multiword expressions (such as Sag et al. (2002) and Villavicencio et al. (2005)) and automatically identifying various types of multiword expressions in corpora (such as Baldwin and Villavicencio (2002), Villavicencio (2003) and Bannard (2007) who extracted English particle verbs), the focus of interest moved towards deeper semantic models of specific types of multiword expressions and towards integrating multiword expressions into applications. Compositionality of MWEs: A wide range of semantic approaches has been concerned with distinguishing degrees of compositionality within multiword expressions, addressing • noun compounds (Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013)), • particle verbs (McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), Ramisch et al. (20"
W15-0903,W14-0804,0,0.0301428,"09)) The most prominent approach exploring measures of association strength within multiword expressions, in order to distinguish literal from collocational interpretations, is probably (Evert, 2005). Addressing the compositionality of multiword expressions is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its parts, and what the expression means. Examples of applications that have profited from integrating the semantics of multiword expressions are Part-ofSpeech Tagging (Constant and Sigogne, 2011), Parsing (Wehrli, 2014), Information Retrieval (Acosta et al., 2011), and SMT (Carpuat and Diab, 2010; Weller et al., 2014), see below for details. 3 http://multiword.sourceforge.net 21 MWEs in SMT: Previous work regarding multiword expressions in SMTcan be divided into static approaches, where the training data is modified in order to facilitate a standard SMT system to learn suitable MWE translations and dynamic approaches where the modification takes place in the phrase table of the SMT system. Static approaches include (Lambert and Banchs, 2005), who first extract bilingual – English and Spanish – MWEs based on"
W15-0903,W14-5709,1,0.87771,"Missing"
W15-0903,P02-1040,0,\N,Missing
W15-0903,P07-2045,0,\N,Missing
W15-1008,2009.eamt-1.9,0,0.0256613,"zed elements are often not adjacent. We use a morphology-aware SMT system which first translates into a lemmatized representation with a component to generate fully inflected forms in a second step, see Toutanova et al. (2008) and Fraser et al. (2012). The inflection step requires the modeling of the grammatical case of noun phrases, which corresponds to determining the syntactic function. Weller et al. (2013) describe modeling case in SMT; we extend their setup to cover the prediction of prepositions in both PP and NPs (i.e., the “empty” preposition). The presented work is similar to that of Agirre et al. (2009), but is applied to a fully statistical MT system. A detailed presentation of our work including a full literature survey can be found in Weller et al. (2015). Methodology To build the translation model, we use an abstract target-language representation in which nouns, adjectives and articles are lemmatized, and prepositions are substituted with place-holders. Additionally, “empty” place-holder prepositions are inserted at the beginning of noun phrases. To obtain a symmetric data structure, “empty” place-holders are also added to source-side NPs. When generating surface forms for the translati"
W15-1008,E12-1068,1,0.813073,"tep: all subcategorized elements of a verb are considered and allotted to their respective functions – as PPs with an overt preposition or as NPs with an “empty” preposition, e.g. to call for sth. → ∅ etw. erfordern. The language model and the translation rules often fail to correctly model subcategorization in standard SMT systems because verbs and their subcategorized elements are often not adjacent. We use a morphology-aware SMT system which first translates into a lemmatized representation with a component to generate fully inflected forms in a second step, see Toutanova et al. (2008) and Fraser et al. (2012). The inflection step requires the modeling of the grammatical case of noun phrases, which corresponds to determining the syntactic function. Weller et al. (2013) describe modeling case in SMT; we extend their setup to cover the prediction of prepositions in both PP and NPs (i.e., the “empty” preposition). The presented work is similar to that of Agirre et al. (2009), but is applied to a fully statistical MT system. A detailed presentation of our work including a full literature survey can be found in Weller et al. (2015). Methodology To build the translation model, we use an abstract target-l"
W15-1008,P08-1059,0,0.0327594,"ons in the post-processing step: all subcategorized elements of a verb are considered and allotted to their respective functions – as PPs with an overt preposition or as NPs with an “empty” preposition, e.g. to call for sth. → ∅ etw. erfordern. The language model and the translation rules often fail to correctly model subcategorization in standard SMT systems because verbs and their subcategorized elements are often not adjacent. We use a morphology-aware SMT system which first translates into a lemmatized representation with a component to generate fully inflected forms in a second step, see Toutanova et al. (2008) and Fraser et al. (2012). The inflection step requires the modeling of the grammatical case of noun phrases, which corresponds to determining the syntactic function. Weller et al. (2013) describe modeling case in SMT; we extend their setup to cover the prediction of prepositions in both PP and NPs (i.e., the “empty” preposition). The presented work is similar to that of Agirre et al. (2009), but is applied to a fully statistical MT system. A detailed presentation of our work including a full literature survey can be found in Weller et al. (2015). Methodology To build the translation model, we"
W15-1008,P13-1058,1,0.85305,"preposition, e.g. to call for sth. → ∅ etw. erfordern. The language model and the translation rules often fail to correctly model subcategorization in standard SMT systems because verbs and their subcategorized elements are often not adjacent. We use a morphology-aware SMT system which first translates into a lemmatized representation with a component to generate fully inflected forms in a second step, see Toutanova et al. (2008) and Fraser et al. (2012). The inflection step requires the modeling of the grammatical case of noun phrases, which corresponds to determining the syntactic function. Weller et al. (2013) describe modeling case in SMT; we extend their setup to cover the prediction of prepositions in both PP and NPs (i.e., the “empty” preposition). The presented work is similar to that of Agirre et al. (2009), but is applied to a fully statistical MT system. A detailed presentation of our work including a full literature survey can be found in Weller et al. (2015). Methodology To build the translation model, we use an abstract target-language representation in which nouns, adjectives and articles are lemmatized, and prepositions are substituted with place-holders. Additionally, “empty” place-ho"
W15-1008,W15-4923,1,0.781665,"fully inflected forms in a second step, see Toutanova et al. (2008) and Fraser et al. (2012). The inflection step requires the modeling of the grammatical case of noun phrases, which corresponds to determining the syntactic function. Weller et al. (2013) describe modeling case in SMT; we extend their setup to cover the prediction of prepositions in both PP and NPs (i.e., the “empty” preposition). The presented work is similar to that of Agirre et al. (2009), but is applied to a fully statistical MT system. A detailed presentation of our work including a full literature survey can be found in Weller et al. (2015). Methodology To build the translation model, we use an abstract target-language representation in which nouns, adjectives and articles are lemmatized, and prepositions are substituted with place-holders. Additionally, “empty” place-holder prepositions are inserted at the beginning of noun phrases. To obtain a symmetric data structure, “empty” place-holders are also added to source-side NPs. When generating surface forms for the translation output, a phrase with a place-holder preposition can be realized as a noun phrase (empty preposition) or as a prepositional phrase by generating the prepos"
W15-4923,2009.eamt-1.9,0,0.417564,"In addition, we compare prepositions in the machine translation output with those in the reference translation for a selected subset. Finally, we discuss examples illustrating typical problems of translating prepositions. 2 Related Work Most research on translating prepositions has been reported for rule-based systems. Naskar and Bandyopadhyay (2006) outline a method to handle prepositions in an English-Bengali MT system using WordNet and an example base for idiomatic PPs. Gustavii (2005) uses bilingual features and selectional constraints to correct translations in a Swedish-English system. Agirre et al. (2009) model Basque prepositions and grammatical case using syntactic-semantic features such as subcategorization triples for a rule-based system which leads to an improved translation quality for prepositions. Shilon et al. (2012) extend this approach 177 input ∅ −→ what role ∅ −→ the giant planet has played in −→ the development of −→ the solar system lemmatized SMT output PREP welch<PWAT> Rolle<+NN><Fem><Sg> PREP die<+ART><Def> riesig<ADJ> Planet<+NN><Masc><Sg> gespielt<VVPP> hat<VAFIN> PREP die<+ART><Def> Entwicklung<+NN><Fem><Sg> PREP die<+ART><Def> Sonnensystem<+NN><Neut><Sg> prep ∅-Acc Acc Ac"
W15-4923,E12-1068,1,0.894077,"the governing verb and governed noun in the translation output match with the reference translation. Conceptually, this is loosely related to semantically focused metrics (e.g. MEANT, Lo and Wu (2011)), as we go beyond a “ﬂat” n-gram matching but evaluate a meaningful entity, in our case a preposition-noun-verb triple. 3 Methodology Our approach is integrated into an English-German morphology-aware SMT system which ﬁrst translates into a lemmatized representation with a component to generate fully inﬂected forms in a second step, an approach similar to the work by Toutanova et al. (2008) and Fraser et al. (2012). The inﬂection requires the modeling of the grammatical case of noun phrases (among other features), which corresponds to determining the syntactic function1 . Weller et al. (2013) describe modeling case in SMT; we want to treat all subcategorized elements of a verb in one step and extend their setup to cover the prediction of prepositions in both PP and NPs (i.e., the “empty” preposition). 3.1 Translation and Prediction Steps To build the translation model, we use an abstract target-language representation in which nouns, adjectives and articles are lemmatized and prepositions are substitute"
W15-4923,2005.eamt-1.16,0,0.0342591,"and (ii) how to predict prepositions in the translation output using a combination of source and targetside information. In addition, we compare prepositions in the machine translation output with those in the reference translation for a selected subset. Finally, we discuss examples illustrating typical problems of translating prepositions. 2 Related Work Most research on translating prepositions has been reported for rule-based systems. Naskar and Bandyopadhyay (2006) outline a method to handle prepositions in an English-Bengali MT system using WordNet and an example base for idiomatic PPs. Gustavii (2005) uses bilingual features and selectional constraints to correct translations in a Swedish-English system. Agirre et al. (2009) model Basque prepositions and grammatical case using syntactic-semantic features such as subcategorization triples for a rule-based system which leads to an improved translation quality for prepositions. Shilon et al. (2012) extend this approach 177 input ∅ −→ what role ∅ −→ the giant planet has played in −→ the development of −→ the solar system lemmatized SMT output PREP welch<PWAT> Rolle<+NN><Fem><Sg> PREP die<+ART><Def> riesig<ADJ> Planet<+NN><Masc><Sg> gespielt<VV"
W15-4923,P10-1052,0,0.168043,"Missing"
W15-4923,P11-1023,0,0.0612315,"hey use a classiﬁer trained on local contextual features to predict whether to generate or remove determiners for the target-side of translation rules. Another related task is error correction of second language learners, e.g. Rozovskaya and Roth (2013), which also comprises the correction of prepositions. In addition to the standard evaluation metric BLEU, we evaluate the accuracy of prepositions in cases where the governing verb and governed noun in the translation output match with the reference translation. Conceptually, this is loosely related to semantically focused metrics (e.g. MEANT, Lo and Wu (2011)), as we go beyond a “ﬂat” n-gram matching but evaluate a meaningful entity, in our case a preposition-noun-verb triple. 3 Methodology Our approach is integrated into an English-German morphology-aware SMT system which ﬁrst translates into a lemmatized representation with a component to generate fully inﬂected forms in a second step, an approach similar to the work by Toutanova et al. (2008) and Fraser et al. (2012). The inﬂection requires the modeling of the grammatical case of noun phrases (among other features), which corresponds to determining the syntactic function1 . Weller et al. (2013)"
W15-4923,W06-2113,0,0.0370603,"eneration model in an English-German morphology-aware SMT system. We study two aspects: (i) features for a meaningful abstract representation of prepositions and (ii) how to predict prepositions in the translation output using a combination of source and targetside information. In addition, we compare prepositions in the machine translation output with those in the reference translation for a selected subset. Finally, we discuss examples illustrating typical problems of translating prepositions. 2 Related Work Most research on translating prepositions has been reported for rule-based systems. Naskar and Bandyopadhyay (2006) outline a method to handle prepositions in an English-Bengali MT system using WordNet and an example base for idiomatic PPs. Gustavii (2005) uses bilingual features and selectional constraints to correct translations in a Swedish-English system. Agirre et al. (2009) model Basque prepositions and grammatical case using syntactic-semantic features such as subcategorization triples for a rule-based system which leads to an improved translation quality for prepositions. Shilon et al. (2012) extend this approach 177 input ∅ −→ what role ∅ −→ the giant planet has played in −→ the development of −→"
W15-4923,D13-1074,0,0.026895,"appropriate translation rules, whereas we generate prepositions in a post-processing step. A related task to generating prepositions is the generation of determiners, which are problematic when translating from languages without deﬁniteness morphemes, e.g. Czech or Russian. Tsvetkov et al. (2013) create synthetic translation options to augment a standard phrase-table. They use a classiﬁer trained on local contextual features to predict whether to generate or remove determiners for the target-side of translation rules. Another related task is error correction of second language learners, e.g. Rozovskaya and Roth (2013), which also comprises the correction of prepositions. In addition to the standard evaluation metric BLEU, we evaluate the accuracy of prepositions in cases where the governing verb and governed noun in the translation output match with the reference translation. Conceptually, this is loosely related to semantically focused metrics (e.g. MEANT, Lo and Wu (2011)), as we go beyond a “ﬂat” n-gram matching but evaluate a meaningful entity, in our case a preposition-noun-verb triple. 3 Methodology Our approach is integrated into an English-German morphology-aware SMT system which ﬁrst translates in"
W15-4923,schmid-etal-2004-smor,0,0.0792705,"setup and results of our experiments. In addition to the traditional metric BLEU, we assess the quality of the translated prepositions for a subset where relevant elements (verb, noun) match with the reference. Finally, we discuss some examples before concluding the paper. 6.1 Data and Experimental Setup We trained a standard phrase-based Moses system on 4.3M lines of EN–DE data (WMT’14) with a 10.3M sentence language model. For the lemmatized representation of the morphologyaware SMT system, the German part was parsed with BitPar (Schmid, 2004) and analyzed with the morphological tool SMOR (Schmid et al., 2004). The models for predicting inﬂectional features and prepositions were built with the Wapiti toolkit (Lavergne et al., 2010). The inﬂectional models (case, number, gender strong/weak) were trained on lemma and tag information of the German part Evaluation with BLEU Table 4 shows the results of experiments with the baseline system (a), a morphology-aware SMT system with no special treatment for prepositions4 . As a variant of the baseline system (b), we removed all prepositions from the translation output to be re-predicted. This does not lead to much change in BLEU, illustrating that the predi"
W15-4923,C04-1024,0,0.0189533,"reposition. 6 Experiments and Evaluation Here, we present the setup and results of our experiments. In addition to the traditional metric BLEU, we assess the quality of the translated prepositions for a subset where relevant elements (verb, noun) match with the reference. Finally, we discuss some examples before concluding the paper. 6.1 Data and Experimental Setup We trained a standard phrase-based Moses system on 4.3M lines of EN–DE data (WMT’14) with a 10.3M sentence language model. For the lemmatized representation of the morphologyaware SMT system, the German part was parsed with BitPar (Schmid, 2004) and analyzed with the morphological tool SMOR (Schmid et al., 2004). The models for predicting inﬂectional features and prepositions were built with the Wapiti toolkit (Lavergne et al., 2010). The inﬂectional models (case, number, gender strong/weak) were trained on lemma and tag information of the German part Evaluation with BLEU Table 4 shows the results of experiments with the baseline system (a), a morphology-aware SMT system with no special treatment for prepositions4 . As a variant of the baseline system (b), we removed all prepositions from the translation output to be re-predicted. Th"
W15-4923,W12-0514,0,0.164234,"Related Work Most research on translating prepositions has been reported for rule-based systems. Naskar and Bandyopadhyay (2006) outline a method to handle prepositions in an English-Bengali MT system using WordNet and an example base for idiomatic PPs. Gustavii (2005) uses bilingual features and selectional constraints to correct translations in a Swedish-English system. Agirre et al. (2009) model Basque prepositions and grammatical case using syntactic-semantic features such as subcategorization triples for a rule-based system which leads to an improved translation quality for prepositions. Shilon et al. (2012) extend this approach 177 input ∅ −→ what role ∅ −→ the giant planet has played in −→ the development of −→ the solar system lemmatized SMT output PREP welch<PWAT> Rolle<+NN><Fem><Sg> PREP die<+ART><Def> riesig<ADJ> Planet<+NN><Masc><Sg> gespielt<VVPP> hat<VAFIN> PREP die<+ART><Def> Entwicklung<+NN><Fem><Sg> PREP die<+ART><Def> Sonnensystem<+NN><Neut><Sg> prep ∅-Acc Acc Acc ∅-Nom Nom Nom Nom – – bei-Dat Dat Dat ∅-Gen Gen Gen morph. feat. – Acc.Fem.Sg.Wk Acc.Fem.Sg.Wk – Nom.Masc.Sg.St Nom.Masc.Sg.Wk Nom.Masc.Sg.Wk – – – Dat.Fem.Sg.St Dat.Fem.Sg.Wk – Gen.Neut.Sg.St Gen.Neut.Sg.Wk inﬂected gloss"
W15-4923,P08-1059,0,0.219634,"prepositions in cases where the governing verb and governed noun in the translation output match with the reference translation. Conceptually, this is loosely related to semantically focused metrics (e.g. MEANT, Lo and Wu (2011)), as we go beyond a “ﬂat” n-gram matching but evaluate a meaningful entity, in our case a preposition-noun-verb triple. 3 Methodology Our approach is integrated into an English-German morphology-aware SMT system which ﬁrst translates into a lemmatized representation with a component to generate fully inﬂected forms in a second step, an approach similar to the work by Toutanova et al. (2008) and Fraser et al. (2012). The inﬂection requires the modeling of the grammatical case of noun phrases (among other features), which corresponds to determining the syntactic function1 . Weller et al. (2013) describe modeling case in SMT; we want to treat all subcategorized elements of a verb in one step and extend their setup to cover the prediction of prepositions in both PP and NPs (i.e., the “empty” preposition). 3.1 Translation and Prediction Steps To build the translation model, we use an abstract target-language representation in which nouns, adjectives and articles are lemmatized and pr"
W15-4923,W13-2234,0,0.331521,"ler et al. (2014) use noun class information as tree labels in syntactic SMT to model selectional preferences of prepositions. The presented work is similar to that of Agirre et al. (2009), but is applied to a fully statistical MT system. The main difference is that Agirre et al. (2009) use linguistic information to select appropriate translation rules, whereas we generate prepositions in a post-processing step. A related task to generating prepositions is the generation of determiners, which are problematic when translating from languages without deﬁniteness morphemes, e.g. Czech or Russian. Tsvetkov et al. (2013) create synthetic translation options to augment a standard phrase-table. They use a classiﬁer trained on local contextual features to predict whether to generate or remove determiners for the target-side of translation rules. Another related task is error correction of second language learners, e.g. Rozovskaya and Roth (2013), which also comprises the correction of prepositions. In addition to the standard evaluation metric BLEU, we evaluate the accuracy of prepositions in cases where the governing verb and governed noun in the translation output match with the reference translation. Conceptu"
W15-4923,P13-1058,1,0.799055,"NT, Lo and Wu (2011)), as we go beyond a “ﬂat” n-gram matching but evaluate a meaningful entity, in our case a preposition-noun-verb triple. 3 Methodology Our approach is integrated into an English-German morphology-aware SMT system which ﬁrst translates into a lemmatized representation with a component to generate fully inﬂected forms in a second step, an approach similar to the work by Toutanova et al. (2008) and Fraser et al. (2012). The inﬂection requires the modeling of the grammatical case of noun phrases (among other features), which corresponds to determining the syntactic function1 . Weller et al. (2013) describe modeling case in SMT; we want to treat all subcategorized elements of a verb in one step and extend their setup to cover the prediction of prepositions in both PP and NPs (i.e., the “empty” preposition). 3.1 Translation and Prediction Steps To build the translation model, we use an abstract target-language representation in which nouns, adjectives and articles are lemmatized and prepositions are substituted with place-holders. Additionally, “empty” place-holder prepositions are inserted at the beginning of noun phrases. To obtain a symmetric data structure, place-holders for “empty”"
W15-4923,2014.amta-researchers.21,1,0.852767,"∅-Gen Gen Gen morph. feat. – Acc.Fem.Sg.Wk Acc.Fem.Sg.Wk – Nom.Masc.Sg.St Nom.Masc.Sg.Wk Nom.Masc.Sg.Wk – – – Dat.Fem.Sg.St Dat.Fem.Sg.Wk – Gen.Neut.Sg.St Gen.Neut.Sg.Wk inﬂected gloss welche Rolle which role der riesige Planet gespielt hat bei der Entwicklung the giant planet played has for the development des Sonnensystems of-the solar system Figure 1: Prediction of prepositions, morphological features and generation of inﬂected forms for the lemmatized SMT output. German cases: Acc-Accusative, Nom-Nominative, Dat-Dative, Gen-Genitive. with a statistical component for ranking translations. Weller et al. (2014) use noun class information as tree labels in syntactic SMT to model selectional preferences of prepositions. The presented work is similar to that of Agirre et al. (2009), but is applied to a fully statistical MT system. The main difference is that Agirre et al. (2009) use linguistic information to select appropriate translation rules, whereas we generate prepositions in a post-processing step. A related task to generating prepositions is the generation of determiners, which are problematic when translating from languages without deﬁniteness morphemes, e.g. Czech or Russian. Tsvetkov et al. ("
W15-4923,P11-2121,0,\N,Missing
W16-1805,2010.iwslt-papers.2,0,0.0233276,"ers from a major re-translation sense problem, because the paraphrase candidates cannot distinguish between the various senses of the target word or phrase. Consequently, (i) the different senses of the original word or phrase are merged, 1 www.duden.de 38 Proceedings of the 12th Workshop on Multiword Expressions, pages 38–43, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics 3.1 Our approach to incorporate word senses into the standard paraphrase extraction applies a graph-based clustering to the set of paraphrase candidates, based on a method described in (Apidianaki and He, 2010; Apidianaki et al., 2014). It divides the set of candidates into clusters by reducing edges in an originally fully-connected graph to those exceeding a dynamic similarity threshold. The resulting clusters are taken as paraphrase senses, and different parameters from the graphical clustering (such as connectedness in clusters; cluster centroid positions; etc.) are supposed to enhance the paraphrase ranking step. With this setting, we aim to achieve higher precision in the top-ranked candidates, and to cover a wider range of senses as the original re-translation method. 2 Following the basic ap"
W16-1805,apidianaki-etal-2014-semantic,0,0.0778907,"lation sense problem, because the paraphrase candidates cannot distinguish between the various senses of the target word or phrase. Consequently, (i) the different senses of the original word or phrase are merged, 1 www.duden.de 38 Proceedings of the 12th Workshop on Multiword Expressions, pages 38–43, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics 3.1 Our approach to incorporate word senses into the standard paraphrase extraction applies a graph-based clustering to the set of paraphrase candidates, based on a method described in (Apidianaki and He, 2010; Apidianaki et al., 2014). It divides the set of candidates into clusters by reducing edges in an originally fully-connected graph to those exceeding a dynamic similarity threshold. The resulting clusters are taken as paraphrase senses, and different parameters from the graphical clustering (such as connectedness in clusters; cluster centroid positions; etc.) are supposed to enhance the paraphrase ranking step. With this setting, we aim to achieve higher precision in the top-ranked candidates, and to cover a wider range of senses as the original re-translation method. 2 Following the basic approach for synonym extract"
W16-1805,P05-1074,0,0.367805,"to tell” (sagen, u¨ bermitteln, weitergeben) existed in the candidate list, but were ranked low. Introduction Alignments in parallel corpora provide a straightforward basis for the extraction of paraphrases by means of re-translating pivots and then ranking the obtained set of candidates. For example, if the German verb aufsteigen is aligned with the English pivot verbs rise and climb up, and the two English verbs are in turn aligned with the German verbs aufsteigen, ansteigen and hochklettern, then ansteigen and hochklettern represent two paraphrase candidates for the German verb aufsteigen. Bannard and Callison-Burch (2005) were the first to apply this method to gather paraphrases for individual words and multi-word expressions, using translation probabilities as criteria for ranking the obtained paraphrase candidates. This standard re-translation approach however suffers from a major re-translation sense problem, because the paraphrase candidates cannot distinguish between the various senses of the target word or phrase. Consequently, (i) the different senses of the original word or phrase are merged, 1 www.duden.de 38 Proceedings of the 12th Workshop on Multiword Expressions, pages 38–43, c Berlin, Germany, Au"
W16-1805,D08-1021,0,0.033464,"f the English pivot fi being a translation of the particle verb e1 , and the return probability p(e2 |fi ), i.e. the probability that the synonym candidate e2 is a translation of the English pivot fi . The final synonym score for e2 is the sum over all pivots f1..n that re-translate into the candidate: Related Work Bannard and Callison-Burch (2005) introduced the idea of extracting paraphrases with the retranslation method. Their work controls for word senses regarding specific test sentences, but not on the type level. Subsequent approaches improved the basic re-translation method, including Callison-Burch (2008) who restrict paraphrases by syntactic type; and Wittmann et al. (2014) who add distributional similarity between paraphrase candidate and target word as a ranking feature. Approaches that applied extracted paraphrases relying on the re-translation method include the evaluation of SMT (Zhou et al., 2006) and query expansion in Q-A systems (Riezler et al., 2007). Most recently, Cocos and Callison-Burch (2016) proposed two clustering algorithms to address one of the sense problems: They discriminate between target word senses, exploiting hierarchical graph factorization clustering and spectral c"
W16-1805,N16-1172,0,0.0637135,"t does not outperform an extended baseline integrating a simple distributional similarity measure. 1 valid + + + + sense 1 1 2 3 gloss to direct to concentrate to concentrate to orientate to organize to rely to strive to steer to aim to achieve Table 1: Top-ranked paraphrases for ausrichten. when the back translations of all pivot words are collected within one set of paraphrase candidates; and (ii) the ranking step does not guarantee that all senses of a target are covered by the top-ranked candidates, as more frequent senses amass higher translation probabilities and are favoured. Recently, Cocos and Callison-Burch (2016) proposed two approaches to distinguish between paraphrase senses (i.e., aiming to solve problem (i) above). In this paper, we address both facets (i) and (ii) of the re-translation sense problem, while focusing on an emprically challenging class of multi-word expressions, i.e., German particle verbs (PVs). German PVs can appear morphologically joint or separated (such as steigt . . . auf ), and are often highly ambiguous. For example, the 138 PVs we use in this paper have an average number of 5.3 senses according to the Duden1 dictionary. Table 1 illustrates the re-translation sense problem f"
W16-1805,D08-1094,0,0.0435596,"Missing"
W16-1805,P06-2111,0,0.0807719,"Missing"
W16-1805,wittmann-etal-2014-automatic,1,0.880956,"nd the return probability p(e2 |fi ), i.e. the probability that the synonym candidate e2 is a translation of the English pivot fi . The final synonym score for e2 is the sum over all pivots f1..n that re-translate into the candidate: Related Work Bannard and Callison-Burch (2005) introduced the idea of extracting paraphrases with the retranslation method. Their work controls for word senses regarding specific test sentences, but not on the type level. Subsequent approaches improved the basic re-translation method, including Callison-Burch (2008) who restrict paraphrases by syntactic type; and Wittmann et al. (2014) who add distributional similarity between paraphrase candidate and target word as a ranking feature. Approaches that applied extracted paraphrases relying on the re-translation method include the evaluation of SMT (Zhou et al., 2006) and query expansion in Q-A systems (Riezler et al., 2007). Most recently, Cocos and Callison-Burch (2016) proposed two clustering algorithms to address one of the sense problems: They discriminate between target word senses, exploiting hierarchical graph factorization clustering and spectral clustering. The approaches cluster all words in the Paraphrase Database"
W16-1805,W09-0420,0,0.0122989,"ed clusters of synonym candidates. Singleton clusters are ignored. The sub-graphs represent the cluster analysis to be used in the ranking of synonyms for the target particle verb. 4 4.1 Experiments, Results and Discussion Data and Evaluation For the extraction of synonym candidates, we use the German–English version of Europarl (1.5M parallel sentences) with GIZA++ word alignments for the extraction of synonym candidates. In the alignments, the German data is lemmatized and reordered in order to treat split occurrences of particle and verb as a single word (Schmid et al., 2004; Schmid, 2004; Fraser, 2009). Iterative Application of Clustering Algorithm Because the resulting clusterings of the synonym candidates typically contain one very large (and many small) clusters, we extend the original algorithm and iteratively re-apply the clustering: After one pass of the clustering algorithm as described 40 system basic basic + distr. sim. clustering + ranking (1) clustering + ranking (2) clustering + ranking (3) clustering + ranking (4) clustering + ranking (5) 1 2 3 4 5 6 7 ranking S(tr) S(tr) · sim(cand,v) S(tr) · S(sim(cand,v)) · C(#(e)) S(tr) · S(sim(cand,v)) · C(av-sim(cand,gc)) S(tr) · S(sim(ca"
W16-1805,N13-1092,0,0.0808578,"Missing"
W16-1805,W06-1610,0,0.037943,"ted Work Bannard and Callison-Burch (2005) introduced the idea of extracting paraphrases with the retranslation method. Their work controls for word senses regarding specific test sentences, but not on the type level. Subsequent approaches improved the basic re-translation method, including Callison-Burch (2008) who restrict paraphrases by syntactic type; and Wittmann et al. (2014) who add distributional similarity between paraphrase candidate and target word as a ranking feature. Approaches that applied extracted paraphrases relying on the re-translation method include the evaluation of SMT (Zhou et al., 2006) and query expansion in Q-A systems (Riezler et al., 2007). Most recently, Cocos and Callison-Burch (2016) proposed two clustering algorithms to address one of the sense problems: They discriminate between target word senses, exploiting hierarchical graph factorization clustering and spectral clustering. The approaches cluster all words in the Paraphrase Database (Ganitkevitch et al., 2013) and focus on English nouns in their evaluation. A different line of research on synonym extraction has exploited distributional models, by relying on the contextual similarity of two words or phrases, e.g."
W16-1805,J07-2002,0,0.0331268,"Missing"
W16-1805,P07-1059,0,0.0433674,"he idea of extracting paraphrases with the retranslation method. Their work controls for word senses regarding specific test sentences, but not on the type level. Subsequent approaches improved the basic re-translation method, including Callison-Burch (2008) who restrict paraphrases by syntactic type; and Wittmann et al. (2014) who add distributional similarity between paraphrase candidate and target word as a ranking feature. Approaches that applied extracted paraphrases relying on the re-translation method include the evaluation of SMT (Zhou et al., 2006) and query expansion in Q-A systems (Riezler et al., 2007). Most recently, Cocos and Callison-Burch (2016) proposed two clustering algorithms to address one of the sense problems: They discriminate between target word senses, exploiting hierarchical graph factorization clustering and spectral clustering. The approaches cluster all words in the Paraphrase Database (Ganitkevitch et al., 2013) and focus on English nouns in their evaluation. A different line of research on synonym extraction has exploited distributional models, by relying on the contextual similarity of two words or phrases, e.g. Sahlgren (2006), van der Plas and Tiedemann (2006), Pad´o"
W16-1805,schafer-bildhauer-2012-building,0,0.0369705,"Missing"
W16-1805,schmid-etal-2004-smor,0,0.0426765,"ulting graph consists of disconnected clusters of synonym candidates. Singleton clusters are ignored. The sub-graphs represent the cluster analysis to be used in the ranking of synonyms for the target particle verb. 4 4.1 Experiments, Results and Discussion Data and Evaluation For the extraction of synonym candidates, we use the German–English version of Europarl (1.5M parallel sentences) with GIZA++ word alignments for the extraction of synonym candidates. In the alignments, the German data is lemmatized and reordered in order to treat split occurrences of particle and verb as a single word (Schmid et al., 2004; Schmid, 2004; Fraser, 2009). Iterative Application of Clustering Algorithm Because the resulting clusterings of the synonym candidates typically contain one very large (and many small) clusters, we extend the original algorithm and iteratively re-apply the clustering: After one pass of the clustering algorithm as described 40 system basic basic + distr. sim. clustering + ranking (1) clustering + ranking (2) clustering + ranking (3) clustering + ranking (4) clustering + ranking (5) 1 2 3 4 5 6 7 ranking S(tr) S(tr) · sim(cand,v) S(tr) · S(sim(cand,v)) · C(#(e)) S(tr) · S(sim(cand,v)) · C(av-s"
W16-1805,C04-1024,0,0.00921709,"of disconnected clusters of synonym candidates. Singleton clusters are ignored. The sub-graphs represent the cluster analysis to be used in the ranking of synonyms for the target particle verb. 4 4.1 Experiments, Results and Discussion Data and Evaluation For the extraction of synonym candidates, we use the German–English version of Europarl (1.5M parallel sentences) with GIZA++ word alignments for the extraction of synonym candidates. In the alignments, the German data is lemmatized and reordered in order to treat split occurrences of particle and verb as a single word (Schmid et al., 2004; Schmid, 2004; Fraser, 2009). Iterative Application of Clustering Algorithm Because the resulting clusterings of the synonym candidates typically contain one very large (and many small) clusters, we extend the original algorithm and iteratively re-apply the clustering: After one pass of the clustering algorithm as described 40 system basic basic + distr. sim. clustering + ranking (1) clustering + ranking (2) clustering + ranking (3) clustering + ranking (4) clustering + ranking (5) 1 2 3 4 5 6 7 ranking S(tr) S(tr) · sim(cand,v) S(tr) · S(sim(cand,v)) · C(#(e)) S(tr) · S(sim(cand,v)) · C(av-sim(cand,gc)) S"
W16-2205,2009.eamt-1.9,0,0.0329357,"phrase-table entries allows to create prepositions in contexts not observed in the parallel training data. The resulting phrase-table entries are unique for each context and provide the best selection of translation options in terms of complement realization on token-level. This variant significantly outperforms the baseline, and is slightly better than the system with inserted placeholder prepositions. 2 the number of missing/wrong content and function words. For the language pair English–German, the combined number of missing/wrong/added prepositions is one of the most observed error types. Agirre et al. (2009) were among the first to use rich linguistic information to model prepositions and grammatical case in Basque within a rule-based system, leading to an improved translation quality for prepositions. Their work is extended by Shilon et al. (2012) with a statistical component for ranking translations. Weller et al. (2013) use a combination of source-side and target-side features to predict grammatical case on the SMT output, but without taking into account different complement types (NP vs. PP). Weller et al. (2015) predict prepositions as a post-processing step to a translation system in which"
W16-2205,D07-1007,0,0.0242699,"target language to obtain more isomorphic parallel data. Also, we translate into the morphologically rich language, which requires morphological modeling with regard to, e.g., grammatical case and portmanteau prepositions (cf. section 3) to ensure morphologically correct output. Related Work Our work is related to three research areas: using source-side information, previous approaches to model case and prepositions and the synthesis of phrase-table entries. Source-side information has been applied to SMT before, often for the purpose of word sense disambiguation and improving lexical choice (Carpuat and Wu, 2007; Gimpel and Smith, 2008; Jeong et al., 2010; Tamchyna et al., 2014), but without a focus on synthesis or syntactic-semantic aspects such as subcategorization. Prepositions are difficult to translate and responsible for many errors, as has been shown in many evaluations of machine translation. For example, Williams et al. (2015) presented a detailed error analysis of their shared task submissions, listing Synthetic phrases have been implemented by Chahuneau et al. (2013) to translate into morphologically rich languages. They use a discriminative model based on source-side features (dependency"
W16-2205,D13-1174,0,0.0175928,"de information has been applied to SMT before, often for the purpose of word sense disambiguation and improving lexical choice (Carpuat and Wu, 2007; Gimpel and Smith, 2008; Jeong et al., 2010; Tamchyna et al., 2014), but without a focus on synthesis or syntactic-semantic aspects such as subcategorization. Prepositions are difficult to translate and responsible for many errors, as has been shown in many evaluations of machine translation. For example, Williams et al. (2015) presented a detailed error analysis of their shared task submissions, listing Synthetic phrases have been implemented by Chahuneau et al. (2013) to translate into morphologically rich languages. They use a discriminative model based on source-side features (dependency information and word clusters) to predict inflected target words based on which phrase-table entries 44 to aus[APPR-Dat] from unedel[ADJA] base Metall&lt;Neut>&lt;Pl>[NN] metals empty[APPR-Acc] emptyprep Gold&lt;Neut>&lt;Sg>[NN] gold into zu[PTKZU] to gold machen[VVINF] make transform nullprp base metals inflection of the respective phrase, too. Portmanteau prepositions (contracted forms of preposition and determiner) are split during the synthesizing and translation process, and ar"
W16-2205,W08-0302,0,0.0281093,"ain more isomorphic parallel data. Also, we translate into the morphologically rich language, which requires morphological modeling with regard to, e.g., grammatical case and portmanteau prepositions (cf. section 3) to ensure morphologically correct output. Related Work Our work is related to three research areas: using source-side information, previous approaches to model case and prepositions and the synthesis of phrase-table entries. Source-side information has been applied to SMT before, often for the purpose of word sense disambiguation and improving lexical choice (Carpuat and Wu, 2007; Gimpel and Smith, 2008; Jeong et al., 2010; Tamchyna et al., 2014), but without a focus on synthesis or syntactic-semantic aspects such as subcategorization. Prepositions are difficult to translate and responsible for many errors, as has been shown in many evaluations of machine translation. For example, Williams et al. (2015) presented a detailed error analysis of their shared task submissions, listing Synthetic phrases have been implemented by Chahuneau et al. (2013) to translate into morphologically rich languages. They use a discriminative model based on source-side features (dependency information and word clu"
W16-2205,W15-3024,0,0.0423433,"ch areas: using source-side information, previous approaches to model case and prepositions and the synthesis of phrase-table entries. Source-side information has been applied to SMT before, often for the purpose of word sense disambiguation and improving lexical choice (Carpuat and Wu, 2007; Gimpel and Smith, 2008; Jeong et al., 2010; Tamchyna et al., 2014), but without a focus on synthesis or syntactic-semantic aspects such as subcategorization. Prepositions are difficult to translate and responsible for many errors, as has been shown in many evaluations of machine translation. For example, Williams et al. (2015) presented a detailed error analysis of their shared task submissions, listing Synthetic phrases have been implemented by Chahuneau et al. (2013) to translate into morphologically rich languages. They use a discriminative model based on source-side features (dependency information and word clusters) to predict inflected target words based on which phrase-table entries 44 to aus[APPR-Dat] from unedel[ADJA] base Metall&lt;Neut>&lt;Pl>[NN] metals empty[APPR-Acc] emptyprep Gold&lt;Neut>&lt;Sg>[NN] gold into zu[PTKZU] to gold machen[VVINF] make transform nullprp base metals inflection of the respective phrase,"
W16-2205,2010.amta-papers.33,0,0.0258573,"llel data. Also, we translate into the morphologically rich language, which requires morphological modeling with regard to, e.g., grammatical case and portmanteau prepositions (cf. section 3) to ensure morphologically correct output. Related Work Our work is related to three research areas: using source-side information, previous approaches to model case and prepositions and the synthesis of phrase-table entries. Source-side information has been applied to SMT before, often for the purpose of word sense disambiguation and improving lexical choice (Carpuat and Wu, 2007; Gimpel and Smith, 2008; Jeong et al., 2010; Tamchyna et al., 2014), but without a focus on synthesis or syntactic-semantic aspects such as subcategorization. Prepositions are difficult to translate and responsible for many errors, as has been shown in many evaluations of machine translation. For example, Williams et al. (2015) presented a detailed error analysis of their shared task submissions, listing Synthetic phrases have been implemented by Chahuneau et al. (2013) to translate into morphologically rich languages. They use a discriminative model based on source-side features (dependency information and word clusters) to predict in"
W16-2205,P10-1052,0,0.0733414,"Missing"
W16-2205,schmid-etal-2004-smor,0,0.0300869,"Missing"
W16-2205,C04-1024,0,0.019651,"number, gender, case and strong/weak for inflecting the stemmed output, we trained 4 CRF sequence models on the target-side of the parallel data. These features are predicted as a sequence of labels (i.e. case/number/etc of consecutive words in an NP/PP) at sentence level. For the prediction of the placeholder prepositions, we trained a maximum entropy model on the parallel training data. In contrast to the morphological features, each preposition in a phrase is predicted independently. For all models, we used the toolkit Wapiti (Lavergne et al., 2010). The German data was parsed with BitPar (Schmid, 2004) and German inflected forms were generated with the morphological resource SMOR (Schmid et al., 2004). 6.2 7 In this section, we summarize the results and in particular, discuss the use of newly generated phrases. We also attempt to analyze potential side-effects on the phrase table and present additional experiments to better handle these effects. Baselines We consider two baselines: BASELINE -1: a standard phrase-based translation system trained on surface forms without any form of morphological modeling. BASELINE -2: a system with morphological modeling, as described in section 3. Portmante"
W16-2205,W12-0514,0,0.0242226,"t realization on token-level. This variant significantly outperforms the baseline, and is slightly better than the system with inserted placeholder prepositions. 2 the number of missing/wrong content and function words. For the language pair English–German, the combined number of missing/wrong/added prepositions is one of the most observed error types. Agirre et al. (2009) were among the first to use rich linguistic information to model prepositions and grammatical case in Basque within a rule-based system, leading to an improved translation quality for prepositions. Their work is extended by Shilon et al. (2012) with a statistical component for ranking translations. Weller et al. (2013) use a combination of source-side and target-side features to predict grammatical case on the SMT output, but without taking into account different complement types (NP vs. PP). Weller et al. (2015) predict prepositions as a post-processing step to a translation system in which prepositions are reduced to placeholders. They find, however, that the reduced representation leads to a general loss in translation quality. Experiments with annotating abstract information to the placeholders indicated that grammatical case pl"
W16-2205,W15-3021,0,0.014543,"aceholders. They find, however, that the reduced representation leads to a general loss in translation quality. Experiments with annotating abstract information to the placeholders indicated that grammatical case plays an important role during translation. We build on their observations, but in contrast with generating prepositions in a post-processing step, prepositions in our work are accessible to the system during decoding, and the phrase-table entries are optimized with regard to the source-sentence. Finnish is a highly inflective language with a very complex case and preposition system. Tiedemann et al. (2015) experimented with pseudo-tokens added to Finnish data to account for the fact that Finnish morphological markers (case) often correspond to a separate English word (typically a preposition). Due to the complexity of Finnish, only a subset of markers is considered. The pseudo-tokens are applied to a Finnish–English translation system, but a manual evaluation remains inconclusive about the effectiveness of their method. For the preposition-informed representation in our work, we adapt both source and target language to obtain more isomorphic parallel data. Also, we translate into the morphologi"
W16-2205,P08-1059,0,0.0388851,"features (dependency information and word clusters) to predict inflected target words based on which phrase-table entries 44 to aus[APPR-Dat] from unedel[ADJA] base Metall&lt;Neut>&lt;Pl>[NN] metals empty[APPR-Acc] emptyprep Gold&lt;Neut>&lt;Sg>[NN] gold into zu[PTKZU] to gold machen[VVINF] make transform nullprp base metals inflection of the respective phrase, too. Portmanteau prepositions (contracted forms of preposition and determiner) are split during the synthesizing and translation process, and are merged after the inflection step. For more details about modeling complex morphology, see for example Toutanova et al. (2008), Fraser et al. (2012) or Chahuneau et al. (2013). Figure 1: Example for preposition-informed representation with empty placeholders heading NPs. 4 Our first approach introduces a simple abstract representation that inserts pseudo-preposition markers to indicate the beginning of noun phrases. This representation serves two purposes: to adjust the source and target sides for structural mismatches of different complement types, and to provide information about syntactic functions and semantic roles via the annotation of grammatical case. Placeholders for empty prepositions are inserted at the be"
W16-2205,W13-2234,0,0.024745,"rovement in translation quality for several language pairs. In contrast, our approach concentrates on the generation of closed-class function words to obtain the most appropriate complement type given the source sentence. This includes generating word sequences not observed in the training data, i.e. adding/changing prepositions for a (different) PP or removing prepositions to form an NP. A task related to synthesizing prepositions is that of generating determiners, the translation of which is problematic when translating from a language like Russian that does not have definiteness morphemes. Tsvetkov et al. (2013) create synthetic translation options to augment the phrase-table. They use a classifier trained on local contextual features to predict whether to add or remove determiners for the target-side of translation rules. In contrast with determiners, which are local to their context, we model and generate function words with semantic content which are subject to complex interactions with verbs and other subcategorized elements throughout the sentence. 3 Preposition-Informed Representation Inflection Prediction System We work with an inflection prediction system which first translates into a stemmed"
W16-2205,P13-1058,1,0.845029,"eline, and is slightly better than the system with inserted placeholder prepositions. 2 the number of missing/wrong content and function words. For the language pair English–German, the combined number of missing/wrong/added prepositions is one of the most observed error types. Agirre et al. (2009) were among the first to use rich linguistic information to model prepositions and grammatical case in Basque within a rule-based system, leading to an improved translation quality for prepositions. Their work is extended by Shilon et al. (2012) with a statistical component for ranking translations. Weller et al. (2013) use a combination of source-side and target-side features to predict grammatical case on the SMT output, but without taking into account different complement types (NP vs. PP). Weller et al. (2015) predict prepositions as a post-processing step to a translation system in which prepositions are reduced to placeholders. They find, however, that the reduced representation leads to a general loss in translation quality. Experiments with annotating abstract information to the placeholders indicated that grammatical case plays an important role during translation. We build on their observations, bu"
W16-2205,E12-1068,1,\N,Missing
W16-2205,P11-2121,0,\N,Missing
W16-2205,W15-4923,1,\N,Missing
W16-2205,W14-3324,0,\N,Missing
W16-5318,W03-1812,0,0.0572615,"hlight an-PRT. ’Peter illuminated the picture with the flashlight.’ b. Der Chef segnete die Pläne ab. The boss blessed the plans ab-PRT. ’The boss approved the plans.’ c. Stella schlug das Wort im Wörterbuch nach. Stella beat the word in-the dictionary nach-PRT. ’Stella looked up the word in the dictionary.’ 125 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 125–133, Osaka, Japan, December 11-17 2016. The compositionality of PVs has received some attention in Computational Linguistics. For example, the assessment of compositionality grades has been studied for English (Baldwin et al., 2003; McCarthy et al., 2003; Bannard et al., 2003; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014) and German (Hartmann et al., 2008; Kühner and Schulte im Walde, 2010; Bott and Schulte im Walde, 2014), mostly with the use of methods from distributional semantics. A central requirement for such studies is the availability of gold standards of human ratings which can serve as the basis for evaluation. Only few gold standards of this kind are available (cf. section 2), and they tend to require a high amount of human work to create. While humans have relatively clear in"
W16-5318,W03-1809,0,0.0933862,"with the flashlight.’ b. Der Chef segnete die Pläne ab. The boss blessed the plans ab-PRT. ’The boss approved the plans.’ c. Stella schlug das Wort im Wörterbuch nach. Stella beat the word in-the dictionary nach-PRT. ’Stella looked up the word in the dictionary.’ 125 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 125–133, Osaka, Japan, December 11-17 2016. The compositionality of PVs has received some attention in Computational Linguistics. For example, the assessment of compositionality grades has been studied for English (Baldwin et al., 2003; McCarthy et al., 2003; Bannard et al., 2003; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014) and German (Hartmann et al., 2008; Kühner and Schulte im Walde, 2010; Bott and Schulte im Walde, 2014), mostly with the use of methods from distributional semantics. A central requirement for such studies is the availability of gold standards of human ratings which can serve as the basis for evaluation. Only few gold standards of this kind are available (cf. section 2), and they tend to require a high amount of human work to create. While humans have relatively clear intuitions on the grade of compositionality of"
W16-5318,bott-schulte-im-walde-2014-optimizing,1,0.88721,"Missing"
W16-5318,W15-0104,1,0.843045,"Missing"
W16-5318,W06-1207,0,0.0274865,"a corpusbased approach to the semantics of particle verb constructions in English. To this end they collected a gold standard containing 40 randomly selected phrasal verbs which were rated by 26 annotators. This gold standard contains ratings on compositionality for each particle verb construction with respect to both the BV and the particle. Ratings were given regarding only three levels: yes, no and don’t know. For our new gold standard we wanted to avoid a simple binary classification, cf. the discussion in the previous section (2). Somewhat related to our topic is the data set created by Cook and Stevenson (2006) for the evaluation of the prediction of particle senses. This gold standard consists of a list of of 389 English particle verb constructions with up balanced over three different frequency bands. Each of the PVs was annotated by two annotators for four different particle senses. The focus of their research was, however, not the study of compositionality, but the classification of particle meanings, and specified for one particle type. 3 Considerations for the Creation of the Gold Standard For the creation of the gold standard we defined a series of properties which we wanted to find reflected"
W16-5318,W03-1810,0,0.14123,"illuminated the picture with the flashlight.’ b. Der Chef segnete die Pläne ab. The boss blessed the plans ab-PRT. ’The boss approved the plans.’ c. Stella schlug das Wort im Wörterbuch nach. Stella beat the word in-the dictionary nach-PRT. ’Stella looked up the word in the dictionary.’ 125 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 125–133, Osaka, Japan, December 11-17 2016. The compositionality of PVs has received some attention in Computational Linguistics. For example, the assessment of compositionality grades has been studied for English (Baldwin et al., 2003; McCarthy et al., 2003; Bannard et al., 2003; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014) and German (Hartmann et al., 2008; Kühner and Schulte im Walde, 2010; Bott and Schulte im Walde, 2014), mostly with the use of methods from distributional semantics. A central requirement for such studies is the availability of gold standards of human ratings which can serve as the basis for evaluation. Only few gold standards of this kind are available (cf. section 2), and they tend to require a high amount of human work to create. While humans have relatively clear intuitions on the grade o"
W16-5318,I11-1024,0,0.102964,"gnete die Pläne ab. The boss blessed the plans ab-PRT. ’The boss approved the plans.’ c. Stella schlug das Wort im Wörterbuch nach. Stella beat the word in-the dictionary nach-PRT. ’Stella looked up the word in the dictionary.’ 125 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 125–133, Osaka, Japan, December 11-17 2016. The compositionality of PVs has received some attention in Computational Linguistics. For example, the assessment of compositionality grades has been studied for English (Baldwin et al., 2003; McCarthy et al., 2003; Bannard et al., 2003; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014) and German (Hartmann et al., 2008; Kühner and Schulte im Walde, 2010; Bott and Schulte im Walde, 2014), mostly with the use of methods from distributional semantics. A central requirement for such studies is the availability of gold standards of human ratings which can serve as the basis for evaluation. Only few gold standards of this kind are available (cf. section 2), and they tend to require a high amount of human work to create. While humans have relatively clear intuitions on the grade of compositionality of PVs, the ambiguity of PVs often rep"
W16-5318,S13-1039,0,0.282535,"The boss blessed the plans ab-PRT. ’The boss approved the plans.’ c. Stella schlug das Wort im Wörterbuch nach. Stella beat the word in-the dictionary nach-PRT. ’Stella looked up the word in the dictionary.’ 125 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 125–133, Osaka, Japan, December 11-17 2016. The compositionality of PVs has received some attention in Computational Linguistics. For example, the assessment of compositionality grades has been studied for English (Baldwin et al., 2003; McCarthy et al., 2003; Bannard et al., 2003; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014) and German (Hartmann et al., 2008; Kühner and Schulte im Walde, 2010; Bott and Schulte im Walde, 2014), mostly with the use of methods from distributional semantics. A central requirement for such studies is the availability of gold standards of human ratings which can serve as the basis for evaluation. Only few gold standards of this kind are available (cf. section 2), and they tend to require a high amount of human work to create. While humans have relatively clear intuitions on the grade of compositionality of PVs, the ambiguity of PVs often represents a problem both"
W16-5318,E14-1050,0,0.285853,"ans ab-PRT. ’The boss approved the plans.’ c. Stella schlug das Wort im Wörterbuch nach. Stella beat the word in-the dictionary nach-PRT. ’Stella looked up the word in the dictionary.’ 125 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 125–133, Osaka, Japan, December 11-17 2016. The compositionality of PVs has received some attention in Computational Linguistics. For example, the assessment of compositionality grades has been studied for English (Baldwin et al., 2003; McCarthy et al., 2003; Bannard et al., 2003; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Salehi et al., 2014) and German (Hartmann et al., 2008; Kühner and Schulte im Walde, 2010; Bott and Schulte im Walde, 2014), mostly with the use of methods from distributional semantics. A central requirement for such studies is the availability of gold standards of human ratings which can serve as the basis for evaluation. Only few gold standards of this kind are available (cf. section 2), and they tend to require a high amount of human work to create. While humans have relatively clear intuitions on the grade of compositionality of PVs, the ambiguity of PVs often represents a problem both for the elicitation of"
W16-5318,schafer-bildhauer-2012-building,0,0.0657397,"tep. We aimed for a selection of 990 PVs (11 particles, 3 frequency bands and 30 PVs per combination), but for one particle (unter) the corpus only contained 38 PVs. We sampled from three different frequency ranges: Frequency tertiles were used to determine the three frequency bands: L(ow), M(id) and H(igh). Since the frequencies of PVs are not independent from the particles they correspond to, the tertiles were computed for each particle separately. The frequencies were obtained as the harmonic mean of frequencies obtained from four different corpora: SdeWaC (Faaß and Eckart, 2013), DECOW12 (Schäfer and Bildhauer, 2012), HGC (Fitschen, 2004) and the German Wikipedia (dump dewiki-20110410). The calculation of word frequency over different corpora was done to balance out known and suspected deficits in the balancedness of each corpus. 4.3 Cleaning of the Gold Standard Since the original list of PVs was created randomly, the gold standard of 938 PVs still contained a certain amount of noisy entries. To remedy this problem we created a reduced gold standard which eliminated problematic entries. The most noticeable problem was the fact that some of the listed verbs were either ambiguous between homophone versions"
W16-5318,L16-1362,1,0.893256,"make larger, public ally available data sets highly desirable. In addition, the availability of standard resources is a prerequisite for inter-study comparability. In this paper we present such a resource containing 400 German PVs. The gold standard was designed as a target selection which is balanced over different types of particles and various ranges of corpus frequency. A subset of the gold standard has already been used in Bott and Schulte im Walde (2015) for the assessment of PV-compositionality. The data set has been created in a larger project which also produced Gh ost-NN (Schulte im Walde et al., 2016), a gold standard for German noun-noun Compounds with a similar design and a similar rating collection process. The resource is available to the research community under a Creative Commons License.1 In the remainder of this paper, section 2 discusses the availability of similar existing resources. In section 3 we describe the criteria which were important for the design of the new resource. In sections 4 and 5 we describe the creation and the properties of the gold standard. 2 Previously Existing Data The only comparable previously existing data set which contains human ratings on German PVs c"
W17-1708,W15-0104,1,0.752229,"Missing"
W17-1708,P15-1010,0,0.0341222,"ned with sense discrimination (e.g., Schütze (1998), Erk (2009), Erk and Pado (2010)), the approaches have rarely been integrated into DSMs across semantic tasks. Alternatively, sense disambiguation/discrimination approaches have been developed for SemEval tasks on Word Sense Disambiguation/Discrimination and (Crosslingual) Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010; Jurgens and Klapaftis, 2013). As to our knowledge, few systems have attempted to distinguish between word senses and then address various semantic relatedness tasks, such as Li and Jurafsky (2015) and Iacobacci et al. (2015). Computational compositionality assessment has been studied for NCs (Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi and Cook, 2013; Schulte im Walde et al., 2016a) and PVs (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005; Kühner and Schulte im Walde, 2010). Most similar to our current work is Salehi et al. (2015a), who addressed the problem of semantic ambiguity in MWEs by using a multi-sense skip gram model with two to five embeddings per word. They expected multiple embeddings to capture different word senses. They could, however, not find an improvement over the use o"
W17-1708,W16-5318,1,0.828467,"onic relationships of ranks that range between -1 (inverse correlation) and 1 (perfect correlation); a ρ value of 0 indicates a lack of correlation. Significance is determined with the use of the Fisher transformation. Soft clustering does not guarantee that each of the pairs of NCs and a constituent word is placed together in at least one of the clusters. This may potentially lead to problems of coverage. In practice, however, we experience coverage problems only for very restrictive threshold settings. 2 This gold standard is a preliminary, but not identical, version of the one presented in Bott et al. (2016). It was also used in Bott and Schulte im Walde (2014). 69 Figure 3: Results for different numbers of clusters for the NC gold standard (heads vs. modifiers) Figure 4: ρ values for variations over thresholds (NC gold standard) Figure 3 shows the effect of the number of clusters which are used in the clustering stage. The graphic shows that the number of clusters has not a strong influence on performance, but slightly better results can be observed with smaller numbers of clusters. This might be due to the fact that larger numbers of clusters split up the feature space into smaller segments and"
W17-1708,S13-2049,0,0.0223584,"und vector is most probably determined by the predominant sense of the word type (which does not necessarily coincide with the relevant sense). While individual vector space approaches have been concerned with sense discrimination (e.g., Schütze (1998), Erk (2009), Erk and Pado (2010)), the approaches have rarely been integrated into DSMs across semantic tasks. Alternatively, sense disambiguation/discrimination approaches have been developed for SemEval tasks on Word Sense Disambiguation/Discrimination and (Crosslingual) Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010; Jurgens and Klapaftis, 2013). As to our knowledge, few systems have attempted to distinguish between word senses and then address various semantic relatedness tasks, such as Li and Jurafsky (2015) and Iacobacci et al. (2015). Computational compositionality assessment has been studied for NCs (Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi and Cook, 2013; Schulte im Walde et al., 2016a) and PVs (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005; Kühner and Schulte im Walde, 2010). Most similar to our current work is Salehi et al. (2015a), who addressed the problem of semantic ambiguity in MWEs by using"
W17-1708,W15-0903,1,0.843377,"rarely been integrated into DSMs across semantic tasks. This paper presents a softclustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compositionality for German noun-noun compounds and German particle verbs. 1 Introduction Addressing the compositionality of complex words is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) have integrated the prediction of multi-word compositionality into statistical machine translation. We are interested in predicting the degrees of compositionality of two types of German multiword expressions: (i) German noun-noun compounds (NCs) represent nominal multi-word expressions (MWEs), e.g., Feuer|werk ‘fire works’ is composed of the constituents Feuer ‘fire’ and Werk ‘opus’. (ii) German particle verbs (PVs) are complex verbs such as an|strahlen (‘beam/smile at’) which are composed of a separable prefix particle (an) and a base verb (strahlen ‘beam’/’smile’"
W17-1708,D14-1024,0,0.0131917,"009), Erk and Pado (2010)), such discrimination has rarely been integrated into DSMs across semantic tasks. This paper presents a softclustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compositionality for German noun-noun compounds and German particle verbs. 1 Introduction Addressing the compositionality of complex words is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) have integrated the prediction of multi-word compositionality into statistical machine translation. We are interested in predicting the degrees of compositionality of two types of German multiword expressions: (i) German noun-noun compounds (NCs) represent nominal multi-word expressions (MWEs), e.g., Feuer|werk ‘fire works’ is composed of the constituents Feuer ‘fire’ and Werk ‘opus’. (ii) German particle verbs (PVs) are complex verbs such as an|strahlen (‘beam/smile at’) which are composed of a separable prefix particle (an)"
W17-1708,D15-1200,0,0.0209779,"approaches have been concerned with sense discrimination (e.g., Schütze (1998), Erk (2009), Erk and Pado (2010)), the approaches have rarely been integrated into DSMs across semantic tasks. Alternatively, sense disambiguation/discrimination approaches have been developed for SemEval tasks on Word Sense Disambiguation/Discrimination and (Crosslingual) Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010; Jurgens and Klapaftis, 2013). As to our knowledge, few systems have attempted to distinguish between word senses and then address various semantic relatedness tasks, such as Li and Jurafsky (2015) and Iacobacci et al. (2015). Computational compositionality assessment has been studied for NCs (Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi and Cook, 2013; Schulte im Walde et al., 2016a) and PVs (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005; Kühner and Schulte im Walde, 2010). Most similar to our current work is Salehi et al. (2015a), who addressed the problem of semantic ambiguity in MWEs by using a multi-sense skip gram model with two to five embeddings per word. They expected multiple embeddings to capture different word senses. They could, however, not find a"
W17-1708,P10-2017,0,0.202703,"eaf’, ’sheet of paper’, ’newspaper’ and ’hand of cards’. If we had individual sense vectors for each sense of Blatt, a DSM might successfully predict a strong compositionality for the compound Blatt|salat regarding this constituent, when comparing the compound vector with the ’leaf’ sense vector, because the vectors agree on Ambiguity represents an obstacle for distributional semantic models (DSMs), which typically subsume the contexts of all word senses within one vector. While individual vector space approaches have been concerned with sense discrimination (e.g., Schütze (1998), Erk (2009), Erk and Pado (2010)), such discrimination has rarely been integrated into DSMs across semantic tasks. This paper presents a softclustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compositionality for German noun-noun compounds and German particle verbs. 1 Introduction Addressing the compositionality of complex words is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (201"
W17-1708,S07-1009,0,0.0790307,"Missing"
W17-1708,W09-1109,0,0.200872,"r senses: ’leaf’, ’sheet of paper’, ’newspaper’ and ’hand of cards’. If we had individual sense vectors for each sense of Blatt, a DSM might successfully predict a strong compositionality for the compound Blatt|salat regarding this constituent, when comparing the compound vector with the ’leaf’ sense vector, because the vectors agree on Ambiguity represents an obstacle for distributional semantic models (DSMs), which typically subsume the contexts of all word senses within one vector. While individual vector space approaches have been concerned with sense discrimination (e.g., Schütze (1998), Erk (2009), Erk and Pado (2010)), such discrimination has rarely been integrated into DSMs across semantic tasks. This paper presents a softclustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compositionality for German noun-noun compounds and German particle verbs. 1 Introduction Addressing the compositionality of complex words is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Chol"
W17-1708,W15-0909,0,0.0827107,"into DSMs across semantic tasks. This paper presents a softclustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compositionality for German noun-noun compounds and German particle verbs. 1 Introduction Addressing the compositionality of complex words is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) have integrated the prediction of multi-word compositionality into statistical machine translation. We are interested in predicting the degrees of compositionality of two types of German multiword expressions: (i) German noun-noun compounds (NCs) represent nominal multi-word expressions (MWEs), e.g., Feuer|werk ‘fire works’ is composed of the constituents Feuer ‘fire’ and Werk ‘opus’. (ii) German particle verbs (PVs) are complex verbs such as an|strahlen (‘beam/smile at’) which are composed of a separable prefix particle (an) and a base verb (strahlen ‘beam’/’smile’). Both types of German M"
W17-1708,W03-1810,0,0.277144,"ation approaches have been developed for SemEval tasks on Word Sense Disambiguation/Discrimination and (Crosslingual) Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010; Jurgens and Klapaftis, 2013). As to our knowledge, few systems have attempted to distinguish between word senses and then address various semantic relatedness tasks, such as Li and Jurafsky (2015) and Iacobacci et al. (2015). Computational compositionality assessment has been studied for NCs (Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi and Cook, 2013; Schulte im Walde et al., 2016a) and PVs (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005; Kühner and Schulte im Walde, 2010). Most similar to our current work is Salehi et al. (2015a), who addressed the problem of semantic ambiguity in MWEs by using a multi-sense skip gram model with two to five embeddings per word. They expected multiple embeddings to capture different word senses. They could, however, not find an improvement over the use of single-word embeddings. In this paper, we suggest soft clustering as an 2 Experiment Setup Distributional Semantics Models Our DSM is a word space model that uses lemmatized words as dimensions in the hig"
W17-1708,S10-1002,0,0.0225701,"pe vector and the compound vector is most probably determined by the predominant sense of the word type (which does not necessarily coincide with the relevant sense). While individual vector space approaches have been concerned with sense discrimination (e.g., Schütze (1998), Erk (2009), Erk and Pado (2010)), the approaches have rarely been integrated into DSMs across semantic tasks. Alternatively, sense disambiguation/discrimination approaches have been developed for SemEval tasks on Word Sense Disambiguation/Discrimination and (Crosslingual) Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010; Jurgens and Klapaftis, 2013). As to our knowledge, few systems have attempted to distinguish between word senses and then address various semantic relatedness tasks, such as Li and Jurafsky (2015) and Iacobacci et al. (2015). Computational compositionality assessment has been studied for NCs (Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi and Cook, 2013; Schulte im Walde et al., 2016a) and PVs (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005; Kühner and Schulte im Walde, 2010). Most similar to our current work is Salehi et al. (2015a), who addressed the problem of seman"
W17-1708,I11-1024,0,0.178676,"y.1 Automatic approaches to predict compositionality degrees typically exploit distributional semantic models (DSMs), i.e. vector representations relying on the distributional hypothesis (Harris, 1954; Firth, 1957), that words with similar distributions have related meanings. Regarding the compositionality prediction, DSMs represent the meanings of the MWEs and their constituents by distributional vectors, and the similarity of a compound–constituent vector pair is taken as the predicted degree of compoundconstituent compositionality. Existing approaches addressed the compositionality of NCs (Reddy et al., 2011; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014) and complex verbs (Baldwin, 2005; Bannard, 2005; Bott and Schulte im Walde, 2015), mainly dfor English and for German. A major obstacle for DSMs is their conflation of contexts across individual word senses. DSMs typically subsume evidence of cooccurring items within one vector for the target word type, rather than discriminating contextual evidence for the specific target word senses. Taking the German noun-noun compound Blatt|salat ’leaf salad’ as an example, its modifier constituent Blatt has at least four senses:"
W17-1708,S13-1038,1,0.936351,"egrees typically exploit distributional semantic models (DSMs), i.e. vector representations relying on the distributional hypothesis (Harris, 1954; Firth, 1957), that words with similar distributions have related meanings. Regarding the compositionality prediction, DSMs represent the meanings of the MWEs and their constituents by distributional vectors, and the similarity of a compound–constituent vector pair is taken as the predicted degree of compoundconstituent compositionality. Existing approaches addressed the compositionality of NCs (Reddy et al., 2011; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014) and complex verbs (Baldwin, 2005; Bannard, 2005; Bott and Schulte im Walde, 2015), mainly dfor English and for German. A major obstacle for DSMs is their conflation of contexts across individual word senses. DSMs typically subsume evidence of cooccurring items within one vector for the target word type, rather than discriminating contextual evidence for the specific target word senses. Taking the German noun-noun compound Blatt|salat ’leaf salad’ as an example, its modifier constituent Blatt has at least four senses: ’leaf’, ’sheet of paper’, ’newspaper’ and ’hand of car"
W17-1708,P99-1014,0,0.0343096,"In the first two methods (highest/lowest) we select the cluster with the highest/lowest distributional similarity between µ and κ and use its similarity value. In the last method (average) the average similarity is computed among those clusters which contain both the MWE µ and the target component κ, while clusters which do not contain the pair &lt;µ, κ> are ignored. vs. Blatt). We also use them (b) as an input matrix for soft clustering and (c) we build word vector models for each cluster. LSC for Soft Clustering We use Latent Semantic Classes (LSC) as a soft clustering algorithm (Rooth, 1998; Rooth et al., 1999). LSC is a two-dimensional soft-clustering algorithm which learns three probability distributions: (a) across the clusters, (b) for the output probabilities of each element within a cluster and (c) for each feature type with regard to a cluster. The access to all three probability distributions is crucial for our approach, since it allows to determine which features are salient for individual clusters. Thresholds The fact that LSC outputs probabilities for both targets and features allows to set two different thresholds on these probabilities. The threshold on the target output probability (tt"
W17-1708,S16-2020,1,0.851471,"sense disambiguation/discrimination approaches have been developed for SemEval tasks on Word Sense Disambiguation/Discrimination and (Crosslingual) Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010; Jurgens and Klapaftis, 2013). As to our knowledge, few systems have attempted to distinguish between word senses and then address various semantic relatedness tasks, such as Li and Jurafsky (2015) and Iacobacci et al. (2015). Computational compositionality assessment has been studied for NCs (Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi and Cook, 2013; Schulte im Walde et al., 2016a) and PVs (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005; Kühner and Schulte im Walde, 2010). Most similar to our current work is Salehi et al. (2015a), who addressed the problem of semantic ambiguity in MWEs by using a multi-sense skip gram model with two to five embeddings per word. They expected multiple embeddings to capture different word senses. They could, however, not find an improvement over the use of single-word embeddings. In this paper, we suggest soft clustering as an 2 Experiment Setup Distributional Semantics Models Our DSM is a word space model that uses lemmatiz"
W17-1708,L16-1362,1,0.884847,"sense disambiguation/discrimination approaches have been developed for SemEval tasks on Word Sense Disambiguation/Discrimination and (Crosslingual) Lexical Substitution (McCarthy and Navigli, 2007; Mihalcea et al., 2010; Jurgens and Klapaftis, 2013). As to our knowledge, few systems have attempted to distinguish between word senses and then address various semantic relatedness tasks, such as Li and Jurafsky (2015) and Iacobacci et al. (2015). Computational compositionality assessment has been studied for NCs (Reddy et al., 2011; Schulte im Walde et al., 2013; Salehi and Cook, 2013; Schulte im Walde et al., 2016a) and PVs (McCarthy et al., 2003; Baldwin et al., 2003; Bannard, 2005; Kühner and Schulte im Walde, 2010). Most similar to our current work is Salehi et al. (2015a), who addressed the problem of semantic ambiguity in MWEs by using a multi-sense skip gram model with two to five embeddings per word. They expected multiple embeddings to capture different word senses. They could, however, not find an improvement over the use of single-word embeddings. In this paper, we suggest soft clustering as an 2 Experiment Setup Distributional Semantics Models Our DSM is a word space model that uses lemmatiz"
W17-1708,S13-1039,0,0.643156,"ches to predict compositionality degrees typically exploit distributional semantic models (DSMs), i.e. vector representations relying on the distributional hypothesis (Harris, 1954; Firth, 1957), that words with similar distributions have related meanings. Regarding the compositionality prediction, DSMs represent the meanings of the MWEs and their constituents by distributional vectors, and the similarity of a compound–constituent vector pair is taken as the predicted degree of compoundconstituent compositionality. Existing approaches addressed the compositionality of NCs (Reddy et al., 2011; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014) and complex verbs (Baldwin, 2005; Bannard, 2005; Bott and Schulte im Walde, 2015), mainly dfor English and for German. A major obstacle for DSMs is their conflation of contexts across individual word senses. DSMs typically subsume evidence of cooccurring items within one vector for the target word type, rather than discriminating contextual evidence for the specific target word senses. Taking the German noun-noun compound Blatt|salat ’leaf salad’ as an example, its modifier constituent Blatt has at least four senses: ’leaf’, ’sheet of paper"
W17-1708,J98-1004,0,0.716561,"has at least four senses: ’leaf’, ’sheet of paper’, ’newspaper’ and ’hand of cards’. If we had individual sense vectors for each sense of Blatt, a DSM might successfully predict a strong compositionality for the compound Blatt|salat regarding this constituent, when comparing the compound vector with the ’leaf’ sense vector, because the vectors agree on Ambiguity represents an obstacle for distributional semantic models (DSMs), which typically subsume the contexts of all word senses within one vector. While individual vector space approaches have been concerned with sense discrimination (e.g., Schütze (1998), Erk (2009), Erk and Pado (2010)), such discrimination has rarely been integrated into DSMs across semantic tasks. This paper presents a softclustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compositionality for German noun-noun compounds and German particle verbs. 1 Introduction Addressing the compositionality of complex words is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies"
W17-1708,E14-1050,0,0.192976,"loit distributional semantic models (DSMs), i.e. vector representations relying on the distributional hypothesis (Harris, 1954; Firth, 1957), that words with similar distributions have related meanings. Regarding the compositionality prediction, DSMs represent the meanings of the MWEs and their constituents by distributional vectors, and the similarity of a compound–constituent vector pair is taken as the predicted degree of compoundconstituent compositionality. Existing approaches addressed the compositionality of NCs (Reddy et al., 2011; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014) and complex verbs (Baldwin, 2005; Bannard, 2005; Bott and Schulte im Walde, 2015), mainly dfor English and for German. A major obstacle for DSMs is their conflation of contexts across individual word senses. DSMs typically subsume evidence of cooccurring items within one vector for the target word type, rather than discriminating contextual evidence for the specific target word senses. Taking the German noun-noun compound Blatt|salat ’leaf salad’ as an example, its modifier constituent Blatt has at least four senses: ’leaf’, ’sheet of paper’, ’newspaper’ and ’hand of cards’. If we had individ"
W17-1708,W14-5709,1,0.863881,"uch discrimination has rarely been integrated into DSMs across semantic tasks. This paper presents a softclustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compositionality for German noun-noun compounds and German particle verbs. 1 Introduction Addressing the compositionality of complex words is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) have integrated the prediction of multi-word compositionality into statistical machine translation. We are interested in predicting the degrees of compositionality of two types of German multiword expressions: (i) German noun-noun compounds (NCs) represent nominal multi-word expressions (MWEs), e.g., Feuer|werk ‘fire works’ is composed of the constituents Feuer ‘fire’ and Werk ‘opus’. (ii) German particle verbs (PVs) are complex verbs such as an|strahlen (‘beam/smile at’) which are composed of a separable prefix particle (an) and a base verb (stra"
W17-1708,N15-1099,0,0.085599,"into DSMs across semantic tasks. This paper presents a softclustering approach to sense discrimination that filters sense-irrelevant features when predicting the degrees of compositionality for German noun-noun compounds and German particle verbs. 1 Introduction Addressing the compositionality of complex words is a crucial ingredient for lexicography and NLP applications, to know whether the expression should be treated as a whole, or through its constituents, and what the expression means. For example, studies such as Cholakov and Kordoni (2014), Weller et al. (2014), Cap et al. (2015), and Salehi et al. (2015b) have integrated the prediction of multi-word compositionality into statistical machine translation. We are interested in predicting the degrees of compositionality of two types of German multiword expressions: (i) German noun-noun compounds (NCs) represent nominal multi-word expressions (MWEs), e.g., Feuer|werk ‘fire works’ is composed of the constituents Feuer ‘fire’ and Werk ‘opus’. (ii) German particle verbs (PVs) are complex verbs such as an|strahlen (‘beam/smile at’) which are composed of a separable prefix particle (an) and a base verb (strahlen ‘beam’/’smile’). Both types of German M"
W17-1708,S14-1022,1,\N,Missing
W17-1708,W03-1812,0,\N,Missing
W17-1708,evert-2004-statistical,0,\N,Missing
W17-1728,P14-1023,0,0.0654523,"r and appropriate image subsets are used, or (b) the textual modality by itself is rich and large (potentially noisy) images are added. 1 Introduction Distributional semantic models (DSMs) rely on the distributional hypothesis (Harris, 1954), that words with similar distributions have related meanings. They represent a well-established tool for modelling semantic relatedness between words and phrases (Bullinaria and Levy, 2007; Turney and Pantel, 2010). In the last decade, standard DSMs using bag-of-words or syntactic cooccurrence counts have been enhanced by integration into neural networks (Baroni et al., 2014; Levy et al., 2015; Nguyen et al., 2016), or by integrating perceptual information (Silberer and Lapata, 2014; Bruni et al., 2014; Kiela et al., 2014; Lazaridou et al., 2015). While standard DSMs have been applied to a variety of semantic relatedness tasks such as word sense discrimination, selectional preferences, relation distinction (among others), multi-modal models have predominantly been evaluated on their general ability to model semantic similarity as captured by SimLex (Hill et al., 2015), WordSim (Finkelstein et al., 2002), etc. 2 Data Target Multi-Word Expressions (MWEs) German nou"
W17-1728,bott-schulte-im-walde-2014-optimizing,1,0.884239,"Missing"
W17-1728,W15-0104,1,0.878709,"Missing"
W17-1728,W16-5318,1,0.774591,"le (such as an) and a base verb (such as strahlen ’beam/smile’). Both types of German MWEs are highly frequent and highly productive in the lexicon. In addition, the particles are notoriously ambiguous, e.g., an has a partitive meaning in anbeißen ’take a bite’, a cumulative meaning in anhäufen ’pile up’, and a topological meaning in anbinden ’tie to’ (Springorum, 2011). We rely on two existing gold standards annotated with compositionality ratings: GS-NN, a set of 868 German noun-noun compounds (Schulte im Walde et al., 2016b), and GS-PV, a set of 400 particle verbs across 11 particle types (Bott et al., 2016). 3 Experiments Predicting Compositionality For the prediction of compositionality, we represented the meanings of the multi-word expressions and their constituent words by textual, visual and textual+visual (i.e., multi-modal) vectors. The similarity of a compound–constituent vector pair as measured by the cosine was taken as the predicted degree of compound–constituent compositionality, and the overall ranking of pair similarities was compared to the gold standard compositionality ratings using Spearman’s Rank-Order Correlation Coefficient ρ (Siegel and Castellan, 1988). Multi-Modal Vector S"
W17-1728,N15-1016,0,0.0641109,"tic models (DSMs) rely on the distributional hypothesis (Harris, 1954), that words with similar distributions have related meanings. They represent a well-established tool for modelling semantic relatedness between words and phrases (Bullinaria and Levy, 2007; Turney and Pantel, 2010). In the last decade, standard DSMs using bag-of-words or syntactic cooccurrence counts have been enhanced by integration into neural networks (Baroni et al., 2014; Levy et al., 2015; Nguyen et al., 2016), or by integrating perceptual information (Silberer and Lapata, 2014; Bruni et al., 2014; Kiela et al., 2014; Lazaridou et al., 2015). While standard DSMs have been applied to a variety of semantic relatedness tasks such as word sense discrimination, selectional preferences, relation distinction (among others), multi-modal models have predominantly been evaluated on their general ability to model semantic similarity as captured by SimLex (Hill et al., 2015), WordSim (Finkelstein et al., 2002), etc. 2 Data Target Multi-Word Expressions (MWEs) German noun-noun compounds represent two-part multi-word expressions where both con200 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 200–206, c Valencia, S"
W17-1728,Q15-1016,0,0.0180269,"ge subsets are used, or (b) the textual modality by itself is rich and large (potentially noisy) images are added. 1 Introduction Distributional semantic models (DSMs) rely on the distributional hypothesis (Harris, 1954), that words with similar distributions have related meanings. They represent a well-established tool for modelling semantic relatedness between words and phrases (Bullinaria and Levy, 2007; Turney and Pantel, 2010). In the last decade, standard DSMs using bag-of-words or syntactic cooccurrence counts have been enhanced by integration into neural networks (Baroni et al., 2014; Levy et al., 2015; Nguyen et al., 2016), or by integrating perceptual information (Silberer and Lapata, 2014; Bruni et al., 2014; Kiela et al., 2014; Lazaridou et al., 2015). While standard DSMs have been applied to a variety of semantic relatedness tasks such as word sense discrimination, selectional preferences, relation distinction (among others), multi-modal models have predominantly been evaluated on their general ability to model semantic similarity as captured by SimLex (Hill et al., 2015), WordSim (Finkelstein et al., 2002), etc. 2 Data Target Multi-Word Expressions (MWEs) German noun-noun compounds re"
W17-1728,P16-2074,1,0.849621,", or (b) the textual modality by itself is rich and large (potentially noisy) images are added. 1 Introduction Distributional semantic models (DSMs) rely on the distributional hypothesis (Harris, 1954), that words with similar distributions have related meanings. They represent a well-established tool for modelling semantic relatedness between words and phrases (Bullinaria and Levy, 2007; Turney and Pantel, 2010). In the last decade, standard DSMs using bag-of-words or syntactic cooccurrence counts have been enhanced by integration into neural networks (Baroni et al., 2014; Levy et al., 2015; Nguyen et al., 2016), or by integrating perceptual information (Silberer and Lapata, 2014; Bruni et al., 2014; Kiela et al., 2014; Lazaridou et al., 2015). While standard DSMs have been applied to a variety of semantic relatedness tasks such as word sense discrimination, selectional preferences, relation distinction (among others), multi-modal models have predominantly been evaluated on their general ability to model semantic similarity as captured by SimLex (Hill et al., 2015), WordSim (Finkelstein et al., 2002), etc. 2 Data Target Multi-Word Expressions (MWEs) German noun-noun compounds represent two-part multi"
W17-1728,J15-4004,0,0.0456821,"-words or syntactic cooccurrence counts have been enhanced by integration into neural networks (Baroni et al., 2014; Levy et al., 2015; Nguyen et al., 2016), or by integrating perceptual information (Silberer and Lapata, 2014; Bruni et al., 2014; Kiela et al., 2014; Lazaridou et al., 2015). While standard DSMs have been applied to a variety of semantic relatedness tasks such as word sense discrimination, selectional preferences, relation distinction (among others), multi-modal models have predominantly been evaluated on their general ability to model semantic similarity as captured by SimLex (Hill et al., 2015), WordSim (Finkelstein et al., 2002), etc. 2 Data Target Multi-Word Expressions (MWEs) German noun-noun compounds represent two-part multi-word expressions where both con200 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 200–206, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics Cluster (a) Complete set of images. (b) Images in largest cluster. Figure 1: Clustering filter for abzupfen ’to pick’. verted all images into high-dimensional numerical representations by using the caffe toolkit (Jia et al., 2014) and pre-trained models. In the defa"
W17-1728,I11-1024,0,0.0251547,"Sprachverarbeitung Universität Stuttgart, Germany {maximilian.koeper,schulte}@ims.uni-stuttgart.de Abstract In this paper, we compare a neural network DSM relying on textual co-occurrences with a multi-modal model extension integrating visual information. We focus on the prediction of compositionality for two types of German multi-word expressions: noun-noun compounds and particle verbs. Differently to most previous multimodal approaches, we thus address a semantically specific task that was traditionally addressed by standard DSMs, mainly for English and German (Baldwin, 2005; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014; Bott and Schulte im Walde, 2014; Bott and Schulte im Walde, 2015; Schulte im Walde et al., 2016a). Furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties (e.g., ambiguity, frequency, compositionality); and filters to optimise the visual space, such as dispersion and imageability filters (Kiela et al., 2014), and a novel clustering filter. Our experiments demonstrate that the contributions of the textual and the visual models differ for predict"
W17-1728,S13-1039,0,0.0803867,"niversität Stuttgart, Germany {maximilian.koeper,schulte}@ims.uni-stuttgart.de Abstract In this paper, we compare a neural network DSM relying on textual co-occurrences with a multi-modal model extension integrating visual information. We focus on the prediction of compositionality for two types of German multi-word expressions: noun-noun compounds and particle verbs. Differently to most previous multimodal approaches, we thus address a semantically specific task that was traditionally addressed by standard DSMs, mainly for English and German (Baldwin, 2005; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014; Bott and Schulte im Walde, 2014; Bott and Schulte im Walde, 2015; Schulte im Walde et al., 2016a). Furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties (e.g., ambiguity, frequency, compositionality); and filters to optimise the visual space, such as dispersion and imageability filters (Kiela et al., 2014), and a novel clustering filter. Our experiments demonstrate that the contributions of the textual and the visual models differ for predictions across the nominal"
W17-1728,P14-2135,0,0.502879,"semantically specific task that was traditionally addressed by standard DSMs, mainly for English and German (Baldwin, 2005; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014; Bott and Schulte im Walde, 2014; Bott and Schulte im Walde, 2015; Schulte im Walde et al., 2016a). Furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties (e.g., ambiguity, frequency, compositionality); and filters to optimise the visual space, such as dispersion and imageability filters (Kiela et al., 2014), and a novel clustering filter. Our experiments demonstrate that the contributions of the textual and the visual models differ for predictions across the nominal vs. verbal compositions. The visual modality adds complementary features in cases where (a) the textual modality performs poorly, and images of the most imaginable targets are added, or (b) the textual modality performs well, and all available –potentially noisy– images are added. In addition, we demonstrate that perceptual features of verbs, such as abstractness and imageability, have a different influence on multi-modality than for"
W17-1728,E14-1050,0,0.067315,"te}@ims.uni-stuttgart.de Abstract In this paper, we compare a neural network DSM relying on textual co-occurrences with a multi-modal model extension integrating visual information. We focus on the prediction of compositionality for two types of German multi-word expressions: noun-noun compounds and particle verbs. Differently to most previous multimodal approaches, we thus address a semantically specific task that was traditionally addressed by standard DSMs, mainly for English and German (Baldwin, 2005; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014; Bott and Schulte im Walde, 2014; Bott and Schulte im Walde, 2015; Schulte im Walde et al., 2016a). Furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties (e.g., ambiguity, frequency, compositionality); and filters to optimise the visual space, such as dispersion and imageability filters (Kiela et al., 2014), and a novel clustering filter. Our experiments demonstrate that the contributions of the textual and the visual models differ for predictions across the nominal vs. verbal compositions. The visual modality adds c"
W17-1728,D16-1043,0,0.14498,"Missing"
W17-1728,schafer-bildhauer-2012-building,0,0.0220675,"pair similarities was compared to the gold standard compositionality ratings using Spearman’s Rank-Order Correlation Coefficient ρ (Siegel and Castellan, 1988). Multi-Modal Vector Space Models For the textual representation we used two sets of embeddings. Based on word2vec (Mikolov et al., 2013), we obtained both representations using the skipgram architecture with negative sampling. The sets differ with respect to window size (5 vs. 10) and dimensionality (400 vs. 500). As corpus resource we relied on the lemmatized version of the DECOW14AX, a German web corpus containing 12 billion tokens (Schäfer and Bildhauer, 2012). The visual features rely on images downloaded from the bing search engine, following Kiela et al. (2016). We queried 25 images per word, and con1 Experiments with other fusion techniques showed that mid-fusion performs best. 201 0.70 0.30 Spearmans ρ 0.67 0.65 0.27 0.65 0.63 0.64 0.60 0.64 Text 0.22 Text+Vision DispFilter ImgFilter ClusterImgFilter 0.25 0.21 0.20 0.55 0.50 0.25 0.22 0.15 GS-NN (heads) GS-PV 0.10 Figure 2: Overall prediction of compositionality for GS-NN (heads) and GS-PV. Lexical, Empirical and Visual Filters The experiments compare the predictions of compositionality across"
W17-1728,S16-2020,1,0.893271,"l co-occurrences with a multi-modal model extension integrating visual information. We focus on the prediction of compositionality for two types of German multi-word expressions: noun-noun compounds and particle verbs. Differently to most previous multimodal approaches, we thus address a semantically specific task that was traditionally addressed by standard DSMs, mainly for English and German (Baldwin, 2005; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014; Bott and Schulte im Walde, 2014; Bott and Schulte im Walde, 2015; Schulte im Walde et al., 2016a). Furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties (e.g., ambiguity, frequency, compositionality); and filters to optimise the visual space, such as dispersion and imageability filters (Kiela et al., 2014), and a novel clustering filter. Our experiments demonstrate that the contributions of the textual and the visual models differ for predictions across the nominal vs. verbal compositions. The visual modality adds complementary features in cases where (a) the textual modality performs poorly, and images of the"
W17-1728,L16-1362,1,0.895532,"l co-occurrences with a multi-modal model extension integrating visual information. We focus on the prediction of compositionality for two types of German multi-word expressions: noun-noun compounds and particle verbs. Differently to most previous multimodal approaches, we thus address a semantically specific task that was traditionally addressed by standard DSMs, mainly for English and German (Baldwin, 2005; Bannard, 2005; Reddy et al., 2011; Salehi and Cook, 2013; Schulte im Walde et al., 2013; Salehi et al., 2014; Bott and Schulte im Walde, 2014; Bott and Schulte im Walde, 2015; Schulte im Walde et al., 2016a). Furthermore, we zoom into factors that might influence the quality of predictions, such as lexical and empirical target properties (e.g., ambiguity, frequency, compositionality); and filters to optimise the visual space, such as dispersion and imageability filters (Kiela et al., 2014), and a novel clustering filter. Our experiments demonstrate that the contributions of the textual and the visual models differ for predictions across the nominal vs. verbal compositions. The visual modality adds complementary features in cases where (a) the textual modality performs poorly, and images of the"
W17-1728,P14-1068,0,0.0345002,"tially noisy) images are added. 1 Introduction Distributional semantic models (DSMs) rely on the distributional hypothesis (Harris, 1954), that words with similar distributions have related meanings. They represent a well-established tool for modelling semantic relatedness between words and phrases (Bullinaria and Levy, 2007; Turney and Pantel, 2010). In the last decade, standard DSMs using bag-of-words or syntactic cooccurrence counts have been enhanced by integration into neural networks (Baroni et al., 2014; Levy et al., 2015; Nguyen et al., 2016), or by integrating perceptual information (Silberer and Lapata, 2014; Bruni et al., 2014; Kiela et al., 2014; Lazaridou et al., 2015). While standard DSMs have been applied to a variety of semantic relatedness tasks such as word sense discrimination, selectional preferences, relation distinction (among others), multi-modal models have predominantly been evaluated on their general ability to model semantic similarity as captured by SimLex (Hill et al., 2015), WordSim (Finkelstein et al., 2002), etc. 2 Data Target Multi-Word Expressions (MWEs) German noun-noun compounds represent two-part multi-word expressions where both con200 Proceedings of the 13th Workshop"
W17-1728,L16-1413,1,\N,Missing
W17-1903,D15-1002,0,0.0644295,"Missing"
W17-1903,W16-1102,0,0.0615179,"Missing"
W17-1903,W13-0908,0,0.0576942,"Missing"
W17-1903,W15-1402,0,0.250441,"phor detection. Finally, with this paper we publish automatically created abstractness norms for 3 million English words and multi-words as well as automatically created sense-specific abstractness ratings. 1 The underlying motivation of using abstractness in metaphor detection goes back to Lakoff and Johnson (1980), who argue that metaphor is a method for transferring knowledge from a concrete domain to an abstract domain. Abstractness was already applied successfully for the detection of metaphors across a variety of languages (Turney et al., 2011; Dunn, 2013; Tsvetkov et al., 2014; Beigman Klebanov et al., 2015; Köper and Schulte im Walde, 2016b). The abstractness information itself is typically taken from a dictionary, created either by manual annotation or by extending manually collected ratings with the help of supervised learning techniques that rely on word representations. While potentially less reliable, automatically created norm-based abstractness ratings can easily cover huge dictionaries. Although some methods have been used to learn abstractness, literature lacks a comparison of these learning techniques. Introduction The standard approach to studying abstractness is to place words on a"
W17-1903,P16-2017,0,0.465209,"n verb metaphors. The collection contains 23 113 verb tokens in running text, annotated as being used literally or metaphorically. In addition we present results for the TroFi metaphor dataset (Birke and Sarkar, 2006) containing 50 verbs and 3 737 labeled sentences. We pre-processed both recourses using Stanford CoreNLP (Manning et al., 2014) for lemmatization, part-of-speech tagging and dependency parsing. We present results by applying ten-fold crossvalidation over the entire data. For the VUA we additionally present results for the test data using the same training/test split as in Beigman Klebanov et al. (2016). 1. Rating of the verbs subject 2. Rating of the verbs object 3. Average rating of all nouns (excluding proper names) 4. Average rating of all proper names 5. Average rating of all verbs, excluding the target verb 6. Average rating of all adjectives 7. Average rating of all adverbs For classification we used a balanced Logistic Regression classifier following the findings from Beigman Klebanov et al. (2015). While this default setup tries to generalize over unseen verbs by only looking at a verb’s context we further present results for a second setup that uses a 6th feature: namely the lemma"
W17-1903,L16-1413,1,0.903744,"Missing"
W17-1903,E06-1042,0,0.712789,"gle sense) global representation. We always pick the sense representation that obtains the largest similarity, measured by cosine. The potential advantage of this method is that in a metaphor detection system we are now able to look up word-sense-specific abstractness ratings instead of globally obtained ratings. For this experiment we use the VU Amsterdam Metaphor Corpus (Steen, 2010) (VUA), focusing on verb metaphors. The collection contains 23 113 verb tokens in running text, annotated as being used literally or metaphorically. In addition we present results for the TroFi metaphor dataset (Birke and Sarkar, 2006) containing 50 verbs and 3 737 labeled sentences. We pre-processed both recourses using Stanford CoreNLP (Manning et al., 2014) for lemmatization, part-of-speech tagging and dependency parsing. We present results by applying ten-fold crossvalidation over the entire data. For the VUA we additionally present results for the test data using the same training/test split as in Beigman Klebanov et al. (2016). 1. Rating of the verbs subject 2. Rating of the verbs object 3. Average rating of all nouns (excluding proper names) 4. Average rating of all proper names 5. Average rating of all verbs, exclud"
W17-1903,D14-1162,0,0.0790128,"Missing"
W17-1903,N16-1039,1,0.875809,"Missing"
W17-1903,S16-2003,0,0.152099,"Spearman’s ρ on commonly covered subset. Red = high correlation. be useful, especially for further research which requires large vocabulary coverage.3 2.2 Abstractness for Phrases A potential advantage of our method is that abstractness can be learned for multi-word units as long as the representation of these units live in the same distributional vector space as the words required for the supervised training. In this section we explore if ratings propagated to verb-noun phrases provide useful information for metaphor detection. As dataset we relied on the collection from Saif M. Mohammad and Turney (2016), who annotated different senses of WordNet verbs for metaphoricity (Fellbaum, 1998). We used the same subset of verb–direct object and verb–subject relations as used in Shutova et al. (2016). As preprocessing step we concatenated verb-noun phrases by relying on dependency information based on a web corpus, the ENCOW14 corpus (Schäfer and Bildhauer, 2012; Schäfer, 2015). We removed words and phrases that appeared less than 50 times in our corpus, thus our selection covers 535 pairs, 238 of which were metaphorical and 297 literal. Given a verb-noun phrase, such as stamp person, we obtained vect"
W17-1903,D15-1200,0,0.0239611,"ing the same five feature dimensions as used by Turney et al. (2011) plus dimensions respectively for subject and object, thus we rely on the seven feature, namely: Sense-specific Abstractness Ratings In this section we investigate if automatically learned multi-sense abstractness ratings, that is having different ratings per word sense, are potentially useful for the task of metaphor detection. Recent advances in word representation learning led to the development of algorithms for nonparametric and unsupervised multi-sense representation learning (Neelakantan et al., 2014; Liu et al., 2015; Li and Jurafsky, 2015; Bartunov et al., 2016). Using these techniques one can learn a different vector representation per word sense. Such representations can be combined with our abstractness learning method from section 2.1.1. While in theory any multi-sense learning technique can be applied, we decided for the one introduced by Pelevina et al. (2016), as it performs sense learning after single senses have been learned. Starting from the public W2V300 representations we apply the multi-sense learning technique using the default settings and learn sensespecific word representations. Finally we propagate abstractn"
W17-1903,schafer-bildhauer-2012-building,0,0.0304522,"utional vector space as the words required for the supervised training. In this section we explore if ratings propagated to verb-noun phrases provide useful information for metaphor detection. As dataset we relied on the collection from Saif M. Mohammad and Turney (2016), who annotated different senses of WordNet verbs for metaphoricity (Fellbaum, 1998). We used the same subset of verb–direct object and verb–subject relations as used in Shutova et al. (2016). As preprocessing step we concatenated verb-noun phrases by relying on dependency information based on a web corpus, the ENCOW14 corpus (Schäfer and Bildhauer, 2012; Schäfer, 2015). We removed words and phrases that appeared less than 50 times in our corpus, thus our selection covers 535 pairs, 238 of which were metaphorical and 297 literal. Given a verb-noun phrase, such as stamp person, we obtained vector representations using word2vec and the same hyper-parameters that were used for the W2V300 embeddings (Section 2.1.1) together with the best learning Feat. Name Type AUC 1 2 3 4 5 6 Random V-NN V-Phrase NN-Phrase V NN Phrase baseline cosine cosine cosine rating rating rating .50 .75 .70 .68 .53 .78 .71 Comb Comb Comb Comb 1+2+3 4+5+6 all(1-6) 1+5+6 co"
W17-1903,D09-1033,0,0.0712212,"Missing"
W17-1903,N10-1039,0,0.0565912,"Missing"
W17-1903,shutova-teufel-2010-metaphor,0,0.165254,"Missing"
W17-1903,C10-1113,0,0.235359,"Missing"
W17-1903,P14-5010,0,0.00341336,". The potential advantage of this method is that in a metaphor detection system we are now able to look up word-sense-specific abstractness ratings instead of globally obtained ratings. For this experiment we use the VU Amsterdam Metaphor Corpus (Steen, 2010) (VUA), focusing on verb metaphors. The collection contains 23 113 verb tokens in running text, annotated as being used literally or metaphorically. In addition we present results for the TroFi metaphor dataset (Birke and Sarkar, 2006) containing 50 verbs and 3 737 labeled sentences. We pre-processed both recourses using Stanford CoreNLP (Manning et al., 2014) for lemmatization, part-of-speech tagging and dependency parsing. We present results by applying ten-fold crossvalidation over the entire data. For the VUA we additionally present results for the test data using the same training/test split as in Beigman Klebanov et al. (2016). 1. Rating of the verbs subject 2. Rating of the verbs object 3. Average rating of all nouns (excluding proper names) 4. Average rating of all proper names 5. Average rating of all verbs, excluding the target verb 6. Average rating of all adjectives 7. Average rating of all adverbs For classification we used a balanced"
W17-1903,N16-1020,0,0.565678,"Detection by Propagating Abstractness to Words, Phrases and Individual Senses Maximilian Köper and Sabine Schulte im Walde Institut für Maschinelle Sprachverarbeitung Universität Stuttgart, Germany {maximilian.koeper,schulte}@ims.uni-stuttgart.de Abstract ried out using a variety of features including selectional preferences (Martin, 1996; Shutova and Teufel, 2010; Shutova et al., 2010; Haagsma and Bjerva, 2016), word-level semantic similarity (Li and Sporleder, 2009; Li and Sporleder, 2010), topic models (Heintz et al., 2013), word embeddings (Dinh and Gurevych, 2016) and visual information (Shutova et al., 2016). Abstract words refer to things that can not be seen, heard, felt, smelled, or tasted as opposed to concrete words. Among other applications, the degree of abstractness has been shown to be a useful information for metaphor detection. Our contribution to this topic are as follows: i) we compare supervised techniques to learn and extend abstractness ratings for huge vocabularies ii) we learn and investigate norms for multi-word units by propagating abstractness to verb-noun pairs which lead to better metaphor detection, iii) we overcome the limitation of learning a single rating per word and s"
W17-1903,D14-1113,0,0.0388011,"26 2.3 Abstractness norms are implemented using the same five feature dimensions as used by Turney et al. (2011) plus dimensions respectively for subject and object, thus we rely on the seven feature, namely: Sense-specific Abstractness Ratings In this section we investigate if automatically learned multi-sense abstractness ratings, that is having different ratings per word sense, are potentially useful for the task of metaphor detection. Recent advances in word representation learning led to the development of algorithms for nonparametric and unsupervised multi-sense representation learning (Neelakantan et al., 2014; Liu et al., 2015; Li and Jurafsky, 2015; Bartunov et al., 2016). Using these techniques one can learn a different vector representation per word sense. Such representations can be combined with our abstractness learning method from section 2.1.1. While in theory any multi-sense learning technique can be applied, we decided for the one introduced by Pelevina et al. (2016), as it performs sense learning after single senses have been learned. Starting from the public W2V300 representations we apply the multi-sense learning technique using the default settings and learn sensespecific word repres"
W17-1903,W13-0906,0,0.384573,"Missing"
W17-1903,W16-1620,0,0.018473,"sense, are potentially useful for the task of metaphor detection. Recent advances in word representation learning led to the development of algorithms for nonparametric and unsupervised multi-sense representation learning (Neelakantan et al., 2014; Liu et al., 2015; Li and Jurafsky, 2015; Bartunov et al., 2016). Using these techniques one can learn a different vector representation per word sense. Such representations can be combined with our abstractness learning method from section 2.1.1. While in theory any multi-sense learning technique can be applied, we decided for the one introduced by Pelevina et al. (2016), as it performs sense learning after single senses have been learned. Starting from the public W2V300 representations we apply the multi-sense learning technique using the default settings and learn sensespecific word representations. Finally we propagate abstractness to every newly created sense representation by using the exact same model and training data as in Section 1. For a given word in a sentence we can now disambiguate the word sense by comparing its sense-specific vector representation to all context words. The context words are represented using the (single sense) global represent"
W17-1903,P14-1024,0,0.388093,"are potentially useful for metaphor detection. Finally, with this paper we publish automatically created abstractness norms for 3 million English words and multi-words as well as automatically created sense-specific abstractness ratings. 1 The underlying motivation of using abstractness in metaphor detection goes back to Lakoff and Johnson (1980), who argue that metaphor is a method for transferring knowledge from a concrete domain to an abstract domain. Abstractness was already applied successfully for the detection of metaphors across a variety of languages (Turney et al., 2011; Dunn, 2013; Tsvetkov et al., 2014; Beigman Klebanov et al., 2015; Köper and Schulte im Walde, 2016b). The abstractness information itself is typically taken from a dictionary, created either by manual annotation or by extending manually collected ratings with the help of supervised learning techniques that rely on word representations. While potentially less reliable, automatically created norm-based abstractness ratings can easily cover huge dictionaries. Although some methods have been used to learn abstractness, literature lacks a comparison of these learning techniques. Introduction The standard approach to studying abstr"
W17-1903,D11-1063,0,0.559533,"multisense abstractness ratings are potentially useful for metaphor detection. Finally, with this paper we publish automatically created abstractness norms for 3 million English words and multi-words as well as automatically created sense-specific abstractness ratings. 1 The underlying motivation of using abstractness in metaphor detection goes back to Lakoff and Johnson (1980), who argue that metaphor is a method for transferring knowledge from a concrete domain to an abstract domain. Abstractness was already applied successfully for the detection of metaphors across a variety of languages (Turney et al., 2011; Dunn, 2013; Tsvetkov et al., 2014; Beigman Klebanov et al., 2015; Köper and Schulte im Walde, 2016b). The abstractness information itself is typically taken from a dictionary, created either by manual annotation or by extending manually collected ratings with the help of supervised learning techniques that rely on word representations. While potentially less reliable, automatically created norm-based abstractness ratings can easily cover huge dictionaries. Although some methods have been used to learn abstractness, literature lacks a comparison of these learning techniques. Introduction The"
W17-1903,W16-1104,0,\N,Missing
W17-5202,N10-1000,0,0.136252,"Missing"
W17-5202,P15-1162,0,0.025946,"lead to good results on SemEval and to state-of-the-art results on SenTubeA and SenTube-T. Similarly to R ETROFIT, C NN does not outperform any of the other methods on any dataset. As said, this method does not beat the baseline on SST-fine, SenTube-A, and SenTube-T. However, it outperforms the AVE baseline on SST-binary and OpeNER. The best models are L STM and B I L STM. The best overall model is B I LSTM, which outperforms the other models on half of the tasks (SST-fine, 5 Discussion While approaches that average the word embeddings for a sentence are comparable to state-of-theart results (Iyyer et al., 2015), AVE and R ETROFIT do not perform particularly well. This is likely due to the fact that logistic regression lacks the nonlinearities which Iyyer et al. (2015) found helped, especially at deeper layers. Averaging all of the embeddings for longer phrases also seems to lead to representations that do not contain enough information for the classifier. We also experimented with using large sentiment lexicons as the semantic lexicon for retrofitting, but found that this hurt the representation more than it helped. We believe this is because there are not enough kinds of relationships to exploit th"
W17-5202,P16-1191,0,0.0230242,"e Stanford Sentiment Treebank (Socher et al., 2013). Since this dataset is annotated for sentiment at each node of a parse tree, they train and test on these annotated phrases. Both Socher et al. (2013) and Tai et al. (2015) also propose various RNNs which are able to take better advantage of the labeled nodes and which achieve better results than standard RNNs. However, these models require annotated parse trees, which are not necessarily available for other datasets. C ONVOLUTIONAL N EURAL N ETWORKS (C NN) have proven effective for text classification (dos Santos and Gatti, 2014; Kim, 2014; Flekova and Gurevych, 2016). Kim (2014) use skipgram vectors (Mikolov et al., 2013) as input to a variety of Convolutional Neural Networks and test on seven datasets, including the Stanford Sentiment Treebank (Socher et al., 2013). The best performing setup across datasets is a single layer CNN which updates the original skipgram vectors during training. Overall, these approaches currently achieve stateof-the-art results on many datasets, but they have not been compared to retrofitting or joint training approaches. (1) and backpropagate the error to the corresponding word embeddings. Here, t is the original n-gram, tr i"
W17-5202,D15-1242,0,0.0326042,"Missing"
W17-5202,Q16-1023,0,0.0895574,"alize the layer. These vectors then pass to an LSTM layer. We feed the final hidden state to a standard fully-connected 50-dimensional dense layer and then to a softmax layer, which gives us a probability distribution over our classes. As a regularizer, we use a dropout (Srivastava et al., 2014) of 0.5 before the LSTM layer. The B IDIRECTIONAL LSTM (B I L STM) has the same architecture as the normal LSTM, but includes an additional layer which runs from the end of the text to the front. This approach has led to state-of-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer6 . We also train a simple one-layer C NN with one convolutional layer on top of pre-trained word embeddings. The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n × R dimensional matrix, where R is the dimensionality of the word embeddings. The embedding matrix is then convoluted with filter sizes of 2, 3, and 4, followed by a pooling layer of length 2. This is then fed to a"
W17-5202,S13-2052,0,0.310686,"ntiment of a sentence. δs (t) is 1 if the true sentiment is positive and −1 if it is negative. They then use a weighted sum of both scores to create their sentiment embeddings: losscombined (t, tr ) = (3) α · losscw (t, tr ) + (1 − α) · losss (t, tr ) . This requires sentiment-annotated data for training both the syntactic and sentiment losses, which they acquire by collecting tweets associated with certain emoticons. In this way, they are able to simultaneously incorporate sentiment and semantic information relevant to their task. They test their approach on the SemEval 2013 twitter dataset (Nakov et al., 2013), changing the task from three-class to binary classification, and find that they outperform other approaches. Overall, the joint approach shows promise for tasks with a large amount of distantly-labeled data. 2.2 Datasets We choose to evaluate the approaches presented in Section 2.1 on a number of different datasets from different domains, which also have differing levels of granularity of class labels. The Stanford Sentiment Treebank and SemEval 2013 shared-task dataset have already been used as benchmarks for some of the approaches mentioned in Section 2.1. Table 1 shows which approaches ha"
W17-5202,P15-2128,0,0.0146677,"(1.1) 64.2 (1.0) 64.0 (0.9) 64.4 (1.5) B OW Table 3: Accuracy on the test sets. For all neural models we perform 5 runs and show the mean and standard deviation. The best results for each dataset is given in bold and results that have been previously reported are highlighted. All results derive from our reimplementation of the methods. We describe significance values in the text and appendix. Footnotes refer to the work where a method was previously tested on a specific dataset, although not necessarily with the same results: [1] Tai et al. (2015) [2] Kim (2014) [3] Faruqui et al. (2015) [4] Lambert (2015) [5] Uryupina et al. (2014) [6] Tang et al. (2014). jargon. We performed a short analysis of datasets (shown in Table 4), where we take frequency of emoticons usage as an indirect indicator of informal speech and found that, indeed, the frequency of emoticons in the SemEval and SenTube datasets diverges significantly from the other datasets. The fact that J OINT is distantly trained on similar data gives it an advantage over other models on these datasets. This leads us to believe that this approach would transfer well to novel sentiment analysis tasks with similar properties. The fact that C"
W17-5202,P11-1015,0,0.0552599,"word while training a skip-gram model (Mikolov et al., 2013). They sample extra context words, taken either from a thesaurus or association data, and incorporate this into the context of the word for each update. The evaluation is both intrinsical, on word similarity and relatedness tasks, as well as extrinsical on TOEFL synonym and document classification tasks. The augmentation strategy improves the word vectors on all tasks. Faruqui et al. (2015) propose a method to refine word vectors by using relational information from semantic lexicons (we will refer to this method 2.1.2 Joint Training Maas et al. (2011) were the first to jointly train semantic and sentiment word vectors. In order to capture semantic similarities, they propose a probabilistic model using a continuous mixture model over words, similar to Latent Dirichlet Allocation (LDA, Blei et al., 2003). To capture sentiment information, they include a sentiment term which uses logistic regression to predict the sentiment of a document. The full objective function is a combination of the semantic and sentiment objectives. They test their model on several sentiment and subjectivity benchmarks. Their results indicate that including the sentim"
W17-5202,W02-1011,0,0.0266137,"Missing"
W17-5202,P16-2067,0,0.126759,"depending on the embeddings used to initialize the layer. These vectors then pass to an LSTM layer. We feed the final hidden state to a standard fully-connected 50-dimensional dense layer and then to a softmax layer, which gives us a probability distribution over our classes. As a regularizer, we use a dropout (Srivastava et al., 2014) of 0.5 before the LSTM layer. The B IDIRECTIONAL LSTM (B I L STM) has the same architecture as the normal LSTM, but includes an additional layer which runs from the end of the text to the front. This approach has led to state-of-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer6 . We also train a simple one-layer C NN with one convolutional layer on top of pre-trained word embeddings. The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n × R dimensional matrix, where R is the dimensionality of the word embeddings. The embedding matrix is then convoluted with filter sizes of 2, 3, and 4, followed"
W17-5202,C00-2137,0,0.222252,"ly significant, except for the difference between B I L STM and C NN at 50 dimensions on the OpeNER dataset. Our analysis of different dimensionalities as input for the classification models reveals that, typically, the higher dimensional vectors (300 or 600) outperform lower dimensions. The only differences are in J OINT for SenTube-T and SemEval and L STM for SenTube-A and AVE on all datasets except OpeNER. Results Table 3 shows the results for the seven models across all datasets, as well as the macro-averaged results. We visualize them in Figure 3. We performed random approximation tests (Yeh, 2000) using the sigf package (Pad´o, 2006) with 10,000 iterations to determine the statistical significance of differences between models. Since the reported accuracies for the neural models are the means over five runs, we cannot use this technique in a straightforward manner. Therefore, we perform the random approximation tests between the runs7 and consider the models statistically different if a majority (at least 3) of the runs are statistically different (p < 0.01, which corresponds to p < 0.05 with Bonferroni correction for 5 hypotheses). The results of statistical testing are summarized in"
W17-5202,W03-1014,0,0.151015,"Missing"
W17-5202,P14-2089,0,0.029036,"paraphrases as the semantic lexicon, to improve the original vectors. This dataset includes 8 million lexical paraphrases collected from bilingual corpora, where words in language A are considered paraphrases if they are consistently translated to the same word in language B. They then test on the Stanford Sentiment Treebank (Socher et al., 2013). They train an L2regularized logistic regression classifier on the average of the word embeddings for a text and find improvements after retrofitting. All above approaches show improvements over previous word embedding approaches (Mnih and Teh, 2012; Yu and Dredze, 2014; Xu et al., 2014) on this data set. Retrofitting to Semantic Lexicons There have been several proposals to improve the quality of word embeddings using semantic lexicons. Yu and Dredze (2014) propose several methods which combine the CBOW architecture (Mikolov et al., 2013) and a second objective function which attempts to maximize the relations found within some semantic lexicon. They use both the Paraphrase Database (Ganitkevitch et al., 2013) and WordNet (Fellbaum, 1999) and test their models on language modeling and semantic similarity tasks. They report that their method leads to an impr"
W17-5202,C14-1008,0,0.034926,"ent neural networks and train on the Stanford Sentiment Treebank (Socher et al., 2013). Since this dataset is annotated for sentiment at each node of a parse tree, they train and test on these annotated phrases. Both Socher et al. (2013) and Tai et al. (2015) also propose various RNNs which are able to take better advantage of the labeled nodes and which achieve better results than standard RNNs. However, these models require annotated parse trees, which are not necessarily available for other datasets. C ONVOLUTIONAL N EURAL N ETWORKS (C NN) have proven effective for text classification (dos Santos and Gatti, 2014; Kim, 2014; Flekova and Gurevych, 2016). Kim (2014) use skipgram vectors (Mikolov et al., 2013) as input to a variety of Convolutional Neural Networks and test on seven datasets, including the Stanford Sentiment Treebank (Socher et al., 2013). The best performing setup across datasets is a single layer CNN which updates the original skipgram vectors during training. Overall, these approaches currently achieve stateof-the-art results on many datasets, but they have not been compared to retrofitting or joint training approaches. (1) and backpropagate the error to the corresponding word embeddin"
W17-5202,C16-1329,0,0.104973,". We feed the final hidden state to a standard fully-connected 50-dimensional dense layer and then to a softmax layer, which gives us a probability distribution over our classes. As a regularizer, we use a dropout (Srivastava et al., 2014) of 0.5 before the LSTM layer. The B IDIRECTIONAL LSTM (B I L STM) has the same architecture as the normal LSTM, but includes an additional layer which runs from the end of the text to the front. This approach has led to state-of-the-art results for POS-tagging (Plank et al., 2016), dependency parsing (Kiperwasser and Goldberg, 2016) and text classification (Zhou et al., 2016), among others. We use the same parameters as the LSTM, but concatenate the two hidden layers before passing them to the dense layer6 . We also train a simple one-layer C NN with one convolutional layer on top of pre-trained word embeddings. The first layer is an embeddings layer that maps the input of length n (padded when needed) to an n × R dimensional matrix, where R is the dimensionality of the word embeddings. The embedding matrix is then convoluted with filter sizes of 2, 3, and 4, followed by a pooling layer of length 2. This is then fed to a fully connected dense layer with ReLU activ"
W17-5202,D13-1170,0,0.020305,"tion into their word embeddings (we refer to this method as J OINT). They extend the word embedding approach of Collobert et al. (2011), who use a neural network to predict whether an n-gram is a true n-gram or a “corrupted” version. They use the hinge-loss losscw (t, tr ) = max(0, 1 − f cw (t) + f cw (tr )) R ECURRENT U NITS (GRUs) (Chung et al., 2014), are a variant of a feed-forward network which includes a memory state capable of learning long distance dependencies. In various forms, they have proven useful for text classification tasks (Tai et al., 2015; Tang et al., 2016). Socher et al. (2013) and Tai et al. (2015) use Glove vectors (Pennington et al., 2014) in combination with a recurrent neural networks and train on the Stanford Sentiment Treebank (Socher et al., 2013). Since this dataset is annotated for sentiment at each node of a parse tree, they train and test on these annotated phrases. Both Socher et al. (2013) and Tai et al. (2015) also propose various RNNs which are able to take better advantage of the labeled nodes and which achieve better results than standard RNNs. However, these models require annotated parse trees, which are not necessarily available for other datase"
W17-5202,W11-2207,0,0.0115737,"ng sentiment information into word embeddings during training gives good results for datasets that are lexically similar to the training data. With our experiments, we contribute to a better understanding of the performance of different model architectures on different data sets. Consequently, we detect novel state-of-the-art results on the SenTube datasets. 1 Introduction The task of analyzing private states expressed by an author in text, such as sentiment, emotion or affect, can give us access to a wealth of hidden information to analyze product reviews (Liu et al., 2005), political views (Speriosu et al., 2011), or to identify potentially dangerous activity on the 1 The code and embeddings for the best models are available at http://www.ims.uni-stuttgart.de/ data/sota_sentiment 2 Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 2–12 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics and sentiment perform well on datasets that are similar to the training data. 2 in this paper as R ETROFIT). They require a vocabulary V = {w1 , . . . , wn }, its word embedˆ = {ˆ dings matrix Q q1 , . . . , qˆ"
W17-5202,P15-1150,0,0.754357,"erminology here, but make no asand sentiment information into their word embeddings (we refer to this method as J OINT). They extend the word embedding approach of Collobert et al. (2011), who use a neural network to predict whether an n-gram is a true n-gram or a “corrupted” version. They use the hinge-loss losscw (t, tr ) = max(0, 1 − f cw (t) + f cw (tr )) R ECURRENT U NITS (GRUs) (Chung et al., 2014), are a variant of a feed-forward network which includes a memory state capable of learning long distance dependencies. In various forms, they have proven useful for text classification tasks (Tai et al., 2015; Tang et al., 2016). Socher et al. (2013) and Tai et al. (2015) use Glove vectors (Pennington et al., 2014) in combination with a recurrent neural networks and train on the Stanford Sentiment Treebank (Socher et al., 2013). Since this dataset is annotated for sentiment at each node of a parse tree, they train and test on these annotated phrases. Both Socher et al. (2013) and Tai et al. (2015) also propose various RNNs which are able to take better advantage of the labeled nodes and which achieve better results than standard RNNs. However, these models require annotated parse trees, which are"
W17-5202,C16-1311,0,0.0331778,"ut make no asand sentiment information into their word embeddings (we refer to this method as J OINT). They extend the word embedding approach of Collobert et al. (2011), who use a neural network to predict whether an n-gram is a true n-gram or a “corrupted” version. They use the hinge-loss losscw (t, tr ) = max(0, 1 − f cw (t) + f cw (tr )) R ECURRENT U NITS (GRUs) (Chung et al., 2014), are a variant of a feed-forward network which includes a memory state capable of learning long distance dependencies. In various forms, they have proven useful for text classification tasks (Tai et al., 2015; Tang et al., 2016). Socher et al. (2013) and Tai et al. (2015) use Glove vectors (Pennington et al., 2014) in combination with a recurrent neural networks and train on the Stanford Sentiment Treebank (Socher et al., 2013). Since this dataset is annotated for sentiment at each node of a parse tree, they train and test on these annotated phrases. Both Socher et al. (2013) and Tai et al. (2015) also propose various RNNs which are able to take better advantage of the labeled nodes and which achieve better results than standard RNNs. However, these models require annotated parse trees, which are not necessarily avai"
W17-5202,P14-1146,0,0.601608,"order to capture semantic similarities, they propose a probabilistic model using a continuous mixture model over words, similar to Latent Dirichlet Allocation (LDA, Blei et al., 2003). To capture sentiment information, they include a sentiment term which uses logistic regression to predict the sentiment of a document. The full objective function is a combination of the semantic and sentiment objectives. They test their model on several sentiment and subjectivity benchmarks. Their results indicate that including the sentiment information during training actually leads to decreased performance. Tang et al. (2014) take the joint training approach and simultaneously incorporate syntactic2 2 3 We use the authors’ terminology here, but make no asand sentiment information into their word embeddings (we refer to this method as J OINT). They extend the word embedding approach of Collobert et al. (2011), who use a neural network to predict whether an n-gram is a true n-gram or a “corrupted” version. They use the hinge-loss losscw (t, tr ) = max(0, 1 − f cw (t) + f cw (tr )) R ECURRENT U NITS (GRUs) (Chung et al., 2014), are a variant of a feed-forward network which includes a memory state capable of learning"
W17-5202,uryupina-etal-2014-sentube,0,0.39914,"a dataset. The number of labels corresponds to the annotation scheme, where: two is positive and negative; three is positive, neutral, negative; four is strong positive, positive, negative, strong negative; five is strong positive, positive, neutral, negative, strong negative. − − + + + − − + − − − − − + − − − − − − − − − + + + − − − − + + − − − − + + − − − − the sentiment phrase is obligatory. Additionally, sentiment phrases are annotated for four levels of sentiment: strong negative, negative, positive and strong positive. We use a split of 2780/186/734 examples. 2.2.3 The SenTube datasets (Uryupina et al., 2014) are texts that are taken from YouTube comments regarding automobiles and tablets. These comments are normally directed towards a commercial or a video that contains information about the product. We take only those comments that have some polarity towards the target product in the video. For the automobile dataset (SenTube-A), this gives a 3381/225/903 training, validation, and test split. For the tablets dataset (SenTube-T) the splits are 4997/333/1334. These are annotated for positive, negative, and neutral sentiment. Table 1: Mapping of previous state-of-the-art methods to previous evaluat"
W17-6910,evert-2004-statistical,0,0.0609123,"s compared to adjectives and verbs. In total we had 9,241 nouns covered in an extensive selection of behavioural measures, such as valency scores (Warriner et al., 2013) and reaction times (Balota et al., 2007) which we aim to include in further analyses. We used the selected nouns both as targets and as context words, and created a symmetric noun– noun co-occurrence matrix relying on a ± 20 word window in the ENCOW14A corpus, a collection of 16-billion English tokens extracted from the web (Sch¨afer, 2015). In this way, each co-occurence is represented by a score (counts or positive LMI, cf. Evert (2004)). In addition, we ensure information about the concreteness scores for both the targets and the context words. All the statistical analyses reported in this paper use linear mixed effects models (LME, Baayen et al. (2008)) with centered continuous predictors, implementing a maximal random effects structure as suggested by Barr et al. (2013). 3 Study 1: Investigating concreteness in distributionally similar words In this study we investigate if distributionally similar words are also similar in their concreteness vs. abstractness scores. After computing the cosine similarity between each pair"
W17-6910,E14-1025,0,0.0694901,"Missing"
W17-6910,W09-0214,0,0.0302271,"the target (different lines), also the average concreteness ratings of the NNs increase (y-axis). An LME analysis indicates that this increase in the ratings associated with the increase of the target’s concreteness is statistically significant (βconcretenessT arget = 0.22, p&lt;.001). On average, there are no significant differences between the different ranks (p=0.94); however, there is a significant reduction in the ratings while increasing the number of NNs of highly concrete targets (βconcretenessT arget:rank = -0.001, p&lt;.001). Finally, we computed the neighbourhood density of each target (Sagi et al., 2009). Higher density scores indicate higher similarity between the vectors of the NNs and the vector of the corresponding target. Table 2 reports means and standard deviations of the neighbour density for each target concreteness bin. The right-most column reports the regression estimates of the pairwise comparison of each bin with its predecessor and the adjusted p-values. The analysis shows a higher neighbourhood density for the more extreme scores, both for concrete and abstract targets. 4.5 4.5 ● ● ● ● ● ● ● ● ● ● 5 ● ● ● 5 ● ● ● ● ● ● ● ● ● ● ● ● 4 ● 3.5 3.5 ● ● ● ● ● ● ● ● 4.5 ● 3 ● 2.5 3.0"
W17-6942,W06-3812,0,0.0422983,"tion extraction (Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte im Walde (2016). Most recently, sensedistinguishing classification approaches have also been defined for predict models by using multi-sense embeddings, e.g., Biemann (2006); Lau et al. (2012); Neelakantan et al. (2014); Li and Jurafsky (2015). In general, clustering efforts are motivated by specific tasks or applications, so it is difficult to provide universal recommendations regarding the optimal clustering setup. This paper nevertheless addresses clustering parameters that are presumably of general importance on the meta level: Focusing on synonymy as a task-independent goal in semantic classification, we provide an extensive clustering setup to explore the role of verb frequency ranges across various numbers of clusters. The contributions of this paper are t"
W17-6942,C10-1011,0,0.0272562,"of earlier approaches, e.g. Schulte im Walde (2000); Korhonen et al. (2003); Schulte im Walde (2006); Scarton et al. (2014), (2) might be considered as general knowledge but has –as far as we are aware of– not explicitly been proven before. 2 Data and Algorithm Using DECOW (Sch¨afer and Bildhauer, 2012; Sch¨afer, 2015) as one of the currently largest German web corpora, we extracted all base verbs and particle verbs from version DECOW14. The corpus sentences were morphologically annotated and parsed using SMOR (Faaß et al., 2010), MarMoT (M¨uller et al., 2013) and the MATE dependency parser (Bohnet, 2010). Relying on the morphological annotation, and after disregarding prefix verbs (i.e., non-separable complex verbs), we extracted a total of 4,871 base verb types and 3,173 particle verb types. As vector spaces for the verbs, we relied on word2vec (Mikolov et al., 2013) using a symmetrical window of sizes 3 and 10. The underlying corpus was again DECOW14. We applied a min-frequency threshold of 50, the dimensionality was set to 400, and we used 10 corpus iterations and 15 negative samples. Other parameters were set to default. For soft clustering, we used Non-negative matrix factorization (NMF)"
W17-6942,W16-5318,1,0.841034,"on: Compositionality In this evaluation, we predict the degree of compositionality of the complex particle verbs, i.e., the degree of relatedness between the particle verbs and their corresponding base verbs (such as abnehmen – nehmen ’take over – take’, and anfangen – fangen ’begin – catch’). We assume that if a particle verb and its base verb tend to co-occur in the same cluster within a cluster analysis, then the particle verb is semantically transparent, rather than opaque. The predictions are evaluated against an existing dataset of human ratings on German particle verb compositionality (Bott et al., 2016). The gold standard contains a total of 400 particle verbs across 11 particle types and 3 frequency bands. Similarly to the evaluation metric described in the previous section, the compositionality evaluation is also applied to all thresholds in the set tmax − k · 0.001, k ∈ N0 , with tmax being the largest inclusion value found in the clustering, as well as to all top-n cluster assignments with 1 ≤ n ≤ N2 . For each pair of particle verb and base verb, e.g., abnehmen – nehmen, we then compare the assignment of the two verbs to the same vs. different clusters in two different ways. • Pointwise"
W17-6942,W98-1114,0,0.124602,"ntic classification, we demonstrate that low-frequency German verbs are clustered significantly worse than mid- or high-frequency German verbs, and that German complex verbs are in general more difficult to cluster than German base verbs. 1 Introduction Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte im Walde (2016). Most recently,"
W17-6942,C96-1055,0,0.0807477,"complex verb types, and focusing on synonymy as a taskindependent goal in semantic classification, we demonstrate that low-frequency German verbs are clustered significantly worse than mid- or high-frequency German verbs, and that German complex verbs are in general more difficult to cluster than German base verbs. 1 Introduction Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); I"
W17-6942,D07-1091,0,0.0579083,"ntly worse than mid- or high-frequency German verbs, and that German complex verbs are in general more difficult to cluster than German base verbs. 1 Introduction Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte im Walde (2016). Most recently, sensedistinguishing classification approaches have also been defined for predict models by u"
W17-6942,P05-1005,0,0.057775,"nd focusing on synonymy as a taskindependent goal in semantic classification, we demonstrate that low-frequency German verbs are clustered significantly worse than mid- or high-frequency German verbs, and that German complex verbs are in general more difficult to cluster than German base verbs. 1 Introduction Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (200"
W17-6942,P16-2042,1,0.87389,"Missing"
W17-6942,P03-1009,0,0.333814,"ion (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte im Walde (2016). Most recently, sensedistinguishing classification approaches have also been defined for predict models by using multi-sense embeddings, e.g., Biemann (2006); Lau et al. (2012); Neelakantan et al. (2014); Li and Jurafsky (2015). In general, clustering efforts are motivated by specific tasks or applications, so it is difficult to provide universal recommendations regarding the optimal clustering setup. This paper nevertheless addresses clustering parameters that are presumably of general importance on the meta level: Focusing on"
W17-6942,E12-1060,0,0.0205375,"(Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte im Walde (2016). Most recently, sensedistinguishing classification approaches have also been defined for predict models by using multi-sense embeddings, e.g., Biemann (2006); Lau et al. (2012); Neelakantan et al. (2014); Li and Jurafsky (2015). In general, clustering efforts are motivated by specific tasks or applications, so it is difficult to provide universal recommendations regarding the optimal clustering setup. This paper nevertheless addresses clustering parameters that are presumably of general importance on the meta level: Focusing on synonymy as a task-independent goal in semantic classification, we provide an extensive clustering setup to explore the role of verb frequency ranges across various numbers of clusters. The contributions of this paper are two-fold: We demonst"
W17-6942,D15-1200,0,0.0243715,"among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte im Walde (2016). Most recently, sensedistinguishing classification approaches have also been defined for predict models by using multi-sense embeddings, e.g., Biemann (2006); Lau et al. (2012); Neelakantan et al. (2014); Li and Jurafsky (2015). In general, clustering efforts are motivated by specific tasks or applications, so it is difficult to provide universal recommendations regarding the optimal clustering setup. This paper nevertheless addresses clustering parameters that are presumably of general importance on the meta level: Focusing on synonymy as a task-independent goal in semantic classification, we provide an extensive clustering setup to explore the role of verb frequency ranges across various numbers of clusters. The contributions of this paper are two-fold: We demonstrate that (1) low-frequency German verbs are cluste"
W17-6942,D07-1039,0,0.0433659,"as a taskindependent goal in semantic classification, we demonstrate that low-frequency German verbs are clustered significantly worse than mid- or high-frequency German verbs, and that German complex verbs are in general more difficult to cluster than German base verbs. 1 Introduction Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte i"
W17-6942,D13-1032,0,0.0539968,"Missing"
W17-6942,D14-1113,0,0.0391589,"003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte im Walde (2016). Most recently, sensedistinguishing classification approaches have also been defined for predict models by using multi-sense embeddings, e.g., Biemann (2006); Lau et al. (2012); Neelakantan et al. (2014); Li and Jurafsky (2015). In general, clustering efforts are motivated by specific tasks or applications, so it is difficult to provide universal recommendations regarding the optimal clustering setup. This paper nevertheless addresses clustering parameters that are presumably of general importance on the meta level: Focusing on synonymy as a task-independent goal in semantic classification, we provide an extensive clustering setup to explore the role of verb frequency ranges across various numbers of clusters. The contributions of this paper are two-fold: We demonstrate that (1) low-frequency"
W17-6942,C00-2094,0,0.10602,"are clustered significantly worse than mid- or high-frequency German verbs, and that German complex verbs are in general more difficult to cluster than German base verbs. 1 Introduction Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte im Walde (2016). Most recently, sensedistinguishing classification approaches have also been defined"
W17-6942,P99-1014,0,0.242528,"in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte im Walde (2016). Most recently, sensedistinguishing classification approaches have also been defined for predict models by using multi-sense embeddings, e.g., Biemann (2006); Lau et al. (2012); Neelakantan et al. (2014); Li and Jurafsky (2015). In general, clustering efforts are motivated by specific tasks or applications, so it is difficult to provide universal recommendations regarding the optimal clustering setup. This paper nevertheless addresses clustering parameters that are presumably of g"
W17-6942,schafer-bildhauer-2012-building,0,0.0320026,"Missing"
W17-6942,C00-2108,1,0.723425,"se disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte im Walde (2016). Most recently, sensedistinguishing classification approaches have also been defined for predict models by using multi-sense embeddings, e.g., Biemann (2006); Lau et al. (2012); Neelakantan et al. (2014); Li and Jurafsky (2015). In general, clustering efforts are motivated by specific tasks or applications, so it is difficult to provide universal recommendations regarding the optimal clustering setup. This paper nevertheless addresses clustering parameters that are presumably of general importance on the"
W17-6942,J06-2001,1,0.693155,"ification, we provide an extensive clustering setup to explore the role of verb frequency ranges across various numbers of clusters. The contributions of this paper are two-fold: We demonstrate that (1) low-frequency German verbs are clustered significantly worse than mid- or high-frequency German verbs, and that (2) German complex verbs are in general more difficult to cluster than German base verbs. While (1) the effect of clustering low-frequency target verbs has been investigated by a restricted number of earlier approaches, e.g. Schulte im Walde (2000); Korhonen et al. (2003); Schulte im Walde (2006); Scarton et al. (2014), (2) might be considered as general knowledge but has –as far as we are aware of– not explicitly been proven before. 2 Data and Algorithm Using DECOW (Sch¨afer and Bildhauer, 2012; Sch¨afer, 2015) as one of the currently largest German web corpora, we extracted all base verbs and particle verbs from version DECOW14. The corpus sentences were morphologically annotated and parsed using SMOR (Faaß et al., 2010), MarMoT (M¨uller et al., 2013) and the MATE dependency parser (Bohnet, 2010). Relying on the morphological annotation, and after disregarding prefix verbs (i.e., no"
W17-6942,P03-1002,0,0.0997258,"lex verbs are in general more difficult to cluster than German base verbs. 1 Introduction Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte im Walde (2016). Most recently, sensedistinguishing classification approaches have also been defined for predict models by using multi-sense embeddings, e.g., Biemann (2006); Lau et al. (2012); Nee"
W17-6942,2014.amta-researchers.21,1,0.789582,"high-frequency German verbs, and that German complex verbs are in general more difficult to cluster than German base verbs. 1 Introduction Semantic classifications are of great interest to computational linguistics, specifically regarding the pervasive problem of data sparseness in the processing of natural language. Such classifications have been used in applications such as word sense disambiguation (Dorr and Jones, 1996; Kohomban and Lee, 2005; McCarthy et al., 2007), parsing (Carroll et al., 1998; Carroll and Fang, 2004), machine translation (Prescher et al., 2000; Koehn and Hoang, 2007; Weller et al., 2014), and information extraction (Surdeanu et al., 2003; Venturi et al., 2009), among many others. Aiming for not only a hard assignment of word types to semantic classes but potentially distinguishing between various word senses, soft-clustering approaches have been exploited as the main tool for automatic semantic classification, e.g., Rooth et al. (1999); Schulte im Walde (2000); Korhonen et al. (2003); Iosif and Potamianos (2007); K¨oper and Schulte im Walde (2016). Most recently, sensedistinguishing classification approaches have also been defined for predict models by using multi-sense embed"
W17-7101,W17-6910,1,0.257938,"auer, 2012; Sch¨afer, 2015), currently one of the largest web corpora, to induce co-occurrence frequency matrices for the target nouns. As dimensions in the target noun vectors we compared two variants: on the one hand, we used the full set of 9,241 target nouns as co-occurring word dimensions; on the other hand, we used the reduced set of 2,000 target nouns as co-occurring word dimensions. The main reason for using the targets also as vector dimensions was to enable explorations of interdependencies between the abstractness/concreteness scores of our targets and their co-occurring words (see Frassinelli et al. (2017) for details). As window size for the co-occurrence counts we looked at two words to the left and to the right of the target words. As embedding vectors, we used the publicly available representations obtained from the word2vec cbow model (Mikolov et al., 2013). This model was trained on a Google-internal news corpus with 100 billion tokens. The visual features were extracted from images downloaded from the Google search engine, following Kiela et al. (2016). We queried the search engine for up to 25 images per word, and converted all images into high-dimensional numerical representations by u"
W17-7101,Q14-1023,0,0.0786443,"tracted from real world situations, including auditory, visual, etc. stimuli (Barsalou, 1999; Shapiro, 2007; Glenberg and Kaschak, 2002). From a computational perspective, multi-modality has been shown to enhance corpus-based cooccurrence models that predict lexical information on various tasks, such as simulating word association, predicting semantic and visual similarity, determining the compositionality of multi-word expressions, and distinguishing between abstract and concrete concepts (Andrews et al., 2008; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014; Hill et al., 2014; Lazaridou et al., 2015). One focus of interest on the computational side has addressed the question of when and which perceptual information is helpful for semantic predictions in computational models, i.e., under which conditions perceptual information enhances or even outperforms textual information. For example, previous work described filters that added visual information to corpus-based information into a computational model of word meaning only in specific conditions: Kiela et al. (2014) suggested the dispersion filter that integrates only images that resemble each other to a certain d"
W17-7101,P14-2135,0,0.102644,"drews et al., 2008; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014; Hill et al., 2014; Lazaridou et al., 2015). One focus of interest on the computational side has addressed the question of when and which perceptual information is helpful for semantic predictions in computational models, i.e., under which conditions perceptual information enhances or even outperforms textual information. For example, previous work described filters that added visual information to corpus-based information into a computational model of word meaning only in specific conditions: Kiela et al. (2014) suggested the dispersion filter that integrates only images that resemble each other to a certain degree. K¨oper and Schulte im Walde (2017) applied the same filter to a computational model of compositionality and added two more filters: the imageability filter integrating only images for highly imaginable words, as determined by existing imageability ratings; and the clustering filter only using images for a word that were similar to each other, as determined by a cluster analysis. In this paper, we explore variants of multi-modal computational models that aim to distinguish between abstract"
W17-7101,D16-1043,0,0.124176,"Missing"
W17-7101,W17-1728,1,0.858699,"Missing"
W17-7101,N15-1016,0,0.0527205,"orld situations, including auditory, visual, etc. stimuli (Barsalou, 1999; Shapiro, 2007; Glenberg and Kaschak, 2002). From a computational perspective, multi-modality has been shown to enhance corpus-based cooccurrence models that predict lexical information on various tasks, such as simulating word association, predicting semantic and visual similarity, determining the compositionality of multi-word expressions, and distinguishing between abstract and concrete concepts (Andrews et al., 2008; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014; Hill et al., 2014; Lazaridou et al., 2015). One focus of interest on the computational side has addressed the question of when and which perceptual information is helpful for semantic predictions in computational models, i.e., under which conditions perceptual information enhances or even outperforms textual information. For example, previous work described filters that added visual information to corpus-based information into a computational model of word meaning only in specific conditions: Kiela et al. (2014) suggested the dispersion filter that integrates only images that resemble each other to a certain degree. K¨oper and Schulte"
W17-7101,D13-1115,1,0.924531,"Missing"
W17-7101,schafer-bildhauer-2012-building,0,0.166864,"Missing"
W17-7101,D12-1130,0,0.0515372,"only through linguistic exposure but also incorporating multi-modal information extracted from real world situations, including auditory, visual, etc. stimuli (Barsalou, 1999; Shapiro, 2007; Glenberg and Kaschak, 2002). From a computational perspective, multi-modality has been shown to enhance corpus-based cooccurrence models that predict lexical information on various tasks, such as simulating word association, predicting semantic and visual similarity, determining the compositionality of multi-word expressions, and distinguishing between abstract and concrete concepts (Andrews et al., 2008; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014; Hill et al., 2014; Lazaridou et al., 2015). One focus of interest on the computational side has addressed the question of when and which perceptual information is helpful for semantic predictions in computational models, i.e., under which conditions perceptual information enhances or even outperforms textual information. For example, previous work described filters that added visual information to corpus-based information into a computational model of word meaning only in specific conditions: Kiela et al. (2014) suggested the dispersion"
W18-0705,P10-1142,0,0.0168551,"ems for coreference and bridging resolution. Cases of coreference and bridging resolution often require knowledge about semantic relations between anaphors and antecedents. We suggest state-of-the-art neural-network classifiers trained on relation benchmarks to predict and integrate likelihoods for relations. Two experiments with representations differing in noise and complexity improve our bridging but not our coreference resolver. 1 Introduction 2 Noun phrase (NP) coreference resolution is the task of determining which noun phrases in a text or dialogue refer to the same discourse entities (Ng, 2010). The most difficult cases in NP coreference are those which require semantic knowledge to infer the relation between the anaphor and the antecedent, as in Example (1) where we need to know that Malaria is a disease. (1) Coreference signals a relation of identity, so we assume that coreference resolution should benefit from relations that link identical or highly similar entities. Obviously, synonymy is a member of this set of relations, as exemplified in Example (3): (3) Malaria is a mosquito-borne infection. The disease is transmitted via a bite ... I live on Shortland Street. The road will"
W18-0705,W11-2501,0,0.0431043,"Missing"
W18-0705,W05-0311,0,0.0592559,"milar entities. Obviously, synonymy is a member of this set of relations, as exemplified in Example (3): (3) Malaria is a mosquito-borne infection. The disease is transmitted via a bite ... I live on Shortland Street. The road will be closed for repair work next week. Hypernymy can also be used to refer to a previously introduced entity, as in Example (4): Related, but even more complicated is the task of bridging resolution: it requires linking anaphoric noun phrases and their antecedents which however do not refer to the same referent, but are related in a way that is not explicitly stated (Poesio and Artstein, 2005; Poesio and Vieira, 1998). Bridging anaphors are discourse-new but still depend on the preceding context. For example, for resolving the windows in (2) to the room, we need to know that a room typically has windows. (2) Relation Hypotheses (4) My neighbour’s dog has been getting on my nerves lately. The stupid animal kept barking all night. Note that the direction of this relation is important, as we can introduce a hyponym and then later refer to it via a hypernym, but not vice versa1 . The relations between a bridging anaphor and its antecedent are assumed to be more diverse. The prototypic"
W18-0705,P14-1005,0,0.153276,"Missing"
W18-0705,poesio-etal-2002-acquiring,1,0.869634,"Missing"
W18-0705,J98-2001,0,0.88251,"synonymy is a member of this set of relations, as exemplified in Example (3): (3) Malaria is a mosquito-borne infection. The disease is transmitted via a bite ... I live on Shortland Street. The road will be closed for repair work next week. Hypernymy can also be used to refer to a previously introduced entity, as in Example (4): Related, but even more complicated is the task of bridging resolution: it requires linking anaphoric noun phrases and their antecedents which however do not refer to the same referent, but are related in a way that is not explicitly stated (Poesio and Artstein, 2005; Poesio and Vieira, 1998). Bridging anaphors are discourse-new but still depend on the preceding context. For example, for resolving the windows in (2) to the room, we need to know that a room typically has windows. (2) Relation Hypotheses (4) My neighbour’s dog has been getting on my nerves lately. The stupid animal kept barking all night. Note that the direction of this relation is important, as we can introduce a hyponym and then later refer to it via a hypernym, but not vice versa1 . The relations between a bridging anaphor and its antecedent are assumed to be more diverse. The prototypical bridging relation is re"
W18-0705,D16-1245,0,0.0345148,"Missing"
W18-0705,D13-1203,0,0.0250805,"0.00 52.03 50.74 R 12.64 12.80 12.16 10.90 F1 19.66 20.38 19.72 17.95 coord, attri, hyper and random. The class with the highest value is returned. We added one feature at a time and analysed the change in CoNLL score. The results are not shown in detail, as the score decreased in every version. For coreference resolution, where the baseline performance is already quite high, the additional semantic information thus does not seem to improve results. This is in line with Bj¨orkelund and Kuhn (2014), where integrating a WordNet synonym/hypernym lookup did not improve the performance, as well as Durrett and Klein (2013), where increased semantic information was not beneficial either. Table 4: Effect of the cosine threshold constraint, for the relation meronymy. 0.2) we now find 14 additional correct pairs, for example: (6) IBM said it expects industrywide efforts to become prevalent because semiconductor manufacturing has become so expensive. A state-of-the-art plant cost 40 million in the mid-1970s but costs 500 million today because the technology is so complex. 5 The first experiment had a few major shortcomings. First, we did not have lemmatised vectors, and as a result, singular and plural forms of the"
W18-0705,P14-2006,0,0.0581452,"Missing"
W18-0705,R15-1027,0,0.0452951,"Missing"
W18-0705,W16-5304,0,0.173381,"Missing"
W18-0705,D14-1222,0,0.438693,"Missing"
W18-0705,N15-1098,0,0.0541689,"Missing"
W18-0705,J00-4003,0,0.623348,"Missing"
W18-0705,P12-1084,0,0.453142,"chain as the gold antecedent. We applied train-developmenttest splits, used the training and development set for optimisation, and report performance on the test set. My car broke down yesterday. It turned out to be a problem with the engine. However, other relations come into play, too, such as attribute-of and part-of-event (Hou, 2016). 3 Experimental Setup 4 Data We based our experiments on the benchmark dataset for coreference resolution, the OntoNotes corpus (Weischedel et al., 2011). For bridging, we used the ISNotes corpus, a small subset of OntoNotes annotated with information status (Markert et al., 2012). In order to obtain candidate pairs for semantic relation prediction, we considered all heads of noun phrases in the OntoNotes corpus (Weischedel et al., 2011) and combined them with preceding heads of noun phrases in the same document. Due to the different corpus sizes, the generally higher frequency of coreferent anaphors and the transitivity of the coreference relation, we obtained many more coreference pairs (65,113 unique pairs) than bridging pairs (633 in total, including 608 unique pairs). First Experiment 4.1 Semantic Relation Classification We used the publicly available relation res"
W18-4909,W16-4702,0,0.128874,"model for term class prediction is introduced. Section 5 explains the data annotation and evaluation. In Section 6, results are presented and discussed. We conclude in Section 7. 2 Related Work Approaches for automatic term identification can broadly be classified into four categories: linguistic (Justeson and Katz, 1995; Basili et al., 1997), statistical (Sch¨afer et al., 2015), hybrid (Frantzi et al., 1998; Maynard and Ananiadou, 1999) and machine learning approaches (Merley da Silva Conrado and Rezende, 2013). Recently, word vector and deep learning approaches (Zadeh and Handschuh, 2014b; Amjadian et al., 2016; Wang et al., 2016) have emerged. Addressing information about components for evaluation of complex terms (multiword or compound terms) has proven to be effective and was exploited in several termhood measures. The C-value method (Frantzi et al., 1998) is commonly used, which combines linguistic and statistical information and takes nested terms into account for evaluating termhood. The FGM score (Nakagawa and Mori, 2003) computes termhood by taking the geometric mean of the number of distinct left and right neighboring words for each component of a complex term. CSvH (Basili et al., 2001) is"
W18-4909,2014.amta-researchers.5,0,0.0261053,"ilable, what constitutes a term can often be taken as a given. However, difficult terms occur in other domains as well, and huge terminologies are not always available. In this work, we make a first attempt to combine the two tasks of automatic term recognition and term difficulty identification by reformulating them as one problem considering different tiers of termhood. In our proposal, we define four classes of termhood, which range from general-language words to obscure, domain-specific terms. This replaces the concept of termhood treated as a binary problem often seen in term annotation (Arcan et al., 2014; Bernier-Colborne and Drouin, 2014; Zadeh and Handschuh, 2014a) and identification tasks (Ventura et al., 2014; Riedl and Biemann, 2015). We demonstrate the effectiveness of our tier system by first conducting an annotation task, followed by an automatic classification using neural networks tailored for our task. In this work, we focus on the cooking domain and the German language. As a basis for the experiments, 400 German closed compounds are taken as term candidates. We focus on closed compounds for the following reasons: Closed compounds are complex expressions that consist of two or more"
W18-4909,W97-0314,0,0.4642,"Missing"
W18-4909,W15-3603,0,0.0257712,"tes termhood by taking the geometric mean of the number of distinct left and right neighboring words for each component of a complex term. CSvH (Basili et al., 2001) is a corpora-comparing measure that computes the termhood for a complex term by biasing the termhood score with the general-language frequency of the head. H¨atty et al. (2017) combine several termhood measures with a random forest classifier to extract single and multi-word terms and apply the measures recursively to the components. Meanwhile, component information has also been used for the related task of keyphrase extraction. Erbs et al. (2015) split German compounds to enhance individual term frequencies, leading to an improved keyphrase extraction. Zhang et al. (2016) propose a new joint-layer RNN architecture for both classifying keywords and keyphrases. The prediction of the keyword influences the prediction of the keyphrase. The task of detecting a lay reader’s familiarity with certain domain terms, i.e. the terms’ difficulty or understandability, is a comparably smaller research area, and a subtask of the more general areas of complex word identification and text readability assessment. It often involves steps of term substitu"
W18-4909,W14-4812,0,0.560574,".e. identifying boundaries of meaningful phrases, and then to evaluate its termhood, i.e. determining the degree of association to a specific domain (Kagueura and Umino, 1996). A related task to term recognition is the identification of domain terms unfamiliar to non-experts, i.e., identifying terms the average reader does not understand. This technology is mostly applied to the health domain, such as medical terms extracted to improve the communication between doctors and patients. The evaluated terms are mostly extracted from medical terminologies (Elhadad, 2006; Zeng-Treitler et al., 2008; Grabar and Hamon, 2014; Grabar et al., 2014). Since these terminologies are available, what constitutes a term can often be taken as a given. However, difficult terms occur in other domains as well, and huge terminologies are not always available. In this work, we make a first attempt to combine the two tasks of automatic term recognition and term difficulty identification by reformulating them as one problem considering different tiers of termhood. In our proposal, we define four classes of termhood, which range from general-language words to obscure, domain-specific terms. This replaces the concept of termhood tr"
W18-4909,W14-1202,0,0.481203,"es of meaningful phrases, and then to evaluate its termhood, i.e. determining the degree of association to a specific domain (Kagueura and Umino, 1996). A related task to term recognition is the identification of domain terms unfamiliar to non-experts, i.e., identifying terms the average reader does not understand. This technology is mostly applied to the health domain, such as medical terms extracted to improve the communication between doctors and patients. The evaluated terms are mostly extracted from medical terminologies (Elhadad, 2006; Zeng-Treitler et al., 2008; Grabar and Hamon, 2014; Grabar et al., 2014). Since these terminologies are available, what constitutes a term can often be taken as a given. However, difficult terms occur in other domains as well, and huge terminologies are not always available. In this work, we make a first attempt to combine the two tasks of automatic term recognition and term difficulty identification by reformulating them as one problem considering different tiers of termhood. In our proposal, we define four classes of termhood, which range from general-language words to obscure, domain-specific terms. This replaces the concept of termhood treated as a binary prob"
W18-4909,E17-4012,1,0.827935,"Missing"
W18-4909,D15-1290,0,0.0182307,"rminologies are not always available. In this work, we make a first attempt to combine the two tasks of automatic term recognition and term difficulty identification by reformulating them as one problem considering different tiers of termhood. In our proposal, we define four classes of termhood, which range from general-language words to obscure, domain-specific terms. This replaces the concept of termhood treated as a binary problem often seen in term annotation (Arcan et al., 2014; Bernier-Colborne and Drouin, 2014; Zadeh and Handschuh, 2014a) and identification tasks (Ventura et al., 2014; Riedl and Biemann, 2015). We demonstrate the effectiveness of our tier system by first conducting an annotation task, followed by an automatic classification using neural networks tailored for our task. In this work, we focus on the cooking domain and the German language. As a basis for the experiments, 400 German closed compounds are taken as term candidates. We focus on closed compounds for the following reasons: Closed compounds are complex expressions that consist of two or more simple words and contain no spaces or hyphens, e.g. Meeresfr¨uchte “seafood”. In German, closed compounds This work is licensed under a"
W18-4909,schmid-etal-2004-smor,0,0.0675009,"Missing"
W18-4909,2007.jeptalnrecital-poster.28,0,0.0601361,"ng the two tasks of automatic term identification and evaluating term difficulty. To achieve this, we propose one task comprising several fine-grained classes of termhood. The task of automatic term annotation and identification is typically implemented as a binary decision, i.e. recognizing terminology within non-specialized vocabulary. However, from a theoretical point of view, a term candidate can be associated to a domain to different degrees. This is reflected in the definition of termhood, and more explicitly in theories describing different kinds of terms (Trimble, 1985; Roelcke, 1999; Tutin, 2007). For example, the model by Roelcke (1999) separates terms into four tiers (Figure 1): intra-subject terminology which is specific to the domain, inter-subject terminology which is specific to this and other domains, extra-subject terminology which does not belong to the domain but is used within it and non-subject terminology which is shared across all specific domains. We aimed to create such a tier model with some variation due to the following considerations: • We want to keep it simple for the annotators, as it is not easy to compare the same term across different domains. We also see no"
W18-4909,U16-1011,0,0.141235,"rediction is introduced. Section 5 explains the data annotation and evaluation. In Section 6, results are presented and discussed. We conclude in Section 7. 2 Related Work Approaches for automatic term identification can broadly be classified into four categories: linguistic (Justeson and Katz, 1995; Basili et al., 1997), statistical (Sch¨afer et al., 2015), hybrid (Frantzi et al., 1998; Maynard and Ananiadou, 1999) and machine learning approaches (Merley da Silva Conrado and Rezende, 2013). Recently, word vector and deep learning approaches (Zadeh and Handschuh, 2014b; Amjadian et al., 2016; Wang et al., 2016) have emerged. Addressing information about components for evaluation of complex terms (multiword or compound terms) has proven to be effective and was exploited in several termhood measures. The C-value method (Frantzi et al., 1998) is commonly used, which combines linguistic and statistical information and takes nested terms into account for evaluating termhood. The FGM score (Nakagawa and Mori, 2003) computes termhood by taking the geometric mean of the number of distinct left and right neighboring words for each component of a complex term. CSvH (Basili et al., 2001) is a corpora-comparing"
W18-4909,W17-1722,0,0.253786,"Missing"
W18-4909,W14-4807,0,0.136905,"given. However, difficult terms occur in other domains as well, and huge terminologies are not always available. In this work, we make a first attempt to combine the two tasks of automatic term recognition and term difficulty identification by reformulating them as one problem considering different tiers of termhood. In our proposal, we define four classes of termhood, which range from general-language words to obscure, domain-specific terms. This replaces the concept of termhood treated as a binary problem often seen in term annotation (Arcan et al., 2014; Bernier-Colborne and Drouin, 2014; Zadeh and Handschuh, 2014a) and identification tasks (Ventura et al., 2014; Riedl and Biemann, 2015). We demonstrate the effectiveness of our tier system by first conducting an annotation task, followed by an automatic classification using neural networks tailored for our task. In this work, we focus on the cooking domain and the German language. As a basis for the experiments, 400 German closed compounds are taken as term candidates. We focus on closed compounds for the following reasons: Closed compounds are complex expressions that consist of two or more simple words and contain no spaces or hyphens, e.g. Meeresfr¨"
W18-4909,zadeh-handschuh-2014-evaluation,0,0.0793684,"given. However, difficult terms occur in other domains as well, and huge terminologies are not always available. In this work, we make a first attempt to combine the two tasks of automatic term recognition and term difficulty identification by reformulating them as one problem considering different tiers of termhood. In our proposal, we define four classes of termhood, which range from general-language words to obscure, domain-specific terms. This replaces the concept of termhood treated as a binary problem often seen in term annotation (Arcan et al., 2014; Bernier-Colborne and Drouin, 2014; Zadeh and Handschuh, 2014a) and identification tasks (Ventura et al., 2014; Riedl and Biemann, 2015). We demonstrate the effectiveness of our tier system by first conducting an annotation task, followed by an automatic classification using neural networks tailored for our task. In this work, we focus on the cooking domain and the German language. As a basis for the experiments, 400 German closed compounds are taken as term candidates. We focus on closed compounds for the following reasons: Closed compounds are complex expressions that consist of two or more simple words and contain no spaces or hyphens, e.g. Meeresfr¨"
W18-4909,D16-1080,0,0.0210162,"mplex term. CSvH (Basili et al., 2001) is a corpora-comparing measure that computes the termhood for a complex term by biasing the termhood score with the general-language frequency of the head. H¨atty et al. (2017) combine several termhood measures with a random forest classifier to extract single and multi-word terms and apply the measures recursively to the components. Meanwhile, component information has also been used for the related task of keyphrase extraction. Erbs et al. (2015) split German compounds to enhance individual term frequencies, leading to an improved keyphrase extraction. Zhang et al. (2016) propose a new joint-layer RNN architecture for both classifying keywords and keyphrases. The prediction of the keyword influences the prediction of the keyphrase. The task of detecting a lay reader’s familiarity with certain domain terms, i.e. the terms’ difficulty or understandability, is a comparably smaller research area, and a subtask of the more general areas of complex word identification and text readability assessment. It often involves steps of term substitution through simpler synonyms (Kandula et al., 2010) or providing an explanation (Elhadad, 2006). Notably, Fukushige and Noguchi"
W19-0506,N18-4002,1,0.877097,"Missing"
W19-0506,W17-7101,1,0.777437,"Missing"
W19-0506,W17-6910,1,0.737009,"ate about the nature of abstract concepts (Barsalou and Wiemer-Hastings, 2005; McRae and Jones, 2013; Hill et al., 2014; Vigliocco et al., 2014). Computational linguists have recognised the importance of investigating the concreteness of contexts in empirical models, for example for the automatic identification of non-literal language usage (Turney et al., 2011; K¨oper and Schulte im Walde, 2016; Aedmaa et al., 2018). Recently, multiple studies have focussed on providing a fine-grained analysis of the nature of concrete vs. abstract words from a corpus-based perspective (Bhaskar et al., 2017; Frassinelli et al., 2017; Naumann et al., 2018). In these studies, the authors have shown a general but consistent pattern: concrete words have a preference to co-occur with other concrete words, while abstract words co-occur more frequently with abstract words. Specifically, Naumann et al. (2018) performed their analyses across parts-of-speech by comparing the behaviour of nouns, verbs and adjectives in large-scale corpora. These results are not fully in line with various theories of cognition which suggest that both concrete and abstract words should co-occur more often with concrete words because concrete informat"
W19-0506,N16-1039,1,0.898118,"Missing"
W19-0506,S18-2008,1,0.195292,"stract concepts (Barsalou and Wiemer-Hastings, 2005; McRae and Jones, 2013; Hill et al., 2014; Vigliocco et al., 2014). Computational linguists have recognised the importance of investigating the concreteness of contexts in empirical models, for example for the automatic identification of non-literal language usage (Turney et al., 2011; K¨oper and Schulte im Walde, 2016; Aedmaa et al., 2018). Recently, multiple studies have focussed on providing a fine-grained analysis of the nature of concrete vs. abstract words from a corpus-based perspective (Bhaskar et al., 2017; Frassinelli et al., 2017; Naumann et al., 2018). In these studies, the authors have shown a general but consistent pattern: concrete words have a preference to co-occur with other concrete words, while abstract words co-occur more frequently with abstract words. Specifically, Naumann et al. (2018) performed their analyses across parts-of-speech by comparing the behaviour of nouns, verbs and adjectives in large-scale corpora. These results are not fully in line with various theories of cognition which suggest that both concrete and abstract words should co-occur more often with concrete words because concrete information links the real-worl"
W19-0506,schafer-bildhauer-2012-building,0,0.0317723,"Missing"
W19-0506,D11-1063,0,0.0381362,"tion of the usage of concrete and abstract words in communication is becoming salient both in cognitive science and in computational linguistics. In the cognitive science community, much has been said about concrete concepts, but there is still an open debate about the nature of abstract concepts (Barsalou and Wiemer-Hastings, 2005; McRae and Jones, 2013; Hill et al., 2014; Vigliocco et al., 2014). Computational linguists have recognised the importance of investigating the concreteness of contexts in empirical models, for example for the automatic identification of non-literal language usage (Turney et al., 2011; K¨oper and Schulte im Walde, 2016; Aedmaa et al., 2018). Recently, multiple studies have focussed on providing a fine-grained analysis of the nature of concrete vs. abstract words from a corpus-based perspective (Bhaskar et al., 2017; Frassinelli et al., 2017; Naumann et al., 2018). In these studies, the authors have shown a general but consistent pattern: concrete words have a preference to co-occur with other concrete words, while abstract words co-occur more frequently with abstract words. Specifically, Naumann et al. (2018) performed their analyses across parts-of-speech by comparing the"
W19-4803,D14-1162,0,0.0862252,"low-frequency words in the test data, we create a small corpus by randomly choosing 1M sentences and shuffling them. The final corpus contains roughly 10M tokens. Under these sparse conditions we expect to observe 4 Find the code under: https://github.com/ Garrafao/SecondOrder. 28 2017). Future work will look into the relationship between the second-order sensitivity of SVD and SGNS and their high performances across tasks. In addition, we aim to use the introduced method of generating artificial context overlap to see which higher orders of co-occurrence SVD, SGNS and other embedding types (Pennington et al., 2014; Peters et al., 2018; Athiwaratkun et al., 2018) capture. Because the aim of the study was only to test the second-order sensitivity of different models, we did not focus on finding the best way to provide this information. Given the results for PPMI, however, developing a smoother way to provide second-order information to models seems to be a promising starting point for further research. Figure 2: Results of experiment 2. Values give correlation (Spearman’s ρ) of model predictions with human similarity judgments. Discussion. The different reactions of PPMI vs. SVD and SGNS partly confirm o"
W19-4803,N18-1202,0,0.0469563,"he test data, we create a small corpus by randomly choosing 1M sentences and shuffling them. The final corpus contains roughly 10M tokens. Under these sparse conditions we expect to observe 4 Find the code under: https://github.com/ Garrafao/SecondOrder. 28 2017). Future work will look into the relationship between the second-order sensitivity of SVD and SGNS and their high performances across tasks. In addition, we aim to use the introduced method of generating artificial context overlap to see which higher orders of co-occurrence SVD, SGNS and other embedding types (Pennington et al., 2014; Peters et al., 2018; Athiwaratkun et al., 2018) capture. Because the aim of the study was only to test the second-order sensitivity of different models, we did not focus on finding the best way to provide this information. Given the results for PPMI, however, developing a smoother way to provide second-order information to models seems to be a promising starting point for further research. Figure 2: Results of experiment 2. Values give correlation (Spearman’s ρ) of model predictions with human similarity judgments. Discussion. The different reactions of PPMI vs. SVD and SGNS partly confirm our hypothesis which w"
W19-4803,islam-inkpen-2006-second,0,0.317393,"nd-order cooccurrence information, while Pointwise Mutual Information is agnostic to it. We support the results with an empirical study finding that the models react differently when provided with additional second-order information. Our findings reveal a basic property of Skip-Gram with Negative Sampling and point towards an explanation of its success on a variety of tasks. 1 Introduction The idea of second-order co-occurrence vectors was introduced by Sch¨utze (1998) for word sense discrimination and has since then been extended and applied to a variety of tasks (Lemaire and Denhiere, 2006; Islam and Inkpen, 2006; Schulte im Walde, 2010; Zhuang et al., 2018). The basic idea is to represent a word w not by a vector of the counts of context words it directly co-occurs with, but instead by a count vector of the context words of the context words, i.e., the second-order context words of w. These second-order vectors are supposed to be less sparse and more robust than firstorder vectors (Sch¨utze, 1998). Moreover, capturing second-order co-occurrence information can be seen as a way of generalization. To see this, cf. examples (1) and (2) inspired by Sch¨utze and Pedersen (1993). (1) As far as the Soviet C"
W19-4803,schulte-im-walde-2010-comparing,1,0.800384,"while Pointwise Mutual Information is agnostic to it. We support the results with an empirical study finding that the models react differently when provided with additional second-order information. Our findings reveal a basic property of Skip-Gram with Negative Sampling and point towards an explanation of its success on a variety of tasks. 1 Introduction The idea of second-order co-occurrence vectors was introduced by Sch¨utze (1998) for word sense discrimination and has since then been extended and applied to a variety of tasks (Lemaire and Denhiere, 2006; Islam and Inkpen, 2006; Schulte im Walde, 2010; Zhuang et al., 2018). The basic idea is to represent a word w not by a vector of the counts of context words it directly co-occurs with, but instead by a count vector of the context words of the context words, i.e., the second-order context words of w. These second-order vectors are supposed to be less sparse and more robust than firstorder vectors (Sch¨utze, 1998). Moreover, capturing second-order co-occurrence information can be seen as a way of generalization. To see this, cf. examples (1) and (2) inspired by Sch¨utze and Pedersen (1993). (1) As far as the Soviet Communist Party and the C"
W19-4803,J98-1004,0,0.936439,"Missing"
W19-4803,D18-1057,0,0.375606,"se Mutual Information is agnostic to it. We support the results with an empirical study finding that the models react differently when provided with additional second-order information. Our findings reveal a basic property of Skip-Gram with Negative Sampling and point towards an explanation of its success on a variety of tasks. 1 Introduction The idea of second-order co-occurrence vectors was introduced by Sch¨utze (1998) for word sense discrimination and has since then been extended and applied to a variety of tasks (Lemaire and Denhiere, 2006; Islam and Inkpen, 2006; Schulte im Walde, 2010; Zhuang et al., 2018). The basic idea is to represent a word w not by a vector of the counts of context words it directly co-occurs with, but instead by a count vector of the context words of the context words, i.e., the second-order context words of w. These second-order vectors are supposed to be less sparse and more robust than firstorder vectors (Sch¨utze, 1998). Moreover, capturing second-order co-occurrence information can be seen as a way of generalization. To see this, cf. examples (1) and (2) inspired by Sch¨utze and Pedersen (1993). (1) As far as the Soviet Communist Party and the Comintern were concerne"
W19-4803,Q15-1016,0,0.375607,"While most traditional count-based vector learning techniques such as raw count vectors or Point-wise Mutual Information (PPMI) do not capture second-order co-occurrence information, truncated Singular Value Decomposition (SVD) has been shown to do so. Regarding the more recently developed embeddings based on shallow neural networks, such as Skip-Gram with Negative Sampling (SGNS), it is presently unknown whether they capture higher-order co-occurrence information. So far, this question has been neglected as a research topic, although the answer is crucial to explain performance differences: Levy et al. (2015) show that SGNS performs similarly to SVD and differently from PPMI across semantic similarity data sets. If SGNS captures secondorder co-occurrence information, this provides a possible explanation for the observed performance differences. We examine this question in two experiments: (i) We create an artificial data set with target words displaying context overlap in different orders of co-occurrence and show that SGNS behaves similarly to SVD in capturing second-order co-occurrence information. The experiment supplies additional and striking evidence to prior We simulate first- and second-or"
wittmann-etal-2014-automatic,C04-1024,0,\N,Missing
wittmann-etal-2014-automatic,J07-2002,0,\N,Missing
wittmann-etal-2014-automatic,W09-0420,0,\N,Missing
wittmann-etal-2014-automatic,P01-1008,0,\N,Missing
wittmann-etal-2014-automatic,P02-1033,0,\N,Missing
wittmann-etal-2014-automatic,N03-1003,0,\N,Missing
wittmann-etal-2014-automatic,P05-1074,0,\N,Missing
wittmann-etal-2014-automatic,P06-2111,0,\N,Missing
wittmann-etal-2014-automatic,P05-1066,0,\N,Missing
wittmann-etal-2014-automatic,P98-2127,0,\N,Missing
wittmann-etal-2014-automatic,C98-2122,0,\N,Missing
wittmann-etal-2014-automatic,2006.amta-papers.3,0,\N,Missing
wittmann-etal-2014-automatic,schmid-etal-2004-smor,0,\N,Missing
