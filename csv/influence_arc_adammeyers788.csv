2021.nllp-1.16,W19-2208,0,0.0230828,"justments to the tool which improve its baseline precision for at least Supreme Court decisions. 2 2.1 Previous Work Legal Document Classification There has been some research on legal document classification using a range of methods such as traditional statistical models, neural networks, and state-of-the-art deep learning classifiers. Previous work includes a study on classifying Supreme Court legal opinions using convolutional neural networks (CNNs) and recurrent neural networks (RNNs) (Undavia et al., 2018). The most successful system in the paper was one that combined word2vec with CNNs. Howe et al. (2019) discusses the performance of different machine learning classifiers for Singapore Supreme Court Judgments. The paper compares state-of-the-art natural language processing methods and statistical models applied to legal documents. The authors found that traditional models outperform neural network classifiers on certain metrics, implying that there is still a need to optimize and improve such tools for legal documents. These papers serve as a motivation for our work because key terminology can be used as an additional feature to improve the training of legal document classifiers. We are consid"
2021.nllp-1.16,2020.computerm-1.12,0,0.0200621,"Brunsman) We focus on whether our extensions to the Termolator generate better results for legal documents • contains any digit (t-2-1) while assessing whether the scope of the issue (broad/narrow) and disparities in the numbers of • is not a noun group (quasi-suspect) Court cases affect the precision in legal terminology extraction. To further analyze the results, we 6 Named entities are sometimes included in terminology perform the experiment on the broad issue 05 - Pri- detection output and sometimes not. For example, Termeval vacy. This broad issue has 110 Court cases, which 2020 (Rigouts Terryn et al., 2020) calculated terminology deis of significantly smaller size compared to the is- tection scores both ways. Citations to laws or court decisions have a similar status. Citations, like terminology are characsues 01 and 08, but is within the same range as the teristic of particular subfields. However, they arguably have a most frequent narrow issues 10050 and 80010. separate status from terms. 157 Rank 0 327 1018 1070 1629 1665 3070 3735 3896 4792 Term derivative work ppg engelgau class certification atomic energy licensor job freeze quasi-suspect California tax howey test Evaluation 3 7 7 7 7 3 3"
C00-1078,W98-1115,0,0.0730048,"edure is trivial for a system if, given a context, one transfer rule can be selected unambiguously. Otherwise, choosing the best set of transfer rules may involve the evaluation of numerous competing sets. In fact, the number of possible transfer rule combinations increases exponentially with the length of the source language sentence. This situation mirrors the problem of choosing productions in a nondeterministic parser. In this paper, we describe a system for choosing transfer rules, based on statistical chart parsing (Bobrow, 1990; Chitrao and Grishman, 1990; Caraballo and Charniak, 1997; Charniak et al., 1998). In our Machine Translation system, transfer rules are generated automatically from parsed parallel text along the lines of (Matsumoto et al., 1993; Meyers et al., 1996; Meyers et al., 1998b). Our system tends to acquire a large number of transfer rules, due mainly to alternative ways of translating the same sequences of words, non-literal translations in parallel text and parsing errors. It is therefore crucial that our system choose the best set of rules eÆciently. While the technique discussed here obviously applies to similar such systems, it could also apply to hand-coded systems in whic"
C00-1078,H90-1053,1,0.636374,"anslation from a given source language sentence. This procedure is trivial for a system if, given a context, one transfer rule can be selected unambiguously. Otherwise, choosing the best set of transfer rules may involve the evaluation of numerous competing sets. In fact, the number of possible transfer rule combinations increases exponentially with the length of the source language sentence. This situation mirrors the problem of choosing productions in a nondeterministic parser. In this paper, we describe a system for choosing transfer rules, based on statistical chart parsing (Bobrow, 1990; Chitrao and Grishman, 1990; Caraballo and Charniak, 1997; Charniak et al., 1998). In our Machine Translation system, transfer rules are generated automatically from parsed parallel text along the lines of (Matsumoto et al., 1993; Meyers et al., 1996; Meyers et al., 1998b). Our system tends to acquire a large number of transfer rules, due mainly to alternative ways of translating the same sequences of words, non-literal translations in parallel text and parsing errors. It is therefore crucial that our system choose the best set of rules eÆciently. While the technique discussed here obviously applies to similar such syst"
C00-1078,P93-1004,0,0.684418,"es may involve the evaluation of numerous competing sets. In fact, the number of possible transfer rule combinations increases exponentially with the length of the source language sentence. This situation mirrors the problem of choosing productions in a nondeterministic parser. In this paper, we describe a system for choosing transfer rules, based on statistical chart parsing (Bobrow, 1990; Chitrao and Grishman, 1990; Caraballo and Charniak, 1997; Charniak et al., 1998). In our Machine Translation system, transfer rules are generated automatically from parsed parallel text along the lines of (Matsumoto et al., 1993; Meyers et al., 1996; Meyers et al., 1998b). Our system tends to acquire a large number of transfer rules, due mainly to alternative ways of translating the same sequences of words, non-literal translations in parallel text and parsing errors. It is therefore crucial that our system choose the best set of rules eÆciently. While the technique discussed here obviously applies to similar such systems, it could also apply to hand-coded systems in which each word or group of words is related to more than one transfer rule. For example, both Multra (Hein, 1996) and the Eurotra system described in ("
C00-1078,C96-1078,1,0.903804,"ation of numerous competing sets. In fact, the number of possible transfer rule combinations increases exponentially with the length of the source language sentence. This situation mirrors the problem of choosing productions in a nondeterministic parser. In this paper, we describe a system for choosing transfer rules, based on statistical chart parsing (Bobrow, 1990; Chitrao and Grishman, 1990; Caraballo and Charniak, 1997; Charniak et al., 1998). In our Machine Translation system, transfer rules are generated automatically from parsed parallel text along the lines of (Matsumoto et al., 1993; Meyers et al., 1996; Meyers et al., 1998b). Our system tends to acquire a large number of transfer rules, due mainly to alternative ways of translating the same sequences of words, non-literal translations in parallel text and parsing errors. It is therefore crucial that our system choose the best set of rules eÆciently. While the technique discussed here obviously applies to similar such systems, it could also apply to hand-coded systems in which each word or group of words is related to more than one transfer rule. For example, both Multra (Hein, 1996) and the Eurotra system described in (Way et al., 1997) req"
C00-1078,meyers-etal-1998-multilingual,1,0.907283,"peting sets. In fact, the number of possible transfer rule combinations increases exponentially with the length of the source language sentence. This situation mirrors the problem of choosing productions in a nondeterministic parser. In this paper, we describe a system for choosing transfer rules, based on statistical chart parsing (Bobrow, 1990; Chitrao and Grishman, 1990; Caraballo and Charniak, 1997; Charniak et al., 1998). In our Machine Translation system, transfer rules are generated automatically from parsed parallel text along the lines of (Matsumoto et al., 1993; Meyers et al., 1996; Meyers et al., 1998b). Our system tends to acquire a large number of transfer rules, due mainly to alternative ways of translating the same sequences of words, non-literal translations in parallel text and parsing errors. It is therefore crucial that our system choose the best set of rules eÆciently. While the technique discussed here obviously applies to similar such systems, it could also apply to hand-coded systems in which each word or group of words is related to more than one transfer rule. For example, both Multra (Hein, 1996) and the Eurotra system described in (Way et al., 1997) require components for d"
C00-1078,P98-2139,1,0.938603,"peting sets. In fact, the number of possible transfer rule combinations increases exponentially with the length of the source language sentence. This situation mirrors the problem of choosing productions in a nondeterministic parser. In this paper, we describe a system for choosing transfer rules, based on statistical chart parsing (Bobrow, 1990; Chitrao and Grishman, 1990; Caraballo and Charniak, 1997; Charniak et al., 1998). In our Machine Translation system, transfer rules are generated automatically from parsed parallel text along the lines of (Matsumoto et al., 1993; Meyers et al., 1996; Meyers et al., 1998b). Our system tends to acquire a large number of transfer rules, due mainly to alternative ways of translating the same sequences of words, non-literal translations in parallel text and parsing errors. It is therefore crucial that our system choose the best set of rules eÆciently. While the technique discussed here obviously applies to similar such systems, it could also apply to hand-coded systems in which each word or group of words is related to more than one transfer rule. For example, both Multra (Hein, 1996) and the Eurotra system described in (Way et al., 1997) require components for d"
C00-1078,J98-2004,0,\N,Missing
C00-1078,H91-1042,0,\N,Missing
C00-1078,J93-2003,0,\N,Missing
C00-1078,C98-2134,1,\N,Missing
C04-1109,P03-1028,0,0.0296085,"Missing"
C04-1109,W98-1105,0,0.379526,"Missing"
C04-1109,W01-1511,1,0.784239,"parser, but are not limited to these. Other general tools can also be included, which are not shown in the diagram. The triangles in the diagram are kernels that encode the corresponding syntactic processing result. In the training phase, the target slot fillers are labeled in the text so that SVM slot detectors can be trained through the kernels to find fillers for the key slots of events. In the testing phase, the SVM classifier will predict the slot fillers from unlabeled text and a merging procedure will merge slots into events if necessary. The main kernel we propose to use is on GLARF (Meyers et al., 2001) dependency graphs. SGML Parser Documents Texts Glarf Parser Sent Parser Slot Detector Name Tagger These kernels can be used alone or combined with each other using the properties of kernels. They can also be combined with high-order kernels like polynomial or RBF kernels, either individually or on the resulting kernel. As the depth of analysis of the preprocessing increases, the accuracy of the result decreases. Combining the results of deeper processing with those of shallower processing (such as n-grams) can also give us a back-off ability to recover from errors in deep processing. In pract"
C04-1109,M98-1009,0,0.0326447,"data for an IE system is often sparse since the target domain changes quickly. Traditional IE approaches try to generate patterns for events by various means using training data. For example, the FASTUS (Appelt et al., 1996) and Proteus (Grishman, 1996) systems, which performed well for MUC-6, used hand-written rules for event patterns. The symbolic learning systems, like AutoSlog (Riloff, 1993) and CRYSTAL (Fisher et al., 1996), generated patterns automatically from specific examples (text segments) using generalization and predefined pattern templates. There are also statistical approaches (Miller et al., 1998) (Collins et al., 1998) trying to encode event patterns in statistical CFG grammars. All of these approaches assume 2 2.1 Background Information Extraction The major task of IE is to find the elements of an event from text and combine them to form templates or populate databases. Most of these elements are named entities (NEs) involved in the event. To determine which entities in text are involved, we need to find reliable clues around each entity. The extraction procedure starts with 1 1. If K 1 ( x, y ) and K 2 ( x, y ) are kernels on X × Y , α , β > 0 , then αK 1 ( x, y ) + βK 2 ( x, y ) is"
C04-1109,M95-1014,1,\N,Missing
C04-1109,M95-1006,0,\N,Missing
C04-1109,M95-1011,0,\N,Missing
C94-1042,J93-2001,0,0.023363,"Missing"
C94-1042,J93-2002,0,0.505393,"Missing"
C94-1042,P91-1030,0,0.0281914,"Missing"
C94-1042,H93-1061,0,0.0129667,"Missing"
C94-1042,J81-4005,0,0.127209,"s (countable :pval (""wlth"")), indicating that it must appear in the singular with a deter,niner unless it is preceded by the preposZion ""with"". 2.1 Subcategorization We have paid p~u'ticular attention to providing detailed subcategorization information (information about complement structure), both for verbs and for tllose nouns and adjectives which do take cmnl)lements. In order to insure the COml)leteness of our codes, we studied the codiug e)ul)loyed by s(weral other u,ajor texicous, includh,g (,he Ih'andeis Verh Lexlcolt 2, the A(JQIJII,EX Prc,ject [10], the NYU Linguistic String l'roject [9], the OALI), and IA)OCI'], a,nd, whenever feasiMe, haw~ sought to incorporate distinctions made in a n y o f t h e s e all(tie,tortes. ()ur r e s u l t i n g feature systen, includes 92 subcategorization features Ibr w~rbs, 14 for adjectives, and 9 for llO,,ns. These features record dilforences in grammatical functional structure as well as constituent structure. In particular, tl,ey Calfl.ure four different types of control: subject control, object control, variable control, and arbitrary control. Furthermore, the notation allows us to indicate that verl) Irlay haw~ dill>rent control features"
C94-1042,P84-1044,0,0.075567,"r example, the noun ""abandon"" is marked as (countable :pval (""wlth"")), indicating that it must appear in the singular with a deter,niner unless it is preceded by the preposZion ""with"". 2.1 Subcategorization We have paid p~u'ticular attention to providing detailed subcategorization information (information about complement structure), both for verbs and for tllose nouns and adjectives which do take cmnl)lements. In order to insure the COml)leteness of our codes, we studied the codiug e)ul)loyed by s(weral other u,ajor texicous, includh,g (,he Ih'andeis Verh Lexlcolt 2, the A(JQIJII,EX Prc,ject [10], the NYU Linguistic String l'roject [9], the OALI), and IA)OCI'], a,nd, whenever feasiMe, haw~ sought to incorporate distinctions made in a n y o f t h e s e all(tie,tortes. ()ur r e s u l t i n g feature systen, includes 92 subcategorization features Ibr w~rbs, 14 for adjectives, and 9 for llO,,ns. These features record dilforences in grammatical functional structure as well as constituent structure. In particular, tl,ey Calfl.ure four different types of control: subject control, object control, variable control, and arbitrary control. Furthermore, the notation allows us to indicate that ver"
C94-1042,P93-1032,0,\N,Missing
C96-1078,C94-1015,0,\N,Missing
C96-1078,J93-2003,0,\N,Missing
C96-1078,C90-3044,0,\N,Missing
C96-1078,C90-3031,0,\N,Missing
C96-1078,C92-2101,0,\N,Missing
C96-1078,P85-1016,0,\N,Missing
C96-1078,P93-1004,0,\N,Missing
C96-1078,P85-1017,0,\N,Missing
C96-1078,P93-1002,0,\N,Missing
C96-1078,1992.tmi-1.23,1,\N,Missing
C98-2134,C90-3001,0,0.0674849,"Missing"
C98-2134,C94-1015,0,0.161477,"Missing"
C98-2134,C92-2101,0,0.764355,"Missing"
C98-2134,P93-1004,0,0.417268,"Missing"
C98-2134,C96-1078,1,0.927343,"Rules from Dominance-Preserving Alignments Adam Meyers, Roman Yangarber, Ralph Grishman, Catherine Macleod, Antonio Moreno-Sandoval* New York University 715 Broadway, 7th Floor, NY, NY 10003, USA tUniversidad Autdnoma de Madrid Cantoblanco, 28049-Madrid, SPAIN meyers/roman/grishman/macleod©cs, nyu. edu sandoval©lola.lllf.uam.es 1 Introduction Automatic acquisition of translation rules from parallel sentence-Migned text takes a variety of forms. Some machine translation (MT) svstems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Grishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. (Kaji et al., 1992), (Matsumoto et al., 1993), and (Kitamura and Matsumoto, 1995)). This has the advantage of acquiring structurM as well as lexical correspondences. A syntactically analyzed, aligned cor~ pus may serve ~s an example base for a form of example-based NIT (cf. (Sato and Nagao, 1990), (I{aji et al.. 1992), and (Furuse and Iida. 1994)). This paper I describes: (1) an efficient algorithm for aligning a pair of source/target language parse trees; ~nd (2) a procedure for deriving transfer rules from this a"
C98-2134,C90-3044,0,0.435501,"lation rules from parallel sentence-Migned text takes a variety of forms. Some machine translation (MT) svstems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Grishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. (Kaji et al., 1992), (Matsumoto et al., 1993), and (Kitamura and Matsumoto, 1995)). This has the advantage of acquiring structurM as well as lexical correspondences. A syntactically analyzed, aligned cor~ pus may serve ~s an example base for a form of example-based NIT (cf. (Sato and Nagao, 1990), (I{aji et al.. 1992), and (Furuse and Iida. 1994)). This paper I describes: (1) an efficient algorithm for aligning a pair of source/target language parse trees; ~nd (2) a procedure for deriving transfer rules from this alignment. Each transfer rule consists of a pair of tree fragments derived bv ""cutting up"" the source and target trees. A set of transfer rules whose left-hand sides match a source language parse tree is used to generate a target language parse tree from their set of right-hand sides, which is a transla.tion of the source tree. This technique resembles work on MT using synchr"
H94-1003,J93-2001,0,0.0607101,"Missing"
H94-1003,P91-1030,0,0.069877,"Missing"
H94-1003,H93-1061,0,0.0394644,"Missing"
H94-1003,J93-2002,0,\N,Missing
H94-1003,P93-1032,0,\N,Missing
he-meyers-2014-corpus,councill-etal-2008-parscit,0,\N,Missing
he-meyers-2014-corpus,P02-1062,0,\N,Missing
meyers-etal-1998-multilingual,J93-1004,0,\N,Missing
meyers-etal-1998-multilingual,W96-0201,0,\N,Missing
meyers-etal-1998-multilingual,C94-1015,0,\N,Missing
meyers-etal-1998-multilingual,C96-1030,0,\N,Missing
meyers-etal-1998-multilingual,J93-2003,0,\N,Missing
meyers-etal-1998-multilingual,C90-3044,0,\N,Missing
meyers-etal-1998-multilingual,C94-2175,0,\N,Missing
meyers-etal-1998-multilingual,C92-2101,0,\N,Missing
meyers-etal-1998-multilingual,P91-1022,0,\N,Missing
meyers-etal-1998-multilingual,P93-1004,0,\N,Missing
meyers-etal-1998-multilingual,J93-1006,0,\N,Missing
meyers-etal-1998-multilingual,C96-1078,1,\N,Missing
meyers-etal-1998-multilingual,J97-2004,0,\N,Missing
meyers-etal-1998-multilingual,P97-1039,0,\N,Missing
meyers-etal-1998-multilingual,P93-1002,0,\N,Missing
meyers-etal-1998-multilingual,P98-2139,1,\N,Missing
meyers-etal-1998-multilingual,C98-2134,1,\N,Missing
meyers-etal-1998-multilingual,P98-1117,0,\N,Missing
meyers-etal-1998-multilingual,C98-1113,0,\N,Missing
meyers-etal-1998-multilingual,1992.tmi-1.23,1,\N,Missing
meyers-etal-2002-formal,A00-2031,0,\N,Missing
meyers-etal-2002-formal,brants-plaehn-2000-interactive,0,\N,Missing
meyers-etal-2002-formal,W98-0604,1,\N,Missing
meyers-etal-2002-formal,W01-1511,1,\N,Missing
meyers-etal-2002-formal,C00-1041,0,\N,Missing
meyers-etal-2002-formal,H94-1020,0,\N,Missing
meyers-etal-2004-annotating,kingsbury-palmer-2002-treebank,0,\N,Missing
meyers-etal-2004-annotating,meyers-etal-2004-cross,1,\N,Missing
meyers-etal-2004-cross,kingsbury-palmer-2002-treebank,0,\N,Missing
meyers-etal-2004-cross,W01-1511,0,\N,Missing
meyers-etal-2004-cross,meyers-etal-2002-formal,1,\N,Missing
meyers-etal-2014-annotating,E12-1020,0,\N,Missing
meyers-etal-2014-annotating,W04-2703,0,\N,Missing
meyers-etal-2014-annotating,C92-2082,0,\N,Missing
meyers-etal-2014-annotating,J04-2002,0,\N,Missing
meyers-etal-2014-annotating,P98-1013,0,\N,Missing
meyers-etal-2014-annotating,C98-1013,0,\N,Missing
meyers-etal-2014-annotating,D09-1155,0,\N,Missing
meyers-etal-2014-annotating,P11-3015,0,\N,Missing
meyers-etal-2014-annotating,J05-1004,0,\N,Missing
meyers-etal-2014-annotating,R13-1060,1,\N,Missing
meyers-etal-2014-annotating,meyers-etal-2004-annotating,1,\N,Missing
meyers-etal-2014-annotating,boisen-etal-2000-annotating,0,\N,Missing
meyers-etal-2014-annotating,W11-0416,0,\N,Missing
N09-1017,P98-1013,0,0.034894,"Missing"
N09-1017,W05-0620,0,0.0474654,"Missing"
N09-1017,P05-1022,0,0.0642812,"Missing"
N09-1017,J02-3001,0,0.0901764,"licable to nominal argument structure. Early work in identifying the argument structure of deverbal nominalizations was primarily rulebased, using rule sets to associate syntactic constituents with semantic roles (Dahl et al., 1987; Hull and Gomez, 1996; Meyers et al., 1998). Lapata (2000) developed a statistical model to classify modifiers of deverbal nouns as underlying subjects or underlying objects, where subject and object denote the grammatical position of the modifier when linked to a verb. FrameNet and NomBank have facilitated machine learning approaches to nominal argument structure. Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates. Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothesis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to the task of NomBank SRL. These studies report argument F1 scores of 0.6914 and 0.7283, respectively. Both studies also investigated the use of features specific to the task of NomBank SRL, but observed only marginal performance gains. NomBank argument structure has also been used in the recent CoNLL Shared Task on Joint Parsing of Sy"
N09-1017,S07-1003,0,0.0937681,"Missing"
N09-1017,P07-1025,0,0.0647072,"ifier with additional new features. Second, we show that this model suffers a substantial performance degradation when evaluated over nominals with implicit arguments. Finally, we identify a set of features - many of them new - that can be used to reliably detect nominals with explicit arguments, thus significantly increasing the performance of the nominal SRL system. Our results also suggest interesting directions for future work. As described in section 5.2, many nominals do not have enough labeled training data to produce accurate argument models. The generalization procedures developed by Gordon and Swanson (2007) for PropBank SRL and Pad´o et al. (2008) for NomBank SRL might alleviate this problem. Additionally, instead of ignoring nominals with implicit arguments, we would prefer to identify the implicit arguments using information contained in the surrounding discourse. Such inferences would help connect entities and events across sentences, providing a fuller interpretation of the text. Acknowledgments The authors would like to thank the anonymous reviewers for their helpful suggestions. The first two authors were supported by NSF grants IIS-0535112 and IIS-0347548, and the third author was support"
N09-1017,W06-1617,0,0.444286,"imarily rulebased, using rule sets to associate syntactic constituents with semantic roles (Dahl et al., 1987; Hull and Gomez, 1996; Meyers et al., 1998). Lapata (2000) developed a statistical model to classify modifiers of deverbal nouns as underlying subjects or underlying objects, where subject and object denote the grammatical position of the modifier when linked to a verb. FrameNet and NomBank have facilitated machine learning approaches to nominal argument structure. Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates. Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothesis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to the task of NomBank SRL. These studies report argument F1 scores of 0.6914 and 0.7283, respectively. Both studies also investigated the use of features specific to the task of NomBank SRL, but observed only marginal performance gains. NomBank argument structure has also been used in the recent CoNLL Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (Surdeanu et al., 2008). In this task, systems were required to identify syntacti"
N09-1017,P07-1027,0,0.452477,"rule sets to associate syntactic constituents with semantic roles (Dahl et al., 1987; Hull and Gomez, 1996; Meyers et al., 1998). Lapata (2000) developed a statistical model to classify modifiers of deverbal nouns as underlying subjects or underlying objects, where subject and object denote the grammatical position of the modifier when linked to a verb. FrameNet and NomBank have facilitated machine learning approaches to nominal argument structure. Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates. Jiang and Ng (2006) and Liu and Ng (2007) have tested the hypothesis that methodologies and representations used in PropBank SRL (Pradhan et al., 2005) can be ported to the task of NomBank SRL. These studies report argument F1 scores of 0.6914 and 0.7283, respectively. Both studies also investigated the use of features specific to the task of NomBank SRL, but observed only marginal performance gains. NomBank argument structure has also been used in the recent CoNLL Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (Surdeanu et al., 2008). In this task, systems were required to identify syntactic dependencies, verbal"
N09-1017,C08-1084,0,0.111469,"Missing"
N09-1017,J05-1004,0,0.0548633,"Missing"
N09-1017,W08-2121,1,\N,Missing
N09-1017,N07-1070,0,\N,Missing
N09-1017,C98-1013,0,\N,Missing
P09-1048,P98-1013,0,0.0125282,"Missing"
P09-1048,W05-0620,0,0.0201316,"Missing"
P09-1048,P01-1017,0,0.0402229,"Missing"
P09-1048,W03-1006,0,0.020803,"Missing"
P09-1048,erk-pado-2006-shalmaneser,0,0.021266,"Missing"
P09-1048,J02-3001,0,0.10105,"Missing"
P09-1048,P02-1031,0,0.0346015,"Missing"
P09-1048,P07-1098,0,0.019224,"Missing"
P09-1048,N07-1051,0,0.0240934,"Missing"
P09-1048,C04-1127,1,0.822126,"Missing"
P09-1048,N04-1032,0,0.0335295,"Missing"
P09-1048,W05-0623,0,0.0539303,"Missing"
P09-1048,W04-3212,0,0.049316,"Missing"
P09-1048,W01-1511,1,0.848003,"Missing"
P09-1048,D07-1077,0,0.0941353,"Missing"
P09-1048,N10-1005,1,\N,Missing
P09-1048,C98-1013,0,\N,Missing
P93-1014,J92-2002,0,0.356812,"e correspondences across strata. Unfortunately, &quot;unchanged&apos; relations in a stratum must be explicitly &quot;carried over&quot; via path equations to the next stratum. Even worse, these &quot;carry over&quot; equations vary from case to case. SFG avoids this problem. INTRODUCTION STRATIFIED MAR Although the impact of relational grammar (RG) on theoretical linguistics has been substantial, it has never previously been put in a form suitable for computational use. RG&apos;s multiple syntactic strata would seem to preclude its use in the kind of monotonic, unification-based parsing system many now consider standard ([1], [11]). However, recent work by Johnson and Moss [2] on a Kasper-Rounds (KR) style logic-based formalism [5] for RG, called S t r a t i f i e d F e a t u r e G r a m m a r (S F G ) , has demonstrated that even RG&apos;s multiple strata are amenable to a feature-structure treatment. Based on this work, we have developed a unification-based, chart parser for a lexical version of SFG suitable for building computational relational grammars. A lexicalized SFG is simply a collection of s t r a t i f i e d f e a t u r e g r a p h s (Sgraphs), each of which is anchored to a lexical item, analogous to lexicalize"
P93-1014,E91-1006,0,0.0284143,"plete outgoing surface arcs; (ii) all surface arcs (here, only [C]) which must follow the [H] arc are incomplete; (iii) [0 1) can precede [H]; and (iv) there are no (incomplete) surface arcs which must occur between [0 1) and [H]. (We say can in (iii) because the parser accomodates variable word order.) The parser precedes to state-set $3. A right completion succeeds with Q~ = state 2:2 = [n~, 0, 2] and Q~ = state 3:1 = [n~,2,3]. State 3:2 [n~&apos;, 0, 3] is added to state set $3, n~&apos; = Unify-atThe algorithm is h e a d - d r i v e n [8] and was inspired by parsing algorithms for lexicalized TAGs ([6], [10]). Simplified Parsing Algorithm: I n p u t : A string of words W l , . . . , w~. O u t p u t : A chart containing all possible parses. Method: A. Initialization: 1. Create a list of k state-sets $ 1 , . . . , Sk, each empty. 2. For c = 1 , . . . , k , for each Graph(hi) of Wc, add [ni, c - 1, c] to Se. B. Completions: For c = 1 , . . . , k, do repeatedly until no more states can be added to Se: 1. L e f t w a r d Completion: For all = ¢] Se, Qj = [nj, Lj, L~] E SL,, such that Complete(nj ) and A E SSR-Out-Arcs(ni), such that Left-Precedence(A, hi) I F Unify-a~-end-of-Path(ni, nj, A ) n~,"
P93-1014,P85-1021,0,0.0271506,"e r m i n a l arc is an SSR arc, the target of which has no incomplete outgoing surface arcs; (ii) all surface arcs (here, only [C]) which must follow the [H] arc are incomplete; (iii) [0 1) can precede [H]; and (iv) there are no (incomplete) surface arcs which must occur between [0 1) and [H]. (We say can in (iii) because the parser accomodates variable word order.) The parser precedes to state-set $3. A right completion succeeds with Q~ = state 2:2 = [n~, 0, 2] and Q~ = state 3:1 = [n~,2,3]. State 3:2 [n~&apos;, 0, 3] is added to state set $3, n~&apos; = Unify-atThe algorithm is h e a d - d r i v e n [8] and was inspired by parsing algorithms for lexicalized TAGs ([6], [10]). Simplified Parsing Algorithm: I n p u t : A string of words W l , . . . , w~. O u t p u t : A chart containing all possible parses. Method: A. Initialization: 1. Create a list of k state-sets $ 1 , . . . , Sk, each empty. 2. For c = 1 , . . . , k , for each Graph(hi) of Wc, add [ni, c - 1, c] to Se. B. Completions: For c = 1 , . . . , k, do repeatedly until no more states can be added to Se: 1. L e f t w a r d Completion: For all = ¢] Se, Qj = [nj, Lj, L~] E SL,, such that Complete(nj ) and A E SSR-Out-Arcs(ni), such tha"
P93-1014,P87-1013,0,0.0354651,"developed within the s t r a t i f i e d f e a t u r e g r a m m a r (SFG) framework, which generalizes Kasper-Rounds logic to handle relational grammar analyses. We first introduce the key aspects of SFG and a lexicalized, graph-based variant of the framework suitable for implementing relational grammars. We then describe a head-driven chart parser for lexicalized SFG. The basic parsing operation is essentially ordinary feature-structure unification augmented with an operation of label unification to build the stratified features characteristic of SFG. RELATED WORK Rounds and Manaster-Ramer [9] suggested encoding multiple strata in terms of a &quot;level&quot; attribute, using path equations to state correspondences across strata. Unfortunately, &quot;unchanged&apos; relations in a stratum must be explicitly &quot;carried over&quot; via path equations to the next stratum. Even worse, these &quot;carry over&quot; equations vary from case to case. SFG avoids this problem. INTRODUCTION STRATIFIED MAR Although the impact of relational grammar (RG) on theoretical linguistics has been substantial, it has never previously been put in a form suitable for computational use. RG&apos;s multiple syntactic strata would seem to preclude its"
P98-2139,C90-3001,0,0.0745159,"Missing"
P98-2139,C94-1015,0,0.151793,"Missing"
P98-2139,C92-2101,0,0.758007,"Missing"
P98-2139,P93-1004,0,0.398705,"Missing"
P98-2139,C96-1078,1,0.924922,"erving Alignments Adam Meyers, Roman Yangarber, Ralph Grishman, Catherine Macleod, Antonio Moreno-Sandoval t New York U n i v e r s i t y 715 Broadway, 7th Floor, NY, NY 10003, USA tUniversidad A u t 6 n o m a de M a d r i d Cantoblanco, 28049-Madrid, SPAIN meyers/roman/grishman/macleod©cs, nyu. edu sandoval©lola, lllf. uam. es 1 Introduction Automatic acquisition of translation rules from parallel sentence-aligned text takes a variety of forms. Some machine translation (MT) systems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Grishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. (Kaji et hi., 1992), (Matsumoto et hi., 1993), and (Kitamura and Matsumoto, 1995)). This has the advantage of acquiring structural as well as lexical correspondences. A syntactically analyzed, aligned corpus may serve as an example base for a form of example-based NIT (cf. (Sato and Nagao, 1990), (l(aji et al., 1992), and (Furuse and Iida. 1994)). This paper 1 describes: (1) an efficient algorithm for aligning a pair of source/target language parse trees; and (9) a procedure for deriving transfer rules from this al"
P98-2139,C90-3044,0,0.413629,"lation rules from parallel sentence-aligned text takes a variety of forms. Some machine translation (MT) systems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Grishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. (Kaji et hi., 1992), (Matsumoto et hi., 1993), and (Kitamura and Matsumoto, 1995)). This has the advantage of acquiring structural as well as lexical correspondences. A syntactically analyzed, aligned corpus may serve as an example base for a form of example-based NIT (cf. (Sato and Nagao, 1990), (l(aji et al., 1992), and (Furuse and Iida. 1994)). This paper 1 describes: (1) an efficient algorithm for aligning a pair of source/target language parse trees; and (9) a procedure for deriving transfer rules from this alignment. Each transfer rule consists of a pair of tree fragments derived by &quot;cutting up&quot; the source and target trees. A set of transfer rules whose left-hand sides match a source language parse tree is used to generate a target language parse tree from their set of right-hand sides, which is a translation of the source tree. This technique resembles work on NIT using synchr"
R13-1052,W06-1613,0,0.324414,"t belonging to other functions The weakness of the cited work is discussed Table 1: Annotation Scheme for Citation Function: + represents POSITIVE sentiment, NEUTRAL sentiment, and − represents negative sentiment = represents citations. Dong and Sch¨afer (2011) proposed a four-category definition of citation functions following Moravcsik and Murugesan (1975) and a self-training-based classification model. Different from previous work that mainly classified citations into sentiment categories or coarse-grained functions, our scheme, we believe, is more finegrained. It is also worth noting that Teufel et al. (2006a), Athar (2011), and Dong and Sch¨afer (2011) all worked on citations in computational linguistics papers, but we investigate citations in biomedical articles. Researchers have introduced several annotation schemes for citation analysis. The work of Teufel et al. (2006b) is the most related to ours. They proposed an annotation scheme for citation functions based on why authors cite a particular paper, following Spiegel-R¨using (1977). This scheme provides clear definition for some of the basic citation functions, such as Contrast, but mainly concerns the citations that authors compare to or b"
R13-1052,W12-4303,0,0.0320289,"Missing"
R13-1052,W06-1312,0,0.0978912,"t belonging to other functions The weakness of the cited work is discussed Table 1: Annotation Scheme for Citation Function: + represents POSITIVE sentiment, NEUTRAL sentiment, and − represents negative sentiment = represents citations. Dong and Sch¨afer (2011) proposed a four-category definition of citation functions following Moravcsik and Murugesan (1975) and a self-training-based classification model. Different from previous work that mainly classified citations into sentiment categories or coarse-grained functions, our scheme, we believe, is more finegrained. It is also worth noting that Teufel et al. (2006a), Athar (2011), and Dong and Sch¨afer (2011) all worked on citations in computational linguistics papers, but we investigate citations in biomedical articles. Researchers have introduced several annotation schemes for citation analysis. The work of Teufel et al. (2006b) is the most related to ours. They proposed an annotation scheme for citation functions based on why authors cite a particular paper, following Spiegel-R¨using (1977). This scheme provides clear definition for some of the basic citation functions, such as Contrast, but mainly concerns the citations that authors compare to or b"
R13-1052,N12-1073,0,0.24777,"Missing"
R13-1052,J96-2004,0,0.272417,"Missing"
R13-1052,I11-1070,0,0.0483684,"Missing"
R13-1052,N09-1066,0,0.0180753,"sis systems are usually able to identify positive, neutral, or negative opinions, but if we want to better understand the exact function of a citation, we need to 2 Related Work The background for our work is in citation analysis. Applications of citation analysis include evaluating the impact of a published literature through a measurable bibliometric (Garfield, 1972; Luukkonen, 1992; Borgman and Furner, 2002), analyzing bibliometric networks (Radev et al., 2009), summarizing scientific papers (Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011), generating surveys of scientific paradigms (Mohammad et al., 2009), among others. Correctly and accurately recognizing citation functions is a cornerstone for these tasks. 402 Proceedings of Recent Advances in Natural Language Processing, pages 402–407, Hissar, Bulgaria, 7-13 September 2013. Citation Function Based on+ Corroboration+ Discover+ Positive+ Practical+ Significant+ Standard+ Supply+ Contrast= Co-citation= Neutral= Negative− Description A work is based on the cited work Two works corroborate each other Acknowledge the invention of a technique The cited work is successful The cited work has a practical use The cited work is important The cited work"
R13-1052,C08-1087,0,0.0202032,"ent, which has achieved good accuracy (see, e.g., (Teufel et al., 2006a)). Citation sentiment analysis systems are usually able to identify positive, neutral, or negative opinions, but if we want to better understand the exact function of a citation, we need to 2 Related Work The background for our work is in citation analysis. Applications of citation analysis include evaluating the impact of a published literature through a measurable bibliometric (Garfield, 1972; Luukkonen, 1992; Borgman and Furner, 2002), analyzing bibliometric networks (Radev et al., 2009), summarizing scientific papers (Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011), generating surveys of scientific paradigms (Mohammad et al., 2009), among others. Correctly and accurately recognizing citation functions is a cornerstone for these tasks. 402 Proceedings of Recent Advances in Natural Language Processing, pages 402–407, Hissar, Bulgaria, 7-13 September 2013. Citation Function Based on+ Corroboration+ Discover+ Positive+ Practical+ Significant+ Standard+ Supply+ Contrast= Co-citation= Neutral= Negative− Description A work is based on the cited work Two works corroborate each other Acknowledge the invention of a technique The cited"
R13-1052,P11-3015,0,\N,Missing
R13-1052,P11-1051,0,\N,Missing
R13-1060,N12-1009,0,0.039395,"Missing"
R13-1060,W12-4303,0,0.126521,"luB viruses have a common origin [3]. Thus, it is expected that aa residues of PA are conserved between FluA and FluB [4]. Introduction The citation of publications has been used to measure the impact of authors, publications, publishers, fields of study, etc., as represented in graphs in which nodes represent documents and edges connect documents if one refers to the other (citation graphs) or if a third document refers to both (cocitation graphs). NLP may be used to supplement these graphs with information about why documents are cited. While previous work (Teufel et al., 2009; Athar, 2011; Athar and Teufel, 2012) record positive and negative sentiment about cited work, we record information about how cited documents are compared to each other. CONTRASTing documents may describe different approaches or different opinions. Documents which CORROBORATE each other may follow a single approach. A document that is cited as corroborating with many other documents may be very salient. Some example instances of these CONTRAST/CORROBORATE relations are provided in Figure 1, the document containing the citations (represented as we or this study) contrasts with [1] and corroborates [2]; [3] and [4] contrast with e"
R13-1060,P11-3015,0,0.0517189,"3. FluA and FluB viruses have a common origin [3]. Thus, it is expected that aa residues of PA are conserved between FluA and FluB [4]. Introduction The citation of publications has been used to measure the impact of authors, publications, publishers, fields of study, etc., as represented in graphs in which nodes represent documents and edges connect documents if one refers to the other (citation graphs) or if a third document refers to both (cocitation graphs). NLP may be used to supplement these graphs with information about why documents are cited. While previous work (Teufel et al., 2009; Athar, 2011; Athar and Teufel, 2012) record positive and negative sentiment about cited work, we record information about how cited documents are compared to each other. CONTRASTing documents may describe different approaches or different opinions. Documents which CORROBORATE each other may follow a single approach. A document that is cited as corroborating with many other documents may be very salient. Some example instances of these CONTRAST/CORROBORATE relations are provided in Figure 1, the document containing the citations (represented as we or this study) contrasts with [1] and corroborates [2]; [3"
R13-1060,W04-2703,0,0.234535,"sented at the parent node can correspond to a citation relation. The sentence “[1] contrasts with [2] regarding whether X is or is not true” is analogous to the sentence “X is often claimed [1]. In contrast, others claim not X [2]” because the subject and object of the verb contrast correspond to the discourse arguments of in contrast. Taking this approach, we assume that discourse units and grammatical arguments are the referential scopes of citations. Furthermore, we limit our attention to citations scopes that are no more than a few sentences long (as in the Penn Discourse Treebank (PDTB) (Miltsakaki et al., 2004)). We assume that: (1) there is a grammatical or discourse relation corresponding to each CORROBORATE and CONTRAST citation relation; (2) each such grammatical/discourse relation takes 2 arguments, each argument being a sequence of sentences, a sentence, a phrase or a word; and (3) more than one citation can be associated with each argument. Given these assumptions we seek to identify: (a) the candidate grammatical/discourse relation and its arguments; and (b) the sets of citations that correspond to these arguments. We then hypothesize the corresponding citation relation for each ARG1/ARG2 pa"
R13-1060,D09-1155,0,0.0992426,"emisphere invasions. 3. FluA and FluB viruses have a common origin [3]. Thus, it is expected that aa residues of PA are conserved between FluA and FluB [4]. Introduction The citation of publications has been used to measure the impact of authors, publications, publishers, fields of study, etc., as represented in graphs in which nodes represent documents and edges connect documents if one refers to the other (citation graphs) or if a third document refers to both (cocitation graphs). NLP may be used to supplement these graphs with information about why documents are cited. While previous work (Teufel et al., 2009; Athar, 2011; Athar and Teufel, 2012) record positive and negative sentiment about cited work, we record information about how cited documents are compared to each other. CONTRASTing documents may describe different approaches or different opinions. Documents which CORROBORATE each other may follow a single approach. A document that is cited as corroborating with many other documents may be very salient. Some example instances of these CONTRAST/CORROBORATE relations are provided in Figure 1, the document containing the citations (represented as we or this study) contrasts with [1] and corrobo"
W04-0413,W04-2705,1,\N,Missing
W04-0413,J98-2001,0,\N,Missing
W04-0413,meyers-etal-2004-annotating,1,\N,Missing
W04-2701,W04-2702,0,0.0711184,"Missing"
W04-2701,W04-2704,0,0.0287577,"t of systems that produce the same form of output, e.g., Penn Treebank-based parsers can be compared in terms of how well they reproduce the Penn Treebank. Alternative systems based on a standard are largely interchangeable. Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of “deeper” structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; Meyers et al., 2004). It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc´antara and Moreno, 2004), English (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; Meyers et al., 2004), Czech (Sgall et al., 2004) and German(Baumann et al., 2004). The sixth, seventh and eighth"
W04-2701,W04-2707,0,0.0468963,"Missing"
W04-2701,W04-2708,0,0.0390962,"Missing"
W04-2701,W04-2709,0,0.0287625,"tages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc´antara and Moreno, 2004), English (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; Meyers et al., 2004), Czech (Sgall et al., 2004) and German(Baumann et al., 2004). The sixth, seventh and eighth papers (Baumann et al., 2004; C¸mejrek et al., 2004; Helmreich et al., 2004) explore questions of multilingual annotation of syntax and semantics, beginning to answer the question of how annotation systems can be made compatible across languages. Indeed (Helmreich et al., 2004) explores the question of integration across languages, as well as levels of annotation. (Baumann et al., 2004) also describes how a number of different linguistic levels can be related in annotation (pragmatic and prosodic) among two languages (English and German). The ninth and tenth papers (Langone et al., 2004; ˇ Zabokrtsk´ y and Lopatkov´a, 2004) are respectively about a corpus related to a"
W04-2701,W04-2705,1,0.747785,"he same form of output, e.g., Penn Treebank-based parsers can be compared in terms of how well they reproduce the Penn Treebank. Alternative systems based on a standard are largely interchangeable. Thus a system that uses one PennTreebank-based parser as a component can easily be adapted to use another better performing PennTreebank-based parser. Standards can be built on. For example, if one accepts the framework of the Penn Treebank, it is easy to move on to representations of “deeper” structure as suggested in three papers in this volume (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; Meyers et al., 2004). It is my view that these advantages outweigh the disadvantages. I propose that the papers in this volume be viewed with the following question in mind: How can the work covered by this collection of papers be integrated together? Put differently, to what extent are these resources mergeable? The first six papers describe linguistic annotation in four languages: Spanish (Alc´antara and Moreno, 2004), English (Miltsakaki et al., 2004; Babko-Malaya et al., 2004; Meyers et al., 2004), Czech (Sgall et al., 2004) and German(Baumann et al., 2004). The sixth, seventh and eighth papers (Baumann et al"
W04-2701,W04-2703,0,0.069906,"Missing"
W04-2701,W04-2706,0,0.0641247,"Missing"
W04-2701,W04-2711,0,\N,Missing
W04-2705,J02-3001,0,0.302593,"rting a new phase in this project: the creation of an automatic annotator. Using techniques similar to those described in (Meyers et al., 1998) in combination with our work on GLARF (Meyers et al., 2001a; Meyers et al., 2001b), we expect to build a hand-coded PROPBANKER a program designed to produce a PropBank/NomBank style analysis from Penn Treebank style input. Although the PropBanker should work with input in the form of either treebank annotation or treebankbased parser output, this project only requires application to the Penn Treebank itself. While previous programs with similar goals (Gildea and Jurafsky, 2002) were statistics-based, this tool will be based completely on hand-coded rules and lexical resources. Depending on its accuracy, automatically produced annotation should be useful as either a preprocessor or as an error detector. We expect high precision for very simple frames, e.g., nouns like lot as in figure 10. Annotators will have the opportunity to judge whether particular automatic annotation is “good enough” to serve as a preprocessor. We hypothesize that a comparison of automatic annotation that fails this level of accuracy against the hand annotation will still be useful for detectin"
W04-2705,W98-0604,1,0.600273,"Missing"
W04-2705,W01-1511,1,0.541016,"hould not appear more than once (the stratal uniqueness condition in Relational Grammar or the theta criterion in Principles and parameters, etc.). Furthermore, it is unlikely for the first word of a sentence to be an argument unless the main predicate is nearby (within three words) or unless there is a nearby support verb. Finally, it is unlikely that there is an empty category that is an argument of a predicate noun unless the empty category is linked to some real NP.8 WRONG-POS We use procedures that are part of our systems for generating GLARF, a predicate argument framework discussed in (Meyers et al., 2001a; Meyers et al., 2001b), to detect incorrect parts of speech in the Penn Treebank. If an instance is predicted to be a part of speech other than a common noun, but it is still tagged, that instance is flagged. For example, if a word tagged as a singular common noun is the first word in a VP, it is probably tagged with the wrong part of speech. 6.4 The Results of Error Detection The processes described in the previous subsections are used to create a list of annotation instances to check along with short standardized descriptions of what was wrong, e.g., wrong-pos, non-functional (if there wer"
W04-2705,meyers-etal-2004-cross,1,0.7174,"n the corpus. We needed to know which nouns were markable and make initial approximations of the inventories of senses and arguments for each noun. Toward this end, we pooled a number of resources: COMLEX Syntax (Macleod et al., 1998a), NOMLEX (Macleod et al., 1998b) and the verb classes from (Levin, 1993). We also used string matching techniques and hand classification in combination with programs that automatically merge crucial features of these resources. The result was NOMLEX-PLUS, a NOMLEX-style dictionary, which includes the original 1000 entries in NOMLEX plus 6000 additional entries (Meyers et al., 2004). The resulting noun classes include verbal nominalizations (e.g., destruction, knowledge, believer, recipient), adjectival nominalizations (ability, bitterness), and 16 other classes such as relational (father, president) and partitive nouns (set, variety). NOMLEX-PLUS helped us break down 3 To make our examples more readable, we have replaced pointers to the corpus with the corresponding strings of words. 4 For a particular noun instance, only a subset of these arguments may appear, e.g., the ARG2 (indirect object) to Dorothy can be left out of the phrase Glinda’s gift of the slippers. the n"
W04-2705,2003.mtsummit-systems.9,0,0.0102805,"icular nominal predicates, adjunct prenominal modifiers usually behave the same way regardless of the noun with which they occur. In order to identify these lexical properties of prenominals, we created a list of all time nouns from COMLEX Syntax (ntime1 and ntime2) and we created a specialized dictionary of adjectives with adverbial properties which we call ADJADV. The list of adjective/adverb pairs in ADJADV came from two sources: (1) a list of adjectives that are morphologically linked to -ly adverbs created using some string matching techniques; and (2) adjective/adverb pairs from CATVAR (Habash and Dorr, 2003). We pruned this list to only include adjectives found in the Penn Treebank and then edited out inappropriate word pairs. We completed the dictionary by transferring portions of the COMLEX Syntax adverb entries to the corresponding adjectives. We now use ADJADV and our list of temporal nouns to evaluate NOMBANK annotation of modifiers. Each annotated left modifier is compared against our dictionaries. If a modifier is a temporal noun, it can bear the ARGM-TMP role (temporal adjunct role), e.g., the temporal noun morning can fill the ARGM-TMP slot in the morning broadcast. Most other common nou"
W04-2705,J98-2001,0,0.00495354,"eatures of Support be implied by discourse processes, but which we do not mark (as we are only handling sentence-level phenomena). For example, the words proponent and rival strongly imply that certain arguments appear in the discourse, but not necessarily in the same sentence. For example in They didn’t want the company to fall into the hands of a rival, there is an implication that the company is an ARG1 of rival, i.e., a rival should be interpreted as a rival of the company.7 The connection between a rival and the company is called a “bridging” relation (a process akin to coreference, cf. (Poesio and Vieira, 1998)) In other words, fall into the hands of does not link “rival” with the company by means of SUPPORT. The fact that a discourse relation is responsible for this connection becomes evident when you see that the link between rival and company can cross sentence boundaries, e.g., The company was losing money. This was because a rival had come up with a really clever marketing strategy. 6.2 Prenominal Adjectives and Error Detection ARGM is the annotation tag used for nonarguments, also known as adjuncts. For nouns, it was decided to only tag such types of adjuncts as are also found with verbs, e.g."
W04-2705,kingsbury-palmer-2002-treebank,0,0.0246365,"sylvania’s PropBank, NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text. This paper describes the NomBank project in detail including its specifications and the process involved in creating the resource. 1 Introduction This paper introduces the NomBank project. When complete, NomBank will provide argument structure for instances of about 5000 common nouns in the Penn Treebank II corpus. NomBank is part of a larger effort to add layers of annotation to the Penn Treebank II corpus. PropBank (Kingsbury et al., 2002; Kingsbury and Palmer, 2002; University of Pennsylvania, 2002), NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text. These annotation projects may be viewed as part of what we think of as an a la carte strategy for corpus-based natural language processing. The fragile and inaccurate multistage parsers of a few decades were replaced by treebank-based parsers, which had better performance, but typically provided more shallow analyses.1 As the same set of data is annotated with more and more levels of annotation, a new type of multistage proces"
W05-0301,P98-1013,0,0.0313744,"Mozart would require Mozart to be decomposed into music by Mozart (although arguably the representation of some of the complex discourse references were of this flavor). 3 What’s in the Latest Pie in the Sky Analysis As of this writing, the latest “Pie in the Sky” analysis includes: (1) argument structure of all parts of speech (verbs, nouns, adjectives, determiners, conjunctions, etc.) using the PropBank/NomBank/Discourse Treebank argument labels (ARG0, ARG1, ARG2,  ), reminiscent of Relational Grammar of the 1970s and 1980s (Perlmutter, 1984), (2) some more specifically labeled FrameNet (Baker et al., 1998) roles for these same constituents; (3) morphological and part of speech features; (4) pointers to gazetteers, both real and hypothetical (thanks to B. Sundheim); (5) Veracity/According-To features based on NYU’s proposed FactBank annotation scheme; (6) various coreference features including some based on a proposed extension to NomBank; (7) temporal features based on Timex2 (Ferro et al., 2002) and TimeML (Pustejovsky et al., 2004); and (8) Information Structure features based on (Calhoun et al., 2005). For more detail, please see: http://nlp.cs.nyu.edu/meyers/pie-in-thesky.html 4 The Future"
W05-0301,W05-0304,0,0.125602,"t aimed to produce a single unified representation that goes beyond what may currently be feasible to annotate consistently or to generate automatically. Rather this “pie in the sky” annotation effort was an attempt at defining a future goal for semantic analysis. We decided to use the “Pie in the Sky” annotation effort (http://nlp.cs.nyu.edu/meyers/piein-the-sky.html) as a theme for this year’s workshop. Consequently this theme has been brought out in many of the papers contained in this volume. The first 4 papers (Pustejovsky et al., 2005; E. W. Hinrichs and S. K¨ubler and K. Naumann, 2005; Bies et al., 2005; Dinesh et al., 2005) all discuss some aspect of merging annotation. (Pustejovsky et al., 2005) describes issues that arise for merging argument structures for verbs, nouns and discourse connectives, as well as time and anaphora representations. (E. W. Hinrichs and S. K¨ubler and K. Naumann, 2005) focuses on the merging of syntactic, morphological, semantic and referential annotation. (E. W. Hinrichs and S. K¨ubler and K. Naumann, 2005) also points out that the “Pie in the Sky”representation lacks syntactic features. This brings to light an important point of discussion: should linguistic ana"
W05-0301,W05-0307,0,0.0581524,"sses a conflict between discourse structure and syntactic structure. I think it is reasonable to assume that some such conflicts will be resolvable, e.g., I believe that the named entity conflicts point to shortcomings of the original Penn Treebank analysis. However, the discourse structure/syntactic structure conflicts may be harder to solve. In fact, some annotation projects, e.g., the Prague Dependency Treebank (Hajiˆcov´a and Ceplov´a, 2000), assume that multiple analyses or “levels” are necessary to describe the full range of phenomena. The 5th through 7th papers (Inui and Okumura, 2005; Calhoun et al., 2005; Wilson and Wiebe, 2005) investigate some additional types of annotation that were not part of the distributed version of Pie in the Sky, but which could be added in principle. In fact, with help from the authors of (Calhoun et al., 2005), I did incorporate their analysis into the latest version (number 6) of the“Pie in the Sky” annotation. Furthermore, it turns out that some units of Information Structure cross the boundaries of the syntactic/semantic constituents, thus raising the sort of difficulties discussed in the previous paragraph. Specifically, information structure divides sentences"
W05-0301,W05-0305,0,0.06824,"a single unified representation that goes beyond what may currently be feasible to annotate consistently or to generate automatically. Rather this “pie in the sky” annotation effort was an attempt at defining a future goal for semantic analysis. We decided to use the “Pie in the Sky” annotation effort (http://nlp.cs.nyu.edu/meyers/piein-the-sky.html) as a theme for this year’s workshop. Consequently this theme has been brought out in many of the papers contained in this volume. The first 4 papers (Pustejovsky et al., 2005; E. W. Hinrichs and S. K¨ubler and K. Naumann, 2005; Bies et al., 2005; Dinesh et al., 2005) all discuss some aspect of merging annotation. (Pustejovsky et al., 2005) describes issues that arise for merging argument structures for verbs, nouns and discourse connectives, as well as time and anaphora representations. (E. W. Hinrichs and S. K¨ubler and K. Naumann, 2005) focuses on the merging of syntactic, morphological, semantic and referential annotation. (E. W. Hinrichs and S. K¨ubler and K. Naumann, 2005) also points out that the “Pie in the Sky”representation lacks syntactic features. This brings to light an important point of discussion: should linguistic analyses be divided out i"
W05-0301,W05-0303,0,0.0339493,"Missing"
W05-0301,C00-1041,0,0.0353612,"Missing"
W05-0301,W05-0306,0,0.0200392,"nesh et al., 2005) discusses a conflict between discourse structure and syntactic structure. I think it is reasonable to assume that some such conflicts will be resolvable, e.g., I believe that the named entity conflicts point to shortcomings of the original Penn Treebank analysis. However, the discourse structure/syntactic structure conflicts may be harder to solve. In fact, some annotation projects, e.g., the Prague Dependency Treebank (Hajiˆcov´a and Ceplov´a, 2000), assume that multiple analyses or “levels” are necessary to describe the full range of phenomena. The 5th through 7th papers (Inui and Okumura, 2005; Calhoun et al., 2005; Wilson and Wiebe, 2005) investigate some additional types of annotation that were not part of the distributed version of Pie in the Sky, but which could be added in principle. In fact, with help from the authors of (Calhoun et al., 2005), I did incorporate their analysis into the latest version (number 6) of the“Pie in the Sky” annotation. Furthermore, it turns out that some units of Information Structure cross the boundaries of the syntactic/semantic constituents, thus raising the sort of difficulties discussed in the previous paragraph. Specifically, information struc"
W05-0301,W05-0310,0,0.0260259,"Missing"
W05-0301,W05-0309,0,0.01191,"., 2005), I did incorporate their analysis into the latest version (number 6) of the“Pie in the Sky” annotation. Furthermore, it turns out that some units of Information Structure cross the boundaries of the syntactic/semantic constituents, thus raising the sort of difficulties discussed in the previous paragraph. Specifically, information structure divides sentences into themes and rhemes. For the sample two sentences, the rheme boundaries do correspond to syntactic units, but the theme boundaries cross syntactic boundaries, forming units made up of parts of multiple syntactic constituents. (Palmer et al., 2005; Xue, 2005) (the eighth and 1 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 1–4, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics eleventh papers) make comparisons of annotated phenomena across English and Chinese. It should be pointed out that seven of the papers at this workshop are predominantly about the annotation of English, one is about German annotation and one is about Japanese annotation. These two are the only papers at the workshop that explicitly discuss attempts to apply the same annotation scheme across two languag"
W05-0301,W05-0311,0,0.0153772,"d 1 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 1–4, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics eleventh papers) make comparisons of annotated phenomena across English and Chinese. It should be pointed out that seven of the papers at this workshop are predominantly about the annotation of English, one is about German annotation and one is about Japanese annotation. These two are the only papers at the workshop that explicitly discuss attempts to apply the same annotation scheme across two languages. (McShane et al., 2005; Poesio and Artstein, 2005) (the ninth and tenth papers) both pertain to issues about improving the annotation process. (Poesio and Artstein, 2005) discusses some better ways of assessing interannotator agreement, particularly when there is a gray area between correct and incorrect annotation. (McShane et al., 2005) discusses the issue of human-aided annotation (human correction of a machine-generated analysis) as it pertains to a single-integrated annotation scheme, similar in many ways to “Pie in the Sky”, although it has been in existence for a lot longer. 2 Issues for Discussion These papers raise a number of import"
W05-0301,W05-0302,1,0.902663,"to produce detailed semantic annotation of two difficult sentences. The effort aimed to produce a single unified representation that goes beyond what may currently be feasible to annotate consistently or to generate automatically. Rather this “pie in the sky” annotation effort was an attempt at defining a future goal for semantic analysis. We decided to use the “Pie in the Sky” annotation effort (http://nlp.cs.nyu.edu/meyers/piein-the-sky.html) as a theme for this year’s workshop. Consequently this theme has been brought out in many of the papers contained in this volume. The first 4 papers (Pustejovsky et al., 2005; E. W. Hinrichs and S. K¨ubler and K. Naumann, 2005; Bies et al., 2005; Dinesh et al., 2005) all discuss some aspect of merging annotation. (Pustejovsky et al., 2005) describes issues that arise for merging argument structures for verbs, nouns and discourse connectives, as well as time and anaphora representations. (E. W. Hinrichs and S. K¨ubler and K. Naumann, 2005) focuses on the merging of syntactic, morphological, semantic and referential annotation. (E. W. Hinrichs and S. K¨ubler and K. Naumann, 2005) also points out that the “Pie in the Sky”representation lacks syntactic features. This"
W05-0301,W05-0308,0,0.0332384,"n discourse structure and syntactic structure. I think it is reasonable to assume that some such conflicts will be resolvable, e.g., I believe that the named entity conflicts point to shortcomings of the original Penn Treebank analysis. However, the discourse structure/syntactic structure conflicts may be harder to solve. In fact, some annotation projects, e.g., the Prague Dependency Treebank (Hajiˆcov´a and Ceplov´a, 2000), assume that multiple analyses or “levels” are necessary to describe the full range of phenomena. The 5th through 7th papers (Inui and Okumura, 2005; Calhoun et al., 2005; Wilson and Wiebe, 2005) investigate some additional types of annotation that were not part of the distributed version of Pie in the Sky, but which could be added in principle. In fact, with help from the authors of (Calhoun et al., 2005), I did incorporate their analysis into the latest version (number 6) of the“Pie in the Sky” annotation. Furthermore, it turns out that some units of Information Structure cross the boundaries of the syntactic/semantic constituents, thus raising the sort of difficulties discussed in the previous paragraph. Specifically, information structure divides sentences into themes and rhemes."
W05-0301,W05-0312,0,0.030811,"porate their analysis into the latest version (number 6) of the“Pie in the Sky” annotation. Furthermore, it turns out that some units of Information Structure cross the boundaries of the syntactic/semantic constituents, thus raising the sort of difficulties discussed in the previous paragraph. Specifically, information structure divides sentences into themes and rhemes. For the sample two sentences, the rheme boundaries do correspond to syntactic units, but the theme boundaries cross syntactic boundaries, forming units made up of parts of multiple syntactic constituents. (Palmer et al., 2005; Xue, 2005) (the eighth and 1 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 1–4, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics eleventh papers) make comparisons of annotated phenomena across English and Chinese. It should be pointed out that seven of the papers at this workshop are predominantly about the annotation of English, one is about German annotation and one is about Japanese annotation. These two are the only papers at the workshop that explicitly discuss attempts to apply the same annotation scheme across two languages. (McShane"
W05-0301,C98-1013,0,\N,Missing
W05-0302,P98-1013,0,0.0768075,"Missing"
W05-0302,W99-0302,0,0.0733028,"Missing"
W05-0302,W01-1514,0,0.0273335,"k, NomBank and Coreference, the core predicate argument structures and referents for the arguments. One possible representation format would be to convert each annotation into features and values to be added to a larger feature structure. 1 The resulting feature structure would combine stand alone and offset annotation – it would include actual words and features from the text as well as special features that point to the actual text (character offsets) and, perhaps, syntactic trees (offsets along the lines of PropBank/NomBank). Alternative global annotation schemes include annotation graphs (Cieri & Bird, 2001), and MATE (Carletta, et. al., 1999). There are many areas in which the boundaries between these annotations have not been clearly defined, such as the treatment of support constructions and light verbs, as discussed below. Determining the most suitable format for the merged representation should be a top priority. NomBank would add arguments for report, trial, launch and beginning as follows: According to [Rel_report.01 reports], [Arg1 [ArgM-LOC sea [Rel_trial.01 trials] [Arg1 for [Arg1-CF_launch.01 a patrol boat] developed by Kazakhstan] are being conducted and the [ArgM-MNR formal] [Rel_lau"
W05-0302,hajicova-kucerova-2002-argument,0,0.0418506,"Missing"
W05-0302,W04-2705,1,0.895192,"Missing"
W05-0302,W04-0413,1,0.854761,"Missing"
W05-0302,miltsakaki-etal-2004-penn,0,0.0711906,"Missing"
W05-0302,J05-1004,1,0.327965,"fforts. This level could provide the foundation for a major advance in our ability to automatically extract salient relationships from text. This will in turn facilitate breakthroughs in message understanding, machine translation, fact retrieval, and information retrieval. Many recent annotation efforts for English have focused on pieces of the larger problem of semantic annotation, rather than producing a single unified representation like Head-driven Phrase Structure Grammar (Pollard and Sag 1994) or the Prague Dependency Tectogramatical Representation (Hajicova & Kucerova, 2002). PropBank (Palmer et al, 2005) annotates predicate argument structure anchored by verbs. NomBank (Meyers, et. al., 2004a) annotates predicate argument structure anchored by nouns. TimeBank (Pustejovsky et al, 2003) 2. The Component Annotation Schemata We describe below existing independent annotation efforts, each one of which is focused on a specific aspect of the semantic representation task: semantic role labeling, 5 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 5–12, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics tuned to a hiring scenario (MUC-6, 1995),"
W05-0302,W99-0309,0,0.0605805,"esolved in order to bring these different layers together seamlessly. Most of these approaches have annotated the same type of data, Wall Street Journal text, so it is also important to demonstrate that the annotation can be extended to other genres such as spoken language. The demonstration of success for the extensions would be the training of accurate statistical semantic taggers. Coreference: Coreference involves the detection of subsequent mentions of invoked entities, as in George Bush,… he…. Researchers at Essex (UK) were responsible for the coreference markup scheme developed in MATE (Poesio et al, 1999; Poesio, 2004a), partially implemented in the annotation tool MMAX and now proposed as an ISO standard; and have been responsible for the creation of two small, but commonly used anaphorically annotated corpora – the Vieira / Poesio subset of the Penn Treebank (Poesio and Vieira, 1998), and the GNOME corpus (Poesio, 2004a). Parallel coreference annotation efforts funded by ACE have resulted in similar guidelines, exemplified by BBN’s recent annotation of Named Entities, common nouns and pronouns. These two approaches provide a suitable springboard for an attempt at achieving a community conse"
W05-0302,J98-2001,1,0.810969,"monstration of success for the extensions would be the training of accurate statistical semantic taggers. Coreference: Coreference involves the detection of subsequent mentions of invoked entities, as in George Bush,… he…. Researchers at Essex (UK) were responsible for the coreference markup scheme developed in MATE (Poesio et al, 1999; Poesio, 2004a), partially implemented in the annotation tool MMAX and now proposed as an ISO standard; and have been responsible for the creation of two small, but commonly used anaphorically annotated corpora – the Vieira / Poesio subset of the Penn Treebank (Poesio and Vieira, 1998), and the GNOME corpus (Poesio, 2004a). Parallel coreference annotation efforts funded by ACE have resulted in similar guidelines, exemplified by BBN’s recent annotation of Named Entities, common nouns and pronouns. These two approaches provide a suitable springboard for an attempt at achieving a community consensus on coreference. PropBank: The Penn Proposition Bank focuses on the argument structure of verbs, and provides a corpus annotated with semantic roles, including participants traditionally viewed as arguments and adjuncts. An important goal is to provide consistent semantic role label"
W05-0302,W04-2709,0,\N,Missing
W05-0302,W04-2704,1,\N,Missing
W05-0302,J93-2004,0,\N,Missing
W05-0302,W04-0210,1,\N,Missing
W05-0302,C98-1013,0,\N,Missing
W05-0302,poesio-kabadjov-2004-general,1,\N,Missing
W05-0302,W04-2327,1,\N,Missing
W06-0606,C00-1041,0,0.0456699,"Missing"
W06-0606,kingsbury-palmer-2002-treebank,0,0.0988539,"Missing"
W06-0606,W04-1107,0,0.0221507,"Missing"
W06-0606,W04-2705,1,0.893218,"Missing"
W06-0606,salmon-alt-romary-2004-towards,0,0.0629369,"Missing"
W06-0606,I05-6003,0,0.0224059,"Missing"
W06-0606,W05-0311,0,0.034989,"Missing"
W06-0606,xia-etal-2000-developing,0,0.12979,"Missing"
W06-0606,W05-0302,1,0.896815,"Missing"
W06-0606,C02-1145,0,0.0398703,"Missing"
W06-0606,J00-4005,0,\N,Missing
W06-0606,W04-2327,0,\N,Missing
W07-1529,W06-2810,0,0.0533584,"Missing"
W07-1529,ide-suderman-2004-american,1,0.897407,"ion adequately handles relative pronouns, a new project that is annotating coreference is less likely to include relative pronouns in their annotation; and (b) The use of particular test corpora to determine whether a particular annotation task can feasibly achieve good agreement scores. (c) The use of underlying models for representing annotation content that facilitate merging, comparison, and analysis. This study will focus on the problem of identifying such corpora as well as the suitability of two candidate corpora: the Open portion of the American National Corpus (Ide and Macleod, 2001; Ide and Suderman, 2004) and the “Controversial” portions of the WikipediaXML corpus (Denoyer and 184 Proceedings of the Linguistic Annotation Workshop, pages 184–190, c Prague, June 2007. 2007 Association for Computational Linguistics (d) To the extent possible, the use of common annotation categories or a mapping among categories for the same phenomenon used by different annotation groups. In selecting shared corpora, we believe that the following issues must be taken into consideration: 1. The diversity of genres, lexical items and linguistic phenomena – this will ensure that the corpora will be useful to many dif"
W07-1529,ide-suderman-2006-integrating,1,0.846263,". ANC annotations are distributed as stand-off documents representing a set of graphs over the primary data, thus allowing for layering of annotations and inclusion of multiple annotations of the same type. Because most existing tools for corpus access and manipulation do not handle stand-off annotations, we have developed an easy-to-use tool and user interface to merge the user’s choice of stand-off annotations with the primary data to form a single document in any of several XML and non-XML formats, which is distributed with the corpus. The ANC architecture and format is described fully in (Ide and Suderman, 2006). 2.1 The ULA Subcorpus The Unified Linguistic Annotation (ULA) project has selected a 40,000 word subcorpus of the Open ANC for annotation with several different annotation schemes including: the Penn Treebank, PropBank, NomBank, the Penn Discourse Treebank, TimeML and Opinion Annotation.2 This initial subcorpus can be broken down as follows: • Spoken Language – charlotte: 5K words – switchboard: 5K words • Slate (Journal): 5K words • Travel guides: 5K words • 911report: 5K words • OUP books (Kaufman): 5K words As the ULA project progresses, the participants intend to expand the corpora annot"
W07-1529,N07-2032,0,0.0226548,"n 3.4) 5. and the corpus has various other properties that many researchers feel would be interesting to exploit. To date research in Computational Linguistics using Wikipedia includes: Automatic derivation of taxonomy information (Strube and Ponzetto, 2006; Suchanek et al., 2007; Zesch and Gurevych, 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.science.uva.nl/WiQA/); and information retrieval (Fuhr et al., 2006). Some other interesting properties of Wikipedia that have yet to be explored to our knowledge include: (1) Most main articles have talk pages which discuss them – perhaps this relation can be exploited by systems which try to detect discussions about topics, e.g., searches for discussions about curr"
W07-1529,N07-3003,0,0.014895,"atistical systems); • letters: 10K words 2. the textual information is well structured 2 Other corpora being annotated by the ULA project include sections of the Brown corpus and LDC parallel corpora. 186 3. Wikipedia is a large and growing corpus 4. the articles are multilingual (cf. section 3.4) 5. and the corpus has various other properties that many researchers feel would be interesting to exploit. To date research in Computational Linguistics using Wikipedia includes: Automatic derivation of taxonomy information (Strube and Ponzetto, 2006; Suchanek et al., 2007; Zesch and Gurevych, 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.science.uva.nl/WiQA/); and information retrieval (Fuhr et al., 2006). So"
W07-1529,W07-0201,0,0.0149065,"lexical information for statistical systems); • letters: 10K words 2. the textual information is well structured 2 Other corpora being annotated by the ULA project include sections of the Brown corpus and LDC parallel corpora. 186 3. Wikipedia is a large and growing corpus 4. the articles are multilingual (cf. section 3.4) 5. and the corpus has various other properties that many researchers feel would be interesting to exploit. To date research in Computational Linguistics using Wikipedia includes: Automatic derivation of taxonomy information (Strube and Ponzetto, 2006; Suchanek et al., 2007; Zesch and Gurevych, 2007; Ponzetto, 2007); automatic recognition of pairs of similar sentences in two languages (Adafre and de Rijke, 2006); corpus mining (R¨udiger Gleim and Alexander Mehler and Matthias Dehmer, 2007), Named Entity Recognition (Toral and noz, 2007; Bunescu and Pasc¸a, 2007) and relation extraction (Nguyen et al., 2007). In addition several shared tasks have been set up using Wikipedia as the target corpus including question answering (cf. (D. Ahn and V. Jijkoun and G. Mishne and K. M¨uller and M. de Rijke and S. Schlobach, 2004) and http://ilps.science.uva.nl/WiQA/); and information retrieval (Fuhr"
W08-2121,D07-1101,0,0.331336,"sults, but the improvement is not large. These initial efforts indicate at least that the joint modeling of this problem is not a trivial task. The D Arch. and D Inference columns summarize the parsing architectures and the corresponding inference strategies. Similar to last year’s shared task (Nivre et al., 2007), the vast majority of parsing models fall in two classes: transition-based (“trans” in the table) or graph-based (“graph”) models. By and large, transition-based models use a greedy inference strategy, whereas graph-based models used different Maximum Spanning Tree (MST) algorithms: Carreras (2007) – MSTC , Eisner (2000) – MSTE , or Chu-Liu/Edmonds (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) – MSTCL/E . More interestingly, most of the best systems used some strategy to mitigate parsing errors. In the top three systems in the closed challenge, two (che and ciaramita) used parser combination through voting and/or stacking of different models (see the D Comb. column). Samuelsson et al. (2008) perform a MST inference with the bag of all dependencies output by the individual MALT parser variants. Johansson and Nugues (2008) use a single parsing model, but this model is extended"
W08-2121,W08-2134,0,0.108513,"movements, split clauses, and split noun phrases. 6.3 Normalized SRL Performance Table 6.3 lists the scores for the semantic subtask measured as the ratio of the labeled F1 score and LAS. As previously mentioned, this score estimates the performance of the SRL component independent of the performance of the syntactic parser. This analysis is not a substitute for the actual experiment where the SRL components are evaluated using correct syntactic information but, nevertheless, it indicates several interesting facts. First, the ranking of the top three systems in Table 10 changes: the system of Che et al. (2008) is now ranked first, and the system of Johansson and Nugues (2008) is second. This shows that Che et al. have a relatively stronger SRL component, whereas Johansson and Nugues developed a better parser. Second, several other systems improved their ranking compared to Table 10: e.g., chen from position thirteenth to ninth and choi from sixteenth to eighth. This indicates that these systems were penalized in the official ranking mainly due to the relative poor performance of their parsers. Note that this experiment is relevant only for systems that implemented pipeline architectures, where the"
W08-2121,W08-2139,0,0.0101528,"MaltParser with labels enriched with semantic information; Llu´ıs and M`arquez, who used a modified version of the Eisner algorithm to jointly predict syntactic and semantic dependencies; and finally, Sun et al. (2008), who integrated dependency label classification and argument identification using a maximum-entropy Markov model. Additionally, Johansson and Nugues (2008), who had the highest ranked system in the closed challenge, integrate syntactic and semantic analysis in a final reranking step, which maximizes the joint syntactic-semantic score in the top k solutions. In the same spirit, Chen et al. (2008) search in the top k solutions for the one that maximizes a global measure, in this case the joint probability of the complete problem. These joint learning strategies are summarized in the Joint Learning/Opt. column in the table. The system of Riedel and MezaRuiz (2008) deserves a special mention: even though Riedel and Meza-Ruiz did not implement a syntactic parser, they are the only group that performed the complete SRL subtask – i.e., predicate identification and classification, argument identification and classification – jointly, simultaneously for all the predicates in a sentence. They"
W08-2121,M98-1028,0,0.0358658,"and other phenomena, based on a theoretical perspective similar to that of Government and Binding Theory (Chomsky, 1981). 3.1.2 BBN Pronoun Coreference and Entity Type Corpus BBN’s NE annotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation, e.g., fearlast and cannot are split in the same ways. Named entity categories include: Person, Organization, Location, GPE, Facility, Money, Percent, Time and Date, based on the definitions of these categories in MUC (Chinchor and Robinson, 1998) and ACE7 tasks. Subcategories are included as well. Note however that from this corpus we only use NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. 3.1.3 Proposition Bank I (PropBank) The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (ARG0, ARG1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall"
W08-2121,W08-2138,1,0.51476,"Missing"
W08-2121,W06-1670,0,0.0107243,"haracters (“ ”) are used in columns 2–5 to ensure the same number of rows for all columns corresponding to one sentence. All syntactic and semantic dependencies are annotated relative to the split word forms (columns 6–8). Table 2 shows the columns available to the systems participating in the open challenge: namedentity labels as in the CoNLL-2003 Shared Task (Tjong Kim San and De Meulder, 2003) and from the BBN Wall Street Journal Entity Corpus,2 WordNet supersense tags, and the output of an offthe-shelf dependency parser (Nivre et al., 2007b). Columns 1–3 were predicted using the tagger of Ciaramita and Altun (2006). Because the BBN corpus shares lexical content with the Penn Treebank, we generated the BBN tags using a 2-fold cross-validation procedure. 2.2 Evaluation Measures We separate the evaluation measures into two groups: (i) official measures, which were used for the ranking of participating systems, and (ii) additional unofficial measures, which provide further insight into the performance of the participating systems. 2.2.1 Official Evaluation Measures The official evaluation measures consist of three different scores: (i) syntactic dependencies are scored using the labeled attachment score (LA"
W08-2121,W05-0620,1,0.81831,"Missing"
W08-2121,gimenez-marquez-2004-svmtool,1,0.278194,"Missing"
W08-2121,C04-1186,0,0.0984296,"formalism, but also extends them significantly: this year’s syntactic dependencies include more information such as named-entity boundaries; the semantic dependencies model roles of both verbal and nominal predicates. In this paper, we define the shared task and describe how the data sets were created. Furthermore, we report and analyze the results and describe the approaches of the participating systems. 1 • SRL is performed and evaluated using a dependency-based representation for both syntactic and semantic dependencies. While SRL on top of a dependency treebank has been addressed before (Hacioglu, 2004), our approach has several novelties: (i) our constituent-to-dependency conversion strategy transforms all annotated semantic arguments in PropBank and NomBank not just a subset; (ii) we address propositions centered around both verbal (PropBank) and nominal (NomBank) predicates. Introduction In 2004 and 2005 the shared tasks of the Conference on Computational Natural Language Learning (CoNLL) were dedicated to semantic role labeling (SRL), in a monolingual setting (English). In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 la"
W08-2121,W08-2122,0,0.366479,"Missing"
W08-2121,W08-2136,0,0.0382094,"Missing"
W08-2121,W08-2123,1,0.339084,"-TMP AM-MNR AM-LOC A3 AM-MOD AM-ADV AM-DIS R-A0 AM-NEG A4 C-A1 R-A1 AM-PNC AM-EXT AM-CAU AM-DIR R-AM-TMP R-A2 R-AM-LOC R-AM-MNR A5 AM-PRD C-A0 C-A2 R-AM-CAU C-A3 R-A3 C-AM-MNR C-AM-ADV AM-REC AA R-AM-PNC C-AM-EXT C-AM-TMP C-A4 Frequency &lt; 10 Frequency 161409 109437 51197 25913 13080 11409 10269 9986 9496 5369 4432 4097 3281 3118 2565 2445 1428 1346 1318 797 307 246 155 91 78 70 65 50 37 29 24 20 16 14 12 11 11 11 70 watanabe). Remarkably, the top-scoring system (johansson) is in a class of its own, with scores 2–3 points higher than the next system. This is most likely caused by the fact that Johansson and Nugues (2008) implemented a thorough system that addressed all facets of the task with state-ofthe-art methods: second-order parsing model, argument identification/classification models separately tuned for PropBank and NomBank, reranking inference for the SRL task, and, finally, joint optimization of the complete task using metalearning (more details in Section 5). Table 11 lists the official results in the open challenge. The results in this challenge are lower than in the closed challenge, but this was somewhat to be expected considering that there were fewer participants in this challenge and none of t"
W08-2121,W08-2135,0,0.0101844,"sks: Henderson et al. (2008), who implemented a generative history-based model (Incremental Sigmoid Belief Networks with vectors of latent variables) where syntactic and semantic structures are separately 170 generated but using a synchronized derivation (sequence of actions); Samuelsson et al. (2008), who, within an ensemble-based architecture, implemented a joint syntactic-semantic model using MaltParser with labels enriched with semantic information; Llu´ıs and M`arquez, who used a modified version of the Eisner algorithm to jointly predict syntactic and semantic dependencies; and finally, Sun et al. (2008), who integrated dependency label classification and argument identification using a maximum-entropy Markov model. Additionally, Johansson and Nugues (2008), who had the highest ranked system in the closed challenge, integrate syntactic and semantic analysis in a final reranking step, which maximizes the joint syntactic-semantic score in the top k solutions. In the same spirit, Chen et al. (2008) search in the top k solutions for the one that maximizes a global measure, in this case the joint probability of the complete problem. These joint learning strategies are summarized in the Joint Learn"
W08-2121,W07-2416,1,0.71052,"oNLL) were dedicated to semantic role labeling (SRL), in a monolingual setting (English). In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 languages. The CoNLL-2008 shared task1 proposes a unified dependency-based c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 http://www.yr-bcn.es/conll2008 159 • Based on the observation that a richer set of syntactic dependencies improves semantic processing (Johansson and Nugues, 2007), the syntactic dependencies modeled are more complex than the ones used in the previous CoNLL shared tasks. For example, we now include apposition links, dependencies derived from named entity (NE) structures, and better modeling of long-distance grammatical relations. • A practical framework is provided for the joint learning of syntactic and semantic dependencies. CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 159–177 Manchester, August 2008 Given the complexity of this shared task, we limited the evaluation to a monolingual, Englishonly set"
W08-2121,W03-0419,0,0.0172595,"Missing"
W08-2121,W08-2124,1,0.554724,"Missing"
W08-2121,W08-2140,0,0.0210024,"a single parsing model, but this model is extended with second-order features. The PA Arch. and PA Inference columns summarize the architectures and inference strategies used for the identification and classification of predicates and arguments. The columns indicate that most systems modeled the SRL problem as a token-by-token classification problem (“class” in the table) with a corresponding greedy inference strategy. Some systems (e.g., yuret, samuelsson, henderson, lluis) incorporate SRL within parsing, in which case we report the corresponding parsing architecture and inference approach. Vickrey and Koller (2008) simplify the sentences to be labeled using a set of hand-crafted rules before deploying a classification model on top of a constituent-based representation. Unlike in the case of parsing, few systems (yuret, samuelssson, and morante) combine several PA models and the combination is limited to simple voting strategies (see the PA Comb. column). Finally, the ML Methods column lists the Machine Learning (ML) methods used. The column indicates that maximum entropy (ME) was the most popular method (12 distinct systems relied on it). Support Vector Machines (SVM) (eight systems) and the Perceptron"
W08-2121,J93-2004,0,0.0476215,"of the constituent-to-dependency conversion process. The section concludes with an overview of the shared task corpora. 3.1 Input Corpora Input to our merging procedures includes the Penn Treebank, BBN’s named entity corpus, PropBank and NomBank. In this section, we will provide brief descriptions of these annotations in terms of both form and content. All annotations are currently being distributed by the Linguistic Data Consortium, with the exception of NomBank, which is freely downloadable.6 6 http://nlp.cs.nyu.edu/meyers/NomBank. html 162 3.1.1 Penn Treebank 3 The Penn Treebank 3 corpus (Marcus et al., 1993) consists of hand-coded parses of the Wall Street Journal (test, development and training) and a small subset of the Brown corpus (W. N. Francis and H. Kuˆcera, 1964) (test only). These hand parses are notated in-line and sometimes involve changing the strings of the input data. For example, in file wsj 0309, the token fearlast in the text corresponds to the two tokens fear and last in the annotated data. In a similar way, cannot is regularly split to can and not. It is significant that the other annotations assume the tokenization of the Penn Treebank, as this makes it easier for us to merge"
W08-2121,W03-3023,0,0.13388,"s(T ) return create-dependency-tree(T ) 3.2 Conversion to Dependencies 3.2.1 Syntactic Dependencies There exists no large-scale dependency treebank for English, and we thus had to construct a dependency-annotated corpus automatically from the Penn Treebank (Marcus et al., 1993). Since dependency syntax represents grammatical structure by means of labeled binary head–dependent relations rather than phrases, the task of the conversion procedure is to identify and label the head–dependent pairs. The idea underpinning constituent-to-dependency conversion algorithms (Magerman, 1994; Collins, 1999; Yamada and Matsumoto, 2003) is that head–dependent pairs are created from constituents by selecting one word in each phrase as the head and setting all other as its dependents. The dependency labels are then inferred from the phrase–subphrase or phrase–word relations. Our conversion procedure (Johansson and Nugues, 2007) differs from this basic approach by exploiting the rich structure of the constituent format used in Penn Treebank 3: procedure import-glarf(T ) Import a GLARF surface dependency graph G for each multi-word name N in G for each token d in N Set the function tag of d to NAME for each dependency link h →L"
W08-2121,H05-1066,0,0.305206,"ate at least that the joint modeling of this problem is not a trivial task. The D Arch. and D Inference columns summarize the parsing architectures and the corresponding inference strategies. Similar to last year’s shared task (Nivre et al., 2007), the vast majority of parsing models fall in two classes: transition-based (“trans” in the table) or graph-based (“graph”) models. By and large, transition-based models use a greedy inference strategy, whereas graph-based models used different Maximum Spanning Tree (MST) algorithms: Carreras (2007) – MSTC , Eisner (2000) – MSTE , or Chu-Liu/Edmonds (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) – MSTCL/E . More interestingly, most of the best systems used some strategy to mitigate parsing errors. In the top three systems in the closed challenge, two (che and ciaramita) used parser combination through voting and/or stacking of different models (see the D Comb. column). Samuelsson et al. (2008) perform a MST inference with the bag of all dependencies output by the individual MALT parser variants. Johansson and Nugues (2008) use a single parsing model, but this model is extended with second-order features. The PA Arch. and PA Inference columns summari"
W08-2121,W08-2126,0,0.0266466,"mBank, reranking inference for the SRL task, and, finally, joint optimization of the complete task using metalearning (more details in Section 5). Table 11 lists the official results in the open challenge. The results in this challenge are lower than in the closed challenge, but this was somewhat to be expected considering that there were fewer participants in this challenge and none of the top five groups in the closed challenge submitted results in the open challenge. Only one of the systems that participated in both challenges (zhang) improved the results submitted in the closed challenge. Zhang et al. (2008) achieved this by extracting features for their semantic subtask models both from the parser used in the closed challenge and a secondary parser that was trained on a different corpus. The improvements measured were relatively small for the in-domain WSJ corpus (0.2 labeled macro F1 points) but larger for the out-of-domain Brown corpus (approximately 1 labeled macro F1 point). Table 9: Statistics for semantic roles. 4 Submissions and Results Nineteen groups submitted test runs in the closed challenge and five groups participated in the open challenge. Three of the latter groups participated on"
W08-2121,W01-1511,0,0.0379316,"Missing"
W08-2121,W04-2705,1,0.723818,"Missing"
W08-2121,W06-2933,1,0.512001,"Missing"
W08-2121,J05-1004,0,0.584805,"compatible with the Penn Treebank annotation, e.g., fearlast and cannot are split in the same ways. Named entity categories include: Person, Organization, Location, GPE, Facility, Money, Percent, Time and Date, based on the definitions of these categories in MUC (Chinchor and Robinson, 1998) and ACE7 tasks. Subcategories are included as well. Note however that from this corpus we only use NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. 3.1.3 Proposition Bank I (PropBank) The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (ARG0, ARG1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types 7 http://projects.ldc.upenn.edu/ace/ of ARGM (TMP, ADV, etc.).8 Rather than using PropBank directly, we used the version created for the CoNLL-2005 shared task (Carreras and M`arquez, 2005). PropBank’s pointers to subtrees are converted into th"
W08-2121,W08-2125,0,0.116607,"g paper in the proceedings. Results are sorted in descending order of the labeled F1 score for semantic dependencies on the WSJ+Brown corpus. The number in parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding task. 5 Approaches Table 5 summarizes the properties of the systems that participated in the closed the open challenges. The second column of the table highlights the overall architectures. We used + to indicate that the components are sequentially connected. The lack of a + sign indicates that the corresponding tasks are performed jointly. For example, Riedel and Meza-Ruiz (2008) perform predicate and argument identification and classification jointly, whereas Ciaramita et al. (2008) implemented a pipeline architecture of three components. We use the ||to indicate that several different architectures that span multiple subtasks were deployed in parallel. This summary of system architectures indicates that it is common that systems combine several components in the semantic or syntactic subtasks – e.g., nine systems jointly performed predicate/argument identification and classification – but only four systems combined components between the syntactic and semantic subta"
W08-2121,J03-4003,0,\N,Missing
W08-2121,D07-1096,1,\N,Missing
W08-2121,W04-2412,1,\N,Missing
W09-1201,burchardt-etal-2006-salsa,1,0.483589,"Missing"
W09-1201,D07-1101,0,0.391748,"Missing"
W09-1201,W09-1202,0,0.0278745,"order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the different subtasks of the CoNLL shared task. The idea is to decompose the joint learning problem into four subtasks – syntactic dependency identification, syntactic dependency labeling, semantic dependency identification and semantic dependency labeling. The initial step is to use a pipeline approach to use the input of one subtask as input to the next, in the order specified. The iterative steps then use additional features that are not available in the initial step to improve the accuracy of the overall system. For ex"
W09-1201,W09-1205,0,0.222475,"al token; and (c), the existence of an edge between each pair of tokens. Subsequently, they combine the (possibly conflicting) output of the three classifiers by a ranking approach to determine the most likely structure that meets all well-formedness constraints. (Llu´ıs et al., 2009) present a joint approach based on an extension of Eisner’s parser to accommodate also semantic dependency labels. This architecture is similar to the one presented by the same authors in the past edition, with the extension to a second-order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the differen"
W09-1201,W09-1212,1,0.83271,"Missing"
W09-1201,S07-1008,1,0.697359,"Missing"
W09-1201,H05-1066,1,0.168004,"Missing"
W09-1201,W04-2705,1,0.527791,"Missing"
W09-1201,W09-1219,0,0.0294123,"Missing"
W09-1201,H05-1108,1,0.506508,"y, adding further manual labels where necessary. Then, we used frequency and grammatical realization information to map the remaining roles onto higher-numbered Arg roles. We considerably simplified the annotations provided by SALSA, which use a rather complex annotation scheme. In particular, we removed annotation for multi-word expressions (which may be non-contiguous), annotations involving multiple frames for the same predicate (metaphors, underspecification), and inter-sentence roles. The out-of-domain dataset was taken from a study on the multi-lingual projection of FrameNet annotation (Pado and Lapata, 2005). It is sampled from the EUROPARL corpus and was chosen to maximize the lexical coverage, i.e., it contains of a large number of infrequent predicates. Both syntactic and semantic structure were annotated manually, in the TIGER and SALSA format, respectively. Since it uses a simplified annotation schemes, we did not have to discard any annotation. For both datasets, we converted the syntactic TIGER (Brants et al., 2002) representations into dependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con1"
W09-1201,C08-1085,1,0.175934,"Missing"
W09-1201,J05-1004,0,0.213522,"nnotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation. For the CoNLL-2008 shared task evaluation, this corpus was extended by the task organizers to cover the subset of the Brown corpus used as a secondary testing dataset. From this corpus we only used NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. • Proposition Bank I (PropBank) – The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (Arg0, Arg1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types of adjuncts (ArgM-TMP, -ADV, etc.). • NomBank – NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun"
W09-1201,E09-1087,1,0.646818,"Missing"
W09-1201,W08-2121,1,0.597132,"Missing"
W09-1201,taule-etal-2008-ancora,0,0.543017,"Missing"
W09-1201,cmejrek-etal-2004-prague,1,0.63993,"Missing"
W09-1201,kawahara-etal-2002-construction,1,\N,Missing
W09-1201,J93-2004,0,\N,Missing
W09-1201,D07-1096,1,\N,Missing
W09-2423,P98-1013,0,0.0111712,"Missing"
W09-2423,P01-1017,0,0.22112,"Missing"
W09-2423,N06-1024,0,0.0607349,"Missing"
W09-2423,W04-3228,0,0.0260109,"Missing"
W09-2423,P06-2055,1,0.767512,"Missing"
W09-2423,C02-1122,0,0.0190732,"rse of eight years or so. In contrast, the Chinese and Japanese systems are newer and considerably less time was spent developing them. Thus they currently do not represent as many regularizations. One obstacle is that we do not currently use subcategorization dictionaries for either language, while we have several for English. In particular, these would be helpful in predicting and filling relative clause and others gaps. We are considering auto153 matically acquiring simple dictionaries by recording frequently occurring argument types of verbs over a larger corpus, e.g., along the lines of (Kawahara and Kurohashi, 2002). In addition, existing Japanese dictionaries such as the IPAL (monolingual) dictionary (technology Promotion Agency, 1987) or previously acquired case information reported in (Kawahara and Kurohashi, 2002). Finally, we are investigating several avenues for using this system output for Machine Translation (MT) including: (1) aiding word alignment for other MT system (Wang et al., 2007); and (2) aiding the creation various MT models involving analyzed text, e.g., (Gildea, 2004; Shen et al., 2008). Acknowledgments This work was supported by NSF Grant IIS0534700 Structure Alignment-based MT. Refe"
W09-2423,W04-2705,1,0.83884,"conjunctions (e.g., and, or, but) and their conjuncts are transparent CONJ relations. Thus although and links together John and Mary, it is these dependents that determine that the resulting phrase is noun-like (an NP in phrase structure terminology) and sentient (and thus can occur as the subject of verbs like ate). Another common example of transparent relations are the relations connecting certain nouns and the prepositional objects under them, e.g., the box of cookies is edible, because cookies are edible even though boxes are not. These features are marked in the NOMLEX-PLUS dictionary (Meyers et al., 2004b). In Figure 4, we represent transparent relations, by prefixing the LOGIC1 label with asterisks. The above description most accurately describes English GLARF. However, Chinese GLARF has most of the same properties, the main exception being that PDTB arguments are not currently marked. 149 For Japanese, we have only a preliminary representation of LOGIC2 relations and they are not derived from PropBank/NomBank/PDTB. 2.1 Scoring the LOGIC1 Structure For purposes of scoring, we chose to focus on LOGIC1 relations, our proposed high-performance level of semantics. We scored with respect to: the"
W09-2423,meyers-etal-2004-cross,1,0.822383,"conjunctions (e.g., and, or, but) and their conjuncts are transparent CONJ relations. Thus although and links together John and Mary, it is these dependents that determine that the resulting phrase is noun-like (an NP in phrase structure terminology) and sentient (and thus can occur as the subject of verbs like ate). Another common example of transparent relations are the relations connecting certain nouns and the prepositional objects under them, e.g., the box of cookies is edible, because cookies are edible even though boxes are not. These features are marked in the NOMLEX-PLUS dictionary (Meyers et al., 2004b). In Figure 4, we represent transparent relations, by prefixing the LOGIC1 label with asterisks. The above description most accurately describes English GLARF. However, Chinese GLARF has most of the same properties, the main exception being that PDTB arguments are not currently marked. 149 For Japanese, we have only a preliminary representation of LOGIC2 relations and they are not derived from PropBank/NomBank/PDTB. 2.1 Scoring the LOGIC1 Structure For purposes of scoring, we chose to focus on LOGIC1 relations, our proposed high-performance level of semantics. We scored with respect to: the"
W09-2423,W07-1529,1,0.799455,"Missing"
W09-2423,W04-2703,0,0.139121,"Missing"
W09-2423,I05-4002,0,0.0390298,"Missing"
W09-2423,J05-1004,0,0.21622,"Missing"
W09-2423,W05-0302,1,0.852675,"els; we have a different set of relational labels; and finally, our approach is designed to be compatible with the Penn Treebank framework and therefore, Penn-Treebankbased parsers. In addition, the expansion of our theory is governed more by available resources than by the underlying theory. As our main goal is to use our system to regularize data, we freely incorporate any analysis that fits this goal. Over time, we have found ways of incorporating Named Entities, PropBank, NomBank and the Penn Discourse Treebank. Our agenda also includes incorporating the results of other research efforts (Pustejovsky et al., 2005). For each sentence, we generate a feature structure (FS) representing our most complete analysis. We distill a subset of this information into a dependency structure governed by theoretical assumptions, e.g., about identifying functors of phrases. Each GLARF dependency is between a functor and an argument, where the functor is the head of a phrase, conjunction, complementizer, or other function word. We have built applications that use each of these two representations, e.g., the dependency representation is used in (Shinyama, 2007) and the FS representation is used in (K. Parton and K. R. Mc"
W09-2423,P08-1066,0,0.0264245,"Missing"
W09-2423,P03-1010,0,0.0528115,"Missing"
W09-2423,D07-1077,0,0.0600045,"Missing"
W09-2423,J08-2004,1,0.877787,"Missing"
W09-2423,J93-2004,0,\N,Missing
W09-2423,W08-2121,1,\N,Missing
W09-2423,J03-4003,0,\N,Missing
W09-2423,P09-1048,1,\N,Missing
W09-2423,C98-1013,0,\N,Missing
W09-2423,W09-1201,1,\N,Missing
W09-3019,P03-1010,0,0.0268446,"Missing"
W09-3019,P01-1017,0,0.0441748,"our grammars of Japanese and Chinese do not currently.; (5) a logic2 label (L2) for Chinese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automati"
W09-3019,J02-3001,0,0.015508,"a logic2 label (L2) for Chinese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generat"
W09-3019,J08-2004,1,0.84166,"inese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammati"
W09-3019,W09-1201,1,0.825301,"Missing"
W09-3019,D09-1087,1,0.855247,"Missing"
W09-3019,P06-2055,1,0.900802,"Missing"
W09-3019,W08-2121,1,\N,Missing
W09-3019,J03-4003,0,\N,Missing
W09-3019,W04-0413,1,\N,Missing
W09-3019,W09-2423,1,\N,Missing
W11-1010,2010.amta-papers.10,0,0.0467879,"Missing"
W11-1010,P01-1017,0,0.0710179,"n MT to future research. In addition to these papers, there has also been some work on rule-based reordering preprocessors to word alignment based on shallower linguistic information. For example (Crego and Mari˜no, 2006) reorders based on patterns of POS tags. We hypothesize that this is similar to the above approaches in that patterns of POS tags are likely to simulate parsing or chunking. 3 Preparing the Data The two stage parsers of previous decades (Hobbs and Grishman, 1976) generated a syntactic repre89 sentation analogous to the (more accurate) output of current treebank-based parsers (Charniak, 2001) and an additional second stage output that regularized constructions (passive, active, relative clauses) to representations similar to active clauses with no gaps, e.g., The book was read by Mary was given a representation similar to that of Mary read the book. Treating the active clause as canonical provides a way to reduce variation in language and thus, making it easier to acquire and apply statistical information from corpora–there is more evidence for particular statistical patterns when applications learn patterns and patterns more readily match data. Two-stage parsers were influenced b"
W11-1010,P05-1066,0,0.1291,"Missing"
W11-1010,2007.mtsummit-tutorials.1,0,0.0242494,"root that can be bridged by a surface path. 4 Manual Reordering Rules We derived manual rules for making the English Word Order more like the Chinese by manually inspecting the data. We inspected the first 100-200 sentences of the DEV corpus by first transliterating the Chinese into English – replaced each Chinese word with the aligned English counterpart. Several patterns emerged which were easy to formalize into rules in the GLARF framework. These patterns were verified and sometimes generalized through discussions with native Chinese speakers and linguists. Our rules, similar to those of (Wang et al., 2007) are as follows (results are discussed in section 6): (1) Front a post-nominal PP headed by a preposition in the list {of, in, with, about)}. (2) Front post-nominal relative clause that begins with that or does not have any relative pronoun, such that the main predicate is not a copula plus adjective construction. (3) Front post-nominal relative clause that begins with that or has no relative pronoun if the main predicate is a copula+adjective construction which is not negated by a word from the set {no neither nor never not n’t}. (4) Front post-nominal reduced relative in the form of a passiv"
W11-1010,P01-1067,0,0.230526,"Missing"
W11-1010,N07-1051,0,\N,Missing
W11-1010,W04-2703,0,\N,Missing
W11-1010,W03-1707,1,\N,Missing
W11-1010,J07-3002,0,\N,Missing
W11-1010,W09-2423,1,\N,Missing
W11-1010,J08-2004,1,\N,Missing
W11-1010,2006.amta-papers.4,0,\N,Missing
W11-1010,P08-1064,0,\N,Missing
W11-1010,P07-2045,0,\N,Missing
W11-1010,P06-2055,0,\N,Missing
W11-1010,W07-0401,0,\N,Missing
W11-1010,C96-1078,1,\N,Missing
W11-1010,J02-3001,0,\N,Missing
W11-1010,J03-1002,0,\N,Missing
W11-1010,J05-1004,0,\N,Missing
W11-1010,P08-1066,0,\N,Missing
W11-1010,P98-2139,1,\N,Missing
W11-1010,C98-2134,1,\N,Missing
W11-1010,D07-1077,0,\N,Missing
W11-1010,meyers-etal-2004-annotating,1,\N,Missing
W11-1010,N09-2004,0,\N,Missing
W11-1010,P00-1056,0,\N,Missing
W13-1103,J93-1003,0,0.217259,"another one with a flexible number (vary from 1 to 4) of tweets. Both ↵ and are set to 0.1 in our implementation. All parameters are set experimentally over a small development dataset consisting of 10 events in Twitter data of September 2012. EventRank−Flexible 0 a Twitter specific name entity tagger1 ) and a reference to the same unique calendar date (resolved using a temporal expression processor (Mani and Wilson, 2000)). Tweets published during the whole period are aggregated together to find top events that happen on each calendar day. We applied the G2 test for statistical significance (Dunning, 1993) to rank the event clusters, considering the corpus frequency of the named entity, the number of times the date has been mentioned, and the number of tweets which mention both together. We randomly picked the events of one day for human evaluation, that is the day of January 16, 2013 with 38 events and an average of 465 tweets per event cluster. 4 5 compactness completeness overall EventRank−Flexible EventRank−Fixed SumBasic Figure 5: human judgments evaluating tweet summarization systems Event System EventRank (Flexible) Google 1/16/2013 SumBasic EventRank (Flexible) Instagram 1/16/2013 SumBa"
W13-1103,P11-2008,0,0.0183447,"Missing"
W13-1103,P06-1047,1,0.898837,"(e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to extract named entities and events in the tweets as has been shown for classic multi-document summarization (Li et al., 2006). Recent progress on building NLP tools for Twitter (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2011b; Ritter et al., 2012; Liu et al., 2012) makes it possible to investigate an approach to summarizing Twitter events which is based on Information Extraction techniques. We investigate a graph-based approach which leverages named entities, event phrases and their connections across tweets. A similar idea has been studied by Li et al. (2006) to rank the salience of event concepts in summarizing news articles. However, the extreme redundancy and simplicity of tweets allows us to explici"
W13-1103,W11-0709,0,0.423753,"er of messages, many containing irrelevant and redundant information, quickly leads to a situation of information overload. This motivates the need for automatic summarization systems which can select a few messages for presentation to a user which cover the most important information relating to the event without redundancy and filter out irrelevant and personal information that is not of interest beyond the user’s immediate social network. Although there is much recent work focusing on the task of multi-tweet summarization (Becker et al., 2011; Inouye and Kalita, 2011; Zubiaga et al., 2012; Liu et al., 2011a; Takamura et al., 2011; Harabagiu and Hickl, 2011; Wei et al., 2012), most previous work relies only on surface lexical clues, redundancy and social network specific signals (e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natura"
W13-1103,P11-1037,0,0.32789,"er of messages, many containing irrelevant and redundant information, quickly leads to a situation of information overload. This motivates the need for automatic summarization systems which can select a few messages for presentation to a user which cover the most important information relating to the event without redundancy and filter out irrelevant and personal information that is not of interest beyond the user’s immediate social network. Although there is much recent work focusing on the task of multi-tweet summarization (Becker et al., 2011; Inouye and Kalita, 2011; Zubiaga et al., 2012; Liu et al., 2011a; Takamura et al., 2011; Harabagiu and Hickl, 2011; Wei et al., 2012), most previous work relies only on surface lexical clues, redundancy and social network specific signals (e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natura"
W13-1103,P12-3003,0,0.028684,"generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to extract named entities and events in the tweets as has been shown for classic multi-document summarization (Li et al., 2006). Recent progress on building NLP tools for Twitter (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2011b; Ritter et al., 2012; Liu et al., 2012) makes it possible to investigate an approach to summarizing Twitter events which is based on Information Extraction techniques. We investigate a graph-based approach which leverages named entities, event phrases and their connections across tweets. A similar idea has been studied by Li et al. (2006) to rank the salience of event concepts in summarizing news articles. However, the extreme redundancy and simplicity of tweets allows us to explicitly split the event graph into subcomponents that cover various aspects of the initial event to be summarized to create comprehen20 Proceedings of the W"
W13-1103,P00-1010,0,0.136012,"epresent the relationship between them. We then rank and partition the events using PageRank-like algorithms, and create summaries of variable length for different topics. 3.1 Event Extraction from Tweets As a first step towards summarizing popular events discussed on Twitter, we need a way to identify events from Tweets. We utilize several natural language processing tools that specially developed for noisy text to extract text phrases that bear essential event information, including named entities (Ritter et al., 2011), event-referring phrases (Ritter et al., 2012) and temporal expressions (Mani and Wilson, 2000). Both the named entity and event taggers utilize Conditional Random Fields models (Lafferty, 2001) trained on annotated data, while the temporal expression resolver uses a mix of hand-crafted and machine-learned rules. Example event information extracted from Tweets are presented in Table 2. The self-contained nature of tweets allows efficient extraction of event information without deep analysis (e.g. co-reference resolution). On the other hand, individual tweets are also very terse, often lacking sufficient context to access the importance of events. It is crucial to exploit the highly redu"
W13-1103,radev-etal-2004-mead,0,0.0270509,"event cluster with a single but complex focus 25 4.2 Figure 4: Event graph of ’West Ham - 1/16/2013’, an example of event cluster with a single focus Baseline SumBasic (Vanderwende et al., 2007) is a simple and effective summarization approach based on term frequency, which we use as our baseline. It uses word probabilities with an update function to avoid redundancy to select sentences or posts in a social media setting. It is shown to outperform three other well-known multi-document summarization methods, namely LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004) and MEAD (Radev et al., 2004) on tweets in (Inouye and Kalita, 2011), possibly because that the relationship between tweets is much simpler than between sentences in news articles and can be well captured by simple frequency methods. The improvement over the LexRank model on tweets is gained by considering the number of retweets and influential users is another side-proof (Wei et al., 2012) of the effectiveness of frequency. Annotator 1 1 https://github.com/aritter/twitter_nlp 26 3 2 1 0 EventRank−Fixed SumBasic Annotator 2 2 3 4 5 compactness completeness overall 1 For each cluster, our systems produce two versions of su"
W13-1103,D11-1141,1,0.233403,"ted advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to extract named entities and events in the tweets as has been shown for classic multi-document summarization (Li et al., 2006). Recent progress on building NLP tools for Twitter (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2011b; Ritter et al., 2012; Liu et al., 2012) makes it possible to investigate an approach to summarizing Twitter events which is based on Information Extraction techniques. We investigate a graph-based approach which leverages named entities, event phrases and their connections across tweets. A similar idea has been studied by Li et al. (2006) to rank the salience of event concepts in summarizing news articles. However, the extreme redundancy and simplicity of tweets allows us to explicitly split the event graph into subcomponents that cover various aspects"
W13-1103,C12-1047,0,0.0630086,"quickly leads to a situation of information overload. This motivates the need for automatic summarization systems which can select a few messages for presentation to a user which cover the most important information relating to the event without redundancy and filter out irrelevant and personal information that is not of interest beyond the user’s immediate social network. Although there is much recent work focusing on the task of multi-tweet summarization (Becker et al., 2011; Inouye and Kalita, 2011; Zubiaga et al., 2012; Liu et al., 2011a; Takamura et al., 2011; Harabagiu and Hickl, 2011; Wei et al., 2012), most previous work relies only on surface lexical clues, redundancy and social network specific signals (e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to ext"
W13-1103,W04-3252,0,\N,Missing
W14-6002,W10-1833,0,0.0163904,", 2003) refers to keyword search terms (microarray, potato, genetic algorithm), single or multi-word (mostly nominal) expressions collectively representing topics of documents that contain them. These same terms are also used for creating domain-specific thesauri and ontologies (Velardi et al., 2001). We will refer to these types of terms as topic-terms and this type of terminology topic-terminology. In other work, types of terminology (genes, chemical names, biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al., 2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may not be terminology-specific, but have terms as arguments, e.g., (Schwartz and Hearst, 2003; Jin et al., 2013) detect abbreviation and definition relations respectively and the arguments are terms. In contrast to this previous work, we have built a system that extracts a larger set of terminology, which we call jargon-terminology"
W14-6002,W07-1008,0,0.0273224,"cquemin and Bourigault, 2003) refers to keyword search terms (microarray, potato, genetic algorithm), single or multi-word (mostly nominal) expressions collectively representing topics of documents that contain them. These same terms are also used for creating domain-specific thesauri and ontologies (Velardi et al., 2001). We will refer to these types of terms as topic-terms and this type of terminology topic-terminology. In other work, types of terminology (genes, chemical names, biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al., 2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may not be terminology-specific, but have terms as arguments, e.g., (Schwartz and Hearst, 2003; Jin et al., 2013) detect abbreviation and definition relations respectively and the arguments are terms. In contrast to this previous work, we have built a system that extracts a larger set of terminology, which we cal"
W14-6002,E06-1051,0,0.0371738,"ng topics of documents that contain them. These same terms are also used for creating domain-specific thesauri and ontologies (Velardi et al., 2001). We will refer to these types of terms as topic-terms and this type of terminology topic-terminology. In other work, types of terminology (genes, chemical names, biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al., 2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may not be terminology-specific, but have terms as arguments, e.g., (Schwartz and Hearst, 2003; Jin et al., 2013) detect abbreviation and definition relations respectively and the arguments are terms. In contrast to this previous work, we have built a system that extracts a larger set of terminology, which we call jargon-terminology. Jargon-terms may include ultracentrifuge, which is unlikely to be a topic-term of a current biology article, but will not include potato, a non-technical"
W14-6002,C92-2082,0,0.172001,"teA1 , also known as CH3-(CH2)20COOAgA2 ”, the chemical name establishes that this substance is a salt, whereas the formula provides the proportions of all its constituent elements; (2) ORIGINATE, the relation between an ARG1 (person, organization or document) and an ARG2 (a term), such that the ARG1 is an inventor, discoverer, manufacturer, or distributor of the ARG2 and some of these roles are differentiated as subtypes of the relation. Examples include the following: “EagleA1 ’s minimum essential mediaA2 and DOPGA2 was obtained from Avanti Polar LipidsA1 ”. (3) EXEMPLIFY, an IS-A relation (Hearst, 1992) between terms so that ARG1 is an instance of ARG2, e.g., “CytokinesA2 , for instance interferonA1 ”; and “proteinsA2 such as insulinA1 ”; (4) CONTRAST relations, e.g., “necrotrophic effector systemA1 that is an exciting contrast to the biotrophic effector modelsA2 ”; (5) BETTER THAN relations, e.g., “Bayesian networksA1 hold a considerable advantage over pairwise association testsA2 ”; and (6) SIGNIFICANT relations, e.g., “Anaerobic SBsA2 are an emerging area of research and development” (ARG1, the author of the article, is implicit). These relations are applicable to most technical genres. 7"
W14-6002,D13-1073,0,0.127817,"logy topic-terminology. In other work, types of terminology (genes, chemical names, biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al., 2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may not be terminology-specific, but have terms as arguments, e.g., (Schwartz and Hearst, 2003; Jin et al., 2013) detect abbreviation and definition relations respectively and the arguments are terms. In contrast to this previous work, we have built a system that extracts a larger set of terminology, which we call jargon-terminology. Jargon-terms may include ultracentrifuge, which is unlikely to be a topic-term of a current biology article, but will not include potato, a non-technical word that could be a valid topic-term. We aim to find all the jargon-terms found in a text, not just the ones that fill slots for specific relations. As we show, jargon-terminology closely matches the notional (e.g., Webste"
W14-6002,meyers-etal-2004-cross,1,0.580909,"ules out tokens beginning with numbers that include letters; tokens including plus signs, ampersands, subscripts, superscripts; tokens containing no alphanumeric characters at all, etc. 6. J must not contain a word that is a member of a list of common patent section headings. Secondly, a jargon-term J must satisfy at least one of the following additional conditions: 1. J = highly ranked topic-term or a substring of J is a highly ranked topic-term. 2. J contains at least one O-NOUN. 3. J consists of at least 4 words, at least 3 of which are either nominalizations (C-NOUNs found in NOMLEX-PLUS (Meyers et al., 2004; Meyers, 2007)) or TECH-ADJs. 4. J = nominalization at least 11 characters long. 5. J = multi-word ending in a common noun and containing a nominalization. A final stage aims to distinguish named entities from jargon-terms. It turns out that named entities, like jargon terms, include many out of vocabulary words. Thus we look for NEs among those PJs that remain after stage 3 and contain capitalized words (a single capital letter followed by lowercase letters). These NE filters are based on manually collected lists of named entities and nationality adjectives, as well as common NE endings. Dic"
W14-6002,meyers-etal-2014-annotating,1,0.834118,"also inaccurate at determining the extent of terms. Baseline 2 restricts the noun groups from this same chunker to those with O-NOUN heads. This improves the precision at a high cost to recall. Similarly, we first ran our system without filtering the potential jargon-terms, and then we ran the full system. Clearly our more complex strategy performs better than these baselines and the linguistic filters increase precision more than they reduce recall, resulting in higher F-measures (though low-precision high-recall output may be better for some applications). 17 6 Relations with Jargon-Terms (Meyers et al., 2014) describes the annotation of 200 PubMed articles from and 26 patents with several relations, as well as a system for automatically extracting relations. It turned out that the automatic system depended on the creation of a jargon-term extraction system and thus that work was the major motivating factor for the research described here. Choosing topic-terms as potential arguments would have resulted in low recall. In contrast, allowing any noun-group to be an argument would have lowered precision, e.g., diagram, large number, accordance and first step are unlikely to be valid arguments of relati"
W14-6002,J04-2002,0,0.0345046,"eativecommons.org/licenses/by/4.0/ 11 Proceedings of SADAATL 2014, pages 11–20, Dublin, Ireland, August 24, 2014. This paper describes a system for extracting jargon-terms in technical documents (patents and journal articles); the evaluation of this system using manually annotated documents; and a set of information extraction (IE) relations which take jargon-terms as arguments. We incorporate previous work in terminology extraction, assuming that terminology is restricted to noun groups (minus some left modifiers) (Justeson and Katz, 1995); 1 and we use both topic-term extraction techniques (Navigli and Velardi, 2004) and relation-based extraction techniques (Jin et al., 2013) in components of our system. Rather than looking at the distribution of noun groups as a whole for determining term-hood, we refine the classes used by the noun group chunker itself, placing limitations on the candidate noun groups proposed and then filtering the output by setting thresholds on the number and quality of the “jargon-like” components of the phrase. The resulting system admits not only topic-terms, but also other non-topic instances of terminology. Using the more inclusive set of jargon-terms (rather than just topic-ter"
W14-6002,W95-0107,0,0.276741,"nd thus provide a smaller search space). In the second stage, potential jargon-term (PJs) are generated by processing tokens from left to right and classifying them using a finite state machine (FSM). The third stage filters the PJs generated with a set of manually constructed constraints, yielding a set of jargon-terms. A final filter (stage 4) identifies named entities and separates them out from the true jargon-terms: it turns out that many named entities have similar phrase-internal properties as jargon-terms. The FSM (that generates PJs) in the second stage includes the following states (Ramshaw and Marcus, 1995): START (S) (marking the beginning of a segment), Begin Term (B-T), Inside Term (I-T), End Term (E-T), and Other (O). A PJ is a sequence consisting of: (a) a single E-T; or (b) exactly one B-T, followed by zero or more instances of I-T, followed by zero or one instances of E-T. Each transition to a new state is conditioned on: (a) the (extended) POS tag of the current word; (b) the extended POS tag of the previous word; and (c) the previous state. The extended POSs are derived from the output of a Penn-Treebank-based POS tagger and refinements based on machine readable dictionaries, including"
W14-6002,W03-1805,0,0.0131788,"a wide variety of genres (unlike those in (BioCreAtIvE, 2006)) – a genre-neutral definition of terminology makes this possible. For example, the CONTRAST relation between the two bold face terms in necrotrophic effector systemA1 that is an exciting contrast to the biotrophic effector modelsA2 . would be applicable in most academic genres. Our jargon-terms also contrast with the tactic of filling terminology slots in relations with any noun-group (Justeson and Katz, 1995), as such a strategy overgenerates, lowering precision. 2 Topic-term Extraction Topic-term extractors (Velardi et al., 2001; Tomokiyo and Hurst, 2003) collect candidate terms (N-grams, noun groups, words) that are more representative of a foreground corpus (documents about a specific topic) than they are of a background corpus (documents about a wide range of topics), using statistical F requency measures such as InverseT erm Document F requency (TFIDF), or a variation thereof. Due to the metrics used and cutoffs assumed, the list of terms selected is usually no more than a few hundred distinct terms, even for a large set of foreground documents and tend to be especially salient to that topic. The terms can be phrases that lay people would"
W14-6002,W01-1005,0,0.31008,"terms and usages appropriate to a particular field, subject, science, or art. Systems for automatically extracting instances of terminology (terms) usually assume narrow operational definitions that are compatible with particular tasks. Terminology, in the context of Information Retrieval (IR) (Jacquemin and Bourigault, 2003) refers to keyword search terms (microarray, potato, genetic algorithm), single or multi-word (mostly nominal) expressions collectively representing topics of documents that contain them. These same terms are also used for creating domain-specific thesauri and ontologies (Velardi et al., 2001). We will refer to these types of terms as topic-terms and this type of terminology topic-terminology. In other work, types of terminology (genes, chemical names, biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al., 2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may not be termin"
W98-0604,M95-1014,1,0.820264,"position) Introduction Although, nominalizationQ are very common in written text, the computational linguistics literature provides few systematic accounts of how to deal with phrases containing these words. This paper focuses on this problem in the context of Information Extraction (IE). 2 Many extraction systems use either parsing combined with some form of syntactic regularization, or a meta-rule mechanism to automatically match variants of clausal syntactic structures (active main clause, passive, relative clause etc.), e.g., FASTUS (Appelt et al., 1995) and the Proteus Extraction System (Grishman, 1995). However, this mechanism does not extend to nominalization patterns, which must be coded separately from the clausal patterns. NOMLEX, a dictionary of nominalizations currently under development at NYU, (Macleod et al., 1997) provides a way to handle nominalizations more automatically, and with INominalizations are nouns which are related to words of another part of speech, most commonly verbs. In this paper, only verbal nominalizatious will be discussed. 2The Message Understanding Colfference Scenario Template Task (MUC, 1995), (MUC, 1998) is ore&quot; model for the kind of information that we ar"
W98-0604,P87-1019,0,\N,Missing
