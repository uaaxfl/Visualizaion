1992.tc-1.14,J90-3001,1,0.812324,"ext is analyzed into a representation (“Quasi Logical Form”, or QLF), which has been carefully designed so as to represent exactly the aspects of linguistic meaning which do not involve context or “common-sense” knowledge. The source QLF representation is transferred into a target counterpart, from which target-language generation is used to produce the target text. In other applications, such as NL query interfaces to databases, the QLF representation would be subjected to further phases of processing; contextually determined factors, such as the referents of pronouns may be added, (Alshawi, (1)), and vague linguistic predicates replaced by more precisely defined relations (Rayner and Alshawi (9)). However, our hypothesis has been that a useful translation can be obtained by performing transfer directly on QLFs, when necessary dealing with problems of contextual interpretation by querying the user. These questions are phrased in such a way as to assume no knowledge on the source-user’s part of either linguistics or the target language. Our judgement, based on the experience gained during the first year of the project, is that QLF-based transfer successfully circumvents many of the di"
1992.tc-1.14,P91-1021,1,0.851495,"Missing"
1992.tc-1.14,P88-1019,0,0.025275,"ic representation. Knowledge-based interlingua-based systems, in contrast, perform translation in two stages: the source text is reduced to a language-independent intermediate representation, and the target text is generated from it directly. Very few systems are of course completely pure examples of either approach; in particular, many architectures based on syntactic transfer also employ some interlingual semantic ideas, of which the most important is usually a version of 1 A similar, though less sophisticated, system for translation between Japanese and English is reported in (Miike et al, (7)) 167 case-grammar. This does not, however, substantially affect the following discussion. On the positive side, syntactic transfer is the easier alternative to implement, since the techniques of syntactic analysis (and to a lesser extent generation) are well-understood and relatively straightforward. However, the fact that different languages use widely different syntactic forms places a great burden on the transfer component, which becomes correspondingly more complex and harder to understand. To take an example from the English-Swedish language-pair: although the structures of He hired a ca"
1992.tc-1.14,A92-1001,1,0.796087,"so as to represent exactly the aspects of linguistic meaning which do not involve context or “common-sense” knowledge. The source QLF representation is transferred into a target counterpart, from which target-language generation is used to produce the target text. In other applications, such as NL query interfaces to databases, the QLF representation would be subjected to further phases of processing; contextually determined factors, such as the referents of pronouns may be added, (Alshawi, (1)), and vague linguistic predicates replaced by more precisely defined relations (Rayner and Alshawi (9)). However, our hypothesis has been that a useful translation can be obtained by performing transfer directly on QLFs, when necessary dealing with problems of contextual interpretation by querying the user. These questions are phrased in such a way as to assume no knowledge on the source-user’s part of either linguistics or the target language. Our judgement, based on the experience gained during the first year of the project, is that QLF-based transfer successfully circumvents many of the difficulties that arise using pure transfer or interlingua methods; it manages to factor out the problems"
1992.tc-1.14,H91-1015,0,\N,Missing
1995.tc-1.11,J94-4005,1,0.89368,"Missing"
1995.tc-1.11,H90-1021,0,0.204956,". There are now a number of high-profile projects with large budgets, the most well-known being the German Verbmobil effort. At the moment, the best systems are at the level of advanced prototypes; making projections from current performance, it seems reasonable to hope that these could be developed into commercially interesting systems within a time-scale of about another five years. This paper describes a project centered around one such advanced prototype, the Spoken Language Translator (SLT) system. SLT can translate spoken English utterances from the domain of air travel planning (ATIS; (Hemphill et al., 1990)) into spoken Swedish or French, using a vocabulary of about 1200 stem entries. Table 1 shows some examples of typical sentences from this domain, together with French translations produced by the system. The Swedish version of SLT has been operational since June 1993, and has been publicly demonstrated on numerous occasions. The French version became operational fairly recently, and was publicly demonstrated for the first time at the Language Engineering Convention in London in October 1995. An initial French-to-Spanish version of the system also exists. By the middle of 1996, we expect to ha"
1995.tc-1.11,H93-1042,1,0.83877,"Missing"
1995.tc-1.11,H94-1040,1,0.83469,"Missing"
2004.tmi-1.3,C02-1095,0,0.0922259,"ss. In the other direction, academics have become more interested in grammar-based methods. The literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al. 1999, Rayner et. al. 2000, Lemon et al. 2001, Rayner et. al. 2001a). Much of this work has involved the idea of compiling grammar-based language models out of descriptions written in higher-level formalisms, in particular unification grammars (Moore 1998, Kiefer & Krieger 2000, Dowding et al. 2001, Rayner et al. 2001b, Bos 2002). Given that a great deal of practical and theoretical work is going on using both statistical and rule-based methods, it is remarkable that there is almost no reported work attempting a systematic comparison of the two approaches. The only example known to us is (Knight et al. 2001), in which one of the present authors participated. In this study, two speech understanding systems were constructed for the same domain, a medium-vocabulary command and control task. Both systems ran on the Nuance 7 platform. The first had a hand-coded grammar-based language model (GLM), compiled using the standar"
2004.tmi-1.3,P01-1022,1,0.849306,"ce&apos;s SayAnythingTM module, with some success. In the other direction, academics have become more interested in grammar-based methods. The literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al. 1999, Rayner et. al. 2000, Lemon et al. 2001, Rayner et. al. 2001a). Much of this work has involved the idea of compiling grammar-based language models out of descriptions written in higher-level formalisms, in particular unification grammars (Moore 1998, Kiefer & Krieger 2000, Dowding et al. 2001, Rayner et al. 2001b, Bos 2002). Given that a great deal of practical and theoretical work is going on using both statistical and rule-based methods, it is remarkable that there is almost no reported work attempting a systematic comparison of the two approaches. The only example known to us is (Knight et al. 2001), in which one of the present authors participated. In this study, two speech understanding systems were constructed for the same domain, a medium-vocabulary command and control task. Both systems ran on the Nuance 7 platform. The first had a hand-coded grammar-based language model ("
2004.tmi-1.3,2000.iwpt-1.15,0,0.00995447,"deling tools, like Nuance&apos;s SayAnythingTM module, with some success. In the other direction, academics have become more interested in grammar-based methods. The literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al. 1999, Rayner et. al. 2000, Lemon et al. 2001, Rayner et. al. 2001a). Much of this work has involved the idea of compiling grammar-based language models out of descriptions written in higher-level formalisms, in particular unification grammars (Moore 1998, Kiefer & Krieger 2000, Dowding et al. 2001, Rayner et al. 2001b, Bos 2002). Given that a great deal of practical and theoretical work is going on using both statistical and rule-based methods, it is remarkable that there is almost no reported work attempting a systematic comparison of the two approaches. The only example known to us is (Knight et al. 2001), in which one of the present authors participated. In this study, two speech understanding systems were constructed for the same domain, a medium-vocabulary command and control task. Both systems ran on the Nuance 7 platform. The first had a hand-coded grammar-b"
2004.tmi-1.3,P03-2024,1,0.63371,"slated by the system, and the patient responds nonverbally, for example by nodding or shaking their head, or pointing. The key requirement of the project is a high level of accuracy: doctors are only interested in using systems they can trust. Since speech recognition can never be wholly reliable, the user interface is structured so that the initial recognition hypothesis is echoed back to the user, who has the option to abort further processing if necessary. Reliability thus means reliability on the utterances which the user considers to be correctly recognized. The current prototype system (Rayner et al. 2003) translates questions in a headache domain from English into Japanese or French, using a vocabulary of about 200 words; a production version would need to cover at least another 25 to 50 similar subdomains. The recognition component for the SLM and GLM version was built with a training corpus of 450 utterances. The initial set of training utterances were supplied to us by a physician, who then interacted with us to expand it by adding enough synonyms and alternative phrasings to make the coverage reasonably habitable. Later versions may use larger development sets, but it is unreasonable to as"
2004.tmi-1.3,E03-2010,1,0.743526,"slated by the system, and the patient responds nonverbally, for example by nodding or shaking their head, or pointing. The key requirement of the project is a high level of accuracy: doctors are only interested in using systems they can trust. Since speech recognition can never be wholly reliable, the user interface is structured so that the initial recognition hypothesis is echoed back to the user, who has the option to abort further processing if necessary. Reliability thus means reliability on the utterances which the user considers to be correctly recognized. The current prototype system (Rayner et al. 2003) translates questions in a headache domain from English into Japanese or French, using a vocabulary of about 200 words; a production version would need to cover at least another 25 to 50 similar subdomains. The recognition component for the SLM and GLM version was built with a training corpus of 450 utterances. The initial set of training utterances were supplied to us by a physician, who then interacted with us to expand it by adding enough synonyms and alternative phrasings to make the coverage reasonably habitable. Later versions may use larger development sets, but it is unreasonable to as"
2004.tmi-1.3,W00-0311,1,0.879114,"Missing"
2004.tmi-1.3,P99-1024,0,0.0383449,"language interface for a new application in a timely fashion, rule-based methods are often the only practicable alternative. Over the last few years, however, the above picture has become more blurred. Commercial platform vendors have begun to introduce statistical modeling tools, like Nuance&apos;s SayAnythingTM module, with some success. In the other direction, academics have become more interested in grammar-based methods. The literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al. 1999, Rayner et. al. 2000, Lemon et al. 2001, Rayner et. al. 2001a). Much of this work has involved the idea of compiling grammar-based language models out of descriptions written in higher-level formalisms, in particular unification grammars (Moore 1998, Kiefer & Krieger 2000, Dowding et al. 2001, Rayner et al. 2001b, Bos 2002). Given that a great deal of practical and theoretical work is going on using both statistical and rule-based methods, it is remarkable that there is almost no reported work attempting a systematic comparison of the two approaches. The only example known to us is (Knight et"
2005.eamt-1.8,A97-1001,0,0.0737007,"Missing"
2005.eamt-1.8,W02-0710,1,0.794173,"Missing"
2005.eamt-1.8,P03-2024,1,0.844535,"Missing"
2005.eamt-1.8,2005.eamt-1.8,1,0.106122,"Missing"
2005.eamt-1.8,2004.tmi-1.3,1,\N,Missing
2005.jeptalnrecital-long.17,H01-1007,0,0.0462992,"Missing"
2005.jeptalnrecital-long.17,2004.tmi-1.3,1,0.912361,"Missing"
2005.jeptalnrecital-long.17,P03-2024,1,0.927157,"Missing"
2005.jeptalnrecital-long.17,E03-2010,1,0.889795,"Missing"
2005.mtsummit-papers.25,E03-2010,1,0.865288,"Missing"
2005.mtsummit-papers.25,2005.eamt-1.8,1,0.80735,"ality of pain, and the factors that increase or decrease the pain. The answers to these questions can be successfully communicated by a limited number of one or two word responses (e.g. yes/no, left/right, numbers) or even gestures (e.g. nodding or shaking the head, pointing to an area of the body). Translation can thus be unidirectional. In order to obtain an accurate translation, the system uses a grammar-based speech recogniser. For this type of application a grammar-based approach appears to give better results than a statistical-based recognition (Knight et al., 2001, Rayner et al. 2004, Bouillon et al. 2005). Diagnosis seems to be a very convergent sublanguage, where it is possible to guess the syntactic structures that a doctor will use, and thus to describe them in a grammar. The advantage of this approach is that the grammar enforces more global constraints on the recognized utterance than the simple bigrams or trigrams of a statistical language model: more complete sentences are thus well recognized, which improves the translation. The drawback is the lack of robustness: if the sentence structure is not in the grammar or if a word is not in the lexicon, the recognition completely fails. Helpi"
2005.mtsummit-papers.25,lavie-etal-2002-nespole,0,0.0623047,"Missing"
2005.mtsummit-papers.25,W02-0710,1,\N,Missing
2005.mtsummit-papers.25,2004.tmi-1.3,1,\N,Missing
2006.jeptalnrecital-long.6,P01-1022,0,0.0712505,"Missing"
2006.jeptalnrecital-long.6,J81-4003,0,0.605542,"Missing"
2006.jeptalnrecital-long.6,2005.jeptalnrecital-long.17,1,0.869615,"Missing"
2007.jeptalnrecital-poster.5,W06-3703,0,0.172189,"Missing"
2007.jeptalnrecital-poster.5,W02-0203,0,0.0701878,"Missing"
2007.jeptalnrecital-poster.5,W06-3711,0,0.0584704,"Missing"
2007.jeptalnrecital-poster.5,2005.jeptalnrecital-long.17,1,0.888808,"Missing"
2007.jeptalnrecital-poster.5,W06-3701,0,0.053261,"Missing"
2007.jeptalnrecital-poster.5,2005.mtsummit-papers.25,1,0.621309,"Missing"
2008.amta-govandcom.4,2005.eamt-1.8,1,0.880038,"ation systems. An ideal system would be able to interpret accurately and flexibly between patients and health care professionals, using unrestricted language and a large vocabulary. Providing functionality of this kind is, unfortunately, well beyond the current state of the art. Although it goes without saying that we are in favour of continued research towards this highly desirable goal, it is also worth thinking about what can be achieved in terms of building systems now, or in the immediate future, which will already be concretely useful. This paper describes the current version of MedSLT (Bouillon et al., 2005), an Open Source medical speech translation system for doctor-patient examination dialogues aimed at short-term deployment. We start by reviewing the basic goals and constraints. The fundamental issue is reliability. In conversations with medical professionals, it has been made clear to us on numerous occasions that doctors are only interested in translation systems which are extremely reliable. At the recent workshop on medical and safety-critical translation (Bouillon et al., 2008a), the question of evaluation metrics for doctor/patient communication applications was considered during the pa"
2008.amta-govandcom.4,bouillon-etal-2008-developing,1,0.824913,"immediate future, which will already be concretely useful. This paper describes the current version of MedSLT (Bouillon et al., 2005), an Open Source medical speech translation system for doctor-patient examination dialogues aimed at short-term deployment. We start by reviewing the basic goals and constraints. The fundamental issue is reliability. In conversations with medical professionals, it has been made clear to us on numerous occasions that doctors are only interested in translation systems which are extremely reliable. At the recent workshop on medical and safety-critical translation (Bouillon et al., 2008a), the question of evaluation metrics for doctor/patient communication applications was considered during the panel discussion; the consensus view reached was that a metric which conforms to most physician’s intuitions should give a negative score for seriously incorrect translations somewhere around 25 to 100 times the positive score for a correct translation. Given the usual tradeoff between precision and recall, this implies that the dial needs to be moved heavily towards the “precision” end. This top-level design decision has several implications. Although data-driven architectures for sp"
2008.amta-govandcom.4,W06-3702,1,0.922411,"Missing"
2008.amta-govandcom.4,W06-3703,0,0.0537955,"Missing"
2008.amta-govandcom.4,W06-3711,0,0.0611898,"Missing"
2008.amta-govandcom.4,P05-3008,1,0.859918,"cal speech translation systems. The world’s current population of 6.6 314 [8th AMTA conference, Hawaii, 21-25 October 2008] This is reflected in the development methodologies, which typically involve optimising BLEU, or a similar surface-oriented metric. It is not at all clear that this kind of approach is appropriate for the kind of high-precision translation required here; the comparative studies that have been carried out suggest that rule-based architectures offer considerable higher accuracy, especially when the system is designed for use by experts, such as doctors (Knight et al., 2001; Rayner et al., 2005a; Lee and Seneff, 2005). These users can both learn the system’s intended coverage, and also adapt to an interface which gives them feedback on what the system has understood, allowing them to abort incorrect speech processing without producing a translation. Thus our first conclusion is that rule-based architectures are, at least at the moment, the most suitable ones for the doctor-patient examination dialogue task. For similar reasons, it appears right now difficult to build a useful bidirectional translation system for this kind of domain. It is in principle highly desirable to support two"
2008.amta-govandcom.4,C08-1090,1,0.878983,"Missing"
2008.amta-govandcom.4,2005.mtsummit-papers.25,1,0.850558,"Missing"
2008.amta-govandcom.4,tsourakis-etal-2008-building,1,0.748812,"Missing"
2008.amta-govandcom.4,N03-4015,0,0.0731748,"Missing"
2008.eamt-1.24,bouillon-etal-2008-developing,1,0.862267,"Missing"
2008.eamt-1.24,2005.eamt-1.8,1,0.901192,"Missing"
2008.eamt-1.24,2006.jeptalnrecital-long.6,1,0.8399,"Missing"
2008.eamt-1.24,2007.jeptalnrecital-poster.5,1,0.769435,"Missing"
2008.eamt-1.24,W06-3706,0,0.0315393,"Missing"
2008.eamt-1.24,W06-3703,0,0.0434588,"Missing"
2008.eamt-1.24,N04-3010,0,0.0304759,"Missing"
2008.eamt-1.24,2005.mtsummit-papers.25,1,\N,Missing
2011.freeopmt-1.5,P03-1021,0,0.005876,"Missing"
2011.freeopmt-1.5,P00-1056,0,0.061546,"Missing"
2011.freeopmt-1.5,2010.eamt-1.39,1,0.889465,"the rule-based omponents and the orpus data used to onstru t them, and then use the same resour es, together with mainstream tools, to bootstrap statisti al proF. S´ anchez-Mart´ınez, J.A. P´ erez-Ortiz (eds.) Proceedings of the Second International Workshop on Free/Open-Source Rule-Based Machine Translation, p. 21–28 Barcelona, Spain, January 2011. http://hdl.handle.net/10609/5647 essing omponents. In (Ho key et al., 2008), we adapted and improved methods originally des ribed in (Jurafsky et al., 1995) to bootstrap a statisti al re ogniser from the original rule-based one. More re ently, in (Rayner et al., 2010) we used similar methods to bootstrap statisti al ma hine translation models. In this urrent paper, we ombine the results of the previous two sets of experiments to build a fully bootstrapped statisti al spee h translation system, whi h we then ompare with the original rule-based one, and also with a hybrid system whi h ombines rule-based and statisti al pro essing. Interestingly, although (Rayner et al., 2010) demonstrated that a bootstrapped statisti al ma hine translation system is able to add substantial robustness to the original rule-based one when both are run on text data, this robustn"
2012.amta-papers.25,P07-2045,0,0.0030871,"nnotator agreement figures we present below suggests that judges were pleased with the conditions offered and worked conscientiously. Restricting judges to a bilingual country appears to be important. We tried removing this condition, and obtained faster turnaround time but much poorerquality results, with weak inter-annotator agreement and many anomalous judgements suggesting that judges lacked fluency in one or the other language or were not taking the job seriously. 3.2 Training Data and SMT Systems The SMT baseline system was a phrase-based system trained with the standard Moses pipeline (Koehn et al., 2007), using GIZA++ (Och and Ney, 2000) for word alignment and SRILM (Stolcke, 2002) for the estimation of 5-gram Kneser-Ney smoothed (Kneser and Ney, 1995) language models. For training the translation and lexicalised reordering models we used the releases of europarl and news-commentary provided for the WMT12 shared task (Callison-Burch et al., 2012), together with a dataset from the ACCEPT project consisting mainly of technical product manuals and marketing materials. This last data set covers the same topics as the forums we wish to translate (so it may be considered as “in-domain”) but it is a"
2012.amta-papers.25,2005.mtsummit-papers.11,0,0.0214548,"ges. In this paper, we will only consider the automatic stages of the translation process in the French-toEnglish translation pair; we wish to translate French forum data for the benefit of English-speaking users. This rapidly exposes a mismatch between training and test data at the level of register. Forum posts are typically informal in tone. The vast majority of available aligned French/English training data is however formal: a typical example, which we will use in the rest of the paper as our primary resource, is the proceedings of the European parliament, the ubiquitous Europarl corpus (Koehn, 2005). Similar problems would have arisen if we had used other corpora, e.g. the UN corpus2 , Callison-Burch’s giga corpus3 or the Canadian Hansard corpus4 . French is a language where the gap between formal and informal usage is large. (For purposes of comparison, it is much larger than in English, though perhaps not as large as in Arabic). We will focus on two immediate problems, verb forms and questions. French, like most European languages (English is the major exception) has two secondperson pronouns, the formal vous and the informal tu (accusative form te, elided to t’ before a vowel). Each p"
2012.amta-papers.25,J04-2003,0,0.0997549,"Missing"
2012.amta-papers.25,P00-1056,0,0.10894,"nt below suggests that judges were pleased with the conditions offered and worked conscientiously. Restricting judges to a bilingual country appears to be important. We tried removing this condition, and obtained faster turnaround time but much poorerquality results, with weak inter-annotator agreement and many anomalous judgements suggesting that judges lacked fluency in one or the other language or were not taking the job seriously. 3.2 Training Data and SMT Systems The SMT baseline system was a phrase-based system trained with the standard Moses pipeline (Koehn et al., 2007), using GIZA++ (Och and Ney, 2000) for word alignment and SRILM (Stolcke, 2002) for the estimation of 5-gram Kneser-Ney smoothed (Kneser and Ney, 1995) language models. For training the translation and lexicalised reordering models we used the releases of europarl and news-commentary provided for the WMT12 shared task (Callison-Burch et al., 2012), together with a dataset from the ACCEPT project consisting mainly of technical product manuals and marketing materials. This last data set covers the same topics as the forums we wish to translate (so it may be considered as “in-domain”) but it is almost exclusively in the formal re"
2014.lilt-10.2,baur-etal-2014-using,1,0.532415,"tested in conjunction with one of their courses (Bouillon et al., 2011a); the topics covered included “greeting”, “talking about my family” and “scheduling a meeting”. A modified and abbreviated version of this course was adapted for use on mobile devices, and also tested over AMT; this work is described in detail in the next section. Most recently, we have been developing an interactive multimedia course for teaching English to beginner German-speaking school students; some examples are shown earlier in §2.5. A first evaluation with real students was carried out in late 2013 and early 2014 (Baur et al., 2014, Tsourakis et al., 2014). In the next section, we describe in detail some of the more substantial evaluations we have carried out using the French L2 courses. 4 Evaluations The central question we consider is an apparently simple one: does the system help students improve their generative language skills? Here, we summarise from this point of view the results of three concrete experiments carried out between 2011 and 2013. One-day experiment using French/Chinese system A typical early study was the one described in (Bouillon et al., 2011b), where the subjects were 10 Chinese-speaking computer"
2014.lilt-10.2,fuchs-etal-2012-scalable,1,0.800779,"g dialogue management, application integration, and large-scale grammar-based language processing on the server, rather than just returning the results of recognition to the client. Another important difference is that speech is passed to the recognition processes in the form of files, rather than using streaming audio. Although this goes against the currently prevailing wisdom, we have found that there are compensating advantages, and that the performance hit, with a little care, can be reduced to only a couple of hundred milliseconds per recognition operation. Full details are presented in (Fuchs et al., 2012). By moving almost all processing to the server, the client can be kept very simple. It only needs to be responsible for the graphical user interface, maintaining a small amount of state logic, performing recording and playback of audio, and requesting services from the remote peer. Versions of the client for standard browsers have been developed using Flash 11 in combination with ActionScript 3.0. An important aspect of the GUI is the way the recognition button is used. Due to the limitations of the target platform (lack of an endpointing mechanism), we have adopted a push-and-hold solution,"
2014.lilt-10.2,rayner-etal-2012-evaluating,1,0.886137,"driven by small corpora typically containing a few hundred examples. The scheme is explained in detail in (Rayner et al., 2006), which also includes a thorough description of the English resource grammar. Similar grammars have since been developed for French, German and Japanese. These have been further extended to cover related languages by a parameterization process. In particular, the French grammar has been extended into a shared grammar which covers French, Spanish and Catalan (Bouillon et al., 2007), and the English grammar has been similarly extended to cover both English and Swedish (Rayner et al., 2012b). The Regulus resource grammars are also parameterized to support 1 www.nuance.com CALL-SLT: A Spoken CALL System / 5 multiple types of semantic representation. In all the work reported here, the semantic formalism used is Almost Flat Functional Semantics (AFF; (Rayner et al., 2008)), a minimal formalism where clauses are represented as unordered lists of elements tagged with functional roles. For example, “Could you give me directions to the zoo?” is represented as the structure [null=[utterance_type, ynq], agent=[pronoun, you], null=[modal, could], null=[action, give], null=[voice, active]"
2014.lilt-10.2,C08-1090,1,0.818402,"ese have been further extended to cover related languages by a parameterization process. In particular, the French grammar has been extended into a shared grammar which covers French, Spanish and Catalan (Bouillon et al., 2007), and the English grammar has been similarly extended to cover both English and Swedish (Rayner et al., 2012b). The Regulus resource grammars are also parameterized to support 1 www.nuance.com CALL-SLT: A Spoken CALL System / 5 multiple types of semantic representation. In all the work reported here, the semantic formalism used is Almost Flat Functional Semantics (AFF; (Rayner et al., 2008)), a minimal formalism where clauses are represented as unordered lists of elements tagged with functional roles. For example, “Could you give me directions to the zoo?” is represented as the structure [null=[utterance_type, ynq], agent=[pronoun, you], null=[modal, could], null=[action, give], null=[voice, active], object=[abstract, directions] indobj=[pronoun, i], to_loc=[loc, zoo]] 2.3 Using interlingua to display prompts The AFF representations produced by speech recognition and parsing are translated into a language-neutral form, also expressed using AFF. The minimal list-based format mean"
2014.lilt-10.2,rayner-etal-2010-multilingual,1,0.824828,"ample is the EduSpeak 2010)), but the basic scheme is simple: the system plays the student a recorded sentence, asks them to imitate it, and then rates them on the accuracy of their imitation, giving advice if appropriate on how to improve pronunciation or prosody. It is easy to believe that this is useful, but it is also very limited in scope: the student is given no opportunity to practice spontaneous spoken generation skills. A more ambitious approach is to design an application where the student can respond flexibly to the system’s prompts. The project we describe in this paper, CALL-SLT (Rayner et al., 2010), is based on an idea originating with (Wang and Seneff, 2007); a related application, described in (Johnson and Valente, 2009), is TLTCS. The system prompts the user in some version of the L1, indicating in an abstract or indirect fashion what they are supposed to say; the student speaks in the L2, and the system provides a response based on speech recognition and language processing. We have built several prototypes on this basic pattern, exploring different language-pairs and strategies. For example, in a minimal version configured to teach French to English-speaking students, a prompt migh"
2014.lilt-10.2,P99-1024,0,0.128389,"context-free (CFG) grammars that can be compiled into language models and also used for parsing. It would be possible to write these grammars by hand, though doing so involves the usual problems, given that CFG is not a very expressive formalism; it is hard to model any non-trivial linguistic phenomena without producing an extremely large grammar that is in practice almost impossible to extend or maintain. A well-known method for addressing these issues is to specify the grammar in some higher-level formalism — most obviously, some kind of feature grammar — and compile this down to CFG form (Stent et al., 1999). This time, the problem is in another direction. Even if a substantial, linguistically motivated feature grammar can in principle be expanded out to a CFG grammar (this is of course by no means guaranteed), the resulting grammar will probably be so large that it exceeds the practical resource limits imposed by the speech recognition framework. For these reasons, grammars that can actually be used as language models need to be domain-specific; this, unfortunately, conflicts with the natural desire to make the grammars general and reusable. The Regulus system steers a middle course between thes"
2014.lilt-10.2,N07-1059,0,0.0306049,": the system plays the student a recorded sentence, asks them to imitate it, and then rates them on the accuracy of their imitation, giving advice if appropriate on how to improve pronunciation or prosody. It is easy to believe that this is useful, but it is also very limited in scope: the student is given no opportunity to practice spontaneous spoken generation skills. A more ambitious approach is to design an application where the student can respond flexibly to the system’s prompts. The project we describe in this paper, CALL-SLT (Rayner et al., 2010), is based on an idea originating with (Wang and Seneff, 2007); a related application, described in (Johnson and Valente, 2009), is TLTCS. The system prompts the user in some version of the L1, indicating in an abstract or indirect fashion what they are supposed to say; the student speaks in the L2, and the system provides a response based on speech recognition and language processing. We have built several prototypes on this basic pattern, exploring different language-pairs and strategies. For example, in a minimal version configured to teach French to English-speaking students, a prompt might be the text string: REQUEST HAMBURGER and the student will b"
2014.tc-1.24,fuchs-etal-2012-scalable,1,\N,Missing
2020.lrec-1.40,E14-2001,1,0.879605,"Missing"
2020.lrec-1.40,W19-6133,0,0.025216,"mmatical function is often the only 7 Turkish NLP web service is available from http:// tools.nlp.itu.edu.tr/. 325 way to disambiguate when a word form from different paradigms can look the same. For lemmatizing Icelandic in LARA we need to run three modules. For sentence splitting and tokenization we use a rule-based tokenizer from the Greynir package 8 . For tagging we use the state-ofthe-art ABLtagger (Steingrímsson et al., 2019), which has been shown to surpass the accuracy of other taggers by a substantial margin when working with Icelandic. And finally for lemmatizing Icelandic, Nefnir (Ingólfsdóttir et al., 2019), employing substitution rules and a large morphological database, gives considerably better results than other tools. When creating Icelandic resources in LARA, we use an open web service that provides the above mentioned tools9 . The web service takes whole untagged texts, tokenizes and splits them into sentences, tags and lemmatizes the texts and returns them to the LARA platform in JSON format. 3.4. Hazm Morphological processing for Farsi is performed using the hazm package10 . So far, the LARA wrapper is very basic and only uses the word tokenizer and lemmatizer. 3.5. Irish lemmatizer The"
2020.lrec-1.40,J06-4003,0,0.0141715,"is simple and straightforward. A plain version of the source LARA text is passed to TreeTagger, which returns a list of hsurface-word, POS-tag, lemmai triples. LARA uses these to add lemma tags to the words in the text. The default is that the lemma tag consists just of the lemma; the user can optionally specify that it should include POS information as well. The same generic interface has been used, with minor adaptations, to connect LARA to the other morphology resources described in the rest of this section. The version of the Punkt sentence tokeniser bundled with the Python NLTK package (Kiss and Strunk, 2006; Bird et al., 2009) is used to perform segmentation into sentences, in order to add default ||annotations (cf. Figure 2). 3.2. Turkish NLP pipeline The complex morphology of Turkish makes the use of morphological processing tools useful for lemmatization purposes. An automatic morphological analyzer would produce all possible lemmas for a word surface form and hopefully a morphological disambiguator would choose the most probable one in the given context. While creating Turkish resources in LARA, we first make an automatic morphological processing of the texts and provide automatically select"
2020.lrec-1.40,R19-1133,1,0.858622,"Missing"
A00-1016,P93-1008,0,0.0218054,"ature, pressure and carbon dioxide levels, and the status of t h e Shuttle's doors (open/closed). A visual display gives direct feedback on some of these parameters. The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999). The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domain~independent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and passed in that form to the dialogue manager. We refer to these as linguistic level representations. The aspects of the system which are of primary interest here concern the dialogue manager"
A00-1016,A97-1001,0,0.115693,"The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CommandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Tranm and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pyre et al., 1995). In most of this and other related work the treatment is some variant of the"
A00-1016,A97-1008,0,0.0184187,"is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CommandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Tranm and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pyre et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech int"
A00-1016,P99-1024,0,0.0144947,"nmental functions. In particular, our simulation allows voice access to the current and past values of the fixed sensor readings. The initial PSA speech interface demo consists of a simple simulation of the Shuttle. State parameters include the PSA's current position, some environmental variables such as local temperature, pressure and carbon dioxide levels, and the status of t h e Shuttle's doors (open/closed). A visual display gives direct feedback on some of these parameters. The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999). The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domain~independent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized. O"
A00-1016,P94-1001,0,0.0282514,"Missing"
A00-1016,H93-1008,0,\N,Missing
A92-1001,P88-1012,0,\N,Missing
A92-1001,P86-1036,0,\N,Missing
A92-1001,H86-1008,0,\N,Missing
baur-etal-2014-using,N07-1059,0,\N,Missing
baur-etal-2014-using,rayner-etal-2010-multilingual,1,\N,Missing
baur-etal-2014-using,fuchs-etal-2012-scalable,1,\N,Missing
baur-etal-2014-using,C08-1090,1,\N,Missing
baur-etal-2014-using,mcgraw-etal-2010-collecting,0,\N,Missing
bouillon-etal-2008-developing,H05-2014,1,\N,Missing
bouillon-etal-2008-developing,2005.mtsummit-papers.25,1,\N,Missing
bouillon-etal-2008-developing,2005.eamt-1.8,1,\N,Missing
bouillon-etal-2008-developing,2005.jeptalnrecital-long.17,1,\N,Missing
bouillon-etal-2008-developing,W07-0806,1,\N,Missing
bouillon-etal-2008-developing,W06-3702,1,\N,Missing
C00-2097,A97-1001,0,0.113553,"ation becomes increasingly burdensome. The grammar tends to become large and unwieldy, with many rules appearing in multiple versions that constantly need to be kept in step with each other. It represents a large development cost, is hard to maintain, and does not usually port well to new applications. It is tempting to consider the option of moving towards a more expressive grammar formalism, like uni cation grammar, writing the original grammar in uni cation grammar form and compiling it down to the context-free notation required by the underlying toolkit. At least one such system (Gemini; (Moore et al 1997)) has been implemented and used to build successful and non-trivial applications, most notably CommandTalk (Stent et al 1999). Gemini accepts a slightly constrained version of the uni cation grammar formalism originally used in the Core Language Engine (Alshawi 1992), and compiles it into context-free grammars in the GSL formalism supported by the Nuance Toolkit. The Nuance Toolkit compiles GSL grammars into sets of probabilistic nite state graphs (PFSGs), which form the nal language model. The relative success of the Gemini system suggests a new question. Uni cation grammars have been used ma"
C00-2097,P99-1024,1,\N,Missing
C08-1090,2005.eamt-1.8,1,0.935429,"Missing"
C08-1090,W06-3702,1,0.850481,"model, which produces a source-langage semantic representation. This is first translated by one set of rules into an interlingual form, and then by a second set into a target language representation. A target-language grammar, compiled into generation form, turns this into one or more possible surface strings, after which a set of generation preferences picks one out. Finally, the selected string is realised in spoken form. There is also some use of corpus-based statistical methods, both to tune the language model (Rayner et al., 2006, Section 11.5) and to drive a robust embedded help system (Chatzichrisafis et al., 2006). The treatment of syntactic structure is a carefully thought-out compromise between linguistic and engineering traditions. All grammars used are extracted from general linguistically motivated resource grammars, using corpus-based methods driven by small sets of examples (Rayner et al., 2006, Chapter 9). This results in a simpler and flatter grammar specific to the domain, whose structure is similar to the ad hoc phrasal grammars typical of engineering approaches. The treatment of semantics is however less sophisticated, and basically represents a minimal approach in the engineering tradition"
C08-1090,P02-1035,0,0.0176156,"l and Frege and views predicate calculus as the paradigm representation language. On this view of things, a c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Approaches based in the linguistic tradition were dominant about 10 to 15 years ago, when they were used in major systems like Germany’s Verbmobil (Wahlster, 2000) and SRI’s Spoken Language Translator (Rayner et al., 2000). They are still reasonably popular today, as exemplified by major systems like PARC’s XLE (Riezler et al., 2002). The competing heritage has its roots in engineering approaches to spoken language systems, which historically have been intimately connected with Machine Learning. On this view of things, a typical semantic representation is a flat list of feature-value pairs, with the features representing semantic concepts: here, “I want a pepperoni pizza” would be represented as something like [utterance_type=request, food=pizza, type=pepperoni] It is interesting to see how little contact there has been between these two traditions. Writers on formal semantics usually treat ad hoc featurevalue representat"
C08-1090,J90-1004,0,0.321346,"Missing"
C88-2111,P85-1009,0,\N,Missing
E03-1078,H94-1010,0,0.0626114,"Missing"
E03-1078,W00-0311,1,0.861623,"change the coverage of the system. For these reasons, commercial speech recognition platform vendors like Nuance and SpeechWorks have focussed on rule-based approaches, which allow rapid prototyping of systems from only very modest quantities of corpus data. Although most commercial rule-based spoken language dialogue systems use directed dialogue strategies and moderately simple recognition grammars, the literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al., 1999; Rayner et al., 2000; Lemon et al., 2001; Rayner et al., 2001b). If a project of this kind is developed over a substantial period of time, corpus material accumulates automatically as input to the system is logged. The more corpus material there is, the stronger the reasons for moving towards datadriven processing; this will however only be easy if the architecture is originally set up to use statistics as well as rules. Summarising the argument so far, we would like an architecture which combines rule-based and data-driven methods as transparently as possible. This will allow us to shift smoothly from an initial"
E03-1078,J80-3005,0,0.452417,"Missing"
E03-1078,P99-1024,0,0.0215818,"ed it is not easy to change the coverage of the system. For these reasons, commercial speech recognition platform vendors like Nuance and SpeechWorks have focussed on rule-based approaches, which allow rapid prototyping of systems from only very modest quantities of corpus data. Although most commercial rule-based spoken language dialogue systems use directed dialogue strategies and moderately simple recognition grammars, the literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al., 1999; Rayner et al., 2000; Lemon et al., 2001; Rayner et al., 2001b). If a project of this kind is developed over a substantial period of time, corpus material accumulates automatically as input to the system is logged. The more corpus material there is, the stronger the reasons for moving towards datadriven processing; this will however only be easy if the architecture is originally set up to use statistics as well as rules. Summarising the argument so far, we would like an architecture which combines rule-based and data-driven methods as transparently as possible. This will allow us to shift smo"
E03-2010,P93-1008,1,0.739557,"The core functionality provided by the REGULUS environment is compilation of typed unification grammars into annotated context-free grammar language models expressed in Nuance Grammar Specification Language (GSL) notation (Nuance, 2002). GSL language models can be converted into runnable speech recognisers by invoking the Nuance Toolkit compiler utility, so the net result is the ability to compile a unification grammar into a speech recogniser. The REGULUS unification grammar formalism is closely related to the one used in the SRI Core Language Engine (CLE) and Gemini systems (Alshawi, 1992; Dowding et al., 1993), and it is most reasonable to compare it with Gemini, which offers a broadly similar range of functionalities. One important difference relates to the treatment of semantics. CLE and Gemini support a general unification-based semantics; REGULUS, however, does not permit the use of unification when constructing semantic representations. Although this involves a slight loss of expressive power, it offers the very significant advantage of allowing the 223 semantics of the original grammar to be compiled into semantic annotations on the target GSL rules. The resulting recogniser can thus be used"
E03-2010,A97-1001,0,0.0394991,"which performs a suitable factoring of the grammar. The current version of REGULUS further refines the naive method by iteratively alternating the expansion and filtering stages, nondeterministically expanding each feature in turn and then filtering the result before proceeding to the next feature. On large grammars, this ""iterative expansion"" technique can reduce time and space requirements of the compilation algorithm by several orders of magnitude. Use of iterative expansion has allowed REGULUS successfully to compile several grammars which exceeded resource bounds for the Gemini compiler (Moore et al., 1997; Moore, 1998). ing corpus-based techniques which extract a specialised version of the original general grammar. REGULUS implements a version of the grammar specialisation scheme which extends the Explanation Based Learning method described in (Rayner et al., 2002). There is a general unification grammar, loosely based on the Core Language Engine grammar for English (Pulman, 1992), which has been developed over the course of about ten individual projects. The cun ent version of the grammar contains 145 unification grammar rules, 465 function word entries, and 72 features. The grammar for each"
E03-2010,2000.iwpt-1.18,0,0.024039,"tween recognition performance and either vocabulary size or size of the training corpus, with performance depending rather more heavily on average utterance length and the types of constructions covered by the specialised grammar. We are actively investigating these issues at the moment. 3 The Development Environment All the functionalities in the REGULUS environment are available via a command-line interface, and also from within a top-loop designed primarily for interactive grammar debugging. In this mode, the grammar is compiled into an efficient left-corner parser, using the algorithm of (Moore, 2000), and also into a Definite Clause Grammar (DCG) form; the advantage of the DCG representation is that it can often be used to help diagnose grammar bugs by attempting to parse non-top constituents. Each separate domain-specific grammar is defined though a config file, which also specifies settings for the various user-defined parameters relevant to the compilation process. 4 Structure of the demo We will demo two recognisers built using REGULUS, one for a command and control application and one for a medical speech translation application. The command and control recogniser is an extended vers"
E03-2010,H93-1008,1,\N,Missing
fuchs-etal-2012-scalable,N07-1059,0,\N,Missing
fuchs-etal-2012-scalable,rayner-etal-2010-multilingual,1,\N,Missing
H05-2014,2005.eamt-1.8,1,0.774145,"me recognizer using Nuance tools. Previously, the R EGULUS grammar specialization programme has only been implemented for English. In this demo, we will show how we can apply the same methodology to Japanese. Japanese is structurally a very different language from English, so it is by no means obvious that methods which work for English will be applicable in this new context: in fact, they appear to work very well. We will demo the grammars and resulting recognizers in the context of Japanese → English and Japanese → French versions of the Open Source MedSLT medical speech translation system (Bouillon et al., 2005; MedSLT, 2005). The generic problem to be solved when building any sort of recognition grammar is that syntax alone is insufficiently constraining; many of the real constraints in a given domain and use situation tend to be semantic and pragmatic in nature. The challenge is thus to include enough non-syntactic constraints in the grammar to create a language model that can support reliable domain-specific speech recognition: we sketch our solution for Japanese. The basic structure of our current general Japanese grammar is as follows. There are four main groups of rules, covering NP, PP, VP an"
H05-2014,H01-1007,0,0.0292655,"ram language model requires substantial quantities of corpus data, which is generally not available at the start of a new project. Head-to-head comparisons of class N-gram/robust and grammar-based systems also suggest that users who are familiar with system coverage get better results from grammar-based architectures (Knight et al., 2001). As a consequence, deployed spoken dialogue systems for real-world applications frequently use grammar-based methods. This is particularly the case for speech translation systems. Although leading research systems like Verbmobil and NESPOLE! (Wahlster, 2000; Lavie et al., 2001) usually employ complex architectures combining statistical and rule-based methods, successful practical examples like Phraselator and S-MINDS (Phraselator, 2005; Sehda, 2005) are typically phrasal translators with grammar-based recognizers. Voice recognition platforms like the Nuance Toolkit provide CFG-based languages for writing grammar-based language models (GLMs), but it is challenging to develop and maintain grammars consisting of large sets of ad hoc phrase-structure rules. For this reason, there has been considerable interest in developing systems that permit language models be specifi"
H05-2014,E03-2010,1,0.69704,"lop and maintain grammars consisting of large sets of ad hoc phrase-structure rules. For this reason, there has been considerable interest in developing systems that permit language models be specified in higher-level formalisms, normally some kind of unification grammar (UG), and then compile these grammars down to the low-level platform formalisms. A prominent early example of this approach is the Gemini system (Moore, 1998). Gemini raises the level of abstraction significantly, but still assumes that the grammars will be domain-dependent. In the Open Source R EGULUS project (Regulus, 2005; Rayner et al., 2003), we have taken a further step in the direction of increased abstraction, and derive all recognizers from a single linguistically motivated UG. This derivation procedure starts with a large, application-independent UG for a language. An application-specific UG is then derived using an Explanation Based Learning (EBL) specialization technique. This corpus-based specialization process is parameterized by the training corpus and operationality criteria. The training corpus, which can be relatively small, consists of examples of utterances that should be recognized by the target application. The s"
H93-1042,P92-1005,1,0.896007,"Missing"
H93-1042,H91-1060,0,0.0229358,"Missing"
H93-1042,P92-1021,1,0.851126,"Missing"
H93-1042,E89-1019,1,0.814985,"inally modified by a progressive VP (aFiights going to Boston&quot;) to Swedish NPs modified by a relative clause ( &quot;Flygningar som gdr till Boston&quot;): [and,1;r(head), form(verb ( t enne=n, perf=P, prog=y), tr (rood))] Domain Adaptation We begin by describing the customizations performed to adapt the general CLE English grammar and lexicon to the ATIS domain. First, about 500 lexical entries needed to be added. Of these, about 450 were regular content words ( airfare, Boston, seven forty seven, etc.), all of which were added by a graduate student 3 using the interactive VEX lexicon acquisition tool [7]. About 55 other entries, not of a regular form, were also added. Of these, 26 corresponded to the letters of the alphabet, which were treated as a new syntactic class, 15 or so were interjections (Sure, OK, etc.), and seven were entries for the days of the week, which turned out to have slightly different syntactic properties in American and British English. The only genuinely new entries were for available, round trip, first class, nonstop and one way, all of which failed to fit syntactic patterns previously implemented within the grammar, (e.g. &quot;Flights available from United&quot;, &quot;Flights to B"
H93-1042,J90-1003,0,0.00854805,"Missing"
H93-1042,H91-1015,0,0.0314687,"urce and target languages (Section 3.3); An Explanation Based Learning (EBL) technique for automatically chunking the grammar into commonly occurring phrase-types, which has proven valuable in maximizing return on effort expended on coverage extension, and a set of procedures for automatic testing and reporting that helps to ensure smooth integration across aspects of the effort performed at the various sites involved (Section 4). 2. C O M P O N E N T S INTERFACES AND The speech translation process begins with SRI&apos;s DECIPHER(TM) system, based on hidden Markov modeling and a progressive search [12, 13]. It outputs to the source language processor a small lattice of word hypotheses generated using acoustic and language model scores. The language processor, for both English and Swedish, is the SRI Core Language Engine (CLE) [1], a unification-based, broad coverage natural language system for analysis and generation. Transfer occurs at the level of quasi logical form (QLF); transfer rules are defined in a simple declarative formalism [2]. Speech synthesis is performed by the Swedish Telecom PROPHON system [8], based on stored polyphones. This section describes in more detail these components a"
H93-1042,J90-1004,0,0.0602124,"Missing"
H93-1042,H93-1041,0,0.0556818,"Missing"
H93-1042,1993.tmi-1.16,0,\N,Missing
H93-1042,P91-1021,1,\N,Missing
H94-1040,H92-1017,0,0.0676816,"e sources to bear, typically some form of syntactic and/or semantic analysis, and uses them to choose the most plausible member of the N-best list. We will call an algorithm that selects a member of the N-best list a preference method. The most common preference method is to select the highest member of the list that receives a valid semantic analysis. We will refer to this as the &quot;highest-incoverage&quot; method. Intuitively, highest-in-coverage seems a promising idea. However, practical experience shows that it is surprisingly hard to use it to extract concrete gains. For example, a recent paper [8] concluded that the highest-incoverage candidate was in terms of the word error rate only very marginally better than the one the recognizer considered best. In view of the considerable computational overhead required to perform linguistic analysis on a large number of speech hypotheses, its worth is dubious. In this paper, we will describe a general strategy for constructing a preference method as a near-optimal combination of a number of different knowledge sources. By a &quot;knowledge source&quot;, we will mean any well-defined procedure that associates some potentially meaningful piece of informati"
H94-1040,H91-1013,0,0.0136088,"r. The method is automatically trainable, acquiring information from both positive and negative examples. In experiments, the method was tested on a 1000-utterance sample of unseen ATIS data. 1. I N T R O D U C T I O N During the last few years, the previously separate fields of speech and natural language processing have moved much closer together, and it is now common to see integrated systems containing components for both speech recognition and language processing. An immediate problem is the nature of the interface between the two. A popular solution has been the N-best list-for example, [9]; for some N, the speech recognizer hands the language processor the N utterance hypotheses it considers most plausible. The recognizer chooses the hypotheses on the basis of the acoustic information in the input signal and, usually, a simple language model such as a bigram grammar. The language processor brings more sophisticated linguistic knowledge sources to bear, typically some form of syntactic and/or semantic analysis, and uses them to choose the most plausible member of the N-best list. We will call an algorithm that selects a member of the N-best list a preference method. The most com"
H94-1040,H93-1042,1,0.791521,"Missing"
H94-1040,J94-4005,1,\N,Missing
H94-1040,H92-1014,0,\N,Missing
J90-2003,P89-1020,0,0.500766,"Missing"
J90-2003,P88-1012,0,0.0156621,"garden))} (rose,hx.send(iohn,x, mary)) for &quot;John sent fewer roses to Mary than there were roses in the garden.&quot; Formulas like the third one above will occur frequently in Section 3, and we advise the reader to spend a minute at this point making sure that she understands the notation. 1.2.2 SPECIFICSYNTACTICAND SEMANTIC ASSUMPTIONS The basic scheme, or some not too distant relative, is the one used in many large-scale implemented systems; as 88 An ImplementableSemanticsfor ComparativeConstructions examples, we can quote T E A M (Grosz et al. 1987), P U N DIT (Dahl et al. 1987), T A C I T U S (Hobbs et al. 1988), M O D L (McCord 1987), C L E (Alshawi et al. 1989), and SNACK-85 (Rayner and Banks 1986). It also has close links with theoretical work in situation semantics (Pollard and Sag 1988; Fenstad et al. 1987). We start by enumerating tlhe basic syntactic constituents S, NP, VP, DET, AP, ADVF&apos;, PP; as far as our ontological commitments go, we need a set of THINGS, a set of EVENTS, and a set of DEGREES. We will assume that DEGREES are isomorphic to some kind of numbers. An N P will consist of a DET and a C N (we borrow this nonstandard term for N-bar from Montague grammar). A DET will semantically c"
J90-2003,C86-1016,0,0.0290972,"verbs (in English do, be, can, etc.) can act anaphorically for verb phrases. We treat a comparative complement consisting of an NP together with one of these verbs as essentially equivalent with the NP on its own, except that it is forced to contrast against an NP which is a subject in a clause whose main verb is a suitable antecedent. The verbal antecedent relationship also appears to lead to an unbounded dependency. 4. &quot;Quantifier binding&quot; in clausal comparatives; we discuss this in Section 4.1. All these forms of long-range dependency are handled by application of&quot;threading&quot; (Pereira 1983; Kartunnen 1986). Properly speaking, each distinct type of dependency ought to be associated with a distinct feature, which will be present in relevant constituents; by unifying features in different constituents against each other, information is propagated through the tree. In the XG-grammar in Appendix 2, we have cheated a little, by using the &quot;extraposition list&quot; feature to handle all of 1, 2, and 4 above. Although this can potentially lead to problems when dependencies &quot;cross,&quot; we felt that the gain in simplicity made this compromise worthwhile. It is a simple matter to reorganize the grammar using diffe"
J90-2003,P85-1023,0,0.0670006,"Missing"
J90-2003,P88-1010,0,0.0209798,"giving rules about where comparative complements m a y occur, and our treatment of phrasal contrastives also appear to allow more readings than really exist. It would certainly be desirable to find rules to eliminate these, or at least heuristics to say which readings can be regarded as unlikely. Another interesting question is whether it is possible to make the theory more compact, by collapsing the three formulas (*), (**), and (***) into one; they are so similar that this seems intuitively quite feasible. We speculate that one m a y be able to do this in a framework like that described in Pereira and Pollack (1988), which allows conditional interpretation. Finally, we say a few more words about the &quot;discourse comparatives&quot; mentioned in Section 6. A cursory examination of the example sentences would suggest that most of the missing comparative complements are of one of the following: &quot;than previously,&quot; &quot;than the one just mentioned,&quot; or &quot;than is the case.&quot; For example, taking another look at sentences 43)-45), we can postulate that the complement is as given below in italics. 102 43) The abandoned creature . . . committed a still graver sin (than the one j u s t mentioned). 44) At eight o&apos;clock the wind w"
J90-2003,P88-1007,1,0.75764,"the intuitions behind them clearly overlap to a large extent with ours. In our own treatment of phrasal comparatives, we take all these ideas to their logical conclusion: we dispense with the C-ellipsis rule altogether, and regard all nonclausal comparatives as essentially phrasal, interpreting them by a method analogous to Pinkham&apos;s distributive copying. The abstract scheme is concretely implemented in the form of rewriting rules, which are applied to an intermediate quasilogical form. A solution along these lines, which builds on previously reported work (Banks 1986; Banks and Rayner 1987; Rayner and Banks 1988), is described in Section 3. As we complete this article, we discover that a similar approach for German comparatives has independently been suggested by Krifka (1987); Krifka also proceeds by using direct interpretation rules involving contrasting of compared constituents, although the details of his analysis are in some respects fairly different. The following is a typical example: 9a) is given the semantics represented by 9b), where pdO(d) is to be read as &quot;the maximum d, for which O(d) holds,&quot; and KOMP(x, y, f) is defined by KOMP(x, y, ~) ,~. &lt;I,(x) &gt; ~ y ) 9a) Ich b a b e ihr bessere ZahN"
J90-2003,P88-1005,0,\N,Missing
J90-2003,P88-1006,0,\N,Missing
L16-1036,S07-1016,0,0.0186744,"nition, metrics 1. Introduction The history of human language technology shows that the introduction of a shared task1 often has a positive effect. Friendly competition motivates people, and the ability to make direct comparisons between different approaches to solving the same problem makes it easier to identify the ideas that work, so that effort can be focused more productively. A prominent series of examples are the various tasks based on the Wall Street Journal corpus, including speech recognition (Bahl et al., 1995), parsing (Riezler et al., 2002) and several types of semantic analysis (Pradhan et al., 2007). Perhaps even more importantly, work on machine learning during the 21st century has to a considerable extent been driven by the handwritten digit recognition task (Goodfellow et al., 2016). Other well-known examples of shared tasks include ATIS in the early 90s (Zue et al., 1994), which had a strong effect on interactive spoken language systems; the Named Entity Recognition task (Tjong Kim Sang and De Meulder, 2003), which similarly influenced work on information extraction; and the Recognizing Textual Entailment task (Dagan et al., 2006), which has influenced work on question answering. In"
L16-1036,rayner-etal-2010-multilingual,1,0.822865,"st common types of spoken CALL exercise is prompt-response: the system gives the student a prompt, the student responds, and the system either accepts or rejects the response, possibly giving some extra feedback. The prompt can be of various forms, including L2 text (“read the following sentence”), L1 text (“translate the following sentence into the L2”), multimedia (“name this object”) or some kind of combination. Prompt-response exercises are for example used heavily in the popular Duolingo application.3 We propose a minimal spoken prompt-response task based on data collected from CALL-SLT (Rayner et al., 2010), a spoken CALL system which has been under development at Geneva University since 20094 . The prompt is a piece of text; the response is a recorded audio file; the task is to accept linguistically correct responses, and reject others. In §2.1., we briefly sketch CALL-SLT and the data that has been collected using it; next, in §2.2., we introduce and motivate the task in intuitive terms. The rest of the paper describes the task in more detail. 2.1. CALL-SLT CALL-SLT is an online CALL tool based on speech recognition, web and language processing technology. In the ver2 http://hstrik.ruhosting.n"
L16-1036,P02-1035,0,0.0145178,"ta in January 2017. Keywords: CALL, shared tasks, speech recognition, metrics 1. Introduction The history of human language technology shows that the introduction of a shared task1 often has a positive effect. Friendly competition motivates people, and the ability to make direct comparisons between different approaches to solving the same problem makes it easier to identify the ideas that work, so that effort can be focused more productively. A prominent series of examples are the various tasks based on the Wall Street Journal corpus, including speech recognition (Bahl et al., 1995), parsing (Riezler et al., 2002) and several types of semantic analysis (Pradhan et al., 2007). Perhaps even more importantly, work on machine learning during the 21st century has to a considerable extent been driven by the handwritten digit recognition task (Goodfellow et al., 2016). Other well-known examples of shared tasks include ATIS in the early 90s (Zue et al., 1994), which had a strong effect on interactive spoken language systems; the Named Entity Recognition task (Tjong Kim Sang and De Meulder, 2003), which similarly influenced work on information extraction; and the Recognizing Textual Entailment task (Dagan et al"
L16-1036,W03-0419,0,0.117006,"Missing"
L16-1036,H94-1037,0,0.288415,"makes it easier to identify the ideas that work, so that effort can be focused more productively. A prominent series of examples are the various tasks based on the Wall Street Journal corpus, including speech recognition (Bahl et al., 1995), parsing (Riezler et al., 2002) and several types of semantic analysis (Pradhan et al., 2007). Perhaps even more importantly, work on machine learning during the 21st century has to a considerable extent been driven by the handwritten digit recognition task (Goodfellow et al., 2016). Other well-known examples of shared tasks include ATIS in the early 90s (Zue et al., 1994), which had a strong effect on interactive spoken language systems; the Named Entity Recognition task (Tjong Kim Sang and De Meulder, 2003), which similarly influenced work on information extraction; and the Recognizing Textual Entailment task (Dagan et al., 2006), which has influenced work on question answering. In all these cases, introduction of the shared task created a new community with frequent productive interactions between many groups, and substantially advanced a whole subfield inside the space of a few years. The sociology of the process has become familiar to many researchers. A s"
N01-1030,H92-1021,0,\N,Missing
N01-1030,H91-1043,0,\N,Missing
N01-1030,P93-1008,1,\N,Missing
N01-1030,H93-1008,1,\N,Missing
N01-1030,C00-2097,1,\N,Missing
N01-1030,P99-1024,1,\N,Missing
N01-1030,A97-1001,0,\N,Missing
P03-2024,C00-2097,1,0.838812,"ght, numbers) or even gestures (e.g. pointing to an area of the body). This is clearly a domain in which the constraints of the task are sufficient for a limited domain, one way spoken translation system to be a useful tool. 2 An architecture for limited-domain speech translation The basic philosophy behind the architecture of the system is to attempt an intelligent compromise between fixed-phrase translation on one hand (e.g. (IntegratedWaveTechnologies, 2002)) and linguistically motivated grammar-based processing on the other (e.g. V ERBMOBIL (Wahlster, 2000) and Spoken Language Translator (Rayner et al., 2000a)). At run-time, the system behaves essentially like a phrasal translator which allows some variation in the input language. This is close in spirit to the approach used in most normal phrase-books, which typically allow “slots” in at least some phrases (“How much does — cost?”; “How do I get to — ?”). However, in order to minimize the overhead associated with defining and maintaining large sets of phrasal patterns, these patterns are derived from a single large linguistically motivated unification grammar; thus the compile-time architecture is that of a linguistically motivated system. Phras"
P03-2024,E03-2010,1,0.673693,"le for source language speech recognition, including parsing and production of semantic representation; transfer and generation; and synthesis of target language speech. The speech processing modules (recognition and synthesis) are implemented on top of the standard Nuance Toolkit platform (Nuance, 2003). Recognition is constrained by a CFG language model written in Nuance Grammar Specification Language (GSL), which also specifies the semantic representations produced. This language model is compiled from a linguistically motivated unification grammar using the Open Source REGULUS 2 platform (Rayner et al., 2003; Regulus, 2003); the compilation process is driven by a small corpus of examples. The language processing modules (transfer and generation) are a suite of simple routines written in SICStus Prolog. The speech and language processing modules communicate with each other through a minimal file-based protocol. The semantic representations on both the source and target sides are expressed as attribute-value structures. In accordance with the generally minimalistic design philosophy of the project, semantic representations have been kept as simple as possible. The basic principle is that the repres"
P03-2038,E03-1078,1,0.807195,"anguage Engine’s Quasi Logical Form notaREGULUS tion (van Eijck and Moore, 1992). A grammar built on top of the general grammar is transformed into a specialised Nuance grammar in the following processing stages: 1. The training corpus is converted into a “treebank” of parsed representations. This is done using a left-corner parser representation of the grammar. 2. The treebank is used to produce a specialised grammar in REGULUS format, using the EBL algorithm (van Harmelen and Bundy, 1988; Rayner, 1988). 3. The final specialised grammar is compiled into a Nuance GSL grammar. 4 ALTERF ALTERF (Rayner and Hockey, 2003) is another Open Source toolkit, whose purpose is to allow a clean combination of rule-based and corpus-driven processing in the semantic interpretation phase. There is typically no corpus data available at the start of a project, but considerable amounts at the end: the intention behind ALTERF is to allow us to shift smoothly from an initial version of the system which is entirely rule-based, to a final version which is largely data-driven. ALTERF characterises semantic analysis as a task slightly extending the “decision-list” classification algorithm (Yarowsky, 1994; Carter, 2000). We start"
P03-2038,E03-2010,1,0.858532,"Missing"
P03-2038,P94-1013,0,\N,Missing
P05-3008,E03-1078,1,0.880446,"Missing"
P05-3008,E03-2010,1,0.241003,"involved (Knight et al., 2001) suggested that grammar-based systems outperformed statistical ones for this kind of user. Given that neither of the above arguments is very strong, we wanted to implement a framework which would allow us to compare grammar-based methods with statistical ones, and retain the option of switching from a grammar-based framework to a statistical one if that later appeared justified. The Regulus and Alterf platforms, which we have developed under Clarissa and other earlier projects, are designed to meet these requirements. The basic idea behind Regulus (Regulus, 2005; Rayner et al., 2003) is to extract grammar-based language models from a single large unification grammar, using example-based methods driven by small corpora. Since grammar construction is now a corpus-driven process, the same corpora can be used to build statistical language models, facilitating a direct comparison between the two methodologies. On its own, however, Regulus only permits comparison at the level of recognition strings. Alterf (Rayner and Hockey, 2003) extends the paradigm to ID 1 2 3 4 5 6 Rec SLM GLM SLM GLM SLM GLM Features Confidence Confidence Confidence + Lexical Confidence + Lexical Confiden"
P16-2027,P05-1033,0,0.0872519,"e extended across several manual activities in a straightforward way; TrPhrase $$name Source claude Gloss C L A U D E Mouthing C L a u: d e EndTrPhrase TrPhrase $$name Source marie Gloss M A R I E Mouthing L23 a R i e EndTrPhrase Figure 1: Toy speech2sign application definition. however, workarounds have been introduced for this (Ebling and Glauert, 2015). Experience with SiGML has shown that it is capable of supporting signed animation of satisfactory quality (Smith and Nolan, 2015). The core translation formalism is a version of Synchronous Context Free Grammar (SCFG; (Aho and Ullman, 1969; Chiang, 2005)) adapted to the peculiarities of sign language translation. A complete toy application definition is shown in Figure 1. The top-level Utterance rule translates French expressions of the form Je m’appelle hNAMEi (“I am called hNAMEi”) to Swiss French Sign Language (LSF-CH) expressions of a form 164 that can be glossed as MOI S_APPELER hNAMEi together with accompanying non-manual components; for example, the manual activity MOI (signed by pointing at one’s chest) is here performed together with a head nod, raised eyebrows, widened eyes, and a series of mouth movements approximating the shapes u"
P88-1007,C69-0101,0,0.377486,"Missing"
P88-1007,H89-2010,0,0.046212,"Missing"
P91-1021,P88-1005,0,0.0225365,"ce of referents for pronouns and definite descriptions, and relations implied by compound nouns and ellipsis. They are also neutral with respect to other ambiguities corresponding to alternative scopings of quantifiers and operators and to the collective/distributive and referential/attributive distinctions. The QLF is thus the level of representation encoding the results of compositional linguistic analysis independently of contextually sensitive aspects of understanding. These aspects are addressed by the contextual interpretation phase which has the following subphases: quantifier scoping (Moran 1988), reference resolution (Alshawi 1990), and plausibility judgement. Deriving a fairly conventional Logical Form (LF) from the RQLF is then a simple formal mapping which removes the information in the RQLF that is not concerned with truth conditions. Linguistic analysis and contextual interpretation each consist of several subphases. For analysis these are: orthography, morphological analysis, syntactic analysis (parsing), and (compositional) semantic analysis. Apart from the first, these analysis subphases are based on the unification grammar paradigm, and they all use declarative bidirectional"
P91-1021,P89-1004,1,0.91546,"Missing"
P91-1021,C86-1021,0,0.0140938,"example, topichood or the given/new distinction is preserved. LEVELS The representational structures on which transfer operates must contain information corresponding to several linguistic levels, including syntax and semantics. For transfer to be general, it must operate recursively on input representations. We call the level of representation on which this recursion operates the ""organizing"" level; semantic structure is the natural choice, since the basic requirement of translation is that it preserves meaning. Syntactic phrase structure transfer, or deepsyntax transfer (e.g. Thurmair 1990, Nagao and Tsujii 1986) results in complex transfer rules, and the predicate-argument structure which is required for the application of sortal restrictions is not represented. Finally, in contrast to systems such as Rosetta (Landsbergen, 1986) which depend on stating rule by rule correspondences between source and target grammars, we wish to make the monolingual descriptions as independent as possible from the task of translating between two languages. Apart from McCord&apos;s (1988, 1989) organizing level appears to be/hat, of surface syntax, with additional deep syntactic and semantic content attached to nodes. As we"
P91-1021,J90-3001,1,0.924809,"d the use of interaction to resolve context dependent ambiguities, but in this paper we concentrate on representational and transfer issues. 1 INTRODUCTION In this paper we describe a translation project whose aim is to build an experimental Bilingual Conversation Interpreter (BCI) which will allow communication through typed text between two monolingual humans using different languages (of Miike et al, 1988). The choice of languages for the prototype system is English and Swedish. Input sentences are analysed by the Core Language Engine (CLE 1) as far as the level of Quasi Logical Form (QLF; Alshawi, 1990), and then, instead of further ambiguity resolution, undergo transfer into another QLF having constants and predicates corresponding to word senses in the other language. The transfer rules used in this process correspond to a certain kind of meaning postulate. The CLE then generates an output, sentence from the target 2 CLE REPRESENTATION LEVELS In this section we explain how QLF fits into the overall architecture of the CLE and in section 3 we discuss the reasons for choosing it for interactive dialogue translation. 1Tile CLE is described in Alshawi (1991) which includes more detailed discus"
P91-1021,J90-1004,0,0.0504575,"Missing"
P91-1021,C86-1025,0,0.0157927,"resentation are produced by successive modular components. [past, [hire, q_term (<t =quant, n=s ing>, E, [event, E] ), a_term(<t =ref, p=pro, l=she, n=sing>, Y, [female, Y] ), q_t erm (<t =quant, n=sing>, C, a_f orm(<t =pred, l=one>, P, [ P . C ] ) ) ] ] . Generation of linguistic expressions in the CLE takes place from QLFs (or from RQLFs by mapping them to suitable QLFs). Since the rules 162 may be related to the fact that McCord&apos;s system is explicitly not symmetrical: different grammars are used for the analysis and synthesis of the same language, which are viewed as quite different tasks. Isabelle and Macklovitch (1986) argue against such asymmetry between analysis and synthesis on the grounds that, although it is tempting as a short-cut to building a structure sufficiently well-specified for synthesis to take place, asymmetry means that the transfer component must contain a lot of knowledge about the target language, with dire consequences for the modularity of the system and the reusability of different parts of it. In the BCI, however, the transfer rules contain only cross-linguistic knowledge, allowing the analysis and generation to make use of exactly the same data. in which categories are shown as list"
P91-1021,J89-1003,0,0.0238708,"Missing"
P91-1021,P88-1019,0,0.0662624,"ing for a choice between word sense paraphrases or between alternative partial bracketings of the sentence. There • is thus a strong connection between our choice of a representation sensitive to context and the use of interaction to resolve context dependent ambiguities, but in this paper we concentrate on representational and transfer issues. 1 INTRODUCTION In this paper we describe a translation project whose aim is to build an experimental Bilingual Conversation Interpreter (BCI) which will allow communication through typed text between two monolingual humans using different languages (of Miike et al, 1988). The choice of languages for the prototype system is English and Swedish. Input sentences are analysed by the Core Language Engine (CLE 1) as far as the level of Quasi Logical Form (QLF; Alshawi, 1990), and then, instead of further ambiguity resolution, undergo transfer into another QLF having constants and predicates corresponding to word senses in the other language. The transfer rules used in this process correspond to a certain kind of meaning postulate. The CLE then generates an output, sentence from the target 2 CLE REPRESENTATION LEVELS In this section we explain how QLF fits into the"
P91-1021,E89-1037,0,\N,Missing
P96-1030,C94-1099,0,0.0166001,"y code a new grammar for each domain. Many people do this, but one cannot help feeling that something is being missed; intuitively, there are many domain-independent grammatical constraints, which one would prefer only to need to code once. In the last ten years, there have been a number of attempts to find ways to automatically adapt a general grammar and/or parser to the sub-language defined by a suitable training corpus. For example, (Briscoe and Carroll, 1993) train an LR parser based on a general grammar to be able to distinguish between likely and unlikely sequences of parsing actions; (Andry et al., 1994) automatically infer sortal constraints, that can be used to rule out otherwise grammatical constituents; and (Grishman et al., 1984) describes methods that reduce the size of a general grammar to include only rules actually useful for parsing the training corpus. We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning. These methods together give an order of magnitude increase in speed, and the coverage loss entailed by grammar specialization"
P96-1030,J93-1002,0,0.0160936,"ted Kingdom manny@cam, sri. com, dmc~cam, sri. com Abstract One possible solution is of course to dispense with the idea of using a general grammar, and simply code a new grammar for each domain. Many people do this, but one cannot help feeling that something is being missed; intuitively, there are many domain-independent grammatical constraints, which one would prefer only to need to code once. In the last ten years, there have been a number of attempts to find ways to automatically adapt a general grammar and/or parser to the sub-language defined by a suitable training corpus. For example, (Briscoe and Carroll, 1993) train an LR parser based on a general grammar to be able to distinguish between likely and unlikely sequences of parsing actions; (Andry et al., 1994) automatically infer sortal constraints, that can be used to rule out otherwise grammatical constituents; and (Grishman et al., 1984) describes methods that reduce the size of a general grammar to include only rules actually useful for parsing the training corpus. We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on exp"
P96-1030,A88-1019,0,0.0521309,"tion, a general grammar allows a great many theoretically valid analyses of almost any non-trivial sentence. However, in the context of a specific domain, most of these will be extremely implausible, and can in practice be ignored. If we want efficient parsing, we want to be able to focus our search on only a small portion of the space of theoretically valid grammatical analyses. 223 The work reported here is a logical continuation of two specific strands of research aimed in this general direction. The first is the popular idea of statistical tagging e.g. (DeRose, 1988; Cutting et al., 1992; Church, 1988). Here, the basic idea is that a given small segment S of the input string may have several possible analyses; in particular, if S is a single word, it may potentially be any one of several parts of speech. However, if a substantial training corpus is available to provide reasonable estimates of the relevant parameters, the immediate context surrounding S will usually make most of the locally possible analyses of S extremely implausible. In the specific case of part-of-speech tagging, it is well-known (DeMarcken, 1990) that a large proportion of the incorrect tags can be eliminated ""safely""~ i"
P96-1030,A92-1018,0,0.0209079,"nature of its construction, a general grammar allows a great many theoretically valid analyses of almost any non-trivial sentence. However, in the context of a specific domain, most of these will be extremely implausible, and can in practice be ignored. If we want efficient parsing, we want to be able to focus our search on only a small portion of the space of theoretically valid grammatical analyses. 223 The work reported here is a logical continuation of two specific strands of research aimed in this general direction. The first is the popular idea of statistical tagging e.g. (DeRose, 1988; Cutting et al., 1992; Church, 1988). Here, the basic idea is that a given small segment S of the input string may have several possible analyses; in particular, if S is a single word, it may potentially be any one of several parts of speech. However, if a substantial training corpus is available to provide reasonable estimates of the relevant parameters, the immediate context surrounding S will usually make most of the locally possible analyses of S extremely implausible. In the specific case of part-of-speech tagging, it is well-known (DeMarcken, 1990) that a large proportion of the incorrect tags can be elimina"
P96-1030,P90-1031,0,0.0215283,"popular idea of statistical tagging e.g. (DeRose, 1988; Cutting et al., 1992; Church, 1988). Here, the basic idea is that a given small segment S of the input string may have several possible analyses; in particular, if S is a single word, it may potentially be any one of several parts of speech. However, if a substantial training corpus is available to provide reasonable estimates of the relevant parameters, the immediate context surrounding S will usually make most of the locally possible analyses of S extremely implausible. In the specific case of part-of-speech tagging, it is well-known (DeMarcken, 1990) that a large proportion of the incorrect tags can be eliminated ""safely""~ i.e. with very low risk of eliminating correct tags. In the present paper, the statistical tagging idea is generalized to a method called ""constituent pruning""; this acts on local analyses of phrases normally larger than single-word units. Constituent pruning is a bottom-up approach, and is complemented by a second, top-down, method based on Explanation-Based Learning (EBL; (Mitchell et al., 1986; van Harmelen and Bundy, 1988)). This part of the paper is essentially an extension and generalization of the line of work de"
P96-1030,H94-1040,1,0.844891,"00 250 500 1000 3000 5000 7000 11000 15000 Experiments This section describes a number of experiments carried out to test the utility of the theoretical ideas presented above. The basic corpus used was a set of 16,000 utterances from the Air Travel Planning (ATIS; (Hemphill et al., 1990)) domain. All of these utterances were available in text form; 15,000 of them were used for training, with 1,000 held out for test purposes. Care was taken to ensure not just that the utterances themselves, but also the speakers of the utterances were disjoint between test and training data; as pointed out in (Rayner et al., 1994a), failure to observe these precautions can result in substantial spurious improvements in test data results. The 16,000 sentence corpus was analysed by the SRI Core Language Engine (Alshawi (ed), 1992), using a lexicon extended to cover the ATIS domain (Rayner, 1994). All possible grammatical analyses of each utterance were recorded, and an interactive tool was used to allow a human judge to identify the correct and incorrect readings of each utterance. The judge was a first-year undergraduate student with a good knowledge of linguistics but no prior experience with the system; the process o"
P96-1030,H90-1051,1,0.877206,"Missing"
P96-1030,H93-1025,0,0.0210281,"ored that could in principle be pruned earlier. However, as the results in section 4 below will show, this is not in practice a serious problem, because the second pruning phase greatly reduces the search space in preparation for the potentially inefficient full parsing phase. Our method has the advantage, compared to beam search, that there is no need for any particular search order to be followed; when pruning takes place, all constituents that could have been found at the stage in question are guaranteed already to exist. Thirdly, our method is a generalization of the strategy employed by (McCord, 1993). McCord interleaved parsing with pruning in the same way as us, but only compared constituents over the same span and with the same major category. Our comparisons are more global and therefore can result in more effective pruning. 3 Grammar specialization the grammar non-triviMly without losing too much coverage. Several attempts to find good ""chunking criteria"" are described in the papers by Rayner and Samuelsson quoted above. In (Rayner and Samuelsson, 1994), a simple scheme is given, which creates rules corresponding to four possible units: full utterances, recursive NPs, PPs, and non-rec"
P96-1030,J88-1003,0,\N,Missing
P96-1030,H90-1065,0,\N,Missing
P96-1030,C94-1062,0,\N,Missing
P96-1030,H90-1021,0,\N,Missing
P96-1030,P94-1026,0,\N,Missing
P96-1030,P84-1023,0,\N,Missing
P96-1030,H93-1042,1,\N,Missing
P96-1030,P94-1013,0,\N,Missing
rayner-etal-2006-regulus,N01-1030,1,\N,Missing
rayner-etal-2006-regulus,E03-2010,1,\N,Missing
rayner-etal-2006-regulus,C02-1095,0,\N,Missing
rayner-etal-2006-regulus,H05-2014,1,\N,Missing
rayner-etal-2006-regulus,P01-1022,1,\N,Missing
rayner-etal-2006-regulus,2005.eamt-1.8,1,\N,Missing
rayner-etal-2010-multilingual,tsourakis-etal-2008-building,1,\N,Missing
rayner-etal-2010-multilingual,2008.amta-govandcom.4,1,\N,Missing
rayner-etal-2010-multilingual,rayner-etal-2006-regulus,1,\N,Missing
rayner-etal-2010-multilingual,N07-1059,0,\N,Missing
rayner-etal-2010-multilingual,bouillon-etal-2008-developing,1,\N,Missing
rayner-etal-2010-multilingual,H05-2014,1,\N,Missing
rayner-etal-2010-multilingual,2005.eamt-1.8,1,\N,Missing
rayner-etal-2010-multilingual,W07-0806,1,\N,Missing
rayner-etal-2012-evaluating,N07-1059,0,\N,Missing
rayner-etal-2012-evaluating,rayner-etal-2010-multilingual,1,\N,Missing
tsourakis-etal-2008-building,N03-4015,0,\N,Missing
tsourakis-etal-2008-building,2005.mtsummit-papers.25,1,\N,Missing
tsourakis-etal-2008-building,P05-3008,1,\N,Missing
tsourakis-etal-2008-building,2005.eamt-1.8,1,\N,Missing
tsourakis-etal-2008-building,W06-3702,1,\N,Missing
tsourakis-etal-2010-examining,tsourakis-etal-2008-building,1,\N,Missing
tsourakis-etal-2010-examining,2005.eamt-1.8,1,\N,Missing
tsourakis-rayner-2012-corpus,fuchs-etal-2012-scalable,1,\N,Missing
W00-0311,P93-1008,0,0.0432865,"erature, pressure and carbon dioxide levels, and the status of the Shuttle's doors (open/closed). A visual display gives direct feedback on some of these parameters. The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999). The system comprises a suite of about 20 agents, connected together using the SRI Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out Using the SRI Gemini system (Dowding et al., 1993), using a domain-independent unification. grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grammar using the methods of (Moore et al., 1997); the n ~ resnlt is that only grammatically wellformed utterances Gan be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and passed in that form to the dialogue manager. We refer to these as linguistic level representations. The aspects of the system which are of primary interest here concern the dialogue manager"
W00-0311,A97-1001,0,0.462118,"he basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this ta~k in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI's Flakey robot (Konolige et al., 1993) and NCARAI's InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CommandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and Tl:tAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the"
W00-0311,A97-1008,0,0.017903,"that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this ta~k in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI's Flakey robot (Konolige et al., 1993) and NCARAI's InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CommandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and Tl:tAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech in"
W00-0311,P99-1024,0,0.39148,"ronmental functions. In particular, our simulation allows voice access to the current and past values of the fixed sensor readings. The initial PSA speech interface demo consists of a simple simulation of the Shuttle. State parameters include the PSA's current position, some environmental variables such as local temperature, pressure and carbon dioxide levels, and the status of the Shuttle's doors (open/closed). A visual display gives direct feedback on some of these parameters. The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999). The system comprises a suite of about 20 agents, connected together using the SRI Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out Using the SRI Gemini system (Dowding et al., 1993), using a domain-independent unification. grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grammar using the methods of (Moore et al., 1997); the n ~ resnlt is that only grammatically wellformed utterances Gan be recognized. Ou"
W00-0311,P94-1001,0,0.0117831,"to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this ta~k in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI's Flakey robot (Konolige et al., 1993) and NCARAI's InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CommandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and Tl:tAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is con"
W00-0311,H93-1008,0,\N,Missing
W00-2026,C86-1016,0,0.156261,"Missing"
W00-2026,J81-4003,0,0.346452,"Missing"
W00-2026,E87-1049,0,0.171809,"Missing"
W00-2026,E87-1003,0,\N,Missing
W01-1617,2000.iwpt-1.15,0,0.0111294,"RB:[sem_obj_type=T] NP:[sem_type=T] NP:[sem_type=T] --> DET NOUN:[sem_type=T] DET --> the TRANSITIVE_VERB:[sem_obj_type=dimmable] --> dim NOUN:[sem_type=dimmable] --> light This kind of parameterisation of a CFG is not in any way new: it is simply uni cation grammar (Pullum and Gazdar, 1982; Gazdar et al., 1985). Thus our rst main idea is to raise the level of abstraction, formulating the device grammar at the level of uni cation grammars, and compiling these down into the underlying CFG representation. There are now a number of systems which can perform this type of compilation (Moore, 1998; Kiefer and Krieger, 2000); the basic methods we use in our system are described in detail elsewhere (Rayner et al., 2001a). Here, we focus on the aspects that are required for distributed"" uni cation grammars needed for Plug and Play. 4.2 Uni cation grammars meet object-oriented programming"". Our basic idea is to start with a general deviceindependent uni cation grammar, which implements the core grammar rules. In our prototype, there are 34 core rules. Typical examples are the NP conjunction and PP modi cations rules, schematically NP --> NP CONJ NP NP --> NP PP which are likely to occur in connection with any kind"
W01-1617,N01-1030,1,0.821703,"johan.boye@trab.se Plug and Play is, not surprisingly, viewed as a pre-requisite for the commercial success of networked devices in the home. There are already several promising candidate platforms for achieving the necessary functionality, including Universal Plug and Play (UPnP) (Microsoft, 2000) and Jini (Oaks and Wong, 2000). In this paper, we address the requirements on spoken dialogue interfaces that arise from a plug and play domain. We also present the current state of our English language plug and play demonstrator for controlling lamps, dimmers and sensors, previously described in (Rayner et al., 2001b). (There is also a Swedish instantiation). First, however, we need brie y to distinguish our notion from other notions of plug and play and recon gurability. The notion of Plug and Play has been used for dialogue system toolkits in which the various di erent language processing components themselves (e.g. recognition, parsing, generation and dialogue management) can be plugged in and out. The most prominent instance of this is the Darpa Communicator architecture (Goldschen and Loehr, 1999), which de nes interoperability standards for language processing components. The intention is simply th"
W01-1617,P00-1018,0,\N,Missing
W06-3702,2005.eamt-1.8,1,0.658561,"Missing"
W06-3702,1999.mtsummit-1.8,0,0.0751905,"systems utilizing probabilistic context-free grammar tuning appear to deliver better results when training data is sparse (Rayner et al., 2005a). One drawback of grammar-based systems is that out-of-coverage utterances will be neither recognized nor translated, an objection that critics have sometimes painted as decisive. It is by no means obvious, however, that restricted coverage is such a serious problem. In text processing, work on several generations of controlled language systems has developed a range of techniques for keeping users within the bounds of system coverage (Kittredge, 2003; Mitamura, 1999). If these techniques work for text processing, it is surely not inconceivable that variants of them will be equally successful for spoken language applications. Users are usually able to adapt to a controlled language system given enough time. The critical questions are how to provide efficient support to guide them towards the system&apos;s coverage, and how much time they will then need before they have acclimatized. With regard to top-level translation functionality, the choice is between unidirectional and bidirectional systems. Bidirectional systems are certainly possible today1, but the argu"
W06-3702,E03-2010,1,0.889866,"Missing"
W06-3702,H05-2014,1,0.898237,"Missing"
W06-3702,2005.mtsummit-papers.25,1,0.809631,"s. Resource grammars are now available for several languages, including English, Japanese (Rayner et al., 2005b), French (Bouillon et al., 2006) and Spanish. MedSLT includes a help module, whose purpose is to add robustness to the system and guide the user towards the supported coverage. The help module uses a second backup recognizer, equipped with a statistical language model; it matches the results from this second recognizer against a corpus of utterances, which are within system coverage and have already been judged to give correct translations. In previous studies (Rayner et al., 2005a; Starlander et al., 2005), we showed that the grammar-based recognizer performs much better than the statistical one on in-coverage utterances, and rather worse on out-of-coverage ones. We also found that having the help module available approximately doubled the speed at which subjects learned to use the system, measured as the average difference in semantic error rate between the results for their first quarter-session and their last quarter-session. It is also possible to recover from recognition errors by selecting one of the displayed help sentences; in the cited studies, we found that this increased the number o"
W06-3707,2005.eamt-1.8,1,0.885548,"Missing"
W06-3707,2002.tmi-papers.17,0,0.0583709,"s domain for the three main input languages. Differences in the sizes of the recognition vocabularies are primarily due to differences in use of inflection. Japanese, with little inflectional morphology, has the smallest vocabulary; French, which inflects most parts of speech, has the largest. 3 The development environment Although the MedSLT system is rule-based, we would, for the usual reasons, prefer to acquire these rules from corpora using some well-defined method. There is, however, little or no material available for most medical speech translation domains, including ours. As noted in (Probst and Levin, 2002), scarcity of data generally implies use of some strategy to obtain a carefully structured training corpus. If the corpus is not organised in this way, conflicts between alternate learned rules occur, and it is hard to inWhere? “do you experience the pain in your jaw” “does the pain spread to the shoulder” When? “have you had the pain for more than a month” “do the headaches ever occur in the morning” How long? “does the pain typically last a few minutes” “does the pain ever last more than two hours” How often? “do you get headaches several times a week” “are the headaches occurring more often"
W06-3707,E03-2010,1,0.863259,"Missing"
W06-3707,2005.jeptalnrecital-long.17,1,0.922576,"k is for the moment statistical, but rule-based systems are still a very respectable alternative. In particular, nearly all systems which have actually been deployed are rulebased. Prominent examples are (Phraselator, 2006; S-MINDS, 2006; MedBridge, 2006). MedSLT (MedSLT, 2005; Bouillon et al., 2005) is a unidirectional medical speech translation system for use in doctor-patient diagnosis dialogues, which covers several different language pairs and subdomains. Recognition is performed using grammarThe MedSLT demonstrator has already been extensively described elsewhere (Bouillon et al., 2005; Rayner et al., 2005a), so this section will only present a brief summary. The main components are a set of speech recognisers for the source languages, a set of generators for the target languages, a translation engine, sets of rules for translating to and from interlingua, a simple discourse engine for dealing with context-dependent translation, and a top-level which manages the information flow between the other modules and the user. MedSLT also includes an intelligent help module, which adds robustness to the system and guides the user towards the supported coverage. The help module uses a backup recogniser,"
W06-3707,2005.mtsummit-papers.25,1,0.872166,"Missing"
W07-0806,2005.eamt-1.8,1,0.863353,"or: moderate? Trad: !؟23$45 ؟63$45 ؟3$45 (muhtamala, muhtamalan, muhtamal) ‘moderate_fem_attributive_adj, moderate_vocalized-predicative_adj, moderate_attributive_adj’. It is also essential for rules of translation to be applied consistently. For instance, in MedSLT, 42 Doctor: was the onset of headaches sudden? Trad: ة؟9 ,  ا;اع- 7 ( هMedSLT) (hal dhahara al sudaa fajatan?) (Q appear-past-3 the headache suddenly?) The Architecture MedSLT is a grammar-based medical speech translation system which uses the commercial Nuance speech recognition platform. It has two main features (Bouillon et al., 2005). First, all the language models (for recognition, analysis, generation) are produced from linguistically motivated, general unification grammars using the Regulus platform (Rayner, et al., 2006). First, domain specific unification grammars are created from the general grammar for the different domains of medical diagnosis through a trainable corpus-based automatic grammar specialization process. They are, next, compiled into Context Free Grammars (CFGs) in a format suitable for use with the Nuance speech recognition platform, and into a form needed for a variant of Semantic Head-driven genera"
W07-0806,W06-3702,1,0.856343,"Grammars (CFGs) in a format suitable for use with the Nuance speech recognition platform, and into a form needed for a variant of Semantic Head-driven generation (Shieber et al., 1990). Therefore, the different grammars needed by the system under this approach are easy to build and maintain. This leads us to the second feature. Because grammar-based speech recognition only produces competitive results for the sentences covered by the grammar, the user will need to learn the coverage of the system. In order to assist in this, a help system is included in the system (Starlander et al., 2005 and Chatzichrisafis et al., 2006). The help system suggests, after each user utterance, similar utterances covered by the grammar which can be taken as a model. In order to derive the help sentences, the system performs, in parallel, a statistical recognition of the input speech. It then compares the recognition result using an N-gram based metric, against a set of known correct in-coverage questions to extract the most similar ones. It is in that way that we introduce some of the robustness of the statistical systems in the controlled application. Once the sentence recognized, the translation is interlingua-based. Regulus al"
W07-0806,W06-3701,0,0.011483,"h, Japanese, Spanish and Catalan. This article focuses on the system development for Arabic. In general, translation in this context raises two specific questions: 1) how to achieve recognition quality that is good enough for translation, and 2) how to get translations to be as idiomatic as possible so they can be understood by the patient. For close languages and domains where accuracy is not very important (e.g. information requests), it may be possible to combine a statistical recognizer with a commercial translation system as it is often done in commercial tools such as SpokenTranslation (Seligman and Dillinger, 2006). However, for this specific application in a multilingual context, this solution is not applicable at all: even if perfect recognition were possible (which is far from being the case), current commercial tools for translating to Arabic do not guarantee good quality. The domain dealt with here contains, in fact, many structures specific to this type of oral dialogue that can not be handled by these systems. For example, all the doctor’s interactions with the MedSLT system consist of questions whose structures differ from one language to another, with each language having its own constraints. C"
W07-0806,2005.mtsummit-papers.25,1,0.844904,"compiled into Context Free Grammars (CFGs) in a format suitable for use with the Nuance speech recognition platform, and into a form needed for a variant of Semantic Head-driven generation (Shieber et al., 1990). Therefore, the different grammars needed by the system under this approach are easy to build and maintain. This leads us to the second feature. Because grammar-based speech recognition only produces competitive results for the sentences covered by the grammar, the user will need to learn the coverage of the system. In order to assist in this, a help system is included in the system (Starlander et al., 2005 and Chatzichrisafis et al., 2006). The help system suggests, after each user utterance, similar utterances covered by the grammar which can be taken as a model. In order to derive the help sentences, the system performs, in parallel, a statistical recognition of the input speech. It then compares the recognition result using an N-gram based metric, against a set of known correct in-coverage questions to extract the most similar ones. It is in that way that we introduce some of the robustness of the statistical systems in the controlled application. Once the sentence recognized, the translatio"
W07-1806,2006.jeptalnrecital-long.6,1,0.802795,"Missing"
W07-1806,1999.mtsummit-1.8,0,\N,Missing
W07-1806,2005.mtsummit-papers.25,1,\N,Missing
W07-1806,P05-3008,1,\N,Missing
W07-1806,2005.eamt-1.8,1,\N,Missing
W07-1806,W06-3705,0,\N,Missing
W07-1806,W06-3702,1,\N,Missing
W07-1807,2005.eamt-1.8,1,0.91887,"Missing"
W07-1807,P05-3008,1,0.710046,"Missing"
W08-1506,W07-1806,1,0.886739,"Missing"
W08-1506,bouillon-etal-2008-developing,1,0.895278,"fr Abstract 1 Introduction MedSLT is a medium-vocabulary grammar-based medical speech translation system built on top of the Regulus platform (Rayner et al., 2006). It is intended for use in doctor-patient diagnosis dialogues, and provides coverage of several subdomains and a large number of different languagepairs. Coverage is based on standard examination questions obtained from physicians, and focusses primarily on yes/no questions, though there is also support for WH-questions and elliptical utterances. Detailed descriptions of MedSLT can be found in earlier papers (Bouillon et al., 2005; Bouillon et al., 2008)1 . In the rest of this note, we will briefly sketch several versions of the system that we intend to demo at the workshop, each of which displays new features developed over the last year. Section 2 describes an any-language-toany-language multilingual version of the system; Section 3, a bidirectional English ↔ Spanish version; Section 4, a version running on a mobile PDA MedSLT is a grammar-based medical speech translation system intended for use in doctor-patient diagnosis dialogues, which provides coverage of several different subdomains and multiple language pairs. Vocabulary ranges from"
W08-1506,W06-3702,1,0.905634,"Missing"
W08-1506,W07-1807,1,0.893685,"Missing"
W08-1506,tsourakis-etal-2008-building,1,0.874379,"Missing"
W08-1506,2005.eamt-1.8,1,0.858642,"ukie.nakao@univ-nantes.fr Abstract 1 Introduction MedSLT is a medium-vocabulary grammar-based medical speech translation system built on top of the Regulus platform (Rayner et al., 2006). It is intended for use in doctor-patient diagnosis dialogues, and provides coverage of several subdomains and a large number of different languagepairs. Coverage is based on standard examination questions obtained from physicians, and focusses primarily on yes/no questions, though there is also support for WH-questions and elliptical utterances. Detailed descriptions of MedSLT can be found in earlier papers (Bouillon et al., 2005; Bouillon et al., 2008)1 . In the rest of this note, we will briefly sketch several versions of the system that we intend to demo at the workshop, each of which displays new features developed over the last year. Section 2 describes an any-language-toany-language multilingual version of the system; Section 3, a bidirectional English ↔ Spanish version; Section 4, a version running on a mobile PDA MedSLT is a grammar-based medical speech translation system intended for use in doctor-patient diagnosis dialogues, which provides coverage of several different subdomains and multiple language pairs."
W08-1511,1983.tc-1.13,0,0.250159,"Missing"
W08-1702,bouillon-etal-2008-developing,1,0.893366,"speaking or updating a visual display. Input management rules map system inputs to dialogue moves; output management rules map abstract actions to system outputs. Speech translation applications are also rulebased, using an interlingua model (Rayner et al., 2006, Chapter 6). The developer writes a second grammar for the target language, using Regulus tools to compile it into a generator; mappings from source representation to interlingua, and from interlingua to target representation, are defined by sets of translation rules. The interlingua itself is specified using a third Regulus grammar (Bouillon et al., 2008). To summarise, the core of a Regulus application consists of several different linguistically oriented rule-sets, some of which can be interpreted in either a text or a speech modality, and all of which need to interact correctly together. In the next section, we describe how this determines the nature of the Regulus development cycle. switching between modalities in regression testing. Section 6 concludes. 2 The Regulus platform The Regulus platform is a comprehensive toolkit for developing grammar-based speech-enabled systems that can be run on the commercially available Nuance recognition"
W08-1702,P93-1008,0,0.138778,"n modalities in regression testing. Section 6 concludes. 2 The Regulus platform The Regulus platform is a comprehensive toolkit for developing grammar-based speech-enabled systems that can be run on the commercially available Nuance recognition environment. The platform has been developed by an Open Source consortium, the main partners of which have been NASA Ames Research Center and Geneva University, and is freely available for download from the SourceForge website1 . In terms of ideas (though not code), Regulus is a descendent of SRI International’s CLE and Gemini platforms (Alshawi, 1992; Dowding et al., 1993); other related systems are LKB (Copestake, 2002), XLE (Crouch et al., 2008) and UNIANCE (Bos, 2002). Regulus has already been used to build several large applications. Prominent examples are Geneva University’s MedSLT medical speech translator (Bouillon et al., 2005), NASA’s Clarissa procedure browser (Rayner et al., 2005) and Ford Research’s experimental SDS in-car spoken dialogue system, which was awarded first prize at the 2007 Ford internal demo fair. Regulus is described at length in (Rayner et al., 2006), the first half of which consists of an extended tutorial introduction. The release"
W08-1702,W08-0210,0,0.0293883,"Missing"
W08-1702,C02-1095,0,0.0196162,"ehensive toolkit for developing grammar-based speech-enabled systems that can be run on the commercially available Nuance recognition environment. The platform has been developed by an Open Source consortium, the main partners of which have been NASA Ames Research Center and Geneva University, and is freely available for download from the SourceForge website1 . In terms of ideas (though not code), Regulus is a descendent of SRI International’s CLE and Gemini platforms (Alshawi, 1992; Dowding et al., 1993); other related systems are LKB (Copestake, 2002), XLE (Crouch et al., 2008) and UNIANCE (Bos, 2002). Regulus has already been used to build several large applications. Prominent examples are Geneva University’s MedSLT medical speech translator (Bouillon et al., 2005), NASA’s Clarissa procedure browser (Rayner et al., 2005) and Ford Research’s experimental SDS in-car spoken dialogue system, which was awarded first prize at the 2007 Ford internal demo fair. Regulus is described at length in (Rayner et al., 2006), the first half of which consists of an extended tutorial introduction. The release includes a command-line development environment, extensive online documentation, and several exampl"
W08-1702,W07-1807,1,0.904909,"n practice, linguist rulewriters have not been able to test their rules in the speech view without writing glue code, scripts, and other infrastructure required to tie together the various generated components. These are not necessarily things that they want to spend their time doing. The consequence can easily be that the linguists end up working exclusively in the text view, and over-refine the text versions of the rule-sets. From a project management viewpoint, this results in bad prioritisation decisions, since there are more pressing issues to address in the speech view. The Regulus GUI (Kron et al., 2007) is intended as a complete redesign of the development environment, which simultaneously attacks all of the central issues. Commands are organised in a structured set of functionality-based windows, each of which has an appropriate set of drop-down menus. Following normal GUI design practice (Dix et al., 1998, Chapters 3 and 4); (Jacko and Sears, 2003, Chapter 13), only currently meaningful commands are executable in each menu, with the others shown greyed out. Both compile-time and run-time speech-related functionality can be invoked directly from the command menus, with no need for external"
W08-1702,2005.eamt-1.8,1,0.846541,"orm has been developed by an Open Source consortium, the main partners of which have been NASA Ames Research Center and Geneva University, and is freely available for download from the SourceForge website1 . In terms of ideas (though not code), Regulus is a descendent of SRI International’s CLE and Gemini platforms (Alshawi, 1992; Dowding et al., 1993); other related systems are LKB (Copestake, 2002), XLE (Crouch et al., 2008) and UNIANCE (Bos, 2002). Regulus has already been used to build several large applications. Prominent examples are Geneva University’s MedSLT medical speech translator (Bouillon et al., 2005), NASA’s Clarissa procedure browser (Rayner et al., 2005) and Ford Research’s experimental SDS in-car spoken dialogue system, which was awarded first prize at the 2007 Ford internal demo fair. Regulus is described at length in (Rayner et al., 2006), the first half of which consists of an extended tutorial introduction. The release includes a command-line development environment, extensive online documentation, and several example applications. The core functionality offered by Regulus is compilation of typed unification grammars into parsers, generators, and Nuance-formatted CFG language model"
W08-1702,P05-3008,1,0.815415,"in partners of which have been NASA Ames Research Center and Geneva University, and is freely available for download from the SourceForge website1 . In terms of ideas (though not code), Regulus is a descendent of SRI International’s CLE and Gemini platforms (Alshawi, 1992; Dowding et al., 1993); other related systems are LKB (Copestake, 2002), XLE (Crouch et al., 2008) and UNIANCE (Bos, 2002). Regulus has already been used to build several large applications. Prominent examples are Geneva University’s MedSLT medical speech translator (Bouillon et al., 2005), NASA’s Clarissa procedure browser (Rayner et al., 2005) and Ford Research’s experimental SDS in-car spoken dialogue system, which was awarded first prize at the 2007 Ford internal demo fair. Regulus is described at length in (Rayner et al., 2006), the first half of which consists of an extended tutorial introduction. The release includes a command-line development environment, extensive online documentation, and several example applications. The core functionality offered by Regulus is compilation of typed unification grammars into parsers, generators, and Nuance-formatted CFG language models, and hence also into Nuance recognition packages. These"
W08-1702,W07-1806,1,\N,Missing
W08-1702,H93-1008,0,\N,Missing
W09-1503,2008.amta-govandcom.4,1,0.832517,"d a number of non-trivial spoken dialogue systems. Prominent examples include NASA’s Clarissa Procedure Navigator (Rayner et al., 2005), Geneva University’s multi-modal mobile-platform Calendar application (Tsourakis et al., 2008), SDS, a prototype in-car system developed by UC Santa Cruz in collaboration with Ford Motors Research which was voted first in Ford’s 2007 internal technology fair, and Taxi, a speech-enabled game in which the user interacts with a simulated cab driver to navigate around a map of Manhattan. It has also been used to build the MedSLT medical speech translation system (Bouillon et al., 2008). The Regulus platform includes tools for developing feature grammars, and compiling them in various ways. In particular, it is possible to compile grammars into generators, and use them to support paraphrasing from the internal semantic representations created during dialogue processing. This capability is key to the newest part of our regression testing approach, and is discussed in detail in Section 3. First, though, Section 2 gives an overview of Regulus and the architecture of Regulus-based systems; we discuss features that complicate regression testing, and how to address these problems"
W09-1503,W08-0119,0,0.0238802,"Missing"
W09-1503,P05-3008,1,0.816873,"ses the methodology we have developed to address regression testing issues within the Regulus framework. Regulus (Rayner et al., 2006) is an Open Source toolkit for builting medium 14 Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 14–21, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics vocabulary spoken dialogue and translation applications, and has been used to build a number of non-trivial spoken dialogue systems. Prominent examples include NASA’s Clarissa Procedure Navigator (Rayner et al., 2005), Geneva University’s multi-modal mobile-platform Calendar application (Tsourakis et al., 2008), SDS, a prototype in-car system developed by UC Santa Cruz in collaboration with Ford Motors Research which was voted first in Ford’s 2007 internal technology fair, and Taxi, a speech-enabled game in which the user interacts with a simulated cab driver to navigate around a map of Manhattan. It has also been used to build the MedSLT medical speech translation system (Bouillon et al., 2008). The Regulus platform includes tools for developing feature grammars, and compiling them in various ways. In par"
W09-1503,P00-1013,0,0.0223923,"lopment cycle. We will in particular be interested in regression testing. 3 Context, regression testing and paraphrasing The three main components of the spoken dialogue system — the IM, DM and OM — all transform one or more inputs into one or more outputs. With the current focus on machine learning techniques, a natural thought is to learn the relevant tranformations from examples. Implemented mainly through Partially Observable Markov Decision Processes (POMDPs), this idea is attractive theoretically, but has been challenging to scale up. Systems have been restricted to very simple domains (Roy et al., 2000; Zhang et al., 2001) and only recently have techniques been developed that show promise for use in real-world systems (Williams and Young, 2007; Gasi´c et al., 2008). The representations required in many systems are more complex than those employed even in the more recent POMDP based work, and there is also the usual problem that it is not easy to obtain training data. In practice, most people are forced to construct the transformation rules by hand; the Regulus framework assumes this will be the case. Hand-coding of dialogue processing components involves the usual software engineering probl"
W09-1503,tsourakis-etal-2008-building,1,0.851551,"s framework. Regulus (Rayner et al., 2006) is an Open Source toolkit for builting medium 14 Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 14–21, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics vocabulary spoken dialogue and translation applications, and has been used to build a number of non-trivial spoken dialogue systems. Prominent examples include NASA’s Clarissa Procedure Navigator (Rayner et al., 2005), Geneva University’s multi-modal mobile-platform Calendar application (Tsourakis et al., 2008), SDS, a prototype in-car system developed by UC Santa Cruz in collaboration with Ford Motors Research which was voted first in Ford’s 2007 internal technology fair, and Taxi, a speech-enabled game in which the user interacts with a simulated cab driver to navigate around a map of Manhattan. It has also been used to build the MedSLT medical speech translation system (Bouillon et al., 2008). The Regulus platform includes tools for developing feature grammars, and compiling them in various ways. In particular, it is possible to compile grammars into generators, and use them to support paraphrasi"
W09-2607,2008.amta-govandcom.4,1,\N,Missing
W09-2607,E06-1008,0,\N,Missing
W09-2607,P02-1040,0,\N,Missing
W09-2607,W08-0327,0,\N,Missing
W09-2607,2006.amta-papers.24,0,\N,Missing
W09-2607,2005.eamt-1.8,1,\N,Missing
W09-2607,W08-1511,1,\N,Missing
W09-2607,P00-1056,0,\N,Missing
W09-2607,W06-3702,1,\N,Missing
W13-2816,P11-2031,0,0.01261,"in the observed translation input sentence is produced while the writer has a particular “true” word wi ∈ Ci in mind, where Ci is the set of words confusable with w ˆi . For the sake of simplicity, we assume that within a confusion set, all “true word” options are equally likely, i.e., p(w ˆi |wi = x) = 1 for x ∈ C . The writer chooses the next i |Ci | word wi+1 according to the conditional word bigram probability p(wi+1 |wi ). 3.3 Automatic evaluation (BLEU) Due to the relatively small size of the evaluation set and instability inherent in minimum error rate training (Foster and Kuhn, 2009; Clark et al., 2011), results of individual tuning and evaluation runs can be unreliable. We therefore preformed multiple tuning and evaluation runs for each system (baseline, rule-based and weighted graph). To illustrate the precision of the BLEU score on our data sets, we plot in Fig. 2 for each individual tuning run the BLEU score achieved on the tuning set (x-axis) against the performance on the evaluation set (y-axis). The variance along the x-axis for each system is due to search errors in parameter optimization. Since the search space is not convex, the tuning process can get stuck in local maxima. The app"
W13-2816,C12-2032,0,0.0222513,"translations produced by the SMT engine from plain and corrected versions. confusions. The second is an engineering method: we use a commercial pronunciation-generation tool to generate a homophone dictionary, then use this dictionary to turn the input into a weighted graph where each word is replaced by a weighted disjunction of homophones. Related, though less elaborate, work has been reported by Bertoldi et al. (2010), who address spelling errors using a character-level confusion network based on common character confusions in typed English and test them on artificially created noisy data. Formiga and Fonollosa (2012) also used character-based models to correct spelling on informally written English data. The two approaches in the present paper exploit fundamentally different knowledge sources in trying to identify and correct homophone errors. The rule-based method relies exclusively on source-side information, encoding patterns indicative of common French homophone confusions. The weighted graph method shifts the balance to the target side; the choice between potential homophone alternatives is made primarily by the target language model, though the source language weights and the translation model are a"
W13-2816,W09-0439,0,0.0299907,"ose that each word w ˆi in the observed translation input sentence is produced while the writer has a particular “true” word wi ∈ Ci in mind, where Ci is the set of words confusable with w ˆi . For the sake of simplicity, we assume that within a confusion set, all “true word” options are equally likely, i.e., p(w ˆi |wi = x) = 1 for x ∈ C . The writer chooses the next i |Ci | word wi+1 according to the conditional word bigram probability p(wi+1 |wi ). 3.3 Automatic evaluation (BLEU) Due to the relatively small size of the evaluation set and instability inherent in minimum error rate training (Foster and Kuhn, 2009; Clark et al., 2011), results of individual tuning and evaluation runs can be unreliable. We therefore preformed multiple tuning and evaluation runs for each system (baseline, rule-based and weighted graph). To illustrate the precision of the BLEU score on our data sets, we plot in Fig. 2 for each individual tuning run the BLEU score achieved on the tuning set (x-axis) against the performance on the evaluation set (y-axis). The variance along the x-axis for each system is due to search errors in parameter optimization. Since the search space is not convex, the tuning process can get stuck in"
W13-2816,2005.mtsummit-papers.11,0,0.00505547,"s trained from a small bicorpus of domain language. With automatic evaluation, the weighted graph method yields an improvement of about +0.63 BLEU points, while the rulebased method scores about the same as the baseline. On contrastive manual evaluation, both methods give highly significant improvements (p < 0.0001) and score about equally when compared against each other. 1 Introduction and motivation The data used to train Statistical Machine Translation (SMT) systems is most often taken from the proceedings of large multilingual organisations, the generic example being the Europarl corpus (Koehn, 2005); for academic evaluation exercises, the test data may well also be taken from the same source. Texts of this kind are carefully cleaned-up formal language. However, real MT systems often need to handle text from very different genres, which as usual causes problems. This paper addresses a problem common in domains containing informally written text: spelling errors based on homophone confusions. Concretely, the work reported was carried out in the context of the ACCEPT project, which deals with the increasingly important topic of translating online forum posts; the experiments we describe wer"
W13-2816,N10-1064,0,0.0214978,"ins on ne rec¸oit pas l’alerte). ... (at least we do not recoit alert). .. (at least it does not receive the alert). Figure 1: Examples of homophone errors in French forum data, contrasting English translations produced by the SMT engine from plain and corrected versions. confusions. The second is an engineering method: we use a commercial pronunciation-generation tool to generate a homophone dictionary, then use this dictionary to turn the input into a weighted graph where each word is replaced by a weighted disjunction of homophones. Related, though less elaborate, work has been reported by Bertoldi et al. (2010), who address spelling errors using a character-level confusion network based on common character confusions in typed English and test them on artificially created noisy data. Formiga and Fonollosa (2012) also used character-based models to correct spelling on informally written English data. The two approaches in the present paper exploit fundamentally different knowledge sources in trying to identify and correct homophone errors. The rule-based method relies exclusively on source-side information, encoding patterns indicative of common French homophone confusions. The weighted graph method s"
W13-2816,2012.amta-papers.25,1,0.781932,", achieves an average BLEU score of 42.47 on this set. 3.1 The rule-based approach Under the ACCEPT project, a set of lightweight pre-editing rules have been developed specifically for the Symantec Forum translation task. Some of the rules are automatic (direct reformulations); others present the user with a set of suggestions. The evaluations described in Gerlach et al. (2013) demonstrate that pre-editing with the rules has a significant positive effect on the quality of SMTbased translation. The implemented rules address four main phenomena: differences between informal and formal language (Rayner et al., 2012), differences between local French and English word-order, elThe set of Acrolinx pre-editing rules potentially relevant to resolution of homophone errors was applied to the devtest b set test corpus (Section 2.1). In order to be able to make a fair comparison with the weighted-graph method, we only used rules with a unique suggestion, which could be run automatically. Applying these rules produced 430 changed words in the test corpus, but did not change the average BLEU score significantly (42.38). Corrections made with a human in the loop, used as “oracle” input for the SMT system, by the 111"
W13-2816,bredenkamp-etal-2000-looking,0,\N,Missing
W13-2816,P07-2045,0,\N,Missing
W13-2816,P00-1056,0,\N,Missing
W97-0411,E95-1028,1,0.921845,"killed grammarian. For a given language pair, the more complex transfer rules, which tend to be for function .words and other commonly-occurring, idiosyncratic words, can also involve arbitrarily large, recursive structures. However, nearly all of these monolingual and bilingual rules are domainindependent. O n the other side of the coin, the main domaindependent aspects of a linguistic description are tE.g. in our initial extension from English to languages with more complicated morphology, which necessitated the development of a morphological processor based on the two-level formalism (see (Carter, 1995)). lexicon entries defining content words in terms of existing behaviours, and simple (atomic-toatomic) transfer rules. These do need to be created manually for each new domain, but they are simple enough to be defined by non-experts with the help of relatively simple graphical tools. See Figures i and 2 for some examples of these two kinds of rule (the details of the formalism are unimportant here, we intend simply to illustrate the differences in complexity). W h e n moving to a new language, more expert intervention is typically required than for a new domain, because m a n y of the complex"
W97-0411,W97-1502,1,0.734533,"tforward as possible. These ends can be achieved by using general-purpose components for both speech and language processing and training them on domain-specific speech and text corpora. The training should be automated whenever possible, and where human intervention is required, the process should be deskilled to the level where, ideally, it can be carried out by people who are familiar with the domain but are not experts in the systems themselves. These points will be discussed in the context of the Spoken Language Translator (SLT) (Rayner, Alshawi eta/, 1993; Agn~s et al., 1994; Rayner and Carter, 1997), a customizable speech translator built as a pipelined sequence of generalpurpose components. These components are: a version of the Decipher (TM) speech recognizer (Murveit eta/, 1993) for the source language; a copy of the Core Language Engine (CLE) (Alshawi (ed), 1992) for the source language; another copy of the C L E for the target language; and a target language text-to-speech synthesizer. The current SLT system carries out multilingual speech translation in near real time in the ATIS domain (Hemphill et al., 1990) for several language pairs. Good demonstration versions exist for the fo"
W97-0411,A94-1016,0,0.0420329,"©form(prep('de bonne beure_Early'),_, P&quot; [P,tr(arg), @term (ref (pro, de _bonne _heure, sing, _), V,W&quot; [time,W] )+_] )). Figure i: Complex, domain-independent linguisticrules a selectionto be made between competing full analyses. See (Alshawi and Carter, 1994) and (Carter, 1997) for details. A similar mechanism has been developed to allow users to specify appropriate translations,giving rise to preferences on outcomes of the transfer process. Work on this continues. 3 Robustness Robustness in the face of ill-formed input and recognition errors is tackled by means of a &quot;multiengine&quot; strategy (Frederking and Nirenburg, 1994; Rayner and Carter, 1997), combining two different translation methods. The main translation method uses transfer at the levelof Q L F (Alshawi et al., 1991; Rayner and Bouillon, 1995); this is supplemented by a simpler, glossary-based translation method. Processing is carried out bottomup. Roughly speaking, the Q L F transfer method is used to translate as much as possible of the input utterance, any remaining gaps being filledby application of the glossary-based method. In more detail, source-language parsing goes through successive stages of lexical (morphological) analysis, low-level phra"
W97-0411,H90-1021,0,0.038802,"uage Translator (SLT) (Rayner, Alshawi eta/, 1993; Agn~s et al., 1994; Rayner and Carter, 1997), a customizable speech translator built as a pipelined sequence of generalpurpose components. These components are: a version of the Decipher (TM) speech recognizer (Murveit eta/, 1993) for the source language; a copy of the Core Language Engine (CLE) (Alshawi (ed), 1992) for the source language; another copy of the C L E for the target language; and a target language text-to-speech synthesizer. The current SLT system carries out multilingual speech translation in near real time in the ATIS domain (Hemphill et al., 1990) for several language pairs. Good demonstration versions exist for the four pairs English ~ Swedish, English French, Swedish ~ English and Swedish Danish. Preliminary versions exist for five more pairs: Swedish ~ French, French --~ English, English ~ Danish, French --d.Spanish and English --~ Spanish. W e describe the methodology used to build the SLT system itself,particularly in the areas of customization (Section 2), robustness (Section 3), and multilinguality (Section 4). For further details on the topics of customization and multilin73 guality, see (Rayner, Bretan et al, 1996; Rayner, Car"
W97-0411,H93-1042,1,0.896049,"Missing"
W97-0411,1995.tc-1.11,1,0.83868,"Missing"
W97-0411,P96-1030,1,0.829214,"a few person weeks. The principal information extracted automatically from a judged corpus is: • Constituent pruning rules, which allow the detection and removal, at intermediate stages of parsing, of syntactic constituents occurring in contexts where they are unlikely to contribute to the correct parse. Removing these constituents significantly constrains the search space and speeds up parsing (Rayner and Carter, 1997). * A n automatic tuning of the grammar to the domain using the technique of ExplanationBased Learning (van Harmelen and Bundy, 1988; Rayner, 1988; Samuelsson and Rayner, 1991; Rayner and Carter, 1996). This rewrites it to a form where only commonlyoccurring rule combinations are represented, thus reducing the search space still further and giving an additional significant speedup. • Preference information attached to certain characteristics of full analyses of sentences the most important being semantic triples of head, relationship and modifier - which allow 74 Syntax rule ~r S--~NP VP: syn(s_np_vp_Normal, core, [s:[@s_np_feats(MMM), @vp_feats(MM), ~sententialsubj=SS,sai=Aux, hascomp=n,conjoined=n], np:[@snp_feats(MMM),vform=(fin/to), relational=_,temporal=_,agr=Ag, sentential=SS, wh=_,"
W97-0910,E95-1028,1,0.844521,"histione. cated translation method is also used, which simply maps surface phrases from the source language into possible target-language counterparts. We refer to the backup method as ""word-to-word"" (WW) translation. The two methods are combined, roughly speaking, by using rule-based QLF transfer to translate as much as possible, filling in any gaps with applications of the WW rules. The parts of the system of central interest here are the rule-based components, in particular the morphologies, grammars, lexica and transfer rules. Morphologies are written in a variant of two-level morphology (Carter, 1995), and grammars in a unification-based formalism (Alshawi (ed), 1992). The lexicon for each language is divided into three main parts: Domain-independent function (closed class) word entries are written directly in terms of definitions of suitable feature-value assignments, and can from a software-engineering standpoint be regarded as part of the grammar. A collection of language-dependent but domainindependent macros define the feature-value assignments needed for each type of regular content-word, e.g. ""count noun"", ""transitive verb"" and so on. These macros are called paradigm macros. Content"
W97-0910,H90-1021,0,0.0518195,"Missing"
W97-0910,1995.tc-1.11,1,0.839756,"Missing"
