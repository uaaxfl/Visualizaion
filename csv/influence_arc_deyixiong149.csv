2007.iwslt-1.17,P96-1021,0,0.0581114,"Missing"
2007.iwslt-1.17,N03-1017,0,0.00951,"Missing"
2007.iwslt-1.17,P02-1038,0,0.132631,"Missing"
2007.iwslt-1.17,koen-2004-pharaoh,0,\N,Missing
2007.iwslt-1.17,I05-1007,1,\N,Missing
2007.iwslt-1.17,P06-1077,1,\N,Missing
2007.iwslt-1.17,P06-1066,1,\N,Missing
2007.iwslt-1.17,P00-1056,0,\N,Missing
2007.iwslt-1.17,P03-1021,0,\N,Missing
2008.iwslt-evaluation.6,E06-1005,0,0.054695,"system combination methods that we have explored. The performance on development and test sets are reported in detail in the paper. The system has shown competitive performance with respect to the BLEU and METEOR measures in Chinese-English Challenge and BTEC tasks. 1. Introduction This paper describes the machine translation (MT) system and approach explored by the Institute for Infocomm Research (I2R) for the International Workshop on Spoken Language Translation (IWSLT) 2008. We submitted runs under the open data conditions for Chinese-to-English BTEC and Challenge tasks. System combination [1, 2, 3] has demonstrated its advantage in the recent machine translation evaluation campaign [4, 5]. In our system, a multi-pass SMT approach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders"
2008.iwslt-evaluation.6,N07-1029,0,0.0312943,"system combination methods that we have explored. The performance on development and test sets are reported in detail in the paper. The system has shown competitive performance with respect to the BLEU and METEOR measures in Chinese-English Challenge and BTEC tasks. 1. Introduction This paper describes the machine translation (MT) system and approach explored by the Institute for Infocomm Research (I2R) for the International Workshop on Spoken Language Translation (IWSLT) 2008. We submitted runs under the open data conditions for Chinese-to-English BTEC and Challenge tasks. System combination [1, 2, 3] has demonstrated its advantage in the recent machine translation evaluation campaign [4, 5]. In our system, a multi-pass SMT approach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders"
2008.iwslt-evaluation.6,P07-2045,0,0.0105658,"ed its advantage in the recent machine translation evaluation campaign [4, 5]. In our system, a multi-pass SMT approach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders, namely Moses [6] (an open source phrasebased MT system), JosHUa [7] (a hierarchical phrase-based translation system) and Tranyu [8] (an in-house linguistically-annotated BTG-based decoder) to generate 2Nbest lists of hypotheses for each decoder. Then each 2N-best lists are rescored and re-ranked with additional feature functions. The 1-best and top N-best lists of these re-ranked lists are then used in system combination1. Secondly, we construct system combination in two-stage. In the first stage, two strategies are applied. The first strategy is n-gram expansion by which we spawn new translation entries thro"
2008.iwslt-evaluation.6,W08-0402,0,0.0242585,"evaluation campaign [4, 5]. In our system, a multi-pass SMT approach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders, namely Moses [6] (an open source phrasebased MT system), JosHUa [7] (a hierarchical phrase-based translation system) and Tranyu [8] (an in-house linguistically-annotated BTG-based decoder) to generate 2Nbest lists of hypotheses for each decoder. Then each 2N-best lists are rescored and re-ranked with additional feature functions. The 1-best and top N-best lists of these re-ranked lists are then used in system combination1. Secondly, we construct system combination in two-stage. In the first stage, two strategies are applied. The first strategy is n-gram expansion by which we spawn new translation entries through a word-based n-gram language model estimated on"
2008.iwslt-evaluation.6,P06-1066,1,0.866553,"oach is exploited which consists of decoding, regeneration, rescoring and system combination. First, multiple systems based on different translation strategies are used to generate various Nbest lists. This aims to leverage on the strength of different translation methods. Then three kinds of different system combination methods are applied in a two-stage procedure to find the 1-best translation. Figure 1 depicts our system architecture. First, we use three decoders, namely Moses [6] (an open source phrasebased MT system), JosHUa [7] (a hierarchical phrase-based translation system) and Tranyu [8] (an in-house linguistically-annotated BTG-based decoder) to generate 2Nbest lists of hypotheses for each decoder. Then each 2N-best lists are rescored and re-ranked with additional feature functions. The 1-best and top N-best lists of these re-ranked lists are then used in system combination1. Secondly, we construct system combination in two-stage. In the first stage, two strategies are applied. The first strategy is n-gram expansion by which we spawn new translation entries through a word-based n-gram language model estimated on the input hypotheses. Then input hypotheses and the newly-gener"
2008.iwslt-evaluation.6,J03-1002,0,0.00526627,"Section 3 details the rescoring models. Section 4 describes three system combination strategies: n-gram expansion, simple cascading and weighted voting. Section 5 reports the experimental setups and results while Section 6 concludes the paper. 2. The SMT Models To integrate the advantages of the state-of-the-art translation methods, we use three different SMT models, phrase-based, hierarchical phrase-based and linguistically-annotated BTGbased in the first pass to generate N-best hypotheses. The three methods share the some common features: word alignment of training data obtained from GIZA++ [9], Language model(s) (LM) trained using SRILM toolkit [10] with modified Kneser-Ney smoothing method [11]. 2.1. Phrasal translation system Phrase-based SMT systems are usually modeled through a log-linear framework [12]. By introducing the hidden word alignment variable a [13], the optimal translation can be searched for based on the following criterion: M e * = arg max(∑ m =1 λm hm (e, f , a)) where e is a string of phrases in the target language, the source language string of phrases, feature functions, weights 1 Since our system combination method n-gram expansion [3] is based on a gener"
2008.iwslt-evaluation.6,P02-1038,0,0.0901849,"ion 6 concludes the paper. 2. The SMT Models To integrate the advantages of the state-of-the-art translation methods, we use three different SMT models, phrase-based, hierarchical phrase-based and linguistically-annotated BTGbased in the first pass to generate N-best hypotheses. The three methods share the some common features: word alignment of training data obtained from GIZA++ [9], Language model(s) (LM) trained using SRILM toolkit [10] with modified Kneser-Ney smoothing method [11]. 2.1. Phrasal translation system Phrase-based SMT systems are usually modeled through a log-linear framework [12]. By introducing the hidden word alignment variable a [13], the optimal translation can be searched for based on the following criterion: M e * = arg max(∑ m =1 λm hm (e, f , a)) where e is a string of phrases in the target language, the source language string of phrases, feature functions, weights 1 Since our system combination method n-gram expansion [3] is based on a generative language model that is trained on the input hypotheses lists, the hypotheses quality is very important to the performance of the n-gram expansion method. Therefore, we filter out N-worse hypotheses from the 2N-be"
2008.iwslt-evaluation.6,P03-1021,0,0.044622,"m =1 λm hm (e, f , a)) where e is a string of phrases in the target language, the source language string of phrases, feature functions, weights 1 Since our system combination method n-gram expansion [3] is based on a generative language model that is trained on the input hypotheses lists, the hypotheses quality is very important to the performance of the n-gram expansion method. Therefore, we filter out N-worse hypotheses from the 2N-best lists before passing them to the n-gram expansion model. (1) e ,a λm hm (e, f , a ) f is are are typically optimized to maximize the scoring function [14]. Our phrasal translation system is based on the Moses open source package [6]. IBM word reordering constraints [15] are applied during decoding to reduce the computational - 46 - Proceedings of IWSLT 2008, Hawaii - U.S.A. Figure 1: system architecture complexity. The other models and feature functions employed by Moses decoder are: • Translation model(s) (TM), direct phrase/word based translation model and inverse • Distortion model, which assigns a cost linear to the reordering distance, the cost is based on the number of source words which are skipped when translating a new source phrase •"
2008.iwslt-evaluation.6,2005.iwslt-1.8,0,0.0381389,"s based on the Moses open source package [6]. IBM word reordering constraints [15] are applied during decoding to reduce the computational - 46 - Proceedings of IWSLT 2008, Hawaii - U.S.A. Figure 1: system architecture complexity. The other models and feature functions employed by Moses decoder are: • Translation model(s) (TM), direct phrase/word based translation model and inverse • Distortion model, which assigns a cost linear to the reordering distance, the cost is based on the number of source words which are skipped when translating a new source phrase • Lexicalized word reordering model [16] (RM) • Word and phrase penalties, which count the numbers of words and phrases in the target string The translation model, reordering model and feature weights are trained and optimized using Moses training and tuning toolkits. Two different N-best lists are generated by the same Moses decoder with the same source input but different preprocessing. 2.2. Hierarchical phrase-based translation system Hierarchical phrase-based translation method is a typical formally syntax-based translation modeling method. Empirically, it has demonstrated better performance than the phrase-based method because"
2008.iwslt-evaluation.6,J07-2003,0,0.103377,"ses in the target string The translation model, reordering model and feature weights are trained and optimized using Moses training and tuning toolkits. Two different N-best lists are generated by the same Moses decoder with the same source input but different preprocessing. 2.2. Hierarchical phrase-based translation system Hierarchical phrase-based translation method is a typical formally syntax-based translation modeling method. Empirically, it has demonstrated better performance than the phrase-based method because it permits phrases with gaps by generalizing the normal phrase-based models [17, 7]. Formally, the hierarchical phrase-based translation model is a weighted synchronous context free grammar. In our system combination framework, for the hierarchical phrase-based translation component, we use the default setting as discussed in [17] for training and tuning and use JosHUa [7]’s implementation for decoding. 2.3. Linguistically annotated BTG-based system Tranyu is an in-house formally and linguistically syntaxbased SMT system, which adopts the bracketing transduction grammars (BTG) as the fundamental framework for phrase translation and reordering. The BTG lexical rules (A --&gt; x/"
2008.iwslt-evaluation.6,P08-2038,1,0.868481,"el uses boundary words of neighboring phrases as features [8], which we call the boundary words based reordering model (BWR). The second model uses linguistic annotations of each BTG node as features, which are automatically learned by projecting source-side parse trees onto the corresponding binary trees generated by BTG. We call the second model the linguistically annotated reordering model (LAR). Based on these two reordering models, we developed two variations of Tranyu. The first variation Tranyu1 only uses the BWR model [8] while the second variation Tranyu2 uses both BWR and LAR models [18]. 3. Rescoring models Rescoring operation plays a very important role in our system. A rich global feature functions set benefits our system greatly. The rescoring models are the same ones which were used in our SMT system for IWSLT 2007 [4]. We apply the - 47 - Proceedings of IWSLT 2008, Hawaii - U.S.A. following feature functions. Weights of feature functions are optimized by the MERT tool in Moses package. • direct and inverse IBM model 1 and 3 and two combined systems. The feature weight of each system is tuned over the development set. If all the weights are set to 1, then we call it simp"
2008.iwslt-evaluation.6,2006.iwslt-papers.4,1,0.869059,"system greatly. The rescoring models are the same ones which were used in our SMT system for IWSLT 2007 [4]. We apply the - 47 - Proceedings of IWSLT 2008, Hawaii - U.S.A. following feature functions. Weights of feature functions are optimized by the MERT tool in Moses package. • direct and inverse IBM model 1 and 3 and two combined systems. The feature weight of each system is tuned over the development set. If all the weights are set to 1, then we call it simple voting. • association scores, i.e. hyper-geometric distribution probabilities and mutual information • lexicalized reordering rule [19] • 6-gram target language model and 8-gram target wordclass based LM, word-classes are clustered by GIZA++ • length ratio between source and target sentence • question feature [20] • Linear sum of n-grams (n=1,2,3,4) relative frequencies within all translations [20] • n-gram and sentence length posterior probabilities within the N-best translations [21] 5. Experiments We participated Chinese-to-English BTEC task (BT) and Challenge task (CT) in open data track for IWSLT 2008. 5.1. Preprocessing Preprocessing includes Chinese word segmentation, English tokenization, and transformation of numbers"
2008.iwslt-evaluation.6,J93-2003,0,\N,Missing
2008.iwslt-evaluation.6,C08-1014,1,\N,Missing
2008.iwslt-evaluation.6,2007.iwslt-1.8,1,\N,Missing
2008.iwslt-evaluation.6,P02-1040,0,\N,Missing
2008.iwslt-evaluation.6,W06-3110,0,\N,Missing
2008.iwslt-evaluation.6,takezawa-etal-2002-toward,0,\N,Missing
2008.iwslt-evaluation.6,2005.iwslt-1.11,1,\N,Missing
2008.iwslt-evaluation.6,W03-1730,1,\N,Missing
2009.iwslt-evaluation.7,N07-1029,0,0.0408514,"y optimized to maximize the scoring function [11]. IBM word reordering constraints [13] are applied during decoding to reduce the computational complexity. The other models and feature functions employed by Lavender are: • Translation model(s) (TM), direct and inverse phrase/word based translation model 1. Introduction This paper describes the machine translation (MT) system and approach explored by the Institute for Infocomm Research (I2R) for the International Workshop on Spoken Language Translation (IWSLT) 2009. Basically, our MT system is a system combination framework. System combination [1, 2, 3] has demonstrated its advantage in the recent machine translation evaluation campaign [4, 5]. In our system combination framework, we adopt mainly two kinds of statistical machine translation (SMT) methods: phrase-based SMT and syntax-based SMT. For syntax-based system, we developed three variations. Totally, we applied four SMT systems. Based on outputs of four single systems, we applied rescoring method [4] to incorporate rich global features. Finally, we adopt two kinds of system combination methods, namely, n-gram expansion [3] and weighted voting, on all rescoring outputs. The rest of pap"
2009.iwslt-evaluation.7,J03-1002,0,0.0057466,"Missing"
2009.iwslt-evaluation.7,P02-1038,0,0.373791,"f the different individual systems. Rescoring is applied on each single system output, and system combination is applied on all rescoring outputs. Finally, our system combination framework shows better performance in Chinese-English BTEC task. Lavender [19] is our newly-developed in-house SMT translation platform, including a phrase-based decoder and most of the current linguistically motivated syntax-based system. Its phrase-based component, which functions very similar to Moses [12], is used as the phrase-based decoder for this campaign. Phrase-based SMT usually adopt a log-linear framework [9]. By introducing the hidden word alignment variable a [10], the optimal translation can be searched for based on the following criterion: ~ M e~ * = arg max( λ h (e~, f , a)) e,a ∑m =1 m m ~ where ~ e is a string of phrases in the target language, f is the ~ source language string of phrases, h (e~, f , a) are feature funcm tions, weights λm are typically optimized to maximize the scoring function [11]. IBM word reordering constraints [13] are applied during decoding to reduce the computational complexity. The other models and feature functions employed by Lavender are: • Translation model(s)"
2009.iwslt-evaluation.7,P03-1021,0,0.038379,"-based system. Its phrase-based component, which functions very similar to Moses [12], is used as the phrase-based decoder for this campaign. Phrase-based SMT usually adopt a log-linear framework [9]. By introducing the hidden word alignment variable a [10], the optimal translation can be searched for based on the following criterion: ~ M e~ * = arg max( λ h (e~, f , a)) e,a ∑m =1 m m ~ where ~ e is a string of phrases in the target language, f is the ~ source language string of phrases, h (e~, f , a) are feature funcm tions, weights λm are typically optimized to maximize the scoring function [11]. IBM word reordering constraints [13] are applied during decoding to reduce the computational complexity. The other models and feature functions employed by Lavender are: • Translation model(s) (TM), direct and inverse phrase/word based translation model 1. Introduction This paper describes the machine translation (MT) system and approach explored by the Institute for Infocomm Research (I2R) for the International Workshop on Spoken Language Translation (IWSLT) 2009. Basically, our MT system is a system combination framework. System combination [1, 2, 3] has demonstrated its advantage in the r"
2009.iwslt-evaluation.7,P07-2045,0,0.00954186,"ng method to improve the individual system performance and use system combination method to combine the strengths of the different individual systems. Rescoring is applied on each single system output, and system combination is applied on all rescoring outputs. Finally, our system combination framework shows better performance in Chinese-English BTEC task. Lavender [19] is our newly-developed in-house SMT translation platform, including a phrase-based decoder and most of the current linguistically motivated syntax-based system. Its phrase-based component, which functions very similar to Moses [12], is used as the phrase-based decoder for this campaign. Phrase-based SMT usually adopt a log-linear framework [9]. By introducing the hidden word alignment variable a [10], the optimal translation can be searched for based on the following criterion: ~ M e~ * = arg max( λ h (e~, f , a)) e,a ∑m =1 m m ~ where ~ e is a string of phrases in the target language, f is the ~ source language string of phrases, h (e~, f , a) are feature funcm tions, weights λm are typically optimized to maximize the scoring function [11]. IBM word reordering constraints [13] are applied during decoding to reduce the"
2009.iwslt-evaluation.7,2005.iwslt-1.8,0,0.128635,"] and weighted voting, on all rescoring outputs. The rest of paper is organized as follows. Section 2 presents each individual SMT system used in our framework. Section 3 details the rescoring method. Section 4 describes two system combination strategies: n-gram expansion and weighted voting. Section 5 reports the experimental setups and results while Section 6 concludes the paper. • Distortion model, which assigns a cost linear to the reordering distance, the cost is based on the number of source words which are skipped when translating a new source phrase • Lexicalized word reordering model [14] (RM) • Word and phrase penalties, which count the numbers of words and phrases in the target string The translation model, reordering model and feature weights are trained and optimized using Moses training and tuning toolkits [12]. 2.2. Tranyu: Syntax-based Translation System Tranyu is our another in-house translation platform. It is a formally syntax-based SMT system, which adapts the bracketing transduction grammars (BTG) for phrase translation and reordering. The BTG lexical rules (A --&gt; x/y) are used to translate source phrase x into target phrase y while the BTG merging rules (A --&gt; [A,"
2009.iwslt-evaluation.7,2006.iwslt-papers.4,0,0.0312358,"using boundary words of these examples and finally estimate feature weights. 3. Rescoring Models Rescoring operation plays a very important role in our system. A rich global feature functions set benefits our system greatly. The rescoring models are the same ones which were used in our SMT system for IWSLT 2007 [4]. We apply the following feature functions. Weights of feature functions are optimized by the MERT tool in Moses package. • direct and inverse IBM model 1 and 3 • association scores, i.e. hyper-geometric distribution probabilities and mutual information • lexicalized reordering rule [15] • 6-gram target language model and 8-gram target wordclass based LM, word-classes are clustered by GIZA++ • length ratio between source and target sentence • question feature • Linear sum of n-grams (n=1,2,3,4) relative frequencies within all translations, which favors the hypotheses containing popular n-grams of higher order [16] • Tranyu(LAR). In order to employ more linguistic knowledge in the ITG reordering, we extend boundary word based reordering further by linguistically annotating each node involved in reordering according to the source-side parse tree. We call this linguistically ann"
2009.iwslt-evaluation.7,J93-2003,0,\N,Missing
2009.iwslt-evaluation.7,E06-1005,0,\N,Missing
2009.iwslt-evaluation.7,2007.iwslt-1.8,1,\N,Missing
2009.iwslt-evaluation.7,P09-1036,1,\N,Missing
2009.iwslt-evaluation.7,W06-3110,0,\N,Missing
2009.iwslt-evaluation.7,P06-1066,1,\N,Missing
2009.iwslt-evaluation.7,P08-2038,1,\N,Missing
2009.iwslt-evaluation.7,I08-1066,1,\N,Missing
2009.iwslt-evaluation.7,I05-3025,0,\N,Missing
2009.iwslt-evaluation.7,2005.iwslt-1.11,0,\N,Missing
2009.iwslt-evaluation.7,N06-1014,0,\N,Missing
2009.mtsummit-posters.24,P07-1019,0,0.175412,"cell is p(H) = pin (H) · π(H)λLM where pin (H) is the probability estimated from inside the hypothesis H, λLM is the weight of the language model. Note that this probability is only used for the beam thresholding. 4 Comparison to Previous Work Efficient decoding is of great importance to rapid SMT development and commercial applications. Much of previous work focuses on reducing the overwhelming overhead introduced by the intersection of the m-gram language model and the translation model (phrase-based or syntaxbased). This is the fundamental motivation for cube pruning/growing(Chiang, 2007; Huang and Chiang, 2007), and multi-pass decoding approaches(Venugopal et al., 2007; Zhang and Gildea, 2008). Other efforts have been made for A* decoding using search heuristics (Och et al., 2001; Zhang and Gildea, 2006). The Pharaoh decoder (Koehn, 2004) uses an estimated score of uncovered source sequences as an important component to compare hypotheses. In A* decoding (Och et al., 2001; Zhang and Gildea, 2006), a heuristic function is used to estimate the probability to complete a partial hypothesis. To some extent, both are similar to our LMLA probability. The biggest difference is that we emphasize the effect o"
2009.mtsummit-posters.24,W05-1507,0,0.0135781,"ing to the following two reorderings. If a straight order is preferred (Fig. 2(a)), the language model look-ahead probability πs (H) can be estimated as follows πs (H) = m-gram(T r (s1 ...si−1 ), H l ) ·m-gram(H r , T l (sj+1 ...sn )) where H l/r are the leftmost/rightmost boundary words of H, which both include m0 = min(m − 4 The reason for caching m0 words is to keep the same with what we do for each hypothesis, where m0 words are also stored on the left/right of the hypothesis for the dynamic programming to compute new m-grams in the CKY algorithm intersected with an m-gram language model (Huang et al., 2005). (a) target source 1 i-1 i j j+1 n 1 i-1 i j j+1 n (b) target source Figure 2: Two Reorderings (straight and inverted) for Language Model Look-Ahead. 1, |H|) words. If an inverted order is preferred (Fig. 2(b)), the language model look-ahead probability πi (H) can be estimated as follows πi (H) = m-gram(T r (sj+1 ...sn ), H l ) ·m-gram(H r , T l (s1 ...si−1 )) Since we don’t know which order will be preferred, we take the maximum of the straight and inverted LM look-ahead probability for the hypothesis π(H) = max(πs (H), πi (H)) The final beam thresholding measure for H when compared to the b"
2009.mtsummit-posters.24,2007.mtsummit-papers.43,0,0.0213044,"re of the full uncovered source sequence for both threshold pruning and histogram pruning (the latter). The Pharaoh-style “future cost” can not provide any discriminative information for our pruning since we compare competing hypotheses within the same cell (This means that they have the same future cost). We remains the same as the Pharaoh decoder to find the most probable path through translation options for source words that are not yet translated. But we go further to take into account the interaction of current hypotheses and the most probable path for not yet translated source sequence. Moore and Quirk (2007) present two modifications for beam-search decoding, the Pharaoh decoder in particular by improving the future cost estimation and early pruning out next-phrase translations. Their success and the high efficiency of our beam thresholding methods (verified by experiments in the next section) show that there is much room for search space reduction in widely-used beam-search decoding. 5 Experiments We carried out a series of experiments to examine the effect of our beam thresholding techniques by comparing them with the fixed beam thresholding as well as the cube pruning, and also by combining al"
2009.mtsummit-posters.24,W01-1408,0,0.0611538,"arly stage of decoding. We call this pruning strategy dynamic beam thresholding (DBT). DBT increases the parameter α to tighten the beam when more source words covered. In theory, DBT runs faster than traditional beam thresholding FBT at the same performance level, as our experiments attest. 3 Language Model Look-ahead In traditional beam thresholding used in SMT decoding, only the probability estimated from inside a partial hypothesis is used. This probability does not give information about the probability of the hypothesis in the context of the complete translation. In A* decoding for SMT (Och et al., 2001; Zhang and Gildea, 2006), different heuristic functions are used to estimate a “future” probability for completing a partial hypothesis. In CKY bottom-up parsing, (Goodman, 1997) introduces a prior probability into the beam thresholding. All of these probabilities are capable of capturing the outside context interaction, to some extent. In this paper, we discuss the LM look-ahead (LMLA) and examine the question of whether, given the complicated reordering in SMT, the LM lookahead can obtain a considerable speedup in SMT decoding. The basic idea of the LM look-ahead is to incorporate the langu"
2009.mtsummit-posters.24,P02-1040,0,0.105792,"Missing"
2009.mtsummit-posters.24,N03-1017,0,0.0224318,"Missing"
2009.mtsummit-posters.24,P96-1021,0,0.0346546,"Missing"
2009.mtsummit-posters.24,J97-3002,0,0.0947939,"r success and the high efficiency of our beam thresholding methods (verified by experiments in the next section) show that there is much room for search space reduction in widely-used beam-search decoding. 5 Experiments We carried out a series of experiments to examine the effect of our beam thresholding techniques by comparing them with the fixed beam thresholding as well as the cube pruning, and also by combining all these pruning approaches step by step. We tested them on a Chinese-to-English system with a CKYstyle decoder. The system is based on the Bracketing Transduction Grammars (BTG) (Wu, 1997), which uses the BTG lexical rules (A → x/y) to translate source phrase x into target phrase y and the BTG merging rules (A → [A, A]|hA, Ai) to combine two neighboring phrases with a straight or inverted order. The BTG lexical rules are weighted with several features, such as phrase translation, word penalty and language model, in a log-linear form. For the merging rules, a MaxEnt-based reordering model using boundary words of neighboring phrases as features is used to predict the merging order, similar to (Xiong et al., 2006). All the log-linear model weights are tuned on the development set"
2009.mtsummit-posters.24,P06-1066,1,0.833241,"e decoder. The system is based on the Bracketing Transduction Grammars (BTG) (Wu, 1997), which uses the BTG lexical rules (A → x/y) to translate source phrase x into target phrase y and the BTG merging rules (A → [A, A]|hA, Ai) to combine two neighboring phrases with a straight or inverted order. The BTG lexical rules are weighted with several features, such as phrase translation, word penalty and language model, in a log-linear form. For the merging rules, a MaxEnt-based reordering model using boundary words of neighboring phrases as features is used to predict the merging order, similar to (Xiong et al., 2006). All the log-linear model weights are tuned on the development set to maximize the BLEU score. A CKY-style decoder is developed to generate the best BTG binary tree for each input sentence, which yields the best translation. We used the FBIS corpus (7.06M Chinese words and 9.15M English words) as our bilingual training data, from which a MaxEnt-based reordering model was also trained. The 4-gram language model training data (181.1M words) consists of English texts mostly derived from Xinhua section of the English Gigaword corpus. We used the NIST MT-05 as our test set (27.4 words per sentence"
2009.mtsummit-posters.24,C04-1030,0,0.0441255,"Missing"
2009.mtsummit-posters.24,W06-1627,0,0.0762381,"ding. We call this pruning strategy dynamic beam thresholding (DBT). DBT increases the parameter α to tighten the beam when more source words covered. In theory, DBT runs faster than traditional beam thresholding FBT at the same performance level, as our experiments attest. 3 Language Model Look-ahead In traditional beam thresholding used in SMT decoding, only the probability estimated from inside a partial hypothesis is used. This probability does not give information about the probability of the hypothesis in the context of the complete translation. In A* decoding for SMT (Och et al., 2001; Zhang and Gildea, 2006), different heuristic functions are used to estimate a “future” probability for completing a partial hypothesis. In CKY bottom-up parsing, (Goodman, 1997) introduces a prior probability into the beam thresholding. All of these probabilities are capable of capturing the outside context interaction, to some extent. In this paper, we discuss the LM look-ahead (LMLA) and examine the question of whether, given the complicated reordering in SMT, the LM lookahead can obtain a considerable speedup in SMT decoding. The basic idea of the LM look-ahead is to incorporate the language model interaction of"
2009.mtsummit-posters.24,P08-1025,0,0.0186269,"side the hypothesis H, λLM is the weight of the language model. Note that this probability is only used for the beam thresholding. 4 Comparison to Previous Work Efficient decoding is of great importance to rapid SMT development and commercial applications. Much of previous work focuses on reducing the overwhelming overhead introduced by the intersection of the m-gram language model and the translation model (phrase-based or syntaxbased). This is the fundamental motivation for cube pruning/growing(Chiang, 2007; Huang and Chiang, 2007), and multi-pass decoding approaches(Venugopal et al., 2007; Zhang and Gildea, 2008). Other efforts have been made for A* decoding using search heuristics (Och et al., 2001; Zhang and Gildea, 2006). The Pharaoh decoder (Koehn, 2004) uses an estimated score of uncovered source sequences as an important component to compare hypotheses. In A* decoding (Och et al., 2001; Zhang and Gildea, 2006), a heuristic function is used to estimate the probability to complete a partial hypothesis. To some extent, both are similar to our LMLA probability. The biggest difference is that we emphasize the effect of the language model interaction on the beam thresholding. We neither use the LMLA p"
2009.mtsummit-posters.25,P05-1033,0,0.506363,"be our future work. To calculate P r(hc ), we use a source dependency model. The challenge to our source dependency model is the acquisition of training data: source dependency trees. The source dependency tree used for training has to satisfy two conditions: 1) it is not necessarily linguistically sensible; and 2) it is produced under bilingual context. We observe that we can obtain such dependency trees from word alignments because word alignments implicitly contain hierarchical structures of both source and target language. 1 We inherit the definition of the formally syntax-based MT from (Chiang, 2005). It refers to syntax-based MT which uses synchronous CFG with no linguistic commitment. To uncover the hidden structures from word alignments, we adopt the algorithm of (Zhang et al., 2008). The algorithm decomposes word-aligned sentence pairs into hierarchical trees where leaf nodes are permuted from left to right according to the target language word order. From these trees, we extract the normalized source trees. In order to obtain dependency relations of source words, we annotate nodes with head words and labels. Without introducing conflict with the original definition of dependency tree"
2009.mtsummit-posters.25,D08-1024,0,0.0133445,"et dependency trees in a similar way as the source dependency trees to produce a target dependency model. This is different from (Shen et al., 2008)’s way of inducing a target dependency language model in that we do not require an external dependency parser. 7 Related Work In the formally syntax-based machine translation, Chiang (2005) propose to add a “constituent feature” to the log-linear model in order to reward hypotheses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. All the prior work listed a"
2009.mtsummit-posters.25,P06-1121,0,0.0122002,"ses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. All the prior work listed above requires linguistically motivated parsing. The big distinction from their work is that we build our source dependency model from word alignments without using any monolingual parsing. 8 Conclusion We have presented a novel method to induce a source dependency model from word alignments without using any linguistic analysis. The induced source dependency model captures dependency relations among source words. We employ"
2009.mtsummit-posters.25,2006.amta-papers.8,0,0.0824804,"nstituent feature” to the log-linear model in order to reward hypotheses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. All the prior work listed above requires linguistically motivated parsing. The big distinction from their work is that we build our source dependency model from word alignments without using any monolingual parsing. 8 Conclusion We have presented a novel method to induce a source dependency model from word alignments without using any linguistic analysis. The induced source depend"
2009.mtsummit-posters.25,W04-3250,0,0.0525472,"ns, and applied the “grow-diag-final” refinement rule (Koehn et al., 2005) to produce the final many-to-many word alignments. We trained a fourgram language model using Xinhua section of the English Gigaword corpus (181.1M words) with the SRILM toolkit (Stolcke, 2002). For the efficiency of MERT, we built our development set (580 sentences) using sentences not exceeding 50 characters from the NIST MT-02 set. We evaluated our systems on the NIST MT-05 and MT03 test sets using case-sensitive BLEU-4. Statistical significance in BLEU score differences was assessed by paired bootstrap re-sampling (Koehn, 2004). 5.2 Source Dependency Model Training Because SRA runs very quickly, which produces within a few minutes all decomposition trees from our word-aligned sentence pairs, we can easily train various source dependency models following the steps described in Section 2. In order to allow the normalization step to keep more source words, we removed less probable links in word alignments. First, we calculate lexicon translation probabilities in both directions (P r(C|E) and P r(E|C)) from the original word alignments. Then we search on the source side or the target side for words which are involved in"
2009.mtsummit-posters.25,2005.iwslt-1.8,0,0.0606032,"Missing"
2009.mtsummit-posters.25,P06-1077,0,0.0610526,"opose to add a “constituent feature” to the log-linear model in order to reward hypotheses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. All the prior work listed above requires linguistically motivated parsing. The big distinction from their work is that we build our source dependency model from word alignments without using any monolingual parsing. 8 Conclusion We have presented a novel method to induce a source dependency model from word alignments without using any linguistic analysis. The i"
2009.mtsummit-posters.25,W06-1606,0,0.0217437,"idden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. All the prior work listed above requires linguistically motivated parsing. The big distinction from their work is that we build our source dependency model from word alignments without using any monolingual parsing. 8 Conclusion We have presented a novel method to induce a source dependency model from word alignments without using any linguistic analysis. The induced source dependency model captures dependency relations among source words. We employ this new model in M"
2009.mtsummit-posters.25,P08-1114,0,0.190728,"ethod, we can also obtain target dependency trees in a similar way as the source dependency trees to produce a target dependency model. This is different from (Shen et al., 2008)’s way of inducing a target dependency language model in that we do not require an external dependency parser. 7 Related Work In the formally syntax-based machine translation, Chiang (2005) propose to add a “constituent feature” to the log-linear model in order to reward hypotheses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. A"
2009.mtsummit-posters.25,P00-1056,0,0.223357,"e do not expand the search space which the original decoder has to explore. These two simplifications greatly reduce potential workload caused by introducing a dependency model into decoding. 5 Experiments We carried out experiments to examine the effect of the source dependency model on Chinese-to-English translation tasks. 5.1 Experimental Setup Our baseline is a formally syntax-based system using BTG, developed by following (Xiong et al., 2006). The training data is from FBIS corpus, which contains 6.59M Chinese words and 8.04M English words. To obtain word-level alignments, we ran GIZA++ (Och and Ney, 2000) on the corpus in both directions, and applied the “grow-diag-final” refinement rule (Koehn et al., 2005) to produce the final many-to-many word alignments. We trained a fourgram language model using Xinhua section of the English Gigaword corpus (181.1M words) with the SRILM toolkit (Stolcke, 2002). For the efficiency of MERT, we built our development set (580 sentences) using sentences not exceeding 50 characters from the NIST MT-02 set. We evaluated our systems on the NIST MT-05 and MT03 test sets using case-sensitive BLEU-4. Statistical significance in BLEU score differences was assessed by"
2009.mtsummit-posters.25,P02-1038,0,0.0688259,"ation tasks. 1 P r(h) ≈ P r(hc )P r(he ) Introduction In the word-based translation model introduced by IBM (Brown et al., 1993), the hidden variable, word alignment, associates the source sentence (c) with the target sentence (e). This model has been advanced to the phrase-based and syntax-based models, and the hidden variable has also been transformed into new forms: phrase alignment in phrasebased translation and hierarchical tree in syntaxbased translation. In all cases, we can search the best translation among all possible target sentences and hidden variables through a log-linear model (Och and Ney, 2002) eˆ = argmax(Σi λi fi (c, h, e)) (1) e,h where fi are feature functions which are dependent not only on c and e but also on the hidden variable h. (2) where hc and he are the tree projections of h on the source and target side respectively. Without loss of generality, we discuss P r(h) within the context of Bracketing Transduction Grammar (BTG) (Wu, 1997), which is a binary synchronous CFG widely adopted in machine translation. We focus on P r(hc ) in this paper and leave the modeling of P r(he ) to be our future work. To calculate P r(hc ), we use a source dependency model. The challenge to o"
2009.mtsummit-posters.25,P03-1021,0,0.00596905,"orpus. To deal with the data sparseness problem, we smooth the two probabilities through Witten-Bell interpolation, similar to (Collins, 1999). Table 2 shows the back-off structures for the smoothing. Further, for words occurring less than 5 times in training data, and words in test data which have never been seen in training, we replace them with the ”UNKNOWN” token. As described in the introduction, P r(hc ) is integrated into the log-linear translation model as a new feature. The weight of this new feature, like the weights of other features, is tuned via minimumerror-rate training (MERT) (Och, 2003) on a development set. 4 Decoding with Source Dependency Model Since we use a bottom-up CKY parsing algorithm for decoding, computing the source dependency model score is quite straightforward. When a new node of the hierarchical tree hc is being constructed, we use the predefined POS tag weight table (Table 1) to determine the head. If the BTG lexical rule (A → x/y) is used to produce a leaf node upon a source span, we select the rightmost source word with the highest POS tag weight within the source span as the head word for the leaf node. If the BTG merging rules (A → [A, A]|hA, Ai) are use"
2009.mtsummit-posters.25,P08-1066,0,0.0406647,"ildea, 2005). It is clearly shown in the experiments that noises in word alignments influence the performance of SDM considerably. To address this problem, we would explore hierarchical alignments as they contain hierarchical structures intrinsically and would enable us to obtain high quality source dependency trees without using complicated transformation. • Induce a target dependency model from word alignments. Based on our proposed method, we can also obtain target dependency trees in a similar way as the source dependency trees to produce a target dependency model. This is different from (Shen et al., 2008)’s way of inducing a target dependency language model in that we do not require an external dependency parser. 7 Related Work In the formally syntax-based machine translation, Chiang (2005) propose to add a “constituent feature” to the log-linear model in order to reward hypotheses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance signifi"
2009.mtsummit-posters.25,J97-3002,0,0.186934,"orms: phrase alignment in phrasebased translation and hierarchical tree in syntaxbased translation. In all cases, we can search the best translation among all possible target sentences and hidden variables through a log-linear model (Och and Ney, 2002) eˆ = argmax(Σi λi fi (c, h, e)) (1) e,h where fi are feature functions which are dependent not only on c and e but also on the hidden variable h. (2) where hc and he are the tree projections of h on the source and target side respectively. Without loss of generality, we discuss P r(h) within the context of Bracketing Transduction Grammar (BTG) (Wu, 1997), which is a binary synchronous CFG widely adopted in machine translation. We focus on P r(hc ) in this paper and leave the modeling of P r(he ) to be our future work. To calculate P r(hc ), we use a source dependency model. The challenge to our source dependency model is the acquisition of training data: source dependency trees. The source dependency tree used for training has to satisfy two conditions: 1) it is not necessarily linguistically sensible; and 2) it is produced under bilingual context. We observe that we can obtain such dependency trees from word alignments because word alignment"
2009.mtsummit-posters.25,P06-1066,1,0.892645,"orcedly assigned according to POS tag weights. No probability is involved in this decision. Second, we do not add the head word to the state of hypothesis. This means we do not expand the search space which the original decoder has to explore. These two simplifications greatly reduce potential workload caused by introducing a dependency model into decoding. 5 Experiments We carried out experiments to examine the effect of the source dependency model on Chinese-to-English translation tasks. 5.1 Experimental Setup Our baseline is a formally syntax-based system using BTG, developed by following (Xiong et al., 2006). The training data is from FBIS corpus, which contains 6.59M Chinese words and 8.04M English words. To obtain word-level alignments, we ran GIZA++ (Och and Ney, 2000) on the corpus in both directions, and applied the “grow-diag-final” refinement rule (Koehn et al., 2005) to produce the final many-to-many word alignments. We trained a fourgram language model using Xinhua section of the English Gigaword corpus (181.1M words) with the SRILM toolkit (Stolcke, 2002). For the efficiency of MERT, we built our development set (580 sentences) using sentences not exceeding 50 characters from the NIST M"
2009.mtsummit-posters.25,C08-1136,0,0.0711919,"The source dependency tree used for training has to satisfy two conditions: 1) it is not necessarily linguistically sensible; and 2) it is produced under bilingual context. We observe that we can obtain such dependency trees from word alignments because word alignments implicitly contain hierarchical structures of both source and target language. 1 We inherit the definition of the formally syntax-based MT from (Chiang, 2005). It refers to syntax-based MT which uses synchronous CFG with no linguistic commitment. To uncover the hidden structures from word alignments, we adopt the algorithm of (Zhang et al., 2008). The algorithm decomposes word-aligned sentence pairs into hierarchical trees where leaf nodes are permuted from left to right according to the target language word order. From these trees, we extract the normalized source trees. In order to obtain dependency relations of source words, we annotate nodes with head words and labels. Without introducing conflict with the original definition of dependency tree, we call the annotated tree obtained from the above steps source dependency tree. Using these source dependency trees, we develop our source dependency model. To the best of our knowledge,"
2009.mtsummit-posters.25,2007.mtsummit-papers.71,1,0.770567,"o the log-linear model in order to reward hypotheses under which the hidden hierarchical tree h respects linguistic structures of source language. While no success is seen with this feature, (Marton and Resnik, 2008) and (Chiang et al., 2008) advance this effort and find that penalizing violations of syntactic boundaries of source language improves performance significantly. In the realm of linguistically syntax-based machine translation, the hidden hierarchical tree becomes more visible because it is explicitly constructed using source-language grammars (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2007) or target-language grammars (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). The modeling of h is tightly coupled with the probabilistic source/target-oriented synchronous grammars. All the prior work listed above requires linguistically motivated parsing. The big distinction from their work is that we build our source dependency model from word alignments without using any monolingual parsing. 8 Conclusion We have presented a novel method to induce a source dependency model from word alignments without using any linguistic analysis. The induced source dependency model captures d"
2015.mtsummit-papers.3,J07-2003,0,0.24639,"Missing"
2015.mtsummit-papers.3,P11-2031,0,0.0198494,"and applying refinement rule grow-diag-final-and Koehn et al. (2003). A 4-gram language model was trained on the Xinhua section of Gigaword by SRILM toolkit Stolcke et al. (2002). We also extracted SCFG rules from the word-aligned training data. The translation performance was measured by case-insensitive BLEU Papineni et al. (2002). We used minimum error rate training (MERT) (Och.2003a) to tune the log-linear feature weights. As MERT is normally instable, we ran the tuning process three times for all our experiments and presented the average BLEU scores on the three MERT runs as suggested by Clark et al (2011). The open source toolkit DISSECT1 was applied to obtain PMI-based vector space word representations with a context window of 5 words, and Word2Vec2 to acquire neural word representations, with each word represented as a 50-dimensional vector. When adopted Word2Vec, we just set the context window of size 5 and using continuous bag-of-words model. DISSECT was also adpoted to train weights in semantic composition of weighted vector addition. Unsupervised greedy RAE was trained in the way following Socher et al. (2011). In the bilingual projection neural network, 50 hidden units were used in the"
2015.mtsummit-papers.3,N03-1017,0,0.119057,"d sense discrimination (Clark and Pulman,2007) and thesaurus compilation (Yang and Powers,2008). In this paper, we explore how to learn semantic representations of bilingual phrases, rather than monolingual words, in the context of statistical machine translation (SMT) to facilitate the computation of semantic similarity between translation equivalents at the phrase level. We also study whether semantic similarity scores calculated in terms of bilingual distributed phrase representations are complementary to phrase translation probabilities estimated by the conventional counting method in SMT Koehn et al. (2003). Very recently we have witnessed some studies on learning bilingual distributed representations for SMT. Mikolov et al. (2013b) train two neural network (NN) based models to learn word embeddings in the source and the target language, respectively, and then map the embeddings from the source to the target language space using a transformation matrix that is learned ∗Corresponding author Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 32 by minimizing the mapping cost on all word pairs. Zou et al. (2013) introduce bilingual word embeddings into phrase-"
2015.mtsummit-papers.3,P08-1028,0,0.134647,"Missing"
2015.mtsummit-papers.3,P03-1021,0,0.0367911,"Missing"
2015.mtsummit-papers.3,J03-1002,0,0.00795929,"Missing"
2015.mtsummit-papers.3,J07-2002,0,0.0236259,"Missing"
2015.mtsummit-papers.3,P02-1040,0,0.0919351,"Missing"
2015.mtsummit-papers.3,D11-1014,0,0.146961,"f representations at the word level Mikolov et al. (2013b); Zou et al. (2013), so as to keep consistency with the SMT that uses phrases rather than words as basic translation units. • We learn phrase representations from distributed word representations via semantic composition, instead of from raw phrases Gao et al. (2013) in order to avoid the data sparseness issue of directly learning phrase representations from data. Particularly, we empirically compare two different composition methods in our framework, namely, weighted vector addition (Mitchell and Lapata,2008) and recursive autoencoder Socher et al. (2011). • Rather than jointly learning phrase representations with feature weights of the log-linear model of SMT Gao et al. (2013), we take a loose coupling strategy to simplify the learning process. We adopt a neural network to project phrase representations from the source onto the target language semantic space, in separation from the process of feature weight tuning in SMT. • Rather than capturing only the linear transformation between the source and target language semantic space Mikolov et al. (2013b), our neural network for the bilingual projection can model both linear and nonlinear transfo"
2015.mtsummit-papers.3,D12-1110,0,0.106474,"Missing"
2015.mtsummit-papers.3,P10-1040,0,0.0103434,"Missing"
2015.mtsummit-papers.3,P12-1079,1,0.885606,"Missing"
2015.mtsummit-papers.3,D13-1141,0,0.0745415,"ted by the conventional counting method in SMT Koehn et al. (2003). Very recently we have witnessed some studies on learning bilingual distributed representations for SMT. Mikolov et al. (2013b) train two neural network (NN) based models to learn word embeddings in the source and the target language, respectively, and then map the embeddings from the source to the target language space using a transformation matrix that is learned ∗Corresponding author Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 32 by minimizing the mapping cost on all word pairs. Zou et al. (2013) introduce bilingual word embeddings into phrase-based machine translation: Word representations are first learned from language A via an NN-based model, word embeddings in the parallel language B are then initialized according to A’s embeddings and word alignments between A and B, and the final word representations of B are obtained by a further training process that optimizes a combined objective on bilingual data. Gao et al. (2013) extend distributional representations from the word level to the phrase level, adopting a fully connected neural network to transfer bag-of-words vector represen"
2015.mtsummit-papers.3,P14-1011,0,0.0370829,"Missing"
2020.acl-main.323,W04-3250,0,0.0998954,"ur approaches based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. We applied our approach to the training of the Transformer, and to compare with Vaswani et al. (2017), we conducted our experiments on the WMT 14 English to German and English to French news translation tasks on 2 GTX 1080Ti GPUs. Hyper parameters were tuned on the development set (newstest 2012 and 2013). We followed all settings of Vaswani et al. (2017) except for the batch size. We used a beam size of 4 for decoding, and evaluated case-sensitive tokenized BLEU5 with significance test (Koehn, 2004). We used an α of 1.1 to determine the fluctuation of gradient direction by default. We regarded each encoder/decoder layer as a parameter group, and used a β of 3 for the parameter group selection. 3.1 ∆ak is positive, but after adding Gumble noise, there is a small possibility that it turns negative. In our case, negative values only occur very few times. Performance We compared the results of our dynamic batch size approach to two fixed batch size baselines, the 25k 4 We use pk as the probability to sample the kth group, and β is a hyper-parameter to sharpen the probability distribution. We"
2020.acl-main.323,W18-6301,0,0.0479447,"n Genabith1,2 Deyi Xiong3 Qiuhui Liu4∗ Saarland University / Saarland, Germany 2 German Research Center for Artificial Intelligence / Saarland, Germany 3 Tianjin University / Tianjin, China 4 China Mobile Online Services / Henan, China hfxunlp@foxmail.com, Josef.Van Genabith@dfki.de, dyxiong@tju.edu.cn, liuqhano@foxmail.com 1 Abstract Specifically, it has been shown that the performance of the Transformer model (Vaswani et al., 2017) for Neural Machine Translation (NMT) (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) relies heavily on the batch size (Popel and Bojar, 2018; Ott et al., 2018; Abdou et al., 2017; Zhang et al., 2019a). The influence of batch size on performance raises the question, how to dynamically find proper and efficient batch sizes during training? In this paper, we investigate the relationship between the batch size and gradients, and propose a dynamic batch size approach by monitoring gradient direction changes. Our contributions are as follows: The choice of hyper-parameters affects the performance of neural models. While much previous research (Sutskever et al., 2013; Duchi et al., 2011; Kingma and Ba, 2015) focuses on accelerating convergence and reducin"
2020.acl-main.323,D19-1083,0,0.114094,"Missing"
2020.acl-main.323,P19-1426,0,0.066836,"Missing"
2020.acl-main.323,W17-4780,0,\N,Missing
2020.acl-main.37,J93-2003,0,0.107473,"Missing"
2020.acl-main.37,P05-1033,0,0.250398,"esentations for Neural Machine Translation Hongfei Xu1,2 Josef van Genabith1,2∗ Deyi Xiong3 Qiuhui Liu4 Jingyi Zhang2 1 Saarland University / Saarland, Germany 2 German Research Center for Artificial Intelligence / Saarland, Germany 3 Tianjin University / Tianjin, China 4 China Mobile Online Services / Henan, China hfxunlp@foxmail.com, Josef.Van Genabith@dfki.de, dyxiong@tju.edu.cn, liuqhano@foxmail.com, Jingyi.Zhang@dfki.de Abstract years (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). Compared to plain SMT (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), a neural language model decoder (Sutskever et al., 2014) is better at long-distance re-ordering, and attention mechanisms (Bahdanau et al., 2015; Vaswani et al., 2017) have been proven effective in modeling longdistance dependencies, while these two issues were both challenging for SMT. The Transformer (Vaswani et al., 2017), which has outperformed previous RNN/CNN based translation models (Bahdanau et al., 2015; Gehring et al., 2017), is based on multi-layer multi-head attention networks and can be trained in parallel very efficiently. Though attentional networks can connect distant words v"
2020.acl-main.37,D14-1179,0,0.111908,"Missing"
2020.acl-main.37,D17-1148,0,0.0767553,"results show how our approach improves long-distance dependency capturing, which supports our conjecture that phrase representation sequences can help the model capture long-distance relations better. 2 Background and Related Work In this section, we first review previous work which utilizes phrases in recurrent sequence-tosequence models, then give a brief introduction to the stronger Transformer translation model that our work is based on. 2.1 Utilizing Phrases in RNN-based NMT Most previous work focuses on utilizing phrases from SMT in NMT to address its coverage (Tu et al., 2016) problem. Dahlmann et al. (2017) suggested that SMT usually performs better in translating rare words and profits from using phrasal translations, even though NMT achieves better overall translation quality. They introduced a hybrid search algorithm for attention-based NMT which extended the beam search of NMT with phrase translations from SMT. Wang et al. (2017a) proposed that while NMT generally produces fluent but often inadequate translations, SMT yields adequate translations though less fluent. They incorporate SMT into NMT through utilizing recommendations from SMT in each decoding step of NMT to address the coverage i"
2020.acl-main.37,N19-1423,0,0.0319857,"move ST from T ; while mt &gt; 0 do Find the adjacent sub-tree STA of depth dst with nsta tokens from the right side of T ; if STA exists and nsta ≤ mt then Insert the token sequence of STA to the beginning of p; Remove STA from T ; mt = mt − nsta; else Break; end if end while Append p to S; end while Reverse S; return S Rekall = Fglance (Rekt , ..., Rektm ) 1 neural models have been proven good at learning competitively effective representations with gate or attention mechanism even without modeling linguistic structures (Cho et al., 2014; Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017; Devlin et al., 2019). In our experiments we also explore phrases extracted from the Stanford Parser (Socher et al., 2013) as as an alternative to our simple segmentation strategy. The maximum number of tokens allowed is consistent with the simple segmentation approach, and we try to use the tokens from the largest sub-tree that complies with the maximum token limitation or from several adjacent sub-trees of the same depth as a phrase for efficiency. Our algorithm to extract phrases from parse trees is shown in Algorithm 1. To efficiently parallelize parser-based phrases of various length in a batch of data, we pa"
2020.acl-main.37,D18-1457,0,0.0182939,"tion in the computation of the phrase-representation attention because of two reasons: • The multi-head attention calculates weights through dot-product, we suggest that a 2-layer neural network might be more powerful at semantic level feature extraction, and it is less likely to be affected by positional embeddings which are likely to vote up adjacent vectors; Rdj phrase = Recent studies show that different encoder layers capture linguistic properties of different levels (Peters et al., 2018), and aggregating layers is of profound value to better fuse semantic information (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). We assume that different decoder layers may value different levels of information i.e. the representation of different encoder layers differently, thus we weighted combined phrase representations from every encoder layer for each decoder layer with the Transparent Attention (TA) mechanism (Bapna et al., 2018). i ephrase (6) i=0 wij where are softmax normalized parameters trained jointly with the full model to learn the importance of encoder layers for the jth decoder layer. d is the number of encoder layers, and 0 corresponds to the embedding layer. 3.2"
2020.acl-main.37,D18-1338,0,0.0185689,"djacent vectors; Rdj phrase = Recent studies show that different encoder layers capture linguistic properties of different levels (Peters et al., 2018), and aggregating layers is of profound value to better fuse semantic information (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). We assume that different decoder layers may value different levels of information i.e. the representation of different encoder layers differently, thus we weighted combined phrase representations from every encoder layer for each decoder layer with the Transparent Attention (TA) mechanism (Bapna et al., 2018). i ephrase (6) i=0 wij where are softmax normalized parameters trained jointly with the full model to learn the importance of encoder layers for the jth decoder layer. d is the number of encoder layers, and 0 corresponds to the embedding layer. 3.2 • Though we employ a 2-layer neural network, it only has one linear transformation and a vector to calculate attention weights, which contains fewer parameters than the multi-head attention model that has 4 linear transformations. d X w j Ri Incorporating Phrase Representation into NMT After the phrase representation sequence for each encoder layer"
2020.acl-main.37,D19-1082,0,0.0193695,"h token vectors. Previous work does not use SMT phrases in this way. and values. It first projects queries, keys and values with 3 independent linear transformations, then splits the transformed key, query and value embeddings into several chunks of dk dimension vectors, each chunk is called a head,1 and scaled dotproduct attention is independently applied in each head: QK T Attn(Q, K, V ) = softmax( √ )V dk Comparison with Previous Works In more recent work, Wang et al. (2019) augment self attention with structural position representations to model the latent structure of the input sentence; Hao et al. (2019) propose multi-granularity self-attention which performs phrase-level attention with several attention heads. (1) 3 where Q, K and V stand for the query vectors, key vectors and value vectors. Finally, the network concatenates the outputs of all heads and transforms it into the target space with another linear layer. The self-attention network uses the query sequence also as the key sequence and the value sequence in computation, while the cross-attention feeds another vector sequence to attend as queries and values. Comparing the computation of the attentional network with RNNs, it is obvious"
2020.acl-main.37,W04-3250,0,0.0744435,"k which was 0.3. The training steps for Transformer Base and Transformer Big were 100k and 300k respectively following Vaswani et al. (2017). The other settings were the same as (Vaswani et al., 2017) except that we did not bind the embedding between the encoder and the decoder for efficiency. We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU 3 with the averaged model of the last 5 checkpoints for Transformer Base and 20 checkpoints for Transformer Big saved with an interval of 1, 500 training steps (Vaswani et al., 2017). We also conducted significance tests (Koehn, 2004). 4.2 Main Results We applied our approach to both the Transformer Base setting and the Transformer Big setting, and conducted experiments on both tasks to validate the effectiveness of our approach. Since parsing a large training set (specifically, the En-Fr dataset) is slow, we did not use phrases from parse results in this experiment (reported in Table 1). Results are shown in Table 1. † indicates p &lt; 0.01 compared to the baseline for the significance test. Table 1 shows that modeling phrase representation can bring consistent and significant improvements on both tasks, and benefit both the"
2020.acl-main.37,N03-1017,0,0.241009,"Missing"
2020.acl-main.37,N18-1202,0,0.0519183,"rallel. Each encoder layer will produce a vector sequence as the phrase representation. We do not use the multi-head attention in the computation of the phrase-representation attention because of two reasons: • The multi-head attention calculates weights through dot-product, we suggest that a 2-layer neural network might be more powerful at semantic level feature extraction, and it is less likely to be affected by positional embeddings which are likely to vote up adjacent vectors; Rdj phrase = Recent studies show that different encoder layers capture linguistic properties of different levels (Peters et al., 2018), and aggregating layers is of profound value to better fuse semantic information (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). We assume that different decoder layers may value different levels of information i.e. the representation of different encoder layers differently, thus we weighted combined phrase representations from every encoder layer for each decoder layer with the Transparent Attention (TA) mechanism (Bapna et al., 2018). i ephrase (6) i=0 wij where are softmax normalized parameters trained jointly with the full model to learn the importance of encod"
2020.acl-main.37,E17-2060,0,0.0413728,"sequences which are shorter than corresponding token sequences can help the model capture longdistance dependencies better, and modeling phrase representations for the Transformer can enhance its performance on long sequences. 4.5 Subject-Verb Agreement Analysis Intuitively, in translating longer sentences we should encounter more long-distance dependencies than in short sentences. To verify whether our method can improve the capability of the NMT model to capture long-distance dependencies, we also conducted a linguistically-informed verb-subject agreement analysis on the Lingeval97 dataset (Sennrich, 2017) following Tang et al. (2018). In German, subjects and verbs must agree with one another in grammatical number and person. In Lingeval97, each contrastive translation pair consists of a correct reference translation, and a contrastive example that has been minimally modified to introduce one translation error. The accuracy of a model is the number of times it assigns a higher score to the reference translation than to the contrastive one, relative to the total number of predictions. Results are shown in Figure 4. Figure 4 shows that our approach can improve the accuracy of long-distance subjec"
2020.acl-main.37,P16-1162,0,0.0724125,"sentation is produced inside the Transformer model and utilized as the input of layers, and all related computations are differentiable, the attentive phrase representation model is simply trained as part of the whole model through backpropagation effectively. 4 Experiments To compare with Vaswani et al. (2017), we conducted our experiments on the WMT 14 English to German and English to French news translation tasks. 4.1 Settings We implemented our approaches based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations on both data sets to address the unknown word problem. We only kept sentences with a maximum of 256 subword tokens for training. Training sets were randomly shuffled in every training epoch. The concatenation of newstest 2012 and newstest 2013 was used for Models En-De En-Fr Transformer Base +PR 27.38 28.67† 39.34 40.71† Transformer Big +PR 28.49 29.60† 41.36 42.45† Table 1: Results on WMT 14 En-De and En-Fr. validation and newstest 2014 as test sets for both tasks. The number of warm-up steps was set to 8k, and each training batch contained at least 25k target token"
2020.acl-main.37,N18-1117,0,0.0268791,"Missing"
2020.acl-main.37,D18-1458,0,0.040503,"Missing"
2020.acl-main.37,P16-1008,0,0.375053,"ith our approach, and the results show how our approach improves long-distance dependency capturing, which supports our conjecture that phrase representation sequences can help the model capture long-distance relations better. 2 Background and Related Work In this section, we first review previous work which utilizes phrases in recurrent sequence-tosequence models, then give a brief introduction to the stronger Transformer translation model that our work is based on. 2.1 Utilizing Phrases in RNN-based NMT Most previous work focuses on utilizing phrases from SMT in NMT to address its coverage (Tu et al., 2016) problem. Dahlmann et al. (2017) suggested that SMT usually performs better in translating rare words and profits from using phrasal translations, even though NMT achieves better overall translation quality. They introduced a hybrid search algorithm for attention-based NMT which extended the beam search of NMT with phrase translations from SMT. Wang et al. (2017a) proposed that while NMT generally produces fluent but often inadequate translations, SMT yields adequate translations though less fluent. They incorporate SMT into NMT through utilizing recommendations from SMT in each decoding step"
2020.acl-main.37,C18-1255,0,0.0165755,"ation of the phrase-representation attention because of two reasons: • The multi-head attention calculates weights through dot-product, we suggest that a 2-layer neural network might be more powerful at semantic level feature extraction, and it is less likely to be affected by positional embeddings which are likely to vote up adjacent vectors; Rdj phrase = Recent studies show that different encoder layers capture linguistic properties of different levels (Peters et al., 2018), and aggregating layers is of profound value to better fuse semantic information (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). We assume that different decoder layers may value different levels of information i.e. the representation of different encoder layers differently, thus we weighted combined phrase representations from every encoder layer for each decoder layer with the Transparent Attention (TA) mechanism (Bapna et al., 2018). i ephrase (6) i=0 wij where are softmax normalized parameters trained jointly with the full model to learn the importance of encoder layers for the jth decoder layer. d is the number of encoder layers, and 0 corresponds to the embedding layer. 3.2 • Though we employ"
2020.acl-main.37,D19-1145,0,0.0596896,"dual connection and Layer normalization are omitted for simplicity. • We iteratively and dynamically generate phrase representations with token vectors. Previous work does not use SMT phrases in this way. and values. It first projects queries, keys and values with 3 independent linear transformations, then splits the transformed key, query and value embeddings into several chunks of dk dimension vectors, each chunk is called a head,1 and scaled dotproduct attention is independently applied in each head: QK T Attn(Q, K, V ) = softmax( √ )V dk Comparison with Previous Works In more recent work, Wang et al. (2019) augment self attention with structural position representations to model the latent structure of the input sentence; Hao et al. (2019) propose multi-granularity self-attention which performs phrase-level attention with several attention heads. (1) 3 where Q, K and V stand for the query vectors, key vectors and value vectors. Finally, the network concatenates the outputs of all heads and transforms it into the target space with another linear layer. The self-attention network uses the query sequence also as the key sequence and the value sequence in computation, while the cross-attention feeds"
2020.acl-main.37,D17-1149,1,0.930498,"els, then give a brief introduction to the stronger Transformer translation model that our work is based on. 2.1 Utilizing Phrases in RNN-based NMT Most previous work focuses on utilizing phrases from SMT in NMT to address its coverage (Tu et al., 2016) problem. Dahlmann et al. (2017) suggested that SMT usually performs better in translating rare words and profits from using phrasal translations, even though NMT achieves better overall translation quality. They introduced a hybrid search algorithm for attention-based NMT which extended the beam search of NMT with phrase translations from SMT. Wang et al. (2017a) proposed that while NMT generally produces fluent but often inadequate translations, SMT yields adequate translations though less fluent. They incorporate SMT into NMT through utilizing recommendations from SMT in each decoding step of NMT to address the coverage issue and the unknown word issue of NMT. Wang et al. (2017b) suggested that phrases play a vital role in machine translation, and proposed to translate phrases in NMT by integrating target phrases from an SMT system with a phrase memory given that it is hard to integrate phrases into NMT which reads and generates sentences in a tok"
2020.acl-main.38,D18-1338,0,0.360185,"ess in the last few years (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). The Transformer (Vaswani et al., 2017), which has outperformed previous RNN/CNN based translation models (Bahdanau et al., 2015; Gehring et al., 2017), is based on multi-layer self-attention networks and can be trained very efficiently. The ∗ multi-layer structure allows the Transformer to model complicated functions. Increasing the depth of models can increase their capacity but may also cause optimization difficulties (Mhaskar et al., 2017; Telgarsky, 2016; Eldan and Shamir, 2016; He et al., 2016; Bapna et al., 2018). In order to ease optimization, the Transformer employs residual connection and layer normalization techniques which have been proven useful in reducing optimization difficulties of deep neural networks for various tasks (He et al., 2016; Ba et al., 2016). However, even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer (Vaswani et al., 2017) only contains 6 encoder/decoder layers. Bapna et al. (2018) show that Transformer models with more than 12 encoder layers fail to converge, and propose the Transparent Attention (TA) mec"
2020.acl-main.38,P18-1008,0,0.216615,"an task and the WMT 15 Czech to English task; • We further investigate deep decoders for the Transformer in addition to the deep encoders studied in previous works, and show that deep decoders can also benefit the Transformer. 2 Convergence of Different Computation Orders In this paper we focus on the convergence of the training of deep transformers. To alleviate the training problem for the standard Transformer model, Layer Normalization (Ba et al., 2016) and Residual Connection (He et al., 2016) are adopted. 2.1 ure 1 b) seems better for harder-to-learn models.1 Even though several studies (Chen et al., 2018; Domhan, 2018) have mentioned this change and although Wang et al. (2019) analyze the difference between the two computation orders during backpropagation, and Zhang et al. (2019) point out the effects of normalization in their work, how this modification impacts on the performance of the Transformer, especially for deep Transformers, has not been deeply studied before. Here we present both empirical convergence experiments (Table 1) and a theoretical analysis of the effect of the interaction between layer normalization and residual connection (Table 2). In order to compare with Bapna et al."
2020.acl-main.38,P18-1167,0,0.0211259,"15 Czech to English task; • We further investigate deep decoders for the Transformer in addition to the deep encoders studied in previous works, and show that deep decoders can also benefit the Transformer. 2 Convergence of Different Computation Orders In this paper we focus on the convergence of the training of deep transformers. To alleviate the training problem for the standard Transformer model, Layer Normalization (Ba et al., 2016) and Residual Connection (He et al., 2016) are adopted. 2.1 ure 1 b) seems better for harder-to-learn models.1 Even though several studies (Chen et al., 2018; Domhan, 2018) have mentioned this change and although Wang et al. (2019) analyze the difference between the two computation orders during backpropagation, and Zhang et al. (2019) point out the effects of normalization in their work, how this modification impacts on the performance of the Transformer, especially for deep Transformers, has not been deeply studied before. Here we present both empirical convergence experiments (Table 1) and a theoretical analysis of the effect of the interaction between layer normalization and residual connection (Table 2). In order to compare with Bapna et al. (2018), we used"
2020.acl-main.38,E17-3017,0,0.0263175,"ion between layer normalization and residual connection (Table 2). In order to compare with Bapna et al. (2018), we used the same datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for our experiments. We applied joint BytePair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base (Vaswani et al., 2017) except the number of warm-up steps was set to 8k. Parameters were initialized with Glorot Initialization2 (Glorot and Bengio, 2010) like in many other Transformer implementations (Klein et al., 2017; Hieber et al., 2017; Vaswani et al., 2018). We conducted experiments based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. Our experiments run on 2 GTX Empirical Study of the Convergence Issue 1 The official implementation (Vaswani et al., 2018) of the Transformer uses a different computation order (Figure 1 b) compared to the published version (Vaswani et al., 2017) (Figure 1 a), since it (Fig398 https://github.com/tensorflow/ tensor2tensor/blob/v1.6.5/tensor2tensor/ layers/common_hparams.py#L110-L112. 2 initialize matrices between qUniformly q [− 6 ,+ (isize+osize) 6 ], ("
2020.acl-main.38,P17-4012,0,0.030273,"fect of the interaction between layer normalization and residual connection (Table 2). In order to compare with Bapna et al. (2018), we used the same datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for our experiments. We applied joint BytePair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base (Vaswani et al., 2017) except the number of warm-up steps was set to 8k. Parameters were initialized with Glorot Initialization2 (Glorot and Bengio, 2010) like in many other Transformer implementations (Klein et al., 2017; Hieber et al., 2017; Vaswani et al., 2018). We conducted experiments based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. Our experiments run on 2 GTX Empirical Study of the Convergence Issue 1 The official implementation (Vaswani et al., 2018) of the Transformer uses a different computation order (Figure 1 b) compared to the published version (Vaswani et al., 2017) (Figure 1 a), since it (Fig398 https://github.com/tensorflow/ tensor2tensor/blob/v1.6.5/tensor2tensor/ layers/common_hparams.py#L110-L112. 2 initialize matrices between qUniformly q [− 6 ,+"
2020.acl-main.38,P16-1162,0,0.029063,"(2019) point out the effects of normalization in their work, how this modification impacts on the performance of the Transformer, especially for deep Transformers, has not been deeply studied before. Here we present both empirical convergence experiments (Table 1) and a theoretical analysis of the effect of the interaction between layer normalization and residual connection (Table 2). In order to compare with Bapna et al. (2018), we used the same datasets from the WMT 14 English to German task and the WMT 15 Czech to English task for our experiments. We applied joint BytePair Encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. We used the same setting as the Transformer base (Vaswani et al., 2017) except the number of warm-up steps was set to 8k. Parameters were initialized with Glorot Initialization2 (Glorot and Bengio, 2010) like in many other Transformer implementations (Klein et al., 2017; Hieber et al., 2017; Vaswani et al., 2018). We conducted experiments based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. Our experiments run on 2 GTX Empirical Study of the Convergence Issue 1 The official implementation (Vaswani et al., 2018) of the Transfor"
2020.acl-main.38,W18-1819,0,0.435436,"nsformers with proper use of layer normalization are able to converge and propose to aggregate previous layers’ outputs for each layer. Wu et al. (2019) explore incrementally increasing the depth of the Transformer Big by freezing pre-trained shallow layers. Concurrent work closest to ours is Zhang et al. (2019). They address the same issue, but propose a different layer-wise initialization approach to reduce the standard deviation. Our contributions are as follows: Corresponding author. • We empirically demonstrate that a simple modification made in the Transformer’s official implementation (Vaswani et al., 2018) which changes the computation order of residual connection and layer normalization can effectively ease its optimization; • We deeply analyze how the subtle difference of computation order affects convergence in 397 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 397–402 c July 5 - 10, 2020. 2020 Association for Computational Linguistics inres inres outLN/res outLN/res Input/Norm Process Dropout inmodel + Norm Process outLN/res Dropout + inmodel Norm Output (a) inres outres Norm Input inres outres Process Dropout inmodel + outLN Norm Process Drop"
2020.acl-main.38,P19-1176,0,0.472114,"lization techniques which have been proven useful in reducing optimization difficulties of deep neural networks for various tasks (He et al., 2016; Ba et al., 2016). However, even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer (Vaswani et al., 2017) only contains 6 encoder/decoder layers. Bapna et al. (2018) show that Transformer models with more than 12 encoder layers fail to converge, and propose the Transparent Attention (TA) mechanism which combines outputs of all encoder layers into a weighted encoded representation. Wang et al. (2019) find that deep Transformers with proper use of layer normalization are able to converge and propose to aggregate previous layers’ outputs for each layer. Wu et al. (2019) explore incrementally increasing the depth of the Transformer Big by freezing pre-trained shallow layers. Concurrent work closest to ours is Zhang et al. (2019). They address the same issue, but propose a different layer-wise initialization approach to reduce the standard deviation. Our contributions are as follows: Corresponding author. • We empirically demonstrate that a simple modification made in the Transformer’s offici"
2020.acl-main.38,P19-1558,0,0.470099,", even with residual connections and layer normalization, deep Transformers are still hard to train: the original Transformer (Vaswani et al., 2017) only contains 6 encoder/decoder layers. Bapna et al. (2018) show that Transformer models with more than 12 encoder layers fail to converge, and propose the Transparent Attention (TA) mechanism which combines outputs of all encoder layers into a weighted encoded representation. Wang et al. (2019) find that deep Transformers with proper use of layer normalization are able to converge and propose to aggregate previous layers’ outputs for each layer. Wu et al. (2019) explore incrementally increasing the depth of the Transformer Big by freezing pre-trained shallow layers. Concurrent work closest to ours is Zhang et al. (2019). They address the same issue, but propose a different layer-wise initialization approach to reduce the standard deviation. Our contributions are as follows: Corresponding author. • We empirically demonstrate that a simple modification made in the Transformer’s official implementation (Vaswani et al., 2018) which changes the computation order of residual connection and layer normalization can effectively ease its optimization; • We dee"
2020.acl-main.38,D19-1083,0,0.168042,"Missing"
2020.acl-main.637,D18-1547,0,0.0970249,"Missing"
2020.acl-main.637,D17-1206,0,0.0207154,"uage Modeling We further propose to incorporate a bi-directional language modeling module into the dialogue state tracking model in a multi-task learning framework for DST, which is shown in Figure 1. The bi-directional language modeling module is to predict the next word and the previous word in the concatenated sequence with the forward and the backward GRU network respectively. We first feed the concatenated dialogue context into the embedding layer. We initialize each word embedding in the dialogue context by concatenating Glove embedding (Pennington et al., 2014) and character embedding (Hashimoto et al., 2017). This word embedding sequence is then fed into a bi-directional −→ GRU network to get the hidden representations hlm t ←− lm and ht in two directions, which are used to predict the next and the previous word through a softmax layer as follows: −→ P lm (wt+1 |w<t+1 ) = sof tmax(Wf hlm t ) ←− P lm (wt−1 |w>t−1 ) = sof tmax(Wb hlm t ) (1) (2) The loss function is defined as the sum of the negative log-likelihood of the next and previous words in the sequence. The language modeling loss Llm is therefore calculated as follows (T is the length of the concatenated dialogue context sequence): lm L =−"
2020.acl-main.637,W14-4337,0,0.0987309,"fferent settings: DST over a predefined domain ontology and DST with slot-value candidates from an open vocabulary. Most of the previous work is based on the first setting, assuming that all possible slot-value candidates are provided in a domain ontology in advance. The task of the dialogue state tracking with this setting is therefore largely simplified to score all predefined slot-value pairs and select the value with the highest score for each slot as the final prediction. Although predefined ontology-based approaches are successfully used on datasets with small ontologies, such as DSTC2 (Henderson et al., 2014) and WOZ2.0 (Wen et al., 2017), they are ∗ Corresponding author quite limited in both scalability to scenarios with infinite slot values and prediction of unseen slot values. In order to address these issues of DST over predefined ontologies, recent efforts have been made to predict slot-value pairs in open vocabularies. Among them, TRADE (Wu et al., 2019) proposes to encode the entire dialogue context and to predict the value for each slot using a copy-augmented decoder, achieving state-of-the-art results on the MultiWOZ 2.0 dataset (Budzianowski et al., 2018). As TRADE simply concatenates al"
2020.acl-main.637,P17-1163,0,0.040323,"Missing"
2020.acl-main.637,P18-2018,0,0.0303043,"Missing"
2020.acl-main.637,D14-1162,0,0.0854904,"user utterances. 3.3 Multi-task Learning with Language Modeling We further propose to incorporate a bi-directional language modeling module into the dialogue state tracking model in a multi-task learning framework for DST, which is shown in Figure 1. The bi-directional language modeling module is to predict the next word and the previous word in the concatenated sequence with the forward and the backward GRU network respectively. We first feed the concatenated dialogue context into the embedding layer. We initialize each word embedding in the dialogue context by concatenating Glove embedding (Pennington et al., 2014) and character embedding (Hashimoto et al., 2017). This word embedding sequence is then fed into a bi-directional −→ GRU network to get the hidden representations hlm t ←− lm and ht in two directions, which are used to predict the next and the previous word through a softmax layer as follows: −→ P lm (wt+1 |w<t+1 ) = sof tmax(Wf hlm t ) ←− P lm (wt−1 |w>t−1 ) = sof tmax(Wb hlm t ) (1) (2) The loss function is defined as the sum of the negative log-likelihood of the next and previous words in the sequence. The language modeling loss Llm is therefore calculated as follows (T is the length of the"
2020.acl-main.637,P18-2069,0,0.0445878,"are provided in an ontology. Mrkˇsi´c et al. (2017) propose a neural belief tracker (NBT) to leverage semantic information from word embeddings by using distributional representation learning for DST. An extension to the NBT is then proposed by Mrkˇsi´c and Vuli´c (2018), which learns to update belief states automatically. Zhong et al. (2018) use slot-specific local modules to learn slot features and propose a global-locally self-attentive dialogue state tracker (GLAD). Nouri and HosseiniAsl (2018) propose GCE model based on GLAD by using only one recurrent networks with global conditioning. Ramadan et al. (2018) introduce an approach that fully utilizes semantic similarity between dialogue utterances and the ontology terms. Ren et al. (2018) propose StateNet which generates a fixed-length representation of the dialogue context and compares the distances between this representation and the value vectors in the candidate set for making prediction. These predefined ontology-based DST approaches suffer from their weak scalability to large ontologies and cannot deal with previously unseen slot values. In open vocabulary-based DST, Xu and Hu (2018) propose a model that learns to predict unknown values by u"
2020.acl-main.637,D19-1196,0,0.0599619,"Missing"
2020.acl-main.637,D18-1299,0,0.140071,"Missing"
2020.acl-main.637,E17-1042,0,0.157223,"Missing"
2020.acl-main.637,P19-1078,0,0.696293,"e all predefined slot-value pairs and select the value with the highest score for each slot as the final prediction. Although predefined ontology-based approaches are successfully used on datasets with small ontologies, such as DSTC2 (Henderson et al., 2014) and WOZ2.0 (Wen et al., 2017), they are ∗ Corresponding author quite limited in both scalability to scenarios with infinite slot values and prediction of unseen slot values. In order to address these issues of DST over predefined ontologies, recent efforts have been made to predict slot-value pairs in open vocabularies. Among them, TRADE (Wu et al., 2019) proposes to encode the entire dialogue context and to predict the value for each slot using a copy-augmented decoder, achieving state-of-the-art results on the MultiWOZ 2.0 dataset (Budzianowski et al., 2018). As TRADE simply concatenates all the system and user utterances in previous turns into a single sequence as the dialogue context for slot-value prediction, it is difficult for the model to identify whether an utterance in the dialogue context is from system or user when the concatenated sequence becomes long. We observe that the longest dialogue context after concatenation on the MultiW"
2020.acl-main.637,P18-1134,0,0.0390949,"only one recurrent networks with global conditioning. Ramadan et al. (2018) introduce an approach that fully utilizes semantic similarity between dialogue utterances and the ontology terms. Ren et al. (2018) propose StateNet which generates a fixed-length representation of the dialogue context and compares the distances between this representation and the value vectors in the candidate set for making prediction. These predefined ontology-based DST approaches suffer from their weak scalability to large ontologies and cannot deal with previously unseen slot values. In open vocabulary-based DST, Xu and Hu (2018) propose a model that learns to predict unknown values by using the index-based pointer network for different slots. Wu et al. (2019) apply an encoder-decoder architecture to generate dialogue states with the copy mechanism. However, their method simply concatenates the whole dialogue context as input and does not perform well when the dialogue context is long. We study this problem and propose methods to help the DST model better model long context. Inspired by Zhou et al. (2019) who use an additional language model in question generation, we attempt to incorporate language modeling into dial"
2020.acl-main.637,P18-1135,0,0.0380543,"uation metrics. The joint of the two methods establish a new state-of-the-art results on the MultiWOZ 2.0. In addition, we provide a detailed analysis on the improvements achieved by our methods. 2 Related Work Predefined ontology-based DST assumes that all slot-value pairs are provided in an ontology. Mrkˇsi´c et al. (2017) propose a neural belief tracker (NBT) to leverage semantic information from word embeddings by using distributional representation learning for DST. An extension to the NBT is then proposed by Mrkˇsi´c and Vuli´c (2018), which learns to update belief states automatically. Zhong et al. (2018) use slot-specific local modules to learn slot features and propose a global-locally self-attentive dialogue state tracker (GLAD). Nouri and HosseiniAsl (2018) propose GCE model based on GLAD by using only one recurrent networks with global conditioning. Ramadan et al. (2018) introduce an approach that fully utilizes semantic similarity between dialogue utterances and the ontology terms. Ren et al. (2018) propose StateNet which generates a fixed-length representation of the dialogue context and compares the distances between this representation and the value vectors in the candidate set for ma"
2020.acl-main.637,D19-1337,0,0.0243585,"ak scalability to large ontologies and cannot deal with previously unseen slot values. In open vocabulary-based DST, Xu and Hu (2018) propose a model that learns to predict unknown values by using the index-based pointer network for different slots. Wu et al. (2019) apply an encoder-decoder architecture to generate dialogue states with the copy mechanism. However, their method simply concatenates the whole dialogue context as input and does not perform well when the dialogue context is long. We study this problem and propose methods to help the DST model better model long context. Inspired by Zhou et al. (2019) who use an additional language model in question generation, we attempt to incorporate language modeling into dialogue state tracking as an auxiliary task. 3 Our Methods In this section, we describe our proposed methods. First, section 3.1 briefly introduces the recent TRADE model (Wu et al., 2019) as background knowledge, followed by our methods: utterance tagging in section 3.2 and multi-task learning with language modeling in section 3.3. 3.1 Transferable Dialogue State Generator TRADE is an encoder-decoder model that encodes concatenated previous system and user utterances as dialogue con"
2020.coling-main.201,K16-1002,0,0.0410842,"017), offensive language translation (Nogueira dos Santos et al., 2018), and data augmentation (Perez and Wang, 2017; Mikołajczyk and Grochowski, 2018; Zhu et al., 2020), etc. However, there are a variety of challenges to text style transfer in practice. First, we do not have large-scale style-to-style parallel data to train a text style transfer model in a supervised way. Second, even with non-parallel corpora, the inherent discrete structure of text sequences aggravates the difficulty of learning desirable continuous representations for style transfer (Huang et al., 2020; Zhao et al., 2018; Bowman et al., 2016; Hjelm et al., 2018). Third, it is difficult to preserve the content of a text when its style is transferred. To obtain good content preservation for text style transfer, various disentanglement approaches (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Sudhakar et al., 2019) are proposed to separate the content and style of a text in the latent space. However, content-style disentanglement is not easily achievable as content and style typically interact with each other in texts in subtle ways (Lample et al., 2019). In order to solve the issues above, we propose a cycle-consistent adver"
2020.coling-main.201,N19-1374,0,0.0126546,"sive experiments and in-depth analyses on two widely-used public datasets consistently validate the effectiveness of proposed CAE in both style transfer and content preservation against several strong baselines in terms of four automatic evaluation metrics and human evaluation. 1 Introduction Unsupervised text style transfer is to rewrite a text in one style into a text in another style while the content of the text remains the same as much as possible without using any parallel data. Style transfer can be utilized in many tasks such as personalization in dialogue systems (Oraby et al., 2018; Colombo et al., 2019), sentiment and word decipherment (Shen et al., 2017), offensive language translation (Nogueira dos Santos et al., 2018), and data augmentation (Perez and Wang, 2017; Mikołajczyk and Grochowski, 2018; Zhu et al., 2020), etc. However, there are a variety of challenges to text style transfer in practice. First, we do not have large-scale style-to-style parallel data to train a text style transfer model in a supervised way. Second, even with non-parallel corpora, the inherent discrete structure of text sequences aggravates the difficulty of learning desirable continuous representations for style"
2020.coling-main.201,P19-1601,0,0.0153258,"0) presents a new probabilistic graphical model for unsupervised text style transfer. In the second line of works that avoid disentangled representations of style and content, Lample et al. (2019) use back-translation technique on denoising autoencoder model with latent representation pooling to control the content preservation. Their experiments and analyses show that the contentstyle disentanglement is neither necessary nor always satisfied with practical requirements, even with the domain adversarial training that explicitly aims at learning disentangled representations. Style Transformer (Dai et al., 2019) uses Transformer as a basic module to train a style transfer system. DualRL (Luo et al., 2019) employs a dual reinforcement learning framework with two sequence-to-sequence models in two directions, using style classifier and back-transfer reconstruction probability as rewards. We follow the second line and propose a novel method that makes no assumption on the latent representation disentanglement. But differently, we perform style transfer in the latent representational spaces of the source and target style. And inspired by CycleGAN (Zhu et al., 2017; Zhu et al., 2018) which uses a cycle lo"
2020.coling-main.201,E17-2068,0,0.0498826,"Missing"
2020.coling-main.201,N18-1169,0,0.389116,"transferred sentences in another style. Bottom: cycle-consistent constraint enforcing that sentences transferred into a different style can be translated back to its original meaning in its original style. true samples in the other to obtain the shared latent content distribution. Fu et al. (2018) use an adversarial network to separate content representations from style representations. Prabhumoye et al. (2018) fix the machine translation model and the encoder of the back-translation model to obtain content representations, then generate texts with classifier-guided style-specific generators. Li et al. (2018) extract content words by deleting style indicator words, then combine the content words with retrieved style words to construct the final output. Xu et al. (2018) use reinforcement learning to jointly train a neutralization module which removes style words based on a classifier and an emotionalization module. ARAE (Zhao et al., 2018) and DAAE (Shen et al., 2020b) train GAN-regularized latent representations to obtain styleindependent content representations, then decodes the content representations conditioned on style. He et al. (2020) presents a new probabilistic graphical model for unsuper"
2020.coling-main.201,P18-2031,0,0.0201726,"osed CAE in both style transfer and content preservation against several strong baselines in terms of four automatic evaluation metrics and human evaluation. 1 Introduction Unsupervised text style transfer is to rewrite a text in one style into a text in another style while the content of the text remains the same as much as possible without using any parallel data. Style transfer can be utilized in many tasks such as personalization in dialogue systems (Oraby et al., 2018; Colombo et al., 2019), sentiment and word decipherment (Shen et al., 2017), offensive language translation (Nogueira dos Santos et al., 2018), and data augmentation (Perez and Wang, 2017; Mikołajczyk and Grochowski, 2018; Zhu et al., 2020), etc. However, there are a variety of challenges to text style transfer in practice. First, we do not have large-scale style-to-style parallel data to train a text style transfer model in a supervised way. Second, even with non-parallel corpora, the inherent discrete structure of text sequences aggravates the difficulty of learning desirable continuous representations for style transfer (Huang et al., 2020; Zhao et al., 2018; Bowman et al., 2016; Hjelm et al., 2018). Third, it is difficult to pre"
2020.coling-main.201,W18-5019,0,0.028298,"ed end-to-end. Extensive experiments and in-depth analyses on two widely-used public datasets consistently validate the effectiveness of proposed CAE in both style transfer and content preservation against several strong baselines in terms of four automatic evaluation metrics and human evaluation. 1 Introduction Unsupervised text style transfer is to rewrite a text in one style into a text in another style while the content of the text remains the same as much as possible without using any parallel data. Style transfer can be utilized in many tasks such as personalization in dialogue systems (Oraby et al., 2018; Colombo et al., 2019), sentiment and word decipherment (Shen et al., 2017), offensive language translation (Nogueira dos Santos et al., 2018), and data augmentation (Perez and Wang, 2017; Mikołajczyk and Grochowski, 2018; Zhu et al., 2020), etc. However, there are a variety of challenges to text style transfer in practice. First, we do not have large-scale style-to-style parallel data to train a text style transfer model in a supervised way. Second, even with non-parallel corpora, the inherent discrete structure of text sequences aggravates the difficulty of learning desirable continuous rep"
2020.coling-main.201,P02-1040,0,0.109122,"Missing"
2020.coling-main.201,P18-1080,0,0.0192336,"nsferred samples from one style with the creativecommons.org/licenses/by/4.0/. 2214 Figure 2: The visualization of style transfer and cycle-consistent constraint in CAE. Upper: sentences in one style and style-transferred sentences in another style. Bottom: cycle-consistent constraint enforcing that sentences transferred into a different style can be translated back to its original meaning in its original style. true samples in the other to obtain the shared latent content distribution. Fu et al. (2018) use an adversarial network to separate content representations from style representations. Prabhumoye et al. (2018) fix the machine translation model and the encoder of the back-translation model to obtain content representations, then generate texts with classifier-guided style-specific generators. Li et al. (2018) extract content words by deleting style indicator words, then combine the content words with retrieved style words to construct the final output. Xu et al. (2018) use reinforcement learning to jointly train a neutralization module which removes style words based on a classifier and an emotionalization module. ARAE (Zhao et al., 2018) and DAAE (Shen et al., 2020b) train GAN-regularized latent re"
2020.coling-main.201,D19-1499,0,0.0193195,"to enforce the back-translation of a transferred image to be equivalent to the original image, we also impose a cycle-consistent constraint on our style transfer network. However, training style transfer networks with such a cycle constraint on discrete texts is quite different from those on images and non-trivial. In order to enable cycle training on texts, we project texts onto adversarially regularized latent space collectively learned by the LSTM autoencoders and adversarial transfer networks. Different from latent cross project with Euclidean distance for semi-supervised style transfer (Shang et al., 2019), we construct latent CycleGAN to generate high quality sentences for unsupervised style transfer. 2215 3 CAE: Cycle-consistent Adversarial Autoencoders Suppose we have two non-parallel text datasets X1 = {x1i }ni=1 and X2 = {x2j }m j=1 with different styles s1 and s2 . The CAE employs LSTM autoencoder models to encode discrete text sequences x1i , x2j into rep˜ 1→2 ˜ 2→1 resentations zi1 , zj2 , and to generate sentences x ,x based on latent representations z˜i1→2 , z˜j2→1 i j transferred by the adversarial transfer network T 1→2 , T 2→1 from zi1 , zj2 . 3.1 LSTM autoencoders We use an LSTM ("
2020.coling-main.201,D19-1322,0,0.0199081,"ge-scale style-to-style parallel data to train a text style transfer model in a supervised way. Second, even with non-parallel corpora, the inherent discrete structure of text sequences aggravates the difficulty of learning desirable continuous representations for style transfer (Huang et al., 2020; Zhao et al., 2018; Bowman et al., 2016; Hjelm et al., 2018). Third, it is difficult to preserve the content of a text when its style is transferred. To obtain good content preservation for text style transfer, various disentanglement approaches (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Sudhakar et al., 2019) are proposed to separate the content and style of a text in the latent space. However, content-style disentanglement is not easily achievable as content and style typically interact with each other in texts in subtle ways (Lample et al., 2019). In order to solve the issues above, we propose a cycle-consistent adversarial autoencoders (CAE) for unsupervised text style transfer. In CAE, we learn the representation of a text where we embed both content and style in the same space. Such space is constructed for each style from non-parallel data. We then transfer the learned representation from on"
2020.coling-main.201,P19-1482,0,0.018046,"ith dropout 0.2) to build the language models and calculate PPL and RPPL. These four evaluation metrics together form a comprehensive evaluation and comparison between different approaches. We also conducted human evaluation. We randomly chose 200 instances from each style for the human evaluation. Four human annotators can proficiently understand English texts and have sufficient background knowledge about this evaluation task. The annotation is blind to them in random order. They grade all sentences with scores from one to five for style transfer, content preservation and fluency. Following Wu et al. (2019) and Li et al. (2018), we regard a style transfer with scores larger than or equal to four on all three measures (style transfer, content preservation and fluency) as a successful transfer. We calculate the percentage of successful transfers and refer to this percentage as Suc. 4.2 4.2.1 Results Yelp restaurant reviews sentiment transfer The results are shown in Table 2 (left), from which we clearly observe that CAE obtains better performance than the five baseline approaches. Specifically, CAE yields improvements of 5.1, 6.1 and 7.0 points over the best baselines for sentiment transfer in ter"
2020.coling-main.201,P18-1090,0,0.116653,"ts original meaning in its original style. true samples in the other to obtain the shared latent content distribution. Fu et al. (2018) use an adversarial network to separate content representations from style representations. Prabhumoye et al. (2018) fix the machine translation model and the encoder of the back-translation model to obtain content representations, then generate texts with classifier-guided style-specific generators. Li et al. (2018) extract content words by deleting style indicator words, then combine the content words with retrieved style words to construct the final output. Xu et al. (2018) use reinforcement learning to jointly train a neutralization module which removes style words based on a classifier and an emotionalization module. ARAE (Zhao et al., 2018) and DAAE (Shen et al., 2020b) train GAN-regularized latent representations to obtain styleindependent content representations, then decodes the content representations conditioned on style. He et al. (2020) presents a new probabilistic graphical model for unsupervised text style transfer. In the second line of works that avoid disentangled representations of style and content, Lample et al. (2019) use back-translation tech"
2020.coling-main.209,D10-1059,0,0.0355734,"ax(˜ ot ), o˜t = (ot + η)/τ, η = − log(− log u) (5) where η is the Gumbel noise calculated from a uniform random variable u ∼ U (0, 1), τ is temperature. When τ → 0, the sample generated from the vocabulary is similar to the argmax operation, and when τ → ∞, the sample is gradually closing uniform distribution. Increasing the temperature increases the use of infrequent words (Holtzman et al., 2019), which has the implicit effect of weakening the tail distribution, making the model to explore more diverse generation. Finally, according to pvocab (yt |Y1:t−1 , X), we apply multinomial sampling (Chatterjee and Cancedda, 2010) to generate sentence Yˆ for computing rewards, which produces each word one by one through multinomial sampling over the model’s output distribution. The sampling terminate the expansion of a candidate sentence when an end of sentence (&lt;EOS&gt;) token is met. 2.4 Reinforcement Learning with Explored Paraphrase We adopt reinforcement learning (RL) (Sutton and Barto, 1998) to train our paraphrase generator by using the sampled sentences. Our paraphrase generator can be viewed as an “agent” that interacts with an external “environment” (original input or reference). The parameters of the agent defi"
2020.coling-main.209,P11-1020,0,0.0558942,"ora dataset. 4.3 Automatic Evaluation Metrics Following previous work (Prakash et al., 2016; Hasan et al., 2016) on paraphrase generation, we adopted well-known automatic evaluation metrics BLEU (B) (Papineni et al., 2002), ROUGE (R) (Lin, 2004) and METEOR (MET) (Lavie and Agarwal, 2007) to compute lexical similarity with reference. Pervious studies have shown that these metrics perform well in evaluating generated paraphrases. These n-gram-based matching may obtain low score for predictions with highly lexical and syntactical variation, but these predictions are not necessarily poor quality (Chen and Dolan, 2011; Wang et al., 2019). We further used Embedding Similarity (Sharma et al., 2017) to evaluate generated paraphrases. This metric measures the semantic similarity between the reference and prediction based on the cosine similarity of their embeddings on word and sentence level. Following previous work (Park et al., 2019; Egonmwan and Chali, 2019), we used average, extreme, and greedy (A/E/G) embedding similarities. Besides, we hope to generate more diverse paraphrases when preserving meaning. However, previous work (Miao et al., 2019) has shown that it is insufficient when only comparing with re"
2020.coling-main.209,P19-1599,0,0.148187,"y a reward function. Li et al. (2019) suppose a sentence-level paraphrase can be decomposed to word/phrase-level paraphrase and learn to generate paraphrases at different levels of granularity. More recent works also focus on generating diverse paraphrases, which is important for improving model generalization capability and robustness of downstream applications. Gupta et al. (2018) use a variational autoencoder framework to generate diverse paraphrases by introducing random noise as input. Iyyer et al. (2018) harness syntactic-tree template information for controllable paraphrase generation. Chen et al. (2019) use sentences as exemplars to graft their syntax style to generated paraphrases. Qian et al. (2019) uses multiple generators trained by reinforcement learning to generate diverse paraphrases. Similar to these works, we also adopt Seq2Seq model for paraphrase generation. However, significantly different from them, our work extend the Seq2Seq model to use explored paraphrases for model training via deep reinforcement learning. We further introduce evaluation metrics in terms of expressive diversity and semantic similarity for model learning. Finally, our model can effectively generate paraphras"
2020.coling-main.209,D14-1179,0,0.0154494,"Missing"
2020.coling-main.209,D17-1091,0,0.0615809,"-based algorithm that automatically balances these training objectives. Experiments and analyses on Quora and Twitter datasets demonstrate that our proposed method not only gains a significant increase in diversity but also improves generation quality over several state-of-the-art baselines. 1 Introduction Paraphrase generation (PG) creates different expressions that share the same meaning (e.g., “how far is Earth from Sun” and “what is the distance between Sun and Earth”). It is a crucial technology in many downstream natural language processing (NLP) applications such as question answering (Dong et al., 2017), machine translation (Zhou et al., 2019), and text summarization (Zhao et al., 2018). Diversity is an essential characteristic of human language, as the meaning of a text can often have multiple different expressions. A good paraphrase generation system is often required to conform to two desired properties (Xu et al., 2018b). The first is diversity, capturing a wide range of linguistic variations. The second is fidelity, preserving semantic meanings while paraphrasing. Therefore, we hope to generate diverse paraphrases while ensuring same meaning, which is important for enhancing generalizat"
2020.coling-main.209,D19-5627,0,0.053267,"us studies have shown that these metrics perform well in evaluating generated paraphrases. These n-gram-based matching may obtain low score for predictions with highly lexical and syntactical variation, but these predictions are not necessarily poor quality (Chen and Dolan, 2011; Wang et al., 2019). We further used Embedding Similarity (Sharma et al., 2017) to evaluate generated paraphrases. This metric measures the semantic similarity between the reference and prediction based on the cosine similarity of their embeddings on word and sentence level. Following previous work (Park et al., 2019; Egonmwan and Chali, 2019), we used average, extreme, and greedy (A/E/G) embedding similarities. Besides, we hope to generate more diverse paraphrases when preserving meaning. However, previous work (Miao et al., 2019) has shown that it is insufficient when only comparing with reference because simply copying the input sentence itself yields the highest BLEU-ref score. To evaluate the variation of generated paraphrases, following Miao et al. (2019), we used BLEU-ori (B-ori) metric that against the original input sentence, in which the lower n-gram overlaps, the better variation and diversity. 4.4 Baselines We compared"
2020.coling-main.209,N18-1170,0,0.241258,"ization (Zhao et al., 2018). Diversity is an essential characteristic of human language, as the meaning of a text can often have multiple different expressions. A good paraphrase generation system is often required to conform to two desired properties (Xu et al., 2018b). The first is diversity, capturing a wide range of linguistic variations. The second is fidelity, preserving semantic meanings while paraphrasing. Therefore, we hope to generate diverse paraphrases while ensuring same meaning, which is important for enhancing generalization capability and robustness of downstream applications (Iyyer et al., 2018). As shown in Table 1, we give some examples. These examples express the same meaning but with different diversities. Most recent state-of-the-art approaches to PG (Prakash et al., 2016; Hasan et al., 2016; Gupta et al., 2018) employ neural sequence-to-sequence (Seq2Seq) models, which mainly uses one given reference for model learning, while the nature of paraphrasing indicates that we can paraphrase one sentence into several different sentences. Meanwhile these methods usually adopt the cross-entropy loss which requires a strict pairwise matching at the word level between the predicted senten"
2020.coling-main.209,P19-1607,0,0.126208,"guage? there is a possible way to improve my English language? what is the best way to increase my English knowledge? Table 1: Paraphrases of an original sentence with increasing diversity. night.” and the reference “I saw a film last night.”, the cross-entropy loss lacks the ability to properly optimize model to generate a diverse paraphrase even with only one changed token at word level. In recent years, there are also growing interests in generating lexically and syntactically diverse paraphrases (Gupta et al., 2018; Xu et al., 2018b; Xu et al., 2018a; Park et al., 2019; Qian et al., 2019; Kajiwara, 2019). For Seq2Seq models, the techniques of generating diverse paraphrases mainly include two categories: i) applying decoding methods such as using beam search or multiple decoders; ii) introducing random noise as model input. Park et al. (2019) use multi-time decoding to diverse generation by considering those generated sentences previously. Qian et al. (2019) use multiple generators to generate a variety of different paraphrases. Gupta et al. (2018) employ a variational auto-encoder framework to produce multiple paraphrases according to different noise inputs. Although these methods can improve"
2020.coling-main.209,D17-1126,0,0.0136537,"ning. 4 Experiment In this section, we described the datasets, experimental setup, evaluation metrics and the results of our experiments. 4.1 Datasets We conducted experiments on two standard datasets Quora and Twitter to evaluate the proposed model. Quora Dataset This dataset is a paired paraphrase dataset in question domain. It consists of 150K paraphrase pairs. Following previous work (Li et al., 2018; Qian et al., 2019), we used 30K pairs and 4K pairs as test set and validation set, and 100K pairs for training, respectively. Twitter Dataset This dataset is Twitter URL paraphrasing corpus (Lan et al., 2017) that contains two subsets, one is labelled by human annotators while the other is labelled automatically by algorithm. Following previous work (Li et al., 2018; Qian et al., 2019), we sampled 5K pairs as the test set and 1K pairs as validation set from the labeled subset, while using the remaining 110K pairs as training set. 4.2 Model Configuration We used the following experimental setting for our model. Following Li et al. (2018), we maintained a fixed-size vocabulary of 5K shared by the words in input and output, and truncate all the sentences longer than 20 words. For paraphrase generator"
2020.coling-main.209,W07-0734,0,0.076928,"Work 59.27 32.00 30.49 61.81 35.27 34.21 63.84 36.97 34.78 59.96 33.93 33.32 63.08 36.61 34.83 64.66 38.22 35.02 Emb(A/E/G)↑ B-ori-1↓ 80.61/-/64.81 81.50/-/65.52 - - 92.58/71.94/82.63 93.35/75.22/92.11 93.73/76.10/92.12 93.10/74.13/90.61 93.81/76.34/92.12 93.74/76.33/92.31 60.57 67.91 70.54 66.63 71.06 67.60 Table 2: Performance on Quora dataset. 4.3 Automatic Evaluation Metrics Following previous work (Prakash et al., 2016; Hasan et al., 2016) on paraphrase generation, we adopted well-known automatic evaluation metrics BLEU (B) (Papineni et al., 2002), ROUGE (R) (Lin, 2004) and METEOR (MET) (Lavie and Agarwal, 2007) to compute lexical similarity with reference. Pervious studies have shown that these metrics perform well in evaluating generated paraphrases. These n-gram-based matching may obtain low score for predictions with highly lexical and syntactical variation, but these predictions are not necessarily poor quality (Chen and Dolan, 2011; Wang et al., 2019). We further used Embedding Similarity (Sharma et al., 2017) to evaluate generated paraphrases. This metric measures the semantic similarity between the reference and prediction based on the cosine similarity of their embeddings on word and sentenc"
2020.coling-main.209,D18-1421,0,0.633688,"hood of the ground-truth reference and (b) it makes our model has the ability to explore unseen paraphrases beyond one single reference. 3.1 Rewards for Multi-Objective Learning ROUGE Reward with Reference The first basic reward is based on the primary evaluation metric of ROUGE package (Lin, 2004). We compare a sampled sentence Yˆ s1 with ground-truth reference Y ref with ROUGE score (namely ROUGE-ref), and then takes the score as a reward. The loss function is given by: ∇θ Lrl1 (θ) ≈ −(ROUGE−ref(Yˆ s1 , Y ref )−b)∇θ log pθ (Yˆ s1 ), b = ROUGE−ref(Yˆ b , Y ref ) (9) Similar to previous work (Li et al., 2018), we find that ROUGE-ref score as a reward works better compared to only using cross-entropy loss. This reward can be taken as sentence-level learning signal, which overcomes the full token-level matching issue of cross-entropy loss at training stage. On the other hand, as pointed in Kajiwara (2019), paraphrase generation rewrites only a limited portion of an original input and the reference often includes some words occurred in the original input, thus a sentence with higher ROUGE-ref score may have low diversity (Miao et al., 2019). Therefore, the reward based on reference do not focus on th"
2020.coling-main.209,P19-1332,0,0.400494,"ual LSTM RbM-SL RbM-IRL DEPD 33.90 44.67 45.74 34.23 Seq2SeqAttn PNet PNet + LE-ROUGE-ref PNet + LE-ROUGE-ori PNet + LE-SEM-ori PNet + all 30.86 31.10 33.45 28.48 28.64 32.84 R-1↑ R-2↑ MET↑ Previous Work 32.50 16.86 13.65 41.87 24.23 19.97 42.15 24.73 20.18 24.29 This Work 40.65 28.35 20.68 43.21 28.94 23.91 45.89 31.20 25.60 39.67 26.30 21.93 40.36 26.43 21.87 45.51 30.61 25.20 Emb(A/E/G)↑ B-ori-1↓ - - 82.91/53.71/87.36 84.04/54.66/90.16 84.92/56.86/92.28 81.52/51.43/88.42 82.68/52.37/90.81 85.57/57.46/93.81 48.31 52.87 57.16 46.39 49.19 53.21 Table 3: Performance on Twitter dataset. • DPNG (Li et al., 2019): This is a Transformer-based model that can learn and generate paraphrases of a sentence at different levels of granularity (word or phrase) in a disentangled way. 4.5 Results Baseline Cross-Entropy Model Results Our paraphrase generation model has attention mechanism (Seq2SeqAtt) and pointer-generator network (PNet). For better observing model behaviour, we first trained two baselines by applying cross-entropy optimization. As we can see, the model with pointergenerator network effectively improves performance in all metrics related to reference. And it is also natural for PNet to obtain hig"
2020.coling-main.209,P02-1040,0,0.106418,"5.84 64.39 38.11 32.84 64.02 37.72 31.97 29.28 63.73 37.75 This Work 59.27 32.00 30.49 61.81 35.27 34.21 63.84 36.97 34.78 59.96 33.93 33.32 63.08 36.61 34.83 64.66 38.22 35.02 Emb(A/E/G)↑ B-ori-1↓ 80.61/-/64.81 81.50/-/65.52 - - 92.58/71.94/82.63 93.35/75.22/92.11 93.73/76.10/92.12 93.10/74.13/90.61 93.81/76.34/92.12 93.74/76.33/92.31 60.57 67.91 70.54 66.63 71.06 67.60 Table 2: Performance on Quora dataset. 4.3 Automatic Evaluation Metrics Following previous work (Prakash et al., 2016; Hasan et al., 2016) on paraphrase generation, we adopted well-known automatic evaluation metrics BLEU (B) (Papineni et al., 2002), ROUGE (R) (Lin, 2004) and METEOR (MET) (Lavie and Agarwal, 2007) to compute lexical similarity with reference. Pervious studies have shown that these metrics perform well in evaluating generated paraphrases. These n-gram-based matching may obtain low score for predictions with highly lexical and syntactical variation, but these predictions are not necessarily poor quality (Chen and Dolan, 2011; Wang et al., 2019). We further used Embedding Similarity (Sharma et al., 2017) to evaluate generated paraphrases. This metric measures the semantic similarity between the reference and prediction base"
2020.coling-main.209,C18-1230,0,0.0262193,"opying the input sentence itself yields the highest BLEU-ref score. To evaluate the variation of generated paraphrases, following Miao et al. (2019), we used BLEU-ori (B-ori) metric that against the original input sentence, in which the lower n-gram overlaps, the better variation and diversity. 4.4 Baselines We compared our model with several state-of-the-art models in the paraphrase generation field. • Residual LSTM (Prakash et al., 2016): This implements stacked residual LSTM networks. • VAE-SVG-eq (Gupta et al., 2018): This employs a variational autoencoder as its main component. • EDD-LG (Patro et al., 2018): This introduces semantic discriminator to learn encoder and decoder. • TRANS and TRANSSEQ (Egonmwan and Chali, 2019): This integrates Transformer model (Vaswani et al., 2017) and Recurrent Neural Network GRU (Cho et al., 2014) as encoder. • RbM-SL and RbM-IRL (Li et al., 2018): This is a generator-evaluator framework with the matching-based semantic evaluator trained by reinforcement learning. • DEPD (Qian et al., 2019): This uses multiple generators trained by reinforcement learning to generate a variety of different paraphrases. 2316 Model B-2↑ Residual LSTM RbM-SL RbM-IRL DEPD 33.90 44.67"
2020.coling-main.209,C16-1275,0,0.0429483,"Missing"
2020.coling-main.209,D19-1313,0,0.409468,"edge in English language? there is a possible way to improve my English language? what is the best way to increase my English knowledge? Table 1: Paraphrases of an original sentence with increasing diversity. night.” and the reference “I saw a film last night.”, the cross-entropy loss lacks the ability to properly optimize model to generate a diverse paraphrase even with only one changed token at word level. In recent years, there are also growing interests in generating lexically and syntactically diverse paraphrases (Gupta et al., 2018; Xu et al., 2018b; Xu et al., 2018a; Park et al., 2019; Qian et al., 2019; Kajiwara, 2019). For Seq2Seq models, the techniques of generating diverse paraphrases mainly include two categories: i) applying decoding methods such as using beam search or multiple decoders; ii) introducing random noise as model input. Park et al. (2019) use multi-time decoding to diverse generation by considering those generated sentences previously. Qian et al. (2019) use multiple generators to generate a variety of different paraphrases. Gupta et al. (2018) employ a variational auto-encoder framework to produce multiple paraphrases according to different noise inputs. Although these me"
2020.coling-main.209,P17-1099,0,0.0602623,"enote the sequence generated by the model. Our proposed model mainly contains three components: paraphrase generator, sample-based exploring algorithm and reinforcement learning with explored paraphrase. Figure 1 gives an overview of our framework. Basically the generator can generate paraphrases of a given sentence, and the evaluator measures the quality of explored paraphrases in term of expressive diversity and semantic fidelity. 2.2 Paraphrase Generator We frame paraphrase generation as a sequence-to-sequence (Seq2Seq) problem. We adopt the encoderdecoder framework (Bahdanau et al., 2014; See et al., 2017), both of which are implemented as recurrent neural networks (RNN). All RNNs use LSTM cells (Hochreiter and Schmidhuber, 1997). Given an input sentence X, the goal is to learn a model p(θ) that can generate a sentence Yˆ = pθ (X) as its paraphrase. Traditionally the parameters θ are learned by maximizing the likelihood of the predicted sentence. Finally, model estimates the conditional probability p(Y |X) via directly mapping the input sentence X to its target paraphrase Y. The learning objective is to minimize the cross-entropy loss: Lce (θ) = − T X log pθ (yt |Y1:t−1 , X) (1) t=1 We choose t"
2020.coling-main.209,D18-1397,0,0.0181518,"e Optimization Our objective function combines the maximum-likelihood cross-entropy loss (Lce ) with rewards from policy gradient reinforcement learning to jointly optimize our model. Finally, the over learning objective is to minimize the following combined losses: Lall (θ) = α0 ∗ Lce + α1 ∗ Lrl1 + α2 ∗ Lrl2 + α3 ∗ Lrl3 (13) where α is the the weights to combine these losses. Optimizing multiple objectives at the same time is important for final performance, in which one objective can easily dominate the learning of a shared model, leading the other objectives are ineffective. Previous work (Wu et al., 2018) choose fixed weights α from manual experience for RL learning. Different from them, we use an adaptive method GradNorm (Chen et al., 2017), and the αi is vary at each training step t: αi = αi (t). The GradNorm algorithm controls gradient magnitudes through tuning of the multi-objective loss function. To optimize the weights αi (t) for gradient balancing, following Chen et al. (2017), we penalize the network when back-propagated gradients from any task are too large or too small. If objective i is training relatively quickly, then its weight αi (t) should decrease relative to other objective w"
2020.coling-main.209,D18-1355,0,0.0957376,"nd analyses on Quora and Twitter datasets demonstrate that our proposed method not only gains a significant increase in diversity but also improves generation quality over several state-of-the-art baselines. 1 Introduction Paraphrase generation (PG) creates different expressions that share the same meaning (e.g., “how far is Earth from Sun” and “what is the distance between Sun and Earth”). It is a crucial technology in many downstream natural language processing (NLP) applications such as question answering (Dong et al., 2017), machine translation (Zhou et al., 2019), and text summarization (Zhao et al., 2018). Diversity is an essential characteristic of human language, as the meaning of a text can often have multiple different expressions. A good paraphrase generation system is often required to conform to two desired properties (Xu et al., 2018b). The first is diversity, capturing a wide range of linguistic variations. The second is fidelity, preserving semantic meanings while paraphrasing. Therefore, we hope to generate diverse paraphrases while ensuring same meaning, which is important for enhancing generalization capability and robustness of downstream applications (Iyyer et al., 2018). As sho"
2020.coling-main.209,P19-2015,0,0.10207,"ces these training objectives. Experiments and analyses on Quora and Twitter datasets demonstrate that our proposed method not only gains a significant increase in diversity but also improves generation quality over several state-of-the-art baselines. 1 Introduction Paraphrase generation (PG) creates different expressions that share the same meaning (e.g., “how far is Earth from Sun” and “what is the distance between Sun and Earth”). It is a crucial technology in many downstream natural language processing (NLP) applications such as question answering (Dong et al., 2017), machine translation (Zhou et al., 2019), and text summarization (Zhao et al., 2018). Diversity is an essential characteristic of human language, as the meaning of a text can often have multiple different expressions. A good paraphrase generation system is often required to conform to two desired properties (Xu et al., 2018b). The first is diversity, capturing a wide range of linguistic variations. The second is fidelity, preserving semantic meanings while paraphrasing. Therefore, we hope to generate diverse paraphrases while ensuring same meaning, which is important for enhancing generalization capability and robustness of downstre"
2020.coling-main.432,D18-1307,0,0.0178858,"r joint intent detection and slot filling. 2.2 Adversarial Training Adversarial examples (Goodfellow et al., 2015) have been originally introduced in the context of image classification to fool neural models with unperceivable perturbations on images. After that, adversarial training (AT) that uses automatically generated adversarial examples to strengthen the robustness of neural models has become a popular research topic. (Miyato et al., 2017) adapt AT to text classification by adding perturbations on word embeddings. Following that, many researchers (Wu et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Ju et al., 2019) have attempted to explore the potential benefits that AT can bring to NLP tasks. (Yasunaga et al., 2018) use AT on POS tagging and find that AT not only improves overall tagging accuracy, but also helps the model to learn cleaner word representations. The same benefits have also emerged in the text classification task with AT (Miyato et al., 2017). Joint intent detection and slot filling is a multi-task learning problem. It is more challenging to develop an AT algorithm that naturally fits in multi-task learning. Different from previous works on adapting AT to single NLP tas"
2020.coling-main.432,N19-1423,0,0.0731183,"Missing"
2020.coling-main.432,P19-1544,0,0.0371506,"Missing"
2020.coling-main.432,N18-2118,0,0.3771,"t results on two datasets. 1 Introduction Intent detection and slot filling are two critical components in dialogue systems. Although these two tasks are normally considered as parallel tasks, they may inherently correlate with each other as one is helpful in defining the other. Some existing works (Liu and Lane, 2016; Vu, 2016; Zhang and Wang, 2016) indicate that sharing parameters in an encoder module to simultaneously detect the intent and fill the slots of an utterance can utilize such correlation between the two tasks. The joint modeling of them has therefore achieved tremendous success (Goo et al., 2018; Li et al., 2018; E et al., 2019) and further improved the performance of spoken language understanding (SLU) systems. However, recent outstanding joint models still show insufficient robustness. For example, some small changes of inputs can mislead the models to give wrong prediction. As shown in Figure 1 (a), misspelled words “SanFranciso” (missing a blank) and “pitsburgh” (missing “t”) are easily to fool the neural models to produce wrong slot predictions. One might think that such wrong predictions are related to the input word embeddings and using character embeddings can avoid these pre"
2020.coling-main.432,N19-3014,0,0.024286,"Missing"
2020.coling-main.432,D18-1417,0,0.348367,"atasets. 1 Introduction Intent detection and slot filling are two critical components in dialogue systems. Although these two tasks are normally considered as parallel tasks, they may inherently correlate with each other as one is helpful in defining the other. Some existing works (Liu and Lane, 2016; Vu, 2016; Zhang and Wang, 2016) indicate that sharing parameters in an encoder module to simultaneously detect the intent and fill the slots of an utterance can utilize such correlation between the two tasks. The joint modeling of them has therefore achieved tremendous success (Goo et al., 2018; Li et al., 2018; E et al., 2019) and further improved the performance of spoken language understanding (SLU) systems. However, recent outstanding joint models still show insufficient robustness. For example, some small changes of inputs can mislead the models to give wrong prediction. As shown in Figure 1 (a), misspelled words “SanFranciso” (missing a blank) and “pitsburgh” (missing “t”) are easily to fool the neural models to produce wrong slot predictions. One might think that such wrong predictions are related to the input word embeddings and using character embeddings can avoid these prediction errors. H"
2020.coling-main.432,N18-2050,0,0.0190711,"TA performance in the datasets. 2 2.1 Related Work Joint Training The joint prediction of intent and slot labels has achieved higher performance due to information sharing between the two tasks than the parallel modeling of them. (Liu and Lane, 2016) explore the sequence-tosequence model (Guo et al., 2014) for the joint task and achieve decent results. Many works follow this idea and make further research. Models based on gating mechanism (Goo et al., 2018; Li et al., 2018; E et al., 2019) have been proposed for dynamically modeling the relationship between slot filling and intent detection. (Wang et al., 2018) propose a dual model which contains two correlated bidirectional LSTMs with cross-impact features. (Chen et al., 2019) utilize large pre-trained language models to learn better representations. Their work directly constructs the relationship between intent and slots from feature interaction. In this work, we also use the joint training setting, but focus on the robustness problem of the joint model in dealing with noises and variances. 4927 Figure 2: Illustration of the proposed model for joint intent detection and slot filling. 2.2 Adversarial Training Adversarial examples (Goodfellow et al."
2020.coling-main.432,D17-1187,0,0.0299384,"2: Illustration of the proposed model for joint intent detection and slot filling. 2.2 Adversarial Training Adversarial examples (Goodfellow et al., 2015) have been originally introduced in the context of image classification to fool neural models with unperceivable perturbations on images. After that, adversarial training (AT) that uses automatically generated adversarial examples to strengthen the robustness of neural models has become a popular research topic. (Miyato et al., 2017) adapt AT to text classification by adding perturbations on word embeddings. Following that, many researchers (Wu et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Ju et al., 2019) have attempted to explore the potential benefits that AT can bring to NLP tasks. (Yasunaga et al., 2018) use AT on POS tagging and find that AT not only improves overall tagging accuracy, but also helps the model to learn cleaner word representations. The same benefits have also emerged in the text classification task with AT (Miyato et al., 2017). Joint intent detection and slot filling is a multi-task learning problem. It is more challenging to develop an AT algorithm that naturally fits in multi-task learning. Different from p"
2020.coling-main.432,N18-1089,0,0.0240771,"f the proposed model for joint intent detection and slot filling. 2.2 Adversarial Training Adversarial examples (Goodfellow et al., 2015) have been originally introduced in the context of image classification to fool neural models with unperceivable perturbations on images. After that, adversarial training (AT) that uses automatically generated adversarial examples to strengthen the robustness of neural models has become a popular research topic. (Miyato et al., 2017) adapt AT to text classification by adding perturbations on word embeddings. Following that, many researchers (Wu et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Ju et al., 2019) have attempted to explore the potential benefits that AT can bring to NLP tasks. (Yasunaga et al., 2018) use AT on POS tagging and find that AT not only improves overall tagging accuracy, but also helps the model to learn cleaner word representations. The same benefits have also emerged in the text classification task with AT (Miyato et al., 2017). Joint intent detection and slot filling is a multi-task learning problem. It is more challenging to develop an AT algorithm that naturally fits in multi-task learning. Different from previous works on adapti"
2020.emnlp-main.223,2020.lrec-1.129,1,0.838482,"t Arg1-as-manner Arg2-as-manner Table 2: PDTB-3 Sense Hierarchy (Webber et al., 2019). The Level-2 senses are used in assessing system performance (Section 5.1). relation between two arguments, we tried to insert a connective for this relation. If a connective conveys more than one sense or more than one relation can be inferred, multiple senses would be assigned to the token. And we use a set of consistency rules due to specific linguistic properties in Chinese such as ellipsis of subject, pair connectives. As some syntactic and textual contexts could not been annotated in our previous work (Long et al., 2020), we loosen the constraints on arguments, connectives, and distance of arguments. In this way, more relations are acquired effectively on the same texts, thus revealing the discourse coherence and structure more fully and clearly. The following are the main additions to our annotation scheme, which future efforts at Chinese discourse annotation might consider adopting as well. In the examples throughout the paper, explicit connectives are underlined, while connectives inserted for implicit relations are both underlined and parenthesized. Sense labels are indicated after the connectives. plicit"
2020.emnlp-main.223,N19-1423,0,0.0483395,"Missing"
2020.emnlp-main.223,D19-1257,0,0.0227234,"nsfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines has been made freely available.1 1 Introduction Recent years have witnessed increasing attention to the properties of discourse for a wide variety of natural language processing (NLP) tasks, e.g., machine translation (Ohtani et al., 2019; Voita et al., 2019), summarization (Isonuma et al., 2019; Xu et al., 2020), machine reading comprehension (Mihaylov and Frank, 2019). One of those interesting properties is the coherence between clauses and sentences arising from shallow discourse relations. As empirical approaches for modeling discourse relations usually require corpora annotated with 1 https://github.com/wanqiulong0923/TED-CDB • the largest PDTB-style Chinese discourse corpus over spoken monologues (Section 3.1). Table 1 compares the TED-CDB with other discourse-annotated Chinese corpora. • new annotation elements to accommodate Chinese-specific discourse phenomena (Section 3.2). • benchmark results on Level-2 discourse relation classification for future"
2020.emnlp-main.223,I11-1170,0,0.033305,"lines should be used more widely instead of just being confined to construct corpora of written texts. Zeyrek et al. (2018) annotate 6 TED talks for 7 languages. Scheffler et al. (2019) build a discourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discourse corpus for Chinese, which is further expanded to contain 164 documents, namely the Treebank (CDTB) (CDTB)(Zhou and Xue, 2015). Huang and Chen (2011b) construct a Chinese discourse corpus with 81 articles. They adopt the top-level senses from PDTB sense hierarchy and focus on the annotation of inter-sentential relations. Zhou et al. (2014) present the CUHK Discourse Treebank. They adapt the annotation scheme of Penn Discourse Treebank 2 (PDTB-2) to Chinese and re-annotate the documents of the Chinese Treebank and with only intersentence explicit discourse relations. The largest Chinese discourse relation corpus for written texts is HIT-CDTB (Zhang et al., 2013), which presents a new Chinese discourse relation hierarchy adapted from the PD"
2020.emnlp-main.223,P19-1206,0,0.0136128,"h same-language cross-domain transfer and same-domain crosslanguage transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines has been made freely available.1 1 Introduction Recent years have witnessed increasing attention to the properties of discourse for a wide variety of natural language processing (NLP) tasks, e.g., machine translation (Ohtani et al., 2019; Voita et al., 2019), summarization (Isonuma et al., 2019; Xu et al., 2020), machine reading comprehension (Mihaylov and Frank, 2019). One of those interesting properties is the coherence between clauses and sentences arising from shallow discourse relations. As empirical approaches for modeling discourse relations usually require corpora annotated with 1 https://github.com/wanqiulong0923/TED-CDB • the largest PDTB-style Chinese discourse corpus over spoken monologues (Section 3.1). Table 1 compares the TED-CDB with other discourse-annotated Chinese corpora. • new annotation elements to accommodate Chinese-specific discourse phenomena (Section 3.2)."
2020.emnlp-main.223,P14-2047,0,0.0219731,"ntential relations in PDTB-3 are almost the same, but clearly, we can see that the discourse relations in our corpus are more commonly annotated within the sentence, consisting of 9,847 intra-sentential relations and 5,693 inter-sentential relations. The reason perhaps lies in the use of punctuation, which is quite different in Chinese than in English. For example, a comma sometimes serves the same function as a full stop in English (Xue and Yang, 2011). Therefore, a long Chinese sentence may require the use of multiple English sentences to express the same content and preserve grammatically (Li et al., 2014). This may be why there are more intra-sentential relations in Chinese than in English. We also compared the CDTB and our TED-CDB with respect to the sense distribution. This is displayed in Figure 1(a) and 1(b). CDTB uses an annotation style similar to the PDTB for the texts from the Chinese Treebank corpus. For a discourse relation, one of eight discourse relation senses is assigned. Although all senses in the CDTB are at the same level of the hierarchy, we can map them to the four top-level relation senses in the PDTB hierarchy according to their definitions: Alternative → Expansion; Causat"
2020.emnlp-main.223,2021.ccl-1.108,0,0.0903242,"Missing"
2020.emnlp-main.223,P15-1121,0,0.0605438,"Missing"
2020.emnlp-main.223,P19-1442,0,0.0446789,"Missing"
2020.emnlp-main.223,D19-6505,0,0.0260351,"xperiments have been carried out with the TED-CDB for both same-language cross-domain transfer and same-domain crosslanguage transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines has been made freely available.1 1 Introduction Recent years have witnessed increasing attention to the properties of discourse for a wide variety of natural language processing (NLP) tasks, e.g., machine translation (Ohtani et al., 2019; Voita et al., 2019), summarization (Isonuma et al., 2019; Xu et al., 2020), machine reading comprehension (Mihaylov and Frank, 2019). One of those interesting properties is the coherence between clauses and sentences arising from shallow discourse relations. As empirical approaches for modeling discourse relations usually require corpora annotated with 1 https://github.com/wanqiulong0923/TED-CDB • the largest PDTB-style Chinese discourse corpus over spoken monologues (Section 3.1). Table 1 compares the TED-CDB with other discourse-annotated Chinese corpora. • new annotation elements to accom"
2020.emnlp-main.223,prasad-etal-2008-penn,1,0.498579,"ere has been just one corpus for spoken discourse (Tonelli et al., 2010), where PDTB annotations are constructed on Italian dialogues. Recently, researchers have realized that the PDTB annotation guidelines should be used more widely instead of just being confined to construct corpora of written texts. Zeyrek et al. (2018) annotate 6 TED talks for 7 languages. Scheffler et al. (2019) build a discourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discourse corpus for Chinese, which is further expanded to contain 164 documents, namely the Treebank (CDTB) (CDTB)(Zhou and Xue, 2015). Huang and Chen (2011b) construct a Chinese discourse corpus with 81 articles. They adopt the top-level senses from PDTB sense hierarchy and focus on the annotation of inter-sentential relations. Zhou et al. (2014) present the CUHK Discourse Treebank. They adapt the annotation scheme of Penn Discourse Treebank 2 (PDTB-2) to Chinese and re-annotate the documents of the Chinese Treebank and with only intersentence exp"
2020.emnlp-main.223,W17-5502,0,0.0239043,"ith an audience, speakers often insert material meant to explain the details of the first argument to audience. Relations can be found across non-adjacent sentences in our annotations. The following are two examples – the first, of an explicit relation, and the second, of an implicit relation. Relations have been annotated across nonadjacent sentences While relations between non-adjacent sentences have only been annotated in the PDTB if Arg1 of an explicit connective is not adjacent to Arg2, implicit relations between non-adjacent sentences were not annotated, except in a small-scale study by Prasad et al. (2017) of relations between paragraph-initial sentences and material in the previous text. In contrast, we annotate relations across non-adjacent sentences not only for explicit relations but also im2795 (1) [我们在空间很早的时候，是做了一个接宝藏的 游戏]1 。[这种设计在现在看起来好像有点不可思 议，但是当时确实有效。因为它帮我们留住了 一些 实 在 等 不了 的 用 户 ， 也 避 免 了 用 户 流 失 。 所 以从 早 一 开 始 ， 我 们 空 间 跟 游 戏 就 息 息相 关 了]2 。ThenASYCHROUNOUS [后来， 我 们 的 团 队 也 参与去做了QQ农场的游戏]3 。 “[When we started to do Qzone, we designed a game about collecting treasures]1 . [This design may seem a bit weird now, but it worked at the time. Because it helps us retain some users who can’t wai"
2020.emnlp-main.223,I08-7010,0,0.0520237,"ere has been just one corpus for spoken discourse (Tonelli et al., 2010), where PDTB annotations are constructed on Italian dialogues. Recently, researchers have realized that the PDTB annotation guidelines should be used more widely instead of just being confined to construct corpora of written texts. Zeyrek et al. (2018) annotate 6 TED talks for 7 languages. Scheffler et al. (2019) build a discourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discourse corpus for Chinese, which is further expanded to contain 164 documents, namely the Treebank (CDTB) (CDTB)(Zhou and Xue, 2015). Huang and Chen (2011b) construct a Chinese discourse corpus with 81 articles. They adopt the top-level senses from PDTB sense hierarchy and focus on the annotation of inter-sentential relations. Zhou et al. (2014) present the CUHK Discourse Treebank. They adapt the annotation scheme of Penn Discourse Treebank 2 (PDTB-2) to Chinese and re-annotate the documents of the Chinese Treebank and with only intersentence exp"
2020.emnlp-main.223,P17-1090,0,0.0545751,"Missing"
2020.emnlp-main.223,E17-1027,0,0.0341222,"Missing"
2020.emnlp-main.223,P11-2111,0,0.0377191,"we try to detect all possible Altlex expressions that are capable of conveying the discourse relations. The number of the intra-sentential relations and inter-sentential relations in PDTB-3 are almost the same, but clearly, we can see that the discourse relations in our corpus are more commonly annotated within the sentence, consisting of 9,847 intra-sentential relations and 5,693 inter-sentential relations. The reason perhaps lies in the use of punctuation, which is quite different in Chinese than in English. For example, a comma sometimes serves the same function as a full stop in English (Xue and Yang, 2011). Therefore, a long Chinese sentence may require the use of multiple English sentences to express the same content and preserve grammatically (Li et al., 2014). This may be why there are more intra-sentential relations in Chinese than in English. We also compared the CDTB and our TED-CDB with respect to the sense distribution. This is displayed in Figure 1(a) and 1(b). CDTB uses an annotation style similar to the PDTB for the texts from the Chinese Treebank corpus. For a discourse relation, one of eight discourse relation senses is assigned. Although all senses in the CDTB are at the same leve"
2020.emnlp-main.223,W19-2707,0,0.0665757,"Missing"
2020.emnlp-main.223,L18-1301,0,0.020605,"k do not mention the number. systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora (Section 6). 2 Related Work Most annotations in PDTB style are conducted on written texts originating from news reports. Before 2015, there has been just one corpus for spoken discourse (Tonelli et al., 2010), where PDTB annotations are constructed on Italian dialogues. Recently, researchers have realized that the PDTB annotation guidelines should be used more widely instead of just being confined to construct corpora of written texts. Zeyrek et al. (2018) annotate 6 TED talks for 7 languages. Scheffler et al. (2019) build a discourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discourse corpus for Chinese, which is further expanded to contain 164 documents, namely the Treebank (CDTB) (CDTB)(Zhou and Xue, 2015). Huang and Chen (2011b) construct a Chinese discourse corpus with 81 articles. They adopt the top-level senses from PDTB sen"
2020.emnlp-main.223,tonelli-etal-2010-annotation,0,0.0354948,"News report Internet news Sino and travel set TED Talks Total Relations 5,534 21,505 3,081 15,540 Availability Through LDC From owner From owner From owner Freely public available Table 1: Comparison of our corpus to related data sets. “-” means the work do not mention the number. systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora (Section 6). 2 Related Work Most annotations in PDTB style are conducted on written texts originating from news reports. Before 2015, there has been just one corpus for spoken discourse (Tonelli et al., 2010), where PDTB annotations are constructed on Italian dialogues. Recently, researchers have realized that the PDTB annotation guidelines should be used more widely instead of just being confined to construct corpora of written texts. Zeyrek et al. (2018) annotate 6 TED talks for 7 languages. Scheffler et al. (2019) build a discourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discours"
2020.emnlp-main.223,P19-1116,0,0.0365635,"Missing"
2020.emnlp-main.223,2020.acl-main.451,0,0.0276861,"domain transfer and same-domain crosslanguage transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines has been made freely available.1 1 Introduction Recent years have witnessed increasing attention to the properties of discourse for a wide variety of natural language processing (NLP) tasks, e.g., machine translation (Ohtani et al., 2019; Voita et al., 2019), summarization (Isonuma et al., 2019; Xu et al., 2020), machine reading comprehension (Mihaylov and Frank, 2019). One of those interesting properties is the coherence between clauses and sentences arising from shallow discourse relations. As empirical approaches for modeling discourse relations usually require corpora annotated with 1 https://github.com/wanqiulong0923/TED-CDB • the largest PDTB-style Chinese discourse corpus over spoken monologues (Section 3.1). Table 1 compares the TED-CDB with other discourse-annotated Chinese corpora. • new annotation elements to accommodate Chinese-specific discourse phenomena (Section 3.2). • benchmark resul"
2020.emnlp-main.223,P13-1013,0,0.189869,"inese corpora. • new annotation elements to accommodate Chinese-specific discourse phenomena (Section 3.2). • benchmark results on Level-2 discourse relation classification for future comparison with other models (Section 5). • experiments with cross-domain and crosslingual transfer learning that show that the TED-CDB can improve the performance of 2793 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2793–2803, c November 16–20, 2020. 2020 Association for Computational Linguistics Corpus CDTB (Zhou and Xue, 2015) CUHK (Zhou et al., 2014) HIT-CTDB (Zhang et al., 2013) NTU (Huang and Chen, 2011a) TED-CDB (ours) Domain News report News report Internet news Sino and travel set TED Talks Total Relations 5,534 21,505 3,081 15,540 Availability Through LDC From owner From owner From owner Freely public available Table 1: Comparison of our corpus to related data sets. “-” means the work do not mention the number. systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora (Section 6). 2 Related Work Most annotations in PDTB style are conducted on written texts originating from news reports. Bef"
2020.emnlp-main.223,zhou-etal-2014-cuhk,0,0.0168606,"iscourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discourse corpus for Chinese, which is further expanded to contain 164 documents, namely the Treebank (CDTB) (CDTB)(Zhou and Xue, 2015). Huang and Chen (2011b) construct a Chinese discourse corpus with 81 articles. They adopt the top-level senses from PDTB sense hierarchy and focus on the annotation of inter-sentential relations. Zhou et al. (2014) present the CUHK Discourse Treebank. They adapt the annotation scheme of Penn Discourse Treebank 2 (PDTB-2) to Chinese and re-annotate the documents of the Chinese Treebank and with only intersentence explicit discourse relations. The largest Chinese discourse relation corpus for written texts is HIT-CDTB (Zhang et al., 2013), which presents a new Chinese discourse relation hierarchy adapted from the PDTB system. Nevertheless, these four corpora can only be acquired by either purchasing or applying from the owners. Therefore, the scarcity of Chinese datasets, especially the lack of corpora fo"
2020.emnlp-main.223,P12-1008,0,0.0314872,"orpus for spoken discourse (Tonelli et al., 2010), where PDTB annotations are constructed on Italian dialogues. Recently, researchers have realized that the PDTB annotation guidelines should be used more widely instead of just being confined to construct corpora of written texts. Zeyrek et al. (2018) annotate 6 TED talks for 7 languages. Scheffler et al. (2019) build a discourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discourse corpus for Chinese, which is further expanded to contain 164 documents, namely the Treebank (CDTB) (CDTB)(Zhou and Xue, 2015). Huang and Chen (2011b) construct a Chinese discourse corpus with 81 articles. They adopt the top-level senses from PDTB sense hierarchy and focus on the annotation of inter-sentential relations. Zhou et al. (2014) present the CUHK Discourse Treebank. They adapt the annotation scheme of Penn Discourse Treebank 2 (PDTB-2) to Chinese and re-annotate the documents of the Chinese Treebank and with only intersentence explicit discourse relatio"
2020.emnlp-main.223,N06-1055,0,0.0721642,"nk (Xue and Palmer, 2003). There is a verb “去” ，which is translated to “to” in this English translation. While the verb “去” is a poly semantic word, and it often refers to “go” in English, it tends to act as a structural auxiliary word in this example. There are several Chinese verbs that have the same function like “来” ，“让”， “用”. They always signal senses of relation like Condition, Purpose, Result and Manner. Noun phrases can serve as arguments Noun phrases have been annotated as arguments previously in Chinese discourse corpora like the CDTB (Zhou and Xue, 2015). While the Chinese NomBank (Xue, 2006) annotates the nominalized predicate, and also the Chinese Proposition Bank (Xue and Palmer, 2009) performs similar annotation of nominalized verbs. Accordingly, we do not annotate all noun phrases as arguments but those nouns which are nominalizations of their verbal form. Chinese verbs and their nominalizations share the same form, but we identify this kind of arguments, depending on whether the structure NP + 的 (of) + nominalizations of predicate appears. Moreover, in this structure, the NP can always be regarded as the object or subject of the nominalized predicate for the argument. 他 自 由"
2020.emnlp-main.223,W03-1707,0,0.180596,"iscourse relations can be expressed through a combination of the adverbial of Arg2 and the anaphoric reference to Arg1 as the implicit subject. In terms of Chinese PropBank Annotation, “使得我们比赛输了（made us lose the game)” is the ARGM-ADV, and there is a relation expressing Cause.result between between the two clauses. (4) [我到柏林]1 toPURPOSE [去参加一个16天的德语强 化]2 。 “[I went to Berlin]1 [toPURPOSE attend a 16 days’ German intensive course]2 .” In the Example (3),“参加一个16天的德语强化 （attend a 16 days’ German intensive course)” is the purpose and has been labelled as an ARGMPRP adjunct in the Chinese PropBank (Xue and Palmer, 2003). There is a verb “去” ，which is translated to “to” in this English translation. While the verb “去” is a poly semantic word, and it often refers to “go” in English, it tends to act as a structural auxiliary word in this example. There are several Chinese verbs that have the same function like “来” ，“让”， “用”. They always signal senses of relation like Condition, Purpose, Result and Manner. Noun phrases can serve as arguments Noun phrases have been annotated as arguments previously in Chinese discourse corpora like the CDTB (Zhou and Xue, 2015). While the Chinese NomBank (Xue, 2006) annotates the"
2020.emnlp-main.67,D18-1547,0,0.0360198,"Missing"
2020.emnlp-main.67,N19-1423,0,0.00664077,"rich semantic annotations of RiSAWOZ make it a suitable testbed for various benchmark tasks. We conduct five different evaluation tasks with the benchmark models and in-depth analyses on RiSAWOZ in this section. We also discuss the applicability of RiSAWOZ for other tasks. Results of the 5 tasks are reported in Table 7. 5.1 Natural Language Understanding Task Definition: In task-oriented dialogue system, the NLU module aims to convert the user utterance into the representation that computer can understand, which includes intent and dialogue act (slot & value) detection. Model: We adapt BERT (Devlin et al., 2019) for the NLU task (intent detection and slot filling). We initialize BERT with the Chinese pre-trained BERT model (Cui et al., 2019) and then finetune it on RiSAWOZ. To take dialogue history into account, we employ the same BERT to model previous dialogue context. We also experiment on the situation without context. For fine-tuning BERT on RiSAWOZ, we set the learning rate to 0.00003 and the dropout rate to 0.1. Results: From Table 7, we can clearly find that the model using dialogue context preforms better than not. Also, the model obtains lower F1 scores 936 on multi-domain dialogues than si"
2020.emnlp-main.67,W17-5526,0,0.0484744,"Missing"
2020.emnlp-main.67,W17-5506,0,0.0851935,"(Gao et al., 2018). These datasets have triggered extensive research in either end-to-end or traditional modular taskoriented dialogue modeling (Wen et al., 2019; Dai et al., 2020). Despite of substantial progress made based on these newly built corpora, more efforts in creating challenging datasets in terms of size, multiple domains, semantic annotations, multilinguality, etc., are still in demand (Wen et al., 2019). Among the existing datasets, the majority of them are not large in size, e.g., ATIS (Hemphill et al., 1990), WOZ 2.0 (Wen et al., 2017), FRAMES (El Asri et al., 2017) and KVRET (Eric et al., 2017), which might not well support datahungry neural dialogue models. Very large taskoriented dialogue datasets can be created in a machine-to-machine fashion, such as M2M (Shah et al., 2018) and SGD (Rastogi et al., 2019a). Datasets collected in this way need to simulate both user and system and contain unnatural conversations. MultiWOZ (Budzianowski et al., 2018), probably the most promising and notable dialogue corpus collected in a Wizard-of-Oz (i.e., Human-toHuman) way recently, is one order of magnitude larger than the aforementioned corpora collected in the same way. However, it contains no"
2020.emnlp-main.67,P18-5002,0,0.0590739,"Missing"
2020.emnlp-main.67,L18-1550,0,0.0265051,"Missing"
2020.emnlp-main.67,H90-1021,0,0.245006,"Missing"
2020.emnlp-main.67,W14-4337,0,0.0271861,"Missing"
2020.emnlp-main.67,D17-1018,0,0.031468,"Missing"
2020.emnlp-main.67,P02-1040,0,0.106766,"Missing"
2020.emnlp-main.67,W12-4501,0,0.287354,"dy of ellipsis and coreference in dialogue, we also provide two kinds of linguistic annotations collected in two different ways. Annotations for unified ellipsis/coreference resolution via utterance rewriting are more comprehensive and at least one order of magnitude larger than existing datasets with such annotations (Quan et al., 2019; Su et al., 2019; Zhang et al., 2019a; Rastogi et al., 2019b). Coreference clusters in each dialogue are also manually annotated, providing a new large-scale coreference dataset on dialogue, which is complementary to previous coreference datasets on documents (Pradhan et al., 2012). In a nutshell, RiSAWOZ integrates human-to-human conversations, dialogue annotations and linguistic annotations on ellipsis/coreference into a single unified dataset. • We use RiSAWOZ as a new benchmark testbed and report benchmark results on 5 tasks for future comparison study and tracking progress on this dataset. The 5 tasks are NLU, DST, Dialogue Context-to-Text Generation, Coreference Resolution and Unified Generative Ellipsis and Coreference Resolution. We discuss the usability of the dataset for other tasks, e.g., Dialogue Policy Learning, Natural Language Generation, User Simulator,"
2020.emnlp-main.67,2020.acl-main.637,1,0.869284,"Missing"
2020.emnlp-main.67,D19-1462,1,0.936244,"(e.g., MultiWOZ or CrossWOZ). Figure 1 shows a dialogue example that demonstrates semantic annotations in RiSAWOZ. User goal description, domain label, dialogue states and dialogue acts at both user and system side are annotated for each dialogue. In order to facilitate the study of ellipsis and coreference in dialogue, we also provide two kinds of linguistic annotations collected in two different ways. Annotations for unified ellipsis/coreference resolution via utterance rewriting are more comprehensive and at least one order of magnitude larger than existing datasets with such annotations (Quan et al., 2019; Su et al., 2019; Zhang et al., 2019a; Rastogi et al., 2019b). Coreference clusters in each dialogue are also manually annotated, providing a new large-scale coreference dataset on dialogue, which is complementary to previous coreference datasets on documents (Pradhan et al., 2012). In a nutshell, RiSAWOZ integrates human-to-human conversations, dialogue annotations and linguistic annotations on ellipsis/coreference into a single unified dataset. • We use RiSAWOZ as a new benchmark testbed and report benchmark results on 5 tasks for future comparison study and tracking progress on this datase"
2020.emnlp-main.67,N19-2013,0,0.229919,"ade based on these newly built corpora, more efforts in creating challenging datasets in terms of size, multiple domains, semantic annotations, multilinguality, etc., are still in demand (Wen et al., 2019). Among the existing datasets, the majority of them are not large in size, e.g., ATIS (Hemphill et al., 1990), WOZ 2.0 (Wen et al., 2017), FRAMES (El Asri et al., 2017) and KVRET (Eric et al., 2017), which might not well support datahungry neural dialogue models. Very large taskoriented dialogue datasets can be created in a machine-to-machine fashion, such as M2M (Shah et al., 2018) and SGD (Rastogi et al., 2019a). Datasets collected in this way need to simulate both user and system and contain unnatural conversations. MultiWOZ (Budzianowski et al., 2018), probably the most promising and notable dialogue corpus collected in a Wizard-of-Oz (i.e., Human-toHuman) way recently, is one order of magnitude larger than the aforementioned corpora collected in the same way. However, it contains noisy systemside state annotations and lacks user-side dialogue acts2 (Eric et al., 2019; Zhu et al., 2020). Yet another very recent dataset CrossWOZ (Zhu et al., 2020), the first large-scale Chinese H2H dataset for tas"
2020.emnlp-main.67,P19-1003,0,0.257264,"CrossWOZ). Figure 1 shows a dialogue example that demonstrates semantic annotations in RiSAWOZ. User goal description, domain label, dialogue states and dialogue acts at both user and system side are annotated for each dialogue. In order to facilitate the study of ellipsis and coreference in dialogue, we also provide two kinds of linguistic annotations collected in two different ways. Annotations for unified ellipsis/coreference resolution via utterance rewriting are more comprehensive and at least one order of magnitude larger than existing datasets with such annotations (Quan et al., 2019; Su et al., 2019; Zhang et al., 2019a; Rastogi et al., 2019b). Coreference clusters in each dialogue are also manually annotated, providing a new large-scale coreference dataset on dialogue, which is complementary to previous coreference datasets on documents (Pradhan et al., 2012). In a nutshell, RiSAWOZ integrates human-to-human conversations, dialogue annotations and linguistic annotations on ellipsis/coreference into a single unified dataset. • We use RiSAWOZ as a new benchmark testbed and report benchmark results on 5 tasks for future comparison study and tracking progress on this dataset. The 5 tasks ar"
2020.emnlp-main.67,D16-1233,0,0.0327264,"Missing"
2020.emnlp-main.67,D19-2003,0,0.0462811,"Missing"
2020.emnlp-main.67,E17-1042,0,0.0362963,"Missing"
2020.emnlp-main.67,W13-4065,0,0.0150984,"anguage utterances via paraphrasing with predefined rules or crowdsourced workers (Shah et al., 2018; Rastogi et al., 2019a). Despite of less human effort required in this approach, the diversity and complexity of created dialogues greatly depend on the quality of user and system simulators. It’s also difficult to avoid mismatch between machine-created dialogues and real human conversations. Human-to-Machine In this method, humans converse with an existing dialogue system to collect dialogue data. The Dialogue State Tracking Challenges (DSTC) has provided several datasets created in this way (Williams et al., 2013; Henderson et al., 2014a,b). Generally, the quality of human-to-machine data heavily relies on the performance of the given dialogue system. Human-to-Human To collect data of this type, crowdsourced workers talk to each other according to given dialogue goals to create diverse and natural dialogues. ATIS (Hemphill et al., 1990), WOZ 2.0 (Wen et al., 2017), FRAMES (El Asri et al., 2017) and KVRET (Eric et al., 2017) are small-scale datasets built in this way. In contrast, MultiWOZ Budzianowski et al. (2018) and CrossWOZ (Zhu et al., 2020) are two large-scale H2H datasets proposed recently. Cor"
2020.emnlp-main.67,P19-1078,0,0.0264294,"Missing"
2020.emnlp-main.67,2020.tacl-1.19,0,0.0612796,"ported, including natural language understanding (intent detection & slot filling), dialogue state tracking and dialogue contextto-text generation, as well as coreference and ellipsis resolution, which facilitate the baseline comparison for future research on this corpus.1 1 Introduction In recent years, we have witnessed that a variety of datasets tailored for task-oriented dialogue have been constructed, such as MultiWOZ (Budzianowski et al., 2018), SGD (Rastogi et al., ∗ Equal Contributions. The corpus is publicly available at https://github. com/terryqj0107/RiSAWOZ. 1 2019a) and CrossWOZ (Zhu et al., 2020), along with the increasing interest in conversational AI in both academia and industry (Gao et al., 2018). These datasets have triggered extensive research in either end-to-end or traditional modular taskoriented dialogue modeling (Wen et al., 2019; Dai et al., 2020). Despite of substantial progress made based on these newly built corpora, more efforts in creating challenging datasets in terms of size, multiple domains, semantic annotations, multilinguality, etc., are still in demand (Wen et al., 2019). Among the existing datasets, the majority of them are not large in size, e.g., ATIS (Hemph"
2020.findings-emnlp.327,N18-1118,0,0.0227514,"le (3) in Figure 1). The former can be correctly interpreted by resorting to commonsense knowledge while the latter cannot be interpreted uniquely if no more context is given. The third rule that we conform to is to 1) create two contrastive source sentences for each lexical or syntactic ambiguity point, where each source sentence corresponds to one reasonable interpretation of the ambiguity point, and 2) to provide two contrastive translations for each created source sentence. This is similar to other linguistic evaluation by contrastive examples in the MT literature (Avramidis et al., 2019; Bawden et al., 2018; M¨uller et al., 2018; Sennrich, 2017). These two contrastive translations have similar wordings: one is correct and the other is not correct in that it translates the ambiguity part into the corresponding translation of the contrastive source sentence. This translation makes sense in the contrastive sentence but not in the sentence in question. Examples of contrastive source sentences and contrastive translations for each source sentence are shown in Figure 2, 3 and 4. 3664 z1 维修 桌子 的 桌脚 。 z1 主力 部队 已经 对 敌人的 建筑 展开 了 攻关 。 1 er1 Repair the legs of the table. The main force has already launched"
2020.findings-emnlp.327,D19-1255,0,0.0264059,"them on monolingual data, we provide a bilingual commonsense test suite for machine translation. Commonsense Reasoning in NLP In addition to common sense datasets, we have also witnessed that commonsense knowledge has been recently explored in different NLP tasks. Just to name a few, Trinh and Le (2018), He et al. (2019) and Klein and Nabi (2019) use language models trained on huge text corpora to do inference on the WSC dataset. Ding et al. (2019) use commonsense knowledge in Atomic (Sap et al., 2019a) and Event2mind (Rashkin et al., 2018) on downstream tasks such as script event prediction. Bi et al. (2019) exploit external commonsense knowledge from ConceptNet (Speer et al., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been performed to examine the commonsense reasoning capability in pre-trained neural language models. Wang et al. (2019) and Zhou et al. (2020) propose to measure the success rate of the pretrained language models in commonsense inference by calculating LM probabilities. Two sentences which are used"
2020.findings-emnlp.327,W19-5351,0,0.0129188,"ntextual SA (e.g., Example (3) in Figure 1). The former can be correctly interpreted by resorting to commonsense knowledge while the latter cannot be interpreted uniquely if no more context is given. The third rule that we conform to is to 1) create two contrastive source sentences for each lexical or syntactic ambiguity point, where each source sentence corresponds to one reasonable interpretation of the ambiguity point, and 2) to provide two contrastive translations for each created source sentence. This is similar to other linguistic evaluation by contrastive examples in the MT literature (Avramidis et al., 2019; Bawden et al., 2018; M¨uller et al., 2018; Sennrich, 2017). These two contrastive translations have similar wordings: one is correct and the other is not correct in that it translates the ambiguity part into the corresponding translation of the contrastive source sentence. This translation makes sense in the contrastive sentence but not in the sentence in question. Examples of contrastive source sentences and contrastive translations for each source sentence are shown in Figure 2, 3 and 4. 3664 z1 维修 桌子 的 桌脚 。 z1 主力 部队 已经 对 敌人的 建筑 展开 了 攻关 。 1 er1 Repair the legs of the table. The main force"
2020.findings-emnlp.327,N19-1423,0,0.10551,"ge has been recently explored in different NLP tasks. Just to name a few, Trinh and Le (2018), He et al. (2019) and Klein and Nabi (2019) use language models trained on huge text corpora to do inference on the WSC dataset. Ding et al. (2019) use commonsense knowledge in Atomic (Sap et al., 2019a) and Event2mind (Rashkin et al., 2018) on downstream tasks such as script event prediction. Bi et al. (2019) exploit external commonsense knowledge from ConceptNet (Speer et al., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been performed to examine the commonsense reasoning capability in pre-trained neural language models. Wang et al. (2019) and Zhou et al. (2020) propose to measure the success rate of the pretrained language models in commonsense inference by calculating LM probabilities. Two sentences which are used to test commonsense inference differ only in commonsense concepts. Feldman et al. (2019) further explore unsupervised methods to generate commonsense knowledge using the world knowledge of pre-trained language model"
2020.findings-emnlp.327,D19-1495,0,0.0124263,"llenge (WSC) test set (Levesque et al., 2012; Sakaguchi et al., 2020) focus on solving the commonsense problems in the form of coreference resolution. Different from them on monolingual data, we provide a bilingual commonsense test suite for machine translation. Commonsense Reasoning in NLP In addition to common sense datasets, we have also witnessed that commonsense knowledge has been recently explored in different NLP tasks. Just to name a few, Trinh and Le (2018), He et al. (2019) and Klein and Nabi (2019) use language models trained on huge text corpora to do inference on the WSC dataset. Ding et al. (2019) use commonsense knowledge in Atomic (Sap et al., 2019a) and Event2mind (Rashkin et al., 2018) on downstream tasks such as script event prediction. Bi et al. (2019) exploit external commonsense knowledge from ConceptNet (Speer et al., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been performed to examine the commonsense reasoning capability in pre-trained neural language models. Wang et al. (2019) and Zhou et al."
2020.findings-emnlp.327,L16-1002,0,0.0264202,"anslation (Nirenburg, 1989). Large ontology that is constructed either manually or automatically to provide world knowledge is one of essential components in KBMT (Knight and Luk, 1994). As data-driven machine translation, such as statistical machine translation (SMT) and neural machine translation, becomes de facto standard in machine translation, world knowledge has been less explicitly explored. Only a few studies have indirectly and partially exploited world knowledge in SMT or NMT, by incorporating linked open data resources such as DBpedia and BabelNet into SMT with modest improvements (Du et al., 2016; Srivastava et al., 2017; Moussallem et al., 2018). 3 Commonsense Reasoning Test Suite for Machine Translation In this section, we discuss the design and construction of the test suite, including the rules and steps for building this test suite. 3.1 Test Suite Design Different from commonsense reasoning in Winogram Schema Challenge (Levesque et al., 2012) or sentence reasonability judgment (i.e., “He put a turkey into the fridge” vs. “He put an elephant into the fridge”) (Wang et al., 2019), where commonsense reasoning normally happens in one language, commonsense reasoning in NMT can be done"
2020.findings-emnlp.327,D19-1109,0,0.0274275,"l., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been performed to examine the commonsense reasoning capability in pre-trained neural language models. Wang et al. (2019) and Zhou et al. (2020) propose to measure the success rate of the pretrained language models in commonsense inference by calculating LM probabilities. Two sentences which are used to test commonsense inference differ only in commonsense concepts. Feldman et al. (2019) further explore unsupervised methods to generate commonsense knowledge using the world knowledge of pre-trained language models. Our commonsense reasoning evaluation resonates with these evaluation efforts. Commonsense Knowledge and Reasoning in Machine Translation Commonsense knowledge has long been acknowledged as an indispensable knowledge source for disambiguation in machine translation (Bar-Hillel, 1960b; Davis and Marcus, 2015). Knowledgebased machine translation (KBMT), one of the popular machine translation paradigms in 1980s, lays much stress on extra-linguistic world knowledge in ma"
2020.findings-emnlp.327,D19-6002,0,0.0199909,"ct cloze tests. Bhagavatula et al. (2019) propose a dataset for abductive reasoning on events. The wellknown Winograd Schema Challenge (WSC) test set (Levesque et al., 2012; Sakaguchi et al., 2020) focus on solving the commonsense problems in the form of coreference resolution. Different from them on monolingual data, we provide a bilingual commonsense test suite for machine translation. Commonsense Reasoning in NLP In addition to common sense datasets, we have also witnessed that commonsense knowledge has been recently explored in different NLP tasks. Just to name a few, Trinh and Le (2018), He et al. (2019) and Klein and Nabi (2019) use language models trained on huge text corpora to do inference on the WSC dataset. Ding et al. (2019) use commonsense knowledge in Atomic (Sap et al., 2019a) and Event2mind (Rashkin et al., 2018) on downstream tasks such as script event prediction. Bi et al. (2019) exploit external commonsense knowledge from ConceptNet (Speer et al., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been pe"
2020.findings-emnlp.327,D19-1243,0,0.0366772,"Missing"
2020.findings-emnlp.327,P19-1477,0,0.0229173,"vatula et al. (2019) propose a dataset for abductive reasoning on events. The wellknown Winograd Schema Challenge (WSC) test set (Levesque et al., 2012; Sakaguchi et al., 2020) focus on solving the commonsense problems in the form of coreference resolution. Different from them on monolingual data, we provide a bilingual commonsense test suite for machine translation. Commonsense Reasoning in NLP In addition to common sense datasets, we have also witnessed that commonsense knowledge has been recently explored in different NLP tasks. Just to name a few, Trinh and Le (2018), He et al. (2019) and Klein and Nabi (2019) use language models trained on huge text corpora to do inference on the WSC dataset. Ding et al. (2019) use commonsense knowledge in Atomic (Sap et al., 2019a) and Event2mind (Rashkin et al., 2018) on downstream tasks such as script event prediction. Bi et al. (2019) exploit external commonsense knowledge from ConceptNet (Speer et al., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been performed to examine the com"
2020.findings-emnlp.327,W18-6307,0,0.0383687,"Missing"
2020.findings-emnlp.327,2001.mtsummit-papers.68,0,0.0627087,"and 512-dimension hidden states. We used Adam (Kingma and Ba, 2015) to train both NMT models. β1 and β2 of Adam were set to 0.9 and 0.999, the learning rate was set to 0.0005, and gradient norm was set to 5. To take full advantage of GPUs, we batched sentences of similar lengths. We trained both models on a single machine with 8 1080Ti cards. Each mini-batch contained 32,000 tokens. During decoding, we employed the beam search algorithm and set the beam size to 5. 5.2 Evaluation Metrics For translation performance evaluation, we used sacrebleu (Post, 2018) to calculate case-sensitive BLEU-4 (Papineni et al., 2001). To evaluate the commonsense reasoning accuracy of NMT on the test suite, we applied NMT models to score each pair (s, t) as follows: |t| Score(t|s) = 1 X logp(ti |t<i , s) |t| (1) i=0 In this section, we conducted extensive experiments to evaluate the commonsense reasoning capability of state-of-the-art neural machine translation on the built test suite. where p(ti |t<i , s) is the probabilty of the target word ti given the target history and source sentence. Given a triple (z, er , ec ), if an NMT model scores the reference translation higher than the contrastive translation (i.e., Score(er"
2020.findings-emnlp.327,W18-6319,0,0.0126514,"ployed neural architecture with 4 layers of LSTM and 512-dimension hidden states. We used Adam (Kingma and Ba, 2015) to train both NMT models. β1 and β2 of Adam were set to 0.9 and 0.999, the learning rate was set to 0.0005, and gradient norm was set to 5. To take full advantage of GPUs, we batched sentences of similar lengths. We trained both models on a single machine with 8 1080Ti cards. Each mini-batch contained 32,000 tokens. During decoding, we employed the beam search algorithm and set the beam size to 5. 5.2 Evaluation Metrics For translation performance evaluation, we used sacrebleu (Post, 2018) to calculate case-sensitive BLEU-4 (Papineni et al., 2001). To evaluate the commonsense reasoning accuracy of NMT on the test suite, we applied NMT models to score each pair (s, t) as follows: |t| Score(t|s) = 1 X logp(ti |t<i , s) |t| (1) i=0 In this section, we conducted extensive experiments to evaluate the commonsense reasoning capability of state-of-the-art neural machine translation on the built test suite. where p(ti |t<i , s) is the probabilty of the target word ti given the target history and source sentence. Given a triple (z, er , ec ), if an NMT model scores the reference translat"
2020.findings-emnlp.327,P18-1043,0,0.148031,". We refer readers to Storks et al. (2019)’s article for a thorough survey in this area. Commonsense Datasets According to Gunning (2018), commonsense knowledge normally consists of a general theory of how the physical world works and a basic understanding of human motives and behaviors. In recent years, a wide variety of datasets on the two kinds of commonsense knowledge have been proposed. Sap et al. (2019b) introduce Social IQA, containing 38k multiple choice questions for probing the commonsense reasoning about emotional and social in people’s daily life. Similarly, Event2mind and Atomic (Rashkin et al., 2018; Sap et al., 2019a) focus on inferred knowledge in the form of if-then to reason about people’s daily life behavior. For datasets on physical common sense, PIQA (Bisk et al., 2020) on commonsense phenomena in the physical world contains 21K QA pairs. SWAG and HellaSwag (Zellers et al., 2018, 2019) are datasets on commonsense NLI, where materials from video subtitles and wikihow articles are used to construct cloze tests. Bhagavatula et al. (2019) propose a dataset for abductive reasoning on events. The wellknown Winograd Schema Challenge (WSC) test set (Levesque et al., 2012; Sakaguchi et al."
2020.findings-emnlp.327,D19-1454,0,0.0302441,"Missing"
2020.findings-emnlp.327,W09-1701,0,0.112068,"can be done either in the encoding of the source language (i.e., encoding reasonable source representations) or in the decoding of the target language (i.e., producing reasonable target outputs). As it is difficult to detect whether reasonable senses are identified and encoded in the encoder, we check target outputs from the decoder to test the commonsense reasoning capability of NMT. This is the first rule that we follow to design the test suite. In the second rule for building the test suite, we manually create source sentences with ambiguity that requires commonsense reasoning. Inspired by Schwartz and Gomez (2009) and Ovchinnikova (2012), we ground the commonsense reasoning test on two types of ambiguity: lexical and syntactic ambiguity (LA and SA), which are common in machine translation. An example in LA is the “batter” in “she put the batter in the refrigerator” (food material vs. baseball player). SA relates to structures, for instance, “I saw a man swimming on the bridge” (I was standing on the bridge vs. The man was swimming on the bridge). We further refine SA into contextless (e.g., Example (2) in Figure 1) and contextual SA (e.g., Example (3) in Figure 1). The former can be correctly interpret"
2020.findings-emnlp.327,E17-2060,0,0.0231249,"ectly interpreted by resorting to commonsense knowledge while the latter cannot be interpreted uniquely if no more context is given. The third rule that we conform to is to 1) create two contrastive source sentences for each lexical or syntactic ambiguity point, where each source sentence corresponds to one reasonable interpretation of the ambiguity point, and 2) to provide two contrastive translations for each created source sentence. This is similar to other linguistic evaluation by contrastive examples in the MT literature (Avramidis et al., 2019; Bawden et al., 2018; M¨uller et al., 2018; Sennrich, 2017). These two contrastive translations have similar wordings: one is correct and the other is not correct in that it translates the ambiguity part into the corresponding translation of the contrastive source sentence. This translation makes sense in the contrastive sentence but not in the sentence in question. Examples of contrastive source sentences and contrastive translations for each source sentence are shown in Figure 2, 3 and 4. 3664 z1 维修 桌子 的 桌脚 。 z1 主力 部队 已经 对 敌人的 建筑 展开 了 攻关 。 1 er1 Repair the legs of the table. The main force has already launched an attack on the enemy’s building. 1 1"
2020.findings-emnlp.327,P16-1162,0,0.0552699,"MT model is believed to make a correct commonsense reasoning prediction. This is reasonable as er and ec are only different in words or structures related to the lexical or syntactical commonsense ambiguity point as described in Section 3.1. By scoring each triple with an NMT model, we can measure the commonsense reasoning accuracy of the model on our test suite. 5.1 5.3 5 Experiments Experimental setup corpus3 We adopted the CWMT Chinese-English of news domain as training data for NMT systems. This corpus contains 9M parallel sentences. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k. We trained two neural machine translation models on the training data: RNNSearch (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017). 3 Results BLEU scores for the two NMT models are given in Table 4. Commonsense reasoning results on the test suite are provided in Table 5. From the table and figure, we can observe that Available at: http://nlp.nju.edu.cn/cwmt-wmt 3667 • Both BLEU and commonsense reasoning accuracy clearly show that Transformer is better than RNNSearch. • Both RNNSearch and Transformer pe"
2020.findings-emnlp.327,D18-1009,0,0.0250886,"recent years, a wide variety of datasets on the two kinds of commonsense knowledge have been proposed. Sap et al. (2019b) introduce Social IQA, containing 38k multiple choice questions for probing the commonsense reasoning about emotional and social in people’s daily life. Similarly, Event2mind and Atomic (Rashkin et al., 2018; Sap et al., 2019a) focus on inferred knowledge in the form of if-then to reason about people’s daily life behavior. For datasets on physical common sense, PIQA (Bisk et al., 2020) on commonsense phenomena in the physical world contains 21K QA pairs. SWAG and HellaSwag (Zellers et al., 2018, 2019) are datasets on commonsense NLI, where materials from video subtitles and wikihow articles are used to construct cloze tests. Bhagavatula et al. (2019) propose a dataset for abductive reasoning on events. The wellknown Winograd Schema Challenge (WSC) test set (Levesque et al., 2012; Sakaguchi et al., 2020) focus on solving the commonsense problems in the form of coreference resolution. Different from them on monolingual data, we provide a bilingual commonsense test suite for machine translation. Commonsense Reasoning in NLP In addition to common sense datasets, we have also witnessed t"
2020.findings-emnlp.327,P19-1472,0,0.0370488,"Missing"
2020.findings-emnlp.327,P19-1393,0,0.248639,"the WSC dataset. Ding et al. (2019) use commonsense knowledge in Atomic (Sap et al., 2019a) and Event2mind (Rashkin et al., 2018) on downstream tasks such as script event prediction. Bi et al. (2019) exploit external commonsense knowledge from ConceptNet (Speer et al., 2016)) in machine reading comprehension. 3663 Commonsense Reasoning Evaluation With pre-trained language models, like BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) being widely used in various NLP tasks, studies have been performed to examine the commonsense reasoning capability in pre-trained neural language models. Wang et al. (2019) and Zhou et al. (2020) propose to measure the success rate of the pretrained language models in commonsense inference by calculating LM probabilities. Two sentences which are used to test commonsense inference differ only in commonsense concepts. Feldman et al. (2019) further explore unsupervised methods to generate commonsense knowledge using the world knowledge of pre-trained language models. Our commonsense reasoning evaluation resonates with these evaluation efforts. Commonsense Knowledge and Reasoning in Machine Translation Commonsense knowledge has long been acknowledged as an indispens"
2020.iwdp-1.3,N18-1118,0,0.0342923,"er fragments in a local window for translation quality assessment, while cross-sentence discourse links are usually neglected. Hence, for document-level models, current automatic evaluation metrics may be not a reasonably good fit for evaluation. One 2 Related Work Research on the evaluation of document-level machine translation is usually on specific discourse phenomena. A few test suites and methods have been designed for evaluating NMT from the perspective of discourse phenomena. For pronoun translation evaluation, recent test sets on pronoun evaluation have consisted of contrastive pairs. Bawden et al. (2018) provide 50 example blocks of English-French contrastive pairs. M¨uller et al. (2018) have also created contrastive pairs of pronoun “it” in English-German translation. 13 Proceedings of the 2nd International Workshop on Discourse Processing, pages 13–17 c Suzhou, China, December 7, 2020. 2020 Association for Computational Linguistics source: context: You rich guys think that money can buy anything. current: How right you are. target: context: 你们富人总以为钱能买到一切。 current: 你们想的太对了。 Contrastive test sets allow us to automatically evaluate document-level NMT by only judging whether the evaluated model"
2020.iwdp-1.3,W19-5355,0,0.0439568,"Missing"
2020.iwdp-1.3,W18-6435,0,0.179805,"n a local window for translation quality assessment, while cross-sentence discourse links are usually neglected. Hence, for document-level models, current automatic evaluation metrics may be not a reasonably good fit for evaluation. One 2 Related Work Research on the evaluation of document-level machine translation is usually on specific discourse phenomena. A few test suites and methods have been designed for evaluating NMT from the perspective of discourse phenomena. For pronoun translation evaluation, recent test sets on pronoun evaluation have consisted of contrastive pairs. Bawden et al. (2018) provide 50 example blocks of English-French contrastive pairs. M¨uller et al. (2018) have also created contrastive pairs of pronoun “it” in English-German translation. 13 Proceedings of the 2nd International Workshop on Discourse Processing, pages 13–17 c Suzhou, China, December 7, 2020. 2020 Association for Computational Linguistics source: context: You rich guys think that money can buy anything. current: How right you are. target: context: 你们富人总以为钱能买到一切。 current: 你们想的太对了。 Contrastive test sets allow us to automatically evaluate document-level NMT by only judging whether the evaluated model"
2020.iwdp-1.3,W15-2522,0,0.0557966,"Missing"
2020.iwdp-1.3,L16-1147,0,0.274667,"(Guillou et al., 2018; Rysov´a et al., 2019; Vojtˇechov´a et al., 2019; Voita et al., 2019; Popovi´c, 2019) have been constructed for several language pairs, such as EnglishCzech, English-German, English-Russian, FrenchGerman, but few in English-Chinese translation. In this paper, we propose a test suite aiming at English-Chinese discourse phenomena evaluation. Three frequent discourse phenomena in EnglishChinese translation are selected in our test suite, namely pronoun, discourse connective and ellipsis, each of which forms an individual test set. We choose examples from the OpenSubtitles (Lison and Tiedemann, 2016) to construct the three test sets. Unlike corpora from news domain, this corpus is more conversational and colloquial. We use this test suite to evaluate several typical context-aware NMT models. The experiment results show that our test suite can evaluate the ability of NMT models in dealing with discourse phenomena and that it is still very challenging for current context-aware models to capture different discourse phenomena. The need to evaluate the ability of contextaware neural machine translation (NMT) models in dealing with specific discourse phenomena arises in document-level NMT. Howe"
2020.iwdp-1.3,D18-1049,0,0.0460975,"t: Everything is so difficult in life, for me. current: While for others it’s all child’s play. target: context: 对于我，生活一切都很艰难。 current: 对于别人却都像儿戏一样。 tion should be present in the target language. 4 We used the proposed test suite as a benchmark test bed to evaluate state-of-the-art context-aware NMT models against the three types of discourse phenomena. Figure 2: An example from the discourse connective test set. source: context: You see, she doesn’t know. current: Neither do I. target: context: 看，她不知道。 current: 我也不知道。 4.1 Models We used the following three document-level NMT models: • thumt: Zhang et al. (2018) extend the Transformer model with a new context encoder to model document-level context, which is then incorporated into the original encoder and decoder. They introduce a two-step training method to explore abundant sentence-level parallel corpora and limited document-level parallel corpora. Figure 3: An example from the ellipsis test set. larly, we select five ambiguous discourse connectives according to Webber et al. (2019), namely while, as, since, though and or. Different senses of these ambiguous connectives are frequently occurring in English texts. The number of cases for each connect"
2020.iwdp-1.3,2012.amta-papers.20,0,0.025312,"think that money can buy anything. current: How right you are. target: context: 你们富人总以为钱能买到一切。 current: 你们想的太对了。 Contrastive test sets allow us to automatically evaluate document-level NMT by only judging whether the evaluated model can choose the correct translation against the wrong from each contrastive pair according to their model score. However, this is an indirect rather than a direct way to evaluate the ability of context-aware NMT in modeling discourse phenomena as we do not evaluate the actual translations generated by these NMT systems. To evaluate discourse connective translation, Meyer et al. (2012) propose ACT (accuracy of connective translation) to evaluate connective translation. For French-English discourse relation and discourse connective translation assessment, Smith and Specia (2018) use pretrained bilingual embeddings of discourse connectives. Popovi´c (2019) investigates conjunction disambiguation in EnglishGerman and French-German translation. For the evaluation on ellipsis translation, Voita et al. (2019) explore contrastive examples to evaluate the verb phrase ellipsis and morphological inflection in English-Russian translation. In our work, we also investigate verb ellipsis"
2020.iwdp-1.3,W18-6307,0,0.0247486,"Missing"
2020.iwdp-1.3,P02-1040,0,0.10645,"ent-level NMT has attracted extensive interest in recent years. Different from sentence-level NMT models, discourse-level models need to not only cope with intra-sentence dependencies, but also incorporate context beyond current sentence into context-aware translation. Inter-sentence links usually exhibit a wide variety of discourse phenomena: coreference, lexical cohesion, coherence, discourse relations, etc. The quality of a documentlevel NMT model therefore can be evaluated based on its ability in dealing with these discourse phenomena. Widely-used automatic evaluation metrics, e.g., BLEU (Papineni et al., 2002), normally consider fragments in a local window for translation quality assessment, while cross-sentence discourse links are usually neglected. Hence, for document-level models, current automatic evaluation metrics may be not a reasonably good fit for evaluation. One 2 Related Work Research on the evaluation of document-level machine translation is usually on specific discourse phenomena. A few test suites and methods have been designed for evaluating NMT from the perspective of discourse phenomena. For pronoun translation evaluation, recent test sets on pronoun evaluation have consisted of co"
2020.iwdp-1.3,W19-5353,0,0.0554963,"Missing"
2020.iwdp-1.3,W19-5352,0,0.0290143,"Missing"
2020.iwdp-1.3,P19-1116,0,0.087871,"Missing"
2020.lrec-1.129,P14-1065,0,0.175133,"Missing"
2020.lrec-1.129,I11-1170,0,0.266975,"Missing"
2020.lrec-1.129,miltsakaki-etal-2004-penn,1,0.613407,"otation before, we annotated Chinese Ted talks to help others be aware of the differences between the Chinese discourse structure of written and spoken texts and will make our corpus publicly available to benefit the discourse-level NLP researches for spoken discourses. 3. PDTB and our Annotation Scheme The annotation scheme we adopted in this work is based on the framework of PDTB, incorporating the most recent PDTB (PDTB-3) relational taxonomy and sense hierarchy (Webber et al., 2019), shown in Table 1. PDTB follows a lexically grounded approach to the representation of discourse relations (Miltsakaki et al., 2004). Discourse relations are taken to hold between two abstract object arguments, named Arg1 and Arg2 using syntactic conventions, and are triggered either by explicit connectives or, otherwise, by adjacency between clauses and sentences. As we can see from Table 1, the PDTB-3 sense hierarchy has 4 top-level senses (Expansion, Temporal, Contingency, Contrast) and second- and third-level senses for 1 CDTB uses a flat set of senses in which Conjunction and Expansion are distinct. Previously, all Chinese annotation work using PDTB style followed the settings of PDTB-2. Some researchers tried to adap"
2020.lrec-1.129,prasad-etal-2008-penn,1,0.729435,"ction 2, we review the related existing discourse annotation work. In Section 3, we briefly introduce PDTB-3 (Webber et al., 2019) and our adapted annotation scheme by examples. In Section 4, we elaborate our annotation process and the results of our inteannotator-agreement study. Finally, in Section 5, we display the results of our annotation and preliminarily analyze corpus statistics, which we compare to the relation distribution of the CUHK Discourse TreeBank for Chinese. (CUHK-DTBC)(Zhou et al., 2014). 2. Related work Following the release of the Penn Discourse Treebank (PDTB-2) in 2008 (Prasad et al., 2008), several remarkable Chinese discourse corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Chinese Discourse Treebank (Zhou and Xue, 2012), HIT Chinese Discourse Treebank (HIT-CDTB) Zhou et al. (2014), and the Discourse Treebank for Chinese (DTBC) (Zhou et al., 2014). Specifically, Xue proposed the Chinese Discourse Treebank (CDTB) Project (Xue et al., 2005). From their annotation work, they discussed the matters such as features of Chinese discourse connectives, definition and scope of arguments, and senses disambiguation, and they argued that determining the ar"
2020.lrec-1.129,C10-2118,1,0.834316,"Missing"
2020.lrec-1.129,J14-4007,1,0.831499,"-3 (Webber et al., 2019) and our adapted annotation scheme by examples. In Section 4, we elaborate our annotation process and the results of our inteannotator-agreement study. Finally, in Section 5, we display the results of our annotation and preliminarily analyze corpus statistics, which we compare to the relation distribution of the CUHK Discourse TreeBank for Chinese. (CUHK-DTBC)(Zhou et al., 2014). 2. Related work Following the release of the Penn Discourse Treebank (PDTB-2) in 2008 (Prasad et al., 2008), several remarkable Chinese discourse corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Chinese Discourse Treebank (Zhou and Xue, 2012), HIT Chinese Discourse Treebank (HIT-CDTB) Zhou et al. (2014), and the Discourse Treebank for Chinese (DTBC) (Zhou et al., 2014). Specifically, Xue proposed the Chinese Discourse Treebank (CDTB) Project (Xue et al., 2005). From their annotation work, they discussed the matters such as features of Chinese discourse connectives, definition and scope of arguments, and senses disambiguation, and they argued that determining the argument scope is the most challenging part of the annotation. To further promote their research, Zhou and X"
2020.lrec-1.129,L16-1165,0,0.0556004,"Missing"
2020.lrec-1.129,L18-1301,0,0.523318,"pe of text – the planned monologues found in TED talks, following the annotation style used in the Penn Discourse TreeBank, but adapted to take account of properties of Chinese described in Section 3. TED talks (TED is short for technology, entertainment, design), as examples of planned monologues delivered to a live audience (Greenbaum, 1996), are scrupulously translated to various languages. Although TED talks have been annotated for discourse relations in several languages Wanqiu Long and Xinyi Cai have contributed equally to this work. Corresponding author: Deyi Xiong, dyxiong@tju.edu.cn (Zeyrek et al., 2018), this is the first attempt to annotate TED talks in Chinese (either translated into Chinese, or presented in Chinese), providing data on features of Chinese spoken discourse. Our annotation by and large follows the annotation scheme in the PDTB-3, adapted to features of Chinese spoken discourse described below. The rest of the paper is organized as follows: in Section 2, we review the related existing discourse annotation work. In Section 3, we briefly introduce PDTB-3 (Webber et al., 2019) and our adapted annotation scheme by examples. In Section 4, we elaborate our annotation process and th"
2020.lrec-1.129,P12-1008,0,0.385725,"amples. In Section 4, we elaborate our annotation process and the results of our inteannotator-agreement study. Finally, in Section 5, we display the results of our annotation and preliminarily analyze corpus statistics, which we compare to the relation distribution of the CUHK Discourse TreeBank for Chinese. (CUHK-DTBC)(Zhou et al., 2014). 2. Related work Following the release of the Penn Discourse Treebank (PDTB-2) in 2008 (Prasad et al., 2008), several remarkable Chinese discourse corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Chinese Discourse Treebank (Zhou and Xue, 2012), HIT Chinese Discourse Treebank (HIT-CDTB) Zhou et al. (2014), and the Discourse Treebank for Chinese (DTBC) (Zhou et al., 2014). Specifically, Xue proposed the Chinese Discourse Treebank (CDTB) Project (Xue et al., 2005). From their annotation work, they discussed the matters such as features of Chinese discourse connectives, definition and scope of arguments, and senses disambiguation, and they argued that determining the argument scope is the most challenging part of the annotation. To further promote their research, Zhou and Xue (2012) presented a PDTB-style discourse corpus for Chinese."
2020.lrec-1.129,zhou-etal-2014-cuhk,0,0.855707,"features of Chinese spoken discourse described below. The rest of the paper is organized as follows: in Section 2, we review the related existing discourse annotation work. In Section 3, we briefly introduce PDTB-3 (Webber et al., 2019) and our adapted annotation scheme by examples. In Section 4, we elaborate our annotation process and the results of our inteannotator-agreement study. Finally, in Section 5, we display the results of our annotation and preliminarily analyze corpus statistics, which we compare to the relation distribution of the CUHK Discourse TreeBank for Chinese. (CUHK-DTBC)(Zhou et al., 2014). 2. Related work Following the release of the Penn Discourse Treebank (PDTB-2) in 2008 (Prasad et al., 2008), several remarkable Chinese discourse corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Chinese Discourse Treebank (Zhou and Xue, 2012), HIT Chinese Discourse Treebank (HIT-CDTB) Zhou et al. (2014), and the Discourse Treebank for Chinese (DTBC) (Zhou et al., 2014). Specifically, Xue proposed the Chinese Discourse Treebank (CDTB) Project (Xue et al., 2005). From their annotation work, they discussed the matters such as features of Chinese discourse conne"
2021.acl-long.23,P19-1174,0,0.0485984,"Missing"
2021.acl-long.23,D19-1139,0,0.0159935,"s the same as the others for fairness. Their training without excluding these long sentences is slower than we reported. Results are shown in Table 2. Table 2 shows that the MHPLSTM is not only the fastest in both training and decoding, but also leads to the best performance compared to baselines. Surprisingly, MHPLSTM even surpasses LN-LSTM. We conjecture potential reasons that MHPLSTM surpasses both self-attention and LNLSTM might be: 277 • The self-attention network relies on absolute positional embedding for position encoding, which has its drawbacks (Shaw et al., 2018; Wang et al., 2019; Chen et al., 2019a; Wang et al., 2020), while LSTMs seem to have natural advantages in (relative) positional encodApproach Transformer MHPLSTM - FFN BLEU dev test 24.00 24.65 24.08 Para. (M) 27.55 28.37 27.67 62.37 62.80 50.21 Speed-Up Train Decode 1.00 1.16 1.49 1.00 1.69 1.91 Table 3: The effects of decoder FFN. Hidden Gates √ √ × √ × × × √ BLEU dev test 24.65 24.71 24.23 24.36 28.37 28.38 27.92 27.97 Table 4: Using 2-layer FFN computation. ing (Chen et al., 2019b). • LSTMs lack a mechanism to directly connect distant words, which may lead to overlooking neighboring information, while the use of a bag-of-wor"
2021.acl-long.23,P18-1008,0,0.184863,"for each token, i.e. n times if the number of tokens in the sequence is n. However, the complexity of a self-attention network which compares each token with all the other tokens is O(n2 ), while for LSTM (Hochreiter and Schmidhuber, 1997) it is only O(n). In practice, however, LSTM is slower than the self-attention network in training. This is mainly due to the fact that the computation of its current step relies on the computation output of the previous step, which prevents efficient parallelization over the sequence. As for the performance of using recurrent models in machine translation, Chen et al. (2018) shows that an LSTM-based decoder can further improve the performance over the Transformer. Introduction The Transformer translation model (Vaswani et al., 2017) has achieved great success and is used extensively in the NLP community. It achieves outstanding performance compared to previous RNN/CNN In this paper, we investigate how we can efficiently parallelize all linear transformations of an LSTM at the sequence level, i.e. compute its linear transformations only once with a given input sequence. Given that linear transformations are implemented by matrix multiplication, compared to the oth"
2021.acl-long.23,D14-1179,0,0.0450971,"Missing"
2021.acl-long.23,W04-3250,0,0.0464317,"ad is 64, thus there were 8 and 16 heads for the base setting and the big setting respectively. We implemented our approaches based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. Parameters were initialized under the Lipschitz constraint (Xu et al., 2020c). We used a beam size of 4 for decoding, and evaluated tokenized case-sensitive BLEU with the averaged model of the last 5 checkpoints for the Transformer Base setting and 20 checkpoints for the Transformer Big setting saved with an interval of 1500 training steps. We also conducted significance tests (Koehn, 2004). 4.2 Main Results We first verify the performance by comparing our approach with the Transformer in both the base setting and the big setting. Results are shown in Table 1. Table 1 shows that using an LSTM-based decoder can bring significant improvements over the self-attention decoder. Specifically, using MHPLSTM improves +0.82 and +0.77 BLEU on the En-De and En-Fr task respectively using the base setting, +1.13 and +0.92 correspondingly using the big setting. The fact that using an LSTM-based decoder can improve the translation quality is consistent with Chen et al. (2018), with MHPLSTM fur"
2021.acl-long.23,W18-6301,0,0.0164371,"LSTM 13 14 15 &gt;15 MHPLSTM Figure 6: Subject-verb agreement analysis. X-axis and y-axis represent subject-verb distance in words and accuracy respectively. rent models. To accelerate RNN models, Zhang et al. (2018b) propose a heavily simplified ATR network to have the smallest number of weight matrices among units of all existing gated RNNs. Peter et al. (2016) investigate exponentially decaying bag-of-words input features for feedforward NMT models. In addition to sequencelevel parallelization, asynchronous optimization (Heigold et al., 2014) and data parallelization with a larger batch size (Ott et al., 2018; Chen et al., 2018; Xu et al., 2020a) can also accelerate training. 6 In this paper, we observe that the sequence-level parallelization issue of LSTM is due to the fact that its computation of gates and hidden states of the current step relies on the computation result of the preceding step, and linear transformations have to be propagated the same number of times as the sequence length. To improve the sequencelevel parallelization of the LSTM, we propose to remove the dependency of the current step LSTM computation on the result of the previous step by computing hidden states and gates with"
2021.acl-long.23,P16-2048,0,0.025915,"aselines in capturing dependencies of various distances with the linguistically-informed verb-subject agreement analysis on the Lingeval97 dataset (Sennrich, 279 0.99 0.98 0.97 0.96 0.95 0.94 0.93 0.92 1 2 3 4 Transformer Base 5 6 7 AAN 8 9 ATR 10 11 12 LN-LSTM 13 14 15 &gt;15 MHPLSTM Figure 6: Subject-verb agreement analysis. X-axis and y-axis represent subject-verb distance in words and accuracy respectively. rent models. To accelerate RNN models, Zhang et al. (2018b) propose a heavily simplified ATR network to have the smallest number of weight matrices among units of all existing gated RNNs. Peter et al. (2016) investigate exponentially decaying bag-of-words input features for feedforward NMT models. In addition to sequencelevel parallelization, asynchronous optimization (Heigold et al., 2014) and data parallelization with a larger batch size (Ott et al., 2018; Chen et al., 2018; Xu et al., 2020a) can also accelerate training. 6 In this paper, we observe that the sequence-level parallelization issue of LSTM is due to the fact that its computation of gates and hidden states of the current step relies on the computation result of the preceding step, and linear transformations have to be propagated the"
2021.acl-long.23,E17-2060,0,0.0592361,"Missing"
2021.acl-long.23,P16-1162,0,0.0435271,"d representation into n folds: i1 |...|in =Ws i + bs (20) (19) Next, the kth input ik is fed into the corresponding HPLSTM network HPLSTMk , and the output ok is obtained: (21) Experiments We replace the self-attention layers of the Transformer decoder with the MHPLSTM in our experiments. 4.1 Settings To compare with Vaswani et al. (2017), we conducted our experiments on the WMT 14 English to German and English to French news translation tasks. The concatenation of newstest 2012 and newstest 2013 was used for validation and newstest 2014 as test set. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32k merging operations on all data sets. We only kept sentences with a maximum of 256 subword tokens for training. Training sets were randomly shuffled in each training epoch. We followed Vaswani et al. (2017) for the experiment settings. The training steps for Transformer Base and Transformer Big were 100k and 300k respectively. We used a dropout of 0.1 for all experiments except for the Transformer Big setting on the En-De task which was 0.3. For the Transformer Base setting, the embedding dimension and the hidden dimension of the position-wise feed-forward neural network were 512 and"
2021.acl-long.23,N18-2074,0,0.0260917,"keep the batch size and training steps the same as the others for fairness. Their training without excluding these long sentences is slower than we reported. Results are shown in Table 2. Table 2 shows that the MHPLSTM is not only the fastest in both training and decoding, but also leads to the best performance compared to baselines. Surprisingly, MHPLSTM even surpasses LN-LSTM. We conjecture potential reasons that MHPLSTM surpasses both self-attention and LNLSTM might be: 277 • The self-attention network relies on absolute positional embedding for position encoding, which has its drawbacks (Shaw et al., 2018; Wang et al., 2019; Chen et al., 2019a; Wang et al., 2020), while LSTMs seem to have natural advantages in (relative) positional encodApproach Transformer MHPLSTM - FFN BLEU dev test 24.00 24.65 24.08 Para. (M) 27.55 28.37 27.67 62.37 62.80 50.21 Speed-Up Train Decode 1.00 1.16 1.49 1.00 1.69 1.91 Table 3: The effects of decoder FFN. Hidden Gates √ √ × √ × × × √ BLEU dev test 24.65 24.71 24.23 24.36 28.37 28.38 27.92 27.97 Table 4: Using 2-layer FFN computation. ing (Chen et al., 2019b). • LSTMs lack a mechanism to directly connect distant words, which may lead to overlooking neighboring info"
2021.acl-long.23,P16-1008,0,0.0238206,"Figure 5: Decoding speed on a single GTX 1080Ti GPU with respect to various input sentence length. Yaxis: number of sentences / second. Beam size: 4. context in one forward pass, while MHPLSTM has to compute 2 forward passes, one for the forward direction, another one for the reverse direction. For each direction, relevant context is processed separately in the recurrent models. 4.6 30 AAN Length Analysis To analyze the effects of MHPLSTM on performance with increasing input length, we conducted a length analysis on the news test set of the WMT 14 En-De task. Following Bahdanau et al. (2015); Tu et al. (2016); Xu et al. (2020b), we grouped sentences of similar lengths together and computed BLEU scores of the MHPLSTM and our baselines for each group. BLEU score results and decoding speed-up of each group are shown in Figure 4 and 5 respectively. Figure 4 shows that MHPLSTM surpasses the other approaches in most length groups, and improvements of using an MHPLSTM based-decoder are more significant for long sentences than short sentences. Figure 5 shows that all recurrent-based approaches are faster than the self-attention decoder in all length groups, and MHPLSTM achieves comparable decoding speed a"
2021.acl-long.23,D19-1145,0,0.0213577,"e and training steps the same as the others for fairness. Their training without excluding these long sentences is slower than we reported. Results are shown in Table 2. Table 2 shows that the MHPLSTM is not only the fastest in both training and decoding, but also leads to the best performance compared to baselines. Surprisingly, MHPLSTM even surpasses LN-LSTM. We conjecture potential reasons that MHPLSTM surpasses both self-attention and LNLSTM might be: 277 • The self-attention network relies on absolute positional embedding for position encoding, which has its drawbacks (Shaw et al., 2018; Wang et al., 2019; Chen et al., 2019a; Wang et al., 2020), while LSTMs seem to have natural advantages in (relative) positional encodApproach Transformer MHPLSTM - FFN BLEU dev test 24.00 24.65 24.08 Para. (M) 27.55 28.37 27.67 62.37 62.80 50.21 Speed-Up Train Decode 1.00 1.16 1.49 1.00 1.69 1.91 Table 3: The effects of decoder FFN. Hidden Gates √ √ × √ × × × √ BLEU dev test 24.65 24.71 24.23 24.36 28.37 28.38 27.92 27.97 Table 4: Using 2-layer FFN computation. ing (Chen et al., 2019b). • LSTMs lack a mechanism to directly connect distant words, which may lead to overlooking neighboring information, while the"
2021.acl-long.23,2020.acl-main.323,1,0.559223,"Missing"
2021.acl-long.23,2020.acl-main.37,1,0.856289,"Missing"
2021.acl-long.23,2020.acl-main.38,1,0.645553,"Missing"
2021.acl-long.23,P19-1295,0,0.0127433,"8.37 27.67 62.37 62.80 50.21 Speed-Up Train Decode 1.00 1.16 1.49 1.00 1.69 1.91 Table 3: The effects of decoder FFN. Hidden Gates √ √ × √ × × × √ BLEU dev test 24.65 24.71 24.23 24.36 28.37 28.38 27.92 27.97 Table 4: Using 2-layer FFN computation. ing (Chen et al., 2019b). • LSTMs lack a mechanism to directly connect distant words, which may lead to overlooking neighboring information, while the use of a bag-of-words representation (Equation 9) enables MHPLSTM to connect tokens directly regardless of the distance, thus MHPLSTM is able to leverage both local (Equation 16) and global patterns (Xu et al., 2019). (Please refer to Section 4.7 for empirical verification.) • Compared to the self-attention network, the MHPLSTM computation is more complex. • The computation for the LSTM hidden state (Equation 14) and output gate (Equation 17) in MHPLSTM is enhanced compared to the LN-LSTM. 4.3 Effect of FFN Layers We conducted ablation studies on the WMT 14 En-De task. Since the LSTM hidden state computation may take the role of the position-wise Feed-Forward Network (FFN) sub-layer of decoder layers, we first study removing the FFN sub-layer in decoder layers. Results are shown in Table 3. Table 3 shows"
2021.acl-long.23,P18-1166,1,0.897969,"ing epoch. We followed Vaswani et al. (2017) for the experiment settings. The training steps for Transformer Base and Transformer Big were 100k and 300k respectively. We used a dropout of 0.1 for all experiments except for the Transformer Big setting on the En-De task which was 0.3. For the Transformer Base setting, the embedding dimension and the hidden dimension of the position-wise feed-forward neural network were 512 and 2048 respectively, the corresponding values for the Transformer Big 276 Speed-Up Train Decode Model BLEU Para. (M) Attention Based Transformer (Vaswani et al., 2017) AAN (Zhang et al., 2018a) 27.55 27.63 62.37 74.97 1.00 1.04 1.00 1.52 Recurrent LN-LSTM (Chen et al., 2018) ATR (Zhang et al., 2018b) 27.96 27.93 68.69 59.23 0.45 0.50 1.47 1.69 Ours MHPLSTM 28.37 62.80 1.16 1.69 Table 2: Comparison on WMT 14 En-De. For recurrent approaches, we replace the self-attention sub-layer of standard Transformer decoder layers with the corresponding module proposed in previous work. setting were 1024 and 4096 respectively. The dimension of each head is 64, thus there were 8 and 16 heads for the base setting and the big setting respectively. We implemented our approaches based on the Neutron"
2021.acl-long.23,D18-1459,1,0.916533,"ing epoch. We followed Vaswani et al. (2017) for the experiment settings. The training steps for Transformer Base and Transformer Big were 100k and 300k respectively. We used a dropout of 0.1 for all experiments except for the Transformer Big setting on the En-De task which was 0.3. For the Transformer Base setting, the embedding dimension and the hidden dimension of the position-wise feed-forward neural network were 512 and 2048 respectively, the corresponding values for the Transformer Big 276 Speed-Up Train Decode Model BLEU Para. (M) Attention Based Transformer (Vaswani et al., 2017) AAN (Zhang et al., 2018a) 27.55 27.63 62.37 74.97 1.00 1.04 1.00 1.52 Recurrent LN-LSTM (Chen et al., 2018) ATR (Zhang et al., 2018b) 27.96 27.93 68.69 59.23 0.45 0.50 1.47 1.69 Ours MHPLSTM 28.37 62.80 1.16 1.69 Table 2: Comparison on WMT 14 En-De. For recurrent approaches, we replace the self-attention sub-layer of standard Transformer decoder layers with the corresponding module proposed in previous work. setting were 1024 and 4096 respectively. The dimension of each head is 64, thus there were 8 and 16 heads for the base setting and the big setting respectively. We implemented our approaches based on the Neutron"
2021.acl-long.291,W09-3302,0,0.0313802,"feed instances from the ZuCo dataset and those from other datasets built for the same target task but without cognitive processing signals into CogAlign. Since CogAlign is a multi-task learning framework, model parameters can be updated either by data with cognitive processing signals or by data without such signals, where task-specific loss is used in both situations. Please notice that only textual inputs are fed into trained CogAlign for inference. To evaluate the capacity of CogAlign in transferring cognitive features, we select benchmark datasets for NER and sentiment analysis: Wikigold (Balasuriya et al., 2009) and Stanford Sentiment Treebank (Socher et al., 2013). Since no other datasets use the same set of relation types as that in ZuCo dataset, we do not test the relation extraction task for transfer learning. To ensure that the same textual data are used for comparison, we add a new baseline model (baseline (+Zuco text)) that is trained on the combination of textual data in ZuCo and benchmark dataset. Additionally, as CogAlign uses two encoders for inference (i.e., the textual encoder and shared encoder), for a fair comparison, we setup another baseline (baseline (two encoders)) that also uses t"
2021.acl-long.291,K18-1030,0,0.0583741,"the biological and cognitive processes and aspects that underlie the mental language processing procedures in human brains while natural language processing (NLP) teaches machines to read, analyze, translate and generate human language sequences (Muttenthaler et al., 2020). The commonality of language processing shared by these two areas forms the base of ∗ Corresponding author cognitively-inspired NLP, which uses cognitive language processing signals generated by human brains to enhance or probe neural models in solving a variety of NLP tasks, such as sentiment analysis (Mishra et al., 2017; Barrett et al., 2018), named entity recognition (NER) (Hollenstein and Zhang, 2019), dependency parsing (Strzyz et al., 2019), relation extraction (Hollenstein et al., 2019a), etc. In spite of the success of cognitively-inspired NLP in some tasks, there are some issues in the use of cognitive features in NLP. First, for the integration of cognitive processing signals into neural models of NLP tasks, most previous studies have just directly concatenated word embeddings with cognitive features from eye-tracking or EEG, ignoring the huge differences between these two types of representations. Word embeddings are usua"
2021.acl-long.291,P16-2094,0,0.0211004,"ent analysis and relation extraction, which show CogAlign achieves new state-of-the-art results and significant improvements over strong baselines. 2 Related Work Eye-tracking for NLP. Eye-tracking data have proved to be associated with language comprehension activity in human brains by numerous research in neuroscience (Rayner, 1998; Henderson and Ferreira, 1993). In cognitively motivated NLP, several studies have investigated the impact of eye-tracking data on NLP tasks. In early works, these signals have been used in machine learning approaches to NLP tasks, such as part-of-speech tagging (Barrett et al., 2016), multiword expression extraction (Rohanian et al., 2017), syntactic category prediction (Barrett and Søgaard, 2015). In neural models, eyetracking data are combined with word embeddings to improve various NLP tasks, such as sentiment analysis (Mishra et al., 2017) and NER (Hollenstein and Zhang, 2019). Eye-tracking data have also been used to enhance or constrain neural attention in (Barrett et al., 2018; Sood et al., 2020b,a; Takmaz et al., 2020). EEG for NLP. Electroencephalography (EEG) measures potentials fluctuations caused by the activity of neurons in cerebral cortex. The exploration o"
2021.acl-long.291,K15-1038,0,0.0294964,"improvements over strong baselines. 2 Related Work Eye-tracking for NLP. Eye-tracking data have proved to be associated with language comprehension activity in human brains by numerous research in neuroscience (Rayner, 1998; Henderson and Ferreira, 1993). In cognitively motivated NLP, several studies have investigated the impact of eye-tracking data on NLP tasks. In early works, these signals have been used in machine learning approaches to NLP tasks, such as part-of-speech tagging (Barrett et al., 2016), multiword expression extraction (Rohanian et al., 2017), syntactic category prediction (Barrett and Søgaard, 2015). In neural models, eyetracking data are combined with word embeddings to improve various NLP tasks, such as sentiment analysis (Mishra et al., 2017) and NER (Hollenstein and Zhang, 2019). Eye-tracking data have also been used to enhance or constrain neural attention in (Barrett et al., 2018; Sood et al., 2020b,a; Takmaz et al., 2020). EEG for NLP. Electroencephalography (EEG) measures potentials fluctuations caused by the activity of neurons in cerebral cortex. The exploration of EEG data in NLP tasks is relatively limited. Chen et al. (2012) improve the performance of automatic speech recogn"
2021.acl-long.291,D18-1017,0,0.0241297,"019a) incorporate EEG signals into NLP tasks, including NER, relation extraction and sentiment analysis. Additionally, Muttenthaler et al. (2020) leverage EEG features to regularize attention on relation extraction. Adversarial Learning. The concept of adversarial training originates from the Generative Adversarial Nets (GAN) (Goodfellow et al., 2014) in computer vision. Since then, it has been also applied in NLP (Denton et al., 2015; Ganin et al., 2016). Recently, a great variety of studies attempt to introduce adversarial training into multi-task learning in NLP tasks, such as Chinese NER (Cao et al., 2018), crowdsourcing learning (Yang et al., 2018), cross-lingual transfer learning (Chen et al., 2018; Kim et al., 2017), just name a few. Different from these studies, we use adversarial learning to deeply align cognitive modality to textual modality at the sentence level. 3 CogAlign CogAlign is a general framework for incorporating cognitive processing signals into various NLP tasks. The target task can be specified at the predictor layer with corresponding task-specific neural network. CogAlign focuses on aligning cognitive processing signals to textual features at the word and encoder level. Th"
2021.acl-long.291,Q18-1039,0,0.0200403,"nalysis. Additionally, Muttenthaler et al. (2020) leverage EEG features to regularize attention on relation extraction. Adversarial Learning. The concept of adversarial training originates from the Generative Adversarial Nets (GAN) (Goodfellow et al., 2014) in computer vision. Since then, it has been also applied in NLP (Denton et al., 2015; Ganin et al., 2016). Recently, a great variety of studies attempt to introduce adversarial training into multi-task learning in NLP tasks, such as Chinese NER (Cao et al., 2018), crowdsourcing learning (Yang et al., 2018), cross-lingual transfer learning (Chen et al., 2018; Kim et al., 2017), just name a few. Different from these studies, we use adversarial learning to deeply align cognitive modality to textual modality at the sentence level. 3 CogAlign CogAlign is a general framework for incorporating cognitive processing signals into various NLP tasks. The target task can be specified at the predictor layer with corresponding task-specific neural network. CogAlign focuses on aligning cognitive processing signals to textual features at the word and encoder level. The text-aware attention aims at learning task-related useful cognitive information (thus filterin"
2021.acl-long.291,N12-1042,0,0.0231843,"al., 2017), syntactic category prediction (Barrett and Søgaard, 2015). In neural models, eyetracking data are combined with word embeddings to improve various NLP tasks, such as sentiment analysis (Mishra et al., 2017) and NER (Hollenstein and Zhang, 2019). Eye-tracking data have also been used to enhance or constrain neural attention in (Barrett et al., 2018; Sood et al., 2020b,a; Takmaz et al., 2020). EEG for NLP. Electroencephalography (EEG) measures potentials fluctuations caused by the activity of neurons in cerebral cortex. The exploration of EEG data in NLP tasks is relatively limited. Chen et al. (2012) improve the performance of automatic speech recognition (ASR) by using EEG signals to classify the speaker’s mental state. Hollenstein et al. (2019a) incorporate EEG signals into NLP tasks, including NER, relation extraction and sentiment analysis. Additionally, Muttenthaler et al. (2020) leverage EEG features to regularize attention on relation extraction. Adversarial Learning. The concept of adversarial training originates from the Generative Adversarial Nets (GAN) (Goodfellow et al., 2014) in computer vision. Since then, it has been also applied in NLP (Denton et al., 2015; Ganin et al., 2"
2021.acl-long.291,Q16-1026,0,0.0279819,"We will elaborate the components of model in the following subsections. 3.1 Input Layer The inputs to our model include textual word embeddings and cognitive processing signals. Word Embeddings. For a given word xi from the dataset of a target NLP task (e.g., NER), we obtain the vector representation hword by looking up a i pre-trained embedding matrix. The obtained word embeddings are fixed during training. For NER, previous studies have shown that character-level features can improve the performance of sequence labeling (Lin et al., 2018). We therefore apply a character-level CNN framework (Chiu and Nichols, 2016; Ma and Hovy, 2016) to capture the characterlevel embedding. The word representation of word xi in NER task is the concatenation of word embedding and character-level embedding. Cognitive Processing Signals. For cognitive inputs, we can obtain word-level eye-tracking and EEG via data preprocessing (see details in Section 5.1). Thus, for each word xi , we employ two cognitive processing signals heye and heeg i i . The cognicog tive input hi can be either a single type of signal or a concatenation of different cognitive processing signals. 3.2 Text-Aware Attention As not all information contain"
2021.acl-long.291,N06-1038,0,0.0210194,"on probability of the word w-1 the fixation probability of the word w+1 the fixation probability of the word w+2 the fixation duration of the word w-2 the fixation duration of the word w-1 the fixation duration of the word w+1 the fixation duration of the word w+2 Table 1: Eye-tracking features used in the NER task. consist of two datasets: 400 movie reviews from Stanford Sentiment Treebank (Socher et al., 2013) with manually annotated sentiment labels, including 123 neutral, 137 negative and 140 positive sentences; 300 paragraphs about famous people from Wikipedia relation extraction corpus (Culotta et al., 2006) labeled with 11 relationship types, such as award, education. We also tested our model on NER task. For NER, the selected 700 sentences in the above two tasks are annotated with three types of entities: PERSON, ORGANIZATION, and LOCATION. All annotated datasets3 are publicly available. The cognitive processing signals and textual features used for each task in this work are the same as (Hollenstein et al., 2019a). Eye-tracking Features. Eye-tracking signals record human gaze behavior while reading. The eye-tracking data of ZuCo are collected by an infrared video-based eye tracker EyeLink 1000"
2021.acl-long.291,N19-1001,0,0.239458,"underlie the mental language processing procedures in human brains while natural language processing (NLP) teaches machines to read, analyze, translate and generate human language sequences (Muttenthaler et al., 2020). The commonality of language processing shared by these two areas forms the base of ∗ Corresponding author cognitively-inspired NLP, which uses cognitive language processing signals generated by human brains to enhance or probe neural models in solving a variety of NLP tasks, such as sentiment analysis (Mishra et al., 2017; Barrett et al., 2018), named entity recognition (NER) (Hollenstein and Zhang, 2019), dependency parsing (Strzyz et al., 2019), relation extraction (Hollenstein et al., 2019a), etc. In spite of the success of cognitively-inspired NLP in some tasks, there are some issues in the use of cognitive features in NLP. First, for the integration of cognitive processing signals into neural models of NLP tasks, most previous studies have just directly concatenated word embeddings with cognitive features from eye-tracking or EEG, ignoring the huge differences between these two types of representations. Word embeddings are usually learned as static or contextualized representations of wor"
2021.acl-long.291,D17-1302,0,0.0293285,"ly, Muttenthaler et al. (2020) leverage EEG features to regularize attention on relation extraction. Adversarial Learning. The concept of adversarial training originates from the Generative Adversarial Nets (GAN) (Goodfellow et al., 2014) in computer vision. Since then, it has been also applied in NLP (Denton et al., 2015; Ganin et al., 2016). Recently, a great variety of studies attempt to introduce adversarial training into multi-task learning in NLP tasks, such as Chinese NER (Cao et al., 2018), crowdsourcing learning (Yang et al., 2018), cross-lingual transfer learning (Chen et al., 2018; Kim et al., 2017), just name a few. Different from these studies, we use adversarial learning to deeply align cognitive modality to textual modality at the sentence level. 3 CogAlign CogAlign is a general framework for incorporating cognitive processing signals into various NLP tasks. The target task can be specified at the predictor layer with corresponding task-specific neural network. CogAlign focuses on aligning cognitive processing signals to textual features at the word and encoder level. The text-aware attention aims at learning task-related useful cognitive information (thus filtering out noises) while"
2021.acl-long.291,P18-1074,0,0.0157045,"resent. The neural architecture of CogAlign is visualized in Figure 1. We will elaborate the components of model in the following subsections. 3.1 Input Layer The inputs to our model include textual word embeddings and cognitive processing signals. Word Embeddings. For a given word xi from the dataset of a target NLP task (e.g., NER), we obtain the vector representation hword by looking up a i pre-trained embedding matrix. The obtained word embeddings are fixed during training. For NER, previous studies have shown that character-level features can improve the performance of sequence labeling (Lin et al., 2018). We therefore apply a character-level CNN framework (Chiu and Nichols, 2016; Ma and Hovy, 2016) to capture the characterlevel embedding. The word representation of word xi in NER task is the concatenation of word embedding and character-level embedding. Cognitive Processing Signals. For cognitive inputs, we can obtain word-level eye-tracking and EEG via data preprocessing (see details in Section 5.1). Thus, for each word xi , we employ two cognitive processing signals heye and heeg i i . The cognicog tive input hi can be either a single type of signal or a concatenation of different cognitive"
2021.acl-long.291,W19-2909,0,0.0456998,"Missing"
2021.acl-long.291,P16-1101,0,0.137029,"mponents of model in the following subsections. 3.1 Input Layer The inputs to our model include textual word embeddings and cognitive processing signals. Word Embeddings. For a given word xi from the dataset of a target NLP task (e.g., NER), we obtain the vector representation hword by looking up a i pre-trained embedding matrix. The obtained word embeddings are fixed during training. For NER, previous studies have shown that character-level features can improve the performance of sequence labeling (Lin et al., 2018). We therefore apply a character-level CNN framework (Chiu and Nichols, 2016; Ma and Hovy, 2016) to capture the characterlevel embedding. The word representation of word xi in NER task is the concatenation of word embedding and character-level embedding. Cognitive Processing Signals. For cognitive inputs, we can obtain word-level eye-tracking and EEG via data preprocessing (see details in Section 5.1). Thus, for each word xi , we employ two cognitive processing signals heye and heeg i i . The cognicog tive input hi can be either a single type of signal or a concatenation of different cognitive processing signals. 3.2 Text-Aware Attention As not all information contained in cognitive proc"
2021.acl-long.291,D14-1162,0,0.0845374,"Missing"
2021.acl-long.291,rohanian-etal-2017-using,0,0.138485,"n achieves new state-of-the-art results and significant improvements over strong baselines. 2 Related Work Eye-tracking for NLP. Eye-tracking data have proved to be associated with language comprehension activity in human brains by numerous research in neuroscience (Rayner, 1998; Henderson and Ferreira, 1993). In cognitively motivated NLP, several studies have investigated the impact of eye-tracking data on NLP tasks. In early works, these signals have been used in machine learning approaches to NLP tasks, such as part-of-speech tagging (Barrett et al., 2016), multiword expression extraction (Rohanian et al., 2017), syntactic category prediction (Barrett and Søgaard, 2015). In neural models, eyetracking data are combined with word embeddings to improve various NLP tasks, such as sentiment analysis (Mishra et al., 2017) and NER (Hollenstein and Zhang, 2019). Eye-tracking data have also been used to enhance or constrain neural attention in (Barrett et al., 2018; Sood et al., 2020b,a; Takmaz et al., 2020). EEG for NLP. Electroencephalography (EEG) measures potentials fluctuations caused by the activity of neurons in cerebral cortex. The exploration of EEG data in NLP tasks is relatively limited. Chen et al"
2021.acl-long.291,D13-1170,0,0.0168478,"es word w that is fixated after the first fixation the probability of word w that is fixated more than once the total duration of regressions from word w the fixation probability of the word w-2 the fixation probability of the word w-1 the fixation probability of the word w+1 the fixation probability of the word w+2 the fixation duration of the word w-2 the fixation duration of the word w-1 the fixation duration of the word w+1 the fixation duration of the word w+2 Table 1: Eye-tracking features used in the NER task. consist of two datasets: 400 movie reviews from Stanford Sentiment Treebank (Socher et al., 2013) with manually annotated sentiment labels, including 123 neutral, 137 negative and 140 positive sentences; 300 paragraphs about famous people from Wikipedia relation extraction corpus (Culotta et al., 2006) labeled with 11 relationship types, such as award, education. We also tested our model on NER task. For NER, the selected 700 sentences in the above two tasks are annotated with three types of entities: PERSON, ORGANIZATION, and LOCATION. All annotated datasets3 are publicly available. The cognitive processing signals and textual features used for each task in this work are the same as (Hol"
2021.acl-long.291,2020.conll-1.2,0,0.0994798,"Missing"
2021.acl-long.291,D19-1160,0,0.0248723,"Missing"
2021.acl-long.291,2020.emnlp-main.377,0,0.0822691,"Missing"
2021.acl-long.469,2020.lrec-1.3,0,0.0973937,"Missing"
2021.acl-long.469,2020.findings-emnlp.81,0,0.046775,"Missing"
2021.acl-long.469,D19-1060,0,0.06048,"Missing"
2021.acl-long.469,2020.acl-main.130,0,0.014664,"unt of data via self-supervised learning, have made remarkable progress on both natural language understanding (NLU) (Wang et al., 2018, 2019) and natural language generation (NLG) (Liu and Lapata, 2019; Weng et al., 2020; Cao et al., 2020). On several NLU datasets, PLM-based neural models have gradually achieved human-level performance in terms of automatic evaluation metrics (e.g., accuracy, F1 ) (He et al., 2020; Zhang et al., 2021). In order to deeply understand and analyze the capability of PLMs on NLU, a variety of more challenging NLU datasets have been proposed (Warstadt et al., 2020; Cui et al., 2020a; Jain et al., 2020; Talmor et al., 2020). These datasets can be used not only to obtain knowledge on how PLM-based models work and what they learn, but also to define new NLU tasks and to serve as a benchmark for future progress. For example, evaluating and analyzing PLM-based models on learning document structures with a carefully created benchmark test suite (Chen et al., 2019), helps to develop new methods to enhance the capability of these models on discourse modeling (Iter et al., 2020). Knowing the weakness of current PLM-based models in commonsense reasoning (Zhou et al., 2020) has in"
2021.acl-long.469,2020.findings-emnlp.58,0,0.0282809,"unt of data via self-supervised learning, have made remarkable progress on both natural language understanding (NLU) (Wang et al., 2018, 2019) and natural language generation (NLG) (Liu and Lapata, 2019; Weng et al., 2020; Cao et al., 2020). On several NLU datasets, PLM-based neural models have gradually achieved human-level performance in terms of automatic evaluation metrics (e.g., accuracy, F1 ) (He et al., 2020; Zhang et al., 2021). In order to deeply understand and analyze the capability of PLMs on NLU, a variety of more challenging NLU datasets have been proposed (Warstadt et al., 2020; Cui et al., 2020a; Jain et al., 2020; Talmor et al., 2020). These datasets can be used not only to obtain knowledge on how PLM-based models work and what they learn, but also to define new NLU tasks and to serve as a benchmark for future progress. For example, evaluating and analyzing PLM-based models on learning document structures with a carefully created benchmark test suite (Chen et al., 2019), helps to develop new methods to enhance the capability of these models on discourse modeling (Iter et al., 2020). Knowing the weakness of current PLM-based models in commonsense reasoning (Zhou et al., 2020) has in"
2021.acl-long.469,2020.emnlp-main.680,0,0.495208,"the best of our knowledge, is the first dataset built on machine-generated texts from state-of-the-art pretrained language models with rich annotations. The key interest of this dataset is detecting and annotating text generation errors from PLMs. Therefore it is different from conventional text generation datasets (e.g., Multi-News (Fabbri et al., 2019), TextCaps (Sidorov et al., 2020)) that are constructed to train models to learn text generation (e.g., generating texts from images or long documents). It is also different from grammatical error correction (GEC) datasets (Zhao et al., 2018; Flachs et al., 2020) that are built from human-written texts usually by second language learners. • TGEA provides rich semantic information for text generation errors, including error types, associated text spans, error corrections and rationals behind errors, as shown in Figure 1. Marking text spans that are closely related to erroneous words allows us to detect longdistance dependencies of errors or reasoning chains related to errors. Rationales behind errors directly explain why errors are annotated. All these error-centered manual annotations not only increase the interpretability of our dataset, but also fac"
2021.acl-long.469,2020.tacl-1.48,0,0.0340173,"ng, have made remarkable progress on both natural language understanding (NLU) (Wang et al., 2018, 2019) and natural language generation (NLG) (Liu and Lapata, 2019; Weng et al., 2020; Cao et al., 2020). On several NLU datasets, PLM-based neural models have gradually achieved human-level performance in terms of automatic evaluation metrics (e.g., accuracy, F1 ) (He et al., 2020; Zhang et al., 2021). In order to deeply understand and analyze the capability of PLMs on NLU, a variety of more challenging NLU datasets have been proposed (Warstadt et al., 2020; Cui et al., 2020a; Jain et al., 2020; Talmor et al., 2020). These datasets can be used not only to obtain knowledge on how PLM-based models work and what they learn, but also to define new NLU tasks and to serve as a benchmark for future progress. For example, evaluating and analyzing PLM-based models on learning document structures with a carefully created benchmark test suite (Chen et al., 2019), helps to develop new methods to enhance the capability of these models on discourse modeling (Iter et al., 2020). Knowing the weakness of current PLM-based models in commonsense reasoning (Zhou et al., 2020) has inspired people to develop various reasoning"
2021.acl-long.469,N19-1421,0,0.0296774,"ogrande, a larger version of WSC, is introduced by Sakaguchi et al. (2020), which contains ∼ 44, 000 examples. Winowhy (Zhang et al., 2020a) asks annotators to provide reasons for their decisions to WSC. In this aspect, the differences of our dataset from Winowhy are twofold. First, we provide reasons for errors rather than correct decisions to anaphora. Second, we provide reasons for all text generation errors, rather than only errors related to commonsense reasoning. In addition to COPA and WSC-style datasets, many large crowdsourced datasets have been also proposed recently. CommonsenseQA (Talmor et al., 2019), a commonsense question answering dataset, has been constructed from ConceptNet. HellaSwag (Zellers et al., 2019b) and Abductive NLI (Bhagavatula et al., 2020) evaluate commonsense reasoning in the form of natural language inference. CosmosQA (Huang et al., 2019) is a dataset with multi-choice questions that require commonsense reading comprehension. Beyond datasets for evaluating commonsense reasoning, there are other datasets providing commonsense knowledge. PIQA (Bisk et al., 2020) focuses on physical commonsense knowledge while SocialIQA (Sap et al., 2019) on social commonsense knowledge."
2021.acl-long.469,W18-5446,0,0.0823812,"Missing"
2021.acl-long.469,2020.aacl-main.20,0,0.0748908,"Missing"
2021.acl-long.469,P11-1019,0,0.0425583,"ble Inference Plausible Inference Reason Explanation Multiple tasks Commonsense Reasoning 6 6 6 6 6 6 3 3 3 3 3 3 3 Machine-Generated Texts 6 6 6 6 6 6 6 Rationales 6 6 6 6 6 6 6 6 6 6 6 6 3 3 6 6 6 6 3 Domain #Sentences Language Essay Journal articles TOFEL Exam Web doc/Essay Web doc Essay Open WikiHow articles Social situations Narratives Physical situations ROCStories Open Open 34K 1.2M 1,511 8K 13K 0.71M 273 70K 38K 35K 21K 200K 2,865 47K EN EN EN EN EN ZH EN EN EN EN EN EN EN ZH Table 1: Comparison between our dataset and other datasets. 2.1 Grammatical Error Correction Datasets 2.2 FCE (Yannakoudakis et al., 2011) is an early largescale English grammatical error correction dataset, where raw texts are produced by English learners taking the First Certificate in English exams. AESW (Daudaravicius et al., 2016) is a GEC dataset from a professional editing company. In addition to common grammatical errors, AESW covers style issues as it contains texts mainly from scholarly papers. JFLEG (Napoles et al., 2017) is a GEC dataset built from TOFEL Exams, which does not force annotators to make minimal edits, preferring holistic fluency rewrites. CMEG (Napoles et al., 2019) is different from general grammatical"
2021.acl-long.469,P19-1472,0,0.0217165,"nowhy (Zhang et al., 2020a) asks annotators to provide reasons for their decisions to WSC. In this aspect, the differences of our dataset from Winowhy are twofold. First, we provide reasons for errors rather than correct decisions to anaphora. Second, we provide reasons for all text generation errors, rather than only errors related to commonsense reasoning. In addition to COPA and WSC-style datasets, many large crowdsourced datasets have been also proposed recently. CommonsenseQA (Talmor et al., 2019), a commonsense question answering dataset, has been constructed from ConceptNet. HellaSwag (Zellers et al., 2019b) and Abductive NLI (Bhagavatula et al., 2020) evaluate commonsense reasoning in the form of natural language inference. CosmosQA (Huang et al., 2019) is a dataset with multi-choice questions that require commonsense reading comprehension. Beyond datasets for evaluating commonsense reasoning, there are other datasets providing commonsense knowledge. PIQA (Bisk et al., 2020) focuses on physical commonsense knowledge while SocialIQA (Sap et al., 2019) on social commonsense knowledge. Commonsense datasets in multiple languages or languages other than English have also been created recently. XCOP"
2021.acl-long.469,2020.acl-main.508,0,0.109801,"ot only to obtain knowledge on how PLM-based models work and what they learn, but also to define new NLU tasks and to serve as a benchmark for future progress. For example, evaluating and analyzing PLM-based models on learning document structures with a carefully created benchmark test suite (Chen et al., 2019), helps to develop new methods to enhance the capability of these models on discourse modeling (Iter et al., 2020). Knowing the weakness of current PLM-based models in commonsense reasoning (Zhou et al., 2020) has inspired people to develop various reasoning datasets (Cui et al., 2020a; Zhang et al., 2020b). On the other hand, state-of-the-art PLMs are able to generate texts that are even not distinguishable from human-written texts by human evaluators (Radford et al., 2019; Brown et al., 2020). This makes us curious about the capability of PLMs on text generation. Are they really reaching humanlevel performance on text generation? In contrast to the studies of PLMs on NLU, research on the 6012 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6012–6025 August 1–6, 2021. ©202"
2021.acl-long.469,2020.emnlp-main.374,0,0.0913543,"Missing"
2021.acl-short.46,C18-1263,0,0.0167812,"elated Work Multilingual NMT includes one-to-many (Dong et al., 2015), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trai"
2021.acl-short.46,P15-1166,0,0.0231329,"yer model of Zhang et al. (2020). Results are shown in Table 1. Table 1 shows that our approach achieves better performance in all evaluations while being 1.31 times as fast. 4.3 Ablation Study We study removing MIMO transformations and task-aware attention. Results are shown in Table 2. Table 2 verifies that both mechanisms contribute to the performance. We also examine different combinations of depth and cardinality. Results are shown in Table 3. Table 3 shows that using 6 layers with 4 transformations in each block leads to the best perforRelated Work Multilingual NMT includes one-to-many (Dong et al., 2015), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wa"
2021.acl-short.46,D16-1026,0,0.0445997,"Missing"
2021.acl-short.46,2020.wmt-1.66,0,0.0226759,"are shown in Table 3. Table 3 shows that using 6 layers with 4 transformations in each block leads to the best perforRelated Work Multilingual NMT includes one-to-many (Dong et al., 2015), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual"
2021.acl-short.46,P19-1120,0,0.0485518,"Missing"
2021.acl-short.46,D19-1167,0,0.0254023,"presentation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism"
2021.acl-short.46,C18-1054,0,0.0160407,"t studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO architecture that allows each transformation of the block to have its own input. We"
2021.acl-short.46,2020.emnlp-main.476,0,0.0283726,"target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2"
2021.acl-short.46,2020.emnlp-main.187,0,0.011521,"(Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectivel"
2021.acl-short.46,P02-1040,0,0.109171,"Missing"
2021.acl-short.46,D18-1039,0,0.0186647,"o-many (Dong et al., 2015), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve"
2021.acl-short.46,W18-6319,0,0.0200658,"Missing"
2021.acl-short.46,W18-6327,0,0.0163699,"dle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Concl"
2021.acl-short.46,P19-1297,0,0.022488,"specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast. Acknowledgments We thank anonymous reviewers for th"
2021.acl-short.46,2020.acl-main.252,0,0.0198946,"massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We pr"
2021.acl-short.46,2020.acl-main.324,0,0.0224391,"s (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast. Acknowledgments We thank anonymous reviewers for their insightful comm"
2021.acl-short.46,2020.emnlp-main.75,0,0.0340627,"NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO archit"
2021.acl-short.46,D19-1089,0,0.0421137,"Missing"
2021.acl-short.46,2020.acl-main.38,1,0.87325,"Missing"
2021.acl-short.46,tiedemann-2012-parallel,0,0.104965,"Missing"
2021.acl-short.46,2020.acl-main.754,0,0.0194131,"NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenarios (Dabre et al., 2019). Multilingual data also has been proven useful for unsupervised NMT (Sen et al., 2019; Sun et al., 2020). 6 Conclusion We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. We present a MIMO archit"
2021.acl-short.46,D18-1326,0,0.0144444,"NMT includes one-to-many (Dong et al., 2015), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NM"
2021.acl-short.46,P19-1117,0,0.0175864,"5), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of"
2021.acl-short.46,2020.emnlp-main.77,0,0.0684984,"Missing"
2021.acl-short.46,2020.acl-main.148,0,0.213971,"ng3,4∗ DFKI and Saarland University, Informatics Campus, Saarland, Germany 2 China Mobile Online Services, Henan, China 3 Tianjin University, Tianjin, China 4 Global Tone Communication Technology Co., Ltd. {hfxunlp, liuqhano}@foxmail.com, josef.van genabith@dfki.de, dyxiong@tju.edu.cn 1 Abstract Despite their advantages, multilingual systems tend to underperform their bilingual counterparts as the number of languages increases (Johnson et al., 2017; Aharoni et al., 2019). This is due to the fact that multilingual NMT must distribute its modeling capacity over different translation directions. Zhang et al. (2020) show that the model capacity is crucial for massively multilingual NMT to support language pairs with varying typological characteristics, and propose to increase the modeling capacity by deepening the Transformer. However, compared to going deeper or wider, modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective when we increase the model capacity (Xie et al., 2017). In this paper, we efficiently increase the capacity of the multilingual NMT model by increasing the cardinality, i.e. stacking sub-layers that aggregate a set of t"
2021.acl-short.46,2020.acl-main.150,0,0.0400861,"on is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020). Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs (Kim et al., 2019; Lin et al., 2020), especially for low-resource scenari"
2021.acl-short.46,N16-1004,0,0.0244862,"ads to the best perforRelated Work Multilingual NMT includes one-to-many (Dong et al., 2015), many-to-many (Firat et al., 2016a) and zero-shot (Firat et al., 2016b) scenarios. A simple solution is to insert a target language token at the beginning of the input sentence (Johnson et al., 2017). Multilingual NMT has to handle different languages in one joint representation space, neglecting their linguistic diversity, especially for massively multilingual NMT (Aharoni et al., 2019; Zhang et al., 2020; Freitag and Firat, 2020). Most studies focus on how to mitigate this representation bottleneck (Zoph and Knight, 2016; Blackwood et al., 2018; Wang et al., 2018; Platanios et al., 2018; Wang et al., 2019a; Tan et al., 2019b; Wang et al., 2019b; Tan et al., 2019a; Bapna and Firat, 2019; Zhu et al., 2020; Lyu et al., 2020). There are also studies on the trade-off between 364 shared and language-specific parameters (Sachan and Neubig, 2018; Zhang et al., 2021), on the training of multilingual NMT (Al-Shedivat and Parikh, 2019; Siddhant et al., 2020; Wang et al., 2020b,a), and on analyzing translations from multilingual NMT (Lakew et al., 2018) or the trained model (Kudugunta et al., 2019; Oncevay et al., 2020)."
2021.acl-short.58,2012.eamt-1.60,0,0.0101455,"are generated by randomly sampling positions to attack. We improve their method by attacking the source side according to Data and Setup We conducted experiments on two translation tasks: English-Chinese and English-Japanese. Data for English-Chinese translation are from the United Nations English-Chinese corpus (Ziemski et al., 2016). We built the training/validate/test set for this task by randomly sampling 3M/2K/2K sentence pairs from the whole corpus. For the EnglishJapanese translation task, we aggregated the training set of KFTT (Neubig, 2011), JESC (Pryzant et al., 2018) and TED talks (Cettolo et al., 2012) as our training set, which consists of 3.9M sentence pairs. We evaluated our models on the validation set and test set of KFTT (Neubig, 2011). We split words into sub-word units with subword regularization (Kudo, 2018) and built a shared vocabulary of 32K subwords for both English-Chinese and English-Japanese. We used the base Transformer model (Vaswani et al., 2017) with 512 hidden units as the victim model. The hyper-parameters of the base Transformer follows the default setting in Vaswani et al. (2017). We implemented the adversarial attack and training methods of Cheng et al. (2019) and f"
2021.acl-short.58,P19-1425,0,0.299461,"Zhao et al. (2018) utilize generative adversarial networks to generate adversarial examples that lie on the data manifold by searching in the semantic space of dense and continuous data representations. Ebrahimi et al. (2018) propose an attack framework for character-level NMT, which uses gradient to rank adversarial manipulations and to search for adversarial examples via either greedy search or beam search methods. Belinkov and Bisk (2018) attack character-level NMT by randomizing the order of letters or randomly replacing letters with their adjacent letters on the keyboard. Vaibhav et al. (2019) use back translation to generate adversarial samples that emulate natural noises. Cheng et al. (2020a) exploit the projected gradient method combined with gradient regularization to generate adversarial samples. Zou et al. (2019) employ reinforcement learning to decide which positions to attack. Tan et al. (2020) present a method to change inflectional morphology of words to craft plausible and semantically similar adversarial examples. Emelin et al. (2020) propose to generate adversarial examples by eliciting disambiguation errors. All these approaches attack the source side of NMT in differ"
2021.acl-short.58,2020.acl-main.529,0,0.0263014,"les that lie on the data manifold by searching in the semantic space of dense and continuous data representations. Ebrahimi et al. (2018) propose an attack framework for character-level NMT, which uses gradient to rank adversarial manipulations and to search for adversarial examples via either greedy search or beam search methods. Belinkov and Bisk (2018) attack character-level NMT by randomizing the order of letters or randomly replacing letters with their adjacent letters on the keyboard. Vaibhav et al. (2019) use back translation to generate adversarial samples that emulate natural noises. Cheng et al. (2020a) exploit the projected gradient method combined with gradient regularization to generate adversarial samples. Zou et al. (2019) employ reinforcement learning to decide which positions to attack. Tan et al. (2020) present a method to change inflectional morphology of words to craft plausible and semantically similar adversarial examples. Emelin et al. (2020) propose to generate adversarial examples by eliciting disambiguation errors. All these approaches attack the source side of NMT in different ways. However distortions exist in not only the source language, but also the target language. Th"
2021.acl-short.58,P18-1163,0,0.140708,"ntences aligned to the front positions of corresponding target sentences is more effective than attacking other positions. We further exploit the attention distribution of the victim model to attack source sentences at positions that have a strong association with front target words. Experiment results demonstrate that our attention-based adversarial attack is more effective than adversarial attacks by sampling positions randomly or according to gradients. 1 Introduction Despite remarkable progress in recent years, neural machine translation (NMT) models are vulnerable to small perturbations (Cheng et al., 2018; Zhao et al., 2018). Adversarial training, which allows NMT models to learn from adversarial samples with perturbations, as a general approach, is widely used to improve the robustness of NMT (Ebrahimi et al., 2018; Vaibhav et al., 2019; Cheng et al., 2019, 2020a,a; Zou et al., 2019). Generally, NMT models yield target translations in an autoregressive way1 , which makes previous incorrectly predicted target tokens have a negative impact on future tokens to be generated. However, most approaches to generating NMT adversarial examples inject perturbations only into source sentences. Hence, are"
2021.acl-short.58,C18-1055,0,0.133638,"th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 454–460 August 1–6, 2021. ©2021 Association for Computational Linguistics based sampling. 2 the attention distribution. Experiments validate the effectiveness of our method. Related Work 3 Robustness is a well-known problem for neural networks (Szegedy et al., 2014; Goodfellow et al., 2015). Recent years have witnessed that many adversarial training approaches have been proposed to improve the robustness of NMT models. Cheng et al. (2018) generate adversarial samples at the lexical and feature level, and apply the adversarial learning to make adversarial samples natural. Zhao et al. (2018) utilize generative adversarial networks to generate adversarial examples that lie on the data manifold by searching in the semantic space of dense and continuous data representations. Ebrahimi et al. (2018) propose an attack framework for character-level NMT, which uses gradient to rank adversarial manipulations and to search for adversarial examples via either greedy search or beam search methods. Belinkov and Bisk (2018) attack character-l"
2021.acl-short.58,2020.emnlp-main.616,0,0.0224027,"rial manipulations and to search for adversarial examples via either greedy search or beam search methods. Belinkov and Bisk (2018) attack character-level NMT by randomizing the order of letters or randomly replacing letters with their adjacent letters on the keyboard. Vaibhav et al. (2019) use back translation to generate adversarial samples that emulate natural noises. Cheng et al. (2020a) exploit the projected gradient method combined with gradient regularization to generate adversarial samples. Zou et al. (2019) employ reinforcement learning to decide which positions to attack. Tan et al. (2020) present a method to change inflectional morphology of words to craft plausible and semantically similar adversarial examples. Emelin et al. (2020) propose to generate adversarial examples by eliciting disambiguation errors. All these approaches attack the source side of NMT in different ways. However distortions exist in not only the source language, but also the target language. This inspires us to compare the effectiveness of adversarial attack on the source and target side to NMT models. We have found that the NMT models are vulnerable to both the source and target attack. However, to our"
2021.acl-short.58,W17-4123,0,0.0285499,"It is surprising that the attack that samples positions according to the gradient (“grad”) is not better than the attack that samples from a uniform distribution (“rand”), which may suggest that the L∞ norm of the gradient cannot measure the importance of a word in a sentence. We can further extend our method to the black-box attack with the alignment from SMT models (Och and Ney, 2003), which is left to our future work. Our attention-based attack is proposed for autoregressive NMT models that generate target translations from left to right. It will not work for non-autoregressive NMT models (Gu et al., 2017) or autoregressive NMT models that generates translations in an arbitrary order (Stern et al., 2019). 9 Conclusion In this paper, we have empirically investigated adversarial attack on NMT models. We compare adversarial attack on the source vs. target side, and find that the former is more effective than the latter. We also study adversarial attack at different positions in either source or target sentences, and observe that attacking front positions in either source or target sentences for English-Chinese and English-Japanese translation is more effective than attacking back positions. We fur"
2021.acl-short.58,P18-1007,0,0.0231924,"a for English-Chinese translation are from the United Nations English-Chinese corpus (Ziemski et al., 2016). We built the training/validate/test set for this task by randomly sampling 3M/2K/2K sentence pairs from the whole corpus. For the EnglishJapanese translation task, we aggregated the training set of KFTT (Neubig, 2011), JESC (Pryzant et al., 2018) and TED talks (Cettolo et al., 2012) as our training set, which consists of 3.9M sentence pairs. We evaluated our models on the validation set and test set of KFTT (Neubig, 2011). We split words into sub-word units with subword regularization (Kudo, 2018) and built a shared vocabulary of 32K subwords for both English-Chinese and English-Japanese. We used the base Transformer model (Vaswani et al., 2017) with 512 hidden units as the victim model. The hyper-parameters of the base Transformer follows the default setting in Vaswani et al. (2017). We implemented the adversarial attack and training methods of Cheng et al. (2019) and followed their hyper-parameter setting in our experiments. The details of our implementation is shown in Section 4. We injected perturbations into either source sentences or target sentences to generate adversarial examp"
2021.acl-short.58,J03-1002,0,0.0127946,"Scores of almost all models under our proposed attack (“attn”) are lower than those under the other two attacking methods, which indicates the superiority of the proposed attention-based attack over the other two attack methods. It is surprising that the attack that samples positions according to the gradient (“grad”) is not better than the attack that samples from a uniform distribution (“rand”), which may suggest that the L∞ norm of the gradient cannot measure the importance of a word in a sentence. We can further extend our method to the black-box attack with the alignment from SMT models (Och and Ney, 2003), which is left to our future work. Our attention-based attack is proposed for autoregressive NMT models that generate target translations from left to right. It will not work for non-autoregressive NMT models (Gu et al., 2017) or autoregressive NMT models that generates translations in an arbitrary order (Stern et al., 2019). 9 Conclusion In this paper, we have empirically investigated adversarial attack on NMT models. We compare adversarial attack on the source vs. target side, and find that the former is more effective than the latter. We also study adversarial attack at different positions"
2021.acl-short.58,P02-1040,0,0.109249,"s. The details of our implementation is shown in Section 4. We injected perturbations into either source sentences or target sentences to generate adversarial examples which were used to evaluate NMT models. Since we could not inject perturbations into the target inputs of NMT models at the test time, we evaluated NMT models with target-side adversarial samples at training time on the validation dataset. Except where otherwise specified, the performance of the victim model was measured by word accuracy on the validation data. If we evaluated the victim model on the test set, detokenized BLEU (Papineni et al., 2002) and BERTScore (Zhang et al., 2020) were reported. Although the target-side inputs of NMT models could not be attacked at test time, there still exists noise or errors in them due to error propagation in the autoregressive decoding. Evaluating NMT with perturbated target sentences at training time enables us to analyze the vulnerability of NMT to the noise in target-side inputs, and inspires us to improve the robustness of NMT models to such noise. 455 src-tgt noisy-clean clean-noisy clean-clean noisy-noisy en-zh 55.79 61.32 71.16 46.55 zh-en 61.89 64.00 78.76 46.55 en-ja 50.21 52.74 60.35 40."
2021.acl-short.58,L18-1182,0,0.0179342,"their sourceside adversarial samples are generated by randomly sampling positions to attack. We improve their method by attacking the source side according to Data and Setup We conducted experiments on two translation tasks: English-Chinese and English-Japanese. Data for English-Chinese translation are from the United Nations English-Chinese corpus (Ziemski et al., 2016). We built the training/validate/test set for this task by randomly sampling 3M/2K/2K sentence pairs from the whole corpus. For the EnglishJapanese translation task, we aggregated the training set of KFTT (Neubig, 2011), JESC (Pryzant et al., 2018) and TED talks (Cettolo et al., 2012) as our training set, which consists of 3.9M sentence pairs. We evaluated our models on the validation set and test set of KFTT (Neubig, 2011). We split words into sub-word units with subword regularization (Kudo, 2018) and built a shared vocabulary of 32K subwords for both English-Chinese and English-Japanese. We used the base Transformer model (Vaswani et al., 2017) with 512 hidden units as the victim model. The hyper-parameters of the base Transformer follows the default setting in Vaswani et al. (2017). We implemented the adversarial attack and training"
2021.acl-short.58,2020.acl-main.263,0,0.0129307,"ank adversarial manipulations and to search for adversarial examples via either greedy search or beam search methods. Belinkov and Bisk (2018) attack character-level NMT by randomizing the order of letters or randomly replacing letters with their adjacent letters on the keyboard. Vaibhav et al. (2019) use back translation to generate adversarial samples that emulate natural noises. Cheng et al. (2020a) exploit the projected gradient method combined with gradient regularization to generate adversarial samples. Zou et al. (2019) employ reinforcement learning to decide which positions to attack. Tan et al. (2020) present a method to change inflectional morphology of words to craft plausible and semantically similar adversarial examples. Emelin et al. (2020) propose to generate adversarial examples by eliciting disambiguation errors. All these approaches attack the source side of NMT in different ways. However distortions exist in not only the source language, but also the target language. This inspires us to compare the effectiveness of adversarial attack on the source and target side to NMT models. We have found that the NMT models are vulnerable to both the source and target attack. However, to our"
2021.acl-short.58,N19-1190,0,0.06037,"ave a strong association with front target words. Experiment results demonstrate that our attention-based adversarial attack is more effective than adversarial attacks by sampling positions randomly or according to gradients. 1 Introduction Despite remarkable progress in recent years, neural machine translation (NMT) models are vulnerable to small perturbations (Cheng et al., 2018; Zhao et al., 2018). Adversarial training, which allows NMT models to learn from adversarial samples with perturbations, as a general approach, is widely used to improve the robustness of NMT (Ebrahimi et al., 2018; Vaibhav et al., 2019; Cheng et al., 2019, 2020a,a; Zou et al., 2019). Generally, NMT models yield target translations in an autoregressive way1 , which makes previous incorrectly predicted target tokens have a negative impact on future tokens to be generated. However, most approaches to generating NMT adversarial examples inject perturbations only into source sentences. Hence, are NMT ∗ Corresponding author We leave the study of adversarial attack to nonautoregressive NMT models to our future work. 1 • By the study, we have empirically found that adversarial attack on the source side is more effective than that o"
2021.acl-short.58,D18-1316,0,0.135763,"th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 454–460 August 1–6, 2021. ©2021 Association for Computational Linguistics based sampling. 2 the attention distribution. Experiments validate the effectiveness of our method. Related Work 3 Robustness is a well-known problem for neural networks (Szegedy et al., 2014; Goodfellow et al., 2015). Recent years have witnessed that many adversarial training approaches have been proposed to improve the robustness of NMT models. Cheng et al. (2018) generate adversarial samples at the lexical and feature level, and apply the adversarial learning to make adversarial samples natural. Zhao et al. (2018) utilize generative adversarial networks to generate adversarial examples that lie on the data manifold by searching in the semantic space of dense and continuous data representations. Ebrahimi et al. (2018) propose an attack framework for character-level NMT, which uses gradient to rank adversarial manipulations and to search for adversarial examples via either greedy search or beam search methods. Belinkov and Bisk (2018) attack character-l"
2021.acl-short.58,L16-1561,0,0.0230211,"take noises in target sentences into account. They generate adversarial samples for both source and target sentences. Their target-side adversarial examples are generated according to the attacked positions in corresponding source sentences, while their sourceside adversarial samples are generated by randomly sampling positions to attack. We improve their method by attacking the source side according to Data and Setup We conducted experiments on two translation tasks: English-Chinese and English-Japanese. Data for English-Chinese translation are from the United Nations English-Chinese corpus (Ziemski et al., 2016). We built the training/validate/test set for this task by randomly sampling 3M/2K/2K sentence pairs from the whole corpus. For the EnglishJapanese translation task, we aggregated the training set of KFTT (Neubig, 2011), JESC (Pryzant et al., 2018) and TED talks (Cettolo et al., 2012) as our training set, which consists of 3.9M sentence pairs. We evaluated our models on the validation set and test set of KFTT (Neubig, 2011). We split words into sub-word units with subword regularization (Kudo, 2018) and built a shared vocabulary of 32K subwords for both English-Chinese and English-Japanese. We"
2021.emnlp-main.203,P19-1602,0,0.0696387,"r. They use the attentional seq2seq framework to build a parse generator and a paraphrase generator. A two-stage generation process is used. In the first stage, they generate full parse trees from syntactic templates, and then produce final generations in the second stage. Both parse and paraphrase generator require parallel data for training. Significantly different from their method, our model based on conditional VAE is an unsupervised method that does not require any parallel data for training. Conditional Variational Autoencoder Our work is also related to syntax-infused text generation (Bao et al., 2019; Zhang et al., 2019). Their models use two variational autoencoders to introduce two latent variables which are designed to capture semantic and syntactic information. The variational autoencoder (VAE) network is proposed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully apply VAE in fluent sentence generation from a latent space. The conditional VAE is a modification of VAE to generate diverse images conditioned on certain attributes, e.g. generating different human faces given skin color (Sohn et al., 2015; Yan et al., 2016). Inspired by conditional VAE, w"
2021.emnlp-main.203,K16-1002,0,0.620027,"araphrase generator require parallel data for training. Significantly different from their method, our model based on conditional VAE is an unsupervised method that does not require any parallel data for training. Conditional Variational Autoencoder Our work is also related to syntax-infused text generation (Bao et al., 2019; Zhang et al., 2019). Their models use two variational autoencoders to introduce two latent variables which are designed to capture semantic and syntactic information. The variational autoencoder (VAE) network is proposed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully apply VAE in fluent sentence generation from a latent space. The conditional VAE is a modification of VAE to generate diverse images conditioned on certain attributes, e.g. generating different human faces given skin color (Sohn et al., 2015; Yan et al., 2016). Inspired by conditional VAE, we view the syntactic structure as the conditional attribute and adopt conditional VAE to generate syntactic paraphrases. Furthermore, to improve the syntactic controllability and semantic consistency of generated sentences, we use syntax controlling and cycle reconstruction objective functions"
2021.emnlp-main.203,P19-1599,0,0.217606,"g, China 2 College of Intelligence and Computing, Tianjin University, Tianjin, China 3 Lenovo Research AI Lab, Beijing, China {egyang,mingtongliu,yjzhang,jaxu,chenyf}@bjtu.edu.cn, dyxiong@tju.edu.cn, {yaomeng1,hucj1}@lenovo.com Abstract Recent years have witnessed that learning controllable paraphrase generation (CPG) with specPrevious works on syntactically controlled ified styles is attracting intense research interests, paraphrase generation heavily rely on largee.g., satisfying particular syntactic templates (Iyyer scale parallel paraphrase data that are not easet al., 2018) or exemplars (Chen et al., 2019; Kuily available for many languages and domains. mar et al., 2020). As CPG can produce diverse In this paper, we take this research direction to the extreme and investigate whether paraphrases by exposing syntactic control, it can it is possible to learn syntactically controlled be also employed for adversarial example generaparaphrase generation with non-parallel data. tion (Iyyer et al., 2018). We propose a syntactically-informed unsuperExisting syntactically controlled paraphrase netvised paraphrasing model based on conditional works (Iyyer et al., 2018) rely on large paraphrase variationa"
2021.emnlp-main.203,P19-1601,0,0.0794332,"and adopt conditional VAE to generate syntactic paraphrases. Furthermore, to improve the syntactic controllability and semantic consistency of generated sentences, we use syntax controlling and cycle reconstruction objective functions to fine-tune the model. Adversarial Example Generation To generate Controlled Text Generation Recent works on adversarial examples for NLP models, most precontrolled generation aim at controlling attributes vious works rely on injecting noise either at the such as sentiment (Hu et al., 2017; John et al., 2019; character level (Ebrahimi et al., 2018; Gao et al., Dai et al., 2019). These works use a categorical 2018) or at the word level by adding and deleting 2595 Figure 1: Architecture of the proposed syntactically-informed unsupervised paraphrasing model. Stage 1: Training a Conditional VAE model by reconstructing the input sentence given the sentence itself and its syntax structure. Here we simply take x = {x1 , x2 } as an example. Stage 2: Fine-tuning the model using novel objective functions. x, s, s0 (different from s), and y denote the input sentence, its syntactic structure, other syntactic structure, and output sentence, respectively. L∗ denote the loss terms"
2021.emnlp-main.203,D17-1091,0,0.0496685,"Missing"
2021.emnlp-main.203,P18-2006,0,0.0135699,"ucture as the conditional attribute and adopt conditional VAE to generate syntactic paraphrases. Furthermore, to improve the syntactic controllability and semantic consistency of generated sentences, we use syntax controlling and cycle reconstruction objective functions to fine-tune the model. Adversarial Example Generation To generate Controlled Text Generation Recent works on adversarial examples for NLP models, most precontrolled generation aim at controlling attributes vious works rely on injecting noise either at the such as sentiment (Hu et al., 2017; John et al., 2019; character level (Ebrahimi et al., 2018; Gao et al., Dai et al., 2019). These works use a categorical 2018) or at the word level by adding and deleting 2595 Figure 1: Architecture of the proposed syntactically-informed unsupervised paraphrasing model. Stage 1: Training a Conditional VAE model by reconstructing the input sentence given the sentence itself and its syntax structure. Here we simply take x = {x1 , x2 } as an example. Stage 2: Fine-tuning the model using novel objective functions. x, s, s0 (different from s), and y denote the input sentence, its syntactic structure, other syntactic structure, and output sentence, respect"
2021.emnlp-main.203,2020.emnlp-main.498,0,0.0467006,"Missing"
2021.emnlp-main.203,2020.acl-main.22,0,0.016592,"1 and 2). ESM-H denotes the percentage that generations follow given syntactic templates. KL-weight Original 0.1 0.3 0.5 0.7 1.0 1.3 1.5 BLEU-ref 31.13 22.71 22.46 22.37 22.43 22.09 21.87 21.41 BLEU-ori 100 53.89 50.55 48.66 47.07 44.49 41.96 39.29 ESM 56.5 70.8 73.9 75.7 77.1 78.3 79.9 80.7 S-BERT 0.845 0.750 0.738 0.731 0.723 0.707 0.691 0.673 Table 4: BLEU-ref, BLEU-ori, ESM, and S-BERT score with varying KL weights on the Quora test set. 4.1.6 Human Evaluation We also conducted human evaluation to measure paraphrase quality in a blind fashion. Following previous work (Iyyer et al., 2018; Goyal and Durrett, 2020), Three annotators were asked to evaluate the 100 randomly selected generations from the Quora test set according to a three-point scale scoring system: 0 denotes that the generated sentence is not a paraphrase at all; 1 means that the generated sentence is a paraphrase containing grammatical errors; 2 indicates that the generated sentence is a grammatically good paraphrase. Additionally, we also asked annotators to evaluate syntactic controllability (ESM-H): whether generations follow given syntactic templates. Table 3 shows the results of human evaluation which are somewhat consistent with t"
2021.emnlp-main.203,N18-1170,0,0.373524,"g intense research interests, paraphrase generation heavily rely on largee.g., satisfying particular syntactic templates (Iyyer scale parallel paraphrase data that are not easet al., 2018) or exemplars (Chen et al., 2019; Kuily available for many languages and domains. mar et al., 2020). As CPG can produce diverse In this paper, we take this research direction to the extreme and investigate whether paraphrases by exposing syntactic control, it can it is possible to learn syntactically controlled be also employed for adversarial example generaparaphrase generation with non-parallel data. tion (Iyyer et al., 2018). We propose a syntactically-informed unsuperExisting syntactically controlled paraphrase netvised paraphrasing model based on conditional works (Iyyer et al., 2018) rely on large paraphrase variational auto-encoder (VAE) which can genparallel data for training. Unfortunately, paraphrase erate texts in a specified syntactic structure. Particularly, we design a two-stage learning parallel corpora are not easily available for many method to effectively train the model using languages, and are expensive to build. Conversely, non-parallel data. The conditional VAE is non-parallel data is much easi"
2021.emnlp-main.203,P19-1041,0,0.020059,"ional VAE, we view the syntactic structure as the conditional attribute and adopt conditional VAE to generate syntactic paraphrases. Furthermore, to improve the syntactic controllability and semantic consistency of generated sentences, we use syntax controlling and cycle reconstruction objective functions to fine-tune the model. Adversarial Example Generation To generate Controlled Text Generation Recent works on adversarial examples for NLP models, most precontrolled generation aim at controlling attributes vious works rely on injecting noise either at the such as sentiment (Hu et al., 2017; John et al., 2019; character level (Ebrahimi et al., 2018; Gao et al., Dai et al., 2019). These works use a categorical 2018) or at the word level by adding and deleting 2595 Figure 1: Architecture of the proposed syntactically-informed unsupervised paraphrasing model. Stage 1: Training a Conditional VAE model by reconstructing the input sentence given the sentence itself and its syntax structure. Here we simply take x = {x1 , x2 } as an example. Stage 2: Fine-tuning the model using novel objective functions. x, s, s0 (different from s), and y denote the input sentence, its syntactic structure, other syntactic"
2021.emnlp-main.203,2020.coling-main.209,1,0.799898,"encoder to effectively model structure information and a syntax controlling learning objective to further improve syntactic controllability. Meanwhile, we also introduce a cycle reconstruction learning objective to preserve the semantics of the input sentence. • Experiments show that our model can successfully generate syntactically adversarial examples. By augmenting training data with such examples, we can improve the robustness of target neural models. 2 Related Work Paraphrase Generation The task of paraphrase generation has recently received significant attention (Li et al., 2018, 2019; Liu et al., 2020a). Previous works mainly explore supervised paraphrasing methods, which require large corpora of parallel sentences for training. Due to the lack of parallel data, unsupervised paraphrasing has become an emerging research direction (Miao et al., 2018; Liu et al., 2020c). However, these methods mainly rely on lexical changes to generate paraphrases. Compared to these approaches, our work focus primarily on the syntactically controlled paraphrase generation, which is able to generate a paraphrase according to a given syntactic structure. feature as a controlling signal. Different from them, we"
2021.emnlp-main.203,2020.acl-main.28,0,0.033019,"encoder to effectively model structure information and a syntax controlling learning objective to further improve syntactic controllability. Meanwhile, we also introduce a cycle reconstruction learning objective to preserve the semantics of the input sentence. • Experiments show that our model can successfully generate syntactically adversarial examples. By augmenting training data with such examples, we can improve the robustness of target neural models. 2 Related Work Paraphrase Generation The task of paraphrase generation has recently received significant attention (Li et al., 2018, 2019; Liu et al., 2020a). Previous works mainly explore supervised paraphrasing methods, which require large corpora of parallel sentences for training. Due to the lack of parallel data, unsupervised paraphrasing has become an emerging research direction (Miao et al., 2018; Liu et al., 2020c). However, these methods mainly rely on lexical changes to generate paraphrases. Compared to these approaches, our work focus primarily on the syntactically controlled paraphrase generation, which is able to generate a paraphrase according to a given syntactic structure. feature as a controlling signal. Different from them, we"
2021.emnlp-main.203,P14-5010,0,0.00322884,"nce and the reference sentence. For syntactic evaluation, we evaluated how often generated paraphrases completely conform to the target syntactic templates by computing the rate of exact syntactic match (ESM): a paraphrase g is deemed as an exact syntactic match to reference r only if the top three levels of its parse tree pg exactly matches those of pr . The tuning of all hyper-parameter was based on the BLEU-ref score on the validation set. 4.1.4 Implementation Details We parsed all sentences in the training set, the reference sentences in the validation and test set using Stanford CoreNLP (Manning et al., 2014). We used the Adam optimizer (Kingma and Ba, 2014) for optimization. For the training of Stage 1 and Stage 2, we set the learning rate to 5e-4 and 1e-4, respectively. The word embedding layer was initialized by the publicly available GloVe 300-dimensional 3 We used the paraphrase-distilroberta-base-v1, which is trained on large-scale paraphrase data. Available at: https: //public.ukp.informatik.tu-darmstadt.de/ reimers/sentence-transformers/v0.2/ 2599 embeddings.4 We adopted the tricks of KL annealing and word dropout following (Bowman et al., 2016). We set λres to 5, λbow to 0.5, λsc to 2.5,"
2021.emnlp-main.203,P02-1040,0,0.10998,"ference sentences from the ParaNMT-50M dataset to train unsupervised methods. The manually annotated 800 sentence pairs created by Chen et al. (2019) were used as our test set, and 500 for the development set. For the supervised method SCPN (Iyyer et al., 2018), we used their trained model.2 4.1.3 Evaluation Metrics We employed original sentences and syntactic templates (or full parse trees) obtained from references as input, which is convenient for evaluation. But in the application scenario, we can give any syntactic templates to the trained model. For semantic evaluation, we computed BLEU (Papineni et al., 2002) scores against the reference and original sentence, denoted as BLEU-ref and BLEU-ori, respectively. Addition2 https://github.com/miyyer/scpn ally, we used i-BLEU (Sun and Zhou, 2012) to measure the diversity of expressions. We also used the embedding-based evaluation method Sentence-BERT3 (Reimers and Gurevych, 2019) to evaluate the semantic similarity between the generated sentence and the reference sentence. For syntactic evaluation, we evaluated how often generated paraphrases completely conform to the target syntactic templates by computing the rate of exact syntactic match (ESM): a parap"
2021.emnlp-main.203,2020.tacl-1.22,0,0.0311238,"Missing"
2021.emnlp-main.203,D19-1410,0,0.0153167,"ics We employed original sentences and syntactic templates (or full parse trees) obtained from references as input, which is convenient for evaluation. But in the application scenario, we can give any syntactic templates to the trained model. For semantic evaluation, we computed BLEU (Papineni et al., 2002) scores against the reference and original sentence, denoted as BLEU-ref and BLEU-ori, respectively. Addition2 https://github.com/miyyer/scpn ally, we used i-BLEU (Sun and Zhou, 2012) to measure the diversity of expressions. We also used the embedding-based evaluation method Sentence-BERT3 (Reimers and Gurevych, 2019) to evaluate the semantic similarity between the generated sentence and the reference sentence. For syntactic evaluation, we evaluated how often generated paraphrases completely conform to the target syntactic templates by computing the rate of exact syntactic match (ESM): a paraphrase g is deemed as an exact syntactic match to reference r only if the top three levels of its parse tree pg exactly matches those of pr . The tuning of all hyper-parameter was based on the BLEU-ref score on the validation set. 4.1.4 Implementation Details We parsed all sentences in the training set, the reference s"
2021.emnlp-main.203,D18-1421,0,0.0181783,"we propose a novel tree encoder to effectively model structure information and a syntax controlling learning objective to further improve syntactic controllability. Meanwhile, we also introduce a cycle reconstruction learning objective to preserve the semantics of the input sentence. • Experiments show that our model can successfully generate syntactically adversarial examples. By augmenting training data with such examples, we can improve the robustness of target neural models. 2 Related Work Paraphrase Generation The task of paraphrase generation has recently received significant attention (Li et al., 2018, 2019; Liu et al., 2020a). Previous works mainly explore supervised paraphrasing methods, which require large corpora of parallel sentences for training. Due to the lack of parallel data, unsupervised paraphrasing has become an emerging research direction (Miao et al., 2018; Liu et al., 2020c). However, these methods mainly rely on lexical changes to generate paraphrases. Compared to these approaches, our work focus primarily on the syntactically controlled paraphrase generation, which is able to generate a paraphrase according to a given syntactic structure. feature as a controlling signal."
2021.emnlp-main.203,P17-1099,0,0.123846,"Missing"
2021.emnlp-main.203,P19-1332,0,0.0283947,"Missing"
2021.emnlp-main.203,D13-1170,0,0.0036219,". contradictory to each other. Usually, a smaller KL weight makes the autoencoder less “variational” but more “deterministic,” leading to a lower syntactic match but better content preservation. In this experiment, to trade-off the content preservation and syntactic controllability, we set the KL weight to 0.3. 4.2 Adversarial Example Generation We further examined the utility of controlled paraphrase generation for adversarial example generation. Following previous work (Iyyer et al., 2018), we evaluated our syntactically adversarial examples on the Stanford Sentiment Treebank Dataset (SST) (Socher et al., 2013). We generated 10 syntactically different paraphrases for each instance using the top 10 frequent syntactic templates and add them to the SST training set. Since we cannot generate a valid paraphrase for each syntactic template, we filtered generated paraphrases using a threshold (BLEU, 1-3gram) to remove nonsensical outputs. In this experiment, we set the threshold to 0.5. 4.2.1 Evaluation Metrics We evaluated this task with the following metrics: 1. Dev Failure (Failure). We assume a development instance x as a prediction failure if the original prediction is correct, but the prediction for"
2021.emnlp-main.203,P12-2008,0,0.0164569,"or the development set. For the supervised method SCPN (Iyyer et al., 2018), we used their trained model.2 4.1.3 Evaluation Metrics We employed original sentences and syntactic templates (or full parse trees) obtained from references as input, which is convenient for evaluation. But in the application scenario, we can give any syntactic templates to the trained model. For semantic evaluation, we computed BLEU (Papineni et al., 2002) scores against the reference and original sentence, denoted as BLEU-ref and BLEU-ori, respectively. Addition2 https://github.com/miyyer/scpn ally, we used i-BLEU (Sun and Zhou, 2012) to measure the diversity of expressions. We also used the embedding-based evaluation method Sentence-BERT3 (Reimers and Gurevych, 2019) to evaluate the semantic similarity between the generated sentence and the reference sentence. For syntactic evaluation, we evaluated how often generated paraphrases completely conform to the target syntactic templates by computing the rate of exact syntactic match (ESM): a paraphrase g is deemed as an exact syntactic match to reference r only if the top three levels of its parse tree pg exactly matches those of pr . The tuning of all hyper-parameter was base"
2021.emnlp-main.203,P15-1150,0,0.0609342,"Missing"
2021.emnlp-main.203,P19-1199,0,0.0582223,"17), machine translation (Zhou et al., 2019) input sentence and a different syntactic structure, and text summarization (Zhao et al., 2018). the model can generate a paraphrase according to ∗ Corresponding Authors. the given structure. 2594 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2594–2604 c November 7–11, 2021. 2021 Association for Computational Linguistics We evaluate SUP on both syntactic paraphrase generation and adversarial example generation tasks. Experiments show that SUP outperforms previous unsupervised paraphrasing method SIVAE (Zhang et al., 2019). It is also capable of generating syntactically adversarial examples that have a significant impact on the performance of attacked neural models. We further show that augmenting training data with such examples can improve the robustness of target neural models. In summary, the major contributions of this paper are as follows: • We propose a syntactically-informed unsupervised paraphrasing model based on conditional VAE framework and use it to generate syntactically adversarial examples. • To enable the model to generate syntacticallycontrolled paraphrases, we propose a novel tree encoder to"
2021.emnlp-main.203,D18-1355,0,0.0241989,"rent surface realization. and semantic consistency of generated sentences, Paraphrase generation (PG) is a key technology of we fine-tune the model trained at stage 1 using automatically generating a restatement for a given carefully-designed objective functions involving text, which has the potential use in many down- syntax controlling and cycle reconstruction. After stream tasks, such as question answering (Dong the conditional VAE model is fine-tuned, given an et al., 2017), machine translation (Zhou et al., 2019) input sentence and a different syntactic structure, and text summarization (Zhao et al., 2018). the model can generate a paraphrase according to ∗ Corresponding Authors. the given structure. 2594 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2594–2604 c November 7–11, 2021. 2021 Association for Computational Linguistics We evaluate SUP on both syntactic paraphrase generation and adversarial example generation tasks. Experiments show that SUP outperforms previous unsupervised paraphrasing method SIVAE (Zhang et al., 2019). It is also capable of generating syntactically adversarial examples that have a significant impact on the performance"
2021.emnlp-main.203,P19-2015,0,0.0219605,"conveying the At stage 2, to improve the syntactic controllability same meaning but with different surface realization. and semantic consistency of generated sentences, Paraphrase generation (PG) is a key technology of we fine-tune the model trained at stage 1 using automatically generating a restatement for a given carefully-designed objective functions involving text, which has the potential use in many down- syntax controlling and cycle reconstruction. After stream tasks, such as question answering (Dong the conditional VAE model is fine-tuned, given an et al., 2017), machine translation (Zhou et al., 2019) input sentence and a different syntactic structure, and text summarization (Zhao et al., 2018). the model can generate a paraphrase according to ∗ Corresponding Authors. the given structure. 2594 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2594–2604 c November 7–11, 2021. 2021 Association for Computational Linguistics We evaluate SUP on both syntactic paraphrase generation and adversarial example generation tasks. Experiments show that SUP outperforms previous unsupervised paraphrasing method SIVAE (Zhang et al., 2019). It is also capable of g"
2021.emnlp-main.252,2020.acl-main.194,0,0.145447,"icult sample classifica- used for oversampling, too (Fu et al., 2018; Guo tion. How can we avoid this? et al., 2018; Nie et al., 2019). One advantage of To address these problems, we propose a these methods is that generated texts still follow the Mutual Information constrained Semantically original data distribution. Kang et al. (2020) proOversampling (MISO) approach with three essen- pose a long-tailed learning approach (τ -norm and tial components: a semantic fusion module (SFM), cRT) to separate representation learning and classia mutual information (MI) loss, and a coupled fier training. Chen et al. (2020) introduce MixText 3149 with TMix, a data augmentation method similar to Mixup used in computer vision, to interpolate new points in their corresponding hidden space. Difficult Sample Modeling in NLP Lin et al. (2017) propose a soft sampling method that dynamically adjusts the weights of difficult samples by redefining the loss function. Dice loss that optimizes the Sørensen–Dice coefficient to immune the imbalance issue has also been proposed (Li et al., 2020). Glazkova (2020) introduces ADASYN to assign a weight for each minority instance. Difficult Sample Modeling in CV Difficult sample lea"
2021.emnlp-main.252,W00-0719,0,0.0315806,"e restores the balance of the In order to make this generation framework fea- class distribution by either undersampling the masible, we should answer the following three ques- jority class or oversampling the minority class (Han tions: (1) Given a difficult sample paired with a et al., 2005; Chawla et al., 2002; Cao et al., 2019). majority sample, how can we capture their entan- Cost-sensitive methods estimate the cost of samgled semantics? (2) How can we decouple and ples with a cost matrix and train the classifier with inject semantics from and into an anchor instance? different penalties (Gomez et al., 2000; McBride (3) Merging anchor instances into the original data et al., 2019). Additionally, text style transfer with may change the data distribution and hence have a generative adversarial networks (GANs) has been negative impact on non-difficult sample classifica- used for oversampling, too (Fu et al., 2018; Guo tion. How can we avoid this? et al., 2018; Nie et al., 2019). One advantage of To address these problems, we propose a these methods is that generated texts still follow the Mutual Information constrained Semantically original data distribution. Kang et al. (2020) proOversampling (MIS"
2021.emnlp-main.252,2020.emnlp-main.545,0,0.0607627,"Missing"
2021.emnlp-main.306,L18-1431,0,0.0752281,"Missing"
2021.emnlp-main.306,C16-1167,0,0.0292639,"candidate choices for models. LAMBADA (Paperno et al., 2016) masks the last word in a target sentence and evaluates the ability of models in predicting the masked target words with broader context beyond target sentences in novels. Winograd Schema Challenge (WSC) (Levesque et al., 2012) and WinoGrande (Sakaguchi et al., 2020) defines a word selection task that focuses on solving commonsense problems in the form of coreference resolution. Details on the differences of Chinese WPLC from previous related datasets are shown in Table 1. In Chinese, People Daily (PD) & Children’s Fairy Tale (CFT) (Cui et al., 2016) corpus is the first cloze-style reading comprehension dataset in chinese. ChID (Zheng et al., 2019) offers an interesting task where words to be predicted are all idioms. CLUEWSC2020 (Xu et al., 2020), a Chinese version of WSC dataset, aims to test the ability of coreference resolution via word prediction. Significantly different from such Chinese datasets, our dataset is specifically developed for evaluating word prediction from long-range context. 3 3.1 Dataset Creation Passage Collection Book Topics Romance Fantasy Urban Supernatural Comprehension Rebirth Science Fiction Horror Suspense Hi"
2021.emnlp-main.306,N19-1423,0,0.0325943,"Missing"
2021.emnlp-main.306,D19-1249,1,0.898753,"Missing"
2021.emnlp-main.306,D16-1241,0,0.0709712,"Missing"
2021.emnlp-main.306,P16-1144,0,0.038703,"EN EN EN EN EN ZH Table 1: Comparison between our dataset and other datasets. AC: Automatically Chosen. RNNF: Filtering by RNN, MC: manual check. PLMF: Filtering by pretrained language models. performance, indicating a large space for further research. 2 Related Work CNN/Daily Mail (Hermann et al., 2015) uses an automatic method to create a large amount of instances of replacing entities with placeholders in news. Children’s Book Test (CBT) (Felix et al., 2016) removes four types of words that are expected to be predicted by evaluated models and provides candidate choices for models. LAMBADA (Paperno et al., 2016) masks the last word in a target sentence and evaluates the ability of models in predicting the masked target words with broader context beyond target sentences in novels. Winograd Schema Challenge (WSC) (Levesque et al., 2012) and WinoGrande (Sakaguchi et al., 2020) defines a word selection task that focuses on solving commonsense problems in the form of coreference resolution. Details on the differences of Chinese WPLC from previous related datasets are shown in Table 1. In Chinese, People Daily (PD) & Children’s Fairy Tale (CFT) (Cui et al., 2016) corpus is the first cloze-style reading com"
2021.emnlp-main.306,P19-1075,0,0.015818,"ence and evaluates the ability of models in predicting the masked target words with broader context beyond target sentences in novels. Winograd Schema Challenge (WSC) (Levesque et al., 2012) and WinoGrande (Sakaguchi et al., 2020) defines a word selection task that focuses on solving commonsense problems in the form of coreference resolution. Details on the differences of Chinese WPLC from previous related datasets are shown in Table 1. In Chinese, People Daily (PD) & Children’s Fairy Tale (CFT) (Cui et al., 2016) corpus is the first cloze-style reading comprehension dataset in chinese. ChID (Zheng et al., 2019) offers an interesting task where words to be predicted are all idioms. CLUEWSC2020 (Xu et al., 2020), a Chinese version of WSC dataset, aims to test the ability of coreference resolution via word prediction. Significantly different from such Chinese datasets, our dataset is specifically developed for evaluating word prediction from long-range context. 3 3.1 Dataset Creation Passage Collection Book Topics Romance Fantasy Urban Supernatural Comprehension Rebirth Science Fiction Horror Suspense Historical Military Detective Mystery Modern Others Total Nums 22,292 12,190 5,277 4,624 3,067 3,023 2"
2021.findings-acl.224,N19-1006,0,0.0209552,"cs: ACL-IJCNLP 2021, pages 2539–2545 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Diagram of the proposed AdaST that dynamically adapts acoustic encoder states in the decoder for end-to-end ST. analyses disclose that dynamically adaptive acoustic representations are more desirable than static acoustic states for end-to-end ST. 2 Related Work Berard et al. (2016) demonstrate the potential of end-to-end neural ST and Weiss et al. (2017) achieve good performance by using an end-to-end neural architecture, which trigger more research interests in end-to-end ST. Both Bansal et al. (2019) and Stoian et al. (2020) train a speech recognition model first and then use the encoder of ASR to initialize the encoder of speech translation. Jia et al. (2019) synthesize training data for end-toend speech translation from MT and ASR dataset. Gangi et al. (2019); Inaguma et al. (2019) adapt the idea of multilingual machine translation to speech translation. In addition to these methods, Bahar et al. (2019) use phoneme-level representations instead of speech frame-level representations as input, greatly reducing the length of acoustic sequences. Knowledge distillation (Liu et al., 2019), me"
2021.findings-acl.224,2004.iwslt-evaluation.13,0,0.0834766,"d the window size as 25ms. We deleted sentences with frame size larger than 3000 and sentences with poor alignments. Following Wang et al. (2020a), we adopted speed perturbation with factors 0.9 and 1.1. To further reduce overfitting, we used SpecAugment strategy (Bahar et al., 2019). In Librispeech, we used subword level decoding, which was performed via SentencePiece with a size of 1K tokens. In IWSLT18, we performed character level decoding. As the tst2013 of IWSLT2018 is not aligned, we employed Espnets default LIUM SpkDiarization tool to segment each audio sequence. We used RWTH toolkit (Bender et al., 2004) to calculate BLEU scores (Papineni et al., 2002). A two-layer CNN was taken in the speech encoder. The step size was set to 2. The size of the convolution kernel was 2 × 2. The dimension of the attention was set to 256. We used 12-layer encoder. The number of decoder layers in both the baseline and AdaST was set to 10. We used the Adam optimizer (Kingma and Ba, 2015) and run our models on four P100 GPUs. 4.3 Main Results In order to make each layer of the decoder to interact with acoustic states, our model requires additional computational overhead. However, the conventional source-to-target"
2021.findings-acl.224,2020.acl-demos.34,0,0.0184849,"consistent with their settings, we also doubled the training data by concatenating the aligned references with pseudo translations by the Google Translate. IWSLT18 English-German. The IWSLT18 speech translation dataset is from TED Talks, which contains 271 hour speech with 171K corresponding English transcripts and German translations. As there is no validation set in this dataset, we randomly sampled 2000 samples from the training data as our validation set. Following Wang et al. 2541 (2020b), we used tst2013 as the test set. 4.2 En-Fr Settings We built our model based on the Espnet toolkit (Inaguma et al., 2020). On the two datasets, we extracted 80-dimensional Fbank features from audio files, setting the step size as 10ms and the window size as 25ms. We deleted sentences with frame size larger than 3000 and sentences with poor alignments. Following Wang et al. (2020a), we adopted speed perturbation with factors 0.9 and 1.1. To further reduce overfitting, we used SpecAugment strategy (Bahar et al., 2019). In Librispeech, we used subword level decoding, which was performed via SentencePiece with a size of 1K tokens. In IWSLT18, we performed character level decoding. As the tst2013 of IWSLT2018 is not"
2021.findings-acl.224,P02-1040,0,0.113766,"with frame size larger than 3000 and sentences with poor alignments. Following Wang et al. (2020a), we adopted speed perturbation with factors 0.9 and 1.1. To further reduce overfitting, we used SpecAugment strategy (Bahar et al., 2019). In Librispeech, we used subword level decoding, which was performed via SentencePiece with a size of 1K tokens. In IWSLT18, we performed character level decoding. As the tst2013 of IWSLT2018 is not aligned, we employed Espnets default LIUM SpkDiarization tool to segment each audio sequence. We used RWTH toolkit (Bender et al., 2004) to calculate BLEU scores (Papineni et al., 2002). A two-layer CNN was taken in the speech encoder. The step size was set to 2. The size of the convolution kernel was 2 × 2. The dimension of the attention was set to 256. We used 12-layer encoder. The number of decoder layers in both the baseline and AdaST was set to 10. We used the Adam optimizer (Kingma and Ba, 2015) and run our models on four P100 GPUs. 4.3 Main Results In order to make each layer of the decoder to interact with acoustic states, our model requires additional computational overhead. However, the conventional source-to-target attention network in Transformer is subsumed in t"
2021.findings-acl.224,2020.acl-main.344,0,0.129928,"first and then use the encoder of ASR to initialize the encoder of speech translation. Jia et al. (2019) synthesize training data for end-toend speech translation from MT and ASR dataset. Gangi et al. (2019); Inaguma et al. (2019) adapt the idea of multilingual machine translation to speech translation. In addition to these methods, Bahar et al. (2019) use phoneme-level representations instead of speech frame-level representations as input, greatly reducing the length of acoustic sequences. Knowledge distillation (Liu et al., 2019), meta-learning (Indurthi et al., 2019), curriculum learning (Wang et al., 2020b), and two-pass decoding (Sung et al., 2019), have also been studied in end-to-end speech translation. To solve the cross-modal and cross-lingual challenges of end-to-end speech translation, Wang et al. (2020a) and Dong et al. (2020) propose to use submodules to separately analyze cross-modal and cross-lingual problems in end-to-end ST. Each module introduced solves one problem. Unfortunately, they introduce a large number of extra parameters and rely on a large amount of external data to pre-train each submodule. In contrast, we do not introduce any additional submodules and therefore we do"
2021.findings-acl.383,P12-1027,0,0.02912,"d reduces the complexity of implementation, inference time, and memory consumption. • We empirically investigate the effectiveness of the proposed method to downstream Chinese NLP tasks and analyze the impact of segmentation results on them via extrinsic evaluations. 2 Related Work Since Xue (2003) formalizes CWS as a sequence labeling problem, many traditional statistical methods have achieved high performance for CWS on several benchmarks (Emerson, 2005). According to (Huang and Zhao, 2007) and (Zhao et al., 2019), CRF-based models (Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013) and neural methods (Zheng et al., 2013; 4370 3 Proposed Framework Aiming at not only keeping competitive performance on benchmarks but also reducing the complexity of the CWS methods, our proposed framework consists of two essential modules: a student model and a teacher model, as shown in Figure 1. There is an obvious performance gap between the model based on PLMs (Huang et al., 2020) and the lightweight model (Duan and Zhao, 2020). The Teacher Model RoBERTa Encoder B Softmax Linear Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Cai et al., 2017) have been rep"
2021.findings-acl.383,2020.acl-main.734,0,0.448392,"nihao 1 processing task for many NLP tasks. In this aspect, Chinese word segmentation (CWS) is widely acknowledged as an essential task for Chinese NLP. CWS has made substantial progress in recent studies on several benchmarks, which is reported by Huang and Zhao (2007) and Zhao et al. (2019). In particular, pretrained language models (PLMs), like BERT (Devlin et al., 2019), have established new state-of-the-art in sequence labeling (Meng et al., 2019). Various fine-tuning methods have been proposed to improve the performance of indomain and cross-domain CWS based on PLMs (Huang et al., 2020; Tian et al., 2020). The two challenging problems in CWS, segmentation ambiguity and out-of-vocabulary (OOV) words, have been significantly mitigated by PLM-based methods that are fine-tuned on large-scale annotated CWS corpora. Such methods are even reaching human performance on benchmarks. Nevertheless, CWS is more valuable as a prelude for downstream NLP tasks than as a standalone task. Intrinsic evaluation of CWS on benchmark datasets only examines the effectiveness of current neural methods on word boundary detection. To better apply CWS in downstream NLP tasks, we should comprehensively re-think CWS from t"
2021.findings-acl.383,I05-3027,0,0.129456,"decoding is thus fast for practical application. Our method reduces the complexity of implementation, inference time, and memory consumption. • We empirically investigate the effectiveness of the proposed method to downstream Chinese NLP tasks and analyze the impact of segmentation results on them via extrinsic evaluations. 2 Related Work Since Xue (2003) formalizes CWS as a sequence labeling problem, many traditional statistical methods have achieved high performance for CWS on several benchmarks (Emerson, 2005). According to (Huang and Zhao, 2007) and (Zhao et al., 2019), CRF-based models (Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013) and neural methods (Zheng et al., 2013; 4370 3 Proposed Framework Aiming at not only keeping competitive performance on benchmarks but also reducing the complexity of the CWS methods, our proposed framework consists of two essential modules: a student model and a teacher model, as shown in Figure 1. There is an obvious performance gap between the model based on PLMs (Huang et al., 2020) and the lightweight model (Duan and Zhao, 2020). The Teacher Model RoBERTa Encoder B Softmax Linear Pei et al., 2014; Chen et al.,"
2021.findings-acl.383,O03-4002,0,0.318519,"er strong baselines for CWS by the traditional intrinsic evaluation. • The lightweight student can be deployed on a small-scale device, even in a non-GPU environment. We abandon the PLM neural architectures (teacher model) during decoding. The speed of decoding is thus fast for practical application. Our method reduces the complexity of implementation, inference time, and memory consumption. • We empirically investigate the effectiveness of the proposed method to downstream Chinese NLP tasks and analyze the impact of segmentation results on them via extrinsic evaluations. 2 Related Work Since Xue (2003) formalizes CWS as a sequence labeling problem, many traditional statistical methods have achieved high performance for CWS on several benchmarks (Emerson, 2005). According to (Huang and Zhao, 2007) and (Zhao et al., 2019), CRF-based models (Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013) and neural methods (Zheng et al., 2013; 4370 3 Proposed Framework Aiming at not only keeping competitive performance on benchmarks but also reducing the complexity of the CWS methods, our proposed framework consists of two essential modules: a student model and"
2021.findings-acl.383,I08-4017,0,0.0369771,"st for practical application. Our method reduces the complexity of implementation, inference time, and memory consumption. • We empirically investigate the effectiveness of the proposed method to downstream Chinese NLP tasks and analyze the impact of segmentation results on them via extrinsic evaluations. 2 Related Work Since Xue (2003) formalizes CWS as a sequence labeling problem, many traditional statistical methods have achieved high performance for CWS on several benchmarks (Emerson, 2005). According to (Huang and Zhao, 2007) and (Zhao et al., 2019), CRF-based models (Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013) and neural methods (Zheng et al., 2013; 4370 3 Proposed Framework Aiming at not only keeping competitive performance on benchmarks but also reducing the complexity of the CWS methods, our proposed framework consists of two essential modules: a student model and a teacher model, as shown in Figure 1. There is an obvious performance gap between the model based on PLMs (Huang et al., 2020) and the lightweight model (Duan and Zhao, 2020). The Teacher Model RoBERTa Encoder B Softmax Linear Pei et al., 2014; Chen et al., 2015; Cai and Zhao,"
2021.findings-acl.383,D13-1061,0,0.0913855,"Missing"
2021.findings-acl.383,D17-1079,0,0.10168,"standalone task. Intrinsic evaluation of CWS on benchmark datasets only examines the effectiveness of current neural methods on word boundary detection. To better apply CWS in downstream NLP tasks, we should comprehensively re-think CWS from the perspective of practicability. In this paper, we define the practicability of CWS with two aspects: low complexity as a standalone task and high beneficiality to downstream tasks. The complexity is twofold: 1) complexity of implementation and 2) time and space complexity of a CWS algorithm. Previous neural methods usually require additional resources (Zhou et al., 2017; Ma et al., 2018; Zhang et al., 2018b; Zhao et al., 2018; Yang et al., 2019; Qiu et al., 2020), such as external pre-trained embeddings. The complexity of implementation is reflected in the difficulty of acquiring external resources. External resources 4369 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4369–4381 August 1–6, 2021. ©2021 Association for Computational Linguistics vary in quality and the length of time for computation, For example, it is time-consuming to obtain effective pre-trained embeddings as they are trained on a huge amount of data. Gene"
2021.findings-acl.383,N19-1278,0,0.0772693,"ines the effectiveness of current neural methods on word boundary detection. To better apply CWS in downstream NLP tasks, we should comprehensively re-think CWS from the perspective of practicability. In this paper, we define the practicability of CWS with two aspects: low complexity as a standalone task and high beneficiality to downstream tasks. The complexity is twofold: 1) complexity of implementation and 2) time and space complexity of a CWS algorithm. Previous neural methods usually require additional resources (Zhou et al., 2017; Ma et al., 2018; Zhang et al., 2018b; Zhao et al., 2018; Yang et al., 2019; Qiu et al., 2020), such as external pre-trained embeddings. The complexity of implementation is reflected in the difficulty of acquiring external resources. External resources 4369 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4369–4381 August 1–6, 2021. ©2021 Association for Computational Linguistics vary in quality and the length of time for computation, For example, it is time-consuming to obtain effective pre-trained embeddings as they are trained on a huge amount of data. Generally, it is difficult to maintain high CWS performance for many previous ne"
2021.findings-acl.383,D13-1031,0,0.0269367,"lexity of implementation, inference time, and memory consumption. • We empirically investigate the effectiveness of the proposed method to downstream Chinese NLP tasks and analyze the impact of segmentation results on them via extrinsic evaluations. 2 Related Work Since Xue (2003) formalizes CWS as a sequence labeling problem, many traditional statistical methods have achieved high performance for CWS on several benchmarks (Emerson, 2005). According to (Huang and Zhao, 2007) and (Zhao et al., 2019), CRF-based models (Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013) and neural methods (Zheng et al., 2013; 4370 3 Proposed Framework Aiming at not only keeping competitive performance on benchmarks but also reducing the complexity of the CWS methods, our proposed framework consists of two essential modules: a student model and a teacher model, as shown in Figure 1. There is an obvious performance gap between the model based on PLMs (Huang et al., 2020) and the lightweight model (Duan and Zhao, 2020). The Teacher Model RoBERTa Encoder B Softmax Linear Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Cai et al., 2017) have been reported to outperform t"
2021.findings-acl.383,Y98-1020,0,0.280737,"Missing"
2021.findings-emnlp.396,P19-1425,0,0.0197578,"E and Secoco-Edit is very close. Therefore, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study tran"
2021.findings-emnlp.396,2020.acl-main.529,0,0.0281867,"use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al."
2021.findings-emnlp.396,C18-1055,0,0.0198205,"ng findings. First, the performance of Secoco-E2E and Secoco-Edit is very close. Therefore, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors."
2021.findings-emnlp.396,W18-6317,0,0.0137347,"real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw"
2021.findings-emnlp.396,W18-1807,0,0.102508,"Missing"
2021.findings-emnlp.396,W18-6453,0,0.0172586,"Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a recon"
2021.findings-emnlp.396,D19-5506,0,0.0151559,"a for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture to deal with natural noise for NMT. Different from ours, most of these works use the synthetic data in a coarsegrained and implicit way (i.e. simply combining the synthetic and raw data). As described in Section 2.3, w"
2021.findings-emnlp.396,P19-1291,0,0.0169515,"l., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture to deal with natural noise for NMT. Different from ours, most of these works use the synthetic data in a coarsegrained and implicit way (i.e. simply combining the synthetic and raw data). As described in Section 2.3, we iteratively edit the input until the input is unchanged and the"
2021.findings-emnlp.396,N19-1314,0,0.0159315,"very close. Therefore, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caus"
2021.findings-emnlp.396,N19-4009,0,0.0150201,"hod enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k separately. For evaluation, we used the standard Sacrebleu (Post, 2018) to calculate BLEU-4. All models were implemented based on Fairseq (Ott et al., 2019). 3.4 Results Table 2 shows the translation results on Dialogue, Speech and WMT En-De. Clearly, all competitors substantially improve the baseline model in terms of BLEU. Secoco achieves the best performance on all three test sets, gaining improvements of 2.2, 0.7, and 0.4 BLEU-4 points over BASE+synthetic respectively. The improvements suggest the effectiveness of self-correcting encoding. It is worth noting that the BLEU scores here are results on noisy test sets, so they are certainly lower 4641 Dialogue BLEU 4 Methods Speech BLEU 4 WMT En-De BLEU 4 AVG BLEU 4 Latency (ms/sent) BASE BASE +s"
2021.findings-emnlp.396,W18-6319,0,0.0117715,"roduce an additional decoder to obtain clean inputs from noisy inputs. This method enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k separately. For evaluation, we used the standard Sacrebleu (Post, 2018) to calculate BLEU-4. All models were implemented based on Fairseq (Ott et al., 2019). 3.4 Results Table 2 shows the translation results on Dialogue, Speech and WMT En-De. Clearly, all competitors substantially improve the baseline model in terms of BLEU. Secoco achieves the best performance on all three test sets, gaining improvements of 2.2, 0.7, and 0.4 BLEU-4 points over BASE+synthetic respectively. The improvements suggest the effectiveness of self-correcting encoding. It is worth noting that the BLEU scores here are results on noisy test sets, so they are certainly lower 4641 Dialogue BL"
2021.findings-emnlp.396,P18-2037,0,0.0182766,"s potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthe"
2021.findings-emnlp.396,P16-1162,0,0.0421259,"et al. (2019) to develop a multi-task based method to solve the robustness problem. We construct triples (clean input, noisy input, target translation), and introduce an additional decoder to obtain clean inputs from noisy inputs. This method enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k separately. For evaluation, we used the standard Sacrebleu (Post, 2018) to calculate BLEU-4. All models were implemented based on Fairseq (Ott et al., 2019). 3.4 Results Table 2 shows the translation results on Dialogue, Speech and WMT En-De. Clearly, all competitors substantially improve the baseline model in terms of BLEU. Secoco achieves the best performance on all three test sets, gaining improvements of 2.2, 0.7, and 0.4 BLEU-4 points over BASE+synthetic respectively. The improvements suggest the effectiveness"
2021.findings-emnlp.396,N19-1190,0,0.0237898,"et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture to deal with natural noise for NMT. Different from ours, most of these works use the synthetic data in a coarsegrained and implicit way (i.e. simply combining the synthetic and raw data). As described in Section 2.3, we iteratively edit the input until the input is unchanged and then translate it. We present examples in Table 3. We 5 Conclusions can see that m"
2021.findings-emnlp.396,P19-1583,0,0.0203659,"ss of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two deco"
2021.findings-emnlp.396,D17-1319,0,0.0238081,"tion. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2"
2021.findings-emnlp.396,D18-1316,0,0.0167893,"e, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recogn"
2021.findings-emnlp.396,W19-5368,0,0.0796658,"ethod against the following three baseline systems. BASE One widely-used way to achieve NMT robustness is to mix raw clean data with noisy data to train NMT models. We refer to models trained with/without synthetic data as BASE/BASE+synthetic. R EPAIR To deal with noisy inputs, one might train a repair model to transform noisy inputs into clean inputs that a normally trained translation model can deal with. Both the repair and translation model are transformer-based models. As a pipeline model (repairing before translating), R EPAIR may suffer from error propagation. R ECONSTRUCTION We follow Zhou et al. (2019) to develop a multi-task based method to solve the robustness problem. We construct triples (clean input, noisy input, target translation), and introduce an additional decoder to obtain clean inputs from noisy inputs. This method enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al"
2021.findings-emnlp.396,D17-1147,0,0.060923,"Missing"
2021.findings-emnlp.396,P18-2048,0,0.0159752,"y divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture t"
2021.findings-emnlp.396,2021.naacl-industry.14,1,0.749888,"-Edit), as illustrated in the right part of Figure 2. In general, Secoco-E2E provides better robustness without sacrificing decoding speed. For Secoco-Edit, iterative editing enables better interpretability. Detailed editing operations provide a different perspective on how the model resists noise. 3 Experiments 3.1 We conducted our experiments on three test sets, including Dialogue, Speech, and WMT14 En-De, to examine the effectiveness of Secoco. Dialogue is a real-world Chinese-English dialogue test set constructed based on TV drama subtitles1 , which contains three types of natural noises (Wang et al., 2021). Speech is an in-house ChineseEnglish speech translation test set which contains various noise from ASR. To evaluate Secoco on different language pairs, we also used WMT14 EnDe test sets to build a noisy test set with random deletion and insertion operations. Table 1 shows the details of the three test sets. For Chinese-English translation, we used WMT2020 Chinese-English data2 (48M) for Dialogue, and CCMT3 (9M) for Speech. For WMT En-De, we adopted the widely-used WMT14 training data4 (4.5M). We synthesized corresponding 1 https://github.com/rgwt123/DialogueMT http://www.statmt.org/wmt20/tra"
2021.findings-emnlp.396,P19-1123,0,0.0345329,"Missing"
2021.findings-emnlp.396,W18-6314,0,0.0205705,"y divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture t"
2021.findings-emnlp.67,P19-1285,0,0.0562935,"Missing"
2021.findings-emnlp.67,2020.acl-main.269,0,0.0157973,"iativity property of matrix products. Wang et al. (2020) approximate the self-attention mechanism by a low-rank matrix. Beltagy et al. (2020) introduce an attention mechanism that scales linearly with sequence length. Child et al. (2019) introduce sparse factorizations of the attention matrix. On using hard (local) attention for machine translation, Luong et al. (2015) selectively focus on a small window of context smoothed by a Gaussian distribution. For self-attentional sentence encoding, Shen et al. (2018) train hard attention mechanisms which select a subset of tokens via policy gradient. Geng et al. (2020) investigate selective self-attention networks implemented with GumbleSigmoid. Sparse attention has been found benefitial for performance (Malaviya et al., 2018; Peters et al., 2019; Correia et al., 2019; Indurthi et al., 2019; Maruf et al., 2019). Our approach learns explicit one-to-one attention for efficiency, pushing such research efforts to the limit. 6 Conclusion efficient retrieval operation. In our experiments on a wide range of machine translation tasks, we show that using the hard retrieval attention for decoder attention networks can achieve competitive performance while being 1.43"
2021.findings-emnlp.67,P19-1290,0,0.017011,"et al. (2019) introduce sparse factorizations of the attention matrix. On using hard (local) attention for machine translation, Luong et al. (2015) selectively focus on a small window of context smoothed by a Gaussian distribution. For self-attentional sentence encoding, Shen et al. (2018) train hard attention mechanisms which select a subset of tokens via policy gradient. Geng et al. (2020) investigate selective self-attention networks implemented with GumbleSigmoid. Sparse attention has been found benefitial for performance (Malaviya et al., 2018; Peters et al., 2019; Correia et al., 2019; Indurthi et al., 2019; Maruf et al., 2019). Our approach learns explicit one-to-one attention for efficiency, pushing such research efforts to the limit. 6 Conclusion efficient retrieval operation. In our experiments on a wide range of machine translation tasks, we show that using the hard retrieval attention for decoder attention networks can achieve competitive performance while being 1.43 times faster in decoding. Acknowledgements We thank our anonymous reviewers for their insightful comments. Hongfei Xu acknowledges the support of China Scholarship Council ([2018]3101, 201807040056). Josef van Genabith and Hon"
2021.findings-emnlp.67,D15-1166,0,0.0624278,"standard residuals. Zhang et al. (2020) propose a dimension-wise attention mechanism to reduce the attention complexity. Katharopoulos et al. (2020) express the selfattention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products. Wang et al. (2020) approximate the self-attention mechanism by a low-rank matrix. Beltagy et al. (2020) introduce an attention mechanism that scales linearly with sequence length. Child et al. (2019) introduce sparse factorizations of the attention matrix. On using hard (local) attention for machine translation, Luong et al. (2015) selectively focus on a small window of context smoothed by a Gaussian distribution. For self-attentional sentence encoding, Shen et al. (2018) train hard attention mechanisms which select a subset of tokens via policy gradient. Geng et al. (2020) investigate selective self-attention networks implemented with GumbleSigmoid. Sparse attention has been found benefitial for performance (Malaviya et al., 2018; Peters et al., 2019; Correia et al., 2019; Indurthi et al., 2019; Maruf et al., 2019). Our approach learns explicit one-to-one attention for efficiency, pushing such research efforts to the l"
2021.findings-emnlp.67,P18-2059,0,0.0427911,"Missing"
2021.findings-emnlp.67,N19-1313,0,0.0213063,"e sparse factorizations of the attention matrix. On using hard (local) attention for machine translation, Luong et al. (2015) selectively focus on a small window of context smoothed by a Gaussian distribution. For self-attentional sentence encoding, Shen et al. (2018) train hard attention mechanisms which select a subset of tokens via policy gradient. Geng et al. (2020) investigate selective self-attention networks implemented with GumbleSigmoid. Sparse attention has been found benefitial for performance (Malaviya et al., 2018; Peters et al., 2019; Correia et al., 2019; Indurthi et al., 2019; Maruf et al., 2019). Our approach learns explicit one-to-one attention for efficiency, pushing such research efforts to the limit. 6 Conclusion efficient retrieval operation. In our experiments on a wide range of machine translation tasks, we show that using the hard retrieval attention for decoder attention networks can achieve competitive performance while being 1.43 times faster in decoding. Acknowledgements We thank our anonymous reviewers for their insightful comments. Hongfei Xu acknowledges the support of China Scholarship Council ([2018]3101, 201807040056). Josef van Genabith and Hongfei Xu are supported"
2021.findings-emnlp.67,P19-1146,0,0.0120926,"scales linearly with sequence length. Child et al. (2019) introduce sparse factorizations of the attention matrix. On using hard (local) attention for machine translation, Luong et al. (2015) selectively focus on a small window of context smoothed by a Gaussian distribution. For self-attentional sentence encoding, Shen et al. (2018) train hard attention mechanisms which select a subset of tokens via policy gradient. Geng et al. (2020) investigate selective self-attention networks implemented with GumbleSigmoid. Sparse attention has been found benefitial for performance (Malaviya et al., 2018; Peters et al., 2019; Correia et al., 2019; Indurthi et al., 2019; Maruf et al., 2019). Our approach learns explicit one-to-one attention for efficiency, pushing such research efforts to the limit. 6 Conclusion efficient retrieval operation. In our experiments on a wide range of machine translation tasks, we show that using the hard retrieval attention for decoder attention networks can achieve competitive performance while being 1.43 times faster in decoding. Acknowledgements We thank our anonymous reviewers for their insightful comments. Hongfei Xu acknowledges the support of China Scholarship Council ([2018]31"
2021.findings-emnlp.67,P16-1162,0,0.124024,"Missing"
2021.findings-emnlp.67,2020.acl-main.38,1,0.893889,"Missing"
2021.findings-emnlp.67,2021.acl-long.23,1,0.827296,"Missing"
2021.findings-emnlp.67,2020.acl-main.323,1,0.895535,"Missing"
2021.findings-emnlp.67,D19-1083,0,0.040009,"Missing"
2021.findings-emnlp.67,P18-1166,1,0.83164,", using the same setting of the Transformer Base as on the WMT 14 En-De task. Results are shown in Table 5. Table 5 shows that hard retrieval attention is able to match the performance in all tested language pairs in both translation directions, with training sets ranging from 0.2M to 52.02M sentence pairs. The largest performance loss (−0.26 BLEU) is on the Cs-En task. decoder or both encoder and decoder. Results are shown in Table 3. Table 3 shows that applying the hard retrieval attention mechanism to encoder self-attention net- 5 Related Work works significantly hampers performance. We conZhang et al. (2018) accelerate the decoder selfjecture potential reasons might be: 1) the encoder might be harder to train than the decoder as its gra- attention with the average attention network. Xu et al. (2021) propose to replace the self-attention dients come from cross-attention networks while layer by multi-head highly parallelized LSTM. Kim the decoder receives more direct supervision from the classifier, and the hard attention training ap- et al. (2019) investigate knowledge distillation and proach makes the encoder’s training even harder. quantization for faster NMT decoding. Tay et al. (2021) investig"
2021.naacl-industry.14,W19-4822,0,0.012826,"gual dialogue. Recently WMT2020 has also proposed a new shared task - machine translation for chats,4 focusing on bilingual customer support chats (Farajian et al., 2020). Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019). 7 Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep evaluation, we construct dialogues with translation and detailed annotations as a benchmark test set. Our proposed model achieves substantial improvements over the baselines. What is more, we further analyze the performance of contextual labeling and pronoun recovery errors. Acknowledgments We thank the bilingual speakers for test set construc"
2021.naacl-industry.14,N19-1311,0,0.0204783,"y WMT2020 has also proposed a new shared task - machine translation for chats,4 focusing on bilingual customer support chats (Farajian et al., 2020). Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019). 7 Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep evaluation, we construct dialogues with translation and detailed annotations as a benchmark test set. Our proposed model achieves substantial improvements over the baselines. What is more, we further analyze the performance of contextual labeling and pronoun recovery errors. Acknowledgments We thank the bilingual speakers for test set construction, and the anonymous review"
2021.naacl-industry.14,P18-1163,0,0.0163769,"m dialogue this paper. inputs into forms that an ordinary NMT system We use a special token <sep&gt; as the separa- can deal with. REPAIR DIAL involves training a tor to concatenate sentences into a parallel sub- repair model to transform x0d to xd and a clean translation model that translates xd to yd . As a document {(xd , yd )}, as shown in Figure 1a. Contextual Perturbation We then consider gener- pipeline method, REPAIR DIAL may suffer from ating perturbation example x0d from xd with re- error propagation. spect to sub-document context. For ProDrop, ROBUST DIAL We extend the robust NMT 107 (Cheng et al., 2018) to dialogue-level translation. Specifically, we take both the original (xd , yd ) and the perturbated (x0d , yd ) bilingual pairs as training instances. So the model is more resilient on dialogue translation. During the inference stage, the robust model directly translates raw inputs into the target language. 3.3 MTL DIAL ROBUST DIAL has the potential to handle translation problems caused by noisy dialogue inputs. However, the internal mechanism is rather implicit and in a black box. Therefore, the improvement is limited, and it is not easy to analyze the improvement. To address this issue, w"
2021.naacl-industry.14,N19-1423,0,0.00610164,". As shown in  of Figure 1b, the only difference is that we have a contextual labeling module based on the encoder. We denote the final layer output of the Transformer encoder as H. For each token hi in H = (h1 , h2 , ..., hm ), the probability of contextual labeling is defined as: P (pi = j|X) = sof tmax(W · hi + b)[j] (1) where X = (x1 , x2 , ..., xm ) is the input sequence, P (pi = j|X) is the conditional probability that token xi is labeled as j (j ∈ 0, 1, 2, 3 as defined above). Here we make the labeling module as simple as possible, so that the Transformer encoder can behave like BERT (Devlin et al., 2019), learning more information related to perturbation and guiding the decoder to find desirable translations. During the training phrase, the model takes (xd , x0d , `x , `0x , yd ) as the training data. The learning process is driven by optimizing two objectives, corresponding to sequence labeling as auxiliary loss (LSL ) and machine translation as the primary loss (LM T ) in a multi-task learning framework. LSL = −log(P (`x |xd ) + P (`0x |x0d )) (2) LM T = −log(P (yd |xd ) + P (yd |x0d )) (3) where update_num is the number of updating steps during training. We introduce multi-task learning fo"
2021.naacl-industry.14,2020.wmt-1.3,0,0.0751299,"Missing"
2021.naacl-industry.14,P14-2047,0,0.0298297,"e analyzed challenges. c) We create a Chinese-English test set specifically containing those problems and conduct experiments to evaluate proposed method on this test set. 2 Analysis on Dialogue Translation There were already some manual analyses of translation errors, especially in the field of discourse translation. Voita et al. (2019) study EnglishRussian translation and find three main challenges for discourse translation: deixis, ellipsis, and lexical cohesion. For Chinese-English translation, tense consistency, connective mismatch, and content-heavy sentences are the most common issues (Li et al., 2014). Different from previous works, we mainly analyze the specific phenomena in dialogue translation. We begin with a study on a bilingual dialogue corpus (Wang et al., 2018).1 We translate source sentences into the target language at sentence level and compare translation results with reference at dialogue level. Around 1,000 dialogues are evaluated, and the results are reported in Table 2. From the statistic, we observe two persistent dialogue translation problems: pronoun dropping (ProDrop), punctuation drop1 Types of phenomena Correct ProDrop PunDrop Incorrect segmentation Other translation e"
2021.naacl-industry.14,E17-2004,0,0.0166318,"ose the task of translating Bilingual Multi-Speaker Conversations. They introduce datasets extracted from Europarl and Opensubtitles and explore how to exploit both source and targetside conversation histories. Bawden et al. (2019) present a new English-French test set for evaluating of Machine Translation (MT) for informal, written bilingual dialogue. Recently WMT2020 has also proposed a new shared task - machine translation for chats,4 focusing on bilingual customer support chats (Farajian et al., 2020). Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019). 7 Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep"
2021.naacl-industry.14,2020.emnlp-main.210,1,0.747598,"slation in English. Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from 1 Introduction enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues Remarkable progress has been made in Neural Mafrom parallel corpora, such as OpenSubtitles (Lison chine Translation (NMT) (Bahdanau et al., 2015; et al., 2019), or leveraging speaker information for Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) integrating dialogue context into neural models. in recent years, which has been widely applied Also, the lack of both training data and benchmark in everyday life. A typical scenario for such aptest set makes current dialogue translation models plication is translating dialogue texts, in particufar from satisfying and need to be further improved. lar the record of group chats or movie subtitles, In this paper, we try to alleviate the aforewhich helps people of different languages undermentioned challenges in dialogue translation. We stand cross-language chat and improve th"
2021.naacl-industry.14,N19-1190,0,0.0681472,"ulti-Speaker Conversations. They introduce datasets extracted from Europarl and Opensubtitles and explore how to exploit both source and targetside conversation histories. Bawden et al. (2019) present a new English-French test set for evaluating of Machine Translation (MT) for informal, written bilingual dialogue. Recently WMT2020 has also proposed a new shared task - machine translation for chats,4 focusing on bilingual customer support chats (Farajian et al., 2020). Robust Training Neural models have been usually affected by noisy issues. Many efforts (Li et al., 2017; Sperber et al., 2017; Vaibhav et al., 2019; Yang et al., 2020) focus on data augmentation to alleviate the problem by adding synthetic noise to the training set. However, generating noise has always been a challenge, as natural noise is always more diversified than artificially constructed noise (Belinkov and Bisk, 2018; Anastasopoulos, 2019; Anastasopoulos et al., 2019). 7 Conclusions In this paper, we manually analyze challenges in dialogue translation and detect three main problems. In order to tackle these issues, we propose a multitask learning method with contextual labeling. For deep evaluation, we construct dialogues with tran"
2021.naacl-industry.14,P19-1116,0,0.11,"Missing"
2021.naacl-industry.14,L16-1436,0,0.199346,"p (2) and DialTypo (3). MT is translation results from Google Translate while REF is references. (Barrault et al., 2020), while the translation of dialogue must take the meaning of context and the input noise into account. Table 1 shows examples of dialogue fragment in Chinese and their translation in English. Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from 1 Introduction enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues Remarkable progress has been made in Neural Mafrom parallel corpora, such as OpenSubtitles (Lison chine Translation (NMT) (Bahdanau et al., 2015; et al., 2019), or leveraging speaker information for Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) integrating dialogue context into neural models. in recent years, which has been widely applied Also, the lack of both training data and benchmark in everyday life. A typical scenario for such aptest set makes current dialogue translation models plication is translating dialogue texts, in"
2021.naacl-industry.14,2020.tacl-1.47,0,0.0223257,". Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from 1 Introduction enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues Remarkable progress has been made in Neural Mafrom parallel corpora, such as OpenSubtitles (Lison chine Translation (NMT) (Bahdanau et al., 2015; et al., 2019), or leveraging speaker information for Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) integrating dialogue context into neural models. in recent years, which has been widely applied Also, the lack of both training data and benchmark in everyday life. A typical scenario for such aptest set makes current dialogue translation models plication is translating dialogue texts, in particufar from satisfying and need to be further improved. lar the record of group chats or movie subtitles, In this paper, we try to alleviate the aforewhich helps people of different languages undermentioned challenges in dialogue translation. We stand cross-language chat and improve their comfirst analyz"
2021.naacl-industry.14,W18-6311,0,0.339718,"(3). MT is translation results from Google Translate while REF is references. (Barrault et al., 2020), while the translation of dialogue must take the meaning of context and the input noise into account. Table 1 shows examples of dialogue fragment in Chinese and their translation in English. Example (1) demonstrates that the omission in traditional translation (e.g., dropped pronouns in Chinese) leads to inaccurate translation results. Despite its vast potential application, efforts of exploration into dialogue translation are far from 1 Introduction enough. Existing works (Wang et al., 2016; Maruf et al., 2018) focus on either extracting dialogues Remarkable progress has been made in Neural Mafrom parallel corpora, such as OpenSubtitles (Lison chine Translation (NMT) (Bahdanau et al., 2015; et al., 2019), or leveraging speaker information for Wu et al., 2016; Lin et al., 2020; Liu et al., 2020) integrating dialogue context into neural models. in recent years, which has been widely applied Also, the lack of both training data and benchmark in everyday life. A typical scenario for such aptest set makes current dialogue translation models plication is translating dialogue texts, in particufar from sati"
2021.naacl-industry.14,2001.mtsummit-papers.68,0,0.0302211,"ages 105–112 June 6–11, 2021. ©2021 Association for Computational Linguistics encoder part automatically learns how to de-noise the noise input via explicit supervisory signals provided by additional contextual labeling. We also propose three strong baselines for dialogue translation, including repair (REPAIR DIAL) and robust (ROBUST DIAL) model. To alleviate the challenges arising from the scarcity of dialogue data, we use sub-documents in the bilingual parallel corpus to enable the model to learn from crosssentence context. Additionally as for evaluation, the most commonly used BLEU metric (Papineni et al., 2001) for NMT is not good enough to provide a deep look into the translation quality in such a scenario. Thus, we build a Chinese-English test set containing sentences with the issues in ProDrop, PunDrop and DialTypo, attached with the human translation and annotation. Finally, we get a test set of 300 dialogues with 1,931 parallel sentences. The main contributions of this paper are as follows: a) We analyze three challenges ProDrop, PunDrop and DialTypo, which greatly impact the understanding and translation of a dialogue. b) We propose a contextual multi-task learning method to tackle the analyze"
2021.naacl-industry.14,W18-6319,0,0.075643,"Missing"
2021.naacl-industry.14,P16-1162,0,0.0565515,"h separately. tences containing missing punctuation or typos according to the annotation information. As for ProDrop, we evaluate the translation quality by the percentage of correctly recovering and translating the dropped pronouns. 4.2 Settings We adopt the Chinese-English corpus from WMT20203 , with about 48M sentence pairs, as our bilingual training data D. We select newstest2019 as the development set. After splicing, we get Ddoc with 1.2M pairs and corresponding pertur0 bated dataset D0 and Ddoc with 48M and 1.2M pairs respectively. We use byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and limit the number of merge operations to a maximum of 30K. In our studies, all translation models are Transformer-big, including 6 layers for both encoders and decoders, 1024 dimensions for model, 4096 dimensions for FFN layers and 16 heads for attention. During training, we use label smoothing = 0.1 (Szegedy et al., 2016), attention dropout = 0.1 and dropout (Hinton et al., 2012) with a rate of 0.3 for all other layers. We use Adam (Kingma and Ba, 2015) to train the NMT models. β1 and β2 of Adam are set to 0.9 and 0.98, the learning rate is set to 0.0005, and gra"
2021.naacl-main.7,2020.cl-1.1,0,0.0203607,"Deep Encoder and Shallow Decoder on other Language Pairs To investigate how a deep encoder with a shallow decoder will perform in other tasks, we conducted experiments on the WMT 14 English-French and WMT 15 Czech-English news translation tasks in addition to the WMT 14 English-German task. Results on newstest 2014 (En-De/Fr) and 2015 (CsEn) respectively are shown in Table 6. Table 6 shows that the 10-2 model consistently achieves higher BLEU scores than the 6-layer model, and the 18-4 model consistently leads to significant improvements in all 3 tasks. 5 Related Work Analysis of NMT Models. Belinkov et al. (2020) analyze the representations learned by NMT models at various levels of granularity and evaluate their quality through relevant extrinsic properties. Li et al. (2019a) analyze the word alignment quality in NMT and the effect of alignment errors on translation errors. They demonstrate that NMT captures word alignment much better for those words mostly contributed from the source than those from the target. Voita et al. (2019b) evaluate the contribution of individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. Yang et al. (2019) pr"
2021.naacl-main.7,D18-1313,0,0.0209431,"components of the Transformer’s attention with the new formulation via the lens of the kernel. Tang et al. (2019) find that encoder hidden states outperform word embeddings significantly in word sense disambiguation. He et al. (2019) measure the word importance by attributing the NMT output to every input word and reveal that words of certain syntactic categories have higher importance while the categories vary across language pairs. Voita et al. (2019a) use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers. Early work by Bisazza and Tump (2018) performs a fine-grained analysis of how various source-side morphological features are captured at different levels of the NMT encoder. While they are unable to find any correlation between the accuracy of source morphology encoding and translation quality, they discover that morphological features are only captured in context and only to the extent that they are directly transferable to the target words, and suggest encoder layers are “lazy”. Our analysis offers an explanation for their results as the translation already starts at the source embedding layer, and possibly source embeddings al"
2021.naacl-main.7,D19-1275,0,0.0304929,"Missing"
2021.naacl-main.7,P18-1198,0,0.0271826,"on already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (EnDe) at a speed-up of 1.4. 1 To investigate the roles of Transformer layers in translation, in this paper, we adopt probing approaches (Adi et al., 2017; Hupkes et al., 2018; Conneau et al., 2018) and propose to measure the word translation accuracy of output representations of individual Transformer layers by probing how capable they are at translating words. Probing uses linear classifiers, referred to as “probes”, where a probe can only use the hidden units of a given intermediate layer as discriminating features. Moreover, these probes cannot affect the training phase of a model, and they are generally added after training (Alain and Bengio, 2017). In addition to analyzing the role of each encoder/decoder layer, we also analyze the contribution of the source context and the decodin"
2021.naacl-main.7,N19-1423,0,0.0374997,"y demonstrate that NMT captures word alignment much better for those words mostly contributed from the source than those from the target. Voita et al. (2019b) evaluate the contribution of individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. Yang et al. (2019) propose a word reordering detection task to quantify how well the word order information is learned by Self-Attention Networks and RNN, and reveal that although recurrence structure makes the model more universally effective on learning word order, Analysis of BERT. BERT (Devlin et al., 2019) uses the Transformer encoder, and analysis of BERT may provide valuable references for analyzing the Transformer. Jawahar et al. (2019) provide support that BERT networks capture structural information, and perform a series of experiments to unpack the elements of English language structure learned by BERT. Tenney et al. (2019) employ the edge probing task suite, and find that BERT represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic role"
2021.naacl-main.7,P19-1356,0,0.0176912,"t. Voita et al. (2019b) evaluate the contribution of individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. Yang et al. (2019) propose a word reordering detection task to quantify how well the word order information is learned by Self-Attention Networks and RNN, and reveal that although recurrence structure makes the model more universally effective on learning word order, Analysis of BERT. BERT (Devlin et al., 2019) uses the Transformer encoder, and analysis of BERT may provide valuable references for analyzing the Transformer. Jawahar et al. (2019) provide support that BERT networks capture structural information, and perform a series of experiments to unpack the elements of English language structure learned by BERT. Tenney et al. (2019) employ the edge probing task suite, and find that BERT represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Pires et al. (2019) present a large number of probing experiments, and show that Multilingual-BERT’s robust ability"
2021.naacl-main.7,N13-1073,0,0.00959033,"ation quality of the transformer. Motivations targeting efficiency include: Discussion Our probing approach involves crucial information from the decoder (encoder-decoder attention from all decoder layers). However, we argue that probe training requires supervision. For the decoder, we can directly use gold references. On the encoder side, parallel data does not provide word translations for source tokens, and we have to generate this data by aligning target tokens to source tokens. One choice is extracting alignments by taking an argmax of alignment matrices or using toolkits like fastalign (Dyer et al., 2013). In this case, probe training does not involve attention matrices, but this has drawbacks: multiple/no target tokens may align to one source token. We use soft aggregation to preserve more information (other attention possibilities besides the highest are kept) and to alleviate error propagation. We argue that the use of attention matrices is only to bring supervision (word translations) from the target side to the source side, which is inevitable. Decoder representations cannot flow back to the frozen encoder. Our paper also empirically reveals the impact of attention matrices: 1) In Section"
2021.naacl-main.7,P19-1124,0,0.312501,"Ad∗k . We use a d ∗ k dimension weight vector w to combine all attention matrices. The weight vector is normalized by softmax to a probability distribution p: pi = ewi d∗k P 2.2 The analysis of the prediction accuracy of the decoder is simpler than the encoder, as we can directly use the shifted target sequence (teacher forcing) without the requirement to bridge different sequence lengths between the source sentence and the target while analyzing the encoder. We use the output representations of the analyzed layer, and evaluate its prediction accuracy after projection. However, as studied by Li et al. (2019a), the decoder involves two kinds of “translation”. One (performed by the self-attention sub-layer) translates the history token sequence to the next token, another (performed by the cross-attention sub-layer) translates by attending source tokens. We additionally analyze the effects of these two kinds of translation on predicting accuracy by dropping the corresponding sub-layer (either cross- or masked self-attention) of the analyzed decoder layer (i.e., we only compute the other sub-layer and the feedforward layer where only the residual connection is kept as the computation of the skipped"
2021.naacl-main.7,D19-1573,0,0.0635857,"Missing"
2021.naacl-main.7,D18-1336,0,0.0238054,"er decoder to accelerate decoding. Wu et al. (2019) introduce lightweight convolution and dynamic convolutions. The number of operations required by their approach scales linearly in the input length, whereas self-attention is quadratic. Zhang et al. (2018b) apply cube pruning to neural machine translation to speed up translation. Zhang et al. (2018c) propose to adopt an n-gram suffix-based equivalence function into beam search decoding, which obtains similar translation quality with a smaller beam size, making NMT decoding more efficient. NonAutoregressive Translation (NAT) (Gu et al., 2018; Libovický and Helcl, 2018; Wei et al., 2019; Shao et al., 2019; Li et al., 2019b; Wang et al., 2019; Guo et al., 2019) enables parallelized decoding, while there is still a significant quality drop compared to traditional autoregressive beam search, our findings on using more encoder layers might also be adapted to NAT. Recently, and independently of our work, Kasai et al. (2021) compare the performance and speed between a 12-layer encoder 1-layer decoder case with NAT approaches, and show that a onelayer autoregressive decoder yields state-of-the-art accuracy with comparable latency to strong nonautoregressive models"
2021.naacl-main.7,P19-1493,0,0.0211164,"encoder, and analysis of BERT may provide valuable references for analyzing the Transformer. Jawahar et al. (2019) provide support that BERT networks capture structural information, and perform a series of experiments to unpack the elements of English language structure learned by BERT. Tenney et al. (2019) employ the edge probing task suite, and find that BERT represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Pires et al. (2019) present a large number of probing experiments, and show that Multilingual-BERT’s robust ability to generalize cross-lingually is underpinned by a multilingual representation. 81 Accelerating Decoding. Zhang et al. (2018a) propose average attention as an alternative to the self-attention network in the Transformer decoder to accelerate decoding. Wu et al. (2019) introduce lightweight convolution and dynamic convolutions. The number of operations required by their approach scales linearly in the input length, whereas self-attention is quadratic. Zhang et al. (2018b) apply cube pruning to neural"
2021.naacl-main.7,D19-1088,0,0.0206476,"relatively small. 80 Depth Encoder Decoder 6 10 18 2 4 En-De En-Fr Cs-En 27.96 28.47 29.38† 40.13 40.49 40.90† 28.69 28.87 29.75† learning objectives matter more in the downstream tasks such as machine translation. Tsai et al. (2019) regard attention as applying a kernel smoother over the inputs with the kernel scores being the similarities between inputs, and analyze individual components of the Transformer’s attention with the new formulation via the lens of the kernel. Tang et al. (2019) find that encoder hidden states outperform word embeddings significantly in word sense disambiguation. He et al. (2019) measure the word importance by attributing the NMT output to every input word and reveal that words of certain syntactic categories have higher importance while the categories vary across language pairs. Voita et al. (2019a) use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers. Early work by Bisazza and Tump (2018) performs a fine-grained analysis of how various source-side morphological features are captured at different levels of the NMT encoder. While they are unable to find any correlation between the accuracy of sou"
2021.naacl-main.7,P16-1009,0,0.125855,"6 -15.12 -8.70 -0.79 Table 2: Word translation accuracy of Transformer layers on the WMT 15 Cs-En task. BLEU score of 27.96 on the test set. The projection matrix and the weight vector w of 48 elements for alignment were trained on the training set with the frozen Transformer. We monitored the accuracy on the development set, and report results on the test set. straint (Xu et al., 2020) to ensure the convergence of deep encoders. We implemented our approaches based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016b) with 32k merge operations. We only kept sentences with a maximum of 256 sub-word tokens for training. The concatenation of newstest 2012 and newstest 2013 was used for validation and newstest 2014 as the test set. The number of warm-up steps was set to 8k.1 The model was trained for 100k training steps with around 25k target tokens in each batch. We followed all the other settings of Vaswani et al. (2017). We averaged the last 5 checkpoints saved with an interval of 1, 500 training steps. For decoding, we used a beam size of 4, and evaluated tokenized casesensitive BLEU.2 The averaged model"
2021.naacl-main.7,P19-1580,0,0.320373,"lp@foxmail.com, Josef.Van_Genabith@dfki.de, liuqhano@foxmail.com, dyxiong@tju.edu.cn 1 Abstract Recently, a wide range of studies related to the Transformer have been conducted. For example, Bisazza and Tump (2018) perform a fine-grained analysis of how various source-side morphological features are captured at different levels of an NMT encoder. Surprisingly, they do not find any correlation between the accuracy of source morphology encoding and translation quality. Morphological features are only captured in context and only to the extent that they are directly transferable to target words. Voita et al. (2019a) study how information flows across Transformer layers and find that representations differ significantly depending on the objectives (machine translation, standard left-toright language models and masked language modeling). Tang et al. (2019) find that encoder hidden states outperform word embeddings significantly in word sense disambiguation. However, to the best of our knowledge, to date there is no study about how the Transformer translation model transforms individual source tokens into corresponding target tokens (i.e., word translations), and specifically, which role each Transformer"
2021.naacl-main.7,P16-1162,0,0.211787,"6 -15.12 -8.70 -0.79 Table 2: Word translation accuracy of Transformer layers on the WMT 15 Cs-En task. BLEU score of 27.96 on the test set. The projection matrix and the weight vector w of 48 elements for alignment were trained on the training set with the frozen Transformer. We monitored the accuracy on the development set, and report results on the test set. straint (Xu et al., 2020) to ensure the convergence of deep encoders. We implemented our approaches based on the Neutron implementation (Xu and Liu, 2019) of the Transformer translation model. We applied joint Byte-Pair Encoding (BPE) (Sennrich et al., 2016b) with 32k merge operations. We only kept sentences with a maximum of 256 sub-word tokens for training. The concatenation of newstest 2012 and newstest 2013 was used for validation and newstest 2014 as the test set. The number of warm-up steps was set to 8k.1 The model was trained for 100k training steps with around 25k target tokens in each batch. We followed all the other settings of Vaswani et al. (2017). We averaged the last 5 checkpoints saved with an interval of 1, 500 training steps. For decoding, we used a beam size of 4, and evaluated tokenized casesensitive BLEU.2 The averaged model"
2021.naacl-main.7,2020.emnlp-main.14,0,0.0316272,"Missing"
2021.naacl-main.7,P19-1288,0,0.0199281,"(2019) introduce lightweight convolution and dynamic convolutions. The number of operations required by their approach scales linearly in the input length, whereas self-attention is quadratic. Zhang et al. (2018b) apply cube pruning to neural machine translation to speed up translation. Zhang et al. (2018c) propose to adopt an n-gram suffix-based equivalence function into beam search decoding, which obtains similar translation quality with a smaller beam size, making NMT decoding more efficient. NonAutoregressive Translation (NAT) (Gu et al., 2018; Libovický and Helcl, 2018; Wei et al., 2019; Shao et al., 2019; Li et al., 2019b; Wang et al., 2019; Guo et al., 2019) enables parallelized decoding, while there is still a significant quality drop compared to traditional autoregressive beam search, our findings on using more encoder layers might also be adapted to NAT. Recently, and independently of our work, Kasai et al. (2021) compare the performance and speed between a 12-layer encoder 1-layer decoder case with NAT approaches, and show that a onelayer autoregressive decoder yields state-of-the-art accuracy with comparable latency to strong nonautoregressive models. Our work explains why using a deep"
2021.naacl-main.7,D19-1149,0,0.0302126,"Missing"
2021.naacl-main.7,P19-1125,0,0.0181091,"coding. Wu et al. (2019) introduce lightweight convolution and dynamic convolutions. The number of operations required by their approach scales linearly in the input length, whereas self-attention is quadratic. Zhang et al. (2018b) apply cube pruning to neural machine translation to speed up translation. Zhang et al. (2018c) propose to adopt an n-gram suffix-based equivalence function into beam search decoding, which obtains similar translation quality with a smaller beam size, making NMT decoding more efficient. NonAutoregressive Translation (NAT) (Gu et al., 2018; Libovický and Helcl, 2018; Wei et al., 2019; Shao et al., 2019; Li et al., 2019b; Wang et al., 2019; Guo et al., 2019) enables parallelized decoding, while there is still a significant quality drop compared to traditional autoregressive beam search, our findings on using more encoder layers might also be adapted to NAT. Recently, and independently of our work, Kasai et al. (2021) compare the performance and speed between a 12-layer encoder 1-layer decoder case with NAT approaches, and show that a onelayer autoregressive decoder yields state-of-the-art accuracy with comparable latency to strong nonautoregressive models. Our work explain"
2021.naacl-main.7,P19-1452,0,0.0250068,"pose a word reordering detection task to quantify how well the word order information is learned by Self-Attention Networks and RNN, and reveal that although recurrence structure makes the model more universally effective on learning word order, Analysis of BERT. BERT (Devlin et al., 2019) uses the Transformer encoder, and analysis of BERT may provide valuable references for analyzing the Transformer. Jawahar et al. (2019) provide support that BERT networks capture structural information, and perform a series of experiments to unpack the elements of English language structure learned by BERT. Tenney et al. (2019) employ the edge probing task suite, and find that BERT represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Pires et al. (2019) present a large number of probing experiments, and show that Multilingual-BERT’s robust ability to generalize cross-lingually is underpinned by a multilingual representation. 81 Accelerating Decoding. Zhang et al. (2018a) propose average attention as an alternative to the self-attention n"
2021.naacl-main.7,D19-1443,0,0.0282376,"han those of the 6-layer encoder (1.90 vs. 0.87), indi4 A full grid search over configurations is tedious and expensive. We take inspiration from Table 4 where going from 5 to 4 decoder layers brings about the biggest relative jump in translation quality. We explored a few configurations and find that using more than 18 encoder layers can still bring improvements, but the gains are relatively small. 80 Depth Encoder Decoder 6 10 18 2 4 En-De En-Fr Cs-En 27.96 28.47 29.38† 40.13 40.49 40.90† 28.69 28.87 29.75† learning objectives matter more in the downstream tasks such as machine translation. Tsai et al. (2019) regard attention as applying a kernel smoother over the inputs with the kernel scores being the similarities between inputs, and analyze individual components of the Transformer’s attention with the new formulation via the lens of the kernel. Tang et al. (2019) find that encoder hidden states outperform word embeddings significantly in word sense disambiguation. He et al. (2019) measure the word importance by attributing the NMT output to every input word and reveal that words of certain syntactic categories have higher importance while the categories vary across language pairs. Voita et al."
2021.naacl-main.7,2020.acl-main.38,1,0.781729,"Missing"
2021.naacl-main.7,P19-1354,0,0.0202659,"elinkov et al. (2020) analyze the representations learned by NMT models at various levels of granularity and evaluate their quality through relevant extrinsic properties. Li et al. (2019a) analyze the word alignment quality in NMT and the effect of alignment errors on translation errors. They demonstrate that NMT captures word alignment much better for those words mostly contributed from the source than those from the target. Voita et al. (2019b) evaluate the contribution of individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. Yang et al. (2019) propose a word reordering detection task to quantify how well the word order information is learned by Self-Attention Networks and RNN, and reveal that although recurrence structure makes the model more universally effective on learning word order, Analysis of BERT. BERT (Devlin et al., 2019) uses the Transformer encoder, and analysis of BERT may provide valuable references for analyzing the Transformer. Jawahar et al. (2019) provide support that BERT networks capture structural information, and perform a series of experiments to unpack the elements of English language structure learned by BE"
2021.naacl-main.7,D19-1448,0,0.212255,"lp@foxmail.com, Josef.Van_Genabith@dfki.de, liuqhano@foxmail.com, dyxiong@tju.edu.cn 1 Abstract Recently, a wide range of studies related to the Transformer have been conducted. For example, Bisazza and Tump (2018) perform a fine-grained analysis of how various source-side morphological features are captured at different levels of an NMT encoder. Surprisingly, they do not find any correlation between the accuracy of source morphology encoding and translation quality. Morphological features are only captured in context and only to the extent that they are directly transferable to target words. Voita et al. (2019a) study how information flows across Transformer layers and find that representations differ significantly depending on the objectives (machine translation, standard left-toright language models and masked language modeling). Tang et al. (2019) find that encoder hidden states outperform word embeddings significantly in word sense disambiguation. However, to the best of our knowledge, to date there is no study about how the Transformer translation model transforms individual source tokens into corresponding target tokens (i.e., word translations), and specifically, which role each Transformer"
2021.naacl-main.7,W18-5448,0,0.0619539,"Missing"
2021.naacl-main.7,D18-1460,0,0.15958,"s and Analysis We examine the effects of reducing the number of decoder layers while adding corresponding numbers of encoder layers, and results are shown in Table 4. “Speed up” stands for the decoding acceleration compared to the 6-layer Transformer. Table 4 shows that while the acceleration of trading decoder layers for encoder layers in training is small, in decoding it is significant. Specifically, the 3 Since there is no re-ordering of the target language performed, which makes the merging of translated sub-word units in the source sentence order pointless. 79 Model Depth Encoder Decoder Zhang et al. (2018a) Transformer BLEU Para. (M) Train Time Decode (/s) Speed up 6 6 28.13 74.97 40h09m 29 1.52 6 7 8 9 10 11 6 5 4 3 2 1 27.96 28.07 28.61 28.53 28.47 27.02 62.37 61.32 60.27 59.22 58.17 57.12 33h33m 32h17m 31h26m 30h29m 30h11m 29h27m 44 38 31 25 19 13 1.00 1.16 1.42 1.76 2.32 3.38 18 4 29.38 91.77 52h56m 32 1.38 Table 4: Effects of encoder/decoder depth on the WMT 14 En-De task. The decoding time is for the test set of 3, 003 sentences with a beam size of 4. Encoder Layer 0 1 2 3 4 5 6 7 8 9 10 Acc 40.48 41.29 43.00 44.07 45.86 46.54 47.46 48.92 49.58 50.24 50.35 ∆ 0.81 1.71 1.07 1.79 0.68 0.92"
2021.naacl-main.7,D18-1511,0,0.159935,"s and Analysis We examine the effects of reducing the number of decoder layers while adding corresponding numbers of encoder layers, and results are shown in Table 4. “Speed up” stands for the decoding acceleration compared to the 6-layer Transformer. Table 4 shows that while the acceleration of trading decoder layers for encoder layers in training is small, in decoding it is significant. Specifically, the 3 Since there is no re-ordering of the target language performed, which makes the merging of translated sub-word units in the source sentence order pointless. 79 Model Depth Encoder Decoder Zhang et al. (2018a) Transformer BLEU Para. (M) Train Time Decode (/s) Speed up 6 6 28.13 74.97 40h09m 29 1.52 6 7 8 9 10 11 6 5 4 3 2 1 27.96 28.07 28.61 28.53 28.47 27.02 62.37 61.32 60.27 59.22 58.17 57.12 33h33m 32h17m 31h26m 30h29m 30h11m 29h27m 44 38 31 25 19 13 1.00 1.16 1.42 1.76 2.32 3.38 18 4 29.38 91.77 52h56m 32 1.38 Table 4: Effects of encoder/decoder depth on the WMT 14 En-De task. The decoding time is for the test set of 3, 003 sentences with a beam size of 4. Encoder Layer 0 1 2 3 4 5 6 7 8 9 10 Acc 40.48 41.29 43.00 44.07 45.86 46.54 47.46 48.92 49.58 50.24 50.35 ∆ 0.81 1.71 1.07 1.79 0.68 0.92"
C08-1127,P05-1033,0,0.10108,"model. We also present an annotation algorithm that captures syntactic information for BTG nodes. The experiments show that the LABTG approach significantly outperforms a baseline BTGbased system and a state-of-the-art phrasebased system on the NIST MT-05 Chineseto-English translation task. Moreover, we empirically demonstrate that the proposed method achieves better translation selection and phrase reordering. 1 Introduction Formal grammar used in statistical machine translation (SMT), such as Bracketing Transduction Grammar (BTG) proposed by (Wu, 1997) and the synchronous CFG presented by (Chiang, 2005), provides a natural platform for integrating linguistic knowledge into SMT because hierarchical structures produced by the formal grammar resemble linguistic structures.1 Chiang (2005) attempts to integrate linguistic information into his formally c 2008.  Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 We inherit the definitions of formal and linguistic from (Chiang, 2005) which makes a distinction between formally syntax-based SMT and linguistically syntax-based SMT."
C08-1127,P05-1067,0,0.0858963,"Missing"
C08-1127,P03-2041,0,0.0607656,"dge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is significant because we annotate standard plain phrases using lingui"
C08-1127,P06-1077,0,0.0216025,"and scheme are capable of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is significant becaus"
C08-1127,W06-1606,0,0.021123,"016 Manchester, August 2008 annotated reordering model. The second is that our proposed annotation algorithm and scheme are capable of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain"
C08-1127,P00-1056,0,0.208745,"prior probability which can be set based on the order preference of the language pairs. In MEBTG (Xiong et al., 2006), however, the probability is calculated more sophisticatedly using a MaxEnt-based classification model with boundary words as its features. by projecting source-side syntax tree to BTG tree, and finally extract rules from these annotated BTG trees. This way restricts learning space to only the best BTG trees2 , and leads to the loss of many useful annotated rules. Therefore, we use an alternative way to extract the annotated rules as illustrated below. Firstly, we run GIZA++ (Och and Ney, 2000) on the training corpus in both directions and then apply the ogrow-diag-finalp refinement rule (Koehn et al., 2003) to obtain many-to-many word alignments. Secondly, we extract bilingual phrases from the word-aligned corpus, then annotate their source sides with linguistic elements to obtain the annotated lexical rules.3 Finally, we learn reordering examples (Xiong et al., 2006), annotate their two neighboring sub-phrases and whole phrases, and then generalize them in the annotated merging rules. Although this alternative way may also miss reorderings due to word alignment errors, it is still"
C08-1127,P03-1021,0,0.0302069,"features for the reordering model PRb (shared by both MEBTG and LABTG systems) using the right boundary words of phrases and 85K features for the annotated reordering model PRa (only included in the LABTG system) using linguistic annotations. We ran the MaxEnt toolkit (Zhang, 2004) to tune reordering feature weights with iteration number being set to 100 and Gaussian prior to 1 to avoid overfitting. We built our four-gram language model using Xinhua section of the English Gigaword corpus (181.1M words) with the SRILM toolkit (Stolcke, 2002). For the efficiency of minimum-error-rate training (Och, 2003), we built our development set (580 sentences) using sentences not exceeding 50 characters from the NIST MT-02 evaluation test data. 5.1 LABTG vs. phrase-based SMT and BTG-based SMT We compared the LABTG system with two baseline systems. The results are given in Table 2. The LABTG outperforms Moses and MEBTG by 2.81 and 1.69 absolute BLEU points, respectively. These significant improvements indicate that BTG formal structures can be successfully extended with linguistic knowledge extracted from syntactic structures without losing the strength of phrasebased method. 5.2 The Effect of Different"
C08-1127,P05-1034,0,0.0431644,"nnotation algorithm and scheme are capable of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is"
C08-1127,P07-1090,1,0.871143,"significant because we annotate standard plain phrases using linguistic elements on the source side. Compared with the target side annotation which improves fluency and grammaticality of translation output, linguistic annotation on the source side helps to improve translation adequacy. Recently, some researchers have extended and created several variations of BTG/ITG. Zhang et al. (2005) propose lexicalized ITG for better word alignment. Xiong et al. (2006) demonstrate that their MEBTG, a BTG variation with MaxEntbased reordering model, can improve phrase reordering significantly. Similarly, Setiawan et al. (2007) use an enhanced BTG variation with function words for reordering. LABTG differs from these BTG variations in that the latter does not use any external linguistic knowledge. Zhang et al. (2007) describe a phrase reordering model based on BTG-style rules which integrates source-side syntactic knowledge. Our annotated reordering model of LABTG differs from their work in two key aspects. Firstly, we allow any phrase reorderings while they only reorder syntactic phrases. In their model, only syntactic phrases can use linguistic knowledge from parse trees for reordering while non-syntactic phrases"
C08-1127,P96-1021,0,0.76653,"labelling both syntactic and non-syntactic phrases. The linguistic elements extracted from parse trees capture both internal lexical content and external context of phrases. With these linguistic annotations, we expect the LABTG to address two traditional issues of standard phrase-based SMT (Koehn et al., 2003) in a more effective manner. They are (1) phrase translation: translating phrases according to their contexts; (2) phrase reordering: incorporating richer linguistic features for better reordering. The proposed LABTG displays two unique characteristics when compared with BTG-based SMT (Wu, 1996; Xiong et al., 2006). The first is that two linguistically-informed sub-models are introduced for better phrase translation and reordering: annotated phrase translation model and 1009 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1009–1016 Manchester, August 2008 annotated reordering model. The second is that our proposed annotation algorithm and scheme are capable of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain t"
C08-1127,J97-3002,0,0.63124,"otivated phrase translation model and reordering model. We also present an annotation algorithm that captures syntactic information for BTG nodes. The experiments show that the LABTG approach significantly outperforms a baseline BTGbased system and a state-of-the-art phrasebased system on the NIST MT-05 Chineseto-English translation task. Moreover, we empirically demonstrate that the proposed method achieves better translation selection and phrase reordering. 1 Introduction Formal grammar used in statistical machine translation (SMT), such as Bracketing Transduction Grammar (BTG) proposed by (Wu, 1997) and the synchronous CFG presented by (Chiang, 2005), provides a natural platform for integrating linguistic knowledge into SMT because hierarchical structures produced by the formal grammar resemble linguistic structures.1 Chiang (2005) attempts to integrate linguistic information into his formally c 2008.  Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 We inherit the definitions of formal and linguistic from (Chiang, 2005) which makes a distinction between formally sy"
C08-1127,P07-1037,0,0.0285055,"Missing"
C08-1127,I05-1007,1,0.844533,"Missing"
C08-1127,N06-1031,0,0.0770157,"in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is significant because we annotate standard plain phrases using linguistic elements on the source side. Compared with the target side annotation which improves fluency and grammaticality of translation output, linguistic annotation on the source side helps to improve translation adequacy. Rece"
C08-1127,P06-1066,1,0.861375,"both syntactic and non-syntactic phrases. The linguistic elements extracted from parse trees capture both internal lexical content and external context of phrases. With these linguistic annotations, we expect the LABTG to address two traditional issues of standard phrase-based SMT (Koehn et al., 2003) in a more effective manner. They are (1) phrase translation: translating phrases according to their contexts; (2) phrase reordering: incorporating richer linguistic features for better reordering. The proposed LABTG displays two unique characteristics when compared with BTG-based SMT (Wu, 1996; Xiong et al., 2006). The first is that two linguistically-informed sub-models are introduced for better phrase translation and reordering: annotated phrase translation model and 1009 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1009–1016 Manchester, August 2008 annotated reordering model. The second is that our proposed annotation algorithm and scheme are capable of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we es"
C08-1127,2006.amta-papers.8,0,0.0165304,"able of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is significant because we annotate standar"
C08-1127,N03-1017,0,0.134062,"c parse trees of source or target language. Along this line, we propose a novel approach: Linguistically Annotated BTG (LABTG) for SMT. The LABTG annotates BTG rules with linguistic elements that are learned from syntactic parse trees on the source side through an annotation algorithm, which is capable of labelling both syntactic and non-syntactic phrases. The linguistic elements extracted from parse trees capture both internal lexical content and external context of phrases. With these linguistic annotations, we expect the LABTG to address two traditional issues of standard phrase-based SMT (Koehn et al., 2003) in a more effective manner. They are (1) phrase translation: translating phrases according to their contexts; (2) phrase reordering: incorporating richer linguistic features for better reordering. The proposed LABTG displays two unique characteristics when compared with BTG-based SMT (Wu, 1996; Xiong et al., 2006). The first is that two linguistically-informed sub-models are introduced for better phrase translation and reordering: annotated phrase translation model and 1009 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1009–1016 Manchester,"
C08-1127,D07-1091,0,0.0285451,"tures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The difference between their work and LABTG is significant because we annotate standard plain phrases using linguistic elements on the source side. Compared"
C08-1127,P07-2045,0,0.0111875,"Missing"
C08-1127,D07-1056,0,0.267646,"ause we annotate standard plain phrases using linguistic elements on the source side. Compared with the target side annotation which improves fluency and grammaticality of translation output, linguistic annotation on the source side helps to improve translation adequacy. Recently, some researchers have extended and created several variations of BTG/ITG. Zhang et al. (2005) propose lexicalized ITG for better word alignment. Xiong et al. (2006) demonstrate that their MEBTG, a BTG variation with MaxEntbased reordering model, can improve phrase reordering significantly. Similarly, Setiawan et al. (2007) use an enhanced BTG variation with function words for reordering. LABTG differs from these BTG variations in that the latter does not use any external linguistic knowledge. Zhang et al. (2007) describe a phrase reordering model based on BTG-style rules which integrates source-side syntactic knowledge. Our annotated reordering model of LABTG differs from their work in two key aspects. Firstly, we allow any phrase reorderings while they only reorder syntactic phrases. In their model, only syntactic phrases can use linguistic knowledge from parse trees for reordering while non-syntactic phrases"
C08-1127,P05-1059,0,0.11755,"Missing"
C08-1127,W06-3119,0,0.0229064,"rdering model. The second is that our proposed annotation algorithm and scheme are capable of conveying linguistic knowledge from source-side syntax structures to BTG structures. We describe the LABTG model and the annotation algorithm in Section 4. To better explain the LABTG model, we establish a unified framework of BTG-based SMT in Section 3. We conduct a series of experiments to study the effect of the LABTG in Section 5. 2 Related Work There have been various efforts to integrate linguistic knowledge into SMT systems, either from the target side (Marcu et al., 2006; Hassan et al., 2007; Zollmann and Venugopal, 2006), the source side (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006) or both sides (Eisner, 2003; Ding et al., 2005; Koehn and Hoang, 2007), just to name a few. LABTG can be considered as, but not limited to, a new attempt that enriches translation model with source-side linguistic annotations. (Huang and Knight, 2006) and (Hassan et al., 2007) introduce relabeling and supertagging on the target side, respectively. The former re-annotates syntactified phrases to learn grammatical distinctions while the latter supertags standard plain phrases, both applied on the target side. The differ"
C12-1176,P09-1088,0,0.136299,"r (Galley et al., 2006; Liu et al., 2006; Chiang, 2007). Most systems induce synchronous grammars (including phrases) from parallel corpora using a heuristic two-step pipeline. This pipeline first aligns a parallel corpus at the word level with heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step"
C12-1176,N10-1015,0,0.0540957,"hat our improvement comes from the theoretical advantage of our approach, and that such advantage does exist even on large-scale data. 7 Related Work Because of the great importance of synchronous grammars to SMT systems, researchers have proposed various methods in order to improve the quality of grammars. In addition to the generative model introduced in Section 1, researchers also have made efforts on word alignment and grammar rescoring. The first effort is to improve word alignment by considering phrase/syntax information (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such approaches also use discriminative framework to combine word alignment and syntactic alignment information. In this way, they prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. 2895 Researchers also try to rescore the weights of translation rules. They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features us"
C12-1176,W07-0403,0,0.245791,"nt combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative fo"
C12-1176,P11-2031,0,0.116176,"Missing"
C12-1176,D11-1005,0,0.0253585,"ted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features using labeled data (Huang and Xiang, 2010). Such methods still need to run the heuristic two-step pipeline to extract the grammar, while our method can directly learn the grammar and correspondent weights. Our work also has a connection to the research direction that exploits resources-rich languages to construct similar tools for resource-poor languages. This can be done with parallel data (Pauls et al., 2010) or without parallel data (Cohen et al., 2011). Saers et al. (2009) also propose a cubic biparsing algorithm based on beam pruning. They apply this algorithm for generative model-based ITG grammar induction. Conclusion and Future Work We have presented a global log-linear model for synchronous grammar induction, and have also proposed efficient training and inference algorithms. In addition to the theoretical advantage, we also achieve significant improvements over the traditional heuristic two-stage pipeline on both medium-scale and large-scale training data. In the future, we hope to find efficient algorithms that are capable of incorpo"
C12-1176,D09-1037,0,0.376558,"extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous grammar induction is unsupervised discriminative model. Unsupervised disc"
C12-1176,J97-3002,0,0.532366,"Missing"
C12-1176,D08-1033,0,0.164223,"-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous grammar induction is unsupe"
C12-1176,P10-1147,0,0.0800555,"; Liu et al., 2006; Chiang, 2007). Most systems induce synchronous grammars (including phrases) from parallel corpora using a heuristic two-step pipeline. This pipeline first aligns a parallel corpus at the word level with heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, de"
C12-1176,N10-1033,0,0.0163797,"sorted candidate lists for every substructure, and create cubes that represent the potential combinations of these candidates (see Fig. 2(a)). In this way, we are able to create k-best hyperedges by cube pruning. More specifically, we maintain charts for source words and source spans respectively: • s-chart for each source word. The cell char t[s, i] in s-chart stores a list of candidate alignments for the i-th source word. A candidate alignment is represented by a list of target index. The vertical dimension in Figure 2(a) is an instance of char t[s, 3]. 1 http://leon.bottou.org/projects/sgd Dyer (2010) has shown that two monolingual parses can be more efficient than one synchronous parse, due to the sparsity of pre-fixed translation rules. Such rules are extracted by the heuristic two-step pipeline. In contrast, there are no pre-fixed translation rules in our case. 2 2888 5 ,3 , 5 0, 2 ,2 , 0, 2 zhiyi3 [0,3] [0,5] r3 1 2 [0,2] zhiyi 3 [2,5] zhiyi 3 4 1 2 Figure 2: Construct hyperedge from a cube. (a) Cube X 0,2 zhiyi3 for source span (0,3). The vertical direction represents the candidate list of zhiyi3 , while the horizontal direction is a list of nodes that share the same source span (0,2)"
C12-1176,P11-1042,0,0.215487,"ules discovered in the training corpus by our algorithm. sr c() denotes the source side of a rule. G() is the set of rules in a hypergraph. The neighbor grammar contains those rules that are discovered during training (rather than all potential rules), and whose source sides occur in the synchronous hypergraph. Sine the size of NG is typical fairly small, the parsing of our source hypergraph becomes tractable in practice. The definition of neighbor source hypergraph is inspired by contrastive estimation (Smith and Eisner, 2005). Similar shrinkage of discriminative neighborhood is also used in Dyer et al. (2011a). Notably, our approximation is consistent with the purpose of synchronous grammar induction for SMT. In SMT, the goal of grammar induction is for translation rather than synchronous parsing. Our source hypergraph corresponds to the potential translation space during SMT decoding. Thus, we expect such approximation to be suitable for SMT. 3.3 Training Algorithm Based on the synchronous hypergraph and source hypergraph introduced above, we optimize L in an online style as shown in Algorithm 1. For each sentence pair, we first use cube-pruning based biparsing (Sec. 4) to construct a synchronou"
C12-1176,W11-2139,0,0.468141,"ules discovered in the training corpus by our algorithm. sr c() denotes the source side of a rule. G() is the set of rules in a hypergraph. The neighbor grammar contains those rules that are discovered during training (rather than all potential rules), and whose source sides occur in the synchronous hypergraph. Sine the size of NG is typical fairly small, the parsing of our source hypergraph becomes tractable in practice. The definition of neighbor source hypergraph is inspired by contrastive estimation (Smith and Eisner, 2005). Similar shrinkage of discriminative neighborhood is also used in Dyer et al. (2011a). Notably, our approximation is consistent with the purpose of synchronous grammar induction for SMT. In SMT, the goal of grammar induction is for translation rather than synchronous parsing. Our source hypergraph corresponds to the potential translation space during SMT decoding. Thus, we expect such approximation to be suitable for SMT. 3.3 Training Algorithm Based on the synchronous hypergraph and source hypergraph introduced above, we optimize L in an online style as shown in Algorithm 1. For each sentence pair, we first use cube-pruning based biparsing (Sec. 4) to construct a synchronou"
C12-1176,P06-1121,0,0.117782,"Missing"
C12-1176,C10-1056,0,0.0569973,"11). Such approaches also use discriminative framework to combine word alignment and syntactic alignment information. In this way, they prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. 2895 Researchers also try to rescore the weights of translation rules. They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features using labeled data (Huang and Xiang, 2010). Such methods still need to run the heuristic two-step pipeline to extract the grammar, while our method can directly learn the grammar and correspondent weights. Our work also has a connection to the research direction that exploits resources-rich languages to construct similar tools for resource-poor languages. This can be done with parallel data (Pauls et al., 2010) or without parallel data (Cohen et al., 2011). Saers et al. (2009) also propose a cubic biparsing algorithm based on beam pruning. They apply this algorithm for generative model-based ITG grammar induction. Conclusion and Futur"
C12-1176,P08-1067,0,0.116367,"Missing"
C12-1176,D09-1107,0,0.449941,"nto t, and d is one such derivation. Given a source sentence s, the conditional probability of a derivation d and the corresponding translation t is: P exp i λi H i (d, t, s) p(d, t|s) = (2) Z(s) P where H i (d, t, s) = r∈d hi (r, s) is feature function. We assume H i decomposes with derivation d in terms of local feature function hi , which is related to a rule r and a source sentence s. λi is the correspondent feature weight. Z(s) is the partition function: X X X Z(s) = exp λi H i (d, t, s) (3) t d∈△(t,s) i Such a discriminative latent variable model is not new to SMT (Blunsom et al., 2008; Kääriäinen, 2009; Xiao et al., 2011). However, we are distinguished from previous work by applying this model to synchronous grammar induction. The purpose of the latent variable model in such previous work is to do max-translation decoding and training (Blunsom et al., 2008), or to eliminate the gap between heuristic extraction and decoding (Kääriäinen, 2009), instead of grammar induction as synchronous rules are still extracted by the heuristic two-step pipeline. In contrast, our interest lies in using latent variable model to learn synchronous grammar directly from sentence pairs. Overall, to the best of o"
C12-1176,W01-1812,0,0.0529357,"tion E p(d|t,s) [H i ] of a parameter given a sentence pair, and the second one E p(d,t|s) [H i ] is the expectation of a parameter given the source sentence. In the following sections, we first introduce how to use hypergraph to compute these expectations by synchronous hypergraph and source hypergraph respectively (Sec. 3.1). Then, we discuss the intractability of the exact training, and achieve tractable training by approximation (Sec. 3.2). Finally, we describe the training algorithm in detail to explain how the rules is induced (Sec. 3.3). 3.1 Inference with Hypergraph We use hypergraph (Klein and Manning, 2001) to compactly represent the space of derivations. Based on hypergraphs, it’s straightforward to calculate the two expectations in Eq. (5). The first expectation E p(d|t,s) [H i ] is the expected value when observing both source sentence s and target sentence t. The second expectation E p(d,t|s) [H i ] is a similar function, but only the source sentence is observed. Thus, in order to calculate the first expectation, we construct a synchronous hypergraph to represent all derivations of a sentence pair. Similarly, for the second expectation, we use a source hypergraph to represent all derivations"
C12-1176,W04-3250,0,0.12891,"reports the average score on the three test set. |G′ | MT03 MT04 MT05 Avg. Moses-Chart 45.0M 32.93 34.73 31.24 32.97 Baseline Baseline-expand 13.2M 46.2M 32.36 33.04 34.51 35.13 31.86 32.08 32.91 33.42 UDSGI Dense UDSGI Dense+Sparse 13.1M 13.1M 33.46 33.58 35.43 36.27 32.74 33.05 33.87 34.30 System Table 3: Evaluation of translation quality in terms of BLEU. Moses-Chart is the running of hierarchical phrased-model in Moses. Baseline-expand uses a similar extraction constraint to Moses-Chart and the same decoder as Baseline. The improvement of UDSGI over Baseline is statistically significant (Koehn, 2004) (p < 0.01). We can not directly evaluate the quality of grammar, since there is not a golden grammar. However, as grammar is used to generate target translations, it’s reasonable to decide the quality of a grammar by testing what best translations it can produce. Therefore, we compare the oracle translation result of the grammar in baseline and the grammar in UDSGI, with four reference translations given.4 As shown in Table 2, our grammar achieves a much higher oracle BLEU (+8.3 BLEU points) than the baseline grammar. This suggests that the grammar induced by our method is able to generate be"
C12-1176,P07-2045,0,0.00773561,"four reference translations given.4 As shown in Table 2, our grammar achieves a much higher oracle BLEU (+8.3 BLEU points) than the baseline grammar. This suggests that the grammar induced by our method is able to generate better translations than the grammar extracted by the traditional two-step pipeline. 6.3 Translation Results Table 3 compares the translation performance of our approach and the baseline on the test sets. When extracting rules as Chiang (2007), the baseline produces an average BLEU of 32.91. We also run Moses-Chart, the implementation of hierarchical phrased-model in Moses (Koehn et al., 2007), on our data with its default settings. As shown in the table, our Baseline is comparable with Moses-Chart. However, Moses-Chart extracts much more rules, because it uses a different extraction constraint from Chiang (2007). The differences mainly include edges of initial phrases can be unaligned and minimum size of source part of sub-phrases is 2. We also relax the Baseline by applying these two options, and call such setting as Baselineexpand. Baseline-expand outperforms Baseline by +0.51 BLEU with 3.5 times of rules. As our UDSGI method learns a similar size of rules with the Baseline’s, w"
C12-1176,N03-1017,0,0.512013,"with millions of features that contain rule level, word level and translation boundary information, we significantly outperform a competitive hierarchical phrased-based baseline system by +1.4 BLEU on average on three NIST test sets. KEYWORDS: synchronous grammar induction, discriminative model, unsupervised learning, machine translation. Proceedings of COLING 2012: Technical Papers, pages 2883–2898, COLING 2012, Mumbai, December 2012. 2883 1 Introduction In the last decade, statistical machine translation (SMT) has been advanced by expanding the basic unit of translation from word to phrase (Koehn et al., 2003) and grammar (Galley et al., 2006; Liu et al., 2006; Chiang, 2007). Most systems induce synchronous grammars (including phrases) from parallel corpora using a heuristic two-step pipeline. This pipeline first aligns a parallel corpus at the word level with heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignm"
C12-1176,D12-1021,0,0.600119,"sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous grammar induction is unsupervised discriminative model. Unsupervised discriminative model can directly learn synchronou"
C12-1176,P09-1067,0,0.0479859,"is the set of hyperedges. Each hyperedge e ∈ E connects a set of antecedent nodes to a single consequent node. And each hyperedge corresponds to an SCFG rule r. In a synchronous hypergraph, a node v ∈ V is in the form X i, j,k,l , which denotes the nonterminal X spanning from i to j (that is si+1 ...s j ) in the source sentence, and from k to l in the target sentence. In a source hypergraph, each node v ∈ V is in the form X i, j , which spans from i to j in the source sentence. Based on these hypergraphs, we compute the two expectations by applying the inside-outside algorithm as described in Li et al. (2009). The computation complexity is linear to the size of hypergraph O(|E|). More exactly, O(|E|) denotes O(|s|3 |t|3 ) for synchronous hypergraph, and O(|G||s|3 ) for source hypergraph. Here, G denotes all potential synchronous grammars. 3.2 Tractable Estimation by Approximation However, the size of potential SCFGs G is extremely large given a vocabulary Ω, resulting in a large number of hyperedges in source hypergraph. See the rule r2 in Figure 1. In reality, there are many potential translation rules that share the same source side “shaoshu X ” as r2 , but with different target side. Suppose n"
C12-1176,P06-1077,1,0.815726,"d level and translation boundary information, we significantly outperform a competitive hierarchical phrased-based baseline system by +1.4 BLEU on average on three NIST test sets. KEYWORDS: synchronous grammar induction, discriminative model, unsupervised learning, machine translation. Proceedings of COLING 2012: Technical Papers, pages 2883–2898, COLING 2012, Mumbai, December 2012. 2883 1 Introduction In the last decade, statistical machine translation (SMT) has been advanced by expanding the basic unit of translation from word to phrase (Koehn et al., 2003) and grammar (Galley et al., 2006; Liu et al., 2006; Chiang, 2007). Most systems induce synchronous grammars (including phrases) from parallel corpora using a heuristic two-step pipeline. This pipeline first aligns a parallel corpus at the word level with heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein,"
C12-1176,W02-1018,0,0.0903456,"heuristic word alignment combination strategies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet"
C12-1176,D07-1038,0,0.220266,"Missing"
C12-1176,P11-1064,0,0.359975,"es from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous grammar induction is unsupervised discriminative model. Unsupervised discriminative model can"
C12-1176,P03-1021,0,0.0658986,"for MERT, and test the translation performance on the NIST 2003-2005 (MT03-05) evaluation sets. Case-insensitive NIST BLEU-4 (Papineni et al., 2002) is used to measure translation performance. We used a bilingual corpus that contains 200K sentence pairs of up to length 40 from the LDC data. 3 There are 8.8M words in the 200K data. We trained a 5-grams language model by the SRILM toolkit (Stolcke, 2002). The monolingual data for language model training includes the Xinhua portion of the GIGAWORD corpus and the English side of the entire LDC data, which contains 432 million words. We used MERT (Och, 2003) to optimize the feature weights for decoding by maximizing BLEU. Since the instability of MERT has a substantial impact on results, we follow Clark et al. (2011) to report the average scores of 3 independent runs. 6.2 Grammar Analysis and Oracle Results The synchronous grammar generated by UDSGI has 13.1 millions rules. The number of these rules is comparable with that of grammar extracted by the traditional pipeline, which has 13.2 millions rules. However, the two grammars are quite different as shown in Table 1. Our grammar is more reusable than the baseline’s, because it contains less sour"
C12-1176,J03-1002,0,0.00589028,"The experiments are aimed at measuring the quality and effectiveness of grammar induced by our method. We present the performance of our unsupervised discriminate synchronous grammar induction (UDSGI) using two groups of features during decoding. We test the translation performance of UDSGI and the baseline on the same decoder. Baseline The baseline system is an in-house implementation of hierarchical phrase-based translation system(Chiang, 2007). The grammar is extracted from word-aligned corpus by traditional two-step pipeline. Symmetric word alignments were created by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-finaland” (Koehn et al., 2003). The system uses 8 dense features including: forward and backward translation probabilities; forward and backward lexical weights; language model; 3 penalties for word count, extracted rule count, and glue rule count. UDSGI Dense This configuration uses the same feature set as the baseline. Our log-linear model for grammar induction does not contain the forward and backward translation probabilities. However, we still compute the forward and backward translation probabilities for UDSGI by normalizin"
C12-1176,P02-1040,0,0.0874883,"ize these sparse features with the dense features by minimum error rate training (MERT), we group features of the same type into one coarse ""summary feature"", and get three such features including: rule, word-pair and phraseboundary features. In this way, we rescale the weights of the three ""summary features"" with the 8 dense features by MERT. Such approach is similar to Dyer et al. (2011b). 6.1 Data We used the NIST evaluation set of 2002 (MT02) as our development set for MERT, and test the translation performance on the NIST 2003-2005 (MT03-05) evaluation sets. Case-insensitive NIST BLEU-4 (Papineni et al., 2002) is used to measure translation performance. We used a bilingual corpus that contains 200K sentence pairs of up to length 40 from the LDC data. 3 There are 8.8M words in the 200K data. We trained a 5-grams language model by the SRILM toolkit (Stolcke, 2002). The monolingual data for language model training includes the Xinhua portion of the GIGAWORD corpus and the English side of the entire LDC data, which contains 432 million words. We used MERT (Och, 2003) to optimize the feature weights for decoding by maximizing BLEU. Since the instability of MERT has a substantial impact on results, we fo"
C12-1176,N10-1014,0,0.0886082,"refore, we believe that our improvement comes from the theoretical advantage of our approach, and that such advantage does exist even on large-scale data. 7 Related Work Because of the great importance of synchronous grammars to SMT systems, researchers have proposed various methods in order to improve the quality of grammars. In addition to the generative model introduced in Section 1, researchers also have made efforts on word alignment and grammar rescoring. The first effort is to improve word alignment by considering phrase/syntax information (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such approaches also use discriminative framework to combine word alignment and syntactic alignment information. In this way, they prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. 2895 Researchers also try to rescore the weights of translation rules. They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporati"
C12-1176,D11-1046,0,0.0596119,"mes from the theoretical advantage of our approach, and that such advantage does exist even on large-scale data. 7 Related Work Because of the great importance of synchronous grammars to SMT systems, researchers have proposed various methods in order to improve the quality of grammars. In addition to the generative model introduced in Section 1, researchers also have made efforts on word alignment and grammar rescoring. The first effort is to improve word alignment by considering phrase/syntax information (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such approaches also use discriminative framework to combine word alignment and syntactic alignment information. In this way, they prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. 2895 Researchers also try to rescore the weights of translation rules. They rescore the weights of rules extracted from the two-step pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features using labeled data (Hua"
C12-1176,W09-3804,0,0.0647723,"pipeline, by using the similar latent log-linear model as us (Blunsom et al., 2008; Kääriäinen, 2009), or incorporating various features using labeled data (Huang and Xiang, 2010). Such methods still need to run the heuristic two-step pipeline to extract the grammar, while our method can directly learn the grammar and correspondent weights. Our work also has a connection to the research direction that exploits resources-rich languages to construct similar tools for resource-poor languages. This can be done with parallel data (Pauls et al., 2010) or without parallel data (Cohen et al., 2011). Saers et al. (2009) also propose a cubic biparsing algorithm based on beam pruning. They apply this algorithm for generative model-based ITG grammar induction. Conclusion and Future Work We have presented a global log-linear model for synchronous grammar induction, and have also proposed efficient training and inference algorithms. In addition to the theoretical advantage, we also achieve significant improvements over the traditional heuristic two-stage pipeline on both medium-scale and large-scale training data. In the future, we hope to find efficient algorithms that are capable of incorporating contextual fea"
C12-1176,P05-1044,0,0.289551,"nerative model. However, the advantage over generative model is that it is able to easily incorporate word alignment information which has been proved useful in the two-step pipeline. In this paper, we propose a global log-linear model (Sec. 2) for the induction of synchronous context free grammar (SCFG) (Chiang, 2007). The log-linear model is able to incorporate arbitrary features. Furthermore, it is trained from sentence pairs without word alignments in an unsupervised fashion. In particular: • We approximate the exact conditional log-likelihood objective inspired by contrastive estimation (Smith and Eisner, 2005) as the optimization of the exact objective is very expensive. The key idea is to estimate parameters via synchronous hypergraphs of sentence pairs and neighbor source hypergraphs of source sentences (Sec. 3). • Synchronous parsing is often impractical in large-scale learning applications due to its high complexity O(n6 ). We address this challenge by proposing a novel and efficient O(n3 ) cube pruning based synchronous parsing algorithm (Sec. 4). • Aiming to enhance the ability to predict whether a translation derivation is good or not, we incorporate a variety of fine-grained features into o"
C12-1176,P09-1054,0,0.0315396,"SE(NG, s) ⊲ generate neighbor source hypergraph λ ← λ + η × ∂∂ λL (H1 , H2 ) ⊲ η is learning rate return G′ , λ and store them in G′ . After that, we create the neighbor source hypergraph H2 by exhaustive bottom-up chart parsing using the neighbor grammar NG (line 6). Finally, we calculate the gradient by these two hypergraphs and update the feature weights (line 8). When the algorithm is complete, we learn a grammar G′ and also the feature weights λ of the model. We implement an stochastic gradient descent (SGD) recommended by Bottou.1 We schedule the learning rate η by an exponential decay (Tsuruoka et al., 2009). We set the regularization strength, initial learning rate and the base of exponential decay as 1.0, 0.2, 0.9 respectively. We choose these values by maximizing the translation performance measured by BLEU on the NIST 2002 development set with a subset of our training data including 20k sentence pairs. 4 Cube Pruning-based Synchronous Parsing The approximation makes the training algorithm tractable. However, there is still one problem: how to efficiently construct the synchronous hypergraph? Exhaustive synchronous parsing requires O(|s|3 |t|3 ) time.2 To overcome this challenge, we propose a"
C12-1176,D11-1081,1,0.868474,"e such derivation. Given a source sentence s, the conditional probability of a derivation d and the corresponding translation t is: P exp i λi H i (d, t, s) p(d, t|s) = (2) Z(s) P where H i (d, t, s) = r∈d hi (r, s) is feature function. We assume H i decomposes with derivation d in terms of local feature function hi , which is related to a rule r and a source sentence s. λi is the correspondent feature weight. Z(s) is the partition function: X X X Z(s) = exp λi H i (d, t, s) (3) t d∈△(t,s) i Such a discriminative latent variable model is not new to SMT (Blunsom et al., 2008; Kääriäinen, 2009; Xiao et al., 2011). However, we are distinguished from previous work by applying this model to synchronous grammar induction. The purpose of the latent variable model in such previous work is to do max-translation decoding and training (Blunsom et al., 2008), or to eliminate the gap between heuristic extraction and decoding (Kääriäinen, 2009), instead of grammar induction as synchronous rules are still extracted by the heuristic two-step pipeline. In contrast, our interest lies in using latent variable model to learn synchronous grammar directly from sentence pairs. Overall, to the best of our knowledge, this i"
C12-1176,N10-1016,1,0.945845,"parameters via synchronous hypergraphs of sentence pairs and neighbor source hypergraphs of source sentences (Sec. 3). • Synchronous parsing is often impractical in large-scale learning applications due to its high complexity O(n6 ). We address this challenge by proposing a novel and efficient O(n3 ) cube pruning based synchronous parsing algorithm (Sec. 4). • Aiming to enhance the ability to predict whether a translation derivation is good or not, we incorporate a variety of fine-grained features into our model, including rule level features, word level features and phrase boundary features (Xiong et al., 2010) (Sec. 5). We evaluate our approach on the NIST Chinese-English translation task. According to the analysis of grammar (Sec. 6.2), our induced grammar is more reusable, and is able to generate better (+8.3 BLEU points) oracle translations than the grammar of the baseline. Meanwhile, in the end-to-end machine translation experiments, our approach outperforms the two-step pipeline by +1.4 BLEU points (Sec. 6.3). 2884 2 Global Log-linear Model We propose a log-linear model to induce SCFG rules for hierarchical phrase-based translation (Chiang, 2007) which transforms a source sentence s into a tar"
C12-1176,P08-1012,0,0.0560722,"ies (e.g., grow-diag-final-and) (Koehn et al., 2003), and then extracts translation rules from word-aligned sentence pairs. It is working well in practice and therefore widely adopted. However, such a pipeline artificially brings an undesirable disconnection between translation model and word alignment model (Blunsom et al., 2009; DeNero and Klein, 2010). Recently, researchers have resorted to principled probabilistic formulations. Various generative models are proposed to learn translation rules directly from sentence pairs without word alignments (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012). Due to the independency assumptions in such generative models, it is hard to extend them to incorporate arbitrary features, especially word alignment information as used in the traditional two-step pipeline. As a result, despite theoretical advantages of these models over the two-step pipeline, in practice they can only produce comparable translation quality with the two-step pipeline in some scenarios, and usually even worse results. Yet another alternative for synchronous gramma"
C12-1176,2009.eamt-smart.4,0,\N,Missing
C12-1176,J07-2003,0,\N,Missing
C12-1176,P08-1024,0,\N,Missing
C16-1136,P05-1074,0,0.026393,"l models, which can help to reproduce training data of monolingual model. Che et al. (2013) exploited the complementary cues between two languages as bilingual constraints to help detect errors in a mono-lingual tagger task, which can improve the annotation quality of named entities. Zhu et al. (2013) translated English sentences into Chinese sentences (with the same topic) in ACE 2005 evaluation data with google machine translation system as a second text representation feature 1448 so as to alleviate the data sparseness problem effectively. Our method is also related to paraphrase learning (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Snover et al., 2009; Ganitkevitch et al., 2013). However, there are two significant differences. First, paraphrase learning translates phrases strictly via word alignments while we use word alignments to find phrase spans on the target language. Second, our purpose is to obtain structured phrases (with syntactic constraints) rather than plain phrases as structured phrases can help us find new phrase structures as shown in Section 3. 7 Conclusion and Future Work We have presented a bilingual structure projection algorithm that explores structural diver"
C16-1136,W10-2906,0,0.0962151,"n word phrase (e.g., “sitins”) or phrases starting with a noun (e.g., “disobedience of order”), even a passive form phrase structure like “rallies held (in)”. In order to address this issue, we propose a simple yet effective bilingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn new event phrases and their syntactic structures, we map phrases2 back and ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 “xcomp” is a dependency relation between a verb or a"
C16-1136,D08-1021,0,0.0361866,"uce training data of monolingual model. Che et al. (2013) exploited the complementary cues between two languages as bilingual constraints to help detect errors in a mono-lingual tagger task, which can improve the annotation quality of named entities. Zhu et al. (2013) translated English sentences into Chinese sentences (with the same topic) in ACE 2005 evaluation data with google machine translation system as a second text representation feature 1448 so as to alleviate the data sparseness problem effectively. Our method is also related to paraphrase learning (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Snover et al., 2009; Ganitkevitch et al., 2013). However, there are two significant differences. First, paraphrase learning translates phrases strictly via word alignments while we use word alignments to find phrase spans on the target language. Second, our purpose is to obtain structured phrases (with syntactic constraints) rather than plain phrases as structured phrases can help us find new phrase structures as shown in Section 3. 7 Conclusion and Future Work We have presented a bilingual structure projection algorithm that explores structural divergences between languag"
C16-1136,W09-2307,0,0.0251991,"auses, LC: localizers, PU: punctuations, CD: cardinal numbers, MSP: some particles. 1445 Method H&R’s Iter #4 Iteration 1 Iteration 2 Iteration 3 Iteration 4 Phrases EP:623 PP:569 EP:1096 PP:2219 EP:4273 PP:4597 EP:8041 PP:9169 EP:9868 PP:11705 Recall Precision F1 71 88 79 76.2 86.5 81.1 79.2 86.0 82.5 79.2 86.0 82.5 79.2 86.0 82.5 Table 1: Results of the projection method using H&R’s phrase lists as seed phrases for expansion and projection Figure 5: F1-score curve against the number of iterations LDC2004T07. We ran Giza++ (Och, 2003) and Stanford dependency parser (De Marneffe et al., 2006; Chang et al., 2009) on the parallel sentence pairs to obtain word alignments and dependency trees. In addition, we used the same evaluation method and data as H&R’s. The evaluation data contains 400 news articles that were randomly sampled from the English Gigaword Fifth Edition corpora (Parker et al., 2011). Each article contains one of six commonly used civil unrest keywords or their morphological variations. The development set contains 100 documents and the rest 300 documents are used as the test set. 4.2 Event Recognition with Expanded Phrases We examine the effectiveness of our bilingual structure projecti"
C16-1136,N13-1006,0,0.136997,", “disobedience of order”), even a passive form phrase structure like “rallies held (in)”. In order to address this issue, we propose a simple yet effective bilingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn new event phrases and their syntactic structures, we map phrases2 back and ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 “xcomp” is a dependency relation between a verb or an adjective and its open clausal complement in a dependency tree"
C16-1136,W09-2209,0,0.180693,"e yet effective bilingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn new event phrases and their syntactic structures, we map phrases2 back and ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 “xcomp” is a dependency relation between a verb or an adjective and its open clausal complement in a dependency tree. In sentence “Workers came out to demonstrate”, the relation between verb “came” and verb “demonstrate” is “xcomp”. 2 We start with initial p"
C16-1136,P11-1061,0,0.0145497,"allies held in”. In addition, we have seen some new verb structures in English phrases that consist of a single verb or a verb with complex objects as shown in Table 5. 6 Related Work Recent years have witnessed increasing interests in leveraging bilingual corpora or resources to improve performance of monolingual NLP tasks. Generally, The introduction of bilingual corpora or resources serves two purposes. The first purpose is to alleviate the problem that we have few labeled instances in some resource-impoverished languages by a resource-rich language (Hwa et al., 2005; Ganchev et al., 2009; Das and Petrov, 2011; He et al., 2015). The second purpose is to leverage divergences found in different languages to obtain complementary cues (Li et al., 2012; Wang et al., 2013; Che et al., 2013) or extra information (Snyder et al., 2009; Burkett et al., 2010) from another language. Our projection method follows the latter. In the first purpose, Das and Petrov (2011) explored existing abundant English labeled resources as features to assist building tools for eight European languages. Different to projecting labels as feature, Wang and Manning (2014) proposed a method that projected model expectations as featu"
C16-1136,de-marneffe-etal-2006-generating,0,0.0528503,"Missing"
C16-1136,P09-1042,0,0.0313473,"voiced verb phrase “rallies held in”. In addition, we have seen some new verb structures in English phrases that consist of a single verb or a verb with complex objects as shown in Table 5. 6 Related Work Recent years have witnessed increasing interests in leveraging bilingual corpora or resources to improve performance of monolingual NLP tasks. Generally, The introduction of bilingual corpora or resources serves two purposes. The first purpose is to alleviate the problem that we have few labeled instances in some resource-impoverished languages by a resource-rich language (Hwa et al., 2005; Ganchev et al., 2009; Das and Petrov, 2011; He et al., 2015). The second purpose is to leverage divergences found in different languages to obtain complementary cues (Li et al., 2012; Wang et al., 2013; Che et al., 2013) or extra information (Snyder et al., 2009; Burkett et al., 2010) from another language. Our projection method follows the latter. In the first purpose, Das and Petrov (2011) explored existing abundant English labeled resources as features to assist building tools for eight European languages. Different to projecting labels as feature, Wang and Manning (2014) proposed a method that projected model"
C16-1136,N13-1092,0,0.0388965,"Missing"
C16-1136,georgi-etal-2012-measuring,0,0.024285,"d are ignored by the proposed algorithm. For instance, a verb phrase where two verbs are connected with a particular dependency relation “xcomp”1 , (e.g., “came out to demonstrate”) is one of these structures. Civil unrest events can also be invoked by some noun structure phrases, such as just a noun word phrase (e.g., “sitins”) or phrases starting with a noun (e.g., “disobedience of order”), even a passive form phrase structure like “rallies held (in)”. In order to address this issue, we propose a simple yet effective bilingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn"
C16-1136,N13-1005,1,0.716283,"languages (Chinese and English) and mines new phrases with new syntactic structures, which have been ignored in the previous work. Experiments show that our approach can successfully find novel event phrases and structures, e.g., phrases headed by nouns. Furthermore, the newly mined phrases are capable of recognizing additional event descriptions and increasing the recall of event recognition. 1 Introduction Event recognition aims to identify documents that describe a specific type of event. Accurate event recognition is challenging due to ambiguities of event keywords. In the previous work, Huang and Riloff (2013) (hereafter H&R) proposed multi-faceted event recognition method that uses event expressions as well as event defining characteristics (aka “event facets”, such as “agents” and “purpose”) to achieve high accuracy in identifying civil unrest events. They also presented a bootstrapping solution that can learn event expressions and event facet phrases from unannotated texts. However, to achieve high quality phrases, strict syntactic constraints have been enforced and their bootstrapping algorithm can only learn two particular types of V-O (Verb-Object) Structure for both event expressions and fac"
C16-1136,P03-1021,0,0.0690839,"Missing"
C16-1136,P09-1009,0,0.0352952,"g interests in leveraging bilingual corpora or resources to improve performance of monolingual NLP tasks. Generally, The introduction of bilingual corpora or resources serves two purposes. The first purpose is to alleviate the problem that we have few labeled instances in some resource-impoverished languages by a resource-rich language (Hwa et al., 2005; Ganchev et al., 2009; Das and Petrov, 2011; He et al., 2015). The second purpose is to leverage divergences found in different languages to obtain complementary cues (Li et al., 2012; Wang et al., 2013; Che et al., 2013) or extra information (Snyder et al., 2009; Burkett et al., 2010) from another language. Our projection method follows the latter. In the first purpose, Das and Petrov (2011) explored existing abundant English labeled resources as features to assist building tools for eight European languages. Different to projecting labels as feature, Wang and Manning (2014) proposed a method that projected model expectations as feature for training. He et al. (2015) transferred the sentiment information of a resource-rich language to replenish the lost information of the target language. In the second purpose, Chen and Ji (2009) proposed a bootstrap"
C16-1136,P09-1027,0,0.0438148,"ingual structure projection method that explores syntactic divergences (Georgi et al., 2012) between two languages and mines new syntactic structures for event expressions and event facet phrases effectively using parallel corpora. This is inspired by many recent cross-lingual research that utilize the second language to provide a different view (Balcan and Blum, 2005; Burkett et al., 2010; Ganchev et al., 2012) and complementary cues (Che et al., 2013; Wang et al., 2013) in improving Natural language Processing (NLP) tasks for the target language, analogous to co-training (Chen and Ji, 2009; Wan, 2009; Hajmohammadi et al., 2015) but between two different languages. In order to learn new event phrases and their syntactic structures, we map phrases2 back and ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 “xcomp” is a dependency relation between a verb or an adjective and its open clausal complement in a dependency tree. In sentence “Workers came out to demonstrate”, the relation between verb “came” and verb “demonstrate” is “xcomp”. 2 We start with initial phrases lear"
C16-1136,Q14-1005,0,0.0156564,"resource-rich language (Hwa et al., 2005; Ganchev et al., 2009; Das and Petrov, 2011; He et al., 2015). The second purpose is to leverage divergences found in different languages to obtain complementary cues (Li et al., 2012; Wang et al., 2013; Che et al., 2013) or extra information (Snyder et al., 2009; Burkett et al., 2010) from another language. Our projection method follows the latter. In the first purpose, Das and Petrov (2011) explored existing abundant English labeled resources as features to assist building tools for eight European languages. Different to projecting labels as feature, Wang and Manning (2014) proposed a method that projected model expectations as feature for training. He et al. (2015) transferred the sentiment information of a resource-rich language to replenish the lost information of the target language. In the second purpose, Chen and Ji (2009) proposed a bootstrap framework of co-training among two languages, which uses Chinese event extraction as a case study and bilingual texts as a new source of information. Burkett et al. (2010) attached a bilingual model as a second view (Balcan and Blum, 2005; Ganchev et al., 2012) onto original monolingual models, and used rich features"
C16-1136,P08-1089,0,0.0303828,"onolingual model. Che et al. (2013) exploited the complementary cues between two languages as bilingual constraints to help detect errors in a mono-lingual tagger task, which can improve the annotation quality of named entities. Zhu et al. (2013) translated English sentences into Chinese sentences (with the same topic) in ACE 2005 evaluation data with google machine translation system as a second text representation feature 1448 so as to alleviate the data sparseness problem effectively. Our method is also related to paraphrase learning (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008; Snover et al., 2009; Ganitkevitch et al., 2013). However, there are two significant differences. First, paraphrase learning translates phrases strictly via word alignments while we use word alignments to find phrase spans on the target language. Second, our purpose is to obtain structured phrases (with syntactic constraints) rather than plain phrases as structured phrases can help us find new phrase structures as shown in Section 3. 7 Conclusion and Future Work We have presented a bilingual structure projection algorithm that explores structural divergences between languages and can effectiv"
C16-1203,D08-1007,0,0.0248665,"cies for SMT. Selectional preferences place semantic restrictions on words, with which words can co-occur in different syntactic patterns. To be more specific, the SPs of a verb can characterize the semantic restrictions that the verb imposes on its arguments. Violating these restrictions inevitably makes sentence senses odd or implausible. For example, in the sentence “The ball drinks a potato.”, both subject and object preferences for the verb “drink” are violated. SPs have proven useful for numerous applications, e.g., semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997) and many more. Therefore, we have sufficient theoretical foundation to believe that SPs between verbs and arguments can be used to alleviate translation errors that we pointed out above. Our work consists of two parts: modeling SPs for verbs and incorporating SPs into an SMT system. In particular, we focus on the verb-object (v, obj) and verb-subject (v, subj) selectional preference instances which can be extracted from our target-side corpus. SPs are computed in two ways: conditionally ∗ Corresponding author T"
C16-1203,J07-2003,0,0.106199,"e fly. As for unseen object headword wun of a verb, we use Eq. (6) to model SPs when integrating conditional probability-based SP model and Eq. (3) to model SPs when integrating topicbased SP model. We store the selectional strength of (v, wun ) for each unknown word so as to avoid repetitive computation. Figure 2 shows the architecture of the SMT system equipped with verbal SPs translation model. Since the system we used is based on a CKY-style decoder, the integration algorithm introduced here can be easily adapted to other CKY-based decoding systems such as the hierarchical phrasal system (Chiang, 2007). 6 Experiments In order to validate the effectiveness of our SMT system enhanced with SPs, we perform a series of experiments on Chinese-to-English translation, which are trained with massive data. Specially, we aim at investigating: • Whether integrating SPs into SMT can improve the system translation accuracy. • Which can achieve better performance, conditionally probabilistic SP model or topic-based SP model? • Whether semantic similarity-based approach is more reasonable than assigning a uniform value as the selectional strength that a verb imposes on its unseen argument headwords. • Whet"
C16-1203,P07-1028,0,0.0110378,"nal association that uses WordNet synsets to provide conceptual classes for nouns co-occurring with a specific predicate in a particular relation. Li and Abe (1998) also rely on WordNet and use the principle of Minimum Description Length to find a suitable generalization level of a noun. But entirely relying on WordNet to generalize nouns to semantic classes has a fatal disadvantage because WordNet is lack of coverage of proper nouns. Therefore, Rooth et al. (1999) propose a probabilistic latent variable model using Expectation-Maximization (EM) clustering algorithm to induce class-based SPs. Erk (2007) investigates a similarity-based model which takes advantage of a corpus-based distributional similarity metrics between arguments for SPs. More recently, a number of researchers come up with methods modeling SPs via unsupervised topic models where topics express a set of latent classes for preferences with different grammatical relations. S´eaghdha (2010) describes a model using latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute SPs composed of a predicate and a single argument. In contrast, Ritter et al. (2010) study acquiring selectional preferences of a predicate and multiple"
C16-1203,J02-3001,0,0.0613325,"ces (SPs) to handle these verb-argument dependencies for SMT. Selectional preferences place semantic restrictions on words, with which words can co-occur in different syntactic patterns. To be more specific, the SPs of a verb can characterize the semantic restrictions that the verb imposes on its arguments. Violating these restrictions inevitably makes sentence senses odd or implausible. For example, in the sentence “The ball drinks a potato.”, both subject and object preferences for the verb “drink” are violated. SPs have proven useful for numerous applications, e.g., semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997) and many more. Therefore, we have sufficient theoretical foundation to believe that SPs between verbs and arguments can be used to alleviate translation errors that we pointed out above. Our work consists of two parts: modeling SPs for verbs and incorporating SPs into an SMT system. In particular, we focus on the verb-object (v, obj) and verb-subject (v, subj) selectional preference instances which can be extracted from our target-side corpus. SPs are computed in two w"
C16-1203,C10-1081,0,0.0242483,"encies between arguments and their dominating verbs. Verbs and arguments are often either incorrectly translated or not translated at all according to the error study by Wu and Fung (2009a). In order to address this issue, predicate-argument structures (PAS), which identify semantic frames within sentences by marking predicates, and labeling arguments with semantic roles, have been explored for SMT via various approaches in recent years. Wu and Fung (2009b) employ target-side PAS to pick out the most suitable translations among translation candidates after the decoding procedure is completed. Gildea (2010) integrates the PAS knowledge into decoding through projecting source-side PAS to the target-side via word alignments. In this paper, we are particularly interested in long-distance dependencies between verbs and their arguments in a predicate-argument structure. We propose to utilize selectional preferences (SPs) to handle these verb-argument dependencies for SMT. Selectional preferences place semantic restrictions on words, with which words can co-occur in different syntactic patterns. To be more specific, the SPs of a verb can characterize the semantic restrictions that the verb imposes on"
C16-1203,J98-2002,0,0.0250297,"nown words in SPs. Section 5 describes how we integrate the verbal SPs into SMT. Section 6 reports the experimental results. In the last section, we conclude with future directions. 2 Related Work Recent two decades have witnessed increasing efforts on automatic acquisition of SPs for verbs as well as wide applications of SPs in NLP tasks. Resnik (1996) is a pioneer on the induction of SPs from corpus, proposing a class-based approach named selectional association that uses WordNet synsets to provide conceptual classes for nouns co-occurring with a specific predicate in a particular relation. Li and Abe (1998) also rely on WordNet and use the principle of Minimum Description Length to find a suitable generalization level of a noun. But entirely relying on WordNet to generalize nouns to semantic classes has a fatal disadvantage because WordNet is lack of coverage of proper nouns. Therefore, Rooth et al. (1999) propose a probabilistic latent variable model using Expectation-Maximization (EM) clustering algorithm to induce class-based SPs. Erk (2007) investigates a similarity-based model which takes advantage of a corpus-based distributional similarity metrics between arguments for SPs. More recently,"
C16-1203,P10-1113,0,0.0311158,"s ) NIST04 36.40 36.93∗ 37.09∗ 36.89 36.99∗ 37.15∗∗ NIST05 33.69 34.22∗∗ 34.43∗∗ 34.19∗ 34.37∗∗ 34.21∗∗ Table 2: Results of conditionally probabilistic SPs with two selectional relations: (v, obj) and (v, sub). **/*: significantly better than the baseline at p &lt; 0.01 and p &lt; 0.05 respectively. Our 4-gram language model was trained on the Xinhua section of the English Gigaword corpus using the SRILM toolkit with modified Kneser-Ney smoothing. In order to automatically learn SPs for verbs, we first parsed all source sentences using Stanford Parser and then ran the Chinese semantic role labeler (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbs. At the same time, we ran SENNA on the target side to not only parse all target sentences but also conduct semantic role labeling for all verbs. It is easy to extract (vt , objt ) pairs or (vt , subt ) pairs after we obtained semantic roles on both sides. As for extracting (vs , objt ) selectional tuples, we first extracted (vs , objs ) pairs from source sentences with PAS and then used word alignments to get the target-side translation objt of objs . We used GibbsLDA++ to infer topics for our topic-based SP models. We set the"
C16-1203,P03-1021,0,0.0393616,"of topics according to results on our development set. We trained word embeddings with word2vec using continuous bag-of-words model (Mikolov et al., 2013). The word vector dimensionality was set to 200 and we set the value of threshold for occurrence of words to 0.00001. Values of other parameters such as the training algorithm and the size of the window were all set by default. We adopted the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We used the case-insensitive BLEU-4 (Papineni et al., 2002) to evaluate translation quality and run MERT (Och, 2003) three times. We finally recorded average BLEU scores over the three runs for all our experiments and used MultEval toolkit3 to perform the significance test. 6.2 Results Our first group of experiments is to investigate whether a simple conditional probability method for modeling SPs is able to improve translation accuracy in terms of BLEU. Moreover, we also would like to know whether the similarity-based SP model for unseen argument headwords will achieve further improvements. Experimental results are shown in Table 1. From the experiments which are conducted only using monolingual SPs, we ca"
C16-1203,N07-1071,0,0.0265878,"e semantic restrictions on words, with which words can co-occur in different syntactic patterns. To be more specific, the SPs of a verb can characterize the semantic restrictions that the verb imposes on its arguments. Violating these restrictions inevitably makes sentence senses odd or implausible. For example, in the sentence “The ball drinks a potato.”, both subject and object preferences for the verb “drink” are violated. SPs have proven useful for numerous applications, e.g., semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997) and many more. Therefore, we have sufficient theoretical foundation to believe that SPs between verbs and arguments can be used to alleviate translation errors that we pointed out above. Our work consists of two parts: modeling SPs for verbs and incorporating SPs into an SMT system. In particular, we focus on the verb-object (v, obj) and verb-subject (v, subj) selectional preference instances which can be extracted from our target-side corpus. SPs are computed in two ways: conditionally ∗ Corresponding author This work is licensed under a Creative Com"
C16-1203,P02-1040,0,0.102474,"50 to 350 with an incremental interval 50. We found the best number of topics according to results on our development set. We trained word embeddings with word2vec using continuous bag-of-words model (Mikolov et al., 2013). The word vector dimensionality was set to 200 and we set the value of threshold for occurrence of words to 0.00001. Values of other parameters such as the training algorithm and the size of the window were all set by default. We adopted the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We used the case-insensitive BLEU-4 (Papineni et al., 2002) to evaluate translation quality and run MERT (Och, 2003) three times. We finally recorded average BLEU scores over the three runs for all our experiments and used MultEval toolkit3 to perform the significance test. 6.2 Results Our first group of experiments is to investigate whether a simple conditional probability method for modeling SPs is able to improve translation accuracy in terms of BLEU. Moreover, we also would like to know whether the similarity-based SP model for unseen argument headwords will achieve further improvements. Experimental results are shown in Table 1. From the experime"
C16-1203,W97-0209,0,0.710203,"s can co-occur in different syntactic patterns. To be more specific, the SPs of a verb can characterize the semantic restrictions that the verb imposes on its arguments. Violating these restrictions inevitably makes sentence senses odd or implausible. For example, in the sentence “The ball drinks a potato.”, both subject and object preferences for the verb “drink” are violated. SPs have proven useful for numerous applications, e.g., semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al., 2008), textual inference (Pantel et al., 2007), word-sense disambiguation (Resnik, 1997) and many more. Therefore, we have sufficient theoretical foundation to believe that SPs between verbs and arguments can be used to alleviate translation errors that we pointed out above. Our work consists of two parts: modeling SPs for verbs and incorporating SPs into an SMT system. In particular, we focus on the verb-object (v, obj) and verb-subject (v, subj) selectional preference instances which can be extracted from our target-side corpus. SPs are computed in two ways: conditionally ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence"
C16-1203,P10-1044,0,0.22079,"Missing"
C16-1203,P99-1014,0,0.150282,"wide applications of SPs in NLP tasks. Resnik (1996) is a pioneer on the induction of SPs from corpus, proposing a class-based approach named selectional association that uses WordNet synsets to provide conceptual classes for nouns co-occurring with a specific predicate in a particular relation. Li and Abe (1998) also rely on WordNet and use the principle of Minimum Description Length to find a suitable generalization level of a noun. But entirely relying on WordNet to generalize nouns to semantic classes has a fatal disadvantage because WordNet is lack of coverage of proper nouns. Therefore, Rooth et al. (1999) propose a probabilistic latent variable model using Expectation-Maximization (EM) clustering algorithm to induce class-based SPs. Erk (2007) investigates a similarity-based model which takes advantage of a corpus-based distributional similarity metrics between arguments for SPs. More recently, a number of researchers come up with methods modeling SPs via unsupervised topic models where topics express a set of latent classes for preferences with different grammatical relations. S´eaghdha (2010) describes a model using latent Dirichlet allocation (LDA) (Blei et al., 2003) to compute SPs compose"
C16-1203,P10-1045,0,0.0397088,"Missing"
C16-1203,D14-1004,0,0.656145,"Missing"
C16-1203,2009.eamt-1.30,0,0.0203047,"hine translation (SMT). Although phrase-based SMT can deal with local context dependencies well, it performs rather poorly with long-distance dependencies and therefore causes a lot of lexical translation errors. Verbs and their arguments form such long-distance dependencies and play important roles in translation as they build skeletons of sentences. However, many SMT systems are not sufficient to capture long-distance dependencies between arguments and their dominating verbs. Verbs and arguments are often either incorrectly translated or not translated at all according to the error study by Wu and Fung (2009a). In order to address this issue, predicate-argument structures (PAS), which identify semantic frames within sentences by marking predicates, and labeling arguments with semantic roles, have been explored for SMT via various approaches in recent years. Wu and Fung (2009b) employ target-side PAS to pick out the most suitable translations among translation candidates after the decoding procedure is completed. Gildea (2010) integrates the PAS knowledge into decoding through projecting source-side PAS to the target-side via word alignments. In this paper, we are particularly interested in long-d"
C16-1203,N09-2004,0,0.0278013,"hine translation (SMT). Although phrase-based SMT can deal with local context dependencies well, it performs rather poorly with long-distance dependencies and therefore causes a lot of lexical translation errors. Verbs and their arguments form such long-distance dependencies and play important roles in translation as they build skeletons of sentences. However, many SMT systems are not sufficient to capture long-distance dependencies between arguments and their dominating verbs. Verbs and arguments are often either incorrectly translated or not translated at all according to the error study by Wu and Fung (2009a). In order to address this issue, predicate-argument structures (PAS), which identify semantic frames within sentences by marking predicates, and labeling arguments with semantic roles, have been explored for SMT via various approaches in recent years. Wu and Fung (2009b) employ target-side PAS to pick out the most suitable translations among translation candidates after the decoding procedure is completed. Gildea (2010) integrates the PAS knowledge into decoding through projecting source-side PAS to the target-side via word alignments. In this paper, we are particularly interested in long-d"
C16-1203,J97-3002,0,0.39082,"seen headword w to compute the value. sim(wun , w) is calculated with word2vec1 and the similarity metric: Cosine. After each word on the target-side corpus is projected into a multidimensional vector space, sim(wun , w) is computed as follows. P (ai × bi ) − − → → − wun • w i − − → → − (7) Sim(wun , w ) = −−−→ = rP − P ||swun ||× ||→ w || a 2× b 2 i i i i where ai and bi are the value of ith dimension of their word embeddings. 5 Decoding In this section, we mainly elaborate how to integrate the proposed SP models into a phrase-based SMT system built on bracketing transduction grammars (BTG) (Wu, 1997). Before we introduce the integration algorithm for SP models, we define two functions F and G on a source sentence and its predicateargument structure following Xiong et al. (2012). We use the sentence in Figure 1 as an example to make the two functions easier to be understood. • F (i, j): The function is used to find positions of all verbs and their object headwords pairs from the predicate-argument structure. These pairs are completely located within the source span (i, j). For example, in Figure 1, F (0, 4)={(2,3)}, F (0, 10)={(2,3), (4,10)} while F (0, 2)={} because the object headword “{"
C16-1203,P06-1066,1,0.472515,"to-English translation, which are trained with massive data. Specially, we aim at investigating: • Whether integrating SPs into SMT can improve the system translation accuracy. • Which can achieve better performance, conditionally probabilistic SP model or topic-based SP model? • Whether semantic similarity-based approach is more reasonable than assigning a uniform value as the selectional strength that a verb imposes on its unseen argument headwords. • Whether bilingual SPs are more effective than monolingual SPs for SMT. 6.1 Setup The baseline is a state-of-the-art BTG-based phrasal system (Xiong et al., 2006). Our training data corpora2 consist of 2.9M sentence pairs with 80.9M Chinese words and 86.4M English words. We ran GIZA++ on these corpora in both directions and then applied the “grow-diag-final” refinement rule to obtain final word alignments. Then we used all these word-aligned corpora to generate our phrase table. 2 The corpora include LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 2159 Model Base Base+Pc (objt |vt ) Base+Pc (objt |vt )+Pc (objtun |vt ) Base+Pc (subt |vt ) Base+Pc (subt |vt )+Pc (subtun |vt ) Base+Pcbil (objt |vs ) NIST04 36"
C16-1203,P12-1095,1,0.894388,"s for word sense disambiguation. Zapirain et al. (2009) employ SPs to process semantic role classification in a large dataset. Many researchers apply SPs to conduct pseudo-disambiguation tasks (Van de Cruys, 2014; Erk, 2007) in order to evaluate the performance of their methods of acquiring SPs. In contrast to plenty of applications of SPs in monolingual tasks, rather few efforts are devoted to incorporate SPs into SMT. To the best of our knowledge, we are the first to model SPs in the context of SMT. 2155 From the perspective of verb and argument translation, the most related work to ours is Xiong et al. (2012). They propose two translation models to incorporate source-side PAS into SMT. One is the predicate translation model exploring both lexical and semantic contexts to predict target-side predicates. The other is the argument reordering model which estimates the direction of target-side arguments movement relative to their predicates. The significant difference is that they separately model the translation of verbs and arguments while we model them in a unified fashion via SPs. 3 Selectional Preference Model Most approaches represent SPs for verbs as a function σ : (v, r, c) → s that maps each v"
C16-1203,P09-2019,0,0.0599932,"Missing"
C16-1240,J07-2003,0,0.0691293,"following intrinsic and extrinsic evaluations are based on this similarity measurement. 6 Experiments In this section, we carried out a series of experiments to validate the effectiveness of our proposed bilingual autoencoders on NIST Chinese-English translation tasks using large-scale bilingual training data. In particular, we investigated 1) whether our model is able to distinguish parallel sentence pairs from nonparallel sentences, and 2) whether our model can improve machine translation quality. 6.1 Setup Our translation decoder is a state-of-the-art hierarchical phrased-based SMT system (Chiang, 2007). The bilingual training data is the combination of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News), which contains 2.9M sentence pairs with 80.9M Chinese words and 86.4M English words. We used a 4-gram language model which was trained on the Xinhua section of the English Gigaword corpus (306M words) using the SRILM4 toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. In addition to the baseline decoder, we also compared our bilingual autoencoder against the abovementioned BBoWAE model (Chandar et al., 2014). To train this model, we used the"
C16-1240,P11-2031,0,0.0194853,"site direction (AS2MM). Throughout all experiments, we set B = 100, α = 0.100, λL = 10−6 , λW = 10−3 and m = 2d. With respect to the dimensionality of word embeddings, we tried four different dimensions from 25 to 100 with an increment of 25 each time. We used the NIST evaluation set of MT05 as our development set, and sets of MT06/MT08 as the test sets. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (MERT) (Och, 2003) to optimize the feature weights. In order to alleviate the instability of MERT, we followed Clark et al. (2011) to run MERT three times and report average BLEU scores over the three runs for all our MT experiments. 6.2 Intrinsic Evaluation: Semantic Analysis The first group of experiments aims at analyzing the ability of our model in distinguishing parallel sequences from nonparallel sequences, at both the phrase and sentence level. For convenience, we used MM2AS model and set d = 25 in all the following experiments. 4 http://www.speech.sri.com/projects/srilm/download.html http://www.sarathchandar.in/crl.html 6 We make this choice due to the following two reasons: 1) the correlation term is meaningless"
C16-1240,P13-1088,0,0.18244,"continuous-valued latent representations for parallel sentences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual sem"
C16-1240,P14-1006,0,0.0212393,"ntence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words embeddings (e.g. the avg representation) and hence are weak in capturing fine-grained complex linguistic phenomena. Therefore we believe that"
C16-1240,W13-3214,0,0.105523,"epresentations for parallel sentences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have pr"
C16-1240,P14-1062,0,0.0595323,"ences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, fo"
C16-1240,D14-1181,0,0.0300596,"intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following eff"
C16-1240,C12-1089,0,0.0339658,"; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are re"
C16-1240,P15-2029,0,0.0710106,"nd extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual"
C16-1240,P03-1021,0,0.00997899,"and max as the input descriptors, while avg and std as the target descriptors (MM2AS); and 2) the opposite direction (AS2MM). Throughout all experiments, we set B = 100, α = 0.100, λL = 10−6 , λW = 10−3 and m = 2d. With respect to the dimensionality of word embeddings, we tried four different dimensions from 25 to 100 with an increment of 25 each time. We used the NIST evaluation set of MT05 as our development set, and sets of MT06/MT08 as the test sets. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (MERT) (Och, 2003) to optimize the feature weights. In order to alleviate the instability of MERT, we followed Clark et al. (2011) to run MERT three times and report average BLEU scores over the three runs for all our MT experiments. 6.2 Intrinsic Evaluation: Semantic Analysis The first group of experiments aims at analyzing the ability of our model in distinguishing parallel sequences from nonparallel sequences, at both the phrase and sentence level. For convenience, we used MM2AS model and set d = 25 in all the following experiments. 4 http://www.speech.sri.com/projects/srilm/download.html http://www.sarathch"
C16-1240,P02-1040,0,0.0961855,"erms of input and target descriptors, we investigated two variants of the proposed bilingual autoencoder: 1) min and max as the input descriptors, while avg and std as the target descriptors (MM2AS); and 2) the opposite direction (AS2MM). Throughout all experiments, we set B = 100, α = 0.100, λL = 10−6 , λW = 10−3 and m = 2d. With respect to the dimensionality of word embeddings, we tried four different dimensions from 25 to 100 with an increment of 25 each time. We used the NIST evaluation set of MT05 as our development set, and sets of MT06/MT08 as the test sets. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (MERT) (Och, 2003) to optimize the feature weights. In order to alleviate the instability of MERT, we followed Clark et al. (2011) to run MERT three times and report average BLEU scores over the three runs for all our MT experiments. 6.2 Intrinsic Evaluation: Semantic Analysis The first group of experiments aims at analyzing the ability of our model in distinguishing parallel sequences from nonparallel sequences, at both the phrase and sentence level. For convenience, we used MM2AS model and set d = 25 in all the"
C16-1240,D11-1014,0,0.313555,"our bilingual antoencoder is able to learn continuous-valued latent representations for parallel sentences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence represent"
C16-1240,D15-1146,1,0.9177,"ed. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words embeddings (e.g."
C16-1240,P15-1150,0,0.163365,"uations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings ("
C16-1240,W11-0329,0,0.0345195,"arallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words embeddings (e.g. the avg representation) and hence are weak in capturing"
C16-1240,P14-1011,0,0.325772,"e language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words"
C16-1240,D15-1266,1,0.931462,"ical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2"
C16-1240,P15-1042,0,0.107821,"ally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words embeddings (e.g. the avg representation) and hence are weak in capturing fine-grained complex linguistic phenomena. Therefore we believe that modeling parallel se"
C16-1240,D13-1141,0,0.0312817,"ost of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and"
C16-1289,P14-1129,0,0.0338229,"the phrase structure inference and the other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it i"
C16-1289,P14-1066,0,0.112826,"ic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it is difficult for them to recover the semantic hierarchy (binary tree structure) of a bilingual phrase. In this respect, Su et al. (2015) improve tree construction by incorporating word alignments into their objective function. Unfortunately, they still employ the recursive auto"
C16-1289,D15-1181,0,0.100224,"n of the semantic embeddings at different levels of granularity is firstly investigated in 3072 Bilingual Semantic Supervision TreeCNN RecNN Figure 1: An illustration of the convolution-enhanced bilingual recursive neural network. ??? ??? ???? ??? ??? ???? ???? ? ?1 ? ???? ?2 ??? ?1 ??? ?2 ?3 Figure 2: An illustration of the proposed RecNN. We use a yellow/green circle to represent the preference score of a node to be an SAC/non-SAC node. (Socher et al., 2011a), where they compute an interaction matrix from which discriminative features are dynamically extracted for paraphrase identification. He et al. (2015) and Yin et al. (2015b) further extend this idea to convolutional neural network. Although our method is partially inspired by them, we implement it in a completely different manner. 3 Convolution-Enhanced Bilingual Recursive Neural Network This section elaborates the proposed ConvBRNN model, of which network structure is shown in Figure 1. We begin with the generation of phrase structures via a recursive neural network. We then elaborate how to perform convolution upon the generated phrase structures. After that, we describe our bilingual semantic similarity model. Finally, we provide a detai"
C16-1289,P15-2088,0,0.0697352,"n Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it is difficult for them to recover the semantic hierarchy (binary tree structure) of a bilingual phrase. In this respect, Su et al. (2015) improve tree construction by incorporating word alignments into their objective function. Unfortunately, they still employ the recursive autoencoder (RAE) as the underlying model to build tree s"
C16-1289,P14-1062,0,0.270956,"ion in RAE (i.e. reconstruction errors) does not allow us to fully benefit from word alignments. Therefore, we introduce a new composition criterion based on word alignment consistency. The proposed recursive neural network works in a way similar to that in (Socher et al., 2011b) except for our specific bilingual supervision. Zhang et al. (2014b) also propose a recursive neural network. However, their model mainly focuses on the composition in machine translation process (namely, swap or monotone), which is different from ours. Additionally, our model also adapts convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014) to extract semantic information encoded in phrase structures. Our model is related to the treebased convolution (Mou et al., 2015). The differences are 1) that we treat the whole tree structure as the window for convolution; and 2) that the underlying phrase structure for a sentence is generated automatically in our model, instead of taking from a given constituency or dependency tree. Besides, the exploration of the semantic embeddings at different levels of granularity is firstly investigated in 3072 Bilingual Semantic Supervision TreeCNN RecNN Figure 1: An illustration of the c"
C16-1289,D14-1181,0,0.00380963,"tion errors) does not allow us to fully benefit from word alignments. Therefore, we introduce a new composition criterion based on word alignment consistency. The proposed recursive neural network works in a way similar to that in (Socher et al., 2011b) except for our specific bilingual supervision. Zhang et al. (2014b) also propose a recursive neural network. However, their model mainly focuses on the composition in machine translation process (namely, swap or monotone), which is different from ours. Additionally, our model also adapts convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014) to extract semantic information encoded in phrase structures. Our model is related to the treebased convolution (Mou et al., 2015). The differences are 1) that we treat the whole tree structure as the window for convolution; and 2) that the underlying phrase structure for a sentence is generated automatically in our model, instead of taking from a given constituency or dependency tree. Besides, the exploration of the semantic embeddings at different levels of granularity is firstly investigated in 3072 Bilingual Semantic Supervision TreeCNN RecNN Figure 1: An illustration of the convolution-e"
C16-1289,W04-3250,0,0.0380397,"e FBIS corpus and Handsards part of LDC2004T07 corpus. We ran GIZA++3 on the training data in two directions and applied the “grow-diag-final-and” heuristic rule to obtain word alignments. We trained a 5-gram language model on the Xinhua portion of the GIGAWORD corpus using SRILM Toolkit4 with modified Kneser-Ney Smoothing. We chose the 2005 NIST MT evaluation test data as the development set, and the 2006, 2008 NIST MT evaluation test data as the test sets. We used case-insensitive BLEU-4 metric (Papineni et al., 2002) to evaluate translation quality, and conducted paired bootstrap sampling (Koehn, 2004) for significance test. Network Training To train ConvBRNN, we applied forced decoding (Wuebker et al., 2010) on the training corpus to extract high-quality bilingual phrases for model training. We tuned the optimal hyperparameters via random search method (Bergstra and Bengio, 2012) to minimize the joint error on a small portion of our training data. Finally, we set ds = dt = dsem = 50, h = 5, L = 10, k = 3, α = 0.116, λL = 2.14e−7 , λRT = 2.43e−5 , λwa = 7.33e−5 and λsem = 4.03e−6 , the L-BFGS iteration number Niter =100. To train BCorrRAE, we used the same training data and method for hyper"
C16-1289,D13-1054,0,0.12769,"NN model: one for the phrase structure inference and the other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two pr"
C16-1289,P13-1078,0,0.0210658,"o train the ConvBRNN model: one for the phrase structure inference and the other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often s"
C16-1289,P14-1140,0,0.0174179,"inference and the other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it is difficult for th"
C16-1289,D15-1279,0,0.0536095,"ber 11-17 2016. To solve these problems, we propose a convolution enhanced bilingual recursive neural network (ConvBRNN), which exploits word alignments to guide the generation of phrase structures and then integrates embeddings of different linguistic units on the phrase structures into bilingual semantic modeling. Specifically, we develop a new recursive neural network, in which the composition criterion for tree construction is the degree of consistency to word alignments rather than the reconstruction error. Furthermore, we propose a variant of the tree-based convolutional neural network (Mou et al., 2015) to fully access all embeddings on the phrase structures, which can be used to produce better phrase representations (see Section 3.2). All these make ConvBRNN more suitable for the subsequent bilingual semantic modeling, where a bilinear model is introduced to interact and compare the source and target phrase representations in terms of the degree of semantic equivalence. To train our model, we introduce two max-margin losses: one for the bilingual semantic structure inference and the other for the semantic similarity model, both of which are derivable. We conduct experiments on large-scale c"
C16-1289,P02-1038,0,0.0337119,"λRT λwa λsem kθL k2 + kθRT k2 + kθwa k2 + kθsem k2 2 2 2 2 (12) We apply L-BFGS to tune parameters based on gradients over the joint error, as implemented in (Socher et al., 2011c). Word vector embeddings θL are initialized with the toolkit Word2Vec2 on a large scale unlabeled data. Other parameters are randomly initialized according to a normal distribution (µ = 0,σ = 0.01). With the trained model parameters, we can easily obtain the dense semantic vectors for bilingual phrases. During translation, we incorporate the derived phrasal similarity feature into the standard log-linear framework (Och and Ney, 2002) of SMT for translation selection. 4 Experiment We conducted experiments on NIST Chinese-English translation task to validate the effectiveness of ConvBRNN. System Overview Our baseline decoder is a state-of-the-art phrase-based translation system equipped with a maximum entropy based reordering model, which adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006). We compared the proposed model with two models: (1) the bilingual correspondence model (BCorrRAE) proposed by Su et al. (2015); (2) the proposed model without the convolutional neural network (ConvBRNN-CNN),"
C16-1289,J03-1002,0,0.0520476,"e pairs, we design a new semantic composition metric based on word alignments. As word alignments are shared across the source and target language, they are suitable to act as a desirable bridge for modeling bilingual semantics. To achieve this goal, we first use the structural alignment consistency (SAC) (Su et al., 2015) that is the basis of our model to classify resultant nodes of semantic compositions into two categories. Specifically, if the node n covers a sub-phrase, and there exists a sub-phrase in the other language such that these two sub-phrases are consistent with word alignments (Och and Ney, 2003), we say n satisfies the structural alignment consistency, and it is referred to as an SAC node, otherwise, it is a non-SAC node. Then, we introduce two functions Scorecon (n) and Scoreinc (n) to measure the preference strength of node n to be an SAC or a non-SAC node, respectively (sac) (sac) Scorecon (n) = Wcon pn , Scoreinc (n) = Winc (sac) pn (2) (sac) where Wcon ∈R1×d and Winc ∈R1×d are parameter matrices. Furthermore, we calculate the final semantic composition score of node n as follows Scoresc (n) = exp(Scorecon (n)) exp(Scoreinc (n)) (3) Obviously, the larger Scorecon (n) is than Scor"
C16-1289,P02-1040,0,0.0963678,"corpus contains 1.0M sentence pairs (25.2M Chinese words and 29M English words) that are from the FBIS corpus and Handsards part of LDC2004T07 corpus. We ran GIZA++3 on the training data in two directions and applied the “grow-diag-final-and” heuristic rule to obtain word alignments. We trained a 5-gram language model on the Xinhua portion of the GIGAWORD corpus using SRILM Toolkit4 with modified Kneser-Ney Smoothing. We chose the 2005 NIST MT evaluation test data as the development set, and the 2006, 2008 NIST MT evaluation test data as the test sets. We used case-insensitive BLEU-4 metric (Papineni et al., 2002) to evaluate translation quality, and conducted paired bootstrap sampling (Koehn, 2004) for significance test. Network Training To train ConvBRNN, we applied forced decoding (Wuebker et al., 2010) on the training corpus to extract high-quality bilingual phrases for model training. We tuned the optimal hyperparameters via random search method (Bergstra and Bengio, 2012) to minimize the joint error on a small portion of our training data. Finally, we set ds = dt = dsem = 50, h = 5, L = 10, k = 3, α = 0.116, λL = 2.14e−7 , λRT = 2.43e−5 , λwa = 7.33e−5 and λsem = 4.03e−6 , the L-BFGS iteration nu"
C16-1289,P15-1004,0,0.0199993,"other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it is difficult for them to recover the semant"
C16-1289,D11-1014,0,0.3969,"ded vector. Unlike the work mentioned above, our model mainly explore word alignments to guide the generation of bilingual phrase structures. The most relevant work to ours is the model proposed by Su et al. (2015), where they treat word alignments as a constraint to the RAE model. However, as discussed in Section 1, the composition criterion in RAE (i.e. reconstruction errors) does not allow us to fully benefit from word alignments. Therefore, we introduce a new composition criterion based on word alignment consistency. The proposed recursive neural network works in a way similar to that in (Socher et al., 2011b) except for our specific bilingual supervision. Zhang et al. (2014b) also propose a recursive neural network. However, their model mainly focuses on the composition in machine translation process (namely, swap or monotone), which is different from ours. Additionally, our model also adapts convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014) to extract semantic information encoded in phrase structures. Our model is related to the treebased convolution (Mou et al., 2015). The differences are 1) that we treat the whole tree structure as the window for convolution; and 2) that the"
C16-1289,D15-1146,1,0.634702,"on. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it is difficult for them to recover the semantic hierarchy (binary tree structure) of a bilingual phrase. In this respect, Su et al. (2015) improve tree construction by incorporating word alignments into their objective function. Unfortunately, they still employ the recursive autoencoder (RAE) as the underlying mod"
C16-1289,J97-3002,0,0.555749,"With the trained model parameters, we can easily obtain the dense semantic vectors for bilingual phrases. During translation, we incorporate the derived phrasal similarity feature into the standard log-linear framework (Och and Ney, 2002) of SMT for translation selection. 4 Experiment We conducted experiments on NIST Chinese-English translation task to validate the effectiveness of ConvBRNN. System Overview Our baseline decoder is a state-of-the-art phrase-based translation system equipped with a maximum entropy based reordering model, which adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006). We compared the proposed model with two models: (1) the bilingual correspondence model (BCorrRAE) proposed by Su et al. (2015); (2) the proposed model without the convolutional neural network (ConvBRNN-CNN), which simply treats the embedding of root node of the phrase structure as the semantic representation of the whole phrase, instead of the convoluted one. Other components of ConvBRNN-CNN are the same as those in the ConvBRNN model. All translation systems used the log-linear framework. The adopted sub-models include: (1) rule translation probabilities in two directio"
C16-1289,P10-1049,0,0.0261707,"rections and applied the “grow-diag-final-and” heuristic rule to obtain word alignments. We trained a 5-gram language model on the Xinhua portion of the GIGAWORD corpus using SRILM Toolkit4 with modified Kneser-Ney Smoothing. We chose the 2005 NIST MT evaluation test data as the development set, and the 2006, 2008 NIST MT evaluation test data as the test sets. We used case-insensitive BLEU-4 metric (Papineni et al., 2002) to evaluate translation quality, and conducted paired bootstrap sampling (Koehn, 2004) for significance test. Network Training To train ConvBRNN, we applied forced decoding (Wuebker et al., 2010) on the training corpus to extract high-quality bilingual phrases for model training. We tuned the optimal hyperparameters via random search method (Bergstra and Bengio, 2012) to minimize the joint error on a small portion of our training data. Finally, we set ds = dt = dsem = 50, h = 5, L = 10, k = 3, α = 0.116, λL = 2.14e−7 , λRT = 2.43e−5 , λwa = 7.33e−5 and λsem = 4.03e−6 , the L-BFGS iteration number Niter =100. To train BCorrRAE, we used the same training data and method for hyper-parameter optimization. 1 Note that the source and target languages have different four sets of parameters."
C16-1289,P06-1066,1,0.623752,"rained model parameters, we can easily obtain the dense semantic vectors for bilingual phrases. During translation, we incorporate the derived phrasal similarity feature into the standard log-linear framework (Och and Ney, 2002) of SMT for translation selection. 4 Experiment We conducted experiments on NIST Chinese-English translation task to validate the effectiveness of ConvBRNN. System Overview Our baseline decoder is a state-of-the-art phrase-based translation system equipped with a maximum entropy based reordering model, which adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006). We compared the proposed model with two models: (1) the bilingual correspondence model (BCorrRAE) proposed by Su et al. (2015); (2) the proposed model without the convolutional neural network (ConvBRNN-CNN), which simply treats the embedding of root node of the phrase structure as the semantic representation of the whole phrase, instead of the convoluted one. Other components of ConvBRNN-CNN are the same as those in the ConvBRNN model. All translation systems used the log-linear framework. The adopted sub-models include: (1) rule translation probabilities in two directions, (2) lexical weigh"
C16-1289,P13-1017,0,0.0180214,"max-margin losses to train the ConvBRNN model: one for the phrase structure inference and the other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their suc"
C16-1289,N15-1091,0,0.054072,"Missing"
C16-1289,P15-1007,0,0.0593269,"Missing"
C16-1289,P14-1011,0,0.2221,"ess of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it is difficult for them to recover the semantic hierarchy (binary tree structure) of a bilingual phrase. In this respect, Su et al. (2015) improve tree construction by incorporating word alignments into their objective function. Unfortunately, they still employ"
C16-1293,P05-1048,0,0.2182,"containing ambiguous words, which have multiple meanings, the state-of-the-art phrase-based SMT is still suffering from inaccurate lexical choice which makes translations unable to correctly convey the meaning of source sentences. Recent studies show that in order to improve translation quality, one must correctly identify the most likely senses of source-side ambiguous words when selecting target translation (Gao et al., 2013; Zou et al., 2013; Zhang et al., 2014). One common approach to deal with ambiguity is to incorporate a word sense disambiguation (WSD) system into SMT system. At first, Carpuat and Wu (2005) attempt to use the senses of source ambiguous words predicted by a standard formulation of WSD directly in a word-based SMT but results are disappointing. They are skeptical of the assumption that WSD systems are useful for SMT. Instead of the standard WSD task, Vickrey et al. (2005) propose a novel formulation of WSD for SMT: directly predicting possible target translation candidates as senses for ambiguous source words. This reformulated WSD has been shown to help SMT by several subsequent studies, including later work by Carpuat and Wu (2007). Following this WSD reformulation for SMT, they"
C16-1293,D07-1007,0,0.208485,"guation (WSD) system into SMT system. At first, Carpuat and Wu (2005) attempt to use the senses of source ambiguous words predicted by a standard formulation of WSD directly in a word-based SMT but results are disappointing. They are skeptical of the assumption that WSD systems are useful for SMT. Instead of the standard WSD task, Vickrey et al. (2005) propose a novel formulation of WSD for SMT: directly predicting possible target translation candidates as senses for ambiguous source words. This reformulated WSD has been shown to help SMT by several subsequent studies, including later work by Carpuat and Wu (2007). Following this WSD reformulation for SMT, they integrate the WSD training, where sense definitions are drawn automatically from all phrasal translation candidates rather than from a predefined sense inventory into a phrase-based SMT. In addition to WSD, topic model (Blei et al., 2003) is yet another technique used to detect most likely senses of source words. Various topic-specific lexicon translation models are proposed to improve translation quality. These models can be classified into two categories: word-level translation models (Zhao and Xing, 2006; Tam et al., 2007) and phrase-level mo"
C16-1293,P07-1005,0,0.0391078,"models. Section 5 presents the way that we integrate supersense-based translation models into SMT. Section 6 discusses our experiments and results. In section 7 we summarize our findings and directions for future work. 2 Related Work The problem of accurate lexical choice is an unsolved challenge for phrase-based SMT. Much work has been done to identify proper senses of source ambiguous words to aid system in choosing appropriate translations. Integrating WSD into an SMT system is typical of this work as described in Section 1 (Carpuat and Wu, 2005; Vickrey et al., 2005; Carpuat and Wu, 2007; Chan et al., 2007). Exploring topic model for SMT is another attempt. Gong et al. (2010) introduce document-level topics to help SMT generate target translations. They use a monolingual LDA model to assign a specific topic to the document to be translated. Similarly, each phrase pair is also assigned with one specific topic. A phrase pair will be filtered from phrase table if its topic mismatches the document topic. Xiao et al. (2012) propose a topic similarity model which incorporates the rule-topic distributions on both the source and target side into a hierarchical phrase-based system for rule selection. 1 h"
C16-1293,P96-1041,0,0.145086,"mum entropy based reordering model (Xiong et al., 2006). Our training corpora are English-Spanish sentences from the Europarl parallel corpus (Koehn, 2005) consisting of 1.9M sentence pairs with 51M English words and 54M Spanish words. We ran GIZA++ on the training data in both directions and then applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain final word alignments. Our phrase table was generated according to the word-aligned data. As to the language model, we trained a separate 5-gram LM using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996) on each subcorpus4 and then interpolated them according to the corpus used for tuning. We trained our MaxEnt classifiers with the off-the-shelf MaxEnt tool.5 We performed 100 iterations of the L-BFGS algorithm implemented in the training toolkit on the collected training events from the sense-annotated data. We set the Gaussian prior to 1 to avoid overfitting. 4 There are 12 subcorpora: commoncrawl, europarl, kde4, news2007, news2008, news2009, news2010, news2011, news2012, newscommentary, openoffice, un 5 http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html 3119 System Base Max ss Max h"
C16-1293,W06-1670,0,0.162193,"into a hierarchical phrase-based system for rule selection. 1 https://code.google.com/archive/p/word2vec/ 3115 noun verb Tops body food person quantity time body consumption perception act cognition group phenomenon relation animal communication location plant shape artifact event motive possession state attribute feeling object process substance change contact possession cognition creation social communication emotion stative competition motion weather Table 1: Supersense labels for nouns and verbs in WordNet. Supersenses are useful and have been used as high-level features in various tasks. Ciaramita and Altun (2006) define a tagset based on WordNet supersenses to perform broad-coverage word sense disambiguation and information extraction which they approach as a unified tagging problem. They achieve considerable improvements over the first sense baseline. Koo and Collins (2005) utilize supersense reranking that provides a partial disambiguation step in syntactic parse to build useful latent semantic features. Other tasks like preposition sense disambiguation (Ye and Baldwin, 2007), noun compound interpretation (Tratz and Hovy, 2010) can also employ supersenses to improve performance. Word embeddings, als"
C16-1293,W03-1022,0,0.0610166,"u et al. (2013) propose a method to learn bilingual word embeddings for recognizing and quantifying semantic similarities across languages. Our work is to integrate supersenses into a phrase-based SMT. We adopt two ways to train our supersense-based models: one is a MaxEnt classifier-based model which is closely related to Xiong and Zhang’s (2014) work, the other is a model built on supersense embeddings that are different from word embeddings in that supersense embeddings can provide more high-level semantic information than word embeddings. 3 Supersense Tagging Supersenses, first defined by Ciaramita and Johnson (2003), are coarse-grained semantic labels used by lexicographers to facilitate the development of WordNet. There are 45 supersense labels, 26 for nouns, 15 for verbs, 3 for adjectives and 1 for adverbs, used in WordNet to classify synsets into several domains based on syntactic category and semantic coherence. Normally, an ambiguous word belongs to several synsets. Since supersense labels are assigned to synsets, word sense ambiguity can be preserved to a certain degree at this level. In this paper, we focus on noun and verb supersenses. Table 1 shows the corresponding supersense labels in WordNet."
C16-1293,W02-1001,0,0.0128247,"social social body stative stative change consumption consumption WN Senses 1 6 2 3 4 8 5 7 Gloss give help or assistance; be of service contribute to the furtherance of improve the condition of be of use abstain from doing; always used with a negative improve; change for the better help to some food; help with food or drink take or use Table 2: An example of grouping different fine-grained senses of a word into supersenses. WN senses: senses from WordNet. that is based on a Hidden Markov Model (HMM) trained in a discriminative way. That is, the model can be seen as a perceptron-trained HMM (Collins, 2002) that jointly models observation/label sequences. The model is trained on Semcor Corpus (Miller et al., 1993) following the experimental setting described in Ciaramita and Altun (2006). WordNet fine-grained senses are mapped to their corresponding supersense. In our case, only nouns and verbs are mapped, labeling as “NULL” the rest of the tokens (including adjectives and adverbs). In some cases “noun.Tops” refers to more specific supersenses, such as “food”, “person”, or “animal”. In those cases we substitute the “noun.Tops” with more specific label (e.g “animal” as “noun.animal”). Although th"
C16-1293,N03-1017,0,0.0755443,"nslation quality. • Whether supersense embeddings can play a role in lexical selection. 6.1 Setup Our baseline is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and augment itself with a maximum entropy based reordering model (Xiong et al., 2006). Our training corpora are English-Spanish sentences from the Europarl parallel corpus (Koehn, 2005) consisting of 1.9M sentence pairs with 51M English words and 54M Spanish words. We ran GIZA++ on the training data in both directions and then applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain final word alignments. Our phrase table was generated according to the word-aligned data. As to the language model, we trained a separate 5-gram LM using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996) on each subcorpus4 and then interpolated them according to the corpus used for tuning. We trained our MaxEnt classifiers with the off-the-shelf MaxEnt tool.5 We performed 100 iterations of the L-BFGS algorithm implemented in the training toolkit on the collected training events from the sense-annotated data. We set the Gaussian prior to 1"
C16-1293,2005.mtsummit-papers.11,0,0.0137104,"ranslation using massive training data. With the trained supersense-based translation model, we would like to investigate the following two questions: • Whether coarse-grained supersenses can improve translation quality. • Whether supersense embeddings can play a role in lexical selection. 6.1 Setup Our baseline is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and augment itself with a maximum entropy based reordering model (Xiong et al., 2006). Our training corpora are English-Spanish sentences from the Europarl parallel corpus (Koehn, 2005) consisting of 1.9M sentence pairs with 51M English words and 54M Spanish words. We ran GIZA++ on the training data in both directions and then applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain final word alignments. Our phrase table was generated according to the word-aligned data. As to the language model, we trained a separate 5-gram LM using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996) on each subcorpus4 and then interpolated them according to the corpus used for tuning. We trained our MaxEnt classifiers with the o"
C16-1293,H05-1064,0,0.0312498,"fact event motive possession state attribute feeling object process substance change contact possession cognition creation social communication emotion stative competition motion weather Table 1: Supersense labels for nouns and verbs in WordNet. Supersenses are useful and have been used as high-level features in various tasks. Ciaramita and Altun (2006) define a tagset based on WordNet supersenses to perform broad-coverage word sense disambiguation and information extraction which they approach as a unified tagging problem. They achieve considerable improvements over the first sense baseline. Koo and Collins (2005) utilize supersense reranking that provides a partial disambiguation step in syntactic parse to build useful latent semantic features. Other tasks like preposition sense disambiguation (Ye and Baldwin, 2007), noun compound interpretation (Tratz and Hovy, 2010) can also employ supersenses to improve performance. Word embeddings, also called distributed word representations, are used in many natural language processing areas such as information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), or representing semantics of words (Reisinger and Mooney, 2010). As to SM"
C16-1293,H93-1061,0,0.449264,"help or assistance; be of service contribute to the furtherance of improve the condition of be of use abstain from doing; always used with a negative improve; change for the better help to some food; help with food or drink take or use Table 2: An example of grouping different fine-grained senses of a word into supersenses. WN senses: senses from WordNet. that is based on a Hidden Markov Model (HMM) trained in a discriminative way. That is, the model can be seen as a perceptron-trained HMM (Collins, 2002) that jointly models observation/label sequences. The model is trained on Semcor Corpus (Miller et al., 1993) following the experimental setting described in Ciaramita and Altun (2006). WordNet fine-grained senses are mapped to their corresponding supersense. In our case, only nouns and verbs are mapped, labeling as “NULL” the rest of the tokens (including adjectives and adverbs). In some cases “noun.Tops” refers to more specific supersenses, such as “food”, “person”, or “animal”. In those cases we substitute the “noun.Tops” with more specific label (e.g “animal” as “noun.animal”). Although the tagger learns 41 semantic categories, we included (B) beginning and (I) continuation as supersense prefixes"
C16-1293,P03-1021,0,0.048436,"r both hardware and software. This material was collected using a support service via chat, this implies that the corpus is composed by naturally occurring utterances produced by users while interacting with a service. Only interactions composed by one question and the respective answer were included in the corpus. We also divided our test set batch2 into two parts equally batch2a and batch2q respectively. In other words, we used three test sets to verify the effectiveness of our proposed models. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as evaluation metric and ran MERT (Och, 2003) three times to alleviate the instability. We reported average BLEU scores over the three runs as final results. 6.2 Results Our first group of experiments are designed to investigate whether supersenses can be modeled like hidden senses using a MaxEnt classifier. We use the same experiment settings as Xiong and Zhang (2014) did. Especially, we also find that 10-word window is the most suitable window for extracting semantic information according to experiments. Table 3 shows the experimental results for the two SMT systems equipped with multiple MaxEnt classifiers trained on supersenses and h"
C16-1293,P02-1040,0,0.0951977,"pairs in the domain of computer and IT troubleshooting for both hardware and software. This material was collected using a support service via chat, this implies that the corpus is composed by naturally occurring utterances produced by users while interacting with a service. Only interactions composed by one question and the respective answer were included in the corpus. We also divided our test set batch2 into two parts equally batch2a and batch2q respectively. In other words, we used three test sets to verify the effectiveness of our proposed models. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as evaluation metric and ran MERT (Och, 2003) three times to alleviate the instability. We reported average BLEU scores over the three runs as final results. 6.2 Results Our first group of experiments are designed to investigate whether supersenses can be modeled like hidden senses using a MaxEnt classifier. We use the same experiment settings as Xiong and Zhang (2014) did. Especially, we also find that 10-word window is the most suitable window for extracting semantic information according to experiments. Table 3 shows the experimental results for the two SMT systems equipped with multiple M"
C16-1293,N10-1013,0,0.0388089,"ense baseline. Koo and Collins (2005) utilize supersense reranking that provides a partial disambiguation step in syntactic parse to build useful latent semantic features. Other tasks like preposition sense disambiguation (Ye and Baldwin, 2007), noun compound interpretation (Tratz and Hovy, 2010) can also employ supersenses to improve performance. Word embeddings, also called distributed word representations, are used in many natural language processing areas such as information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), or representing semantics of words (Reisinger and Mooney, 2010). As to SMT, Zou et al. (2013) propose a method to learn bilingual word embeddings for recognizing and quantifying semantic similarities across languages. Our work is to integrate supersenses into a phrase-based SMT. We adopt two ways to train our supersense-based models: one is a MaxEnt classifier-based model which is closely related to Xiong and Zhang’s (2014) work, the other is a model built on supersense embeddings that are different from word embeddings in that supersense embeddings can provide more high-level semantic information than word embeddings. 3 Supersense Tagging Supersenses, fi"
C16-1293,P10-1070,0,0.0122164,"useful and have been used as high-level features in various tasks. Ciaramita and Altun (2006) define a tagset based on WordNet supersenses to perform broad-coverage word sense disambiguation and information extraction which they approach as a unified tagging problem. They achieve considerable improvements over the first sense baseline. Koo and Collins (2005) utilize supersense reranking that provides a partial disambiguation step in syntactic parse to build useful latent semantic features. Other tasks like preposition sense disambiguation (Ye and Baldwin, 2007), noun compound interpretation (Tratz and Hovy, 2010) can also employ supersenses to improve performance. Word embeddings, also called distributed word representations, are used in many natural language processing areas such as information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), or representing semantics of words (Reisinger and Mooney, 2010). As to SMT, Zou et al. (2013) propose a method to learn bilingual word embeddings for recognizing and quantifying semantic similarities across languages. Our work is to integrate supersenses into a phrase-based SMT. We adopt two ways to train our supersense-based model"
C16-1293,H05-1097,0,0.227933,"on quality, one must correctly identify the most likely senses of source-side ambiguous words when selecting target translation (Gao et al., 2013; Zou et al., 2013; Zhang et al., 2014). One common approach to deal with ambiguity is to incorporate a word sense disambiguation (WSD) system into SMT system. At first, Carpuat and Wu (2005) attempt to use the senses of source ambiguous words predicted by a standard formulation of WSD directly in a word-based SMT but results are disappointing. They are skeptical of the assumption that WSD systems are useful for SMT. Instead of the standard WSD task, Vickrey et al. (2005) propose a novel formulation of WSD for SMT: directly predicting possible target translation candidates as senses for ambiguous source words. This reformulated WSD has been shown to help SMT by several subsequent studies, including later work by Carpuat and Wu (2007). Following this WSD reformulation for SMT, they integrate the WSD training, where sense definitions are drawn automatically from all phrasal translation candidates rather than from a predefined sense inventory into a phrase-based SMT. In addition to WSD, topic model (Blei et al., 2003) is yet another technique used to detect most"
C16-1293,J97-3002,0,0.01683,"12) ssrci , stsrci ) ssrci ∈Γ where Γ is a set of source phrases which have translation rules in the phrase table. 6 Experiments In this section, we conducted a series of experiments on English-to-Spanish translation using massive training data. With the trained supersense-based translation model, we would like to investigate the following two questions: • Whether coarse-grained supersenses can improve translation quality. • Whether supersense embeddings can play a role in lexical selection. 6.1 Setup Our baseline is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and augment itself with a maximum entropy based reordering model (Xiong et al., 2006). Our training corpora are English-Spanish sentences from the Europarl parallel corpus (Koehn, 2005) consisting of 1.9M sentence pairs with 51M English words and 54M Spanish words. We ran GIZA++ on the training data in both directions and then applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain final word alignments. Our phrase table was generated according to the word-aligned data. As to the language model, we trained a separate 5-gram LM using the SRILM toolki"
C16-1293,P12-1079,1,0.932049,"owing this WSD reformulation for SMT, they integrate the WSD training, where sense definitions are drawn automatically from all phrasal translation candidates rather than from a predefined sense inventory into a phrase-based SMT. In addition to WSD, topic model (Blei et al., 2003) is yet another technique used to detect most likely senses of source words. Various topic-specific lexicon translation models are proposed to improve translation quality. These models can be classified into two categories: word-level translation models (Zhao and Xing, 2006; Tam et al., 2007) and phrase-level models (Xiao et al., 2012). Especially, Xiong and Zhang (2014) propose a sense-based translation model that integrates hidden word senses into machine ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/ Licence details: 3114 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3114–3123, Osaka, Japan, December 11-17 2016. translation to investigate whether hidden senses are useful for SMT. They resort to word sense induction (WSI) and build a broad-coverage"
C16-1293,P14-1137,1,0.817405,"SMT, they integrate the WSD training, where sense definitions are drawn automatically from all phrasal translation candidates rather than from a predefined sense inventory into a phrase-based SMT. In addition to WSD, topic model (Blei et al., 2003) is yet another technique used to detect most likely senses of source words. Various topic-specific lexicon translation models are proposed to improve translation quality. These models can be classified into two categories: word-level translation models (Zhao and Xing, 2006; Tam et al., 2007) and phrase-level models (Xiao et al., 2012). Especially, Xiong and Zhang (2014) propose a sense-based translation model that integrates hidden word senses into machine ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/ Licence details: 3114 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3114–3123, Osaka, Japan, December 11-17 2016. translation to investigate whether hidden senses are useful for SMT. They resort to word sense induction (WSI) and build a broad-coverage sense tagger that relies on the nonp"
C16-1293,P06-1066,1,0.719581,"e phrase table. 6 Experiments In this section, we conducted a series of experiments on English-to-Spanish translation using massive training data. With the trained supersense-based translation model, we would like to investigate the following two questions: • Whether coarse-grained supersenses can improve translation quality. • Whether supersense embeddings can play a role in lexical selection. 6.1 Setup Our baseline is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and augment itself with a maximum entropy based reordering model (Xiong et al., 2006). Our training corpora are English-Spanish sentences from the Europarl parallel corpus (Koehn, 2005) consisting of 1.9M sentence pairs with 51M English words and 54M Spanish words. We ran GIZA++ on the training data in both directions and then applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain final word alignments. Our phrase table was generated according to the word-aligned data. As to the language model, we trained a separate 5-gram LM using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1996) on each subcorpus4 and then int"
C16-1293,S07-1051,0,0.0291408,"abels for nouns and verbs in WordNet. Supersenses are useful and have been used as high-level features in various tasks. Ciaramita and Altun (2006) define a tagset based on WordNet supersenses to perform broad-coverage word sense disambiguation and information extraction which they approach as a unified tagging problem. They achieve considerable improvements over the first sense baseline. Koo and Collins (2005) utilize supersense reranking that provides a partial disambiguation step in syntactic parse to build useful latent semantic features. Other tasks like preposition sense disambiguation (Ye and Baldwin, 2007), noun compound interpretation (Tratz and Hovy, 2010) can also employ supersenses to improve performance. Word embeddings, also called distributed word representations, are used in many natural language processing areas such as information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), or representing semantics of words (Reisinger and Mooney, 2010). As to SMT, Zou et al. (2013) propose a method to learn bilingual word embeddings for recognizing and quantifying semantic similarities across languages. Our work is to integrate supersenses into a phrase-based SMT."
C16-1293,P06-2124,0,0.0338829,"ent studies, including later work by Carpuat and Wu (2007). Following this WSD reformulation for SMT, they integrate the WSD training, where sense definitions are drawn automatically from all phrasal translation candidates rather than from a predefined sense inventory into a phrase-based SMT. In addition to WSD, topic model (Blei et al., 2003) is yet another technique used to detect most likely senses of source words. Various topic-specific lexicon translation models are proposed to improve translation quality. These models can be classified into two categories: word-level translation models (Zhao and Xing, 2006; Tam et al., 2007) and phrase-level models (Xiao et al., 2012). Especially, Xiong and Zhang (2014) propose a sense-based translation model that integrates hidden word senses into machine ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/ Licence details: 3114 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3114–3123, Osaka, Japan, December 11-17 2016. translation to investigate whether hidden senses are useful for SMT. They r"
C16-1293,D13-1141,0,0.177155,"gle words, are used as translation units so that useful context information can be captured for selecting appropriate translations. Even so, when translating sentences containing ambiguous words, which have multiple meanings, the state-of-the-art phrase-based SMT is still suffering from inaccurate lexical choice which makes translations unable to correctly convey the meaning of source sentences. Recent studies show that in order to improve translation quality, one must correctly identify the most likely senses of source-side ambiguous words when selecting target translation (Gao et al., 2013; Zou et al., 2013; Zhang et al., 2014). One common approach to deal with ambiguity is to incorporate a word sense disambiguation (WSD) system into SMT system. At first, Carpuat and Wu (2005) attempt to use the senses of source ambiguous words predicted by a standard formulation of WSD directly in a word-based SMT but results are disappointing. They are skeptical of the assumption that WSD systems are useful for SMT. Instead of the standard WSD task, Vickrey et al. (2005) propose a novel formulation of WSD for SMT: directly predicting possible target translation candidates as senses for ambiguous source words."
C18-1050,2013.mtsummit-papers.5,0,0.0180382,"opose a dynamic adaptive translation model using cache-based implementation for interactive machine translation, and develop a monolingual dynamic adaptive model and a bilingual dynamic adaptive model. Tiedemann (2010) propose a cache-based translation model, filling the cache with bilingual phrase pairs from the best translation hypotheses of previous sentences in a document. Gong et al. (2011) further propose a cache-based approach 597 to document-level translation, which includes three caches, a dynamic cache, a static cache and a topic cache, to capture various document-level information. Bertoldi et al. (2013) describe a cache mechanism to implement online learning in phrase-based SMT and use a repetition rate measure to predict the utility of cached items expected to be useful for the current translation. Our caches are similar to those used by Gong et al. (2011) who incorporate these caches into statistical machine translation. We adapt them to neural machine translation with a neural cache model. It is worthwhile to emphasize that such adaptation is nontrivial as shown below because the two translation philosophies and frameworks are significantly different. 3 Attention-based NMT In this section"
C18-1050,H92-1020,0,0.67311,"eline. 2 Related Work In the literature, several cache-based translation models have been proposed for conventional statistical machine translation, besides traditional n-gram language models and neural language models. In this section, we will first introduce related work in cache-based language models and then in translation models. For traditional n-gram language models, Kuhn and De Mori (1990) propose a cache-based language model, which mixes a large global language model with a small local model estimated from recent items in the history of the input stream for speech recongnition. Della Pietra et al. (1992) introduce a MaxEntbased cache model by integrating a cache into a smoothed trigram language model, reporting reduction in both perplexity and word error rates. Chueh and Chien (2010) present a new topic cache model for speech recongnition based on latent Dirichlet language model by incorporating a large-span topic cache into the generation of topic mixtures. For neural language models, Huang et al. (2014) propose a cache-based RNN inference scheme, which avoids repeated computation of identical LM calls and caches previously computed scores and useful intermediate results and thus reduce the"
C18-1050,D11-1084,1,0.926259,"nslated in a coherent way. In the literature, such informative constraints have been occasionally investigated in statistical machine translation and achieved certain success via a variety of document-level models, such as cache-based ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 596 Proceedings of the 27th International Conference on Computational Linguistics, pages 596–606 Santa Fe, New Mexico, USA, August 20-26, 2018. language and translation models (Tiedemann, 2010; Gong et al., 2011; Nepveu et al., 2004) for the consistency constraint, topic-based coherence model (Xiong and Zhang, 2013; Tam et al., 2007) for the coherence constraint. By contrast, in neural machine translation, to the best of our knowledge, such constraints have not been explored so far. Partially inspired by the success of cache models in SMT, we propose a cache-based approach to capturing coherence for neural machine translation. Particularly, we incorporate two types of caches into NMT: a static topic cache and a dynamic cache being updated on the fly. For the topic cache, we first use a projection-bas"
C18-1050,P15-1001,0,0.0288145,"o the final word prediction probabilities via a gating mechanism. Finally, the proposed cache-based neural model is trained jointly with NMT system in an end-to-end manner. Experiments and analysis presented in this paper demonstrate that the proposed cache-based model achieves substantial improvements over several state-of-the-art SMT and NMT baselines. 1 Introduction Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) as an emerging machine translation approach, quickly achieves the state-of-the-art translation performance on many language pairs, e.g., English-French (Jean et al., 2015; Luong et al., 2015b), English-German (Shen et al., 2015; Luong et al., 2015a) and so on. In principle, NMT is established on an encoder-decoder framework, where the encoder reads a source sentence and encodes it into a fixed-length semantic vector, and the decoder generates a translation according to this vector. In spite of its current success, NMT translates sentences of a text independently, ignoring documentlevel information during translation. This largely limits its success since document-level information imposes constraints on the translations of individual sentences of a text. And s"
C18-1050,2005.iwslt-1.8,0,0.0349865,", and NIST02, NIST04, NIST06 (878, 1788, 1664 sentence pairs. respectively) as our test sets. We compared our proposed model against the following two systems: • Moses (Koehn et al., 2007): an off-the-shelf phrase-based translation system with its default setting. • RNNSearch*: our in-house attention-based NMT system which adopts the feedback attention as described in Section 3 . For Moses, we used the full training data to train the model. We ran GIZA++ (Och and Ney, 2000) on the training data in both directions, and merged alignments in two directions with “grow-diag-final” refinement rule (Koehn et al., 2005) to obtain final word alignments. We trained a 5-gram language model on the Xinhua portion of GIGA-WORD corpus using SRILM Toolkit with a modified KneserNey smoothing. For RNNSearch, we used the parallel corpus to train the attention-based NMT model. The encoder of RNNSearch consists of a forward and backward recurrent neural network. The word embedding dimension is 620 and the size of a hidden layer is 1000. The maximum length of sentences that we used to train RNNSearch in our experiments was set to 50 on both Chinese and English side. We used the most 601 Model Moses RNNSearch* + Cd + Cd, C"
C18-1050,P07-2045,0,0.00723363,"Setting We selected corpora LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and a portion of data from the corpus LDC2004T08 (Hong Kong Hansards/Laws/News) as our bilingual training data, where document boundaries are explicitly kept. In total, our training data contain 103,236 documents and 2.80M sentences. On average, each document consists of 28.4 sentences. We chose NIST05 dataset (1082 sentence pairs) as our development set, and NIST02, NIST04, NIST06 (878, 1788, 1664 sentence pairs. respectively) as our test sets. We compared our proposed model against the following two systems: • Moses (Koehn et al., 2007): an off-the-shelf phrase-based translation system with its default setting. • RNNSearch*: our in-house attention-based NMT system which adopts the feedback attention as described in Section 3 . For Moses, we used the full training data to train the model. We ran GIZA++ (Och and Ney, 2000) on the training data in both directions, and merged alignments in two directions with “grow-diag-final” refinement rule (Koehn et al., 2005) to obtain final word alignments. We trained a 5-gram language model on the Xinhua portion of GIGA-WORD corpus using SRILM Toolkit with a modified KneserNey smoothing. F"
C18-1050,D15-1166,0,0.0840203,"ediction probabilities via a gating mechanism. Finally, the proposed cache-based neural model is trained jointly with NMT system in an end-to-end manner. Experiments and analysis presented in this paper demonstrate that the proposed cache-based model achieves substantial improvements over several state-of-the-art SMT and NMT baselines. 1 Introduction Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) as an emerging machine translation approach, quickly achieves the state-of-the-art translation performance on many language pairs, e.g., English-French (Jean et al., 2015; Luong et al., 2015b), English-German (Shen et al., 2015; Luong et al., 2015a) and so on. In principle, NMT is established on an encoder-decoder framework, where the encoder reads a source sentence and encodes it into a fixed-length semantic vector, and the decoder generates a translation according to this vector. In spite of its current success, NMT translates sentences of a text independently, ignoring documentlevel information during translation. This largely limits its success since document-level information imposes constraints on the translations of individual sentences of a text. And such document-level c"
C18-1050,P15-1002,0,0.0616669,"ediction probabilities via a gating mechanism. Finally, the proposed cache-based neural model is trained jointly with NMT system in an end-to-end manner. Experiments and analysis presented in this paper demonstrate that the proposed cache-based model achieves substantial improvements over several state-of-the-art SMT and NMT baselines. 1 Introduction Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) as an emerging machine translation approach, quickly achieves the state-of-the-art translation performance on many language pairs, e.g., English-French (Jean et al., 2015; Luong et al., 2015b), English-German (Shen et al., 2015; Luong et al., 2015a) and so on. In principle, NMT is established on an encoder-decoder framework, where the encoder reads a source sentence and encodes it into a fixed-length semantic vector, and the decoder generates a translation according to this vector. In spite of its current success, NMT translates sentences of a text independently, ignoring documentlevel information during translation. This largely limits its success since document-level information imposes constraints on the translations of individual sentences of a text. And such document-level c"
C18-1050,W04-3225,0,0.393575,"nt way. In the literature, such informative constraints have been occasionally investigated in statistical machine translation and achieved certain success via a variety of document-level models, such as cache-based ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 596 Proceedings of the 27th International Conference on Computational Linguistics, pages 596–606 Santa Fe, New Mexico, USA, August 20-26, 2018. language and translation models (Tiedemann, 2010; Gong et al., 2011; Nepveu et al., 2004) for the consistency constraint, topic-based coherence model (Xiong and Zhang, 2013; Tam et al., 2007) for the coherence constraint. By contrast, in neural machine translation, to the best of our knowledge, such constraints have not been explored so far. Partially inspired by the success of cache models in SMT, we propose a cache-based approach to capturing coherence for neural machine translation. Particularly, we incorporate two types of caches into NMT: a static topic cache and a dynamic cache being updated on the fly. For the topic cache, we first use a projection-based bilingual topic lea"
C18-1050,P00-1056,0,0.187369,"2.80M sentences. On average, each document consists of 28.4 sentences. We chose NIST05 dataset (1082 sentence pairs) as our development set, and NIST02, NIST04, NIST06 (878, 1788, 1664 sentence pairs. respectively) as our test sets. We compared our proposed model against the following two systems: • Moses (Koehn et al., 2007): an off-the-shelf phrase-based translation system with its default setting. • RNNSearch*: our in-house attention-based NMT system which adopts the feedback attention as described in Section 3 . For Moses, we used the full training data to train the model. We ran GIZA++ (Och and Ney, 2000) on the training data in both directions, and merged alignments in two directions with “grow-diag-final” refinement rule (Koehn et al., 2005) to obtain final word alignments. We trained a 5-gram language model on the Xinhua portion of GIGA-WORD corpus using SRILM Toolkit with a modified KneserNey smoothing. For RNNSearch, we used the parallel corpus to train the attention-based NMT model. The encoder of RNNSearch consists of a forward and backward recurrent neural network. The word embedding dimension is 620 and the size of a hidden layer is 1000. The maximum length of sentences that we used t"
C18-1050,W10-2602,0,0.497666,"ual sentences translated in a coherent way. In the literature, such informative constraints have been occasionally investigated in statistical machine translation and achieved certain success via a variety of document-level models, such as cache-based ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 596 Proceedings of the 27th International Conference on Computational Linguistics, pages 596–606 Santa Fe, New Mexico, USA, August 20-26, 2018. language and translation models (Tiedemann, 2010; Gong et al., 2011; Nepveu et al., 2004) for the consistency constraint, topic-based coherence model (Xiong and Zhang, 2013; Tam et al., 2007) for the coherence constraint. By contrast, in neural machine translation, to the best of our knowledge, such constraints have not been explored so far. Partially inspired by the success of cache models in SMT, we propose a cache-based approach to capturing coherence for neural machine translation. Particularly, we incorporate two types of caches into NMT: a static topic cache and a dynamic cache being updated on the fly. For the topic cache, we first u"
C18-1050,P16-1125,0,0.0284115,"which stores recent hidden activations to be used as contextual representations. Our caches significantly differ from these two caches in that we store linguistic items in the cache rather than scores or activations. For neural machine translation, Wang et al. (2017) propose a cross-sentence context-aware approach and employ a hierarchy of Recurrent Neural Networks (RNNs) to summarize the cross-sentence context from source-side previous sentences. Jean et al. (2017) propose a novel larger-context neural machine translation model based on the recent works on larger-context language modelling (Wang and Cho, 2016) and employ the method to model the surrounding text in addition to the source sentence. For cache-based translation models, Nepveu et al. (2004) propose a dynamic adaptive translation model using cache-based implementation for interactive machine translation, and develop a monolingual dynamic adaptive model and a bilingual dynamic adaptive model. Tiedemann (2010) propose a cache-based translation model, filling the cache with bilingual phrase pairs from the best translation hypotheses of previous sentences in a document. Gong et al. (2011) further propose a cache-based approach 597 to documen"
C18-1050,D17-1301,0,0.0714475,"ral language models, Huang et al. (2014) propose a cache-based RNN inference scheme, which avoids repeated computation of identical LM calls and caches previously computed scores and useful intermediate results and thus reduce the computational expense of RNNLM. Grave et al. (2016) extend the neural network language model with a neural cache model, which stores recent hidden activations to be used as contextual representations. Our caches significantly differ from these two caches in that we store linguistic items in the cache rather than scores or activations. For neural machine translation, Wang et al. (2017) propose a cross-sentence context-aware approach and employ a hierarchy of Recurrent Neural Networks (RNNs) to summarize the cross-sentence context from source-side previous sentences. Jean et al. (2017) propose a novel larger-context neural machine translation model based on the recent works on larger-context language modelling (Wang and Cho, 2016) and employ the method to model the surrounding text in addition to the source sentence. For cache-based translation models, Nepveu et al. (2004) propose a dynamic adaptive translation model using cache-based implementation for interactive machine t"
C18-1051,W14-4012,0,0.156545,"Missing"
C18-1051,D11-1084,0,0.067244,"August 20-26, 2018. used to update the hidden states of the decoder. In this way, we can model the links and dependencies between adjacent sentences. To some extent, our approach models the inter-sentence relationship from the underlying semantic coherence perspective. On the NIST Chinese-English translation tasks, our experimental results show that the proposed approach can achieve significant improvements of up to 2.0 BLEU points over the NMT baseline. 2 Related Work In the literature, a series of document-level translation models have been proposed for conventional SMT. Just to name a few, Gong et al. (2011) propose a cache-based approach to document-level translation, which includes three caches, a dynamic cache, a static cache and a topic cache to capture various kind of document-level information. Hardmeier et al. (2012) present a beam search decoding procedure for phrase-based SMT with features modeling cross-sentence dependencies. Xiong and Zhang (2013) propose a topic-based coherence model to produce discourse coherence for document translation. Xiong et al. (2013) present a lexical cohesion model to capture lexical cohesion for document-level translation. In neural language models, inter-s"
C18-1051,D12-1108,0,0.020295,"ip from the underlying semantic coherence perspective. On the NIST Chinese-English translation tasks, our experimental results show that the proposed approach can achieve significant improvements of up to 2.0 BLEU points over the NMT baseline. 2 Related Work In the literature, a series of document-level translation models have been proposed for conventional SMT. Just to name a few, Gong et al. (2011) propose a cache-based approach to document-level translation, which includes three caches, a dynamic cache, a static cache and a topic cache to capture various kind of document-level information. Hardmeier et al. (2012) present a beam search decoding procedure for phrase-based SMT with features modeling cross-sentence dependencies. Xiong and Zhang (2013) propose a topic-based coherence model to produce discourse coherence for document translation. Xiong et al. (2013) present a lexical cohesion model to capture lexical cohesion for document-level translation. In neural language models, inter-sentence connections can be captured in a contextual model. For example, Lin et al. (2015) propose a hierarchical recurrent neural network (HRNN) language model for document modeling, consisting of a sentence-level and wo"
C18-1051,2005.iwslt-1.8,0,0.162887,"Missing"
C18-1051,P07-2045,0,0.00452656,". N M TISG is the proposed model without replacing UNK words. The BLEU scores are case-insensitive. Avg means the average BLEU score on all the test sets. “‡”: statistically better than RNNSearch (p <0.01). (Hong Kong Hansards/Laws/News). In total, our training data contain 103,236 documents and 2.80M sentences. Averagely, each document consists of 28.4 sentences. We chose NIST05 dataset as our development set, and NIST02, NIST03, NSIT04, NIST06, NIST08 as our test sets. We used case-insensitive BLEU-4 as our evaluation metric. We compared our N M TISG with the following two systems: • Moses (Koehn et al., 2007): an open phrase-based translation system with its default setting. • RNNSearch: our new implementation of NMT system with the feedback attention as described in Section 3. For Moses, we used the full training data (parallel corpus) to train the model. Word alignments were produced by GIZA++ (Och and Ney, 2000). We ran GIZA++ on the corpus in both directions, and merged alignments in two directions with “grow-diag-final” refinement rule (Koehn et al., 2005). We trained a 5-gram language model on the Xinhua portion of the Gigaword corpus using SRILM Toolkit with a modified Kneser-Ney smoothing."
C18-1051,D15-1106,0,0.0365021,"ludes three caches, a dynamic cache, a static cache and a topic cache to capture various kind of document-level information. Hardmeier et al. (2012) present a beam search decoding procedure for phrase-based SMT with features modeling cross-sentence dependencies. Xiong and Zhang (2013) propose a topic-based coherence model to produce discourse coherence for document translation. Xiong et al. (2013) present a lexical cohesion model to capture lexical cohesion for document-level translation. In neural language models, inter-sentence connections can be captured in a contextual model. For example, Lin et al. (2015) propose a hierarchical recurrent neural network (HRNN) language model for document modeling, consisting of a sentence-level and word-level language model, and use the proposed model to model sentence-level coherence. In speech recognition, as input speech signals can contain thousands of frames, Chan et al. (2016) employ Bidirectional Long Short Term Memory with a pyramidal structure to capture the context of a large number of input time steps. Wang and Cho (2016) introduce a late fusion method to incorporate corpus-level discourse information into recurrent language modeling. In neural conve"
C18-1051,D15-1166,0,0.048171,"act on the translation of the current sentence. However, the drop of the performance is not as big as that of N M TISG (+z=0). This suggests that the gate mechanism is able to effectively shield these useless and counteractive information from a pseudo and random preceding sentence. The three experiments further demonstrate that the proposed inter-sentence gate is able to detect useful information for translation and block unrelated information and reconfirm that inter-sentence information is useful for translation. 5.4 Analysis on Inter-Sentence Attention Many studies (Bahdanau et al., 2015; Luong et al., 2015) on attention-based NMT have proved that attention networks are able to detect alignments between parallel sentence pairs. In our N M TISG model, we use two attention networks: the first is built for the correspondences between the current source sentence and its target translation and the second for the correspondences between the preceding sentence and the target translation of the current sentence. We are interested in what correspondences the second attention network detect. We use the entropy as the evaluation criterion to measure how attention weights distribute over words in the precedi"
C18-1051,P00-1056,0,0.603009,"Missing"
C18-1051,W14-4009,0,0.0397621,"Missing"
C18-1051,W10-2602,0,0.0702361,"Missing"
C18-1051,P16-5005,0,0.137134,"e have to make sure that the decoder is provided with sufficient information from sentence B, and at the same time, with helpful imformation from the preceding sentence A. In other words, we need a mechanism to control the scale of information flowing from the sentence A and sentence B to the decoder. Inspired by the success of gated units in RNN (Chung et al., 2014), we propose an inter-sentence gate to control the amount of information flowing from A and B. Formally, we employ a sigmoid neural network layer and an element-wise multiplication operation, as illustrated in Figure 2. Similarly, Tu et al. (2016) also propose a gating mechanism to combine source and target contexts. The gate framework assigns element-wise weights z to the input signals, calculated by zt = σ(Uz st−1 + Wz yt−1 + Cb cbt + Ca cat ) (9) here σ is a logistic sigmoid function, and Uz , Wz , Cb , Ca are the parameter matrix. 4.3 Decoder Next, we integrate the inter-sentence gate into the decoder to decide the amount of context information used in producing the decoder hidden state at each time step. In this way, we want the hidden states of the decoder to store the inter-sentence context information. The framework of updating"
C18-1051,P16-1125,0,0.0224911,"ocument-level translation. In neural language models, inter-sentence connections can be captured in a contextual model. For example, Lin et al. (2015) propose a hierarchical recurrent neural network (HRNN) language model for document modeling, consisting of a sentence-level and word-level language model, and use the proposed model to model sentence-level coherence. In speech recognition, as input speech signals can contain thousands of frames, Chan et al. (2016) employ Bidirectional Long Short Term Memory with a pyramidal structure to capture the context of a large number of input time steps. Wang and Cho (2016) introduce a late fusion method to incorporate corpus-level discourse information into recurrent language modeling. In neural conversation systems, links between multi-turn conversations are usually modeled with hierarchical neural networks. Serban et al. (2015) use a hierarchical recurrent encoder-decoder(HRED) to model the dialogue into two-level hierarchy: a sequence of utterances and a sequence of words. The proposed model can track states over many utterances to generate context-aware multiple rounds of dialogue. Serban et al. (2016) further propose a HRED model with an additional compone"
C18-1051,D16-1027,0,0.018942,"sentence gate. where g is a softmax layer, st is the state of decoder RNN at time step t computed as st = f (st−1 , yt−1 , ct ). (3) where f is a function, the same as function used in the encoder. The context vector ct is calculated as a weighted sum of all hidden states of the encoder as follows: ct = Tx X αtj hj , (4) i=1 exp(etj ) , αtj = PTx k=1 exp(etk ) (5) etj = a(st−1 , hj ). (6) where αtj is the weight of each hidden state hj computed by the attention model, a is a feedforward neural network with a single hidden layer. We also implement an NMT system which adopts feedback attention (Wang et al., 2016b; Wang et al., 2016a), which will be referred to as RNNSearch in this paper. In the feedback attention, etj is computed as follows: etj = a(e st−1 , hj ), (7) where set−1 = GRU (st−1 , yt−1 ). The hidden state of the decoder is updated as follows: st = GRU (e st−1 , ct ) (8) In this paper, our proposed model is implemented on the top of RNNSearch system. 4 The Inter-Sentence Gate Model In this section, we will elaborate the proposed inter-sentence gate model, which we refer to as N M TISG . Figure 1 shows the entire architecture of our NMT with the inter-sentence gate. For notational convenie"
C18-1124,buck-etal-2014-n,0,0.0611415,"Missing"
C18-1124,P15-1001,0,0.0303019,"ranslated text conditioned on the input representation. A potential issue with this encoderdecoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with translating long sentences. A recent successful extension of NMT models is the attention mechanism which conducts a soft search over source tokens and yields an attentive vector to represent the most relevant segments of the source sentence for the current decoding state (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2014; Vaswani et al., 2017). The typical attention mechanism frees the neural translation model from having to squash all the information of the source sentence into a fixed vector, however, it ignores some important information hidden in the target sequence (Cheng et al., 2016). At least three challenges still remains in the translation process. The first issue relates to the memory compression problems in the decoding process. During each translation step, a hidden state vector implicitly maintains at least two types of information, including both the most relevant source con"
C18-1124,P17-1064,1,0.851355,"for sequence prediction. These connections create a direct path from previous layers, helping the transmission of information. Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. Comparatively, D HE A attends to the previous hidden state and make a combination with the source context. Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information(Wang and Cho, 2016; Li et al., 2017; Zhang and Zong, 2016). Recently Tu et al. (2016a) propose using context gates in NMT to dynamically control the contributions from the source contexts and the RNN hidden state. Our approach focuses on integrating the decoding history and the source side context to NMT architecture. In addition, we have a multi-layer approach to better utilize the contextual information. Experiments in Section 4.3 show the superiority of D HE A. In the same period of our work, Lin et al. (2018) and Xia et al. (2017) first turn eyes to the target side attention of NMT architecture. Our approach share the simil"
C18-1124,D15-1166,0,0.0604492,"ext incrementally from left to right. In the NMT task, Wang et al. (2016) present a decoder enhanced decoder with an external shared memory which extends the capacity of the network and has the potential to read, write, and forget information. In fact D HE A can be viewed as a special case of memory networks, with only reading mechanism for the translation task. Quite remarkably D HE A incorporates two different types of memory (source memory and decoding history memory) and significantly improves upon state-of-the-arts. Attention Mechanism Attention in neural networks (Bahdanau et al., 2014; Luong et al., 2015) is designed to assign weights to different inputs instead of threating all input sequences equally as original neural networks do. A number of efforts have been made to improve the attention mechanism(Tu et al., 2016b; Mi et al., 2016; Zhang et al., 2017). Some of them incorporated the previous attention history into the current attention for better alignment, but none of them are based on the decoding history. The application of self-attention mechanisms in RNNs have been previously studied, and in general, it appears to capture syntactic dependencies among distant words (Liu and Lapata, 201"
C18-1124,D16-1096,0,0.0185334,"ion. In fact D HE A can be viewed as a special case of memory networks, with only reading mechanism for the translation task. Quite remarkably D HE A incorporates two different types of memory (source memory and decoding history memory) and significantly improves upon state-of-the-arts. Attention Mechanism Attention in neural networks (Bahdanau et al., 2014; Luong et al., 2015) is designed to assign weights to different inputs instead of threating all input sequences equally as original neural networks do. A number of efforts have been made to improve the attention mechanism(Tu et al., 2016b; Mi et al., 2016; Zhang et al., 2017). Some of them incorporated the previous attention history into the current attention for better alignment, but none of them are based on the decoding history. The application of self-attention mechanisms in RNNs have been previously studied, and in general, it appears to capture syntactic dependencies among distant words (Liu and Lapata, 2017; Lee et al., 2017; Kim et al., 2017; Lin et al., 2017). Vaswani et al. (2017) resort to self-attention mechanism and showed outstanding performance. Our approach is diffrent from their work in two aspect. First, our method can be vie"
C18-1124,N18-1124,0,0.0379124,"Missing"
C18-1124,P02-1040,0,0.100662,"mplementation of gate combination where the the gating weights are equal to the attention weights. 4 4.1 Experiments Datasets We mainly evaluated our approaches on the widely used NIST Chinese-English translation task. In order to show the usefulness of our approaches, we also provide results on other two translation tasks: English-French, English-German. The evaluation metric is BLEU. For Chinese-English task, we apply case-insensitive NIST BLEU. For other tasks, we tokenized the reference and evaluated the performance with multi-bleu.pl. The metrics are exactly the same as in previous work (Papineni et al., 2002). For Chinese-English, our training data consists of 1.25M sentence pairs extracted from LDC corpora1 , with 27.9M Chinese words and 34.5M English words respectively. We choose NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04), 2005 (MT05), and 2006 (MT06) datasets as our test sets. For English-German, to compare with the results reported by previous work, we used the same subset of the WMT 2014 training corpus that contains 4.5M sentence pairs with 91M English words and 87M German words. The concatenation of news-test 2012 and news-test 2013 is used as the"
C18-1124,P16-5005,0,0.360122,"ination At time step i, D HE A reads from M B : zi` = Read(si , M B ) = i X ` ` αij (s`−1 j WV ) (9) j=1 Similarly, αij is computed by Eqn.(6) where j ≤ i and e`ij is computed as : 1 ` T e`ij = √ (s`−1 WQ` )(s`−1 j WK ) ds i 1467 (10) One simple way to aggregate information from zi and ci is by summing them, then the new context vector is computed as: cˆ = z + c (11) It is also worth mentioning that we use s`−1 as the context information to update the hidden state on layer `, since the lower layer states can be perpared in advance to facilitate parallel training. Gate Combination As argued by Tu et al. (2016a), the source side context and the target side context plays a different role during the decoding process, we therefore design a context gate which assigns an element-wise weight to the two-side input: cˆ = g(c, z) · c + (1 − g(c, z)) · z (12) where g(c, z) ∈ (0, 1) is a sigmoid neural network which dynamically controls the amount of information flowing from the source and target contexts. cˆ is the new context vector fed to the decoder in Eqn.(3) which refines s`t = LSTM(s`t−1 , s`−1 ˆ`t ). t ,c With the gated control, the new context vector cˆ can be rectified based on the decoding history"
C18-1124,P16-1008,0,0.340085,"ination At time step i, D HE A reads from M B : zi` = Read(si , M B ) = i X ` ` αij (s`−1 j WV ) (9) j=1 Similarly, αij is computed by Eqn.(6) where j ≤ i and e`ij is computed as : 1 ` T e`ij = √ (s`−1 WQ` )(s`−1 j WK ) ds i 1467 (10) One simple way to aggregate information from zi and ci is by summing them, then the new context vector is computed as: cˆ = z + c (11) It is also worth mentioning that we use s`−1 as the context information to update the hidden state on layer `, since the lower layer states can be perpared in advance to facilitate parallel training. Gate Combination As argued by Tu et al. (2016a), the source side context and the target side context plays a different role during the decoding process, we therefore design a context gate which assigns an element-wise weight to the two-side input: cˆ = g(c, z) · c + (1 − g(c, z)) · z (12) where g(c, z) ∈ (0, 1) is a sigmoid neural network which dynamically controls the amount of information flowing from the source and target contexts. cˆ is the new context vector fed to the decoder in Eqn.(3) which refines s`t = LSTM(s`t−1 , s`−1 ˆ`t ). t ,c With the gated control, the new context vector cˆ can be rectified based on the decoding history"
C18-1124,P16-1125,0,0.0295194,"Wang and Tian, 2016) for sequence prediction. These connections create a direct path from previous layers, helping the transmission of information. Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. Comparatively, D HE A attends to the previous hidden state and make a combination with the source context. Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information(Wang and Cho, 2016; Li et al., 2017; Zhang and Zong, 2016). Recently Tu et al. (2016a) propose using context gates in NMT to dynamically control the contributions from the source contexts and the RNN hidden state. Our approach focuses on integrating the decoding history and the source side context to NMT architecture. In addition, we have a multi-layer approach to better utilize the contextual information. Experiments in Section 4.3 show the superiority of D HE A. In the same period of our work, Lin et al. (2018) and Xia et al. (2017) first turn eyes to the target side attention of NMT architecture. Our approac"
C18-1124,D16-1093,0,0.0257529,"the potential to better handle sentences of arbitrary length. Second, we forcus on controlling the information flow between the source side memory and the target side memory and design a gate to balance the contribution of the two-sides. Recurrent Residual Networks Our work is also related to residual connections, which have been shown to improve the learning process of deep neural networks by addressing the vanishing gradient problem (He et al., 2015; Szegedy et al., 2016). Recently, several architectures using residual connections with LSTMs have been proposed (Kim et al., 2017; Wang, 2017; Wang and Tian, 2016) for sequence prediction. These connections create a direct path from previous layers, helping the transmission of information. Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. Comparatively, D HE A attends to the previous hidden state and make a combination with the source context. Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information(Wang and Cho, 2016;"
C18-1124,P17-1013,1,0.800529,"Missing"
C18-1124,D16-1160,0,0.0213139,"diction. These connections create a direct path from previous layers, helping the transmission of information. Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. Comparatively, D HE A attends to the previous hidden state and make a combination with the source context. Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information(Wang and Cho, 2016; Li et al., 2017; Zhang and Zong, 2016). Recently Tu et al. (2016a) propose using context gates in NMT to dynamically control the contributions from the source contexts and the RNN hidden state. Our approach focuses on integrating the decoding history and the source side context to NMT architecture. In addition, we have a multi-layer approach to better utilize the contextual information. Experiments in Section 4.3 show the superiority of D HE A. In the same period of our work, Lin et al. (2018) and Xia et al. (2017) first turn eyes to the target side attention of NMT architecture. Our approach share the similar idea with these work"
C18-1124,P17-1140,1,0.837212,"A can be viewed as a special case of memory networks, with only reading mechanism for the translation task. Quite remarkably D HE A incorporates two different types of memory (source memory and decoding history memory) and significantly improves upon state-of-the-arts. Attention Mechanism Attention in neural networks (Bahdanau et al., 2014; Luong et al., 2015) is designed to assign weights to different inputs instead of threating all input sequences equally as original neural networks do. A number of efforts have been made to improve the attention mechanism(Tu et al., 2016b; Mi et al., 2016; Zhang et al., 2017). Some of them incorporated the previous attention history into the current attention for better alignment, but none of them are based on the decoding history. The application of self-attention mechanisms in RNNs have been previously studied, and in general, it appears to capture syntactic dependencies among distant words (Liu and Lapata, 2017; Lee et al., 2017; Kim et al., 2017; Lin et al., 2017). Vaswani et al. (2017) resort to self-attention mechanism and showed outstanding performance. Our approach is diffrent from their work in two aspect. First, our method can be viewed as a variant of R"
C18-1124,Q16-1027,0,0.0191054,"de attention based NMT. Encoder The goal of the encoder is to build meaningful representations of source sentences. The typical encoder consists of a bidirectional RNN which processes the raw input in backward and forward direction with two separate layers, and then concatenates them together. In this work, we choose another bidirectional approach to process the sequence in order to learn more temporal dependencies. Specifically, an RNN layer processes the input sequence in a forward direction. The output of this layer is taken by an upper RNN layer as input, processed in a reverse direction (Zhou et al., 2016). More formally,The encoder network reads the source input x = {x1 , ..., xT } and processes it into a source side memory M s = {h1 , h2 · · · , hT }. where xi ∈ Rdx . The output on layer ` is  xt , `=1 ` (2) ht = `−1 ` LSTM(ht+d , ht ), ` > 1 where • h`t ∈ Rdh gives the output of layer ` at location t. • The directions are marked by a direction term d = (−1)` . If we fixed d to −1, the input will be processed in forward direction, otherwise backward direction. • We only apply the top-most hidden states as the source side memory which is then fed to the decoder. Decoder The decoder uses anoth"
C18-1269,D11-1033,0,0.358992,"g, fine-tuning it on the closer domain and finalizing it by fine-tuning it on the in-domain data. Muti-model ensemble combines multiple models during decoding using a balanced or weighted averaging method. At the data level, traditional domain adaptation approach can be done by data selection, data weighting or data joining. Data selection approaches select data similar to the in-domain data according to some criteria. Normally, the out-of-domain data can be scored by a model trained on the in-domain data and out-of-domain data. For example, a language model can be used for scoring sentences (Axelrod et al., 2011). Data weighting methods weight each item which can be a corpus, a sentence or a phrase, and then train SMT models on weighted items. Although some existing SMT domain adaptation techniques can be directly applied to NMT, it is challenging for applying data weighting to NMT. For NMT, the data selection approach can also be used. Wang et al. (2017a) employ the data selection method for domain adaptation, which uses sentence embeddings to measure the similarity of a sentence pair to the in-domain data. A recent method to apply sentence weights to NMT is cost weighting (Wang et al., 2017b; Chen e"
C18-1269,W17-3205,0,0.320031,"ic. Domain similarity has been successfully used in some tasks such as parsing, knowledge adaptation, etc.(Plank and Van Noord, 2011; Ruder et al., 2017). We employ domain similarity to measure relevance between the sentences of out-of-domain and in-domain data. In this way, we take into consideration both the goodness and the badness brought by out-of-domain data, in a weighting fashion. Additionally, different from current sentence weighting methods used for NMT that score sentences by making use of other toolkits like the SRI Language Modeling Toolkit (Stolcke, 2002), or an RNN classifier (Chen et al., 2017), our method exploits the information from the NMT system itself. This means that we do not need to train extra toolkits. We also examine the effectiveness of the proposed sentence weighting method on NMT trained with only synthetic parallel data, which is beneficial for the low-resource domain translation. Our method can also be used in back translation and in conjunction with other training methods. ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 3181 Proceedings of the"
C18-1269,kobus-etal-2017-domain,0,0.155411,"A recent method to apply sentence weights to NMT is cost weighting (Wang et al., 2017b; Chen et al., 2017). The NMT objective function is updated by sentence weighting when computing the cost of each mini-batch during NMT training. Wang et al. (2017b) exploit an in-domain language model (Axelrod et al., 2011) to score sentences. Chen et al. (2017) use a classifier to assign weights for individual sentences pairs. Domain control uses word-level domain features in the word embedding layer, aiming to allow a model to be built from a diverse set of training data to produce in-domain translations (Kobus et al., 2017). 3 NMT In this paper, we use the vanilla attention-based NMT (Bahdanau et al., 2014), which we will briefly summarize here.It is built on a recurrent neural networks (RNN) in an encoder-decoder framework (Sutskever et al., 2014; Pascanu et al., 2013). Given a source sentence x = (x1 , x2 , …, xm ) and its corresponding target sentence y = (y1 , y2 , …, yn ), NMT employs the encoder-decoder framework to jointly train to maximize the conditional probability p(y|x). To solve the problem that a basic encoder-decoder framework deteriorates rapidly as the length of input sentence increases, the att"
C18-1269,2015.iwslt-evaluation.11,0,0.309137,"o on. However, it is still confronted with a big challenge in domain adaptation as the translation quality of NMT is heavily dependent on the quantity of training data and the relevance between the training data and the in-domain testing data. The training corpus usually varies across domains, where out-of-domain instances that are relevant to the target domain are beneficial for training while those which are irrelevant to the in-domain may deteriorate translation quality. The widely-used domain adaptation method in NMT is to fine-tune an existing out-of-domain model with the in-domain data (Luong and Manning, 2015). However, the fine-tuning method is of no consideration for the harm caused by irrelevant out-of-domain data. It also tends to overfit rapidly due to the small size of the in-domain data. In this paper, we propose a sentence weighting method that evaluates the weights of sentences with respect to the relevance of sentences to the target domain. We train NMT models by assigning each sentence with a corresponding weight computed according to a domain similarity metric. Domain similarity has been successfully used in some tasks such as parsing, knowledge adaptation, etc.(Plank and Van Noord, 201"
C18-1269,P02-1040,0,0.100222,"Missing"
C18-1269,P11-1157,0,0.050224,"Missing"
C18-1269,E17-2045,0,0.0862341,"nd the NMT model configurations. In section 6 we show and discuss the results of our two groups of experiments, followed by our conclusion and future works in section 7. 2 Related Work A lot of investigations have already been conducted for domain adaptation in SMT while few in neural machine translation. These methods can be roughly categorized into two classes: the model-level and data-level method. At the model level, combining multiple translation models in a weighted manner is used for SMT domain adaptation. For NMT, fine tuning, model stacking and muti-model ensemble have been explored (Sajjad et al., 2017). Luong and Manning (2015) propose a fine-tuning method, which continues to train the already trained out-of-domain system on the in-domain data. Model stacking is to build an NMT model in an online fashion, training the model from the most distant domain at the beginning, fine-tuning it on the closer domain and finalizing it by fine-tuning it on the in-domain data. Muti-model ensemble combines multiple models during decoding using a balanced or weighted averaging method. At the data level, traditional domain adaptation approach can be done by data selection, data weighting or data joining. Da"
C18-1269,P17-2089,0,0.117394,"Missing"
C18-1269,D17-1155,0,0.193768,"l data, which is beneficial for the low-resource domain translation. Our method can also be used in back translation and in conjunction with other training methods. ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 3181 Proceedings of the 27th International Conference on Computational Linguistics, pages 3181–3190 Santa Fe, New Mexico, USA, August 20-26, 2018. Experiments show clear gains on the IWSLT Chinese-English task. Comparing to the previous sentence weighting method (Wang et al., 2017b), we achieve the highest improvement of 1.71 BLEU among four test sets, and obtain an average gain of 1.42 BLEU over Wang et al. (2017b)’s method. The experiments on NMT trained with only synthetic parallel data further confirm the effectiveness of our sentence weighting method on domain adaptation. The paper is organized as follows. Section 2 overviews related work. In section 3, we give a background introduction of NMT. We present our sentence weighting method in Section 4. In section 5, we introduce the experimental setting and the NMT model configurations. In section 6 we show and discus"
D13-1026,P06-1009,0,0.0333627,"hrase based system. Moses denotes the implementation of hierarchical phrased-model in Moses (Koehn et al., 2007). +Sparsef eature means that those sparse features used in the grammar induction are also used during decoding. The improvement of maxmargin over Baseline is statistically significant (p &lt; 0.01). target span boundary as: {B : tk+1 ; E : tl ; BE : tk+1 , tl }. Target span orientation features Similar target orientation features are used for a swapping span [i, j, k, l] with feature templates {B : tk+1 ; E : tl ; BE : tk+1 , tl }. Relative position features Following Blunsom and Cohn (Blunsom and Cohn, 2006), we integrate features indicating the closeness to the alignment matrix diagonal. For an aligned word pair with source position i and target position j, the value of j i − |t| |. As this feature depends this feature is ||s| on the length of the target sentence, it is a non-local feature. Language model We also incorporate an ngram language model which is an important component in SMT. For efficiency, we use a 3-gram language model trained on the target side of our training data during the induction of synchronous grammars. 5 Experiment In this section, we present our experiments on the NIST C"
D13-1026,P08-1024,0,0.242184,"Missing"
D13-1026,P09-1088,0,0.0598241,"cs (Koehn et al., 2003). This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline ∗ Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by Xiao et al. (2012) only incorporates lo"
D13-1026,N10-1015,0,0.0467485,"Missing"
D13-1026,W07-0403,0,0.107332,"learn translation rules via a pipeline with word-based heuristics (Koehn et al., 2003). This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline ∗ Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar indu"
D13-1026,N09-1025,0,0.316741,"idered as the geometric central point in the space D(s, t). Another possible way to deal with the latent derivation is max-derivation, which uses the maxoperator over D(s, t). The max derivation method sets f (s, t) as maxd∈D(s,t) f (s, t, d). It is often adopted in traditional SMT systems. Nevertheless, we instead use average-derivation for two reasons.2 Imagine that H(s, t) in the Algorithm 1 is replaced by a maximum derivation in H(s, t). 2 258 First, as a translation has an exponential number of derivations, finding the max derivation of a reference translation for learning is nontrivial (Chiang et al., 2009). Second, the max derivation estimation will result in a low rule coverage, as rules in a max derivation only covers a small fraction of rules in the D(s, t). Because rule coverage is important in synchronous grammar induction, we would like to explore the entire derivation space using the average operator. 3.2 Learning Algorithm We reformulate the equation (3) as an unconstrained empirical loss minimization problem as follows: N λ 1 ∑ min ∥θ∥2 + L(s(i) , t(i) , θ) 2 N (6) n=1 Where λ denotes the regularization strength for L2-norm. The loss function of a sentence pair L(s(i) , t(i) , θ) is a"
D13-1026,J07-2003,0,0.817311,"ding to their translation performance. We further incorporate various nonlocal features defined on target parse trees. We efficiently calculate the non-local feature values of a translation over its exponential derivation space using the inside-outside algorithm. Because our maxmargin estimation optimizes feature weights only by the feature values of Viterbi and reference translations, we are able to efficiently perform optimization even with non-local features. We apply the proposed max-margin estimation method to learn synchronous grammars for a hierarchical phrase-based translation system (Chiang, 2007) which typically produces state-of-the-art performance. With non-local features defined on target parse trees, our max-margin method significantly outperforms the baseline that uses synchronous rules learned from the traditional pipeline by 1.3 B LEU points on large-scale Chinese-English bilingual training data. The remainder of this paper is organized as follows. Section 2 presents the discriminative synchronous grammar induction model with the nonlocal features. In Section 3, we elaborate our maxmargin estimation method which is able to directly optimize B LEU, and discuss how we induce gram"
D13-1026,D09-1037,0,0.111574,"). This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline ∗ Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by Xiao et al. (2012) only incorporates local features defined on"
D13-1026,P10-1147,0,0.0139752,"s, are helpful for grammar induction. This further confirms the advance of the max-margin estimation, as it provides us a convenient way to use non-local features. 6 Related Work As the synchronous grammar is the key component in SMT systems, researchers have proposed various methods to improve the quality of grammars. In addition to the generative and discriminative models introduced in Section 1, researchers also have made efforts on word alignment and grammar weight rescoring. The first line is to modify word alignment by exploring information of syntactic structures (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such syntactic information is combined with word alignment via a discriminative framework. These methods prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by u"
D13-1026,D08-1033,0,0.138474,"Missing"
D13-1026,W11-2139,0,0.0292381,"Missing"
D13-1026,N12-1023,0,0.043343,"robabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al., 2008; K¨aa¨ ri¨ainen, 2009; He and Deng, 2012), or incorporating various features using labeled word aligned bilingual data (Huang and Xiang, 2010). However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012) that has been widely used in SMT systems. Notably, these discriminative training methods are not used to learn grammar. Instead, they assume that grammar are extracted by the traditional two-step pipeline. 7 Conclusion In this paper we have presented a max-margin estimation for discriminative synchronous grammar induction. By associating the margin with the translation quality, we directly learn translation rules that optimize the translation performance measured by B LEU. Max-margin estimation also provides us a convenient way to incorporate non-local features. 263 Experiment results validat"
D13-1026,P12-1031,0,0.203838,"roaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by Xiao et al. (2012) only incorporates local features defined on parse trees of the source language. Nonlocal features, which encode information from parse trees of the target language, have never been exploited before due to the computational complexity of normalization in max-likelihood estimation. Consequently, we would like to learn synchronous grammars in a discriminative way that can directly maximize the end-to-end translatio"
D13-1026,C10-1056,0,0.0188545,"s that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al., 2008; K¨aa¨ ri¨ainen, 2009; He and Deng, 2012), or incorporating various features using labeled word aligned bilingual data (Huang and Xiang, 2010). However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012) that has been widely used in SMT systems. Notably, these discriminative training methods are not used to learn grammar. Instead, they assume that grammar are extracted by the traditional two-step pipeline. 7 Conclusion In this paper we have present"
D13-1026,P08-1067,0,0.0736458,"Missing"
D13-1026,D09-1107,0,0.0408841,"Missing"
D13-1026,N03-1017,0,0.0144726,"n method significantly outperforms the traditional twostep pipeline for synchronous rule extraction by 1.3 B LEU points and is also better than previous max-likelihood estimation method. 1 Introduction Synchronous grammar induction, which refers to the process of learning translation rules from bilingual corpus, still remains an open problem in statistical machine translation (SMT). Although stateof-the-art SMT systems model the translation process based on synchronous grammars (including bilingual phrases), most of them still learn translation rules via a pipeline with word-based heuristics (Koehn et al., 2003). This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline ∗ Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; C"
D13-1026,P07-2045,0,0.0064059,"want to assess the target tree structure in a derivation. We define these features in a way similar to source span boundary features. For a bispan [i, j, k, l] in a derivation, we define the feature templates that indicates System Grammar Size MT03 MT04 MT05 Avg. Moses Baseline 302.5M 77.8M 34.26 33.83 36.56 35.81 32.69 33.23 34.50 34.29 Max-margin +Sparse feature 59.4M 34.62 35.48 37.14 37.31 34.00 34.07 35.25 35.62 Table 2: Experiment results. Baseline is an in-house implementation of hierarchical phrase based system. Moses denotes the implementation of hierarchical phrased-model in Moses (Koehn et al., 2007). +Sparsef eature means that those sparse features used in the grammar induction are also used during decoding. The improvement of maxmargin over Baseline is statistically significant (p &lt; 0.01). target span boundary as: {B : tk+1 ; E : tl ; BE : tk+1 , tl }. Target span orientation features Similar target orientation features are used for a swapping span [i, j, k, l] with feature templates {B : tk+1 ; E : tl ; BE : tk+1 , tl }. Relative position features Following Blunsom and Cohn (Blunsom and Cohn, 2006), we integrate features indicating the closeness to the alignment matrix diagonal. For an"
D13-1026,W04-3250,0,0.0913935,"th 40 from the LDC data.3 Our 5-gram language model was trained by SRILM toolkit (Stolcke, 2002). The monolingual training data includes the Xinhua section of the English Gigaword corpus and the English side of the entire LDC data (432 million words). We used the NIST 2002 (MT02) as our development set, and the NIST 2003-2005 (MT03-05) as the test set. Case-insensitive NIST B LEU-4 (Papineni et al., 2002) is used to measure translation performance, and also the cost function in the max-margin estimation. Statistical significance in B LEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). We used minimum error rate training (MERT) (Och, 2003) to optimize feature weights for the traditional log-linear model. We used the same decoder as the baseline system in all estimation methods. Without special explanation, we used the same features as those in the traditional pipeline: forward and backward translation probabilities, forward and backward lexical weights, count of extracted rules, count of glue rules, length of translation, and language model. For the lexical weights we used the noisy-or in all configurations including the baseline system. For the discriminative grammar indu"
D13-1026,D12-1021,0,0.624106,"Missing"
D13-1026,P09-1067,0,0.0495631,"Missing"
D13-1026,C04-1072,0,0.257616,"Missing"
D13-1026,W02-1018,0,0.0821781,"), most of them still learn translation rules via a pipeline with word-based heuristics (Koehn et al., 2003). This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline ∗ Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative sy"
D13-1026,D07-1038,0,0.0229851,"target parse structures, are helpful for grammar induction. This further confirms the advance of the max-margin estimation, as it provides us a convenient way to use non-local features. 6 Related Work As the synchronous grammar is the key component in SMT systems, researchers have proposed various methods to improve the quality of grammars. In addition to the generative and discriminative models introduced in Section 1, researchers also have made efforts on word alignment and grammar weight rescoring. The first line is to modify word alignment by exploring information of syntactic structures (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such syntactic information is combined with word alignment via a discriminative framework. These methods prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. Th"
D13-1026,P03-1021,0,0.471928,"ained by SRILM toolkit (Stolcke, 2002). The monolingual training data includes the Xinhua section of the English Gigaword corpus and the English side of the entire LDC data (432 million words). We used the NIST 2002 (MT02) as our development set, and the NIST 2003-2005 (MT03-05) as the test set. Case-insensitive NIST B LEU-4 (Papineni et al., 2002) is used to measure translation performance, and also the cost function in the max-margin estimation. Statistical significance in B LEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). We used minimum error rate training (MERT) (Och, 2003) to optimize feature weights for the traditional log-linear model. We used the same decoder as the baseline system in all estimation methods. Without special explanation, we used the same features as those in the traditional pipeline: forward and backward translation probabilities, forward and backward lexical weights, count of extracted rules, count of glue rules, length of translation, and language model. For the lexical weights we used the noisy-or in all configurations including the baseline system. For the discriminative grammar induction, rule translation probabilities were calculated us"
D13-1026,P02-1040,0,0.0923926,"are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by Xiao et al. (2012) only incorporates local features defined on parse trees of the source language. Nonlocal features, which encode information from parse trees of the target language, have never been exploited before due to the computational complexity of normalization in max-likelihood estimation. Consequently, we would like to learn synchronous grammars in a discriminative way that can directly maximize the end-to-end translation quality measured by B LEU (Papineni et al., 2002), and is also able to incorporate non-local features from target parse trees. We thus propose a max-margin estimation method 1 More precisely, the discriminative model by Xiao et al. (2012) maximizes conditional likelihood. 255 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 255–264, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics to discriminatively induce synchronous grammar directly from sentence pairs without word alignments. We try to maximize the margin between a reference translation and a candi"
D13-1026,N10-1014,0,0.0171498,"ar induction. This further confirms the advance of the max-margin estimation, as it provides us a convenient way to use non-local features. 6 Related Work As the synchronous grammar is the key component in SMT systems, researchers have proposed various methods to improve the quality of grammars. In addition to the generative and discriminative models introduced in Section 1, researchers also have made efforts on word alignment and grammar weight rescoring. The first line is to modify word alignment by exploring information of syntactic structures (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such syntactic information is combined with word alignment via a discriminative framework. These methods prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar lat"
D13-1026,D11-1046,0,0.0170796,"vance of the max-margin estimation, as it provides us a convenient way to use non-local features. 6 Related Work As the synchronous grammar is the key component in SMT systems, researchers have proposed various methods to improve the quality of grammars. In addition to the generative and discriminative models introduced in Section 1, researchers also have made efforts on word alignment and grammar weight rescoring. The first line is to modify word alignment by exploring information of syntactic structures (May and Knight, 2007; DeNero and Klein, 2010; Pauls et al., 2010; Burkett et al., 2010; Riesa et al., 2011). Such syntactic information is combined with word alignment via a discriminative framework. These methods prefer word alignments that are consistent with syntactic structure alignments. However, labeled word alignment data are required in order to learn the discriminative model. Yet another line is to rescore the weights of translation rules. This line of work tries to improve the relative frequency estimation used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al"
D13-1026,D09-1008,0,0.0382465,"Missing"
D13-1026,W04-3201,0,0.146631,"e translation when updating feature weights. Therefore, the defined non-local features allow us to not only explore useful knowledge on the target parse trees, but also compute them efficiently over D(s, t) during maxmargin estimation. 3 Max-Margin Estimation In this section, we describe how we use a parallel training corpus {S, T} = {(s(i) , t(i) )}N i=1 to estimate feature weights θ, which contain parameters of the induced synchronous grammars and the defined non-local features. We choose the parameters that maximize the translation quality measured by B LEU using the max-margin estimation (Taskar et al., 2004). Margin refers to the difference of the model score between a reference translation t(i) and a candidate translation t. We hope that the worse the translation quality of t, the larger the margin between t and t(i) . In this way, we penalize larger translation errors more severely than smaller ones. This intuition is expressed by the following equation. 1 ∥θ∥2 (3) 2 s.t. f (s(i) , t(i) ) − f (s(i) , t) ≥ cost(t(i) , t) min ∀t ∈ T (s(i) ) Here, f (s, t) is the feature function of a translation, and cost function cost(t(i) , t) measures the translation errors of a candidate translation t compari"
D13-1026,D07-1080,0,0.179263,"used in the traditional pipeline. They rescore the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al., 2008; K¨aa¨ ri¨ainen, 2009; He and Deng, 2012), or incorporating various features using labeled word aligned bilingual data (Huang and Xiang, 2010). However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012) that has been widely used in SMT systems. Notably, these discriminative training methods are not used to learn grammar. Instead, they assume that grammar are extracted by the traditional two-step pipeline. 7 Conclusion In this paper we have presented a max-margin estimation for discriminative synchronous grammar induction. By associating the margin with the translation quality, we directly learn translation rules that optimize the translation performance measured by B LEU. Max-margin estimation also provides us a convenient way"
D13-1026,D11-1081,1,0.883385,"re the weights or probabilities of extracted rules. The rescoring is done by using the similar latent log-linear model as ours (Blunsom et al., 2008; K¨aa¨ ri¨ainen, 2009; He and Deng, 2012), or incorporating various features using labeled word aligned bilingual data (Huang and Xiang, 2010). However, in rescoring, translation rules are still extracted by the heuristic two-step pipeline. Therefore these previous work still suffers from the inelegance problem of the traditional pipeline. Our work also relates to the discriminative training (Och, 2003; Watanabe et al., 2007; Chiang et al., 2009; Xiao et al., 2011; Gimpel and Smith, 2012) that has been widely used in SMT systems. Notably, these discriminative training methods are not used to learn grammar. Instead, they assume that grammar are extracted by the traditional two-step pipeline. 7 Conclusion In this paper we have presented a max-margin estimation for discriminative synchronous grammar induction. By associating the margin with the translation quality, we directly learn translation rules that optimize the translation performance measured by B LEU. Max-margin estimation also provides us a convenient way to incorporate non-local features. 263 E"
D13-1026,C12-1176,1,0.747211,"istically extracts rules that are consistent with word alignments. Such heuristic pipeline ∗ Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by Xiao et al. (2012) only incorporates local features defined on parse trees of the source language. Nonlocal features, which encode information from parse"
D13-1026,N10-1016,1,0.852467,"initial weight of these two lexical scores with equivalent positive values. The lexical weights enable our system to score and rank the hyperedges at the beginning. Although word alignment features are used, we do not constrain the derivation space of a sentence pair by prefixed word alignment, and do not require any heuristic alignment combination strategy. Length feature We integrate the length of target translation that is used in traditional SMT system as our feature. Source span boundary features We use this kind of feature to assess the source parse tree in a derivation. Previous work (Xiong et al., 2010) has shown the importance of phrase boundary features for translation. Actually, this kind of feature is a good cue for deciding the boundary where a rule is to be learnt. Following Taskar et al. (2004), for a bispan [i, j, k, l] in a derivation, we define the feature templates that indicates the boundaries of a span by its beginning and end words: {B : si+1 ; E : sj ; BE : si+1 , sj }. Source span orientation features Orientation features are only used for those spans that are swapping. In Figure 1, the translation of source span [1, 3] is swapping with that of span [4, 5] by r2 , thus orient"
D13-1026,N04-1033,0,0.0345333,"ge of the discriminative method is that it enables us to incorporate arbitrary features. As shown in Section 2, our model incorporates both local and non-local features. 4.1 Local Features Rule features We associate each rule with an indicator feature. Each indicator feature counts the number of times that a rule appears in a derivation. In 260 this way, we are able to learn a weight for every rule according to the entire structure of sentence. Word association features Lexicalized features are widely used in traditional SMT systems. Here we adopt two lexical weights called noisy-or features (Zens and Ney, 2004). The noisy-or feature is estimated by word translation probabilities output by GIZA++. We set the initial weight of these two lexical scores with equivalent positive values. The lexical weights enable our system to score and rank the hyperedges at the beginning. Although word alignment features are used, we do not constrain the derivation space of a sentence pair by prefixed word alignment, and do not require any heuristic alignment combination strategy. Length feature We integrate the length of target translation that is used in traditional SMT system as our feature. Source span boundary fea"
D13-1026,P08-1012,0,0.0781544,"s via a pipeline with word-based heuristics (Koehn et al., 2003). This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments. Such heuristic pipeline ∗ Corresponding author is not elegant theoretically. It brings an undesirable gap that separates modeling and learning in an SMT system. Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009; Neubig et al., 2011; Levenberg et al., 2012) or discriminative models (Xiao et al., 2012). Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way. However, they learn synchronous grammars by maximizing likelihood,1 which only has a loose relation to translation quality (He and Deng, 2012). Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed"
D13-1026,2009.eamt-smart.4,0,\N,Missing
D13-1026,P11-1064,0,\N,Missing
D13-1163,J08-1001,0,0.0522055,"mong the three cohesion models proposed by Xiong et al. (2013). Modeling Coherence in Document-Level SMT In discourse analysis, cohesion is often studied together with coherence which is another dimension of the linguistic structure of a text (Barzilay and Elhadad, 1997). Cohesion is related to the surface structure of a text while coherence is concerned with the underlying meaning connectedness in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choic"
D13-1163,W13-3304,0,0.0103188,"nts our large-scale experiments and results. Finally, we conclude with future directions in Section 6. 1564 2 Related Work Recent years have witnessed growing research interests in document-level statistical machine translation. Such research efforts can be roughly divided into two groups: 1) general document-level machine translation that does not explore or explores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language top"
D13-1163,2007.tmi-papers.6,0,0.00909198,"ess in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on"
D13-1163,D07-1007,0,0.0489016,"ess in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on"
D13-1163,W09-2404,0,0.168473,"e model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining algorithm (See Section 4.1). 3 Background: Lexical Chain and Chain Computation Lexical chains are sequences of semantically related words (Morris and Hirst, 1991). They represent the lexical cohesion structure of a text. Figure 2 displays six lexical chains computed from the Chinese new"
D13-1163,P07-1005,0,0.0358991,"ith cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining alg"
D13-1163,C10-3004,0,0.0083293,"as only two sub-classes at level i + 1. Actually, they have multiple sub-classes. tended Cilin contains 77,343 Chinese words, which are organized in a hierarchical structure containing 5 levels as shown in Figure 3. In the 5th level, each node represents an atomic concept which consists of a set of synonyms. These atomic concepts are just like synsets in WordNet. We use them to represent senses of words in the disambiguation graph. We select nouns, verbs, abbreviations and idioms as candidate words for the disambiguation graph. These words are identified by a Chinese part-tospeech tagger LTP (Che et al., 2010) in a preprocessing step. In order to build the disambiguation graph, we first build an array indexed by the atomic concepts of Cilin, then insert a copy of each candidate word into its all concept (sense) entries in the array. After that, we create all semantic links among senses of different candidate words in the disambiguation graph following Galley and McKeown (2003). In the second step, we use the principle of one sense per discourse to perform WSD for each candidate word in the disambiguation graph. We sum the weights of all semantic links under the different senses of the candidate wor"
D13-1163,J07-2003,0,0.0588006,"which show the number of documents (#Doc) and sentences (#Sent), the number of lexical chains extracted from the source documents (#Chain), the average number of lexical chains per document (#AvgC) and the average number of words per lexical chain (#AvgW). Figure 4: Architecture of an SMT system with the lexical chain based cohesion model. 5 Experiments In this section, we conducted a series of experiments to validate the effectiveness of the proposed lexical chain based cohesion models for Chinese-to-English document-level machine translation. We used a hierarchical phrased-based SMT system (Chiang, 2007) trained on large-scale data. In particular, we aim at: • Measuring the impact of the threshold  on the probability cohesion model and selecting the best threshold on a development test set. • Investigating the effect of the two lexical-chain based cohesion models. • Comparing our lexical chain based cohesion models against the previous lexical cohesion device based models (Xiong et al., 2013). 5.1 Setup We collected our bilingual training data from LDC, which includes the corpus LDC2002E18, LDC2003E07, LDC2003E14, LDC2004E12, LDC2004T07, LDC2004T08 (Only Hong Kong News), LDC2005T06 and LDC20"
D13-1163,P11-2031,0,0.0155574,"C2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 3 Available at: http://homepages.inf.ed.ac.uk/lzhang10/ maxent toolkit.html  0.05 0.1 0.2 0.3 0.4 System Baseline LexChainCount(top 1) LexChainCount LexChainProb MT06 30.53 31.64 31.45 30.73 31.01 Table 2: BLEU scores of the probability cohesion model Mp (TDt , { LCtk }N k=1 ) with different values for the threshold . et al., 2002) as our evaluation metric. As MERT is normally instable, we ran the tuning process three times for all our experiments and presented the average BLEU scores on the three MERT runs as suggested by Clark et al (2011). 5.2 Setting the Threshold  As the two lexical chain based cohesion models are built on the super target lexical chains that are associated with a parameter , we need to tune the threshold parameter  on the development test set NIST MT06. We conducted a group of experiments using the probability cohesion model defined in Eq. (5) to find the best threshold. Experiment results are shown in Table 2. If we set the threshold too small (e.g., 0.05), the super target lexical chains may contain too many noisy words that are not the translations of source lexical chain words, which may jeopardise t"
D13-1163,H92-1045,0,0.467758,"s defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document. Carpuat (2009) explores the principle of one sense per discourse (Gale et al., 1992) in the context of SMT and imposes the constraint of one translation 1565 per discourse on document translation. We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining algorithm (See Section 4.1). 3 Background: Lexical Chain and Chain Computation Lexical chains are sequences of semantically related words (Morris and Hirst, 1991). They represent the lexical cohesion structure of a text. Figure 2 displays six lexical chains computed from the Chinese news article shown in Figure 1. Words in these lexical chains have lexica"
D13-1163,D11-1084,1,0.745373,"Missing"
D13-1163,W13-3302,0,0.591796,"models. Section 5 presents our large-scale experiments and results. Finally, we conclude with future directions in Section 6. 1564 2 Related Work Recent years have witnessed growing research interests in document-level statistical machine translation. Such research efforts can be roughly divided into two groups: 1) general document-level machine translation that does not explore or explores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a to"
D13-1163,D12-1108,0,0.0625948,"l. (2012) soften this consistency constraint by integrating three counting features into decoder. Using Lexical Cohesion Devices in DocumentLevel SMT Lexical cohesion devices are semantically related words, including word repetition, synonyms/near-synonyms, hyponyms and so on. They are also the cohesion-building elements in lexical chains. Wong and Kit (2012) use lexical cohesion device based metrics to improve machine translation evaluation at the document level. These metrics measure the proportion of content words that are used as lexical cohesion devices in machine-generated translations. Hardmeier et al. (2012) propose a documentwide phrase-based decoder and integrate a semantic language model into the decoder. They argue that their semantic language model can capture lexical cohesion by exploring n-grams that cross sentence boundaries. Most recently Xiong et al. (2013) integrate three categories of lexical cohesion devices into document-level machine translation. They define three cohesion models based on lexical cohesion devices: a direct reward model, a conditional probability model and a mutual information trigger model. The latter two models measure the strength of lexical cohesion relation bet"
D13-1163,D12-1106,0,0.0318609,"els proposed by Xiong et al. (2013). Modeling Coherence in Document-Level SMT In discourse analysis, cohesion is often studied together with coherence which is another dimension of the linguistic structure of a text (Barzilay and Elhadad, 1997). Cohesion is related to the surface structure of a text while coherence is concerned with the underlying meaning connectedness in a text (Vasconcellos, 1989). Compared with cohesion, coherence is not easy to be detected. Even so, various models have been proposed to explore coherence for document summarization and generation (Barzilay and Lapata, 2008; Louis and Nenkova, 2012). Following this line, Xiong and Zhang (2013) integrate a topic-based coherence model into document-level machine translation, where coherence is defined as a continuous sentence topic transition. Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT (Carpuat and Wu, 2007b; Carpuat and Wu, 2007a; Chan et al., 2007). The difference is that we use document-wide lexical chains to build our cohesion models rather than sentence-level context features. In our framework, lexical choice is performed to make the"
D13-1163,J91-1002,0,0.949629,"tween text units, namely co-reference, ellipsis, substitution, conjunction and lexical cohesion that is realized via semantically related words. The former four cohesion relations can be grouped as grammatical cohesion. Generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion (Barzilay and Elhadad, 1997). As most SMT systems translate a text in a sentence-by-sentence fashion, they tend to build less lexical cohesion than human translators (Wong and Kit, 2012). We therefore study lexical cohesion for document-level translation. We use lexical chains (Morris and Hirst, 1991) to capture lexical cohesion in a text. Lexical chains are connected graphs that represent the lexical cohesion structure of a text. They have been successfully used for information retrieval (Stairmand, 1996), document summarization (Barzilay and Elhadad, 1997) and so on. In this paper, we investigate how lexical chains can be used to incorporate lexical cohesion into document-level translation. Our basic assumption is that the lexical chains of a target document are direct correspondences of the lexical chains of its counterpart source document. This assumption is reasonable as the target do"
D13-1163,P02-1040,0,0.0861266,"Missing"
D13-1163,W10-2602,0,0.175477,"arch interests in document-level statistical machine translation. Such research efforts can be roughly divided into two groups: 1) general document-level machine translation that does not explore or explores very little linguistic discourse information; 2) linguistically-motivated document-level machine translation that incorporates discourse information such as cohesion and coherence into SMT. Recent studies (Guillou, 2013; Beigman Klebanov and Flor, 2013) show that this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language topic words. Xiao et al. (2011) study the translation consistency issue in document-level machine translation. They use a hard constraint to consistently tran"
D13-1163,N12-1046,0,0.287225,"Missing"
D13-1163,D12-1097,0,0.648458,"76). Cohesion is a surface-level property of wellformed texts. It deals with five categories of relationships between text units, namely co-reference, ellipsis, substitution, conjunction and lexical cohesion that is realized via semantically related words. The former four cohesion relations can be grouped as grammatical cohesion. Generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion (Barzilay and Elhadad, 1997). As most SMT systems translate a text in a sentence-by-sentence fashion, they tend to build less lexical cohesion than human translators (Wong and Kit, 2012). We therefore study lexical cohesion for document-level translation. We use lexical chains (Morris and Hirst, 1991) to capture lexical cohesion in a text. Lexical chains are connected graphs that represent the lexical cohesion structure of a text. They have been successfully used for information retrieval (Stairmand, 1996), document summarization (Barzilay and Elhadad, 1997) and so on. In this paper, we investigate how lexical chains can be used to incorporate lexical cohesion into document-level translation. Our basic assumption is that the lexical chains of a target document are direct corr"
D13-1163,2011.mtsummit-papers.13,0,0.396437,"this discourse information is very important for document-level machine translation. General Document-Level Machine Translation Tiedemann (2010) propose cache-based language and translation models for document-level machine translation. These models are built on recently translated sentences. Following this cache-based approach, Gong et al. (2011) further introduce two additional caches. They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated. They also adopt a topic cache with target language topic words. Xiao et al. (2011) study the translation consistency issue in document-level machine translation. They use a hard constraint to consistently translate ambiguous source words into the most frequent translation options. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Using Lexical Cohesion Devices in DocumentLevel SMT Lexical cohesion devices are semantically related words, including word repetition, synonyms/near-synonyms, hyponyms and so on. They are also the cohesion-building elements in lexical chains. Wong and Kit (2012) use lexical cohesion device b"
D13-1163,W97-0703,0,\N,Missing
D14-1060,E14-1035,0,0.0217039,"acketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of mach"
D14-1060,E12-1073,0,0.0152036,"tes separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. This section presents the three models of term translation. They are the term translation disambiguation model, term translation consistency model and term bracketing model respectively. 4 4.1 Models Term Translation Disambiguation Model The most straightforward way to disambiguate term translations in different domain"
D14-1060,W14-3358,0,0.0581314,"acketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of mach"
D14-1060,J07-2003,0,0.423966,"should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. ses where bracketable multi-word terms are translated as a whole unit. We integrate the three models into hierarchical phrase-based SMT (Chiang, 2007). Large-scale experiment results show that they are all able to achieve significant improvements of up to 0.89 BLEU points over the baseline. When simultaneously integrating the three models into SMT, we can gain a further improvement, which outperforms the baseline by up to 1.16 BLEU points. In the remainder of this paper, we begin with a brief overview of related work in Section 2, and bilingual term extraction in Section 3. We then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Secti"
D14-1060,W07-2415,0,0.0148819,"erefore, we use these two methods to extract monolingual multiword terms and then combine the extracted terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. This section presents the three models of term translation"
D14-1060,P11-2031,0,0.150467,"pirically, we set the maximum length of a term to 6 words5 . For both the C-value/NC-value and LLRbased extraction methods, we set the context window size to 5 words, which is a widely-used setting in previous work. And we set C-value/NCvalue score threshold to 0 and LLR score threshold to 10 according to the training corpora. We used the case-insensitive 4-gram BLEU6 as our evaluation metric. In order to alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for all our experiments and presented the average BLEU scores on the three runs following the suggestion by Clark et al. (2011). We used an in-house hierarchical phrase-based decoder to verify our proposed models. Although the decoder translates a document in a sentenceby-sentence fashion, it incorporates documentinformed information for sentence translation via the proposed term translation models trained on documents. Experiments In this section, we conducted experiments to answer the following three questions. 1. Are our term translation disambiguation, consistency and bracketing models able to improve translation quality in BLEU? 2. Does the combination of the three models provide further improvements? 3. To what"
D14-1060,itagaki-aikawa-2008-post,0,0.0268507,"ranslated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of machine translation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit dom"
D14-1060,2007.mtsummit-papers.36,0,0.137099,"es is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a term pair. Pinis and Skadins (2012) investigate that bilingual terms are important for domain adaptation of machine translation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been"
D14-1060,N03-1017,0,0.0461755,"t of 4.28M sentence pairs extracted from LDC1 data with document boundaries explicitly provided. The bilingual training data contain 67,752 documents, 124.8M Chinese words and 140.3M English words. We chose NIST MT05 as the MERT (Och, 2003) tuning set, NIST MT06 as the development test set, and NIST MT08 as the final test set. The numbers of documents/sentences in NIST MT05, MT06 and MT08 are 100/1082, 79/1664 and 109/1357 respectively. The word alignments were obtained by running GIZA++ (Och and Ney, 2003) on the corpora in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We adopted SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the English Gigaword corpus. For the topic model, we used the open source 2 http://sourceforge.net/projects/gibbslda/ http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://nlp.stanford.edu/software/tagger.shtml 5 We determine the maximum length of a term by testing {5, 6, 7, 8} in our preliminary experiments. We find that length 6 produces a slightly better performance than other values. 6 ftp://jaguar.ncsl.nist.gov/mt/"
D14-1060,P12-2023,0,0.0198425,"dels for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic model is not used to ad"
D14-1060,E09-1057,0,0.0196172,"o extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. This section presents the three models of term translation. They are the term translation disambiguation model, term translation consistency model and term bracketing model respectively. 4 4.1 Models Term Translation Disambiguation Model The most straightforward way to disambiguate term translati"
D14-1060,W07-2456,0,0.0231945,"pora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as a term pair. This section presents the three models of term translation. They are the term translation disambiguation model, term translation consistency model and term bracketing model respectively. 4 4.1 Models Term Translation Disambiguation Model The most straightforward way to disam"
D14-1060,D11-1084,0,0.0602182,"istribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic model is not used to address the issues of term translation mentioned in Section 1. Our work is also related to document-level SMT in that we use document-informed information for term translation. Tiedemann (2010) propose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted 3 Bilingual Term Extraction Bilingual term extraction is to extract terms from two languages with the purpose of creating or ex547 tending a bilingual term bank, which in turn can be used to improve other tasks such as information retrieval and machine translation. In this paper, we want to automatically build a bilingual term bank so that we can model term translation to improve translation quality of SMT. Our interest is to extract multi-word terms. tracts more flexible"
D14-1060,J03-1002,0,0.00837229,"hat extent do the proposed models affect the translations of test sets? 5.1 Setup Our training data consist of 4.28M sentence pairs extracted from LDC1 data with document boundaries explicitly provided. The bilingual training data contain 67,752 documents, 124.8M Chinese words and 140.3M English words. We chose NIST MT05 as the MERT (Och, 2003) tuning set, NIST MT06 as the development test set, and NIST MT08 as the final test set. The numbers of documents/sentences in NIST MT05, MT06 and MT08 are 100/1082, 79/1664 and 109/1357 respectively. The word alignments were obtained by running GIZA++ (Och and Ney, 2003) on the corpora in both directions and using the “grow-diag-finaland” balance strategy (Koehn et al., 2003). We adopted SRI Language Modeling Toolkit (Stolcke and others, 2002) to train a 4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of the English Gigaword corpus. For the topic model, we used the open source 2 http://sourceforge.net/projects/gibbslda/ http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://nlp.stanford.edu/software/tagger.shtml 5 We determine the maximum length of a term by testing {5, 6, 7, 8} in our preliminary experiments. We fi"
D14-1060,P03-1021,0,0.0697129,"n probability Pd (tei |tfi , D ) given the docu0 ment D is formulated as follows: 0 Pd (tei |tfi , D ) = K X 0 p(tei |tfi , z = k) ∗ p(z = k|D ) (3) k=1 Whenever a source term tfi is translated into tei , we check whether the pair of tfi and its translation tei can be found in our bilingual term bank. If it can be found, we calculate the conditional translation probability from tfi to tei given the document 0 D according to Eq. (3). The term translation disambiguation model is integrated into the log-linear model of SMT as a feature. Its weight is tuned via minimum error rate training (MERT) (Och, 2003). Through the feature, we can enable the decoder to favor translation hypotheses that contain target term translations appropriate for the domain represented by the topic distribution of the corresponding document. 4.2 Qk = m=1 n=1 Nm M X X qmn ∗ p(k|m) (5) m=1 n=1 where M is the number of documents in which the source term tf occurs, Nm is the number of unique corresponding term translations of tf in the mth document, qmn is the frequency of the nth translation of tf in the mth document, p(k|m) is the conditional probability of the mth document over topic k, and Qk is the normalization factor"
D14-1060,W13-3302,0,0.016021,"tion 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. ses where bracketable multi-word terms are translated as a whole unit. We integrate the three models int"
D14-1060,D12-1108,0,0.0215582,"(Vasconcellos et al., 2001). In this paper, we study domain-specific and context-sensitive term translation for SMT. • Term Bracketing Model: We use the bracketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicat"
D14-1060,W06-2403,0,0.012483,"cts more flexible terms, these two methods are complementary to each other. Therefore, we use these two methods to extract monolingual multiword terms and then combine the extracted terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases which are terms and find aligned target phrases for them via word alignments. If the target side is also a term, we store the source and target term as"
D14-1060,P09-1036,1,0.931411,"with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. ses where bracketable multi-word terms are translated as a whole unit. We integrate the three models into hierarchical phrase-based SMT (Chiang, 2007). Large-scale experiment results show that they are all able to achieve significant improvements of up to 0.89 BLEU points over the baseline. When simultaneous"
D14-1060,W09-2907,1,0.898735,"Missing"
D14-1060,P12-1048,1,0.841057,"then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these"
D14-1060,D13-1163,1,0.848608,"we study domain-specific and context-sensitive term translation for SMT. • Term Bracketing Model: We use the bracketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingual phrase contains a"
D14-1060,W10-2602,0,0.0853383,"elationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic model is not used to address the issues of term translation mentioned in Section 1. Our work is also related to document-level SMT in that we use document-informed information for term translation. Tiedemann (2010) propose cache-based language and translation models, which are built on recently translated sentences. Gong et al. (2011) extend this by further introducing two additional caches. They employ a static cache to store bilingual phrases extracted 3 Bilingual Term Extraction Bilingual term extraction is to extract terms from two languages with the purpose of creating or ex547 tending a bilingual term bank, which in turn can be used to improve other tasks such as information retrieval and machine translation. In this paper, we want to automatically build a bilingual term bank so that we can model"
D14-1060,P06-2124,0,0.02353,"ion in Section 3. We then elaborate the proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during tran"
D14-1060,N12-1046,0,0.0216783,"ation. These studies do not focus on the three issues of term translation as discussed in Section 1. Furthermore, domain and document-informed information is not used to assist term translation. Itagaki et al. (2007) propose a statistical method to calculate translation consistency for terms with explicit domain information. Partially inspired by their study, we introduce a term translation consistency metric with document-informed information. Furthermore, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. (2007). Ture et al. (2012) use IR-inspired tf-idf scores to encourage consistent translation choice. Guillou (2013) investigates what kind of words should be translated consistently. Term translation consistency has not been investigated in these studies. Our term bracketing model is also related to Xiong et al. (2009)’s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is bracketable or not using rich syntactic constraints. The difference is that we construct the model with automatically created bilingual term bank and do not depend on any syntactic knowledge. ses where brack"
D14-1060,I08-2084,0,0.0314953,"ild a bilingual term bank so that we can model term translation to improve translation quality of SMT. Our interest is to extract multi-word terms. tracts more flexible terms, these two methods are complementary to each other. Therefore, we use these two methods to extract monolingual multiword terms and then combine the extracted terms. Currently, there are mainly two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term metrics, such as C-value/NCvalue (Frantzi et al., 1998; Vu et al., 2008), or other common cooccurrence measures such as Log-Likelihood Ratio, Dice coefficient and Pointwise Mutual Information (Daille, 1996; Piao et al., 2006). The extracted monolingual terms are then paired together (Hjelm, 2007; Fan et al., 2009; Ren et al., 2009). The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms (Merkel and Foo, 2007; Lefever et al., 2009; Bouamor et al., 2012). In this paper, we adopt the first strategy. In particular, for each sentence pair, we collect all source phrases"
D14-1060,D12-1097,0,0.0241244,"01). In this paper, we study domain-specific and context-sensitive term translation for SMT. • Term Bracketing Model: We use the bracketing model to reward translation hypothe546 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546–556, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics from documents in training data that are similar to the document being translated and a topic cache with target language topic words. Recently we have also witnessed efforts that model lexical cohesion (Hardmeier et al., 2012; Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b) as well as coherence (Xiong and Zhang, 2013) for document-level SMT. Hasler et al. (2014a) use topic models to learn document-level translation probabilities. Hasler et al. (2014b) use topicadapted model to improve lexical selection. The significant difference between our work and these studies is that term translation has not been investigated in these document-level SMT models. Itagaki and Aikawa (2008) employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. (2009) propose a binary feature to indicate whether a bilingua"
D14-1060,P12-1079,1,0.927846,"e proposed three models for term translation in Section 4. Next, we conduct experiments to validate the effectiveness of the proposed models in Section 5. Finally, we conclude and provide directions for future work in Section 6. 2 Related Work In this section, we briefly introduce related work and highlight the differences between our work and previous studies. As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores the topic model (Blei et al., 2003) for machine translation (Zhao and Xing, 2006; Su et al., 2012; Xiao et al., 2012; Eidelman et al., 2012). Zhao and Xing (2006) employ three models that enable word alignment process to leverage topical contents of document-pairs with topic model. Su et al. (2012) establish the relationship between out-ofdomain bilingual corpus and in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation for translation model adaptation. Xiao et al. (2012) propose a topic similarity model for rule selection. Eidelman et al. (2012) use topic models to adapt lexical weighting probabilities dynamically during translation. In these studies, the topic"
D15-1145,D13-1106,0,0.0200271,"ncies between translations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been appl"
D15-1145,D07-1007,0,0.758393,"associations of selected translations with lexical items on the source side, including corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance depende"
D15-1145,P07-1005,0,0.747251,"to two factors: 1) associations of selected translations with lexical items on the source side, including corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing"
D15-1145,J90-1003,0,0.339875,"ence, the most ideal translation graph is a graph that includes all source words and their candidate translations. However, this ideal graph has two problems: intensive computation for graph inference and difficulty in modeling dependencies between function and content words. In order to get around these two issues, we only consider lexical selection for source content words2 . We first identify source-side content word pairs using statistical metrics, and then keep word pairs with a high relatedness strength in the translation graph. To be specific, we use pointwise mutual information (PMI) (Church and Hanks, 1990) and co-occurrence frequency to measure the relatedness strength of two source-side words s and s0 within a window ds . Content word pairs will be kept when their co-occurrence frequencies are more than cf times in our training corpus and PMI values are larger than pmi . In this process, we remove noisy word pairs using the following heuristic rules: (1) As an adjective only has relations with its head nouns or dependent adverbs, we remove all word pairs where an adjective is paired with words other than its head nouns or dependent adverbs; (2) We apply a similar constraint to adverbs too, s"
D15-1145,P11-2031,0,0.0127217,"set as 10−10 , and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)5 (TSM), which explores document-level topic information for translation rule selection in the HPB system. Furthermor"
D15-1145,P13-2061,0,0.0248668,"Missing"
D15-1145,P14-1129,0,0.0370286,"Missing"
D15-1145,D08-1039,0,0.140317,"ed translations with lexical items on the source side, including corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the"
D15-1145,C08-1041,0,0.290315,"source side, including corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the tran"
D15-1145,E14-1003,0,0.0123714,"another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approa"
D15-1145,D13-1176,0,0.0103886,"lations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et"
D15-1145,N03-1017,0,0.0296397,"tion. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch´ıyˇou” and “l`ıchˇang” are far from each other, our baseline can only correctly translate “l`ıchˇang” as “stance”. It inappropriately translates the other two words as “problem” and null, respectively, even with the support of an n-gram language model. If we could model long-distance dependencies among target translations of source words “w`ent´ı”(issue), “ch´ıyˇ"
D15-1145,W04-3250,0,0.0291274,"emented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)5 (TSM), which explores document-level topic information for translation rule selection in the HPB system. Furthermore, we combined our model with the two models to see if we could"
D15-1145,D08-1010,0,0.0188469,"o model global interdependences among different EL decisions. We successfully adapt this algorithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross sentence boundaries into the decoder. Based on topic model"
D15-1145,P14-1030,0,0.100474,"where N (t˜) denotes the set of candidate translations that link to t˜, and RS(t˜, t˜0 ) measures the strength of relatedness between t˜ and t˜0 which is calculated as the average word-level relatedness over all content words in these two translations t˜ and t˜0 . As for the word-level relatedness RS(t, t0 ) for a content word pair (t, t0 ), we estimate it with the following two approaches over collected cooccurring word pairs within a window of size dt : (1) RS(t, t0 ) is computed as a bigram conditional probability plm (t0 |t) via the language model; (2) Following (Xiong et al., 2011) and (Liu et al., 2014), we employ PMI to define RS(t, t0 ) as p(t,t0 ) ln p(t)p(t 0) . 3 Collective Lexical Selection Algorithm Based on the translation graph, we propose a collective lexical selection algorithm to jointly identify translations of all source words in the graph. 3.1 Problem Statement and Solution Method As stated previously, the translation of a sourceside content word s should be: 1) associated with s; 2) related to the translations of other source-side content words. Thus, in the translation graph, the translation of s should be a target-side node which has: 1) an association edge with the node of"
D15-1145,D09-1022,0,0.0150969,"among different EL decisions. We successfully adapt this algorithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross sentence boundaries into the decoder. Based on topic models, Xiao et al. (2012"
D15-1145,J03-1002,0,0.00618241,"Missing"
D15-1145,P03-1021,0,0.0293498,"of adjectives and adverbs. In the procedure of collective lexical selection, the difference threshold  was set as 10−10 , and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)"
D15-1145,P12-1079,1,0.818387,"rget translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch´ıyˇou” and “l`ıchˇang” are far from each other, our baseline can only correctl"
D15-1145,P14-1137,1,0.830823,"Missing"
D15-1145,P11-1129,1,0.870636,"Missing"
D15-1145,D14-1021,0,0.0151054,"n the exploitation of global dependencies among target translations, which has attracted little attention before. Different from exploring source-side context, other researchers pay attention to the utilization of target-side context information. The common practice in SMT is to use an n-gram language model to capture local dependencies between translations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the bes"
D15-1145,D13-1050,0,0.0242003,"Missing"
D15-1145,P02-1040,0,0.0939119,"his, we converted each word into its corresponding lemma with the exception of adjectives and adverbs. In the procedure of collective lexical selection, the difference threshold  was set as 10−10 , and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection i"
D15-1145,P08-1066,0,0.228479,"source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch´ıyˇou” and “l`ıchˇang” are far from each other, our baseline can only correctly translate “l`ıchˇang” as “stance”. It inappropriately translates the other two words as “problem” and null, respectively, even with the support of an n-gram language model. If we could model long-distance dependencies among target translations of source words “w`ent´ı”(issue), “ch´ıyˇou”(hold) and “l`ıc"
D15-1145,D09-1008,0,0.130528,"luding corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemo"
D15-1145,D14-1003,0,0.0131094,"ide context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approach to optimize phrase scoring and corpus weig"
D15-1145,N12-1046,0,0.0463556,"Missing"
D15-1145,2011.mtsummit-papers.13,0,0.158413,"d 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch´ıyˇou” and “l`ıchˇang” are far from each"
D15-1145,J07-2003,0,\N,Missing
D15-1146,D14-1082,0,0.10557,"units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phr"
D15-1146,P14-1013,0,0.0880714,"ressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us"
D15-1146,P14-1129,0,0.0900992,"Missing"
D15-1146,P14-1066,0,0.0451391,"l “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us learn better phrase representations si"
D15-1146,D14-1176,0,0.224828,"on for x, we employ a greedy algorithm (Socher et al., 2011c) to minimize the sum of reconstruction error at each node in the binary tree T (x): X 1 Erec (x; θ) = k [c1 ; c2 ]n −[c01 ; c02 ]n k2 (3) 2 n∈T (x) where θ denotes model parameters and n represents a node in T (x). 2.2 BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different semantic representations. Zhang et al. (2014) use this intuition to constrain semantic pharse embedding learning. As shown in Figure 2, in addition to the abovementioned reconstruction error, BRAE introduces a max-semantic-margin error to minimize the semantic distance between translation equivalents and maximize the semantic distance between non(5) where Esem (f |e, θ) is defined as the semantic distance between the learned vector representations of f and e, denoted by pf and pe , respectively. Since phrase embeddings for the source and target language are learned separately in different vec(3) tor spaces, a transformation matrix Wf ∈ R"
D15-1146,P14-1006,0,0.0826745,"Missing"
D15-1146,D13-1176,0,0.0714049,"tures, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn b"
D15-1146,P14-1062,0,0.234774,"ation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and targ"
D15-1146,D14-1181,0,0.0346093,"ed from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases."
D15-1146,D13-1054,1,0.947567,"tations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal stru"
D15-1146,P13-1078,0,0.088091,"ic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more c"
D15-1146,P14-1140,0,0.190425,"on for x, we employ a greedy algorithm (Socher et al., 2011c) to minimize the sum of reconstruction error at each node in the binary tree T (x): X 1 Erec (x; θ) = k [c1 ; c2 ]n −[c01 ; c02 ]n k2 (3) 2 n∈T (x) where θ denotes model parameters and n represents a node in T (x). 2.2 BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different semantic representations. Zhang et al. (2014) use this intuition to constrain semantic pharse embedding learning. As shown in Figure 2, in addition to the abovementioned reconstruction error, BRAE introduces a max-semantic-margin error to minimize the semantic distance between translation equivalents and maximize the semantic distance between non(5) where Esem (f |e, θ) is defined as the semantic distance between the learned vector representations of f and e, denoted by pf and pe , respectively. Since phrase embeddings for the source and target language are learned separately in different vec(3) tor spaces, a transformation matrix Wf ∈ R"
D15-1146,P02-1038,0,0.273486,"es a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reordering model with the following parameters: iteration number iter=200 and gaussian prior g=1.0. Following Xiong et al. (2006), we use only boundary words of blocks to trigger the reordering model. The whole translation model is organized in a log-linear framework (Och and Ney, 2002). The adopted sub-models mainly include: (1) rule translation probabilities in two directions, (2) lexical weights in two directions, (3) targets-side word number, (4) phrase number, (5) language model score, and (6) the score of maximal entropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combina"
D15-1146,J03-1002,0,0.00554671,"consistency encoded in bilingual phrase structure learning, which is the basis of our model. Then, we describe the objective function which is composed of three types of errors. Finally, we provide details on the training of our model. 3.1 Structural Alignment Consistency We adapt word alignment to structural alignment and introduce some related concepts. Given a bilingual phrase (f, e) with its binary tree structures (Tf , Te ), if the source node nf¯ ∈ Tf covers a source-side sub-phrase f¯, and there exists a target-side sub-phrase e¯ such that (f¯, e¯) are consistent with word alignments (Och and Ney, 2003), we say nf¯ satisfies the structural alignment consistency, and it is referred to as a structuralalignment-consistent (SAC) node. Further, if e¯ is covered by a target node ne¯ ∈ Te , we say ne¯ is the aligned node of nf¯. In this way, several different target nodes may be all aligned to the same source node because of null alignments. For this, we choose the target node with the smallest span as the aligned one for the considered source node. This is because a smaller span reflects a stronger semantic relevance in most situations. Likewise, we have similar definitions for target nodes. Note"
D15-1146,P03-1021,0,0.0252438,"he reordering model with the following parameters: iteration number iter=200 and gaussian prior g=1.0. Following Xiong et al. (2006), we use only boundary words of blocks to trigger the reordering model. The whole translation model is organized in a log-linear framework (Och and Ney, 2002). The adopted sub-models mainly include: (1) rule translation probabilities in two directions, (2) lexical weights in two directions, (3) targets-side word number, (4) phrase number, (5) language model score, and (6) the score of maximal entropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram"
D15-1146,P02-1040,0,0.0961193,"es in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4 . Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html Parameter α β γ λL λrec λcon λlcrec BRAE 0.119 4.95 ×10−5 2.64 ×10−7 9.31 ×10−5 BCorrRAE 0.121 0.6331 0.2459 3.13 ×10−5 2.05 ×10−5 7.32 ×10−6 5.25 ×10−6 Table 1: Hyper-parameters for BCorrRAE and BRAE model. Method BCorrRAESM BCorrRAEST d 2"
D15-1146,W04-3250,0,0.191213,"reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4 . Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html Parameter α β γ λL λrec λcon λlcrec BRAE 0.119 4.95 ×10−5 2.64 ×10−7 9.31 ×10−5 BCorrRAE 0.121 0.6331 0.2459 3.13 ×10−5 2.05 ×10−5 7.32 ×10−6 5.25 ×10−6 Table 1: Hyper-parameters for BCorrRAE and BRAE model. Method BCorrRAESM BCorrRAEST d 25 50 75 100 25 50 75 100 MT06 30.81 30.58↓ 30.50 30.34"
D15-1146,D11-1014,0,0.360999,"Missing"
D15-1146,P14-2037,0,0.0449862,"Missing"
D15-1146,P13-1045,0,0.253422,". However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual c"
D15-1146,D13-1170,0,0.0484163,". However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual c"
D15-1146,D14-1003,0,0.014634,"f BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initia"
D15-1146,P14-1138,0,0.0135589,"e the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, man"
D15-1146,D14-1175,0,0.0183492,"learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits different levels of correspondence relations inside bilingual phrases instead of only the top level of entire"
D15-1146,D14-1023,0,0.0803451,"both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning rep"
D15-1146,D14-1015,0,0.0152033,"pecially the BCorrRAEST model, tends to choose shorter translations that are consistent with word alignments. 6 Related Work A variety of efforts have been devoted to learning vector representations for words/phrases with deep neural networks. According to the difference of learning contexts, previous work mainly include the following two strands. (1) Monolingual Word/Phrase Embeddings. The straightforward approach to represent word/phrases is to learn their hidden representations with traditional feature vectors, which requires manual and task-dependent feature engineering (Cui et al., 2014; Wu et al., 2014; 1255 Source Phrase 䌓㥎 (advocate) 惮䜃 坝揔 (serious challenge) 䋺㟙 䀛 嗪䛢 (data released) BRAE to advocate the in preaching the the promotion of as well as severe challenges a serious challenge to a serious challenge from by the figures published by the the statistics released by data published by the BCorrRAESM out to advocate been encouraging an advocate of rigorous challenges as well as severe challenges of severe challenges to the estimates announced at the figures published the statistics released by BCorrRAEST encouraging claimed advocate rigorous challenge enormous challenge severe challenge"
D15-1146,J97-3002,0,0.606435,"de n, and Cf and Ce count the number of nodes in the source and target tree structure respectively. Note that if we only compute the similarity for root nodes in the bilingual tree of (f, e), the structural similarity equals to the semantic similarity in Eq. (19). 5 Experiments We conducted experiments on NIST ChineseEnglish translation task to validate the effectiveness of BCorrRAE. 5.1 System Overview Our baseline decoder is a state-of-the-art phrasebased translation system equipped with a maximum entropy based reordering model (MEBTG). It adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006): merging 1253 rules A → [A1 , A2 ]|hA1 , A2 i which are used to merge two neighboring blocks2 A1 and A2 in a straight|inverted order, and lexical rule A → f /e used to translate a source phrase f into a target phrase e. The MEBTG system features a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reor"
D15-1146,P10-1049,0,0.0513633,"ntropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4 . Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html Pa"
D15-1146,P06-1066,1,0.784584,"Cf and Ce count the number of nodes in the source and target tree structure respectively. Note that if we only compute the similarity for root nodes in the bilingual tree of (f, e), the structural similarity equals to the semantic similarity in Eq. (19). 5 Experiments We conducted experiments on NIST ChineseEnglish translation task to validate the effectiveness of BCorrRAE. 5.1 System Overview Our baseline decoder is a state-of-the-art phrasebased translation system equipped with a maximum entropy based reordering model (MEBTG). It adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006): merging 1253 rules A → [A1 , A2 ]|hA1 , A2 i which are used to merge two neighboring blocks2 A1 and A2 in a straight|inverted order, and lexical rule A → f /e used to translate a source phrase f into a target phrase e. The MEBTG system features a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reordering model with the"
D15-1146,P13-1017,0,0.0236294,"nt levels of semantic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer exp"
D15-1146,P14-1011,0,0.106705,"rucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us learn better phrase"
D15-1146,D13-1141,0,0.299247,"n bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for success"
D15-1164,2011.eamt-1.38,0,0.0142119,"en et al. (2009) penalize substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels. Similar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima’an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012). Li et al. (2012) incorporate head information extracted from source-side dependency structures into translation rules. Besides, semantic knowledge is also used to refine nonterminals. Gao and Vogel (2011) utilize target-side semantic roles to form SRL-aware SCFG rules. Most of approaches introduced here explicitly require syntactic or semantic parsers trained on manually labeled data. On the other hand, efforts have also been directed towards attaching distributional linguistic knowledge to nonterminals. Venugopal et al. (2009) propose a preference grammar to annotate no"
D15-1164,P14-1062,0,0.0151151,"Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT? Learning semantic representations for terminals (words, multi-word phrases or sentences) from unlabeled data has achieved substantial progress in recent years (Mitchell and Lapata, 2008; Turian et al., 2010; Socher et al., 2010; Mikolov et al., 2013c; Blunsom et al., 2014). These representations have been used successfully in various NLP tasks. However, there is no attempt to learn semantic representations for nonterminals from unlabeled data. In this paper we propose a framework to learn semantic representations for nonterminal Xs in translation rules. Our framework is established on the basis of realvalued vector representations learned for multiword phrases, which are substituted with nonterminal Xs during hierarchical rule extraction. We propose a weighted mean value and a minimum distance method to obtain nonterminal representations from representations of"
D15-1164,C14-1210,0,0.0112383,"g distributional linguistic knowledge to nonterminals. Venugopal et al. (2009) propose a preference grammar to annotate nonterminals based on preference distributions of syntactic categories. Huang et al. (2010) learn la1392 tent syntactic distributions for each nonterminal. They use these distributions to decorate nonterminal Xs in SCFG rules with a real-valued feature vectors and utilize these vectors to measure the similarities between source phrases and applied rules. Similar to this work, Huang et al. (2013) utilize treebank tags based on dependency parsing to learn latent distributions. Cao et al. (2014) attach translation rules with dependency knowledge, which contains both dependency relations inside rules and dependency relations between rules and their contexts. The difference of our work from these studies is that our semantic representations are learned from unlabeled bilingual (or monolingual) data and do not depend on any linguistic resources, e.g., parsers. We also believe that our model is able to exploit both syntactic and semantic information for nonterminals since vector representations learned in our way are able to capture both syntactic and semantic properties (Turian et al.,"
D15-1164,W09-2307,0,0.0164908,"l weights, a word count, a phrase count and a glue rule count. In order to compare our proposed models with previous methods on nonterminal refinement, we re-implemented a syntax mismatch model (SynMis) which was used by Huang et al. (2013) and integrated it into hierarchical phrase-based system. Syn-Mis model decorates each nonterminal with a distribution of head POS tags and uses this distribution to measure the degree of syntactic compatibility of translation rules with corresponding source spans. In order to obtain head POS tags for Syn-Mis model, we used the Stanford dependency parser 6 (Chang et al., 2009) to parse Chinese sentences in our training corpus and NIST development/test sets. 5 We choose bilingual sentences because we want to obtain bilingual training examples to train our projection neural network as described in Section 3.3. 6 http://nlp.stanford.edu/software/lex-parser.shtml Baseline Syn-Mis MV + CS α = 1.0 MV + CS α = 0 MV + CS α = -1.0 MD + ED β = 0 MD + ED β = 0.5 MD + ED β = 1.0 MT06 30.54 31.23∗ 31.44+ 31.63∗ 31.13 31.02+ 31.35+ 31.06 MT08 23.58 24.38∗ 24.23∗ 24.51∗ 24.07∗ 23.74 24.08∗ 23.90+ Avg 27.06 27.81 27.84 28.07 27.60 27.38 27.72 27.48 Table 1: BLEU scores of our mode"
D15-1164,P10-1146,0,0.0228455,"here is a data sparseness problem in this model due to thousands of extracted syntactic categories. One solution to address this issue is to reduce the number of syntactic categories. Zollmann and Vogel (2011) use word tags, generated by either POS tagger or unsupervised word class induction, instead of syntactic categories. Hanneman and Lavie (2013) coarsen the label set by introducing a label collapsing algorithm to SAMT grammars (Zollmann and Venugopal, 2006). Yet another solution is easing restrictions on label matching. Shen et al. (2009) penalize substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels. Similar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima’an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012). Li et al. (2012) incorporate head i"
D15-1164,P11-2031,0,0.014447,". We used NIST MT03 as our development set, NIST MT06 as our development test set and MT08 as our final test set. We ran Giza++ on the training corpus in both Chinese-to-English and English-to-Chinese directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. We used the SRI Language Modeling Toolkit2 (Stolcke and others, 2002) to train our language models. MERT (Och, 2003) was adopted to tune feature weights of the decoder. We used the case-insensitive BLEU3 as our evaluation metric. In order to alleviate the instability of MERT , we followed Clark et al. (2011) to perform three runs of MERT and reported average BLEU scores over the three runs for all our experiments. We used word2vec toolkit4 to train our word embeddings and set the vector dimension d to 30. In our training experiment, we used the continuous bag-of-words model with a context window of size 5. The monolingual corpus, which was used to pre-train word embeddings, is extracted from 1 The corpora include LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 2 http://www.speech.sri.com/projects/srilm/download.html 3 ftp://jaguar.ncsl.nist.gov/mt/res"
D15-1164,W11-1012,0,0.103294,"manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules. In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information ∗ Corresponding author from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT? Learning semantic representations for terminals (words, multi-word phrases or sentences) from unlabeled data has achieved substantial progress in recen"
D15-1164,N13-1029,0,0.0723068,"007) explores formal synchronous context free grammar (SCFG) rules for translation. Two types of nonterminal symbols are used in translation rules: nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules. In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information ∗ Corresponding author from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mi"
D15-1164,W10-1761,0,0.0141531,"his issue is to reduce the number of syntactic categories. Zollmann and Vogel (2011) use word tags, generated by either POS tagger or unsupervised word class induction, instead of syntactic categories. Hanneman and Lavie (2013) coarsen the label set by introducing a label collapsing algorithm to SAMT grammars (Zollmann and Venugopal, 2006). Yet another solution is easing restrictions on label matching. Shen et al. (2009) penalize substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels. Similar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima’an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012). Li et al. (2012) incorporate head information extracted from source-side dependency structures into translation rules. Besides, semantic knowledge is also used to refine"
D15-1164,D10-1014,0,0.0178928,"xtracted from source-side dependency structures into translation rules. Besides, semantic knowledge is also used to refine nonterminals. Gao and Vogel (2011) utilize target-side semantic roles to form SRL-aware SCFG rules. Most of approaches introduced here explicitly require syntactic or semantic parsers trained on manually labeled data. On the other hand, efforts have also been directed towards attaching distributional linguistic knowledge to nonterminals. Venugopal et al. (2009) propose a preference grammar to annotate nonterminals based on preference distributions of syntactic categories. Huang et al. (2010) learn la1392 tent syntactic distributions for each nonterminal. They use these distributions to decorate nonterminal Xs in SCFG rules with a real-valued feature vectors and utilize these vectors to measure the similarities between source phrases and applied rules. Similar to this work, Huang et al. (2013) utilize treebank tags based on dependency parsing to learn latent distributions. Cao et al. (2014) attach translation rules with dependency knowledge, which contains both dependency relations inside rules and dependency relations between rules and their contexts. The difference of our work f"
D15-1164,D13-1053,0,0.181292,"ry SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules. In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information ∗ Corresponding author from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT? Learning semantic representati"
D15-1164,N03-1017,0,0.0607536,"er for integrating the target-side semantic model into translation, projection or two-pass decoding? 3. Does the combination of source and target semantic nonterminal refinement models provide further improvement? 6.1 Setup Our training corpus contains 2.9M sentence pairs with 80.9M Chinese words and 86.4M English words from LDC data1 . We used NIST MT03 as our development set, NIST MT06 as our development test set and MT08 as our final test set. We ran Giza++ on the training corpus in both Chinese-to-English and English-to-Chinese directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. We used the SRI Language Modeling Toolkit2 (Stolcke and others, 2002) to train our language models. MERT (Och, 2003) was adopted to tune feature weights of the decoder. We used the case-insensitive BLEU3 as our evaluation metric. In order to alleviate the instability of MERT , we followed Clark et al. (2011) to perform three runs of MERT and reported average BLEU scores over the three runs for all our experiments. We used word2vec toolkit4 to train our word embeddings and set the vector dimension d to 30. In our training experiment, we used the continuous bag-of-wor"
D15-1164,P12-2007,0,0.0318201,"Missing"
D15-1164,P08-1028,0,0.0287859,"se efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT? Learning semantic representations for terminals (words, multi-word phrases or sentences) from unlabeled data has achieved substantial progress in recent years (Mitchell and Lapata, 2008; Turian et al., 2010; Socher et al., 2010; Mikolov et al., 2013c; Blunsom et al., 2014). These representations have been used successfully in various NLP tasks. However, there is no attempt to learn semantic representations for nonterminals from unlabeled data. In this paper we propose a framework to learn semantic representations for nonterminal Xs in translation rules. Our framework is established on the basis of realvalued vector representations learned for multiword phrases, which are substituted with nonterminal Xs during hierarchical rule extraction. We propose a weighted mean value and"
D15-1164,P11-1065,0,0.0395414,"Missing"
D15-1164,P03-1021,0,0.0635561,"cond pass, we decode source sentence with our target semantic nonterminal refinement model using learned target phrase vector representations. If a target phrase appears in the collected set, the target-side semantic nonterminal refinement model will calculate the semantic similarity between the target phrase and the corresponding nonterminal on the target semantic space; otherwise the model will give a penalty. This is because this phrase is not a desirable phrase as it is not used in 100-best translations. The weights of these two features are tuned by the Minimum Error Rate Training (MERT)(Och, 2003), together with weights of other sub-models on a development set. Figure 2 shows the architecture of SMT system with the proposed semantic nonterminal refinement model. 2. Can the target-side semantic nonterminal refinement model improve translation quality? And which method is better for integrating the target-side semantic model into translation, projection or two-pass decoding? 3. Does the combination of source and target semantic nonterminal refinement models provide further improvement? 6.1 Setup Our training corpus contains 2.9M sentence pairs with 80.9M Chinese words and 86.4M English w"
D15-1164,D09-1008,0,0.0152513,"trees to augment nonterminals in hierarchical rules. Unfortunately, there is a data sparseness problem in this model due to thousands of extracted syntactic categories. One solution to address this issue is to reduce the number of syntactic categories. Zollmann and Vogel (2011) use word tags, generated by either POS tagger or unsupervised word class induction, instead of syntactic categories. Hanneman and Lavie (2013) coarsen the label set by introducing a label collapsing algorithm to SAMT grammars (Zollmann and Venugopal, 2006). Yet another solution is easing restrictions on label matching. Shen et al. (2009) penalize substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels. Similar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima’an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al."
D15-1164,N09-1027,0,0.147556,"nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules. In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information ∗ Corresponding author from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT? Learning"
D15-1164,W12-3127,0,0.0147002,"e substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels. Similar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model. Mylonakis and Sima’an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar. Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012). Li et al. (2012) incorporate head information extracted from source-side dependency structures into translation rules. Besides, semantic knowledge is also used to refine nonterminals. Gao and Vogel (2011) utilize target-side semantic roles to form SRL-aware SCFG rules. Most of approaches introduced here explicitly require syntactic or semantic parsers trained on manually labeled data. On the other hand, efforts have also been directed towards attaching distributional linguistic knowledge to nonterminals. Venugopal et al. (2009) propose a preference grammar to annotate nonterminals based on p"
D15-1164,D07-1071,0,0.0358701,"ear projection methods for integrating the target-side semantic nonterminal refinement model in terms of BLEU scores. /*” and /+” : significantly better than Baseline at significance level p < 0.01 and p < 0.05 respectively. 6.3 1 2 • Two-pass decoding achieves the highest BLEU scores, which are higher than those of the baseline by 0.75 and 0.66 BLEU points on MT06 and MT08 respectively. The reason may be that noisy translation candidates are filtered out in the first pass. This finding is consistent with many other multiple-pass systems in natural language processing, e.g., two-pass parsing (Zettlemoyer and Collins, 2007). • Nonlinear projection achieves an improvement of 0.62 BLEU points over the baseline on MT06. It outperforms linear projection method on both sets. These empirical results support our assumption that nonlinear relations between languages are more reasonable than linear relations. MT08 23.58 24.38∗ 24.51∗ 24.11∗ 24.72∗ Avg 27.06 27.81 28.07 27.64 28.22 (MV + CS α = 0) is used. Nonlinear Projection is used. Table 3: BLEU scores of the combination of the source- and target-side semantic nonterminal refine model. /*” and /+” : significantly better than Baseline at significance level p < 0.01 and"
D15-1164,W06-3119,0,0.299692,"est sets. 1 Introduction Hierarchical phrase-based translation (Chiang, 2007) explores formal synchronous context free grammar (SCFG) rules for translation. Two types of nonterminal symbols are used in translation rules: nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules. In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information ∗ Corresponding author from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a lar"
D15-1164,P11-1001,0,0.128495,"rchical phrase-based translation (Chiang, 2007) explores formal synchronous context free grammar (SCFG) rules for translation. Two types of nonterminal symbols are used in translation rules: nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner. The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules. In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information ∗ Corresponding author from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013). In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011). All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled dat"
D15-1164,D11-1014,0,0.558278,"We employ a neural method, specifically the continuous bag-of-words model (Mikolov et al., 2013a) to learn high-quality vector representations for words. Once we complete the training of the continuous bag-of-words model, word embeddings form an embedding matrix M ∈ Rd×|V |, where d is a pre-determined embedding dimensionality and each word w in the vocabulary V corresponds to a vector ~v ∈ Rd . Given the embedding matrix M , mapping words to vectors can be done by simply looking up their respective columns in M . We further feed these learned word embeddings to recursive autoencoders (RAE) (Socher et al., 2011) for learning phrase representations. In traditional RAE (shown in Figure 1), given two input children representation vectors c~1 ∈ Rd and c~2 ∈ Rd , their parent representation p~ can be calculated as follows: p~ = f (1) (W (1) [c~1 ; c~2 ] + b(1) ) (1) where [c~1 ; c~2 ] ∈ R2d is the concatenation of vectors of two children, W (1) ∈ Rd×2d is a weight matrix, b(1) ∈ Rd is a bias term, and f (1) is an element-wise activation function such as tanh. The above output representation p~ can be used as a child vector to construct the representation for a larger subphrase. This process is repeated un"
D15-1164,P10-1040,0,0.181821,"rating linguistic knowledge into hierarchical translation rules. Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data. In contrast, a large amount of unlabeled data are easily available. Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data? Or can we exploit these data to refine nonterminals for SMT? Learning semantic representations for terminals (words, multi-word phrases or sentences) from unlabeled data has achieved substantial progress in recent years (Mitchell and Lapata, 2008; Turian et al., 2010; Socher et al., 2010; Mikolov et al., 2013c; Blunsom et al., 2014). These representations have been used successfully in various NLP tasks. However, there is no attempt to learn semantic representations for nonterminals from unlabeled data. In this paper we propose a framework to learn semantic representations for nonterminal Xs in translation rules. Our framework is established on the basis of realvalued vector representations learned for multiword phrases, which are substituted with nonterminal Xs during hierarchical rule extraction. We propose a weighted mean value and a minimum distance m"
D15-1164,J07-2003,0,\N,Missing
D15-1266,C14-1160,0,0.0563749,", “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specificall"
D15-1266,P14-1065,0,0.0174588,"Missing"
D15-1266,D14-1181,0,0.0388993,"and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specifically for implicit DRR. The neglect of this important domain may be due to the following two reasons: (1) discourse relation distribution is rather unbalanced, where the generalization of deep models is relatively insufficient despite their powerful studying ability; (2) training dataset in implicit DRR is relatively small, where overfitting problems become more prominent. In this paper, we propose a Shallow Convolutional Neural Network (SCNN) for implicit DRR, with only one simple convolution layer on the top o"
D15-1266,prasad-etal-2008-penn,0,0.902888,"logical relationship of coherent text (e.g., T EMPORAL, C ONTIN GENCY , E XPANSION , etc). It provides important information to many other natural language processing systems, such as question answering (Verberne et al., 2007), information extraction (Cimiano et al., 2005), machine translation (Guzm´an et al., 2014) and so on. Despite great progress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern"
D15-1266,P13-1047,0,0.347474,"e the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is"
D15-1266,E14-1068,0,0.630863,"l.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specifically for implicit DRR. The neg"
D15-1266,D13-1054,0,0.0139689,"al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specifically for implicit DRR. The neglect of this important domain may be due to the following two reasons: (1) discourse relation distribution is rather unbalanced, where the generalization of deep models is relatively insufficient despite their powerful studying ability; (2) training dataset in implicit DRR is relatively small, where overfitting problems become more prominent. In this paper, we propose a Shallow Convolutional Neural Network (SCNN) for implicit DRR, with only one simple convolution layer"
D15-1266,D14-1220,0,0.0166044,"for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specifically for implicit DRR. The neglect of this important domain may be due to the following two reasons: (1) discourse relation distribution is rather unbalanced, where the generalization of deep models is relatively insufficient despite"
D15-1266,D09-1036,0,0.825769,"2005), machine translation (Guzm´an et al., 2014) and so on. Despite great progress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al.,"
D15-1266,W10-4310,0,0.306051,"anslation (Guzm´an et al., 2014) and so on. Despite great progress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al"
D15-1266,D13-1170,0,0.00350287,"et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specifically for implicit DRR. The neglect of this important domain may be due to the following two reasons: (1) discourse relation distribution is rather unbalanced, where the generalization of deep models is relatively insufficient despite their powerful studying ability; (2) training dataset in implicit DRR is relatively small, where overfitting problems become more prominent. In this paper, we propose a Shallow Convolutional Neural Network (SCNN) for implicit DRR, with only one simple c"
D15-1266,P13-2013,0,0.24879,"ress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our k"
D15-1266,W13-0123,0,0.0109976,"nnectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep le"
D15-1266,C12-1168,0,0.189073,"Missing"
D15-1266,W12-1614,0,0.682233,"on. Despite great progress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). Howe"
D15-1266,C08-2022,0,0.0204587,"scourse analysis, discourse relation recognition (DRR) aims to automatically identify the internal structure and logical relationship of coherent text (e.g., T EMPORAL, C ONTIN GENCY , E XPANSION , etc). It provides important information to many other natural language processing systems, such as question answering (Verberne et al., 2007), information extraction (Cimiano et al., 2005), machine translation (Guzm´an et al., 2014) and so on. Despite great progress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these"
D15-1266,P09-1077,0,0.869714,"tion (Cimiano et al., 2005), machine translation (Guzm´an et al., 2014) and so on. Despite great progress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 201"
D15-1266,miltsakaki-etal-2004-penn,0,\N,Missing
D16-1037,D15-1262,0,0.0464737,"hich we describe in succession. Implicit Discourse Relation Recognition Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015"
D16-1037,P15-2015,0,0.0197483,"(Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from theirs significantly, which can be summarized in the following three as"
D16-1037,C14-1088,0,0.0210779,"3,4 , Rongrong Ji1 , Hong Duan1 , Min Zhang2 Xiamen University, Xiamen, China 3610051 Provincial Key Laboratory for Computer Information Processing Technology Soochow University, Suzhou, China 2150062 ADAPT Centre, School of Computing, Dublin City University3 Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences4 zb@stu.xmu.edu.cn, {jssu, rrji, hduan}@xmu.edu.cn qun.liu@dcu.ie, {dyxiong, minzhang}@suda.edu.cn Abstract other relevant natural language processing tasks, such as text summarization (Yoshida et al., 2014), conversation (Higashinaka et al., 2014), question answering (Verberne et al., 2007) and information extraction (Cimiano et al., 2005). Generally, discourse relations can be divided into two categories: explicit and implicit, which can be illustrated in the following example: Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse"
D16-1037,Q15-1024,0,0.411507,"and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic space. It is this latent variable that generates both discourse arguments and the corresponding relation"
D16-1037,N16-1037,0,0.0279266,"implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from theirs significantly, which can be summarized in the following three aspects: 1) they employ the recurrent neural network to represent the discourse arguments, while we use the simple feedforward neural network; 2) they treat the discourse relations directly as latent variables, rather than the underlying semantic representation of discourses; 3) their model is optimized in terms of the data likelihood, since the discourse relations are observed during training. However, VarNDRR is optimized under the variational theory. Variational Neural Model In the presence"
D16-1037,P13-1047,0,0.0602548,"ies (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from theirs significantly,"
D16-1037,D09-1036,0,0.36099,"sample z from qφ (z|x, y) that not only bridges the gap between the recognizer and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic space. It is this l"
D16-1037,W10-4310,0,0.0179631,"nition and variational neural model, which we describe in succession. Implicit Discourse Relation Recognition Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud"
D16-1037,W12-1614,0,0.0150111,"Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capab"
D16-1037,D13-1094,0,0.0931866,"Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestig"
D16-1037,P09-1077,0,0.340564,"rization technique to sample z from qφ (z|x, y) that not only bridges the gap between the recognizer and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic"
D16-1037,prasad-etal-2008-penn,0,0.672371,"us making the setting z˜ = µ0 during testing reasonable. The second term is the approximate expectation of Eqφ (z|x,y) [log pθ (x, y|z)], which is also differentiable. As the objective function in Eq. (13) is differentiable, we can optimize both the model parameters θ and variational parameters φ jointly using standard gradient ascent techniques. The training procedure for VarNDRR is summarized in Algorithm 1. 4 Experiments We conducted experiments on English implicit DRR task to validate the effectiveness of VarNDRR.4 4.1 Dataset We used the largest hand-annotated discourse corpus PDTB 2.05 (Prasad et al., 2008) (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work (Pitler et al., 2009; Zhou et al., 2010; Lan et 4 Source code is available https://github.com/DeepLearnXMU/VarNDRR. 5 http://www.seas.upenn.edu/ pdtb/ at Model R & X (2015) J & E (2015) SVM SCNN VarNDRR Acc 70.27 63.10 60.42 63.30 P 22.79 22.00 24.00 R 64.47 67.76 71.05 F1 41.00 35.93 33.68 33.22 35.88 Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR (a) C OM vs Other Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR Acc 6"
D16-1037,E14-1068,0,0.0315926,"Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have bee"
D16-1037,N15-1081,0,0.502665,"h02 = dm = dhy = 400, dy = 2 for all experiments.7 . All parameters of VarNDRR are initialized by a Gaussian distribution (µ = 0, σ = 0.01). For Adam, we set β1 = 0.9, β2 = 0.999 with a learning rate 0.001. Additionally, we tied the following parameters in practice: Wh1 and Wh2 , Wx01 and Wx02 . We compared VarNDRR against the following two different baseline methods: • SVM: a support vector machine (SVM) classifier8 trained with several manual features. • SCNN: a shallow convolutional neural network proposed by Zhang et al. (2015). We also provide results from two state-of-the-art systems: • Rutherford and Xue (2015) convert explicit discourse relations into implicit instances. • Ji and Eisenstein (2015) augment discourse representations via entity connections. 7 8 http://nlp.stanford.edu/software/corenlp.shtml 387 There is one dimension in dx1 and dx2 for unknown words. http://svmlight.joachims.org/ 45 40 35 30 25 20 15 10 5 0 80 70 60 50 40 30 20 10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 117 218 19 13 420 521 622 723 824 25 9 26 10 27 11 28 1229 1330 1431 1532 1633 1734 1835 1936 2037 2138 1 39 22 2340 2441 2542 43 26 44 27 45 28 46 2947 3048 3149 3250 3351 3452 3553 3654 -1270.24 25.3012 -207.21 26.0"
D16-1037,C12-1168,0,0.0167503,"en exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from thei"
D16-1037,D14-1196,0,0.0259392,", Deyi Xiong2∗, Jinsong Su1 , Qun Liu3,4 , Rongrong Ji1 , Hong Duan1 , Min Zhang2 Xiamen University, Xiamen, China 3610051 Provincial Key Laboratory for Computer Information Processing Technology Soochow University, Suzhou, China 2150062 ADAPT Centre, School of Computing, Dublin City University3 Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences4 zb@stu.xmu.edu.cn, {jssu, rrji, hduan}@xmu.edu.cn qun.liu@dcu.ie, {dyxiong, minzhang}@suda.edu.cn Abstract other relevant natural language processing tasks, such as text summarization (Yoshida et al., 2014), conversation (Higashinaka et al., 2014), question answering (Verberne et al., 2007) and information extraction (Cimiano et al., 2005). Generally, discourse relations can be divided into two categories: explicit and implicit, which can be illustrated in the following example: Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models a"
D16-1037,D15-1266,1,0.82891,"also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic space. It is this latent variable that generates both discourse arguments and the corresponding relation, i.e. p(x, y|z). The"
D16-1037,C10-2172,0,0.205752,"differentiable, we can optimize both the model parameters θ and variational parameters φ jointly using standard gradient ascent techniques. The training procedure for VarNDRR is summarized in Algorithm 1. 4 Experiments We conducted experiments on English implicit DRR task to validate the effectiveness of VarNDRR.4 4.1 Dataset We used the largest hand-annotated discourse corpus PDTB 2.05 (Prasad et al., 2008) (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work (Pitler et al., 2009; Zhou et al., 2010; Lan et 4 Source code is available https://github.com/DeepLearnXMU/VarNDRR. 5 http://www.seas.upenn.edu/ pdtb/ at Model R & X (2015) J & E (2015) SVM SCNN VarNDRR Acc 70.27 63.10 60.42 63.30 P 22.79 22.00 24.00 R 64.47 67.76 71.05 F1 41.00 35.93 33.68 33.22 35.88 Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR (a) C OM vs Other Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR Acc 69.80 60.71 63.00 57.36 P 65.89 56.29 56.46 Acc 76.95 62.62 63.00 53.82 P 39.14 39.80 35.39 R 72.40 75.29 88.53 F1 53.80 52.78 50.82 52.04 50.56 R 68.24 62.35 97.65 F1 33.30 27.63 24.73 30.54 29.54 (b) C ON"
D16-1050,D16-1025,0,0.0129473,"ts that gain 0.86 and 1.35 BLEU points over Moses and GroundHog respectively. Besides, without the KL objective, VNMT w/o KL obtains even worse results than GroundHog. These results indicate the following two points: 1) explicitly modeling underlying semantics by a latent variable indeed benefits neural machine translation, and 2) the improvements of our model are not from enlarging the network. https://github.com/DeepLearnXMU/VNMT. 527 Results on Long Sentences We further testify VNMT on long sentence translation where the vanilla NMT usually suffers from attention failures (Tu et al., 2016; Bentivogli et al., 2016). We believe that the global latent variable can play an important role on long sentence translation. Our first experiment is carried out on 6 disjoint groups according to the length of source sentences in our test sets. Figure 3 shows the BLEU scores of two neural models. We find that the performance curve of our VNMT model always appears to be on top of that of GroundHog with a certain margin. Specifically, on the final group with the longest source sentences, our VNMT obtains the biggest improvement (3.55 BLEU points). Overall, these obvious improvements on all groups in terms of the length"
D16-1050,P15-1001,0,0.416218,"t2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 (Papineni et al., 2002) metric to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test. We compared our model against two state-of-theart SMT and NMT systems: • Moses (Koehn et al., 2007): a phrase-based SMT system. 4 This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT’14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/ 526 • GroundHog (Bahdanau et al., 2014): attention-based NMT system. an Additionally, we also compared with a variant of VNMT, which does not contain the KL part in the objective (VNMT w/o KL). This is achieved by setting hz to µ0 . For Moses, we adopted all the default settings except for the language model. We trained a 4-gram language model on the Xinhua section of the English Gigaword corpus (306M words) using the SRILM7 toolkit with modified Kneser-Ney smoothing. Important"
D16-1050,D13-1176,0,0.0324504,"the vanilla neural machine translation baselines. 1 Introduction Neural machine translation (NMT) is an emerging translation paradigm that builds on a single and unified end-to-end neural network, instead of using a variety of sub-models tuned in a long training pipeline. It requires a much smaller memory than ∗ Corresponding author phrase- or syntax-based statistical machine translation (SMT) that typically has a huge phrase/rule table. Due to these advantages over traditional SMT system, NMT has recently attracted growing interests from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016). Current NMT models mainly take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into distributed representations, and a neural decoder generates the corresponding target sentence y according to these representations1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Typically, the underlying semantic representations of source and target sentences are learned in an impl"
D16-1050,P07-2045,0,0.0263654,"the NIST MT02/03/04/06/08 datasets as the test sets for the Chinese-English task. Our English-German training data5 consists of 4.5M sentence pairs with 116M English words and 110M German words6 . We used the newstest2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 (Papineni et al., 2002) metric to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test. We compared our model against two state-of-theart SMT and NMT systems: • Moses (Koehn et al., 2007): a phrase-based SMT system. 4 This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT’14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/ 526 • GroundHog (Bahdanau et al., 2014): attention-based NMT system. an Additionally, we also compared with a variant of VNMT, which does not contain the KL part in the objective (VNMT w/o KL). This is achieved by setting hz to µ0 . For Moses, we adopted all the defau"
D16-1050,W04-3250,0,0.0315104,"80.9M Chinese words and 86.4M English words respectively. We used the NIST MT05 dataset as the development set, and the NIST MT02/03/04/06/08 datasets as the test sets for the Chinese-English task. Our English-German training data5 consists of 4.5M sentence pairs with 116M English words and 110M German words6 . We used the newstest2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 (Papineni et al., 2002) metric to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test. We compared our model against two state-of-theart SMT and NMT systems: • Moses (Koehn et al., 2007): a phrase-based SMT system. 4 This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT’14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/ 526 • GroundHog (Bahdanau et al., 2014): attention-based NMT system. an Additionally, we also compared with a variant of VNMT, which does not con"
D16-1050,P09-1067,0,0.0159698,"hat allows the iterative construction of complex images. Very recently, Miao et al. (2015) propose a generic variational inference framework for generative and conditional models of text. The most related work is that of Bowman et 529 al. (2015), where they develop a variational autoencoder for unsupervised generative language modeling. The major difference is that they focus on the monolingual language model, while we adapt this technique to bilingual translation. Although variational neural models have been widely used in NLP tasks and the variational decoding has been investigated for SMT (Li et al., 2009), the adaptation and utilization of variational neural model to neural machine translation, to the best of our knowledge, has never been investigated before. 6 Conclusion and Future Work In this paper, we have presented a variational model for neural machine translation that incorporates a continuous latent variable to model the underlying semantics of sentence pairs. We approximate the posterior distribution with neural networks and reparameterize the variational lower bound. This enables our model to be an end-to-end neural network that can be optimized through the stochastic gradient algori"
D16-1050,D15-1166,0,0.478478,") is an emerging translation paradigm that builds on a single and unified end-to-end neural network, instead of using a variety of sub-models tuned in a long training pipeline. It requires a much smaller memory than ∗ Corresponding author phrase- or syntax-based statistical machine translation (SMT) that typically has a huge phrase/rule table. Due to these advantages over traditional SMT system, NMT has recently attracted growing interests from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016). Current NMT models mainly take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into distributed representations, and a neural decoder generates the corresponding target sentence y according to these representations1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Typically, the underlying semantic representations of source and target sentences are learned in an implicit way in this framework, which heavily relies on the attention mechanism (Bahdanau"
D16-1050,P15-1002,0,0.0600673,"Missing"
D16-1050,P02-1040,0,0.0970109,"n translation tasks. Our Chinese-English training data4 consists of 2.9M sentence pairs, with 80.9M Chinese words and 86.4M English words respectively. We used the NIST MT05 dataset as the development set, and the NIST MT02/03/04/06/08 datasets as the test sets for the Chinese-English task. Our English-German training data5 consists of 4.5M sentence pairs with 116M English words and 110M German words6 . We used the newstest2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 (Papineni et al., 2002) metric to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test. We compared our model against two state-of-theart SMT and NMT systems: • Moses (Koehn et al., 2007): a phrase-based SMT system. 4 This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT’14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/ 526 • GroundHog (Bahdanau et al., 2014): attention-based NMT"
D16-1050,P16-5005,0,0.196025,"-end neural network, instead of using a variety of sub-models tuned in a long training pipeline. It requires a much smaller memory than ∗ Corresponding author phrase- or syntax-based statistical machine translation (SMT) that typically has a huge phrase/rule table. Due to these advantages over traditional SMT system, NMT has recently attracted growing interests from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016). Current NMT models mainly take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into distributed representations, and a neural decoder generates the corresponding target sentence y according to these representations1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Typically, the underlying semantic representations of source and target sentences are learned in an implicit way in this framework, which heavily relies on the attention mechanism (Bahdanau et al., 2014) to identify semantic alignments between source and target words"
D16-1050,D07-1091,0,\N,Missing
D17-1149,P17-2021,0,0.00734708,"ts of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc´ıa-Mart´ınez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model. Li et al. (2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016)"
D17-1149,D16-1162,0,0.0486271,"ropose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpo"
D17-1149,J93-2003,0,0.107649,"e, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese→English translation show that the proposed model achieves significant improvements over the baseline on various test sets. 1 Introduction Neural machine translation (NMT) has been receiving increasing attention due to its impressive ∗ Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used. Specifically, the encoder encodes a source sentence into a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine tra"
D17-1149,P05-1033,0,0.0429496,"the vocabulary as the general NMT decoder does. Experiment results on the Chinese→English translation show that the proposed model achieves significant improvements over the baseline on various test sets. 1 Introduction Neural machine translation (NMT) has been receiving increasing attention due to its impressive ∗ Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used. Specifically, the encoder encodes a source sentence into a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavi"
D17-1149,P16-1160,0,0.0905162,"o a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005) as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts. Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units. In contrast, phrases are much better than words as translation units in SMT and have made a significant advance in translation quality. Therefore, a natural question arises: Can we translate phrases in NMT? Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016). However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the wordby-word generation framework"
D17-1149,P16-2058,0,0.0325284,"Missing"
D17-1149,P15-1002,0,0.0266067,"rporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT."
D17-1149,P16-1078,0,0.0231179,"istic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc´ıa-Mart´ınez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model. Li et al. (2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of appro"
D17-1149,D16-1249,0,0.0212281,"ich dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system’s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016). 6 Conclusion In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the encoder-decoder architecture. At decoding, the SMT model dynamically generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory. Then the proposed model reads the phrase memory and uses the balancer to make probability estimations for the phrases in the phrase memory. Finally the NMT decoder selects a phrase from the phrase memory or"
D17-1149,P16-5005,0,0.0185392,"Missing"
D17-1149,C16-1172,0,0.0273232,"Missing"
D17-1149,P16-1154,0,0.0169774,"e log-likelihood: C(θ) = Ty N X X n=1 i=1 n log P (yin |y&lt;i , xn ) (5) given the training data with N bilingual sentences (Cho, 2015). In the testing phase, given a source sentence x, we use beam search strategy to search a target senˆ that approximately maximizes the conditence y tional probability P (y|x) ˆ = argmax P (y|x) y y 3 (6) Approach In this section, we introduce the proposed model which incorporates a phrase memory into the encoder-decoder architecture of NMT. Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT. 1422 The balancing weight λ is produced by the balancer – a multi-layer network. The balancer network takes as input the decoding information, including the context vector ci , the previous decoding state si−1 and the previous generated word yi−1 : λi = σ(fb (si , yi−1 , ci )) (8) where σ(·) is a sigmoid function and fb (·) is the activation function. Intuitively, the weight λ can be treated as the estimated importance of the phrase to be generated. We expect λ to be high if the phra"
D17-1149,P16-1014,0,0.00748876,"model by maximizing the log-likelihood: C(θ) = Ty N X X n=1 i=1 n log P (yin |y&lt;i , xn ) (5) given the training data with N bilingual sentences (Cho, 2015). In the testing phase, given a source sentence x, we use beam search strategy to search a target senˆ that approximately maximizes the conditence y tional probability P (y|x) ˆ = argmax P (y|x) y y 3 (6) Approach In this section, we introduce the proposed model which incorporates a phrase memory into the encoder-decoder architecture of NMT. Inspired by the recent work on attaching an external structure to the encoder-decoder architecture (Gulcehre et al., 2016; Gu et al., 2016; Tang et al., 2016; Wang et al., 2017), we adopt a similar approach to incorporate the phrase memory into NMT. 1422 The balancing weight λ is produced by the balancer – a multi-layer network. The balancer network takes as input the decoding information, including the context vector ci , the previous decoding state si−1 and the previous generated word yi−1 : λi = σ(fb (si , yi−1 , ci )) (8) where σ(·) is a sigmoid function and fb (·) is the activation function. Intuitively, the weight λ can be treated as the estimated importance of the phrase to be generated. We expect λ to be"
D17-1149,P15-1001,0,0.0571924,"ral sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT po"
D17-1149,D13-1176,0,0.164447,"Missing"
D17-1149,P03-1021,0,0.0613915,"n source and target words, which is derived from attention distribution produced by the NMT model (Wang et al., 2017). SMT coverage vector in (Wang et al., 2017) is also introduced to avoid repeat phrasal recommendations. In our work, the potential phrase is phrase with high SMT score which is defined as following: SM Tscore (pl |y&lt;t , x) = M X m=1 wm hm (pl , x(pl )) (10) where pl is a target phrase and x(pl ) is its corresponding source span. hm (pl , x(pl )) is a SMT feature function and wm is its weight. The feature weights can be tuned by the minimum error rate training (MERT) algorithm (Och, 2003). This leads to a better interaction between SMT and NMT models. It should be emphasized that our memory is dynamically updated at each decoding step based on the decoding history from both SMT and NMT models. The proposed model is very flexible, where the phrase memory can be either fully dynamically generated by an SMT model or directly extracted from a bilingual dictionary, or any other bilingual resources storing idiomatic translations or bilingual multi-word expressions, which may lead to a further improvement. 2 Reading Phrase Memory When phrases are read from the memory, they are rescor"
D17-1149,W16-2209,0,0.161524,"d model with its selection preference for special target phrases. With these information, we enrich the context vector ci to enable the proposed model to make better decisions, as described below. Following the commonly-used strategy in sequence tagging tasks (Xue and Shen, 2003), we allow the words in a phrase to share the same chunk tag and introduce a special tag for the beginning word. For example, the phrase “ &E S (information security)” is tagged as a noun phrase “NP”, and the tag sequence should be “NP B NP”. Partially motivated by the work on integrating linguistic features into NMT (Sennrich and Haddow, 2016), we represent the encoder input as the combination of word embeddings and chunking tag embeddings, instead of word embeddings alone in the conventional NMT. The new input is formulated as follows: [E w xi , E t ti ] 1 (9) Overlapped phrases may result in a high dimensionality in translation hypothesis representation and make it hard to employ shared fragments for efficient dynamic programming. 1423 NMT |is a word embedding where E w ∈ Rdw×|V matrix and dw is the word embedding dimensionT AG | ality, E t ∈ Rdt×|V is a tag embedding matrix and dt is the tag embedding dimensionality. [·] is the"
D17-1149,P16-1162,0,0.0342203,"ory that stores phrase pairs in symbolic forms for NMT. During decoding, the NMT decoder enquires the phrase memory and properly generates phrase translations. The significant differences between these efforts and ours are 1) that we dynamically generate phrase translations via an SMT model, and 2) that at the same time we modify the encoder to incorporate structural information to enhance the capability of NMT in phrase translation. Incorporating linguistic information into NMT NMT is essentially a sequence to sequence mapping network that treats the input/output units, eg., words, subwords (Sennrich et al., 2016), characters (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016), as non-linguistic symbols. However, linguistic information can be viewed as the taskspecific knowledge, which may be a useful supplementary to the sequence to sequence mapping network. To this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc´ıa-Mart´ınez et al. (2016) propo"
D17-1149,E17-2058,0,0.0163956,"Missing"
D17-1149,N03-1017,0,0.0969388,"enerates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese→English translation show that the proposed model achieves significant improvements over the baseline on various test sets. 1 Introduction Neural machine translation (NMT) has been receiving increasing attention due to its impressive ∗ Corresponding author translation performance (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Significantly different from conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT adopts a big neural network to perform the entire translation process in one shot, for which an encoderdecoder architecture is widely used. Specifically, the encoder encodes a source sentence into a continuous vector representation, then the decoder uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al."
D17-1149,P16-2049,0,0.093448,"phrase cannot be always deducible from the meanings of its individual words or parts. Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units. In contrast, phrases are much better than words as translation units in SMT and have made a significant advance in translation quality. Therefore, a natural question arises: Can we translate phrases in NMT? Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016). However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the wordby-word generation framework. To explore the phrase generation in NMT beyond the word-byword generation framework, we propose a novel architecture that integrates a phrase-based SMT 1421 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1421–1431 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics model into NMT. Specifically, we add an auxiliary phrase memory to store target phrases i"
D17-1149,P17-1064,1,0.855535,"this end, various kinds of linguistic annotations have been introduced into NMT to improve its translation performance. Sennrich and Haddow (2016) enrich the input units of NMT with various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels and morphological features. Garc´ıa-Mart´ınez et al. (2016) propose factored NMT using the morphological and grammatical decomposition of the words (factors) in output units. Eriguchi et al. (2016) explore the phrase structures of input sentences and propose a tree-to-sequence attention model for the vanilla NMT model. Li et al. (2017) propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while Aharoni and Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NM"
D17-1149,C16-1291,0,0.0145267,"ssigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system’s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016). 6 Conclusion In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the encoder-decoder architecture. At decoding, the SMT model dynamically generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory. Then the proposed model reads the phrase memory and uses the balancer to make probability estimations for the phrases in the phrase memory. Finally the NMT decoder selects a phrase from the phrase memory or a word from the voc"
D17-1149,P16-1100,0,0.024674,"uses the continuous vector representation to generate the corresponding target translation word by word. The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expressions, are crucial for natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005) as the meaning of a phrase cannot be always deducible from the meanings of its individual words or parts. Unfortunately current NMT is essentially a word-based or character-based (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016) translation system where phrases are not considered as translation units. In contrast, phrases are much better than words as translation units in SMT and have made a significant advance in translation quality. Therefore, a natural question arises: Can we translate phrases in NMT? Recently, there have been some attempts on multi-word phrase generation in NMT (Stahlberg et al., 2016b; Zhang and Zong, 2016). However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the wordby-word generation framework. To explore the phrase generation in NMT beyond the word-by"
D17-1149,Q17-1007,1,0.899484,"Missing"
D17-1149,P16-1008,1,0.871968,"Goldberg (2017) propose to incorporate target-side syntactic information into NMT by serializing the target sequences into linearized, lexicalized constituency trees. Zhang 1428 et al. (2016) integrate topic knowledge into NMT for domain/topic adaptation. Combining NMT and SMT A variety of approaches have been explored for leveraging the advantages of both NMT and conventional SMT. He et al. (2016) integrate SMT features with the NMT model under the log-linear framework in order to help NMT alleviate the limited vocabulary problem (Luong et al., 2015; Jean et al., 2015) and coverage problem (Tu et al., 2016). Arthur et al. (2016) observe that NMT is prone to making mistakes in translating low-frequency content words and therefore attempt at incorporating discrete translation lexicons into the NMT model, to alliterate the imprecise translation problem (Wang et al., 2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations"
D17-1149,1983.tc-1.13,0,0.583759,"Missing"
D17-1149,P16-1007,0,0.0372004,"the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system’s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016). 6 Conclusion In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the encoder-decoder architecture. At decoding, the SMT model dynamically generates relevant target phrases with contextual information provided by the NMT model and writes them to the phrase memory. Then the proposed model reads the phrase memory and"
D17-1149,W03-1728,0,0.0345713,"c programming, we restrict ourselves to non-overlap phrases.1 (2) We explicitly utilize the boundary information of the source-side chunk phrases, to better guide the proposed model to adopt a target phrase at an appropriate decoding step. (3) We enable the model to exploit the syntactic categories of chunk phrases to enhance the proposed model with its selection preference for special target phrases. With these information, we enrich the context vector ci to enable the proposed model to make better decisions, as described below. Following the commonly-used strategy in sequence tagging tasks (Xue and Shen, 2003), we allow the words in a phrase to share the same chunk tag and introduce a special tag for the beginning word. For example, the phrase “ &E S (information security)” is tagged as a noun phrase “NP”, and the tag sequence should be “NP B NP”. Partially motivated by the work on integrating linguistic features into NMT (Sennrich and Haddow, 2016), we represent the encoder input as the combination of word embeddings and chunking tag embeddings, instead of word embeddings alone in the conventional NMT. The new input is formulated as follows: [E w xi , E t ti ] 1 (9) Overlapped phrases may result i"
D17-1149,C16-1170,0,0.0367497,"Missing"
D17-1149,P17-2060,0,0.0224158,"2017). Motivated by the complementary strengths of syntactical SMT and NMT, different combination schemes of Hiero and NMT have been exploited to form SGNMT (Stahlberg et al., 2016a,b). Wang et al. (2017) propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpolation implemented by a gating function which dynamically assigns the weights. Niehues et al. (2016) propose to use SMT to pre-translate the inputs into target translations and employ the target pre-translations as input sequences in NMT. Zhou et al. (2017) propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine translation to improve the system’s suggestion quality (Wuebker et al., 2016). In addition, word alignments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT (Cohn et al., 2016; Mi et al., 2016; Liu et al., 2016). 6 Conclusion In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the en"
D17-1149,P15-4025,0,0.0387348,"Missing"
D17-1149,P07-2045,0,\N,Missing
D18-1340,P10-1064,0,0.0605493,"Missing"
D18-1340,2010.jec-1.4,0,0.0998183,"logy, has made remarkable progress in the past few years (Cho et al., 2014; Sutskever et al., 2014), which strongly encourages many translation agencies to embrace it for product deployment. A natural question during this deployment is how the strengths of both the traditional TM and new NMT technologies can be combined together for professional high-quality translation. Such attempts to the TM and MT combination have been already conducted in the context of statistical machine translation (SMT). A variety of efforts have been made to incorporate matched translation segments from TM into SMT (Koehn and Senellart, 2010). Partially inspired by these efforts, we aim at combining TM and NMT in this paper. Different from TM and SMT, both of which use symbolic fragments to construct translations, NMT induces translations from a real-valued continuous space. Furthermore, NMT is trained in an ∗ Corresponding author end-to-end fashion, which makes it not easy to be amenable to external intervention. Therefore, incorporating TM as external knowledge into NMT is challenging. In this paper, we propose a novel and effective method to address this issue in the combination of TM and NMT. The key idea behind this method is"
D18-1340,P02-1040,0,0.100542,"Missing"
D18-1340,2009.mtsummit-posters.15,0,0.0886738,"Missing"
D18-1340,Q17-1007,0,0.0207127,"matched translation segments from tm t for the decoder. 2.2 TM Gating Network When we translate a source sentence, in addition to the input of the sentence itself, we also have a TM matched translation (tm t) semantically similar to the sentence as an additional input. We want the additional input to act as a translation example for providing positive guide to target word prediction. In order to balance the information flow from the two inputs (src and tm t) into the decoder, we further introduce a TM gating network to control the respective proportions of tm t and src, partially inspired by Tu et al. (2017) who propose a gating mechanism to combine source and target contexts. We formulate the TM gating network as follows: g tm = f (st−1 , yt−1 , csrc , ctm t ) where st−1 is the previous hidden state, yt−1 is the previously predicted target word, and f is a 1 In this paper, we use GRU encoders and decoders. However, our method can be applicable to other encoders and decoders. #Sentences Average FMS train 1, 117, 452 0.1890 dev 804 0.5493 test 1, 614 0.5392 Table 1: Statistics of the training data, development and test set. FMS: fuzzy match score. logistic sigmoid function. 2.3 TM-Guided Decoder I"
D18-1340,P13-1002,0,0.208241,"ecoder. 4 For the proposed NMT-GTM model, we used tuples (src, tm t, tgt) as input. The rest of the parameter settings were consistent with the baseline model. To calculate the cosine similarity, we used the fasttext tool 2 with the dimension of 100 to obtain sentence embeddings. Experimental Results Related Work Various strategies have been proposed to combine TM and SMT (Koehn and Senellart, 2010; He et al., 2010). Their key ideas are to integrate the translations of the same fragments from TM into SMT, and let SMT only translate those different parts. In order to better model this process, Wang et al. (2013, 2014) use different features to allow relevant TM information to guide SMT decoding. 3045 主席 说 ，津巴布韦 代表 根据 议事规则 43 条 要求 参加 该项 目的 讨论 the chairman said that the representative of zimbabwe asked to participate in the discussion of the item in accordance with rule 43 of the rules of procedure . tm s 主席 说 ，塞尔维亚 代表 请求 依据 议事规则 第 43 条 参与 讨论 项目 tm t the chairman said that the representative of serbia had asked to participate in the discussion of the item in accordance with rule 43 of the rules of procedure . RNNSearch the chairman said that the representative of zimbabwe, in accordance with rule 43,"
D18-1340,C14-1039,0,0.0364742,"Missing"
D18-1340,N16-1004,0,0.0432586,"Missing"
D18-1459,D15-1075,0,0.0398847,"Missing"
D18-1459,P16-1139,0,0.0297346,"Missing"
D18-1459,buck-etal-2014-n,0,0.0330415,"Missing"
D18-1459,D15-1141,0,0.0565569,"Missing"
D18-1459,D16-1053,0,0.0417364,"Missing"
D18-1459,P17-1012,0,0.333363,"hitectures. In practice, 4-layer QRNN encoder and decoder are used to gain translation quality that is comparable to that of singlelayer LSTM/GRU NMT. In particular, our onelayer model achieves significantly higher performance than a 10-layer SRU system. Finally, our work is also related to the efforts in developing alternative architectures for NMT models. Zhou et al. (2016) introduce fast-forward connections between adjacent stacked RNN layers to ease gradient propagation. Wang et al. (2017a) propose a linear associate unit to reduce the gradient propagation length along layers in deep NMT. Gehring et al. (2017b) and Vaswani et al. (2017) explore purely convolutional and attentional archi4274 σ σ tanh σ σ ht−1 × σ ct + × ht−1 tanh σ × tanh × σ ht−1 tanh xt xt ct−1 σ ht × + 1- σ ht × ht−1 × σ xt × (b) GRU + × xt (a) LSTM h ht σσ+ tanh ht xt + (c) ATR h ht−1 and ATR. c× indicates + ht Neural Network Pointwise Vector to Concatenate Copy Figure t−1 1: Architecture for LSTM,t GRU the memory cell specific the LSTM ∗ Operation Transfer Layer 1- × × σnetwork. x∗ and h∗ denote the input and output hidden states respectively. σ σ tanh σ+ x tectures as alternatives to RNNs for neural translax tion. With carefu"
D18-1459,P15-1001,0,0.508996,"tion mechanism has proved very useful in NMT (Vaswani et al., 2017), we conjecture that such property of ATR partially contributes to its success in machine translation as shown in our experiments. We visualize the dependencies captured by Equation (10) in Section 5.3. 4 4.1 Experiments Setup We conducted our main experiments on WMT14 English-German and English-French translation tasks. Translation quality is measured by casesensitive BLEU-4 metric (Papineni et al., 2002). Details about each dataset are as follows: English-German To compare with previous reported results (Luong et al., 2015b; Jean et al., 2015; Zhou et al., 2016; Wang et al., 2017a), we used the same training data of WMT 2014, which consist of 4.5M sentence pairs. We used the newstest2013 as our dev set, and the newstest2014 as our test set. English-French We used the WMT 2014 training data. This corpora contain 12M selected sentence pairs. We used the concatenation of newstest2012 and newstest2013 as our dev set, and the newstest2014 as our test set. The used NMT system is an attention-based encoder-decoder system, which employs a bidirectional recurrent network as its encoder and a two-layer hierarchical unidirectional recurrent"
D18-1459,D15-1166,0,0.689307,"ns. As the self-attention mechanism has proved very useful in NMT (Vaswani et al., 2017), we conjecture that such property of ATR partially contributes to its success in machine translation as shown in our experiments. We visualize the dependencies captured by Equation (10) in Section 5.3. 4 4.1 Experiments Setup We conducted our main experiments on WMT14 English-German and English-French translation tasks. Translation quality is measured by casesensitive BLEU-4 metric (Papineni et al., 2002). Details about each dataset are as follows: English-German To compare with previous reported results (Luong et al., 2015b; Jean et al., 2015; Zhou et al., 2016; Wang et al., 2017a), we used the same training data of WMT 2014, which consist of 4.5M sentence pairs. We used the newstest2013 as our dev set, and the newstest2014 as our test set. English-French We used the WMT 2014 training data. This corpora contain 12M selected sentence pairs. We used the concatenation of newstest2012 and newstest2013 as our dev set, and the newstest2014 as our test set. The used NMT system is an attention-based encoder-decoder system, which employs a bidirectional recurrent network as its encoder and a two-layer hierarchical unidi"
D18-1459,P15-1002,0,0.0597018,"Missing"
D18-1459,P02-1040,0,0.106518,"17). It can be considered as a forward unnormalized selfattention where each hidden state attends to all its previous positions. As the self-attention mechanism has proved very useful in NMT (Vaswani et al., 2017), we conjecture that such property of ATR partially contributes to its success in machine translation as shown in our experiments. We visualize the dependencies captured by Equation (10) in Section 5.3. 4 4.1 Experiments Setup We conducted our main experiments on WMT14 English-German and English-French translation tasks. Translation quality is measured by casesensitive BLEU-4 metric (Papineni et al., 2002). Details about each dataset are as follows: English-German To compare with previous reported results (Luong et al., 2015b; Jean et al., 2015; Zhou et al., 2016; Wang et al., 2017a), we used the same training data of WMT 2014, which consist of 4.5M sentence pairs. We used the newstest2013 as our dev set, and the newstest2014 as our test set. English-French We used the WMT 2014 training data. This corpora contain 12M selected sentence pairs. We used the concatenation of newstest2012 and newstest2013 as our dev set, and the newstest2014 as our test set. The used NMT system is an attention-based"
D18-1459,P14-1028,0,0.0705337,"Missing"
D18-1459,D14-1162,0,0.0819338,"Missing"
D18-1459,P16-1162,0,0.130298,"two-layer hierarchical unidirectional recurrent network as its decoder, companied with an additive attention mechanism (Bahdanau et al., 2015). We replaced the recurrent unit with our proposed ATR model. More details are given in Appendix A.1. We also conducted experiments on ChineseEnglish translation, natural language inference and Chinese word segmentation. Details and experiment results are provided in Appendix A.2. 4.2 Training We set the maximum length of training instances to 80 words for both English-German and EnglishFrench task. We used the byte pair encoding compression algorithm (Sennrich et al., 2016) to reduce the vocabulary size as well as to deal with the issue of rich morphology. We set the vocabulary size of both source and target languages to 40K for all translation tasks. All out-of-vocabulary words were replaced with a token “unk”. We used 1000 hidden units for both encoder and decoder. All word embeddings had dimensionality 620. We initialized all model parameters randomly according to a uniform distribution ranging from -0.08 to 0.08. These tunable parameters were then optimized using Adam algorithm (Kingma and Ba, 2015) with the two momentum parameters set to 0.9 and 0.999 respe"
D18-1459,P16-1159,0,0.0463158,"Missing"
D18-1459,W03-1719,0,0.0550863,"Missing"
D18-1459,P17-1013,0,0.0769732,"their RNN model can be trained as fast as CNNs. However, to obtain promising results, QRNN and SRU have to use deep architectures. In practice, 4-layer QRNN encoder and decoder are used to gain translation quality that is comparable to that of singlelayer LSTM/GRU NMT. In particular, our onelayer model achieves significantly higher performance than a 10-layer SRU system. Finally, our work is also related to the efforts in developing alternative architectures for NMT models. Zhou et al. (2016) introduce fast-forward connections between adjacent stacked RNN layers to ease gradient propagation. Wang et al. (2017a) propose a linear associate unit to reduce the gradient propagation length along layers in deep NMT. Gehring et al. (2017b) and Vaswani et al. (2017) explore purely convolutional and attentional archi4274 σ σ tanh σ σ ht−1 × σ ct + × ht−1 tanh σ × tanh × σ ht−1 tanh xt xt ct−1 σ ht × + 1- σ ht × ht−1 × σ xt × (b) GRU + × xt (a) LSTM h ht σσ+ tanh ht xt + (c) ATR h ht−1 and ATR. c× indicates + ht Neural Network Pointwise Vector to Concatenate Copy Figure t−1 1: Architecture for LSTM,t GRU the memory cell specific the LSTM ∗ Operation Transfer Layer 1- × × σnetwork. x∗ and h∗ denote the input"
D18-1459,N16-1170,0,0.0487476,"Missing"
D18-1459,P18-1166,1,0.774222,"K Wang et al. (2017a) GRU with 4 layers + LAU + PosUnk 80K Wang et al. (2017a) GRU with 4 layers + LAU + PosUnk + ensemble 80K Wu et al. (2016) LSTM with 8 layers + RL-refined WPM 32K Wu et al. (2016) LSTM with 8 layers + RL-refined ensemble 80K Transformer with 6 layers + base model 37K Vaswani et al. (2017) Comparable NMT systems (the same tokenization) Luong et al. (2015a) LSTM with 4 layers + local att. + unk replace 50K Zhang et al. (2017a) GRU with gated attention + BPE 40K Gehring et al. (2017b) CNN with 15 layers + BPE 40K CNN with 15 layers + BPE + ensemble 40K Gehring et al. (2017b) Zhang et al. (2018a) Transformer with 6 layers + aan + base model 32K Our end-to-end NMT systems RNNSearch + GRU + BPE 40K RNNSearch + LSTM + BPE 40K RNNSearch + RAN + BPE 40K this work RNNSearch + ATR + BPE 40K RNNSearch + ATR + CA + BPE 40K GNMT + ATR + BPE 40K RNNSearch + ATR + CA + BPE + ensemble 40K tok BLEU - detok BLEU 20.70 20.60 20.70 23.32 23.80 26.10 24.61 26.30 27.30 - 20.90 23.84 25.16 26.43 26.31 - 22.54 22.96 22.14 22.48 23.31 24.16 24.97 22.06 22.39 21.40 21.99 22.70 23.59 24.33 Table 2: Tokenized (tok) and detokenized (detok) case-sensitive BLEU scores on the WMT14 EnglishGerman translation tas"
D18-1459,P17-1140,0,0.0198865,"16) respectively. “aan” denotes the average attention network proposed by Zhang et al. (2018a). of word embedding and hidden state which we set to be 512. 4.3 Results on English-German Translation The translation results are shown in Table 2. We also provide results of several existing systems that are trained with comparable experimental settings to ours. In particular, our single model yields a detokenized BLEU score of 21.99. In order to show that the proposed model can be orthogonal to previous methods that improve LSTM/GRU-based NMT, we integrate a singlelayer context-aware (CA) encoder (Zhang et al., 2017b) into our system. The ATR+CA system further reaches 22.7 BLEU, outperforming the winner system (Buck et al., 2014) by a substantial improvement of 2 BLEU points. Enhanced with the deep GNMT architecture, the GNMT+ATR system yields a gain of 0.89 BLEU points over the RNNSearch+ATR+CA and 1.6 BLEU points over the RNNSearch + ATR. Notice that different from our system which was trained on the parallel corpus alone, the winner system used a huge monolingual text to enhance its language model. Compared with the existing LSTM-based (Luong et al., 2015a) deep NMT system, our shallow/deep model achi"
D18-1459,D13-1061,0,0.0655976,"Missing"
D18-1459,Q18-1011,0,0.0310456,"Missing"
D18-1459,Q16-1027,0,0.246768,"ove parallelism. Very recently, Lei and Zhang (2017) propose a simple recurrent unit (SRU). With the cuDNN optimization, their RNN model can be trained as fast as CNNs. However, to obtain promising results, QRNN and SRU have to use deep architectures. In practice, 4-layer QRNN encoder and decoder are used to gain translation quality that is comparable to that of singlelayer LSTM/GRU NMT. In particular, our onelayer model achieves significantly higher performance than a 10-layer SRU system. Finally, our work is also related to the efforts in developing alternative architectures for NMT models. Zhou et al. (2016) introduce fast-forward connections between adjacent stacked RNN layers to ease gradient propagation. Wang et al. (2017a) propose a linear associate unit to reduce the gradient propagation length along layers in deep NMT. Gehring et al. (2017b) and Vaswani et al. (2017) explore purely convolutional and attentional archi4274 σ σ tanh σ σ ht−1 × σ ct + × ht−1 tanh σ × tanh × σ ht−1 tanh xt xt ct−1 σ ht × + 1- σ ht × ht−1 × σ xt × (b) GRU + × xt (a) LSTM h ht σσ+ tanh ht xt + (c) ATR h ht−1 and ATR. c× indicates + ht Neural Network Pointwise Vector to Concatenate Copy Figure t−1 1: Architecture f"
D19-1168,N18-1118,0,0.0631454,"el machine translation. Most of existing studies aim to improve overall translation quality with the aid of document context. Among them, Maruf and Haffrai (2018), Wang et al. (2017), Zhang et al. (2018) and Miculicich et al. (2018) use extractionbased models to extract partial document context from previous sentences of the current sentence. In addition, Tu et al. (2018) and Kuang et al. (2018) employ cache-based models to selectively memorize the most relevant information in the document context. Different from above extraction-based models and cache-based models, there are also some works (Bawden et al., 2018; Voita et al., 2018) that pay much attention to discourse phenomena (Mitkov, 1999) related to document-level translation. Although these approaches have achieved some progress in document-level machine translation, they still suffer from incomplete document context. Further more, most of previous works are based on the RNNSearch model, and only few exceptions (Zhang et al., 2018; Miculicich et al., 2018) are on top of the state-of-the-art Transformer model. 6 Conclusion We have presented a hierarchical model to capture the global document context for documentlevel NMT. The proposed model can"
D19-1168,2012.eamt-1.60,0,0.231333,": X logP (y|x; θd , θˆs ) (12) θˆd = arg max θd 3 &lt;x,y>∈Dd Experimentation To examine the effect of our proposed HM-GDC model, we conduct experiments on both ChineseEnglish and German-English translation. 3.1 Experimental Settings Datasets For Chinese-English translation, we carry out experiments with sentence- and document-level corpora on two different domains: news and talks. For the sentence-level corpus, we use 2.8M sentence pairs from news corpora LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hongkong Hansards/Laws/News). We use the Ted talks corpus from the IWSLT 2017 (Cettolo et al., 2012) evaluation campaigns1 as the document-level parallel corpus, including 1,906 documents with 226K sentence pairs. We use dev2010 which contains 8 documents with 879 sentence pairs for development and tst2012tst2015 which contain 62 documents with 5566 sentence pairs for testing. For German-English translation, we use the document-level parallel Ted talks corpus from the IWSLT 2014 (Cettolo et al., 2012) evaluation campaigns, which contain 1,361 documents with 172K 1579 1 https://wit3.fbk.eu Method pre-training tst12 tst13 tst14 tst15 Avg RNNSearch (Bahdanau et al., 2015) Wang et al. (2017) Our"
D19-1168,P17-4012,0,0.160195,"Missing"
D19-1168,C18-1050,1,0.568516,"he NMT model to take global context into consideration and thus it can safely disambiguate those multi-sense words like chengxu. 5 Related Work Recent years have witnessed a variety of approaches proposed for document-level machine translation. Most of existing studies aim to improve overall translation quality with the aid of document context. Among them, Maruf and Haffrai (2018), Wang et al. (2017), Zhang et al. (2018) and Miculicich et al. (2018) use extractionbased models to extract partial document context from previous sentences of the current sentence. In addition, Tu et al. (2018) and Kuang et al. (2018) employ cache-based models to selectively memorize the most relevant information in the document context. Different from above extraction-based models and cache-based models, there are also some works (Bawden et al., 2018; Voita et al., 2018) that pay much attention to discourse phenomena (Mitkov, 1999) related to document-level translation. Although these approaches have achieved some progress in document-level machine translation, they still suffer from incomplete document context. Further more, most of previous works are based on the RNNSearch model, and only few exceptions (Zhang et al., 2"
D19-1168,P18-1118,0,0.0980894,"Missing"
D19-1168,D18-1325,0,0.732557,"ument-level translation under the guidance of context. Introduction ∗ 瀖 瀖 translation and results on news benchmark test sets have shown its “translation quality at human parity when compared to professional human translators” (Hassan et al., 2018). However, when turning to document-level translation, even the Transformer model yields a low performance as it translates each sentence in the document independently and suffers from the problem of ignoring document context. To address above challenge, various extractionbased methods (Maruf and Haffari, 2018; Wang et al., 2017; Zhang et al., 2018; Miculicich et al., 2018) have been proposed to extract previous context (pre-context) to guide the translation of the current sentence si , as shown in Figure 1. However, when there exists a huge gap between the pre-context and the context after the current sentence si , the guidance from pre-context is not sufficient for the NMT model to fully disambiguate the sentence si . On the one hand, the translation of the current sentence si may be inaccurate due to the one-sidedness of partial context. On the other hand, translating the succeeding sentences in the document may much suffer from the semantic bias due to the t"
D19-1168,W17-4802,0,0.312779,"antly improved by 5.26 BLEU points (the last row in Table 4). The overall results prove that our proposed model is robust and promising. It can significantly improve the performance of document-level translation when a two-step training strategy is used. 4.4 Pronoun & Noun Translation To intuitively illustrate how the translation performance is improved by our proposed HM-GDC model, we conduct a further analysis on pronoun and noun translation. For the pronoun translation, we evaluate the coreference and anaphora using the referencebased metric: the accuracy of pronoun translation (Miculicich Werlen and Popescu-Belis, 2017) in Chinese-English and German-English translation as shown in Table 5. From the results, our proposed HM-GDC model can well improve the performance of pronoun translation in both corpora due to the well captured global document context assigned to each word. Correspondingly, we display a translation example in Table 6 to further illustrate this. From the example, given the surrounding context, our proposed HM-GDC model can well infer the latent pronoun it and thus improve the translation performance of the Transformer model. For the analysis of noun translation, we display another example in"
D19-1168,P02-1040,0,0.103662,"the performance of our proposed HM-GDC model, we integrate the HM-GDC into the standard RNNSearch model to serve as a supplementary experiment. For the RNNSearch network, we borrow the implementation from OpenNMT (Klein et al., 2017). The encoder and decoder layers are all set to 2, the size of the hidden layer is set to 500, and the batch size is set to 64. Same as the Transformer model, we use the most frequent 50K words for both source and target vocabularies. We borrow other settings from (Bahdanau et al., 2015). The evaluation metric for both tasks is case-insensitive BLEU (multi-bleu) (Papineni et al., 2002). 3.2 Experimental Results In this paper, we compare our model with four strong baselines as shown in Table 1. Among them, the RNNSearch (Bahdanau et al., 2015) is a traditional RNN-based encoder-decoder model. Wang et al. (2017) propose to use a hierarchical model to extract partial document context based on the RNNSearch model. To compare with these two RNN-based works, we integrate our proposed HM-GDC model into the encoder of the RNNSearch model using the same method in Section 2.2 and keep other settings the same as the basic RNNSearch model. Different from RNNbased works, Vaswani et al."
D19-1168,P16-1159,0,0.0857443,"s, we combine the outputs of both the Encoder-Decoder attention sub-layer and the DocEnc-Decoder attention sub-layer into one single output H (n) : H (n) = E (n) + G(n) (10) where E (n) is the output of the Encoder-Decoder attention sub-layer, and G(n) is the output of the DocEnc-Decoder attention sub-layer. 2.3 Model Training In document-level translation, the standard training objective is to maximize the log-likelihood of the document-level parallel corpus. However, due to the size limitation of document-level parallel corpora, previous studies (Zhang et al., 2018; Miculicich et al., 2018; Shen et al., 2016) use two-step training strategies to take advantage of large-scale sentence-level parallel pairs. Following their studies, we also take a two-step training strategy to train our model. Specially, we borrow a large-scale corpus with out-of-domain sentencelevel parallel pairs Ds to pre-train our model first, and then use a small-scale corpus with in-domain document-level parallel pairs Dd to fine-tune it. In this work, we follow Voita et al. (2018) to make the sentence and document encoders share the same model parameters. For the RNNSearch model, we share the parameters in the hidden layers of"
D19-1168,Q18-1029,0,0.137865,"ate the HM-GDC into the NMT model to take global context into consideration and thus it can safely disambiguate those multi-sense words like chengxu. 5 Related Work Recent years have witnessed a variety of approaches proposed for document-level machine translation. Most of existing studies aim to improve overall translation quality with the aid of document context. Among them, Maruf and Haffrai (2018), Wang et al. (2017), Zhang et al. (2018) and Miculicich et al. (2018) use extractionbased models to extract partial document context from previous sentences of the current sentence. In addition, Tu et al. (2018) and Kuang et al. (2018) employ cache-based models to selectively memorize the most relevant information in the document context. Different from above extraction-based models and cache-based models, there are also some works (Bawden et al., 2018; Voita et al., 2018) that pay much attention to discourse phenomena (Mitkov, 1999) related to document-level translation. Although these approaches have achieved some progress in document-level machine translation, they still suffer from incomplete document context. Further more, most of previous works are based on the RNNSearch model, and only few exc"
D19-1168,P18-1117,0,0.27534,"arallel corpus. However, due to the size limitation of document-level parallel corpora, previous studies (Zhang et al., 2018; Miculicich et al., 2018; Shen et al., 2016) use two-step training strategies to take advantage of large-scale sentence-level parallel pairs. Following their studies, we also take a two-step training strategy to train our model. Specially, we borrow a large-scale corpus with out-of-domain sentencelevel parallel pairs Ds to pre-train our model first, and then use a small-scale corpus with in-domain document-level parallel pairs Dd to fine-tune it. In this work, we follow Voita et al. (2018) to make the sentence and document encoders share the same model parameters. For the RNNSearch model, we share the parameters in the hidden layers of Bi-RNNs in the sentence and document encoders. For the Transformer model, we share the parameters of the multi-head self-attention layers in the sentence and document encoders. During training, we first optimize the sentencelevel parameters θs (colored in wheat in Figure 3) with the large-scale sentence-level parallel pairs Ds : X θˆs = arg max logP (y|x; θs ) (11) θs &lt;x,y>∈Ds Then, we optimize the document-level parameters θd (colored in pale bl"
D19-1168,D17-1301,0,0.402415,"""$ !""# Figure 1: An illustration of document-level translation under the guidance of context. Introduction ∗ 瀖 瀖 translation and results on news benchmark test sets have shown its “translation quality at human parity when compared to professional human translators” (Hassan et al., 2018). However, when turning to document-level translation, even the Transformer model yields a low performance as it translates each sentence in the document independently and suffers from the problem of ignoring document context. To address above challenge, various extractionbased methods (Maruf and Haffari, 2018; Wang et al., 2017; Zhang et al., 2018; Miculicich et al., 2018) have been proposed to extract previous context (pre-context) to guide the translation of the current sentence si , as shown in Figure 1. However, when there exists a huge gap between the pre-context and the context after the current sentence si , the guidance from pre-context is not sufficient for the NMT model to fully disambiguate the sentence si . On the one hand, the translation of the current sentence si may be inaccurate due to the one-sidedness of partial context. On the other hand, translating the succeeding sentences in the document may m"
D19-1168,D18-1049,0,0.377626,"Missing"
D19-1249,O08-3002,0,0.082358,"Missing"
D19-1249,P16-1223,0,0.0685014,"Missing"
D19-1249,P17-1171,0,0.056072,"Missing"
D19-1249,P17-1055,0,0.0138507,"e resolution, inter-sentential reasoning, implicit causality understanding, etc. • We build monolingual, multilingual and cross-lingual MRC baseline models on BiPaR and provides baseline results as well as human performance on this dataset. 2 Related Work MRC Datasets and Models Large-scale clozestyle datasets, such as CNN/Daily Mail (Hermann 2453 et al., 2015), have been automatically developed in the early days of MRC. Several neural network models have been proposed and tested on these datasets, such as ASReader (Kadlec et al., 2016), StanfordAttentiveReader (Chen et al., 2016), AoAReader (Cui et al., 2017), etc. However, Chen et al. (2016) argue that such datasets may be noisy due to the automatic data creation method and co-reference errors. Rajpurkar et al. (2016) propose SQuAD, a dataset created from English Wikipedia, where questions are manually generated by crowdsourced workers, and answers are spans in the Wikipedia passages. Along with the development of this dataset, a variety of neural MRC models have been proposed, such as BiDAF (Seo et al., 2016), R-NET (Wang et al., 2017), ReasonNet (Shen et al., 2017), DCN (Xiong et al., 2016), QANet (Yu et al., 2018), SAN (Liu et al., 2018), etc."
D19-1249,C16-1167,0,0.180166,"Missing"
D19-1249,L18-1431,0,0.573763,"Missing"
D19-1249,N19-1423,0,0.0253959,"e on BiPaR, we hired three other bilingual crowdsourced workers to independently answer questions (both Chinese and English) on the test set which contains three answers per question as described in Section 3.3. We then calculated the average results of the three human workers as the final human performance on this dataset, which are shown in Table 5. 6.3 Baseline models We adapted the following state-of-the-art models to the dataset and MRC tasks as described in Section 4. DrQA6 : DrQA (Chen et al., 2017) is a simple but effective neural network model for reading comprehension. BERT7 : BERT (Devlin et al., 2019) is a strong method for pre-training language representations, which obtains the state-of-the-art results on many reading comprehension datasets. We used the multilingual model of BERT trained on multiple languages for the evaluation of our multilingual MRC task. 6.4 Experimental Setup All the baselines were tested using their default hyper-parameters except BERT. We only changed the batch size to 8 for BERT base and 6 for BERT large due to the memory limit of our 5 https://github.com/ymcui/cmrc2018 https://github.com/hitvoice/DrQA 7 https://github.com/huggingface/pytorch-pretrainedBERT 6 3 Ev"
D19-1249,P15-1166,0,0.0185177,"o translate questions into the language of passages, and then treat them as a monolingual MRC task. For the second two forms of cross-lingual MRC, such as (Pzh , Pen , Qen , Aen , Azh ), we first obtain Aen through a monolingual MRC model, then use the word alignment tool fast align4 to obtain the aligned Azh from Pzh . Alternative approaches that do not rely on machine translation or word alignments for the cross-lingual MRC tasks are to directly build cross-lingual MRC models on language-mixed training instances constructed from BiPaR or to explore multi-task learning on multiple languages (Dong et al., 2015). 6 Experiments We carried out experiments with state-of-the-art MRC models on BiPaR to provide machine results for these 7 MRC tasks defined above. We also provide human performance on the monolingual tasks and demonstrate the performance trajectory of human and machine in answering BiPaR questions. 6.1 Like evaluations on other extraction-based datasets, we used EM and F1 to evaluate model accuracy on BiPaR. Particularly, we used the evaluation program of SQuAD1.1 for the English dataset in BiPaR, and the evaluation program5 of CMRC2018 for the Chinese dataset in BiPaR. 6.2 4 https://transla"
D19-1249,P17-1147,0,0.0439803,"N (Xiong et al., 2016), QANet (Yu et al., 2018), SAN (Liu et al., 2018), etc. Recent years have witnessed substantial progress made on this dataset. However, there are some limitations on SQuAD, which lie in that questions are created based on single passages, that answers are limited to a single span in passages, and that most questions can be answered from a single supporting sentence without requiring multi-sentence reasoning (Chen, 2018). To address these limitations, a number of datasets have be built recently, such as MS MARCO (Nguyen et al., 2016), DuReader (He et al., 2018), TriviaQA (Joshi et al., 2017), RACE (Lai et al., 2017), NarrativeQA (Kocisky et al., 2018), SQuAD2.0 (Rajpurkar et al., 2018), hotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), etc. These datasets and models are only for monolingual text understanding. By contrast, BiPaR, following these efforts of creating challenging MRC datasets, aims at setting up a new benchmark dataset for MRC on novels and bilingual/cross-lingual MRC. Multilingual MRC and Datasets Previous studies on multilingual MRC are very limited. Asai et al. (2018) propose a multilingual MRC system by translating the target language into a pivot languag"
D19-1249,P16-1086,0,0.0196428,"reveals that MRC on novels is very challenging, requiring skills of coreference resolution, inter-sentential reasoning, implicit causality understanding, etc. • We build monolingual, multilingual and cross-lingual MRC baseline models on BiPaR and provides baseline results as well as human performance on this dataset. 2 Related Work MRC Datasets and Models Large-scale clozestyle datasets, such as CNN/Daily Mail (Hermann 2453 et al., 2015), have been automatically developed in the early days of MRC. Several neural network models have been proposed and tested on these datasets, such as ASReader (Kadlec et al., 2016), StanfordAttentiveReader (Chen et al., 2016), AoAReader (Cui et al., 2017), etc. However, Chen et al. (2016) argue that such datasets may be noisy due to the automatic data creation method and co-reference errors. Rajpurkar et al. (2016) propose SQuAD, a dataset created from English Wikipedia, where questions are manually generated by crowdsourced workers, and answers are spans in the Wikipedia passages. Along with the development of this dataset, a variety of neural MRC models have been proposed, such as BiDAF (Seo et al., 2016), R-NET (Wang et al., 2017), ReasonNet (Shen et al., 2017), DCN"
D19-1249,Q18-1023,0,0.0262608,"t al., 2018), etc. Recent years have witnessed substantial progress made on this dataset. However, there are some limitations on SQuAD, which lie in that questions are created based on single passages, that answers are limited to a single span in passages, and that most questions can be answered from a single supporting sentence without requiring multi-sentence reasoning (Chen, 2018). To address these limitations, a number of datasets have be built recently, such as MS MARCO (Nguyen et al., 2016), DuReader (He et al., 2018), TriviaQA (Joshi et al., 2017), RACE (Lai et al., 2017), NarrativeQA (Kocisky et al., 2018), SQuAD2.0 (Rajpurkar et al., 2018), hotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), etc. These datasets and models are only for monolingual text understanding. By contrast, BiPaR, following these efforts of creating challenging MRC datasets, aims at setting up a new benchmark dataset for MRC on novels and bilingual/cross-lingual MRC. Multilingual MRC and Datasets Previous studies on multilingual MRC are very limited. Asai et al. (2018) propose a multilingual MRC system by translating the target language into a pivot language via runtime machine translation. They still rely on SQuAD t"
D19-1249,D17-1082,0,0.0283067,"et (Yu et al., 2018), SAN (Liu et al., 2018), etc. Recent years have witnessed substantial progress made on this dataset. However, there are some limitations on SQuAD, which lie in that questions are created based on single passages, that answers are limited to a single span in passages, and that most questions can be answered from a single supporting sentence without requiring multi-sentence reasoning (Chen, 2018). To address these limitations, a number of datasets have be built recently, such as MS MARCO (Nguyen et al., 2016), DuReader (He et al., 2018), TriviaQA (Joshi et al., 2017), RACE (Lai et al., 2017), NarrativeQA (Kocisky et al., 2018), SQuAD2.0 (Rajpurkar et al., 2018), hotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), etc. These datasets and models are only for monolingual text understanding. By contrast, BiPaR, following these efforts of creating challenging MRC datasets, aims at setting up a new benchmark dataset for MRC on novels and bilingual/cross-lingual MRC. Multilingual MRC and Datasets Previous studies on multilingual MRC are very limited. Asai et al. (2018) propose a multilingual MRC system by translating the target language into a pivot language via runtime machine tra"
D19-1249,P18-2023,0,0.0444096,"Missing"
D19-1249,P18-1157,0,0.0475056,"JRQ 4]K$]K东ቅᇓ⾔嗏ᮏѣԱӶѾ㚂փϋⲳ嗏ֵ &URVVOLQJXDO05& 4HQ:KRGLG0DGDPH+RQJDOORZWROHDYHILUVW&quot; $]Kᰖṯ䚉Ӱૂ唇嗏ֵȽ哺嗏ֵ Figure 1: Illustration of BiPaR with the monolingual, multilingual and cross-lingual MRC on the dataset. Introduction Machine reading comprehension is to evaluate how well computer systems understand natural language texts, where machines read a given text passage and answer questions about the passage. It has been regarded as a crucial technology for many applications such as question answering, dialogue systems (Nguyen et al., 2016; Chen et al., ∗ Corresponding author 2017; Liu et al., 2018; Wang et al., 2018) and so on. In order to enable machine to understand texts, large-scale reading comprehension datasets have been developed, such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), MS MACRO (Nguyen et al., 2016), hotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), etc. The majority of such datasets, unfortunately, are only for monolingual text understanding. To the best of our knowledge, there is no publicly available bilingual parallel reading comprehension dataset, which is exactly what BiPaR is mainly developed for, as illustrated in Figure 1."
D19-1249,D14-1162,0,0.0810674,"Missing"
D19-1249,P18-2124,0,0.026649,"ave witnessed substantial progress made on this dataset. However, there are some limitations on SQuAD, which lie in that questions are created based on single passages, that answers are limited to a single span in passages, and that most questions can be answered from a single supporting sentence without requiring multi-sentence reasoning (Chen, 2018). To address these limitations, a number of datasets have be built recently, such as MS MARCO (Nguyen et al., 2016), DuReader (He et al., 2018), TriviaQA (Joshi et al., 2017), RACE (Lai et al., 2017), NarrativeQA (Kocisky et al., 2018), SQuAD2.0 (Rajpurkar et al., 2018), hotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), etc. These datasets and models are only for monolingual text understanding. By contrast, BiPaR, following these efforts of creating challenging MRC datasets, aims at setting up a new benchmark dataset for MRC on novels and bilingual/cross-lingual MRC. Multilingual MRC and Datasets Previous studies on multilingual MRC are very limited. Asai et al. (2018) propose a multilingual MRC system by translating the target language into a pivot language via runtime machine translation. They still rely on SQuAD to train the MRC model of the pivot"
D19-1249,Q19-1016,0,0.111053,"understand natural language texts, where machines read a given text passage and answer questions about the passage. It has been regarded as a crucial technology for many applications such as question answering, dialogue systems (Nguyen et al., 2016; Chen et al., ∗ Corresponding author 2017; Liu et al., 2018; Wang et al., 2018) and so on. In order to enable machine to understand texts, large-scale reading comprehension datasets have been developed, such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), MS MACRO (Nguyen et al., 2016), hotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), etc. The majority of such datasets, unfortunately, are only for monolingual text understanding. To the best of our knowledge, there is no publicly available bilingual parallel reading comprehension dataset, which is exactly what BiPaR is mainly developed for, as illustrated in Figure 1. BiPaR 2452 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2452–2462, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Dataset CNN/DM (Hermann et al., 2"
D19-1249,P17-1018,0,0.019073,"these datasets, such as ASReader (Kadlec et al., 2016), StanfordAttentiveReader (Chen et al., 2016), AoAReader (Cui et al., 2017), etc. However, Chen et al. (2016) argue that such datasets may be noisy due to the automatic data creation method and co-reference errors. Rajpurkar et al. (2016) propose SQuAD, a dataset created from English Wikipedia, where questions are manually generated by crowdsourced workers, and answers are spans in the Wikipedia passages. Along with the development of this dataset, a variety of neural MRC models have been proposed, such as BiDAF (Seo et al., 2016), R-NET (Wang et al., 2017), ReasonNet (Shen et al., 2017), DCN (Xiong et al., 2016), QANet (Yu et al., 2018), SAN (Liu et al., 2018), etc. Recent years have witnessed substantial progress made on this dataset. However, there are some limitations on SQuAD, which lie in that questions are created based on single passages, that answers are limited to a single span in passages, and that most questions can be answered from a single supporting sentence without requiring multi-sentence reasoning (Chen, 2018). To address these limitations, a number of datasets have be built recently, such as MS MARCO (Nguyen et al., 2016), DuR"
D19-1249,P18-1178,0,0.012242,"⾔嗏ᮏѣԱӶѾ㚂փϋⲳ嗏ֵ &URVVOLQJXDO05& 4HQ:KRGLG0DGDPH+RQJDOORZWROHDYHILUVW&quot; $]Kᰖṯ䚉Ӱૂ唇嗏ֵȽ哺嗏ֵ Figure 1: Illustration of BiPaR with the monolingual, multilingual and cross-lingual MRC on the dataset. Introduction Machine reading comprehension is to evaluate how well computer systems understand natural language texts, where machines read a given text passage and answer questions about the passage. It has been regarded as a crucial technology for many applications such as question answering, dialogue systems (Nguyen et al., 2016; Chen et al., ∗ Corresponding author 2017; Liu et al., 2018; Wang et al., 2018) and so on. In order to enable machine to understand texts, large-scale reading comprehension datasets have been developed, such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), MS MACRO (Nguyen et al., 2016), hotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), etc. The majority of such datasets, unfortunately, are only for monolingual text understanding. To the best of our knowledge, there is no publicly available bilingual parallel reading comprehension dataset, which is exactly what BiPaR is mainly developed for, as illustrated in Figure 1. BiPaR 2452 Proceedin"
D19-1249,D18-1259,0,0.0994707,"how well computer systems understand natural language texts, where machines read a given text passage and answer questions about the passage. It has been regarded as a crucial technology for many applications such as question answering, dialogue systems (Nguyen et al., 2016; Chen et al., ∗ Corresponding author 2017; Liu et al., 2018; Wang et al., 2018) and so on. In order to enable machine to understand texts, large-scale reading comprehension datasets have been developed, such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), MS MACRO (Nguyen et al., 2016), hotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), etc. The majority of such datasets, unfortunately, are only for monolingual text understanding. To the best of our knowledge, there is no publicly available bilingual parallel reading comprehension dataset, which is exactly what BiPaR is mainly developed for, as illustrated in Figure 1. BiPaR 2452 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2452–2462, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Datase"
D19-1249,D16-1264,0,0.711321,"dataset. Introduction Machine reading comprehension is to evaluate how well computer systems understand natural language texts, where machines read a given text passage and answer questions about the passage. It has been regarded as a crucial technology for many applications such as question answering, dialogue systems (Nguyen et al., 2016; Chen et al., ∗ Corresponding author 2017; Liu et al., 2018; Wang et al., 2018) and so on. In order to enable machine to understand texts, large-scale reading comprehension datasets have been developed, such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), MS MACRO (Nguyen et al., 2016), hotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), etc. The majority of such datasets, unfortunately, are only for monolingual text understanding. To the best of our knowledge, there is no publicly available bilingual parallel reading comprehension dataset, which is exactly what BiPaR is mainly developed for, as illustrated in Figure 1. BiPaR 2452 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2452–2462, c Hong Kong, China, November 3–"
D19-1462,D16-1245,0,0.0314284,"ion, antecedent head resolution, and antecedent boundary detection. Co-reference resolution: Co-reference resolution is mainly concerned with two sub-tasks, referring expressions (i.e., mentions) detection, and entity candidate ranking. Uryupina and Moschitti (2013) propose a rule-based approach for coreference detection which employs parse tree features with an SVM model. Peng et al. (2015) improve the performance of mention detection by applying a binary classififier on their feature set. In recent years, applying deep neural networks to the co-reference resolution has gained great success. Clark and Manning (2016) apply reinforcement learning on mention-ranking co-reference resolution. Lee et al. (2017) introduce the first endto-end co-reference resolution model. Lee et al. (2018) present a high-order co-reference resolution model with coarse-to-fine inference. Ellipsis and co-reference resolution in QA and Dialogue: The methods mentioned above do not generalize well to dialogues because they normally require a large amount of well-annotated contextual data with syntactic norms and candidate antecedents. In recent years, a few studies try to solve ellipsis / co-reference resolution tailored for dialogu"
D19-1462,P16-1154,0,0.271776,"dialogue modeling. Unlike previous methods that combine detection and ranking models, our generation-based formulation is not constrained by the syntactic forms of ellipsis or co-reference in sentences. They can be either words (e.g., noun, verb) or phrases or even clauses. Furthermore, the formulation does not need to provide a set of candidate antecedents to be resolved. Previous studies usually need to Model Structure The GECOR model is shown in Figure 1. The model essentially contains an embedding module, a user utterance encoder, a dialogue context encoder and a decoder with either copy (Gu et al., 2016) or gated copy mechanism (modified from See et al. (2017)). Both the generation probability over the entire vocabulary and the copy probability over all words from the dialogue context are taken into account for predicting the complete user utterance. Embedding Layer In GECOR, we first tokenize the input user utterance and the dialogue context. We then use GloVe (Pennington et al., 2014) (the pre-trained 50-dimensional word vectors) in the embedding layer to obtain word embeddings. Let U = {u1 , ..., um }, C = {c1 , ..., cn } be representations of the tokenized utterance and context sequence."
D19-1462,K15-1002,0,0.0260799,"end-to-end computable system to perform English verb phrase ellipsis recovery on the original input text. Liu et al. (2016) propose to decompose the resolution of the verb phrase ellipsis into three sub-tasks: target detection, antecedent head resolution, and antecedent boundary detection. Co-reference resolution: Co-reference resolution is mainly concerned with two sub-tasks, referring expressions (i.e., mentions) detection, and entity candidate ranking. Uryupina and Moschitti (2013) propose a rule-based approach for coreference detection which employs parse tree features with an SVM model. Peng et al. (2015) improve the performance of mention detection by applying a binary classififier on their feature set. In recent years, applying deep neural networks to the co-reference resolution has gained great success. Clark and Manning (2016) apply reinforcement learning on mention-ranking co-reference resolution. Lee et al. (2017) introduce the first endto-end co-reference resolution model. Lee et al. (2018) present a high-order co-reference resolution model with coarse-to-fine inference. Ellipsis and co-reference resolution in QA and Dialogue: The methods mentioned above do not generalize well to dialog"
D19-1462,D14-1162,0,0.0819929,"us studies usually need to Model Structure The GECOR model is shown in Figure 1. The model essentially contains an embedding module, a user utterance encoder, a dialogue context encoder and a decoder with either copy (Gu et al., 2016) or gated copy mechanism (modified from See et al. (2017)). Both the generation probability over the entire vocabulary and the copy probability over all words from the dialogue context are taken into account for predicting the complete user utterance. Embedding Layer In GECOR, we first tokenize the input user utterance and the dialogue context. We then use GloVe (Pennington et al., 2014) (the pre-trained 50-dimensional word vectors) in the embedding layer to obtain word embeddings. Let U = {u1 , ..., um }, C = {c1 , ..., cn } be representations of the tokenized utterance and context sequence. Utterance and Context Encoder We use a single-layer bidirectional GRU to construct both encoders. The forward and backward hidden states over the input embeddings from the embedding layer are concatenated to form the hidden states of the two encoders. Decoder The decoder is a single-layer unidirectional GRU. In the decoder, the attention distribution at is calculated as in Bahdanau et al"
D19-1462,C16-1190,0,0.113933,"ement learning on mention-ranking co-reference resolution. Lee et al. (2017) introduce the first endto-end co-reference resolution model. Lee et al. (2018) present a high-order co-reference resolution model with coarse-to-fine inference. Ellipsis and co-reference resolution in QA and Dialogue: The methods mentioned above do not generalize well to dialogues because they normally require a large amount of well-annotated contextual data with syntactic norms and candidate antecedents. In recent years, a few studies try to solve ellipsis / co-reference resolution tailored for dialogue or QA tasks. Kumar and Joshi (2016) train a semantic sequence model to learn semantic patterns and a syntactic sequence model to learn linguistic patterns to tackle with the non-sentential (incomplete) questions in a question answering system. Zheng et al. (2018) builds a seq2seq neural network model for short texts to identify and recover ellipsis. However, these methods are still limited to short texts or one-shot dialogues. Our work is the first attempt to provide both solution and dataset for ellipsis and co-reference resolution in multi-turn dialogues. End-to-end task-oriented dialogue: Taskoriented dialogue systems have e"
D19-1462,D17-1018,0,0.0425292,"ference resolution is mainly concerned with two sub-tasks, referring expressions (i.e., mentions) detection, and entity candidate ranking. Uryupina and Moschitti (2013) propose a rule-based approach for coreference detection which employs parse tree features with an SVM model. Peng et al. (2015) improve the performance of mention detection by applying a binary classififier on their feature set. In recent years, applying deep neural networks to the co-reference resolution has gained great success. Clark and Manning (2016) apply reinforcement learning on mention-ranking co-reference resolution. Lee et al. (2017) introduce the first endto-end co-reference resolution model. Lee et al. (2018) present a high-order co-reference resolution model with coarse-to-fine inference. Ellipsis and co-reference resolution in QA and Dialogue: The methods mentioned above do not generalize well to dialogues because they normally require a large amount of well-annotated contextual data with syntactic norms and candidate antecedents. In recent years, a few studies try to solve ellipsis / co-reference resolution tailored for dialogue or QA tasks. Kumar and Joshi (2016) train a semantic sequence model to learn semantic pat"
D19-1462,N18-2108,0,0.0758611,"Missing"
D19-1462,P18-1133,0,0.082913,"framework, we manually construct a new dataset on the basis of the public dataset CamRest676 with both ellipsis and co-reference annotation. On this dataset, intrinsic evaluations on the resolution of ellipsis and co-reference show that the GECOR model significantly outperforms the sequenceto-sequence (seq2seq) baseline model in terms of EM, BLEU and F1 while extrinsic evaluations on the downstream dialogue task demonstrate that our multi-task learning framework with GECOR achieves a higher success rate of task completion than TSCP, a state-of-the-art end-to-end task-oriented dialogue model (Lei et al., 2018). 1 Introduction Due to the rhetorical principle of saving words and avoiding repetitions, ellipsis and co-reference occur frequently in multi-turn dialogues leaving utterances paragmatically incomplete if they are separate from context. Humans can easily understand utterances with anaphorically referenced or absent ∗ Work performed during an internship at Lenovo Research AI Lab. † Corresponding author information (e.g., Q2 and Q3 in Table 1) based on the dialogue context while dialogue systems often fail to understand such utterances correctly, which may result in false or incoherent response"
D19-1462,D16-1127,0,0.0365081,"dialogue system. The overall results demonstrate that the proposed multi-task learning framework for the end-to-end dialogue is able to improve the task completion rate by incorporating an auxiliary ellipsis/co-reference resolution task. Since the BSpan decoder is also used in the baseline system to capture contextual information and track dialogue states, we believe that our multi-task learning model with the integrated GECOR will play a more important role in endto-end dialgoue models that do not use state tracking modules, e.g., neural open-domain conversation models (Vinyals and Le, 2015; Li et al., 2016). improve the performance in terms of the exact match rate, BLEU and word-level F1 score. Experiments on the dialogue task demonstrate that the task completion rate of the task-oriented dialogue system is significantly improved with the aid of ellipsis and co-reference resolution. Our work could be extended to end-to-end open-domain multi-turn dialogue. We will further improve our model by incorporating syntactic and location information. We would also like to adapt the proposed methods to document-level neural machine translation in the future. 7 Mihail Eric and Christopher D Manning. 2017a."
D19-1462,W16-0705,0,0.0142549,"ar as we know is the PUNDIT system (Palmer et al., 1986) which discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit. Dalrymple et al. (1991) and Shieber et al. (1996) establish 1 The new dataset and the code of our proposed system are available at https://multinlp.github.io/ GECOR/ a set of linguistic theories in the ellipsis recovery of English verb phrases. Nielsen (2003) first proposes an end-to-end computable system to perform English verb phrase ellipsis recovery on the original input text. Liu et al. (2016) propose to decompose the resolution of the verb phrase ellipsis into three sub-tasks: target detection, antecedent head resolution, and antecedent boundary detection. Co-reference resolution: Co-reference resolution is mainly concerned with two sub-tasks, referring expressions (i.e., mentions) detection, and entity candidate ranking. Uryupina and Moschitti (2013) propose a rule-based approach for coreference detection which employs parse tree features with an SVM model. Peng et al. (2015) improve the performance of mention detection by applying a binary classififier on their feature set. In r"
D19-1462,P86-1004,0,0.582024,"ence resolution network with two phases of detection and candidate ranking. • To the best of our knowledge, this is the first attempt to combine the task of ellipsis and coreference resolution with the multi-turn taskoriented dialogue. The success rate of task completion is significantly improved with the assistance of the ellipsis and co-reference resolution. • We construct a new dataset based on CamRest676 for ellipsis and co-reference resolution in the context of task-oriented dialogue.1 2 Related Work Ellipsis recovery: The earliest work on ellipsis as far as we know is the PUNDIT system (Palmer et al., 1986) which discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit. Dalrymple et al. (1991) and Shieber et al. (1996) establish 1 The new dataset and the code of our proposed system are available at https://multinlp.github.io/ GECOR/ a set of linguistic theories in the ellipsis recovery of English verb phrases. Nielsen (2003) first proposes an end-to-end computable system to perform English verb phrase ellipsis recovery on the original input text. Liu et al. (2016) propose to decompose the resolution of"
D19-1462,P02-1040,0,0.104733,"Missing"
D19-1462,P17-1099,0,0.026974,"detection and ranking models, our generation-based formulation is not constrained by the syntactic forms of ellipsis or co-reference in sentences. They can be either words (e.g., noun, verb) or phrases or even clauses. Furthermore, the formulation does not need to provide a set of candidate antecedents to be resolved. Previous studies usually need to Model Structure The GECOR model is shown in Figure 1. The model essentially contains an embedding module, a user utterance encoder, a dialogue context encoder and a decoder with either copy (Gu et al., 2016) or gated copy mechanism (modified from See et al. (2017)). Both the generation probability over the entire vocabulary and the copy probability over all words from the dialogue context are taken into account for predicting the complete user utterance. Embedding Layer In GECOR, we first tokenize the input user utterance and the dialogue context. We then use GloVe (Pennington et al., 2014) (the pre-trained 50-dimensional word vectors) in the embedding layer to obtain word embeddings. Let U = {u1 , ..., um }, C = {c1 , ..., cn } be representations of the tokenized utterance and context sequence. Utterance and Context Encoder We use a single-layer bidir"
D19-1462,I13-1012,0,0.0309292,"/multinlp.github.io/ GECOR/ a set of linguistic theories in the ellipsis recovery of English verb phrases. Nielsen (2003) first proposes an end-to-end computable system to perform English verb phrase ellipsis recovery on the original input text. Liu et al. (2016) propose to decompose the resolution of the verb phrase ellipsis into three sub-tasks: target detection, antecedent head resolution, and antecedent boundary detection. Co-reference resolution: Co-reference resolution is mainly concerned with two sub-tasks, referring expressions (i.e., mentions) detection, and entity candidate ranking. Uryupina and Moschitti (2013) propose a rule-based approach for coreference detection which employs parse tree features with an SVM model. Peng et al. (2015) improve the performance of mention detection by applying a binary classififier on their feature set. In recent years, applying deep neural networks to the co-reference resolution has gained great success. Clark and Manning (2016) apply reinforcement learning on mention-ranking co-reference resolution. Lee et al. (2017) introduce the first endto-end co-reference resolution model. Lee et al. (2018) present a high-order co-reference resolution model with coarse-to-fine"
D19-1462,D16-1233,0,0.116042,"Missing"
D19-1614,W14-3348,0,0.0124599,"and coverage mechanism (See et al., 2017) were included while additional lexical features (POS, NER) were not. We used adam (Kingma and Ba, 2015) as the optimizer during training. The beam size was set to 20 for the decoder. In the experiments of the partial copy mechanism, we set the threshold γ to 0.7. Three values (0.5, 1 and 2) were tried for λ1 . In the experiments of the QA-based reranking, we used the SAN model (Liu et al., 2018) as the QA model. 0.2, 0.5 and 0.8 were tried for λ2 . We used automatic evaluation metrics: BLEU1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). The calculation script was provided by Du et al. (2017). 3.3 Results Experiment results are shown in Table 1, from which we observe that our methods substantially improves the baseline in terms of all evaluation metrics. The combination of the proposed two methods (λ1 = 1, λ2 = 0.2) achieve the best performance, gaining 0.73 BLEU-4 and nearly 1 METEOR point of improvements over the baseline (obtained by re-running the source code of the baseline, higher than the results reported in the paper (Song et al., 2018)). The proposed partial copy mechanism alone obtain substantial improvements over"
D19-1614,D17-1090,0,0.154445,"e been conducted on the basis of the widely-used seq2seq architecture together with the attention and copy mechanism. Song et al. (2018) propose two encoders for both the passage and the target answer. Du and Cardie (2018) employ coreferences as an additional feature. Kim et al. (2019) propose a model of answer separation. Yuan et al. (2017) and Kumar et al. (2018) adopt reinforcement learning to optimize the generation process. QA and QG are closely related to each other. Tang et al. (2017) treat QA and QG as dual tasks, and many other studies use QG to enhance QA or jointly learn QG and QA (Duan et al., 2017; Wang et al., 2017; Sachan and Xing, 2018). 5986 6 Conclusion In this paper, we have presented two methods to improve the relevance of generated questions to the given passage and target answer. Experiments and analyses on SQuAD show that both the partial copy mechanism and QA-based reranking improve the relevance of generated questions in terms of both BLEU and METEOR. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting objective function for neural conversation models. north american chapter of the association for computational linguistics, pag"
D19-1614,P16-1154,0,0.0673669,"based reranker to rerank QG results. Particularly, we use a neural QA model to evaluate the quality of generated questions, and rerank them according to the QA model scores. Normally, generic questions get low scores from the neural QA model, hence the reranker is able to select non-generic and highly relevant questions. While alleviating the low-relevance issue, the proposed two methods alone and their combination have also improved the quality of generated questions in terms of both BLEU and METEOR in our experiments. 2 2.1 Methods The Partial Copy Mechanism The conventional copy mechanism (Gu et al., 2016; See et al., 2017) can allow the decoder to copy a word from the input passage to the generated question. However, such copy mechanism works at the word level. It cannot copy a part of a word to reproduce an appropriate form of the 5983 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5983–5987, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics word in the generated question. In other words, it cannot copy words with morphological changes."
D19-1614,N10-1086,0,0.0835554,"QG model. Experiments and analyses demonstrate that the proposed two methods substantially improve the relevance of generated questions to passages and answers. 1 Introduction Question generation is to generate a valid and fluent question according to a given passage and the target answer. In general, the answer is a span of words in the passage. QG can be used in many scenarios, such as automatical tutoring systems, improving the performance of QA models and enabling chatbots to lead a conversation. In the early days, QG has been tackled mainly via rule-based approaches (Mitkov and Ha, 2003; Heilman and Smith, 2010). These methods rely on many handcrafted rules and templates. Constructing such rules is time-consuming and it is difficult to adapt them to other domains. Very recently, neural networks have been used for QG. Specifically, the encoder-decoder seq2seq model (Du et al., 2017; Zhou et al., 2017; Song et al., 2018) is used to encode a passage and generate a question corresponding to the answer. Such end-to-end neural models are able to generate better questions than traditional rule-based approaches. However, one issue with the current ∗ Corresponding author neural models is that the generated qu"
D19-1614,P02-1040,0,0.105133,"ained the model for 10 epochs. Copy and coverage mechanism (See et al., 2017) were included while additional lexical features (POS, NER) were not. We used adam (Kingma and Ba, 2015) as the optimizer during training. The beam size was set to 20 for the decoder. In the experiments of the partial copy mechanism, we set the threshold γ to 0.7. Three values (0.5, 1 and 2) were tried for λ1 . In the experiments of the QA-based reranking, we used the SAN model (Liu et al., 2018) as the QA model. 0.2, 0.5 and 0.8 were tried for λ2 . We used automatic evaluation metrics: BLEU1, BLEU-2, BLEU-3, BLEU-4 (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). The calculation script was provided by Du et al. (2017). 3.3 Results Experiment results are shown in Table 1, from which we observe that our methods substantially improves the baseline in terms of all evaluation metrics. The combination of the proposed two methods (λ1 = 1, λ2 = 0.2) achieve the best performance, gaining 0.73 BLEU-4 and nearly 1 METEOR point of improvements over the baseline (obtained by re-running the source code of the baseline, higher than the results reported in the paper (Song et al., 2018)). The proposed partial copy mechanism alon"
D19-1614,D14-1162,0,0.0820452,"Settings Our baseline is based on the QG model proposed by Song et al. (2018). To be specific, it is a seq2seq model with attention and copy mechanism. The model consists of two encoders and a decoder. The two encoders encode a passage = (p1 ,...,pM ) and an answer = (a1 ,...,aN ) respectively. Additionally, multi-perspective matching strategies are used to combine the two encoders. With information from the encoders, the decoder generates a question word by word. We retained the same values for most hyperparameters in our experiments as the baseline system (Song et al., 2018). We used Glove (Pennington et al., 2014) to initialize the word embeddings and trained the model for 10 epochs. Copy and coverage mechanism (See et al., 2017) were included while additional lexical features (POS, NER) were not. We used adam (Kingma and Ba, 2015) as the optimizer during training. The beam size was set to 20 for the decoder. In the experiments of the partial copy mechanism, we set the threshold γ to 0.7. Three values (0.5, 1 and 2) were tried for λ1 . In the experiments of the QA-based reranking, we used the SAN model (Liu et al., 2018) as the QA model. 0.2, 0.5 and 0.8 were tried for λ2 . We used automatic evaluation"
D19-1614,N18-1058,0,0.015696,"idely-used seq2seq architecture together with the attention and copy mechanism. Song et al. (2018) propose two encoders for both the passage and the target answer. Du and Cardie (2018) employ coreferences as an additional feature. Kim et al. (2019) propose a model of answer separation. Yuan et al. (2017) and Kumar et al. (2018) adopt reinforcement learning to optimize the generation process. QA and QG are closely related to each other. Tang et al. (2017) treat QA and QG as dual tasks, and many other studies use QG to enhance QA or jointly learn QG and QA (Duan et al., 2017; Wang et al., 2017; Sachan and Xing, 2018). 5986 6 Conclusion In this paper, we have presented two methods to improve the relevance of generated questions to the given passage and target answer. Experiments and analyses on SQuAD show that both the partial copy mechanism and QA-based reranking improve the relevance of generated questions in terms of both BLEU and METEOR. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting objective function for neural conversation models. north american chapter of the association for computational linguistics, pages 110–119. Xiaodong Liu, Yelong Shen, Kevi"
D19-1614,N18-2090,0,0.695805,"span of words in the passage. QG can be used in many scenarios, such as automatical tutoring systems, improving the performance of QA models and enabling chatbots to lead a conversation. In the early days, QG has been tackled mainly via rule-based approaches (Mitkov and Ha, 2003; Heilman and Smith, 2010). These methods rely on many handcrafted rules and templates. Constructing such rules is time-consuming and it is difficult to adapt them to other domains. Very recently, neural networks have been used for QG. Specifically, the encoder-decoder seq2seq model (Du et al., 2017; Zhou et al., 2017; Song et al., 2018) is used to encode a passage and generate a question corresponding to the answer. Such end-to-end neural models are able to generate better questions than traditional rule-based approaches. However, one issue with the current ∗ Corresponding author neural models is that the generated questions are not quite relevant to the corresponding passages and target answers. In other words, the neural QG models tend to generate generic questions (e.g., “what is the name of...?”). In this paper, we propose two methods to deal with this low-relevance issue for QG. First, we present a partial copy method t"
I05-1007,P96-1051,0,0.00907816,"f information available in treebank. 1 Introduction In the recent development of full parsing technology, semantic knowledge is seldom used, though it is known to be useful for resolving syntactic ambiguities. The reasons for this may be twofold. The ﬁrst one is that it can be very diﬃcult to add additional features which are not available in treebanks to generative models like Collins (see [1]), which are very popular for full parsing. For smaller tasks, like prepositional phrase attachment disambiguation, semantic knowledge can be incorporated ﬂexibly using diﬀerent learning algorithms (see [2,3,4,5]). For full parsing with generative models, however, incorporating semantic knowledge may involve great changes of model structures. The second reason is that semantic knowledge from external dictionaries seems to be noisy, ambiguous and not available in explicit forms, compared with the information from treebanks. Given these two reasons, it seems to be diﬃcult to combine the two diﬀerent information sources–treebank and semantic knowledge–into one integrated statistical parsing model. One feasible way to solve this problem is to keep the original parsing model unchanged and build an addition"
I05-1007,W98-0717,0,0.024978,"f information available in treebank. 1 Introduction In the recent development of full parsing technology, semantic knowledge is seldom used, though it is known to be useful for resolving syntactic ambiguities. The reasons for this may be twofold. The ﬁrst one is that it can be very diﬃcult to add additional features which are not available in treebanks to generative models like Collins (see [1]), which are very popular for full parsing. For smaller tasks, like prepositional phrase attachment disambiguation, semantic knowledge can be incorporated ﬂexibly using diﬀerent learning algorithms (see [2,3,4,5]). For full parsing with generative models, however, incorporating semantic knowledge may involve great changes of model structures. The second reason is that semantic knowledge from external dictionaries seems to be noisy, ambiguous and not available in explicit forms, compared with the information from treebanks. Given these two reasons, it seems to be diﬃcult to combine the two diﬀerent information sources–treebank and semantic knowledge–into one integrated statistical parsing model. One feasible way to solve this problem is to keep the original parsing model unchanged and build an addition"
I05-1007,W04-2410,0,0.0115748,"f information available in treebank. 1 Introduction In the recent development of full parsing technology, semantic knowledge is seldom used, though it is known to be useful for resolving syntactic ambiguities. The reasons for this may be twofold. The ﬁrst one is that it can be very diﬃcult to add additional features which are not available in treebanks to generative models like Collins (see [1]), which are very popular for full parsing. For smaller tasks, like prepositional phrase attachment disambiguation, semantic knowledge can be incorporated ﬂexibly using diﬀerent learning algorithms (see [2,3,4,5]). For full parsing with generative models, however, incorporating semantic knowledge may involve great changes of model structures. The second reason is that semantic knowledge from external dictionaries seems to be noisy, ambiguous and not available in explicit forms, compared with the information from treebanks. Given these two reasons, it seems to be diﬃcult to combine the two diﬀerent information sources–treebank and semantic knowledge–into one integrated statistical parsing model. One feasible way to solve this problem is to keep the original parsing model unchanged and build an addition"
I05-1007,P03-1054,0,0.0932181,"dependencies of internal structures of NCs due to the sparseness of bilexical dependencies. In our new parser, however, the selection preference model is able to build semantically preferable structures through word-class dependency statistics. For NCs like (n1 , n2 , n3 ), where ni is a noun, dependency structures 7 8 9 “Every way ambiguous” constructions are those for which the number of analyses is the number of binary trees over the terminal elements. Prepositional phrase attachment, coordination, and nominal compounds are all ”every way ambiguous” constructions. Just as Klein et al. (see [8]) said, one million words of training data just isn’t enough. Henceforth, [s1 , s2 ] denotes a dependency structure, where s1 is a modiﬁer word or its semantic class (C), and s2 is the head word. 78 a. D. Xiong et al. NPB  H H NR NN NN Ӕ ᆟ کห b. NP HH NP  H H NPB NPB NPB NN NR NN ห Ӕ ᆟک Fig. 1. Nominal Compounds: The North Korean government’s special envoy. a. is the incorrect ﬂat parse, b. is the right one in corpus {[Cn1 , n2 ], [Cn1 , n3 ], [Cn2 , n3 ]} will be checked in terms of semantic acceptability and semantically preferable structures will be built ﬁnally. For more comp"
I05-1007,W01-0521,0,0.011109,"Missing"
I05-1007,P03-1056,0,0.456973,"wo Chinese electronic dictionaries and their combination as our semantic information sources. Several experiments are carried out on the Penn Chinese Treebank to test our hypotheses. The results indicate that a signiﬁcant improvement in performance is achieved when semantic knowledge is incorporated into parsing model. Further improvement analysis is made. We conﬁrm that semantic knowledge is indeed useful for nominal compounds and coordination ambiguity resolution. And surprisingly, semantic knowledge is also helpful to correct Chinese NV mistagging errors mentioned by Levy and Manning (see [12]). Yet another great beneﬁt to incorporating semantic knowledge is to alleviate the sparseness of information available in treebank. 2 The Baseline Parser Our baseline parsing model is similar to the history-based, generative and lexicalized Model 1 of Collins (see [1]). In this model, the right hand side of lexicalized rules is decomposed into smaller linguistic objects as follows: P (h) → #Ln (ln )...L1 (l1 )H(h)R1 (r1 )...Rm (rm )# . The uppercase letters are delexicalized nonterminals, while the lowercase letters are lexical items, e.g. head word and head tag (part-of-speech tag of the hea"
I05-1007,W00-1201,0,0.0572772,"semantic knowledge indeed helps alleviate the fundamental sparseness of the lexical dependency information available in the CTB. For many word pairs [mod,head], whose count information is not available in the training data, the dependency statistics of head and modiﬁer can still work through the semantic category of mod. During our manual analysis of performance improvement, many other structural ambiguities are addressed due to the smoothing function of semantic knowledge. 4 Related Work on CTB Parsing Previous work on CTB parsing and their results are shown in table 5. Bikel and Chiang (see [14]) used two diﬀerent models on CTB, one based on the modiﬁed BBN model which is very similar to our baseline model, the other on Tree Insertion Grammar (TIG). While our baseline model used the same unknown word threshold with Bikel and Chiang but smaller beam width, our result outperforms theirs due to other features like distance, basic NP re-annotation used by our baseline model. Levy and Manning (see [12]) used a factored model with rich re-annotations guided by error analysis. In the baseline model, we also used several re-annotations but ﬁnd most re-annotations they suggested do not ﬁt 80"
I05-1007,C02-1126,0,0.118335,"Missing"
I05-1007,J04-4004,0,\N,Missing
I05-1007,J03-4003,0,\N,Missing
I08-1066,P06-1067,0,0.122811,"Missing"
I08-1066,H05-1098,0,0.184236,"h space using swapping window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a st"
I08-1066,J07-2003,0,0.0945551,"exity of BTG decoding with m-gram language model is O(n3+4(m−1) ). If a 4-gram language model is used (common in many current SMT systems), the time complexity is as high as O(n15 ). Therefore with this time complexity translating long sentences is time-consuming even with highly stringent pruning strategy. To speed up BTG decoding, Huang et al. (2005) adapted the hook trick which changes the time complexity from O(n3+4(m−1) ) to O(n3+3(m−1) ). However, the implementation of the hook trick with pruning is quite complicated. Another method to increase decoding speed is cube pruning proposed by Chiang (2007) which reduces search space significantly. In this paper, we propose two refinements to address the two issues, including (1) reordering heuristics to prevent incorrect swapping and reduce search space using swapping window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT a"
I08-1066,W00-2010,0,0.0338613,"swapping window and punctuation restriction. Swapping Window (SW): It constrains block swapping in the following way ACTIVATE A → hA1 , A2 i IF |A1s |+ |A2s |&lt; sws where |Ais |denotes the number of words on the source side Ais of block Ai , sws is a pre-defined swapping window size. Any inverted reordering beyond the pre-defined swapping window size is prohibited. Punctuation Restriction (PR): If two neighboring blocks include any of the punctuation marks p ∈ {， 、 ： ； 「 」 《 》 （ ） “ ”}, the two blocks will be merged with straight order. Punctuation marks were already used in parsing (Christine Doran, 2000) and statistical machine translation (Och et al., 2003). In (Och et al., 2003), three kinds of features are defined, all related to punctuation marks like quotes, parentheses and commas. Unfortunately, no statistically significant improvement on the BLEU score was reported in (Och et al., 2003). In this paper, we consider this problem from a different perspective. We emphasize that words around punctuation marks are reordered ungrammatically and therefore we positively use punctuation marks as a hard decision to restrict such reordering around punctuations. This is straightforward but yet resu"
I08-1066,W05-1507,0,0.0156052,"of bilingual phrases as features to predict their orders. Xiong et al. (2006) reported significant performance improvement on Chinese-English translation tasks in two different domains when compared with both Pharaoh (Koehn, 2004) and the original BTG using flat reordering. However, error analysis of the translation output of Xiong et al. (2006) reveals that boundary words predict wrong swapping, especially for long phrases although the MaxEnt-based reordering model shows better performance than baseline reordering models. Another big problem with BTG-based SMT is the high computational cost. Huang et al. (2005) reported that the time complexity of BTG decoding with m-gram language model is O(n3+4(m−1) ). If a 4-gram language model is used (common in many current SMT systems), the time complexity is as high as O(n15 ). Therefore with this time complexity translating long sentences is time-consuming even with highly stringent pruning strategy. To speed up BTG decoding, Huang et al. (2005) adapted the hook trick which changes the time complexity from O(n3+4(m−1) ) to O(n3+3(m−1) ). However, the implementation of the hook trick with pruning is quite complicated. Another method to increase decoding speed"
I08-1066,koen-2004-pharaoh,0,0.044683,"ss, BTG restriction is widely used for reordering in SMT (Zens et al., 2004). However, BTG restriction does not provide a mechanism to predict final orders between two neighboring blocks. 505 {htmi, liuqun, sxlin}@ict.ac.cn To solve this problem, Xiong et al. (2006) proposed an enhanced BTG with a maximum entropy (MaxEnt) based reordering model (MEBTG). MEBTG uses boundary words of bilingual phrases as features to predict their orders. Xiong et al. (2006) reported significant performance improvement on Chinese-English translation tasks in two different domains when compared with both Pharaoh (Koehn, 2004) and the original BTG using flat reordering. However, error analysis of the translation output of Xiong et al. (2006) reveals that boundary words predict wrong swapping, especially for long phrases although the MaxEnt-based reordering model shows better performance than baseline reordering models. Another big problem with BTG-based SMT is the high computational cost. Huang et al. (2005) reported that the time complexity of BTG decoding with m-gram language model is O(n3+4(m−1) ). If a 4-gram language model is used (common in many current SMT systems), the time complexity is as high as O(n15 )."
I08-1066,P07-2045,0,0.00543539,"prevent incorrect swapping and reduce search space using swapping window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has"
I08-1066,P06-1077,1,0.851772,"tion restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose rules are weighted"
I08-1066,W02-2018,0,0.00938526,"hrase penalty and word penalty, respectively and λs are weights of features. These features are commonly used in the state-of-the-art systems (Koehn et al., 2005; Chiang et al., 2005). 2.2 MaxEnt-based Reordering Model The MaxEnt-based reordering model is defined on two consecutive blocks A1 and A2 together with their order o ∈ {straight, inverted} according to the maximum entropy framework. P exp( i θi hi (o, A1 , A2 )) P Ω = pθ (o|A , A ) = P 1 2 o exp( i θi hi (o, A , A )) (6) where the functions hi ∈ {0, 1} are model features and θi are weights of the model features trained automatically (Malouf, 2002). There are three steps to train a MaxEnt-based reordering model. First, we need to extract reordering examples from unannotated bilingual data, then generate features from these examples and finally estimate feature weights. 1 2 For extracting reordering examples, there are two points worth mentioning: 1. In the extraction of useful reordering examples, there is no length limitation over blocks compared with extracting bilingual phrases. 2. When enumerating all combinations of neighboring blocks, a good way to keep the number of reordering examples acceptable is to extract smallest blocks wit"
I08-1066,W06-1606,0,0.050471,"g window and punctuation restriction, and (2) phrases with special tags to indicate beginning and ending of sentence. Experimental results show that both refinements improve the BLEU score significantly on large-scale data. The above refinements can be easily implemented and integrated into a baseline BTG-based SMT system. However, they are not specially designed for BTG-based SMT and can also be easily integrated into other systems with different underlying translation strategies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose"
I08-1066,P02-1038,0,0.0523918,"ining data, (3) a CKY-style decoder using beam search similar to that of Wu (1996). We describe the first two components briefly below. 2.1 Model The translation process is modeled using BTG rules which are listed as follows A → [A1 , A2 ] (1) A → hA1 , A2 i (2) A → x/y (3) The lexical rule (3) is used to translate source phrase x into target phrase y and generate a block A. The 506 two rules (1) and (2) are used to merge two consecutive blocks into a single larger block in a straight or inverted order. To construct a stochastic BTG, we calculate rule probabilities using the log-linear model (Och and Ney, 2002). For the two merging rules (1) and (2), the assigned probability P rm (A) is defined as follows LM P rm (A) = ΩλΩ · 4λpLM (A1 ,A2 ) (4) where Ω, the reordering score of block A1 and A2 , is calculated using the MaxEnt-based reordering model (Xiong et al., 2006) described in the next section, λΩ is the weight of Ω, and 4pLM (A1 ,A2 ) is the increment of language model score of the two blocks according to their final order, λLM is its weight. For the lexical rule (3), it is applied with a probability P rl (A) P rl (A) = p(x|y)λ1 · p(y|x)λ2 · plex (x|y)λ3 ·plex (y|x)λ4 · exp(1)λ5 · exp(|y|)λ6 LM"
I08-1066,P96-1021,0,0.151694,"valuation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose rules are weighted using different features in log-linear form, (2) a MaxEnt-based reordering model with features automatically learned from bilingual training data, (3) a CKY-style decoder using beam search similar to that of Wu (1996). We describe the first two components briefly below. 2.1 Model The translation process is modeled using BTG rules which are listed as follows A → [A1 , A2 ] (1) A → hA1 , A2 i (2) A → x/y (3) The lexical rule (3) is used to translate source phrase x into target phrase y and generate a block A. The 506 two rules (1) and (2) are used to merge two consecutive blocks into a single larger block in a straight or inverted order. To construct a stochastic BTG, we calculate rule probabilities using the log-linear model (Och and Ney, 2002). For the two merging rules (1) and (2), the assigned probabilit"
I08-1066,P06-1066,1,0.959397,"tegies, such as the state-of-the-art phrasebased system (Koehn et al., 2007), syntax-based systems (Chiang et al., 2005; Marcu et al., 2006; Liu et al., 2006). The rest of the paper is organized as follows. In section 2, we review briefly the core elements of the baseline system. In section 3 we describe our proposed refinements in detail. Section 4 presents the evaluation results on Chinese-to-English translation based on these refinements as well as results obtained in the NIST MT-06 evaluation exercise. Finally, we conclude our work in section 5. 2 The Baseline System In this paper, we use Xiong et al. (2006)’s system Bruin as our baseline system. Their system has three essential elements which are (1) a stochastic BTG, whose rules are weighted using different features in log-linear form, (2) a MaxEnt-based reordering model with features automatically learned from bilingual training data, (3) a CKY-style decoder using beam search similar to that of Wu (1996). We describe the first two components briefly below. 2.1 Model The translation process is modeled using BTG rules which are listed as follows A → [A1 , A2 ] (1) A → hA1 , A2 i (2) A → x/y (3) The lexical rule (3) is used to translate source ph"
I08-1066,C04-1030,0,0.374843,"Missing"
I08-1066,zhang-etal-2004-interpreting,0,0.0650612,"Missing"
I08-1066,2005.iwslt-1.8,0,\N,Missing
J10-3009,P06-1067,0,0.0764773,"ith Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the ﬂat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more ﬂexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous g"
J10-3009,1992.tmi-1.8,0,0.355669,"he test corpus when we consider the gap in syntactic reordering patterns. BWR (gap) BWR+LAR (gap) 562 Precision Recall F1 46.28 48.80 44.91 50 45.58 49.39 Xiong et al. Linguistically Annotated Reordering approach, reordering knowledge is included in synchronous rules. The last two categories reorder the source sentence during decoding, which distinguishes them from the ﬁrst approach. Note that some researchers integrate multiple reordering approaches in one decoder (Lin 2004; Quirk, Menezes, and Cherry 2005; Ge, Ittycheriah, and Papineni 2008). 9.1.1 The Preprocessing Approach. In early work, Brown et al. (1992) describe an approach to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a preprocessing approach which automatically learns reordering patterns based on CFG productions. Since then, the preprocessing approach seems to have been more popular. Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees using ﬁne-grained human-written rules, mostly concentrating on VP and NP structures. Li et al. (2007) improve the preprocessing approach by generating"
J10-3009,W07-0718,0,0.0483372,"em outputs in a ﬁne-grained manner with regard to reordering. In their method, common word n-grams occurring in both reference translations and system translations are extracted and generalized to part-ofspeech tag sequences. A recall is calculated for each certain tag sequence to indicate the ability of reordering models to capture this tag sequence in system translations. Popovic et al. (2006) use the relative difference between WER (word error rate) and PER (position independent word error rate) to indicate reordering errors. The larger the difference, the more reordering errors there are. Callison-Burch et al. (2007) propose a constituent-based evaluation that is very similar to our method in Steps (1)–(3). They also parse the source sentence and automatically align the parse tree with the reference/system translations. The difference is that they highlight constituents from the parse tree to enable human evaluation of the translations of these constituents, rather than automatically analyzing constituent movement. They use this method for human evaluation in the shared translation task of the 2007 and 2008 ACL Workshop on Statistical Machine Translation. Fox (2002) systematically studies syntactic cohesi"
J10-3009,P08-1009,0,0.059842,"rst combined with d d d d, which is a sub-phrase of PP preceding VP in an inverted order. The remaining part of the VP phrase is then merged. This merging process continues regardless of the source parse tree. The comparison of BTG trees of BWR+LAR and BWR in the two examples suggests that reordering models should respect syntactic structures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita"
J10-3009,P05-1033,0,0.753435,"tricted by the ITG constraint (Wu 1997). Although it only allows two orders (straight or inverted) of nodes in any binary branching structure, it is broadly veriﬁed that the ITG constraint has good coverage of word reorderings on various language pairs (Wu, Carpuat, and Shen 2006). This makes phrase reordering in phrasal SMT a more tractable task. After enhancing phrasal SMT with a hard hierarchical skeleton, we further inject soft linguistic information into the nodes of the skeleton. We annotate each BTG node 1 In this article, we use Penn Chinese Treebank phrase labels (Xue et al. 2000). 2 Chiang (2005) also generates hierarchical structures in phrasal SMT. One difference is that Chiang’s hierarchical grammar is lexicon-sensitive because the model requires at least one pair of aligned words in each rule except for the “glue rule.” The other difference is that his grammar allows multiple nonterminals. These two differences make Chiang’s grammar more expressive than the BTG but at the cost of learning a larger model. 536 Xiong et al. Linguistically Annotated Reordering with syntactic and lexical elements by projecting the source parse tree onto the BTG binary tree. The challenge, of course, is"
J10-3009,H05-1098,0,0.0193236,"cluded in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax-based translation rules. 9.2 Automatic Analysis of Reordering Although there is a variety of work on phrase reordering, automatic analysis of phrase reordering is not widely explored in the SMT literature. Chiang et al. (2005) propose 563 Computational Linguistics Volume 36, Number 3 an automatic method to compare different system outputs in a ﬁne-grained manner with regard to reordering. In their method, common word n-grams occurring in both reference translations and system translations are extracted and generalized to part-ofspeech tag sequences. A recall is calculated for each certain tag sequence to indicate the ability of reordering models to capture this tag sequence in system translations. Popovic et al. (2006) use the relative difference between WER (word error rate) and PER (position independent word erro"
J10-3009,P05-1066,0,0.0849743,"Missing"
J10-3009,P03-2041,0,0.0375795,"; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more ﬂexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax"
J10-3009,W02-1039,0,0.156118,"rdering when combined with BWR. We want to further study what happens when we combine BWR with LAR. In particular, we want to investigate to what extent the integrated linguistic knowledge (from LAR) changes phrase movement in an actual SMT system, and in what direction the change takes place. The investigations will enable us to have a better understanding of the relationship between phrase movement and linguistic context, and therefore to explore linguistic knowledge more effectively in phrasal SMT. Because syntactic constituents are often moved together across languages during translation (Fox 2002), we particularly study how linguistic knowledge affects syntactic constituent movement. To that end, we introduce a syntax-based analysis method. We parse source sentences, and align the parse trees with reference translations as well as system translations. We then summarize syntactic reordering patterns using contextfree grammar (CFG) rules from the obtained tree-to-string alignments. The extracted reordering patterns clearly show the trace of syntactic constituent movement in both reference translations and system translations. With the proposed analysis method, we analyze the combination"
J10-3009,N04-1035,0,0.0231289,"ctures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a hard constraint on the ITG constraint to allow reorderings which respect the source parse tree. They all report signiﬁcant improvements on different language pairs, which indicates that syntactic cohesion is very useful for phrasal SMT. Our analysis demonstrates that linguistically annotated reordering provides an alte"
J10-3009,W08-0408,0,0.0311117,"Missing"
J10-3009,2006.amta-papers.8,0,0.0332198,"Missing"
J10-3009,W04-3250,0,0.161864,"Missing"
J10-3009,2005.iwslt-1.8,0,0.263134,"submission received: 12 March 2010; accepted for publication: 21 April 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 local word reorderings within phrases. Unfortunately, reordering at the phrase level is still problematic for phrasal SMT. The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases. In order to utilize lexical information for phrase reordering, Tillman (2004) and Koehn et al. (2005) propose lexicalized reordering models which directly condition phrase movement on phrases themselves. One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data. To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering evidence. Although these lexicalized reordering models signiﬁcantly outperform the distortion-based reordering model as reported, only using lexical information (e.g., boundary words) is"
J10-3009,N03-1017,0,0.0454533,"Missing"
J10-3009,H05-1021,0,0.0226242,"ting Phrase Movement with Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the ﬂat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more ﬂexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and"
J10-3009,P07-1091,0,0.066262,"precision/recall for this structure is c/b and c/a, respectively. We can further calculate the F1 -score as 2 × c/(a + b). These syntax-based metrics intuitively show how well the reordering model can reorder this structure. By summarizing all reordering patterns of all constituents, we can obtain an overall precision, recall, and F1 -score for the tested reordering model. This new syntax-based analysis for reordering is motivated in part by recent work which transforms the order of nodes in the source-side parse tree before translation (Xia and McCord 2004; Collins, Koehn, and Kucerova 2005; Li et al. 2007; Wang, Collins, and Koehn 2007). Here we focus on the order transformation of syntactic constituents performed by reordering models during translation. In addition to aligning parse trees with reference translations, we also align parse trees with system translations so that we can learn the movement of syntactic constituents carried out by the reordering models and investigate the performance of the reordering models by comparing both alignments. For notational convenience, we denote syntactic reordering patterns that are extracted from the alignments between source parse trees and reference"
J10-3009,C04-1090,0,0.0184945,"ities of movement with linguistic information. In the third Table 12 Revised overall precision and recall of BWR+LAR vs. BWR on the test corpus when we consider the gap in syntactic reordering patterns. BWR (gap) BWR+LAR (gap) 562 Precision Recall F1 46.28 48.80 44.91 50 45.58 49.39 Xiong et al. Linguistically Annotated Reordering approach, reordering knowledge is included in synchronous rules. The last two categories reorder the source sentence during decoding, which distinguishes them from the ﬁrst approach. Note that some researchers integrate multiple reordering approaches in one decoder (Lin 2004; Quirk, Menezes, and Cherry 2005; Ge, Ittycheriah, and Papineni 2008). 9.1.1 The Preprocessing Approach. In early work, Brown et al. (1992) describe an approach to reordering French phrases in a preprocessing step. Xia and McCord (2004) present a preprocessing approach which automatically learns reordering patterns based on CFG productions. Since then, the preprocessing approach seems to have been more popular. Collins, Koehn, and Kucerova (2005) propose reordering German clauses with six types of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees using ﬁne-g"
J10-3009,P06-1077,0,0.0481188,"Missing"
J10-3009,W06-1606,0,0.143376,"n ﬂuent translations for these constituents. However, the allowance of interruptions is sometimes beyond the representability of BTG rules. For example, to solve the lexical divergence problem, bilingual rules with aligned lexicons have to be introduced. To capture reorderings of these constituents, we propose to integrate special reordering rules with richer contextual information into BTG to extend BTG’s ability to deal with interruptions. Completely replacing BTG with richer formalisms, such as hierarchical phrase (Chiang 2005) and tree-to-string (Liu, Liu, and Lin 2006) or string-to-tree (Marcu et al. 2006), introduces a huge extra cost. Instead, integrating a small number of reordering rules into BTG to model reorderings of non-reorderable constituents would be more desirable. 8.5 Discussion In the deﬁnition of syntactic reordering patterns, we only consider the relative order of individual constituents on the target side. We do not consider whether or not they remain contiguous on the target side. It is possible that other words are inserted between spans of two contiguous constituents. We use the term gap to refer to when this happens. The absence of a gap in the deﬁnition of syntactic reorde"
J10-3009,P08-1114,0,0.0620184,"with d d d d, which is a sub-phrase of PP preceding VP in an inverted order. The remaining part of the VP phrase is then merged. This merging process continues regardless of the source parse tree. The comparison of BTG trees of BWR+LAR and BWR in the two examples suggests that reordering models should respect syntactic structures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a"
J10-3009,2001.mtsummit-papers.45,0,0.0347596,"lauses with six types of manual rules. Similarly, Wang, Collins, and Koehn (2007) reorder Chinese parse trees using ﬁne-grained human-written rules, mostly concentrating on VP and NP structures. Li et al. (2007) improve the preprocessing approach by generating n-best reordered source sentences with reordering knowledge automatically learned from the alignments between source parse trees and target translations. The approach proposed in Li et al. also enhances the connection between the preprocessing and decoding by adding a source reordering probability feature. Other approaches introduced in Nießn and Ney (2001), Popovi´c and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological, POS, and chunk knowledge in the preprocessing approach, respectively. 9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the ﬂat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved."
J10-3009,P03-1021,0,0.0120847,"Missing"
J10-3009,P00-1056,0,0.0905788,"TG-based phrasal SMT system, developed following Section 2. We integrate the boundary word–based reordering model and the linguistically annotated reordering model into our system according to our reordering conﬁguration. We carried out various experiments to evaluate the reordering example extraction algorithms of Section 3, the linguistically annotated reordering model vs. boundary word–based reordering model, and the effects of linguistically annotated features on the Chinese-toEnglish translation task of the NIST MT-05 using large scale training data. 7.1 Experimental Setup We ran GIZA++ (Och and Ney 2000) on the parallel corpora (consisting of 101.93M Chinese words and 112.78M English words) listed in Table 2 in both directions and then applied the “grow-diag-ﬁnal” reﬁnement rule (Koehn, Och, and Marcu 2003) to 550 Xiong et al. Linguistically Annotated Reordering Table 2 Corpora used. Corpus LDC catalog Chinese words English words United Nations Hong Kong News Sinorama Magazine FBIS Xinhua Chinese News Translation Chinese Treebank Multiple Translation Chinese LDC2004E12 LDC2004T08 LDC2005T10 LDC2003E14 LDC2002E18 LDC2005T06 LDC2003E07 LDC2004T07 68.63M 15.07M 10.26M 7.09M 0.40M 0.28M 0.10M 0.1"
J10-3009,P02-1040,0,0.0831864,"ITG constraint. We call this two-step phrase reordering strategy linguistically annotated reordering (LAR) (Xiong et al. 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reordering strategy based on BTG. However, they use boundary words as reordering features at the second step. To distinguish this from our work, we call their approach boundary word–based reordering (BWR). LAR and BWR can be considered as two reordering variants for BTG-based phrasal SMT, which have similar training procedures. Furthermore, they can be combined. We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al. 2002). The BLEU scores show that LAR is comparable to BWR and signiﬁcantly improves phrase reordering when combined with BWR. We want to further study what happens when we combine BWR with LAR. In particular, we want to investigate to what extent the integrated linguistic knowledge (from LAR) changes phrase movement in an actual SMT system, and in what direction the change takes place. The investigations will enable us to have a better understanding of the relationship between phrase movement and linguistic context, and therefore to explore linguistic knowledge more effectively in phrasal SMT. Beca"
J10-3009,W06-3101,0,0.344324,"Missing"
J10-3009,popovic-ney-2006-pos,0,0.273344,"The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases. In order to utilize lexical information for phrase reordering, Tillman (2004) and Koehn et al. (2005) propose lexicalized reordering models which directly condition phrase movement on phrases themselves. One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data. To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering evidence. Although these lexicalized reordering models signiﬁcantly outperform the distortion-based reordering model as reported, only using lexical information (e.g., boundary words) is not adequate to move phrases to appropriate positions. Consider the following Chinese example with its English translation: [VP [PP d(while) dd(develop) dd(related) dd(legislation) d] [VP [VV d d(consider)] [NP [DNP [NP dd(this) dddd(referendum)] [DEG d(of)]] [NP d d(results)]]]]1 consider th"
J10-3009,P05-1034,0,0.0719418,"Missing"
J10-3009,N04-4026,0,0.654536,"eived: 24 October 2008; revised submission received: 12 March 2010; accepted for publication: 21 April 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 local word reorderings within phrases. Unfortunately, reordering at the phrase level is still problematic for phrasal SMT. The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases. In order to utilize lexical information for phrase reordering, Tillman (2004) and Koehn et al. (2005) propose lexicalized reordering models which directly condition phrase movement on phrases themselves. One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data. To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering evidence. Although these lexicalized reordering models signiﬁcantly outperform the distortion-based reordering model as reported, only using lexical information ("
J10-3009,D07-1077,0,0.0334249,"Missing"
J10-3009,P96-1021,0,0.353417,"important and challenging tasks in building a BTG-based phrasal SMT system is to deﬁne P(r m ). 2.2 Reordering Under the ITG Constraint Under the ITG constraint, three nodes {Al , Ar , Ap } are involved when we consider the order o between the two children {Al , Ar } in any binary subtrees. Therefore it is natural to deﬁne the ITG reordering P(r m ) as a function as follows: P(rm ) = f (Al , Ar , Ap , o) (5) where o ∈ {straight, inverted}. Based on this function, various reordering models are built according to different assumptions. For example, the ﬂat reordering model in the original BTG (Wu 1996) assigns prior probabilities for the straight and inverted order assuming the order is highly related to the properties of language pairs. It is formulated as  m P(r ) = ps , o = straight 1 − ps , o = inverted (6) Supposing French and English are the source and target language, respectively, the value of ps can be set as high as 0.8 to prefer monotone orientations because the two languages have similar word orders in most cases. The main problem of the ﬂat reordering model is also the problem of the standard distortion model (Koehn, Och, and Marcu 2003): Neither model considers linguistic con"
J10-3009,J97-3002,0,0.787827,"ch succeeding phrase should be translated ﬁrst. If high-level linguistic knowledge, such as the syntactic context VP→PP VP, is given, the position of the PP phrase can be easily determined since the pre-verbal modiﬁer PP in Chinese is frequently translated into a post-verbal counterpart in English. In this article, we focus on linguistically motivated phrase reordering, which integrates high-level linguistic knowledge in phrase reordering. We adopt a two-step strategy. In the ﬁrst step, we establish a hierarchical skeleton in phrasal SMT by incorporating Bracketing Transduction Grammar (BTG) (Wu 1997) into phrasal SMT. In the second step, we inject soft linguistic information into nodes of the skeleton. There are two signiﬁcant advantages to using BTG in phrasal SMT. First, BTG is able to generate hierarchical structures.2 This not only enhances phrasal SMT’s capability for hierarchical and long-distance reordering but also establishes a platform for phrasal SMT to incorporate knowledge from linguistic structure. Second, phrase reordering is restricted by the ITG constraint (Wu 1997). Although it only allows two orders (straight or inverted) of nodes in any binary branching structure, it i"
J10-3009,C04-1073,0,0.197301,"4 October 2008; revised submission received: 12 March 2010; accepted for publication: 21 April 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 local word reorderings within phrases. Unfortunately, reordering at the phrase level is still problematic for phrasal SMT. The default distortion-based reordering model simply penalizes phrase movement according to the jump distance, without considering any linguistic contexts (morphological, lexical, or syntactic) around phrases. In order to utilize lexical information for phrase reordering, Tillman (2004) and Koehn et al. (2005) propose lexicalized reordering models which directly condition phrase movement on phrases themselves. One problem with such lexicalized reordering models is that they are restricted only to reorderings of phrases seen in training data. To eliminate this restriction, Xiong, Liu, and Lin (2006) suggest using boundary words of phrases (i.e., leftmost/rightmost words of phrases), instead of phrases, as reordering evidence. Although these lexicalized reordering models signiﬁcantly outperform the distortion-based reordering model as reported, only using lexical information ("
J10-3009,I05-1007,1,0.889647,"Missing"
J10-3009,P06-1066,1,0.924664,"Missing"
J10-3009,P08-2038,1,0.858008,". The challenge, of course, is that BTG hierarchical structures are not always aligned with the linguistic structures in the source language parse tree. To address this issue, we propose an annotation algorithm. The algorithm is able to label any BTG nodes during decoding with very little overhead, regardless of whether the BTG nodes are aligned with syntactic constituent nodes in the source parse tree. The annotated linguistic elements are then used to guide phrase reordering under the ITG constraint. We call this two-step phrase reordering strategy linguistically annotated reordering (LAR) (Xiong et al. 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reordering strategy based on BTG. However, they use boundary words as reordering features at the second step. To distinguish this from our work, we call their approach boundary word–based reordering (BWR). LAR and BWR can be considered as two reordering variants for BTG-based phrasal SMT, which have similar training procedures. Furthermore, they can be combined. We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al. 2002). The BLEU scores show that LAR is comparable to BWR and signiﬁcantly improves phrase reordering when co"
J10-3009,I08-1066,1,0.939151,". The challenge, of course, is that BTG hierarchical structures are not always aligned with the linguistic structures in the source language parse tree. To address this issue, we propose an annotation algorithm. The algorithm is able to label any BTG nodes during decoding with very little overhead, regardless of whether the BTG nodes are aligned with syntactic constituent nodes in the source parse tree. The annotated linguistic elements are then used to guide phrase reordering under the ITG constraint. We call this two-step phrase reordering strategy linguistically annotated reordering (LAR) (Xiong et al. 2008a). Xiong, Liu, and Lin (2006) also adapt a two-step reordering strategy based on BTG. However, they use boundary words as reordering features at the second step. To distinguish this from our work, we call their approach boundary word–based reordering (BWR). LAR and BWR can be considered as two reordering variants for BTG-based phrasal SMT, which have similar training procedures. Furthermore, they can be combined. We evaluate LAR vs. BWR using the automatic metric BLEU (Papineni et al. 2002). The BLEU scores show that LAR is comparable to BWR and signiﬁcantly improves phrase reordering when co"
J10-3009,P01-1067,0,0.0235166,"and BWR in the two examples suggests that reordering models should respect syntactic structures in order to capture reorderings under these structures. Our observation on phrase movement change resonates with the recent efforts in phrasal SMT that allow the decoder to prefer translations which show more respect for syntactic constituent boundaries (Cherry 2008; Marton and Resnik 2008; Yamamoto, Okuma, and Sumita 2008). Mapping to syntactic constituent boundaries, or in other words, syntactic cohesion (Fox 2002; Cherry 2008), has been studied and used in early syntax-based SMT models (Wu 1997; Yamada and Knight 2001). But its value has receded in more powerful syntax-based models (Galley et al. 2004; Chiang 2005) and non-syntactic phrasal models (Koehn, Och, and Marcu 2003). Marton and Resnik (2008) and Cherry (2008) use syntactic cohesion as a soft constraint by penalizing hypotheses which violate constituent boundaries. Yamamoto, Okuma, and Sumita (2008) impose this as a hard constraint on the ITG constraint to allow reorderings which respect the source parse tree. They all report signiﬁcant improvements on different language pairs, which indicates that syntactic cohesion is very useful for phrasal SMT."
J10-3009,W08-0401,0,0.0240288,"Missing"
J10-3009,P03-1019,0,0.0283626,"n-best reordered source sentences with reordering knowledge automatically learned from the alignments between source parse trees and target translations. The approach proposed in Li et al. also enhances the connection between the preprocessing and decoding by adding a source reordering probability feature. Other approaches introduced in Nießn and Ney (2001), Popovi´c and Ney (2006), and Zhang, Zens, and Ney (2007) use morphological, POS, and chunk knowledge in the preprocessing approach, respectively. 9.1.2 Estimating Phrase Movement with Embedded Reordering Models. Under the IBM constraint (Zens and Ney 2003), the early work uses a distortion-based reordering model to penalize word movements (Koehn, Och, and Marcu 2003). Similarly, under the ITG constraint, the corresponding model is the ﬂat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more ﬂ"
J10-3009,D07-1056,0,0.0189614,"ITG constraint, the corresponding model is the ﬂat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more ﬂexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in sy"
J10-3009,P05-1059,0,0.0165509,"caused by the use of heuristic selection rules: keeping some block pairs as reordering examples while abandoning other block pairs. The kept block pairs are not necessarily the best training instances for tuning an ITG order predictor. To avoid this problem we can extract reordering examples from the BTG trees of sentence pairs. Reordering examples extracted in this way are naturally suitable for BTG order prediction. There are various ways to build BTG trees over sentence pairs. One can use BTG to produce bilingual parses of sentence pairs, similar to the approaches proposed by Wu (1997) and Zhang and Gildea (2005) but using the more sophisticated reordering models BWR or LAR. After parsing, reordering examples can be extracted from bilingual parse trees and a better reordering model is therefore induced from the extracted reordering examples. Using the better reordering model, the bilingual sentences are parsed again. This procedure is run iteratively until no performance gain is obtained in terms of translation or parsing accuracy. Formally, we can use expectation-maximization (EM) training in this procedure. In the expectation step, we ﬁrst estimate the likelihood of all BTG trees of sentence pairs w"
J10-3009,C08-1136,0,0.013257,"tactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax-based translation rules. 9.2 Automatic Analysis of Reordering Although there is a variety of work on phrase reordering, automatic analysis of phrase reordering is not widely explored in the SMT literature. Chiang et al. (2005) propose 563 Computational Linguistics Volume 36, Number 3 an automatic method to compare different system outputs in a ﬁne-grained manner with regard to reordering. In their method, common word n-grams occurring in both reference translations and"
J10-3009,P08-1064,1,0.846608,"tactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in synchronous rules which are automatically learned from word-aligned corpus. In linguistically syntax-based models, stringto-tree (Marcu et al. 2006), tree-to-string (Huang, Knight, and Joshi 2006; Liu, Liu, and Lin 2006), and tree-to-tree (Zhang et al. 2008) translation rules, just to name a few, are explored. Linguistical reordering knowledge is naturally included in these syntax-based translation rules. 9.2 Automatic Analysis of Reordering Although there is a variety of work on phrase reordering, automatic analysis of phrase reordering is not widely explored in the SMT literature. Chiang et al. (2005) propose 563 Computational Linguistics Volume 36, Number 3 an automatic method to compare different system outputs in a ﬁne-grained manner with regard to reordering. In their method, common word n-grams occurring in both reference translations and"
J10-3009,W07-0401,0,0.0144484,"ITG constraint, the corresponding model is the ﬂat model which assigns a prior probability to the straight or inverted order (Wu 1996). These two models don’t respect the content of phrases which are moved. To address this issue, lexicalized reordering models which are sensitive to lexical information about phrases are introduced (Tillman 2004; Koehn et al. 2005; Kumar and Byrne 2005; Al-Onaizan and Papineni 2006). Xiong, Liu, and Lin (2006) introduce a more ﬂexible reordering model under the ITG constraint using discriminative features which are automatically learned from a training corpus. Zhang et al. (2007) propose a model for syntactic phrase reordering which uses syntactic knowledge from source parse trees. Our reordering approach is most similar to those in Xiong, Liu, and Lin (2006) and Zhang et al. but extends them further by using syntactic knowledge and allowing non-syntactic phrase reordering. 9.1.3 Capturing Reorderings by Synchronous Grammars. Wu (1997) and Eisner (2003) use synchronous grammars to capture reorderings between two languages. Chiang (2005) introduces formal synchronous grammars for phrase-based translation. In his work, hierarchical reordering knowledge is included in sy"
J10-3009,2008.iwslt-evaluation.2,0,0.0426999,"Missing"
N10-1016,J96-1002,0,0.00599551,". In addition, hard linguistic constraints are also explored. (Wu and Ng, 1995) employs syntactic bracketing information to constrain search in order to improve speed and accuracy. (Collins et al., 2005) and (Wang et al., 2007) use hard syntactic constraints to perform reorderings according to source-side parse trees. (Xiong et al., 2008) prohibit any swappings which violate punctuation based constraints. Non-linguistic constraints are also widely used in phrase-based decoding. The IBM and ITG constraints (Zens et al., 2004) are used to restrict reorderings in practical phrase-based systems. (Berger et al., 1996) introduces the concept of rift into a machine translation system, which is similar to our definition of translation boundary. They also use a maximum entropy model to predict whether a source position is a rift based on features only from source sentences. Our work differs from (Berger et al., 1996) in three major respects. 1) We distinguish a segment boundary into two categories: beginning and ending boundary due to their different distributions (see Table 1). However, Berger et al. ignore this difference. 2) We train two classifiers to predict beginning and ending boundary respectively whil"
N10-1016,H92-1053,0,0.182045,"Missing"
N10-1016,P08-1009,0,0.236099,"ins. 1 Introduction It has been known that phrase-based decoding (phrase segmentation/translation/reordering (Chiang, 2005)) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. (Xiong et al., 2009) further presents a bracketing model to include thousands of context-sensitive syntactic constraints. All of these approaches achieve their improvements by guiding the phrase-based decoder to prefer translations which respect source-side parse trees. One major probl"
N10-1016,P05-1033,0,0.123433,"ies for any source sentences. The classifiers are trained directly on word-aligned corpus without using any additional resources. We report the accuracy of our translation boundary classifiers. We show that using constraints based on translation boundaries predicted by our classifiers achieves significant improvements over the baseline on large-scale Chinese-toEnglish translation experiments. The new constraints also significantly outperform constituent boundary based syntactic constrains. 1 Introduction It has been known that phrase-based decoding (phrase segmentation/translation/reordering (Chiang, 2005)) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn"
N10-1016,P05-1066,0,0.00867273,"ide parse tree boundary violation counting feature to build soft constraints for phrase-based decoding, and (Xiong et al., 2009), which calculates a score to indicate to what extent a source phrase can be translated as a unit using a bracketing model with richer syntactic features. More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. In addition, hard linguistic constraints are also explored. (Wu and Ng, 1995) employs syntactic bracketing information to constrain search in order to improve speed and accuracy. (Collins et al., 2005) and (Wang et al., 2007) use hard syntactic constraints to perform reorderings according to source-side parse trees. (Xiong et al., 2008) prohibit any swappings which violate punctuation based constraints. Non-linguistic constraints are also widely used in phrase-based decoding. The IBM and ITG constraints (Zens et al., 2004) are used to restrict reorderings in practical phrase-based systems. (Berger et al., 1996) introduces the concept of rift into a machine translation system, which is similar to our definition of translation boundary. They also use a maximum entropy model to predict whether"
N10-1016,J99-4005,0,0.0418273,"sing any additional resources. We report the accuracy of our translation boundary classifiers. We show that using constraints based on translation boundaries predicted by our classifiers achieves significant improvements over the baseline on large-scale Chinese-toEnglish translation experiments. The new constraints also significantly outperform constituent boundary based syntactic constrains. 1 Introduction It has been known that phrase-based decoding (phrase segmentation/translation/reordering (Chiang, 2005)) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a countin"
N10-1016,N03-1017,0,0.0561793,"2005)) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. (Xiong et al., 2009) further presents a bracketing model to include thousands of context-sensitive syntactic constraints. All of these approaches achieve their improvements by guiding the phrase-based decoder to prefer translations which respect source-side parse trees. One major problem with such constituent boundary based constraints is that syntactic structures of the source language do not necessarily"
N10-1016,W04-3250,0,0.181357,"Missing"
N10-1016,P08-1114,0,0.295696,"It has been known that phrase-based decoding (phrase segmentation/translation/reordering (Chiang, 2005)) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. (Xiong et al., 2009) further presents a bracketing model to include thousands of context-sensitive syntactic constraints. All of these approaches achieve their improvements by guiding the phrase-based decoder to prefer translations which respect source-side parse trees. One major problem with such constituent bound"
N10-1016,P00-1056,0,0.0697293,"Missing"
N10-1016,P03-1021,0,0.0107032,"add a new feature to the decoder’s loglinear model: translation boundary violation counting feature. This counting feature accumulates whenever hypotheses have a partial translation spanning ci ...cj (j > i) where ci ∈ / By or cj ∈ / Ey . The 140 LDC ID LDC2004E12 LDC2004T08 LDC2005T10 LDC2003E14 LDC2002E18 LDC2005T06 LDC2003E07 LDC2004T07 Description United Nations Hong Kong News Sinorama Magazine FBIS Xinhua News V1 beta Chinese News Translation Chinese Treebank Multiple Translation Chinese Table 5: Training corpora. weight λv of this feature is tuned via minimal error rate training (MERT) (Och, 2003) with other feature weights. Unlike hard constraints, which simply prevent any hypotheses from violating translation boundaries, soft constraints allow violations of translation boundaries but with a penalty of exp(−λv Cv ) where Cv is the violation count. By using soft constraints, we can enable the model to prefer hypotheses which are consistent with translation boundaries. 5 Experiment Our baseline system is a phrase-based system using BTGs (Wu, 1997), which includes a contentdependent reordering model discriminatively trained using reordering examples (Xiong et al., 2006). We carried out v"
N10-1016,P02-1040,0,0.104176,"Missing"
N10-1016,C08-1094,0,0.0139392,"age do not necessarily reflect translation structures where the source and target language correspond to each other. In this paper, we investigate building classifiers that directly address the problem of translation boundary, rather than extracting constituent boundary from sourceside parsers built for a different purpose. A translation boundary is a position in the source sequence which begins or ends a translation zone 1 spanning multiple source words. In a translation zone, the source phrase is translated as a unit. Reorderings which cross translation zones are not desirable. Inspired by (Roark and Hollingshead, 2008) which introduces classifiers to decide if a word can begin/end a multi-word constituent, we build two discriminative classifiers to tag each word in the source sequence with a binary class label. The first classifier decides if a word can begin a multi-sourceword translation zone; the second classifier decides if a word can end a multi-source-word translation 1 We will give a formal definition of translation zone in Section 2. 136 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 136–144, c Los Angeles, California, June 2010. 2010 Associat"
N10-1016,D07-1077,0,0.00652594,"lation counting feature to build soft constraints for phrase-based decoding, and (Xiong et al., 2009), which calculates a score to indicate to what extent a source phrase can be translated as a unit using a bracketing model with richer syntactic features. More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. In addition, hard linguistic constraints are also explored. (Wu and Ng, 1995) employs syntactic bracketing information to constrain search in order to improve speed and accuracy. (Collins et al., 2005) and (Wang et al., 2007) use hard syntactic constraints to perform reorderings according to source-side parse trees. (Xiong et al., 2008) prohibit any swappings which violate punctuation based constraints. Non-linguistic constraints are also widely used in phrase-based decoding. The IBM and ITG constraints (Zens et al., 2004) are used to restrict reorderings in practical phrase-based systems. (Berger et al., 1996) introduces the concept of rift into a machine translation system, which is similar to our definition of translation boundary. They also use a maximum entropy model to predict whether a source position is a"
N10-1016,Y95-1025,0,0.0786103,"er. Our introduction has already briefly mentioned (Cherry, 2008) and (Marton and Resnik, 2008), which utilize source-side parse tree boundary violation counting feature to build soft constraints for phrase-based decoding, and (Xiong et al., 2009), which calculates a score to indicate to what extent a source phrase can be translated as a unit using a bracketing model with richer syntactic features. More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. In addition, hard linguistic constraints are also explored. (Wu and Ng, 1995) employs syntactic bracketing information to constrain search in order to improve speed and accuracy. (Collins et al., 2005) and (Wang et al., 2007) use hard syntactic constraints to perform reorderings according to source-side parse trees. (Xiong et al., 2008) prohibit any swappings which violate punctuation based constraints. Non-linguistic constraints are also widely used in phrase-based decoding. The IBM and ITG constraints (Zens et al., 2004) are used to restrict reorderings in practical phrase-based systems. (Berger et al., 1996) introduces the concept of rift into a machine translation"
N10-1016,J97-3002,0,0.0255434,"Treebank Multiple Translation Chinese Table 5: Training corpora. weight λv of this feature is tuned via minimal error rate training (MERT) (Och, 2003) with other feature weights. Unlike hard constraints, which simply prevent any hypotheses from violating translation boundaries, soft constraints allow violations of translation boundaries but with a penalty of exp(−λv Cv ) where Cv is the violation count. By using soft constraints, we can enable the model to prefer hypotheses which are consistent with translation boundaries. 5 Experiment Our baseline system is a phrase-based system using BTGs (Wu, 1997), which includes a contentdependent reordering model discriminatively trained using reordering examples (Xiong et al., 2006). We carried out various experiments to evaluate the impact of integrating translation boundary based soft constraints into decoding on the system performance on the Chinese-to-English translation task of the NIST MT-05 using large scale training data. 5.1 Experimental Setup Our training corpora are listed in Table 5. The whole corpora consist of 96.9M Chinese words and 109.5M English words in 3.8M sentence pairs. We ran GIZA++ (Och and Ney, 2000) on the parallel corpora"
N10-1016,I05-1007,1,0.393667,"Missing"
N10-1016,P06-1066,1,0.524346,"rror rate training (MERT) (Och, 2003) with other feature weights. Unlike hard constraints, which simply prevent any hypotheses from violating translation boundaries, soft constraints allow violations of translation boundaries but with a penalty of exp(−λv Cv ) where Cv is the violation count. By using soft constraints, we can enable the model to prefer hypotheses which are consistent with translation boundaries. 5 Experiment Our baseline system is a phrase-based system using BTGs (Wu, 1997), which includes a contentdependent reordering model discriminatively trained using reordering examples (Xiong et al., 2006). We carried out various experiments to evaluate the impact of integrating translation boundary based soft constraints into decoding on the system performance on the Chinese-to-English translation task of the NIST MT-05 using large scale training data. 5.1 Experimental Setup Our training corpora are listed in Table 5. The whole corpora consist of 96.9M Chinese words and 109.5M English words in 3.8M sentence pairs. We ran GIZA++ (Och and Ney, 2000) on the parallel corpora in both directions and then applied the “grow-diag-final” refinement rule (Koehn et al., 2005) to obtain many-to-many word a"
N10-1016,P09-1036,1,0.852114,"008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. (Xiong et al., 2009) further presents a bracketing model to include thousands of context-sensitive syntactic constraints. All of these approaches achieve their improvements by guiding the phrase-based decoder to prefer translations which respect source-side parse trees. One major problem with such constituent boundary based constraints is that syntactic structures of the source language do not necessarily reflect translation structures where the source and target language correspond to each other. In this paper, we investigate building classifiers that directly address the problem of translation boundary, rather"
N10-1016,C08-1136,0,0.145511,"straints based on translation boundaries predicted by our classifiers achieves significant improvements over the baseline on large-scale Chinese-toEnglish translation experiments. The new constraints also significantly outperform constituent boundary based syntactic constrains. 1 Introduction It has been known that phrase-based decoding (phrase segmentation/translation/reordering (Chiang, 2005)) should be constrained to some extent not only for transferring the NP-hard problem (Knight, 1999) into a tractable one in practice but also for improving translation quality. For example, Xiong et al. (2008) find that translation quality can be significantly improved by either prohibiting reorderings around punctuation or restricting reorderings within a 15-word window. Recently, more linguistically motivated constraints are introduced to improve phrase-based decoding. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al., 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. (Xion"
N10-1016,C04-1030,0,\N,Missing
N10-1016,I08-1066,1,\N,Missing
N10-1016,2005.iwslt-1.8,0,\N,Missing
P06-1066,P00-1056,0,0.682597,"Missing"
P06-1066,P03-1021,0,0.0549363,"inese-English tasks, the probability for the straight order is set at pm = 0.95. This is because word order in Chinese and English is usually similar. The last one is the maximum entropy based reordering model proposed by us, which will be described in the next section. We define a derivation D as a sequence of applications of rules (1) − (3), and let c(D) and e(D) be the Chinese and English yields of D. The probability of a derivation D is P r(D) = Y P r(i) (7) Another feature of our decoder is the k-best list generation. The k-best list is very important for the minimum error rate training (Och, 2003a) which is used for tuning the weights λ for our model. We use a very lazy algorithm for the k-best list generation, which runs two phases similarly to the one by Huang et al. (2005). In the first phase, the decoder runs as usual except that it keeps some information of weaker derivations which are to be discarded during recombination. This will generate not only the first-best of final derivation but also a shared forest. In the second phase, the lazy algorithm runs recursively on the shared forest. It finds the second-best of the final derivation, which makes its children to find their seco"
P06-1066,J04-4002,0,0.59146,"ion, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. 1 Introduction Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently. Compared with word-based SMT systems, phrase-based systems can easily address reorderings of words within phrases. However, at the phrase level, reordering is still a computationally expensive problem just like reordering at the word level (Knight, 1999). Many systems use very simple models to reorder phrases 1 . One is distortion model (Och and Ney, 2004; Koehn et al., 2003) which penalizes translations according to their jump distance instead of their content. For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered. This model takes the risk of penalizing long distance jumps 1 In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. 521 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521–528, c Sydney, July 2006. 2006 Association for Computational Linguistics cl"
P06-1066,W02-1039,0,0.0516889,"1 .t1 = E1 , o = O 0, otherwise 1, b1 .t1 = E1 , b2 .t1 = E2 , o = O 0, otherwise Figure 3: MaxEnt-based reordering feature templates. The first one is a lexical feature, and the second one is a target collocation feature, where Ei are English words, O ∈ {straight, inverted}. is block collocation, b1 .s1 &b1 .t1 and b2 .s1 &b2 .t1 . The templates for the lexical feature and the collocation feature are shown in Figure 3. Why do we use the first words as features? These words are nicely at the boundary of blocks. One of assumptions of phrase-based SMT is that phrase cohere across two languages (Fox, 2002), which means phrases in one language tend to be moved together during translation. This indicates that boundary words of blocks may keep information for their movements/reorderings. To test this hypothesis, we calculate the information gain ratio (IGR) for boundary words as well as the whole blocks against the order on the reordering examples extracted by the algorithm described above. The IGR is the measure used in the decision tree learning to select features (Quinlan, 1993). It represents how precisely the feature predicate the class. For feature f and class c, the IGR(f, c) Figure 2: Reor"
P06-1066,J99-4005,0,0.0622403,"ing events of neighbor blocks from bilingual data. In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. 1 Introduction Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently. Compared with word-based SMT systems, phrase-based systems can easily address reorderings of words within phrases. However, at the phrase level, reordering is still a computationally expensive problem just like reordering at the word level (Knight, 1999). Many systems use very simple models to reorder phrases 1 . One is distortion model (Och and Ney, 2004; Koehn et al., 2003) which penalizes translations according to their jump distance instead of their content. For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered. This model takes the risk of penalizing long distance jumps 1 In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. 521 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Me"
P06-1066,W05-1506,0,0.030856,"Missing"
P06-1066,N03-1017,0,0.0910021,"sed reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks. 1 Introduction Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently. Compared with word-based SMT systems, phrase-based systems can easily address reorderings of words within phrases. However, at the phrase level, reordering is still a computationally expensive problem just like reordering at the word level (Knight, 1999). Many systems use very simple models to reorder phrases 1 . One is distortion model (Och and Ney, 2004; Koehn et al., 2003) which penalizes translations according to their jump distance instead of their content. For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered. This model takes the risk of penalizing long distance jumps 1 In this paper, we focus our discussions on phrases that are not necessarily aligned to syntactic constituent boundary. 521 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521–528, c Sydney, July 2006. 2006 Association for Computational Linguistics classification with onl"
P06-1066,koen-2004-pharaoh,0,0.29269,"rings will be extracted, not limited to reorderings of bilingual phrases of length less than a predefined number of words. Secondly, features will be extracted from reordering examples according to feature templates. Finally, a maximum entropy classifier will be trained on the features. In this paper we describe our system and the MaxEnt-based reordering model with the associated algorithm. We also present experiments that indicate that the MaxEnt-based reordering model improves translation significantly compared with other reordering approaches and a state-of-the-art distortion-based system (Koehn, 2004). used to translate source phrase y into target phrase x and generate a block A. Later, the straight rule (1) merges two consecutive blocks into a single larger block in the straight order; while the inverted rule (2) merges them in the inverted order. These two merging rules will be used continuously until the whole source sentence is covered. When the translation is finished, a tree indicating the hierarchical segmentation of the source sentence is also produced. In the following, we will define the model in a straight way, not in the dynamic programming recursion way used by (Wu, 1996; Zens"
P06-1066,2005.eamt-1.36,0,0.0562487,"Missing"
P06-1066,C04-1030,0,0.2676,"However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem. Another smart reordering model was proposed by Chiang (2005). In his approach, phrases are reorganized into hierarchical ones by reducing subphrases to variables. This template-based scheme not only captures the reorderings of phrases, but also integrates some phrasal generalizations into the global model. In this paper, we propose a novel solution for phrasal reordering. Here, under the ITG constraint (Wu, 1997; Zens et al., 2004), we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks. Therefore reordering can be modelled as a problem of We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs). The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext. We present an algorithm to extract all reordering events of neighbor blocks from bilingual"
P06-1066,N04-4026,0,0.493594,"Chinese Academy of Sciences {liuqun, sxlin}@ict.ac.cn dyxiong@ict.ac.cn Abstract which are common between two languages with very different orders. Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either. Flat model assigns constant probabilities for monotone order and non-monotone order. The two probabilities can be set to prefer monotone or non-monotone orientations depending on the language pairs. In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent. Lexicalized reordering model learns local orientations (monotone or non-monotone) with probabilities for each bilingual phrase from training data. During decoding, the model attempts to finding a Viterbi local orientation sequence. Performance gains have been reported for systems with lexicalized reordering model. However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems,"
P06-1066,W02-2018,0,0.0177583,"ures of blocks as reordering evidences. Good features can not only capture reorderings, avoid sparseness, but also integrate generalizations. It is very straight to use maximum entropy model to integrate features to predicate reorderings of blocks. Under the MaxEnt model, we have b must be consistent with the word alignment M ∀(i, j) ∈ M, i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2 P exp( i θi hi (o, A1 , A2 )) P Ω = pθ (o|A , A ) = P 1 2 o exp( i θi hi (o, A , A )) (10) where the functions hi ∈ {0, 1} are model features and the θi are weights of the model features which can be trained by different algorithms (Malouf, 2002). 1 2 3.1 Reordering Example Extraction Algorithm The input for the algorithm is a bilingual corpus with high-precision word alignments. We obtain the word alignments using the way of Koehn et al. (2005). After running GIZA++ (Och and Ney, 524 This definition is similar to that of bilingual phrase except that there is no length limitation over block. A reordering example is a triple of (o, b1 , b2 ) where b1 and b2 are two neighbor blocks and o is the order between them. We define each vertex of block as corner. Each corner has four links in four directions: topright, topleft, bottomright, bot"
P06-1066,P05-1069,0,0.0165247,"Missing"
P06-1066,H05-1021,0,0.477653,"Missing"
P06-1066,P05-1033,0,0.930274,"exicalized reordering model that is phrase dependent. Lexicalized reordering model learns local orientations (monotone or non-monotone) with probabilities for each bilingual phrase from training data. During decoding, the model attempts to finding a Viterbi local orientation sequence. Performance gains have been reported for systems with lexicalized reordering model. However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem. Another smart reordering model was proposed by Chiang (2005). In his approach, phrases are reorganized into hierarchical ones by reducing subphrases to variables. This template-based scheme not only captures the reorderings of phrases, but also integrates some phrasal generalizations into the global model. In this paper, we propose a novel solution for phrasal reordering. Here, under the ITG constraint (Wu, 1997; Zens et al., 2004), we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks. Therefore reordering can be modelled as a problem of We propose a novel reordering model for phrase-based statistical"
P06-1066,zhang-etal-2004-interpreting,0,0.0749989,"Missing"
P06-1066,P96-1021,0,0.83763,"006. 2006 Association for Computational Linguistics classification with only two labels, straight and inverted. In this paper, we build a maximum entropy based classification model as the reordering model. Different from lexicalized reordering, we do not use the whole block as reordering evidence, but only features extracted from blocks. This is more flexible. It makes our model reorder any blocks, observed in training or not. The whole maximum entropy based reordering model is embedded inside a log-linear phrase-based model of translation. Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically. To create a maximum entropy based reordering model, the first step is learning reordering examples from training data, similar to the lexicalized reordering model. But in our way, any evidences of reorderings will be extracted, not limited to reorderings of bilingual phrases of length less than a predefined number of words. Secondly, features will be extracted from reordering examples according to feature templates. Finally, a maximum entropy classifier will be trained on the features. In"
P06-1066,J97-3002,0,0.946004,"ng model. However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem. Another smart reordering model was proposed by Chiang (2005). In his approach, phrases are reorganized into hierarchical ones by reducing subphrases to variables. This template-based scheme not only captures the reorderings of phrases, but also integrates some phrasal generalizations into the global model. In this paper, we propose a novel solution for phrasal reordering. Here, under the ITG constraint (Wu, 1997; Zens et al., 2004), we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks. Therefore reordering can be modelled as a problem of We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs). The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext. We present an algorithm to extract all reordering events of neighbor b"
P06-1066,2005.iwslt-1.8,0,\N,Missing
P08-2038,P03-1021,0,0.0408658,"Missing"
P08-2038,D07-1077,0,0.060863,"ng Transduction Grammar (BTG) proposed by (Wu, 1997) has been widely used in statistical machine translation (SMT). However, the original BTG does not provide an effective mechanism to predict the most appropriate orders between two neighboring phrases. To address this problem, Xiong et al. (2006) enhance the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual phrases as features. Although this model outperforms previous unlexicalized models, it does not utilize any linguistically syntactic features, which have proven useful for phrase reordering (Wang et al., 2007). Zhang et al. (2007) integrates source-side syntactic knowledge into a phrase reordering model based on BTG-style rules. However, one limitation of this method is that it only reorders syntactic phrases because linguistic knowledge from parse trees is only carried by syntactic phrases as far as reordering is concerned, while non-syntactic phrases are combined monotonously with a flat reordering score. In this paper, we propose a linguistically annotated reordering model for BTG-based SMT, which is a significant extension to the work mentioned above. The new model annotates each BTG node with"
P08-2038,J97-3002,0,0.403967,"Missing"
P08-2038,I05-1007,1,0.90995,"Missing"
P08-2038,P06-1066,1,0.929388,"nclude in Section 5. 2 Baseline SMT System The baseline system is a phrase-based system which uses the BTG lexical rules (A → x/y) to translate source phrase x into target phrase y and the BTG merging rules (A → [A, A]|hA, Ai) to combine two neighboring phrases with a straight or inverted order. The BTG lexical rules are weighted with several features, such as phrase translation, word penalty and language models, in a log-linear form. For the merging rules, a MaxEnt-based reordering model using boundary words of neighboring phrases as features is used to predict the merging order, similar to (Xiong et al., 2006). We call this reordering model 149 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 149–152, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics boundary words based reordering model (BWR). In this paper, we propose to incorporate a linguistically annotated reordering model into the log-linear translation model, so as to strengthen the BWR’s phrase reordering ability. We train all the model scaling factors on the development set to maximize the BLEU score. A CKY-style decoder is developed to generate the best BTG binary tree for each input senten"
P08-2038,D07-1056,0,0.0632565,"posed by (Wu, 1997) has been widely used in statistical machine translation (SMT). However, the original BTG does not provide an effective mechanism to predict the most appropriate orders between two neighboring phrases. To address this problem, Xiong et al. (2006) enhance the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual phrases as features. Although this model outperforms previous unlexicalized models, it does not utilize any linguistically syntactic features, which have proven useful for phrase reordering (Wang et al., 2007). Zhang et al. (2007) integrates source-side syntactic knowledge into a phrase reordering model based on BTG-style rules. However, one limitation of this method is that it only reorders syntactic phrases because linguistic knowledge from parse trees is only carried by syntactic phrases as far as reordering is concerned, while non-syntactic phrases are combined monotonously with a flat reordering score. In this paper, we propose a linguistically annotated reordering model for BTG-based SMT, which is a significant extension to the work mentioned above. The new model annotates each BTG node with linguistic knowledge"
P09-1036,P08-1009,0,0.602914,"uent translations and reorderings. This, unfortunately, significantly jeopardizes performance (Koehn et al., 2003; Xiong et al., 2008) because by integrating syntactic constraint into decoding as a hard constraint, it simply prohibits any other useful non-syntactic translations which violate constituent boundaries. To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. These constituent matching/violation counts are used as a feature in the decoder’s log-linear model and their weights are tuned via minimal error rate training (MERT) (Och, 2003). In this way, syntactic constraint is integrated into decoding as a soft constraint to enable the decoder to reward hypotheses that respect syntactic analyses or to peSyntactic analysis influences the way in which the source sentence is translated. Previous efforts add syntactic constraints to phrase-based translation by directly rewarding/punishi"
P09-1036,P05-1033,0,0.731805,"nly on phrase movement but also on the lexical selection for the multi-meaning word “节”1 . To avert such errors, the decoder can fully respect linguistic structures by only allowing syntactic constituent translations and reorderings. This, unfortunately, significantly jeopardizes performance (Koehn et al., 2003; Xiong et al., 2008) because by integrating syntactic constraint into decoding as a hard constraint, it simply prohibits any other useful non-syntactic translations which violate constituent boundaries. To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. These constituent matching/violation counts are used as a feature in the decoder’s log-linear model and their weights are tuned via minimal error rate training (MERT) (Och, 2003). In this way, syntactic constraint is integrated into decoding as a soft constraint to enable the decoder to reward hypotheses that respect syntactic"
P09-1036,D08-1024,0,0.0439355,"Missing"
P09-1036,N03-1017,0,0.34237,"r inadequately breaks up the second NP phrase and translates the two words “航海” and “节” separately. However, the parse tree of the source fragment constrains the phrase “航海 节” to be translated as a unit. Without considering syntactic constraints from the parse tree, the decoder makes wrong decisions not only on phrase movement but also on the lexical selection for the multi-meaning word “节”1 . To avert such errors, the decoder can fully respect linguistic structures by only allowing syntactic constituent translations and reorderings. This, unfortunately, significantly jeopardizes performance (Koehn et al., 2003; Xiong et al., 2008) because by integrating syntactic constraint into decoding as a hard constraint, it simply prohibits any other useful non-syntactic translations which violate constituent boundaries. To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. These constituent matching/v"
P09-1036,W04-3250,0,0.433632,"Missing"
P09-1036,C02-1003,0,0.0654943,"Missing"
P09-1036,P08-1114,0,0.39465,"ly allowing syntactic constituent translations and reorderings. This, unfortunately, significantly jeopardizes performance (Koehn et al., 2003; Xiong et al., 2008) because by integrating syntactic constraint into decoding as a hard constraint, it simply prohibits any other useful non-syntactic translations which violate constituent boundaries. To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. These constituent matching/violation counts are used as a feature in the decoder’s log-linear model and their weights are tuned via minimal error rate training (MERT) (Och, 2003). In this way, syntactic constraint is integrated into decoding as a soft constraint to enable the decoder to reward hypotheses that respect syntactic analyses or to peSyntactic analysis influences the way in which the source sentence is translated. Previous efforts add syntactic constraints to phrase-based translation by directly"
P09-1036,P00-1056,0,0.707896,"Missing"
P09-1036,P03-1021,0,0.160074,"on-syntactic translations which violate constituent boundaries. To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. These constituent matching/violation counts are used as a feature in the decoder’s log-linear model and their weights are tuned via minimal error rate training (MERT) (Och, 2003). In this way, syntactic constraint is integrated into decoding as a soft constraint to enable the decoder to reward hypotheses that respect syntactic analyses or to peSyntactic analysis influences the way in which the source sentence is translated. Previous efforts add syntactic constraints to phrase-based translation by directly rewarding/punishing a hypothesis whenever it matches/violates source-side constituents. We present a new model that automatically learns syntactic constraints, including but not limited to constituent matching/violation, from training corpus. The model brackets a sou"
P09-1036,P02-1040,0,0.0758415,"ot use any syntactic constraints on Chinese-to-English translation. To compare with the CMVC, we also conduct experiments using (Marton and Resnik, 2008)’s XP+. The XP+ accumulates a count for each hypothesis whenever it violates the boundaries of a constituent with a label from {NP, VP, CP, IP, PP, ADVP, QP, LCP, DNP}. The XP+ is the best feature among all features that Marton and Resnik use for Chinese-toEnglish translation. Our experimental results display that our SDB model achieves a substantial improvement over the baseline and significantly outperforms XP+ according to the BLEU metric (Papineni et al., 2002). In addition, our analysis shows further evidences of the performance gain from a different perspective than that of BLEU. The paper proceeds as follows. In section 2 we describe how to learn bracketing instances from a training corpus. In section 3 we elaborate the syntax-driven bracketing model, including feature generation and the integration of the SDB model into phrase-based SMT. In section 4 and 5, we present our experiments and analysis. And we finally conclude in section 6. 2 The Acquisition of Bracketing Instances In this section, we formally define the bracketing instance, comprisin"
P09-1036,J97-3002,0,0.783539,"Missing"
P09-1036,I05-1007,1,0.887578,"Missing"
P09-1036,P06-1066,1,0.890811,"ndaries of a constituent. Otherwise, a lower probability is given. Through this additional feature, we want the decoder to prefer hypotheses that translate source spans which can be translated as a unit, and avoids translating those which are discontinuous after translation. The weight of this new feature is tuned via MERT, which measures the extent to which this feature should be trusted. In this paper, we implement the SDB model in a state-of-the-art phrase-based system which adapts a binary bracketing transduction grammar (BTG) (Wu, 1997) to phrase translation and reordering, described in (Xiong et al., 2006). Whenever a BTG merging rule (s → [s1 s2 ] or s → hs1 s2 i) is used, the SDB model gives a probability to the span s covered by the rule, which estimates the extent to which the span is bracketable. For the unary SDB model, we only consider the features from τ (s). For the binary SDB model, we use all features from τ (s1 ), τ (s2 ) and τ (s) since the binary SDB model is naturally suitable to the binary BTG rules. The SDB model, however, is not only limited to phrase-based SMT using BTG rules. Since it is applied on a source span each time, any other hierarchical phrase-based or syntax-based"
P09-1036,C08-1127,1,0.822854,"s up the second NP phrase and translates the two words “航海” and “节” separately. However, the parse tree of the source fragment constrains the phrase “航海 节” to be translated as a unit. Without considering syntactic constraints from the parse tree, the decoder makes wrong decisions not only on phrase movement but also on the lexical selection for the multi-meaning word “节”1 . To avert such errors, the decoder can fully respect linguistic structures by only allowing syntactic constituent translations and reorderings. This, unfortunately, significantly jeopardizes performance (Koehn et al., 2003; Xiong et al., 2008) because by integrating syntactic constraint into decoding as a hard constraint, it simply prohibits any other useful non-syntactic translations which violate constituent boundaries. To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. These constituent matching/violation counts are u"
P09-1036,W02-1039,0,\N,Missing
P09-1036,2005.iwslt-1.8,0,\N,Missing
P10-1062,P03-1021,0,0.003576,"Missing"
P10-1062,P02-1040,0,0.101194,"Missing"
P10-1062,2009.eamt-1.15,0,0.0390746,"Missing"
P10-1062,2007.mtsummit-papers.54,0,0.830199,"tic features, according to their final results. Ueffing and Ney (2007) exhaustively explore various word-level confidence measures to label each word in a generated translation hypothesis as correct or incorrect. All their measures are based on word posterior probabilities, which are estimated from 1) system output, such as word lattices or N -best lists and 2) word or phrase translation table. Their experimental results show that word posterior probabilities directly estimated from phrase translation table are better than those from system output except for the Chinese-English language pair. Sanchis et al. (2007) adopt a smoothed naive Bayes model to combine different word posterior probability based confidence features which are estimated from N -best lists, similar to (Ueffing and Ney, 2007). Raybaud et al. (2009) study several confidence features based on mutual information between words and n-gram and backward n-gram language model for word-level and sentence-level CE. They also explore linguistic features using information from syntactic category, tense, gender and so on. Unfortunately, such linguistic features neither improve performance at the word level nor at the sentence level. Our work depa"
P10-1062,1993.iwpt-1.22,0,0.00848038,"o be incorrect. The challenge of using syntactic knowledge for error detection is that machinegenerated hypotheses are rarely fully grammatical. They are mixed with grammatical and ungrammatical parts, which hence are not friendly to traditional parsers trained on grammatical sentences because ungrammatical parts of a machinegenerated sentence could lead to a parsing failure. To overcome this challenge, we select the Link Grammar (LG) parser 3 as our syntactic parser to generate syntactic features. The LG parser produces a set of labeled links which connect pairs of words with a link grammar (Sleator and Temperley, 1993). The main reason why we choose the LG parser is that it provides a robustness feature: null-link scheme. The null-link scheme allows the parser to parse a sentence even when the parser can not fully interpret the entire sentence (e.g. including ungrammatical parts). When the parser fail to parse the entire sentence, it ignores one word each time until it finds linkages for remaining words. After parsing, those ignored words are not connected to any other words. We call them null-linked words. Our hypothesis is that null-linked words are prone to be syntactically incorrect. We hence straightfo"
P10-1062,C04-1047,0,0.0535615,"Missing"
P10-1062,H05-1006,0,0.0326993,"development set. Sometimes the step 2) is not necessary if only one effective feature is used (Ueffing and Ney, 2007); and sometimes the step 2) and 3) can be merged into a single step if we directly output predicting results from binary classifiers instead of making thresholding decision. Various features from different SMT models and system outputs are investigated (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007; Raybaud et al., 2009). Experimental results show that they are useful for error detection. However, it is not adequate to just use these features as discussed in (Shi and Zhou, 2005) because the information that they carry is either from the inner components of SMT systems or from system outputs. To some extent, it has already been considered by SMT systems. Hence finding external information Introduction Translation hypotheses generated by a statistical machine translation (SMT) system always contain both correct parts (e.g. words, n-grams, phrases matched with reference translations) and incorrect parts. Automatically distinguishing incorrect parts from correct parts is therefore very desirable not only for post-editing and interactive machine translation (Ueffing and N"
P10-1062,J96-1002,0,0.0522358,"Missing"
P10-1062,2003.mtsummit-papers.52,0,0.344912,"likely to be incorrect than words in frequently occurring patterns. To some extent, these two features have similar function to a target language model or pos-based target language model. yes, w has links no, otherwise In Figure 1 we show an example of a generated translation hypothesis with its link parse. Here links are denoted with dotted lines which are annotated with link types (e.g., Jp, Op). Bracketed words, namely “,” and “including”, are null-linked words. 3.3 Word Posterior Probability Features Our word posterior probability is calculated on N best list, which is first proposed by (Ueffing et al., 2003) and widely used in (Blatz et al., 2003; Ueffing and Ney, 2007; Sanchis et al., 2007). Given a source sentence f , let {en }N 1 be the N best list generated by an SMT system, and let ein is the i-th word in en . The major work of calculating word posterior probabilities is to find the Levenshtein alignment (Levenshtein, 1966) between the best hypothesis e1 and its competing hypothesis 3.2 Syntactic Features High-level linguistic knowledge such as syntactic information about a word is a very natural and promising indicator to decide whether this word is syntactically correct or not. Words occur"
P10-1062,W06-3110,0,0.0456356,"sidered by SMT systems. Hence finding external information Introduction Translation hypotheses generated by a statistical machine translation (SMT) system always contain both correct parts (e.g. words, n-grams, phrases matched with reference translations) and incorrect parts. Automatically distinguishing incorrect parts from correct parts is therefore very desirable not only for post-editing and interactive machine translation (Ueffing and Ney, 2007) but also for SMT itself: either by rescoring hypotheses in the N -best list using the probability of correctness calculated for each hypothesis (Zens and Ney, 2006) or by generating new hypotheses using N best lists from one SMT system or multiple sys604 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 604–611, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics confidence estimation at the word level as well as at the sentence level. The features they use for word level CE include word posterior probabilities estimated from N -best lists, features based on SMT models, semantic features extracted from WordNet as well as simple syntactic features, i.e. parentheses and quotation m"
P10-1062,W03-0413,0,0.0622143,"Missing"
P10-1062,P05-3026,0,0.0200006,"Missing"
P10-1062,N03-1017,0,0.00275441,"variable c to indicate whether this word is correct or not. In the feature vector, we look at 2 words before and 2 words after the current word position (w−2 , w−1 , w, w1 , w2 ). We collect features {wd, pos, link, dwpp} for each word among these words and combine them into the feature vector ψ for w. As such, we want the feature vector to capture the contextual environment, e.g., pos sequence pattern, syntactic pattern, where the word w occurs. To obtain machine-generated translation hypotheses for our error detection, we use a state-of-the-art phrase-based machine translation system MOSES (Koehn et al., 2003; Koehn et al., 2007). The translation task is on the official NIST Chineseto-English evaluation data. The training data consists of more than 4 million pairs of sentences (including 101.93M Chinese words and 112.78M English words) from LDC distributed corpora. Table 2 shows the corpora that we use for the translation task. We build a four-gram language model using the SRILM toolkit (Stolcke, 2002), which is trained 607 Feature wd pos link dwpp Example { 1, f (c, ψ) = { 0, 1, f (c, ψ) = { 0, 1, f (c, ψ) = { 0, 1, f (c, ψ) = 0, ψ.w.wd = ”.”, c = correct otherwise ψ.w2 .pos = ”N N ”, c = incorre"
P10-1062,J07-1003,0,\N,Missing
P10-1062,P07-2045,0,\N,Missing
P10-1062,C04-1046,0,\N,Missing
P10-1062,2005.eamt-1.20,0,\N,Missing
P11-1129,D07-1090,0,0.234408,"the efforts that advance translation models from word-based paradigm to syntax-based philosophy, in recent years we have also witnessed increasing efforts dedicated to extend standard n-gram language models for SMT. We roughly categorize these efforts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed manner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation. The second direction digs deeply into monolingual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine translation which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing long-distance dependenci"
P11-1129,J93-2003,0,0.016013,"information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve translation quality and collectively achieve up to 1 BLEU point over a competitive baseline. 1 Introduction Language model is one of the most important knowledge sources for statistical machine translation (SMT) (Brown et al., 1993). The standard n-gram language model (Goodman, 2001) assigns probabilities to hypotheses in the target language conditioning on a context history of the preceding n − 1 words. Along with the efforts that advance translation models from word-based paradigm to syntax-based philosophy, in recent years we have also witnessed increasing efforts dedicated to extend standard n-gram language models for SMT. We roughly categorize these efforts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC"
P11-1129,2003.mtsummit-papers.6,0,0.593667,"rts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed manner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation. The second direction digs deeply into monolingual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine translation which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing long-distance dependencies. In this paper, we pursue the second direction without resorting to any linguistic resources such as a syntactic parser. With a belief that a language model that embraces a larger context provides better prediction ability, we learn additional inf"
P11-1129,J07-2003,0,0.832723,"language model assigns a probability Pb (w1m ) to w1m by looking at the succeeding context according to Pb (w1m ) = m ∏ i=1 m )≈ P (wi |wi+1 m ∏ i+n−1 P (wi |wi+1 ) (2) i=1 3.1 Training et al., 2006) and 2) a standard phrase-based decoder (Koehn et al., 2003). Both decoders translate source sentences from the beginning of a sentence to the ending. Wu (1996) introduce a dynamic programming algorithm to integrate a forward bigram language model with inversion transduction grammar. His algorithm is then adapted and extended for integrating forward n-gram language models into synchronous CFGs by Chiang (2007). Our algorithms are different from theirs in two major aspects 1. The string input to the algorithms is in a reverse order. 2. We adopt a different way to calculate language model probabilities for partial hypotheses so that we can utilize incomplete n-grams. Before we introduce the integration algorithms, we define three functions P, L, and R on strings (in a reverse order) over the English terminal alphabet T . The function P is defined as follows. P(wk ...w1 ) = P (wk )...P (wk−n+2 |wk ...wk−n+3 ) {z } | a ∏ P (wi |wi+n−1 ...wi+1 ) × 1≤i≤k−n+1 | For the convenience of training, we invert t"
P11-1129,J90-1003,0,0.0548025,"n that long-distance dependencies between words are very important for statistical language modeling. However, n-gram language models can only capture short-distance dependencies within an n-word window. In order to model long-distance dependencies, previous work such as (Rosenfeld et al., 1994) and (Zhou, 2004) exploit trigger pairs. A trigger pair is defined as an ordered 2-tuple (x, y) where word x occurs in the preceding context of word y. It can also be denoted in a more visual manner as x → y with x being the trigger and y the triggered word5 . We use pointwise mutual information (PMI) (Church and Hanks, 1990) to measure the strength of the association between x and y, which is defined as follows P M I(x, y) = log( P (x, y) ) P (x)P (y) (12) 5 In this paper, we require that word x and y occur in the same sentence. 1292 i−1 P (wi |wi−n+1 )) m i−n ∏ ∏ exp(P M I(wk , wi , i − k − 1)) i=n+1 k=1 (13) There are two components in his model. The first component is still the standard n-gram language model. The second one is the MI trigger model which multiples all exponential PMI values for trigger pairs where the current word is the triggered word and all preceding words outside the n-gram window of the cu"
P11-1129,D09-1117,0,0.277157,"build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a scenario where sentences are proceeded from the ending to the beginning. Duchateau et al. (2002) use the BLM score as a confidence measure to detect wrongly recognized words in speech recognition. Finch and Sumita (2009) use the BLM in their reverse translation decoder where source sentences are proceeded from the ending to the beginning. Our BLM is different from theirs in that we access the BLM during decoding (rather than after decoding) where source sentences are still proceeded from the beginning to the ending. Rosenfeld et al. (1994) introduce trigger pairs into a maximum entropy based language model as features. The trigger pairs are selected according to their mutual information. Zhou (2004) also propose an enhanced language model (MI-Ngram) which consists of a standard forward n-gram language model a"
P11-1129,N03-1017,0,0.482736,"= m ∏ P (wi |w1i−1 ) ≈ m ∏ i−1 ) (1) P (wi |wi−n+1 i=1 i=1 where the approximation is based on the nth order Markov assumption. In other words, when we predict the current word wi , we only consider the preceding n − 1 words wi−n+1 ...wi−1 instead of the whole context history w1 ...wi−1 . Different from the forward n-gram language model, the backward n-gram language model assigns a probability Pb (w1m ) to w1m by looking at the succeeding context according to Pb (w1m ) = m ∏ i=1 m )≈ P (wi |wi+1 m ∏ i+n−1 P (wi |wi+1 ) (2) i=1 3.1 Training et al., 2006) and 2) a standard phrase-based decoder (Koehn et al., 2003). Both decoders translate source sentences from the beginning of a sentence to the ending. Wu (1996) introduce a dynamic programming algorithm to integrate a forward bigram language model with inversion transduction grammar. His algorithm is then adapted and extended for integrating forward n-gram language models into synchronous CFGs by Chiang (2007). Our algorithms are different from theirs in two major aspects 1. The string input to the algorithms is in a reverse order. 2. We adopt a different way to calculate language model probabilities for partial hypotheses so that we can utilize incomp"
P11-1129,W04-3250,0,0.221159,"Missing"
P11-1129,D09-1022,0,0.0230483,"guage model and an MI trigger model. The latter model measures the mutual information of distancedependent trigger pairs. Our MI trigger model is mostly inspired by the work of these two papers, especially by Zhou’s MI-Ngram model (2004). The difference is that our model is distance-independent and, of course, we are interested in an SMT problem rather than a speech recognition one. Raybaud et al. (2009) use MI triggers in their confidence measures to assess the quality of translation results after decoding. Our method is different from theirs in the MI calculation and trigger pair selection. Mauser et al. (2009) propose bilingual triggers where two source words trigger one target word to 1 Language model adaptation is not very related to our work so we ignore it. improve lexical choice of target words. Our analysis (Section 6) show that our monolingual triggers can also help in the selection of target words. 3 Backward Language Model Given a sequence of words w1m = (w1 ...wm ), a standard forward n-gram language model assigns a probability Pf (w1m ) to w1m as follows. Pf (w1m ) = m ∏ P (wi |w1i−1 ) ≈ m ∏ i−1 ) (1) P (wi |wi−n+1 i=1 i=1 where the approximation is based on the nth order Markov assumpti"
P11-1129,P03-1021,0,0.0184105,"istance-dependent since trigger pairs (wk , wi ) are sensitive to their distance i − k − 1 (zero distance for adjacent words). Therefore the distance between word x and word y should be taken into account when calculating their PMI. In this paper, for simplicity, we adopt a distanceindependent MI trigger model as follows M I(w1m ) = m i−n ∏ ∏ exp(P M I(wk , wi )) (14) i=n+1 k=1 We integrate the MI trigger model into the loglinear model of machine translation as an additional knowledge source which complements the standard n-gram language model in capturing long-distance dependencies. By MERT (Och, 2003), we are even able to tune the weight of the MI trigger model against the weight of the standard n-gram language model while Zhou (2004) sets equal weights for both models. 4.1 Training We can use the maximum likelihood estimation method to calculate PMI for each trigger pair by taking counts from training data. Let C(x, y) be the co-occurrence count of the trigger pair (x, y) in the training data. The joint probability of (x, y) is calculated as C(x, y) x,y C(x, y) P (x, y) = ∑ (15) phrase-based decoder. But we still can handle it by dynamic programming as follows M I(e1 e2 ) = M I(e1 )M I(e2"
P11-1129,P02-1040,0,0.0811835,"English Gigaword corpus (306 million words). Firstly, we built a forward 5-gram language model using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. Then we trained a backward 5-gram language model on the same monolingual corpus in the way described in Section 3.1. Finally, we trained our MI trigger model still on this corpus according to the method in Section 4.1. The trained MI trigger model consists of 2.88M trigger pairs. We used the NIST MT03 evaluation test data as the development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU4 (Papineni et al., 2002) as the evaluation metric, which uses the shortest reference sentence length for the brevity penalty. Statistical significance in BLEU differences is tested by paired bootstrap re-sampling (Koehn, 2004). 5.3 Experimental Results The experimental results on the two NIST test sets are shown in Table 2. When we combine the backward language model with the forward language 7 LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07 and LDC2004T07. 1294 Model Forward (Baseline) Forward+Backward Forward+MI Forward+Backward+MI MT-04 35.67 36.16+ 36.00+ 36.76+ MT-05 34.41 34.9"
P11-1129,2008.amta-papers.16,0,0.0143574,"in more detail, describe the training procedures and explain how the models are integrated into the phrase-based decoder. Section 5 will empirically evaluate the effectiveness of these two models. Section 6 will conduct an indepth analysis. In the end, we conclude in Section 7. 2 Related Work Previous work devoted to improving language models in SMT mostly focus on two categories as we 1289 mentioned before1 : large language models (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007; Talbot and Osborne, 2007) and syntax-based language models (Charniak et al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philosophy is fundamentally different from them in that we build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a scenario where sentences are proceeded from the ending to the beginning. Duchateau et al. (2002) use the BLM score as a confide"
P11-1129,P08-1066,0,0.2483,"from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed manner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation. The second direction digs deeply into monolingual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine translation which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing long-distance dependencies. In this paper, we pursue the second direction without resorting to any linguistic resources such as a syntactic parser. With a belief that a language model that embraces a larger context provides better prediction ability, we learn additional information from training data to enhance conventional n-gram language models and extend their ability to capture richer contexts and"
P11-1129,P07-1065,0,0.060148,"ork. Section 3 and 4 will elaborate the backward language model and the MI trigger model respectively in more detail, describe the training procedures and explain how the models are integrated into the phrase-based decoder. Section 5 will empirically evaluate the effectiveness of these two models. Section 6 will conduct an indepth analysis. In the end, we conclude in Section 7. 2 Related Work Previous work devoted to improving language models in SMT mostly focus on two categories as we 1289 mentioned before1 : large language models (Zhang et al., 2006; Emami et al., 2007; Brants et al., 2007; Talbot and Osborne, 2007) and syntax-based language models (Charniak et al., 2003; Shen et al., 2008; Post and Gildea, 2008). Since our philosophy is fundamentally different from them in that we build contextually-informed language models by using backward n-grams and MI triggers, we discuss previous work that explore these two techniques (backward n-grams and MI triggers) in this section. Since the context “history” in the backward language model (BLM) is actually the future words to be generated, BLM is normally used in a postprocessing where all words have already been generated or in a scenario where sentences are"
P11-1129,P96-1021,0,0.127851,"arkov assumption. In other words, when we predict the current word wi , we only consider the preceding n − 1 words wi−n+1 ...wi−1 instead of the whole context history w1 ...wi−1 . Different from the forward n-gram language model, the backward n-gram language model assigns a probability Pb (w1m ) to w1m by looking at the succeeding context according to Pb (w1m ) = m ∏ i=1 m )≈ P (wi |wi+1 m ∏ i+n−1 P (wi |wi+1 ) (2) i=1 3.1 Training et al., 2006) and 2) a standard phrase-based decoder (Koehn et al., 2003). Both decoders translate source sentences from the beginning of a sentence to the ending. Wu (1996) introduce a dynamic programming algorithm to integrate a forward bigram language model with inversion transduction grammar. His algorithm is then adapted and extended for integrating forward n-gram language models into synchronous CFGs by Chiang (2007). Our algorithms are different from theirs in two major aspects 1. The string input to the algorithms is in a reverse order. 2. We adopt a different way to calculate language model probabilities for partial hypotheses so that we can utilize incomplete n-grams. Before we introduce the integration algorithms, we define three functions P, L, and R"
P11-1129,J97-3002,0,0.0282538,"age model without any other changes. To be consistent with training, we also need to reverse the order of translation hypotheses when we access the trained backward language model2 . Note that the Markov context history of Eq. (2) is wi+n−1 ...wi+1 instead of wi+1 ...wi+n−1 after we invert the order. The words are the same but the order is completely reversed. 3.2 Decoding In this section, we will present two algorithms to integrate the backward n-gram language model into two kinds of phrase-based decoders respectively: 1) a CKY-style decoder that adopts bracketing transduction grammar (BTG) (Wu, 1997; Xiong 2 This is different from the reverse decoding in (Finch and Sumita, 2009) where source sentences are reversed in the order. 1290 {z } b (3) This function consists of two parts: • The first part (a) calculates incomplete n-gram language model probabilities for word wk to wk−n+2 . That means, we calculate the unigram probability for wk (P (wk )), bigram probability for wk−1 (P (wk−1 |wk )) and so on until we take n − 1-gram probability for wk−n+2 (P (wk−n+2 |wk ...wk−n+3 )). This resembles the way in which the forward language model probability in the future cost is computed in the stand"
P11-1129,P06-1066,1,0.940398,"2 )P (b1 |b2 ) P (a3 )P (a2 |a3 ) P (a3 )P (a2 |a3 )P (a1 |a3 a2 ) P (b3 )P (b2 |b3 )P (b1 |b3 b2 ) P (b2 )P (b1 |b2 ) P (a3 |b2 b1 )P (a2 |b1 a3 ) P (b3 )P (b2 |b3 )P (b1 |b3 b2 ) P (a3 |b2 b1 )P (a2 |b1 a3 )P (a1 |a3 a2 ) Table 1: Values of P, L, and R in a 3-gram example . (4) P(e2 e1 ) = P(e1 )P(e2 ) (5) The L and R function return the leftmost and rightmost n − 1 words from a string in a reverse order respectively. Following Chiang (2007), we describe our algorithms in a deductive system. We firstly show the algorithm3 that integrates the backward language model into a BTG-style decoder (Xiong et al., 2006) in Figure 1. The item [A, i, j; l|r] indicates that a BTG node A has been constructed spanning from i to j on the source side with the leftmost|rightmost n − 1 words l|r on the target side. As mentioned before, all target strings assessed by the defined functions (P, L, and R) are in an inverted order (denoted by e). We only display the backward language model probability for each item, ignoring all other scores such as phrase translation probabilities. The Eq. (8) in Figure 1 shows how we calculate the backward language model probability for the axiom which applies a BTG lexicon rule to tran"
P11-1129,W06-1626,0,0.307465,"1 words. Along with the efforts that advance translation models from word-based paradigm to syntax-based philosophy, in recent years we have also witnessed increasing efforts dedicated to extend standard n-gram language models for SMT. We roughly categorize these efforts into two directions: data-volume-oriented and data-depth-oriented. In the first direction, more data is better. In order to benefit from monolingual corpora (LDC news data or news data collected from web pages) that consist of billions or even trillions of English words, huge language models are built in a distributed manner (Zhang et al., 2006; Brants et al., 2007). Such language models yield better translation results but at the cost of huge storage and high computation. The second direction digs deeply into monolingual data to build linguistically-informed language models. For example, Charniak et al. (2003) present a syntax-based language model for machine translation which is trained on syntactic parse trees. Again, Shen et al. (2008) explore a dependency language model to improve translation quality. To some extent, these syntactically-informed language models are consistent with syntax-based translation models in capturing lo"
P11-1129,C04-1014,0,0.354464,"use the BLM score as a confidence measure to detect wrongly recognized words in speech recognition. Finch and Sumita (2009) use the BLM in their reverse translation decoder where source sentences are proceeded from the ending to the beginning. Our BLM is different from theirs in that we access the BLM during decoding (rather than after decoding) where source sentences are still proceeded from the beginning to the ending. Rosenfeld et al. (1994) introduce trigger pairs into a maximum entropy based language model as features. The trigger pairs are selected according to their mutual information. Zhou (2004) also propose an enhanced language model (MI-Ngram) which consists of a standard forward n-gram language model and an MI trigger model. The latter model measures the mutual information of distancedependent trigger pairs. Our MI trigger model is mostly inspired by the work of these two papers, especially by Zhou’s MI-Ngram model (2004). The difference is that our model is distance-independent and, of course, we are interested in an SMT problem rather than a speech recognition one. Raybaud et al. (2009) use MI triggers in their confidence measures to assess the quality of translation results aft"
P12-1079,W09-0432,0,0.0127262,"y rule (c). To address such issue of the topic similarity model, we further introduce a topic sensitivity model to describe the topic sensitivity of a rule using entropy as a metric: Sensitivity(P (z|r)) (1) =− k=1 Hellinger function is used to calculate distribution distance and is popular in topic model (Blei and Lafferty, 2007).1 By topic similarity, we aim to encourage or penalize the application of a rule for a given document according to their topic distributions, which then helps the SMT system make better translation decisions. 3.2 Topic Sensitivity Domain adaptation (Wu et al., 2008; Bertoldi and Federico, 2009) often distinguishes general-domain data from in-domain data. Similarly, we divide the rules into topic-insensitive rules and topic-sensitive 1 We also try other distance functions, including Euclidean distance, Kullback-Leibler divergence and cosine function. They produce similar results in our preliminary experiments. 752 K ∑ P (z = k|r) × log (P (z = k|r)) (2) k=1 According to the Eq. (2), a topic-insensitive rule has a large entropy, while a topic-sensitive rule has a smaller entropy. By incorporating the topic sensitivity model with the topic similarity model, we enable our SMT system to"
P12-1079,2007.mtsummit-papers.11,0,0.0504094,"cific topic. A phrase pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and Future Work We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system. Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topicspecific lexicon translation. This verifies the advantage of exploiting topic model at t"
P12-1079,D08-1024,0,0.0143161,"one-to-one mapping between source-side and target-side topics. 6.5 Effect on Various Types of Rules To get a more detailed analysis of the result, we further compare the effect of our method on different types of rules. We divide the rules into three types: phrase rules, which only contain terminals and are the same as the phrase pairs in phrasebased system; monotone rules, which contain nonterminals and produce monotone translations; reordering rules, which also contain non-terminals but change the order of translations. We define the monotone and reordering rules according to Chiang et al., (2008). Table 5 show the results. We can see that our method achieves improvements on all the three types of rules. Our topic similarity method on monotone rule achieves the most improvement which is 0.6 B LEU points, while the improvement on reordering rules is the smallest among the three types. This shows that topic information also helps the selections of rules with non-terminals. 7 Related Work In addition to the topic-specific lexicon translation method mentioned in the previous sections, researchers also explore topic model for machine translation in other ways. Foster and Kunh (2007) describ"
P12-1079,J07-2003,0,0.909756,"nstitute of Computing Technology Institute for Infocomm Research Chinese Academy of Sciences {xiaoxinyan, liuqun, sxlin}@ict.ac.cn Abstract by these probabilities. However, the state-of-theart SMT systems translate sentences by using sequences of synchronous rules or phrases, instead of translating word by word. Since a synchronous rule is rarely factorized into individual words, we believe that it is more reasonable to incorporate the topic model directly at the rule level rather than the word level. Consequently, we propose a topic similarity model for hierarchical phrase-based translation (Chiang, 2007), where each synchronous rule is associated with a topic distribution. In particular, Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given doc"
P12-1079,W07-0717,0,0.166869,"Missing"
P12-1079,C08-1041,1,0.127263,"pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and Future Work We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system. Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topicspecific lexicon translation. This verifies the advantage of exploiting topic model at the rule level ove"
P12-1079,N03-1017,0,0.025585,"which comes from the FBIS portion of LDC data. There are 10,947 documents in the FBIS corpus. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST B LEU (Papineni et al., 2002) was used to mea755 sure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are est"
P12-1079,W04-3250,0,0.229373,"2.29 22.69 22.39 22.69 22.92 Avg 26.07 26.47 26.55 26.45 26.71 26.94 Speed 12.6 3.3 11.5 11.7 11.2 10.2 Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the traditional hierarchical system (“Baseline”) and the topic-specific lexicon translation method (“TopicLex”). “SimSrc” and “SimTgt” denote similarity by source-side and target-side rule-distribution respectively, while “Sim+Sen” activates the two similarity and two sensitivity features. “Avg” is the average B LEU score on the two test sets. Scores marked in bold mean significantly (Koehn, 2004) better than Baseline (p &lt; 0.01). 2. Is it helpful to introduce the topic sensitivity model to distinguish topic-insensitive and topic-sensitive rules? 3. Is it necessary to project topics by one-to-many correspondence instead of one-to-one correspondence? 4. What is the effect of our method on various types of rules, such as phrase rules and rules with non-terminals? 6.1 Data We present our experiments on the NIST ChineseEnglish translation tasks. The bilingual training data contains 239K sentence pairs with 6.9M Chinese words and 9.14M English words, which comes from the FBIS portion of LDC"
P12-1079,D09-1092,0,0.0451605,"Missing"
P12-1079,P02-1038,0,0.0955267,"Missing"
P12-1079,J03-1002,0,0.00274223,"al training data contains 239K sentence pairs with 6.9M Chinese words and 9.14M English words, which comes from the FBIS portion of LDC data. There are 10,947 documents in the FBIS corpus. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST B LEU (Papineni et al., 2002) was used to mea755 sure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampl"
P12-1079,P03-1021,0,0.0245589,"6/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST B LEU (Papineni et al., 2002) was used to mea755 sure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are estimated from the Chinese part and English part of FBIS corpus respectively. We set the number of topic K = 30 for both source-side and target-side, and use the default setting of the tool for training and inference.4 During decoding, we first infer the topic distribution of given documents before translation accord"
P12-1079,P02-1040,0,0.0984212,"ns 238M English words. We used the NIST evaluation set of 2005 (MT05) as our development set, and sets of MT06/MT08 as test sets. The numbers of documents in MT05, MT06, MT08 are 100, 79, and 109 respectively. We obtained symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2003) in both directions and then applying refinement rule “grow-diag-final-and” (Koehn et al., 2003). The SCFG rules are extracted from this word-aligned training data. A 4-gram language model was trained on the monolingual data by the SRILM toolkit (Stolcke, 2002). Case-insensitive NIST B LEU (Papineni et al., 2002) was used to mea755 sure translation performance. We used minimum error rate training (Och, 2003) for optimizing the feature weights. For the topic model, we used the open source LDA tool GibbsLDA++ for estimation and inference.3 GibssLDA++ is an implementation of LDA using gibbs sampling for parameter estimation and inference. The source-side and target-side topic models are estimated from the Chinese part and English part of FBIS corpus respectively. We set the number of topic K = 30 for both source-side and target-side, and use the default setting of the tool for training and inference.4 Du"
P12-1079,W11-2133,0,0.168652,". Finally, they 757 combine a specific domain translation model with a general domain translation model depending on various text distances. One way to calculate the distance is using topic model. Gong et al. (2010) introduce topic model for filtering topic-mismatched phrase pairs. They first assign a specific topic for the document to be translated. Similarly, each phrase pair is also assigned with one specific topic. A phrase pair will be discarded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and"
P12-1079,D09-1008,0,0.0603759,"arded if its topic mismatches the document topic. Researchers also introduce topic model for crosslingual language model adaptation (Tam et al., 2007; Ruiz and Federico, 2011). They use bilingual topic model to project latent topic distribution across languages. Based on the bilingual topic model, they apply the source-side topic weights into the target-side topic model, and adapt the n-gram language model of target side. Our topic similarity model uses the document topic information. From this point, our work is related to context-dependent translation (Carpuat and Wu, 2007; He et al., 2008; Shen et al., 2009). Previous work typically use neighboring words and sentence level information, while our work extents the context into the document level. 8 Conclusion and Future Work We have presented a topic similarity model which incorporates the rule-topic distributions on both the source and target side into traditional hierarchical phrase-based system. Our experimental results show that our model achieves a better performance with faster decoding speed than previous work on topicspecific lexicon translation. This verifies the advantage of exploiting topic model at the rule level over the word level. Fu"
P12-1079,C08-1125,0,0.0128983,"one-to-one mapping between source-side and target-side topics. 6.5 Effect on Various Types of Rules To get a more detailed analysis of the result, we further compare the effect of our method on different types of rules. We divide the rules into three types: phrase rules, which only contain terminals and are the same as the phrase pairs in phrasebased system; monotone rules, which contain nonterminals and produce monotone translations; reordering rules, which also contain non-terminals but change the order of translations. We define the monotone and reordering rules according to Chiang et al., (2008). Table 5 show the results. We can see that our method achieves improvements on all the three types of rules. Our topic similarity method on monotone rule achieves the most improvement which is 0.6 B LEU points, while the improvement on reordering rules is the smallest among the three types. This shows that topic information also helps the selections of rules with non-terminals. 7 Related Work In addition to the topic-specific lexicon translation method mentioned in the previous sections, researchers also explore topic model for machine translation in other ways. Foster and Kunh (2007) describ"
P12-1079,P06-2124,0,0.674686,"aches that work at the word level. 1 • Given a document to be translated, we calculate the topic similarity between a rule and the document based on their topic distributions. We augment the hierarchical phrase-based system by integrating the proposed topic similarity model as a new feature (Section 3.1). Introduction Topic model (Hofmann, 1999; Blei et al., 2003) is a popular technique for discovering the underlying topic structure of documents. To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality. Topic-specific lexicon translation models focus on word-level translations. Such models first estimate word translation probabilities conditioned on topics, and then adapt lexical weights of phrases ∗ {dyxiong, mzhang∗ }@i2r.a-star.edu.sg Corresponding author • As we will discuss in Section 3.2, the similarity between a generic rule and a given source document computed by our topic similarity model is often very low. We don’t want to penalize these generic rules. Therefore we further propose a topic sensitivity model whic"
P12-1095,W11-2136,0,0.123696,"rectly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). There are other previous studies that explore only source side predicate-argument structures. Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. Wu et al. (2011) automate this procedure by automatically extracting reordering rules from predicate-argument structures and applying these rules to reorder source language sentences. Aziz et al. (2011) incorporate source language semantic role labels into a tree-to-string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argument movement (rel903 ative to its predicate) prediction problem and use a discriminatively trained classifier for such predictions. Our predicate translation model is also related to prev"
P12-1095,J96-1002,0,0.0224461,"s into a tree-to-string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argument movement (rel903 ative to its predicate) prediction problem and use a discriminatively trained classifier for such predictions. Our predicate translation model is also related to previous discriminative lexicon translation models (Berger et al., 1996; Venkatapathy and Bangalore, 2007; Mauser et al., 2009). While previous models predict translations for all words in vocabulary, we only focus on verbal predicates. This will tremendously reduce the amount of training data required, which usually is a problem in discriminative lexicon translation models (Mauser et al., 2009). Furthermore, the proposed translation model also differs from previous lexicon translation models in that we use both lexical and semantic features. Our experimental results show that semantic features are able to further improve translation accuracy. 3 Predicate Transla"
P12-1095,J07-2003,0,0.609026,"tion 3.3 to train the maximum entropy classifier as formulated in Eq. (4). We perform 100 iterations of L-BFGS. 5 Integrating the Two Models into SMT In this section, we elaborate how to integrate the two models into phrase-based SMT. In particular, we integrate the models into a phrase-based system which uses bracketing transduction grammars (BTG) (Wu, 1997) for phrasal translation (Xiong et al., 2006). Since the system is based on a CKY-style decoder, the integration algorithms introduced here can be easily adapted to other CKY-based decoding systems such as the hierarchical phrasal system (Chiang, 2007). 5.1 Integrating the Predicate Translation Model It is straightforward to integrate the predicate translation model into phrase-based SMT (Koehn et al., 906 2003; Xiong et al., 2006). We maintain word alignments for each phrase pair in the phrase table. Given a source sentence with its predicateargument structure, we detect all verbal predicates and load trained predicate translation classifiers for these verbs. Whenever a hypothesis covers a new verbal predicate v, we find the target translation e for v through word alignments and then calculate its translation probability pt (e|C(v)) accord"
P12-1095,N03-1017,0,0.107241,"(v).Ah1 = d e = adjourn and C(v).Ar2 = null e = adjourn and C(v).Ah3 = null This will increase the number of classes to be predicted by the maximum entropy classifier. But according to our observation, it is still computationally tractable (see Section 3.3). If a verbal predicate is not translated, we set e = NULL so that we can also capture null translations for verbal predicates. Table 1: Semantic feature examples. 3.2 Features The apparent advantage of discriminative lexicon translation models over generative translation models (e.g., conventional lexical translation model as described in (Koehn et al., 2003)) is that discriminative models allow us to integrate richer contexts (lexical, syntactic or semantic) into target translation prediction. We use two kinds of features to predict translations for verbal predicates: 1) lexical features and 2) semantic features. All features are in the following binary form. f (e, C(v)) =  1, if e = ♣ and C(v).♥ = ♠ 0, else (3) where the symbol ♣ is a placeholder for a possible target translation (up to 4 words), the symbol ♥ indicates a contextual (lexical or semantic) element for the verbal predicate v, and the symbol ♠ represents the value of ♥. Lexical Feat"
P12-1095,W04-3250,0,0.219224,"ntic role labeler6 (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbal predicates. After we obtained semantic roles on the source side, we extracted features as described in Section 3.2 and 4.2 and used these features to train our two models as described in Section 3.3 and 4.3. We used the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as the evaluation metric. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). 6.2 Results Our first group of experiments is to investigate whether the predicate translation model is able to improve translation accuracy in terms of BLEU and whether semantic features are useful. The experimental results are shown in Table 4. From the table, we have the following two observations. • The proposed predicate translation models achieve an average improvement of 0.57 BLEU points across the two NIST test sets when all features (lex+sem) are used. Such an improvement is statistically significant (p < 0.01). According to our statistics, there are 5.07 verbal predicates per sente"
P12-1095,2006.iwslt-evaluation.11,0,0.121648,"ork. As PAS analysis widely employs global and sentence-wide features, it is computationally expensive to integrate target side predicateargument structures into the dynamic programming style of SMT decoding (Wu and Fung, 2009b). Therefore they either postpone the integration of target side PASs until the whole decoding procedure is completed (Wu and Fung, 2009b), or directly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). There are other previous studies that explore only source side predicate-argument structures. Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. Wu et al. (2011) automate this procedure by automatically extracting reordering rules from predicate-argument structures and applying these rules to reorder source language sentences. Aziz et al. (2011) incorporate source language semantic role labels into a tree-to-string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separa"
P12-1095,P10-1113,0,0.215021,"Missing"
P12-1095,C10-1081,0,0.636321,"nt reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English translation tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy. 1 Introduction Recent years have witnessed increasing efforts towards integrating predicate-argument structures into statistical machine translation (SMT) (Wu and Fung, 2009b; Liu and Gildea, 2010). In this paper, we take a step forward by introducing a novel approach to incorporate such semantic structures into SMT. Given a source side predicate-argument structure, we attempt to translate each semantic frame (predicate and its associated arguments) into an appropriate target string. We believe that the translation of predicates and reordering of arguments are the two central ∗ issues concerning the transfer of predicate-argument structure across languages. Predicates1 are essential elements in sentences. Unfortunately they are usually neither correctly translated nor translated at all"
P12-1095,D09-1022,0,0.0179658,"cus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argument movement (rel903 ative to its predicate) prediction problem and use a discriminatively trained classifier for such predictions. Our predicate translation model is also related to previous discriminative lexicon translation models (Berger et al., 1996; Venkatapathy and Bangalore, 2007; Mauser et al., 2009). While previous models predict translations for all words in vocabulary, we only focus on verbal predicates. This will tremendously reduce the amount of training data required, which usually is a problem in discriminative lexicon translation models (Mauser et al., 2009). Furthermore, the proposed translation model also differs from previous lexicon translation models in that we use both lexical and semantic features. Our experimental results show that semantic features are able to further improve translation accuracy. 3 Predicate Translation Model In this section, we present the features and"
P12-1095,P02-1040,0,0.0970476,"ordering model, we first parsed all source sentences using the Berkeley Chinese parser (Petrov et al., 2006) and then ran the Chinese semantic role labeler6 (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbal predicates. After we obtained semantic roles on the source side, we extracted features as described in Section 3.2 and 4.2 and used these features to train our two models as described in Section 3.3 and 4.3. We used the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as the evaluation metric. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). 6.2 Results Our first group of experiments is to investigate whether the predicate translation model is able to improve translation accuracy in terms of BLEU and whether semantic features are useful. The experimental results are shown in Table 4. From the table, we have the following two observations. • The proposed predicate translation models achieve an average improvement of 0.57 BLEU points across the two NIST test sets when all features (lex+sem) are used. Such"
P12-1095,P06-1055,0,0.00611792,"rs with 96.9M Chinese words and 109.5M English words. We ran GIZA++ on these corpora in both directions and then applied the “grow-diag-final” refinement rule to obtain word alignments. We then used all these word-aligned corpora to generate our phrase table. Our 5-gram language model was trained on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. To train the proposed predicate translation model and argument reordering model, we first parsed all source sentences using the Berkeley Chinese parser (Petrov et al., 2006) and then ran the Chinese semantic role labeler6 (Li et al., 2010) on all source parse trees to annotate semantic roles for all verbal predicates. After we obtained semantic roles on the source side, we extracted features as described in Section 3.2 and 4.2 and used these features to train our two models as described in Section 3.3 and 4.3. We used the NIST MT03 evaluation test data as our development set, and the NIST MT04, MT05 as the test sets. We adopted the case-insensitive BLEU-4 (Papineni et al., 2002) as the evaluation metric. Statistical significance in BLEU differences was tested by"
P12-1095,W07-0413,0,0.0437434,"Missing"
P12-1095,2009.eamt-1.30,0,0.497454,"redicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English translation tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy. 1 Introduction Recent years have witnessed increasing efforts towards integrating predicate-argument structures into statistical machine translation (SMT) (Wu and Fung, 2009b; Liu and Gildea, 2010). In this paper, we take a step forward by introducing a novel approach to incorporate such semantic structures into SMT. Given a source side predicate-argument structure, we attempt to translate each semantic frame (predicate and its associated arguments) into an appropriate target string. We believe that the translation of predicates and reordering of arguments are the two central ∗ issues concerning the transfer of predicate-argument structure across languages. Predicates1 are essential elements in sentences. Unfortunately they are usually neither correctly translate"
P12-1095,N09-2004,0,0.55119,"redicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English translation tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy. 1 Introduction Recent years have witnessed increasing efforts towards integrating predicate-argument structures into statistical machine translation (SMT) (Wu and Fung, 2009b; Liu and Gildea, 2010). In this paper, we take a step forward by introducing a novel approach to incorporate such semantic structures into SMT. Given a source side predicate-argument structure, we attempt to translate each semantic frame (predicate and its associated arguments) into an appropriate target string. We believe that the translation of predicates and reordering of arguments are the two central ∗ issues concerning the transfer of predicate-argument structure across languages. Predicates1 are essential elements in sentences. Unfortunately they are usually neither correctly translate"
P12-1095,I11-1004,0,0.200715,"yle of SMT decoding (Wu and Fung, 2009b). Therefore they either postpone the integration of target side PASs until the whole decoding procedure is completed (Wu and Fung, 2009b), or directly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). There are other previous studies that explore only source side predicate-argument structures. Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. Wu et al. (2011) automate this procedure by automatically extracting reordering rules from predicate-argument structures and applying these rules to reorder source language sentences. Aziz et al. (2011) incorporate source language semantic role labels into a tree-to-string SMT system. Although we also focus on source side predicateargument structures, our models differ from the previous work in two main aspects: 1) we propose two separate discriminative models to exploit predicateargument structures for predicate translation and argument reordering respectively; 2) we consider argument reordering as an argume"
P12-1095,J97-3002,0,0.28733,"Missing"
P12-1095,P06-1066,1,0.600139,"): the function finds all predicateargument pairs that cross the two neighboring spans (i, k) and (k + 1, j). S It can be formulated as A(i, j, τ ) − (A(i, k, τ ) A(k + 1, j, τ )). We then define another function Pr to calculate the argument reordering model probability on all arguments which are found by the previous two functions A and N as follows. Y Pr (B) = pr (mA |C(A)) (6) A∈B where B denotes either A or N . Following (Chiang, 2007), we describe the algorithm in a deductive system. It is shown in Figure 2. The algorithm integrates the argument reordering model into a CKY-style decoder (Xiong et al., 2006). The item [X, i, j] denotes a BTG node X spanning from i to j on the source side. For notational convenience, we only show the argument reordering model probability for each item, ignoring all other sub-model probabilities such as the language model probability. The Eq. (7) shows how we calculate the argument reordering model probability when a lexical rule is applied to translate a source phrase c to a target phrase e. The Eq. (8) shows how we compute the argument reordering model probability for a span (i, j) in a dynamic programming manner when a merging rule is applied to combine its two"
P12-1095,J08-2004,0,0.0947935,"tly translated nor translated at all in many SMT systems according to the error study by Wu and Fung (2009a). This suggests that conventional lexical and phrasal translation models adopted in those SMT systems are not sufficient to correctly translate predicates in source sentences. Thus we propose a discriminative, feature-based predicate translation model that captures not only lexical information (i.e., surrounding words) but also high-level semantic contexts to correctly translate predicates. Arguments contain information for questions of who, what, when, where, why, and how in sentences (Xue, 2008). One common error in translating arguments is about their reorderings: arguments are placed at incorrect positions after translation. In order to reduce such errors, we introduce a discriminative argument reordering model that uses the position of a predicate as the reference axis to estimate positions of its associated arguments on the target side. In this way, the model predicts moving directions of arguments relative to their predicates with semantic features. We integrate these two discriminative models into a state-of-the-art phrase-based system. Experimental results on large-scale Chine"
P13-2068,D11-1084,0,0.0638986,"utput, cohesion has served as a high level quality criterion in post-editing (Vasconcellos, 1989). As a part of COMTIS project, grammatical cohesion is integrated into machine translation models to capture inter-sentential links (Cartoni et al., 2011). Wong and Kit (2012) incorporate lexical cohesion to machine translation evaluation metrics to evaluate document-level machine translation quality. Xiong et al. (2013) integrate various target-side lexical cohesion devices into document-level machine translation. Lexical cohesion is also partially explored in the cachebased translation models of Gong et al. (2011) and translation consistency constraints of Xiao et al. Corresponding author 382 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382–386, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics s(w). Near-synonym set s1 is defined as the union of all synsets that are defined by the function s(w) where w∈ s0 . It can be formulated as follows. [ s(w) (1) s1 = (2011). All previous methods on lexical cohesion for document-level machine translation as mentioned above have one thing in common, which is that they do not use any"
P13-2068,N03-1017,0,0.0466197,"Missing"
P13-2068,J03-1002,0,0.00769993,"Missing"
P13-2068,P03-1021,0,0.180049,"Missing"
P13-2068,P02-1040,0,0.0858031,"Missing"
P13-2068,D12-1097,0,0.471355,"ation. We integrate the model into hierarchical phrase-based machine translation and achieve an absolute improvement of 0.85 BLEU points on average over the baseline on NIST Chinese-English test sets. 1 Introduction Current statistical machine translation (SMT) systems are mostly sentence-based. The major drawback of such a sentence-based translation fashion is the neglect of inter-sentential dependencies. As a linguistic means to establish inter-sentential links, lexical cohesion ties sentences together into a meaningfully interwoven structure through words with the same or related meanings (Wong and Kit, 2012). This paper studies lexical cohesion devices and incorporate them into document-level machine translation. We propose a bilingual lexical cohesion trigger model to capture lexical cohesion for document-level SMT. We consider a lexical cohesion item in the source language and its corresponding counterpart in the target language as a trigger pair, in which we treat the source language lexical cohesion item as the trigger and its target language counterpart as the triggered item. Then we use mutual information to measure the strength of the dependency between the trigger and triggered item. We i"
P13-2068,2011.mtsummit-papers.13,0,0.354878,"Missing"
P13-2068,P11-1129,1,0.892762,"Missing"
P13-2068,J07-2003,0,\N,Missing
P14-1137,P07-1020,0,0.0342617,"l for rule selection in hierarchical phrasebased translation. Xiong and Zhang (2013) employ a sentence-level topic model to capture coherence for document-level machine translation. The difference between our work and these previous studies on topic model for SMT lies in that we adopt topic-based WSI to obtain word senses rather than generic topics and integrate induced word senses into machine translation. Lexical selection Our work is also related to lexical selection in SMT where appropriate target lexical items for source words are selected by a statistical model with context information (Bangalore et al., 2007; Mauser et al., 2009). The reformulated WSD discussed before can also be considered as a lexical selection model. The significant difference from these studies is that we perform lexical selection using automatically induced word senses by the HDP on the source side. 7 Conclusion We have presented a sense-based translation model that integrates word senses into machine translation. We capitalize on the broad-coverage word sense induction system that is built on the nonparametric Bayesian HDP to learn sense clusters for words in the source language. We generate pseudo documents for word tokens"
P14-1137,J96-1002,0,0.0724325,"xtract both the lexicon and sense features from a ±k-word window centered on the word c. The lexicon features are defined as the preceding k words, the succeeding k words and the word c itself: {c−k , ..., c−1 , c, c1 , ..., ck }. The sense features are defined as the predicted senses for these words: {sc−k , ..., sc−1 , sc , sc1 , ..., sck }. As we also use these neighboring words to predict word senses in the HDP-based WSI, the information provided by the lexicon and sense features may overlap. This is not a issue for the MaxEnt classifier as it can deal with arbitrary overlapping features (Berger et al., 1996). One may also wonder whether the sense features can contribute to SMT new information that can NOT be obtained from the lexicon features. First, we believe that the senses induced by the HDP-based WSI provide a different view of data than that of the lexicon features. Second, the sense features contain semantic distributional information learned by the HDP across contexts where lexical words occur. Third, we empirically investigate this doubt by comparing two MaxEnt-based translation models 1462 in Section 5. One model only uses the lexicon features while the other integrates both the lexicon"
P14-1137,W07-0717,0,0.0559068,"Missing"
P14-1137,E09-1013,0,0.559708,"word tokens occur. The biggest difference from word sense disambiguation lies in that WSI does not rely on a predefined sense inventory. Such a prespecified list of senses is normally assumed by WSD which predicts senses of word tokens using this given inventory. From this perspective, WSI can be treated as a clustering problem while WSD a classification one. Various clustering algorithms, such as k-means, have been previously used for WSI. Recently, we have also witnessed that WSI is cast as a topic modeling problem where the sense clusters of a word type are considered as underlying topics (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). We follow this line to tailor a topic modeling framework to induce word senses for our large-scale training data. In the topic-based WSI, surrounding context of a word token is considered as a pseudo document of the corresponding word type. A pseudo document is composed of either a bag of neighboring words of a word token, or the Part-to-Speech tags of neighboring words, or other contextual information elements. In this paper, we define a pseudo 1460 document as ±N neighboring words centered on a given word token. Table 1 shows examples of pseudo docum"
P14-1137,N03-1017,0,0.0180357,"ated WSD for SMT? 5.1 Setup Our baseline system is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (Xiong et al., 2006). We used LDC corpora LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07, LDC2004T07 as our bilingual training data which consists of 3.84M bilingual sentences, 109.5M English word tokens and 96.9M Chinese word tokens. We ran Giza++ on the training data in two directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. From the word-aligned data, we extracted weighted phrase pairs to generate our phrase table. We trained a 5-gram language model on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit (Stolcke, 2002) with the modified Kneser-Ney smoothing (Chen and Goodman, 1996). We trained our HDP-based WSI models via the C++ HDP toolkit3 (Wang and Blei, 2012). We set the hyperparameters γ = 0.1 and α0 = 1.0 following Lau et al. (2012).We extracted pseudo documents from a ±10-word window centered on the corresponding word token for each wor"
P14-1137,P05-1048,0,0.774466,"anslation task. Results show that the proposed model substantially outperforms not only the baseline but also the previous reformulated word sense disambiguation. 1 Introduction One of very common phenomena in language is that a plenty of words have multiple meanings. In the context of machine translation, such different meanings normally produce different target translations. Therefore a natural assumption is that word sense disambiguation (WSD) may contribute to statistical machine translation (SMT) by providing appropriate word senses for target translation selection with context features (Carpuat and Wu, 2005). ∗ Corresponding author This assumption, however, has not been empirically verified in the early days. Carpuat and Wu (2005) adopt a standard formulation of WSD: predicting word senses that are defined on an ontology for ambiguous words. As they apply WSD to Chinese-to-English translation, they predict word senses from a Chinese ontology HowNet and project the predicted senses to English glosses provided by HowNet. These glosses, used as the sense predictions of their WSD system, are integrated into a word-based SMT system either to substitute for translation candidates of their translation m"
P14-1137,E12-1060,0,0.390364,"m word sense disambiguation lies in that WSI does not rely on a predefined sense inventory. Such a prespecified list of senses is normally assumed by WSD which predicts senses of word tokens using this given inventory. From this perspective, WSI can be treated as a clustering problem while WSD a classification one. Various clustering algorithms, such as k-means, have been previously used for WSI. Recently, we have also witnessed that WSI is cast as a topic modeling problem where the sense clusters of a word type are considered as underlying topics (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). We follow this line to tailor a topic modeling framework to induce word senses for our large-scale training data. In the topic-based WSI, surrounding context of a word token is considered as a pseudo document of the corresponding word type. A pseudo document is composed of either a bag of neighboring words of a word token, or the Part-to-Speech tags of neighboring words, or other contextual information elements. In this paper, we define a pseudo 1460 document as ±N neighboring words centered on a given word token. Table 1 shows examples of pseudo documents for a Chinese word “wǎngluò” (netwo"
P14-1137,D07-1007,0,0.816903,"didates of their translation model or to postedit the output of their SMT system. They report that WSD degenerates the translation quality of SMT. In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulate the task of WSD for SMT as predicting possible target translations rather than senses for ambiguous source words. They show that such a reformulated WSD can improve the accuracy of a simplified word translation task. Following this WSD reformulation for SMT, Chan et al. (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). Carpuat and Wu (2007) also use this reformulated WSD and further adapt it to multi-word phrasal disambiguation. They both report that the redefined WSD can significantly improve SMT. Although this reformulated WSD has proved helpful for SMT, one question is not answered yet: are pure word senses useful for SMT? The early WSD for SMT (Carpuat and Wu, 2005) uses projected word senses while the reformulated WSD sidesteps word senses. In this paper we would like to re-investigate this question by resorting to word sense induction (WSI) that is related to but different from WSD.1 We use 1 We will discuss the relation a"
P14-1137,D09-1022,0,0.148492,"Missing"
P14-1137,P07-1005,0,0.849062,"edictions of their WSD system, are integrated into a word-based SMT system either to substitute for translation candidates of their translation model or to postedit the output of their SMT system. They report that WSD degenerates the translation quality of SMT. In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulate the task of WSD for SMT as predicting possible target translations rather than senses for ambiguous source words. They show that such a reformulated WSD can improve the accuracy of a simplified word translation task. Following this WSD reformulation for SMT, Chan et al. (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). Carpuat and Wu (2007) also use this reformulated WSD and further adapt it to multi-word phrasal disambiguation. They both report that the redefined WSD can significantly improve SMT. Although this reformulated WSD has proved helpful for SMT, one question is not answered yet: are pure word senses useful for SMT? The early WSD for SMT (Carpuat and Wu, 2005) uses projected word senses while the reformulated WSD sidesteps word senses. In this paper we would like to re-investigate this question by resor"
P14-1137,P96-1041,0,0.142489,"DC2005T06, LDC2003E07, LDC2004T07 as our bilingual training data which consists of 3.84M bilingual sentences, 109.5M English word tokens and 96.9M Chinese word tokens. We ran Giza++ on the training data in two directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. From the word-aligned data, we extracted weighted phrase pairs to generate our phrase table. We trained a 5-gram language model on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit (Stolcke, 2002) with the modified Kneser-Ney smoothing (Chen and Goodman, 1996). We trained our HDP-based WSI models via the C++ HDP toolkit3 (Wang and Blei, 2012). We set the hyperparameters γ = 0.1 and α0 = 1.0 following Lau et al. (2012).We extracted pseudo documents from a ±10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). As described in Section 2.2, we preprocessed the source part of our bilingual training data by removing stop words and infrequent words that occurs less than 3 http://www.cs.cmu.edu/˜chongw/ resource.html # Word Types # Total Pseudo Documents # Avg Pseudo Documents # Total Senses # Avg Sen"
P14-1137,P05-1033,0,0.0277485,"translation candidates of their translation model or to postedit the output of their SMT system. They report that WSD degenerates the translation quality of SMT. In contrast to the standard WSD formulation, Vickrey et al. (2005) reformulate the task of WSD for SMT as predicting possible target translations rather than senses for ambiguous source words. They show that such a reformulated WSD can improve the accuracy of a simplified word translation task. Following this WSD reformulation for SMT, Chan et al. (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). Carpuat and Wu (2007) also use this reformulated WSD and further adapt it to multi-word phrasal disambiguation. They both report that the redefined WSD can significantly improve SMT. Although this reformulated WSD has proved helpful for SMT, one question is not answered yet: are pure word senses useful for SMT? The early WSD for SMT (Carpuat and Wu, 2005) uses projected word senses while the reformulated WSD sidesteps word senses. In this paper we would like to re-investigate this question by resorting to word sense induction (WSI) that is related to but different from WSD.1 We use 1 We will"
P14-1137,P11-2031,0,0.00652729,"ta as described in Section 3.2. We set the Gaussian prior to 1 to avoid overfitting. On average, we obtained 346 classes (target translations) per source word type with the maximum number of classes being 256,243. It took an average of 57.5 seconds for training a Maxent classifier. We used the NIST MT03 evaluation test data as our development set, and the NIST MT05 as the test set. We evaluated translation quality with the case-insensitive BLEU-4 (Papineni et al., 2002) and NIST (Doddington, 2002). In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al. (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments. 5.2 Statistics and Examples of Word Senses Before we present our experiment results of the sense-based translation model, we study some statistics of the HDP-based WSI on the training and test data. We show these statistics in Table 2. There are 67,723 and 4,348 unique word types in the training and test data after the preprocessing step. For these word types, we extract 27.73M and 11,777 pseudo documents from the training and test set respectively. On average, there are 427.79 4 http://ho"
P14-1137,P02-1040,0,0.101344,"We performed 100 iterations of the L-BFGS algorithm implemented in the training toolkit on the collected training events from the sense-annotated data as described in Section 3.2. We set the Gaussian prior to 1 to avoid overfitting. On average, we obtained 346 classes (target translations) per source word type with the maximum number of classes being 256,243. It took an average of 57.5 seconds for training a Maxent classifier. We used the NIST MT03 evaluation test data as our development set, and the NIST MT05 as the test set. We evaluated translation quality with the case-insensitive BLEU-4 (Papineni et al., 2002) and NIST (Doddington, 2002). In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al. (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments. 5.2 Statistics and Examples of Word Senses Before we present our experiment results of the sense-based translation model, we study some statistics of the HDP-based WSI on the training and test data. We show these statistics in Table 2. There are 67,723 and 4,348 unique word types in the training and test data after the preprocessing step. For thes"
P14-1137,J97-3002,0,0.0186056,"n using large-scale bilingual training data. In order to build the proposed sense-based translation model, we annotated the source part of the bilingual training data with word senses induced by the HDPbased WSI. With the trained sense-based translation model, we would like to investigate the following two questions: • Do word senses automatically induced by the HDP-based WSI improve translation quality? • Does the sense-based translation model outperform the reformulated WSD for SMT? 5.1 Setup Our baseline system is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (Xiong et al., 2006). We used LDC corpora LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07, LDC2004T07 as our bilingual training data which consists of 3.84M bilingual sentences, 109.5M English word tokens and 96.9M Chinese word tokens. We ran Giza++ on the training data in two directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. From the word-aligned data, we extracted weighted phrase pairs to generate our phrase tabl"
P14-1137,P12-1079,1,0.588216,"ction and empirically show that the HDP-based WSI is better than the LDA-based WSI. We follow them to set the hyperparameters of HDP for training and incorporate automatically induced word senses into SMT in our work. Topic model for SMT Generic topic models are also explored for SMT. Zhao and Xing (2007) propose a bilingual topic model and integrate a topic-specific lexicon translation model into SMT. Tam et al. (2007) also explore a bilingual topic model for translation and language model adaptation. Foster and Kunh (2007) introduce a mixture model approach for translation model adaptation. Xiao et al. (2012) propose a topic-based similarity model for rule selection in hierarchical phrasebased translation. Xiong and Zhang (2013) employ a sentence-level topic model to capture coherence for document-level machine translation. The difference between our work and these previous studies on topic model for SMT lies in that we adopt topic-based WSI to obtain word senses rather than generic topics and integrate induced word senses into machine translation. Lexical selection Our work is also related to lexical selection in SMT where appropriate target lexical items for source words are selected by a statis"
P14-1137,P06-1066,1,0.659265,"n model, we annotated the source part of the bilingual training data with word senses induced by the HDPbased WSI. With the trained sense-based translation model, we would like to investigate the following two questions: • Do word senses automatically induced by the HDP-based WSI improve translation quality? • Does the sense-based translation model outperform the reformulated WSD for SMT? 5.1 Setup Our baseline system is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (Xiong et al., 2006). We used LDC corpora LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07, LDC2004T07 as our bilingual training data which consists of 3.84M bilingual sentences, 109.5M English word tokens and 96.9M Chinese word tokens. We ran Giza++ on the training data in two directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments. From the word-aligned data, we extracted weighted phrase pairs to generate our phrase table. We trained a 5-gram language model on the Xinhua section of the English Gigaword corpus (306 million word"
P14-1137,W11-1102,0,0.106183,"Missing"
P14-1137,H05-1097,0,\N,Missing
P14-1137,P03-1021,0,\N,Missing
P15-1023,W11-1014,0,0.0265826,"Missing"
P15-1023,D07-1007,0,0.24854,"ish translation example to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish transl"
P15-1023,P07-1005,0,0.0775738,"Missing"
P15-1023,P11-2031,0,0.106229,"Missing"
P15-1023,P12-2023,0,0.214218,"r a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual Meeting of the Associati"
P15-1023,P06-1121,0,0.055962,"a translation system for lexical selection. Experiment results on NIST ChineseEnglish test sets demonstrate that 1) our model significantly outperforms previous lexical selection methods and 2) modeling correlations between local words and global topics can further improve translation quality. 1 Introduction Lexical selection is a very important task in statistical machine translation (SMT). Given a sentence in the source language, lexical selection statistically predicts translations for source words, based on various translation knowledge. Most conventional SMT systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007) exploit very limited context information contained in bilingual rules for lexical selection. ∗ Corresponding author. duì gāi wèntí zhōngguó bǎochí zhōnglì lìchǎng {problem, issue ...}wèntí {stance, attitude ...}lìchǎng [Economy topic, Politics topic ...] Figure 1: A Chinese-English translation example to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local"
P15-1023,D12-1010,1,0.890599,"Missing"
P15-1023,D08-1039,0,0.104532,"e to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presen"
P15-1023,E14-1035,0,0.15347,"anslations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conf"
P15-1023,W14-3358,0,0.182188,"anslations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conf"
P15-1023,C08-1041,0,0.0994588,"xts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if l"
P15-1023,N03-1017,0,0.0655323,"l is integrated into a translation system for lexical selection. Experiment results on NIST ChineseEnglish test sets demonstrate that 1) our model significantly outperforms previous lexical selection methods and 2) modeling correlations between local words and global topics can further improve translation quality. 1 Introduction Lexical selection is a very important task in statistical machine translation (SMT). Given a sentence in the source language, lexical selection statistically predicts translations for source words, based on various translation knowledge. Most conventional SMT systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007) exploit very limited context information contained in bilingual rules for lexical selection. ∗ Corresponding author. duì gāi wèntí zhōngguó bǎochí zhōnglì lìchǎng {problem, issue ...}wèntí {stance, attitude ...}lìchǎng [Economy topic, Politics topic ...] Figure 1: A Chinese-English translation example to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that"
P15-1023,W04-3250,0,0.175547,"baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the toolkit4 developed by Zhang (2004) to train the reordering model with the following parameters: iteration number iter=200 and Gaussian prior g=1.0. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002) metric. Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 2 http://people.sutd.edu.sg/∼yue zhang/doc/index.html http://nlp.stanford.edu/software 4 http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html 3 (6) 233 Model CATM (± 6w) CATM (± 8w) CATM (± 10w) CATM (± 12w) CATM (± 14w) MT05 33.35 33.43 33.42 33.49 33.30 Table 2: Experiment results on the development set using different window sizes ws . To train CATM, we set the topic number Nz as 25.5 For hyperparameters α and β, we empirically set α=50/Nz and β=0.1, as implemented in (Griffiths and Steyvers, 2004). Following Han et al. (2012), we se"
P15-1023,D08-1010,1,0.79717,"inspired by (Han and Sun, 2012), where an entity-topic model is presented for entity linking. We successfully adapt this work to lexical selection in SMT. The related work mainly includes the following two strands. (1) Lexical Selection in SMT. In order to explore rich context information for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Along this line, Shen et al. (2009) introduce four new linguistic and contextual features for translation selection in SMT. Recently, we have witnessed an increasing efforts in exploiting document-level context information to improve lexical selection. Xiao et al. (2011) impose a hard constraint to guarantee 235 Target-side Topical Items UNHCR republic refugee refugee Kosovo federal military missile military United States system war country development economy international economic trade Taiwan China relation cross-strait cross-strait relation issue Topic Source-side Contextual Words J¬(ref"
P15-1023,D09-1022,0,0.124816,"effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On t"
P15-1023,P02-1038,0,0.0449124,"topical items. Formally, for the position i in the document corresponding to the content word f , we collect the sampled count that translation e˜ generates f , denoted by Csam (˜ e, f ). This count can be normalized to form a new translation probability in the following way: Csam (˜ e, f ) + k p(˜ e|f ) = Csam + k · Ne˜,f where Csam is the total number of samples during inference and Ne˜,f is the number of candidate translations of f . Here we apply add-k smoothing to refine this translation probability, where k is a tunable global smoothing constant. Under the framework of log-linear model (Och and Ney, 2002), we use this translation probability as a new feature to improve lexical selection in SMT. 4 Experiments In order to examine the effectiveness of our model, we carried out several groups of experiments on Chinese-to-English translation. 4.1 Setup Our bilingual training corpus is from the FBIS corpus and the Hansards part of LDC2004T07 corpus (1M parallel sentences, 54.6K documents, with 25.2M Chinese words and 29M English words). We first used ZPar toolkit2 and Stanford toolkit3 to preprocess (i.e., word segmenting, PoS tagging) the Chinese and English parts of training corpus, and then word-"
P15-1023,J03-1002,0,0.00668871,"robability as a new feature to improve lexical selection in SMT. 4 Experiments In order to examine the effectiveness of our model, we carried out several groups of experiments on Chinese-to-English translation. 4.1 Setup Our bilingual training corpus is from the FBIS corpus and the Hansards part of LDC2004T07 corpus (1M parallel sentences, 54.6K documents, with 25.2M Chinese words and 29M English words). We first used ZPar toolkit2 and Stanford toolkit3 to preprocess (i.e., word segmenting, PoS tagging) the Chinese and English parts of training corpus, and then word-aligned them using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of MT05 as the development set, and the sets of MT06/MT08 as test sets. On average, these three sets contain 17.2, 13.9 and 14.1 content words per sentence, respectively. We trained a 5-gram language model on the Xinhua portion of Gigaword corpus using the SRILM Toolkit (Stolcke, 2002). Our baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the"
P15-1023,P03-1021,0,0.0641451,"Missing"
P15-1023,J04-4002,0,0.305645,"Missing"
P15-1023,D09-1008,0,0.126118,"pics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts sugges"
P15-1023,P12-1048,1,0.783622,"or missile and war, respectively; “ü” and “W” together means cross-starit. the document-level translation consistency. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Also relevant is the work of Xiong et al.(2013), who use three different models to capture lexical cohesion for document-level SMT. (2) SMT with Topic Models. In this strand, Zhao and Xing (2006, 2007) first present a bilingual topical admixture formalism for word alignment in SMT. Tam et al. (2007) and Ruiz et al. (2012) apply topic model into language model adaptation. Su et al. (2012) conduct translation model adaptation with monolingual topic information. Gong et al. (2010) and Xiao et al. (2012) introduce topic-based similarity models to improve SMT system. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Eidelman et al. (2012) incorporate topic-specific lexical weights into translation model. Hewavitharana et al. (2013) propose an incremental topic based translation model adaptation approach that satisfies the causality constraint imposed by spoken conversations. Hasl"
P15-1023,N12-1046,0,0.0474752,"Missing"
P15-1023,J97-3002,0,0.0509132,"ord segmenting, PoS tagging) the Chinese and English parts of training corpus, and then word-aligned them using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of MT05 as the development set, and the sets of MT06/MT08 as test sets. On average, these three sets contain 17.2, 13.9 and 14.1 content words per sentence, respectively. We trained a 5-gram language model on the Xinhua portion of Gigaword corpus using the SRILM Toolkit (Stolcke, 2002). Our baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the toolkit4 developed by Zhang (2004) to train the reordering model with the following parameters: iteration number iter=200 and Gaussian prior g=1.0. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002) metric. Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 2 http://people.sutd.edu.sg/∼yue zhang/do"
P15-1023,2011.mtsummit-papers.13,0,0.169815,"h black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated"
P15-1023,P12-1079,1,0.93568,"ation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual M"
P15-1023,P06-1066,1,0.778544,"using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of MT05 as the development set, and the sets of MT06/MT08 as test sets. On average, these three sets contain 17.2, 13.9 and 14.1 content words per sentence, respectively. We trained a 5-gram language model on the Xinhua portion of Gigaword corpus using the SRILM Toolkit (Stolcke, 2002). Our baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the toolkit4 developed by Zhang (2004) to train the reordering model with the following parameters: iteration number iter=200 and Gaussian prior g=1.0. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002) metric. Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 2 http://people.sutd.edu.sg/∼yue zhang/doc/index.html http://nlp.stanford.edu/software 4 http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html 3 (6) 233"
P15-1023,P14-1137,1,0.852105,"Missing"
P15-1023,P06-2124,0,0.0377211,"topical items and contextual words learned by CATM with Nz =25 and Ws =12. Chinese words that do not have direct English translations are denoted with ”*”. Here “q” and “|” are Chinese quantifiers for missile and war, respectively; “ü” and “W” together means cross-starit. the document-level translation consistency. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Also relevant is the work of Xiong et al.(2013), who use three different models to capture lexical cohesion for document-level SMT. (2) SMT with Topic Models. In this strand, Zhao and Xing (2006, 2007) first present a bilingual topical admixture formalism for word alignment in SMT. Tam et al. (2007) and Ruiz et al. (2012) apply topic model into language model adaptation. Su et al. (2012) conduct translation model adaptation with monolingual topic information. Gong et al. (2010) and Xiao et al. (2012) introduce topic-based similarity models to improve SMT system. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Eidelman et al. (2012) incorporate topic-specific lexical weights into t"
P15-1023,P02-1040,0,\N,Missing
P15-1023,W11-2133,0,\N,Missing
P15-1023,P13-2122,0,\N,Missing
P15-1023,J07-2003,0,\N,Missing
P17-1064,J07-2003,0,0.0490256,"Figure 5 suggest that the Mixed RNN encoder is the simplest. Moreover, comparing to conventional NMT encoders, the difference lies only in the length of the input sequence. Statistics on our training data reveal that the Mixed RNN encoder approximately triples the input sequence length compared to conventional NMT encoders. 4 Experimentation We have presented our approaches to incorporating the source syntax into NMT encoders. In this section, we evaluate their effectiveness on Chinese-to-English translation. 4.1 • cdec (Dyer et al., 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.7 Experimental Settings Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively.4 We choose NIST MT 06 dataset (1664 sentence pairs) as our development set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respectively) as our test sets.5 To get the source syntax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser 6"
P17-1064,W14-4012,0,0.0168517,"Missing"
P17-1064,D14-1179,0,0.0247785,"Missing"
P17-1064,D16-1257,0,0.0120836,"any two different words. However, considering the lack of efficient way to directly model structural information, an alternative way is to linearize the phrase parse tree into a sequence of structural labels and learn the structural context through the sequence. For example, Figure 3(c) shows the structural label sequence of Figure 3(b) in a simple way following a depth-first traversal order. Note that linearizing a parse tree in a depth-first traversal order into a sequence of structural labels has also been widely adopted in recent advances in neural syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), suggesting that the linearized sequence can be viewed as an alternative to its tree structure.2 There is no doubt that the structural label sequence is much longer than its word sequence. In order to obtain the structural label annotation vector for wi in word sequence, we simply look for wi ’s part-of-speech (POS) tag in the label sequence and view the tag’s annotation vector as wi ’s label annotation vector. This is because wi ’s POS tag location can also represent wi ’s location in the parse tree. For example, in Figure 3, word w1 in (a) maps to l3 in (c) since l3 is the POS tag of w1 . L"
P17-1064,P10-4002,0,0.0137269,"ur method with two state-of-theart models of SMT and NMT: • Figure 4 and Figure 5 suggest that the Mixed RNN encoder is the simplest. Moreover, comparing to conventional NMT encoders, the difference lies only in the length of the input sequence. Statistics on our training data reveal that the Mixed RNN encoder approximately triples the input sequence length compared to conventional NMT encoders. 4 Experimentation We have presented our approaches to incorporating the source syntax into NMT encoders. In this section, we evaluate their effectiveness on Chinese-to-English translation. 4.1 • cdec (Dyer et al., 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.7 Experimental Settings Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively.4 We choose NIST MT 06 dataset (1664 sentence pairs) as our development set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respectively) as our test sets.5 To get the source syntax for sentences on th"
P17-1064,P16-1078,0,0.160511,"Missing"
P17-1064,P16-1162,0,0.0130766,"Missing"
P17-1064,W15-3014,0,0.0140583,"n-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT exp"
P17-1064,P08-1066,0,0.0134567,"improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008; Introduction Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) ∗ VV Work done at Huawei Noah’s Ark Lab, HongKong. 688 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1064 Shen et al., 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT effectively, we find that translations of NMT sometimes fail to well respect source syntax. Figure 1 (a) shows a Chinese-to-English translation example of NMT. In this example, the NMT seq2seq model incorrectly translates the Chinese noun phrase (i.e., 新 生/xinsheng 银 行/yinhang) into a discontinuous phrase in English (i.e., new ... bank) due to the failure of capturing the internal syntactic structure in the input Chinese sentence. Statistics on our development set show that one forth of Chinese noun phrases are translate"
P17-1064,W04-3250,0,0.104681,"Missing"
P17-1064,D16-1159,0,0.168192,"e syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translati"
P17-1064,N13-1060,1,0.934388,"Missing"
P17-1064,Q17-1007,1,0.0517642,"Missing"
P17-1064,P06-1077,0,0.0689507,"training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy. In principle, syntax is a promising avenue for translation modeling. This has been verified by tremendous encouraging studies on syntaxbased SMT that substantially improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008; Introduction Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) ∗ VV Work done at Huawei Noah’s Ark Lab, HongKong. 688 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1064 Shen et al., 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT"
P17-1064,P16-1008,1,0.860335,"we group sentences of similar lengths together and compute BLEU scores. Figure 6 presents the BLEU scores over different lengths of input sentences. It shows that Mixed RNN system outperforms RNNSearch over sentences with all different lengths. It also shows that the performance drops substantially 693 System RNNSearch Mixed RNN AER 50.1 47.9 System RNNSearch Table 2: Evaluation of alignment quality. The lower the score, the better the alignment quality. when the length of input sentences increases. This performance trend over the length is consistent with the findings in (Cho et al., 2014a; Tu et al., 2016, 2017a). We also observe that the NMT systems perform surprisingly bad on sentences over 50 in length, especially compared to the performance of SMT system (i.e., cdec). We think that the bad behavior of NMT systems towards long sentences (e.g., length of 50) is due to the following two reasons: (1) the maximum source sentence length limit is set as 50 in training, 9 making the learned models not ready to translate sentences over the maximum length limit; (2) NMT systems tend to stop early for long input sentences. 5.2 Mixed RNN Cont. 57.3 59.8 47.3 54.0 58.1 63.3 63.1 54.5 56.2 60.4 Dis. 33."
P17-1064,2015.iwslt-evaluation.11,0,0.0637674,"ves is provided to reveal how source syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syn"
P17-1064,D15-1166,0,0.0233336,"vided to reveal how source syntax benefits NMT. 1 NP2 input: output: tokoyo stock exchange approves new listing bank reference: tokyo exchange approves shinsei bank 's application for listing (a). An example of discontinuous translation NP input: , output: they came from six families with two girls and two girls . reference: they came from six families and two girls are without parents . (b). An example of over translation Figure 1: Examples of NMT translation that fail to respect source syntax. on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015). However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus. Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syn"
P17-1064,P08-1114,0,0.0438962,"m to recover the hidden syntax in NMT. As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax. In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy. In principle, syntax is a promising avenue for translation modeling. This has been verified by tremendous encouraging studies on syntaxbased SMT that substantially improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008; Introduction Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT) ∗ VV Work done at Huawei Noah’s Ark Lab, HongKong. 688 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1064 Shen et al., 2008; Li et al., 2013). While it is yet to be seen how syntax can benefit NMT effectively, we find tha"
P17-1064,D16-1249,0,0.0283303,"Missing"
P17-1064,J03-1002,0,0.00476306,"Word Alignment Due to the capability of carrying syntactic information in source annotation vectors, we conjecture that our model with source syntax is also beneficial for alignment. To test this hypothesis, we carry out experiments of the word alignment task on the evaluation dataset from Liu and Sun (2015), which contains 900 manually aligned Chinese-English sentence pairs. We force the decoder to output reference translations, as to get automatic alignments between input sentences and their reference translations. To evaluate alignment performance, we report the alignment error rate (AER) (Och and Ney, 2003) in Table 2. Table 2 shows that source syntax information improves the attention model as expected by maintaining an annotation vector summarizing structural information on each source word. 5.3 XP PP NP CP QP ALL PP NP CP QP ALL 5.4 Analysis on Over Translation To estimate the over translation generated by NMT, we propose ratio of over translation (ROT): Analysis on Phrase Alignment ROT = The above subsection examines the alignment performance at the word level. In this subsection, we turn to phrase alignment analysis by moving from word unit to phrase unit. Given a source phrase XP, we use w"
P17-1064,P02-1040,0,0.13215,"Missing"
P17-1064,N07-1051,0,0.0913188,"Missing"
P17-1064,W16-2209,0,0.284718,"tion, over translation usually happens along with the disrespect of syntax which results in the repeated translation of the same source words in multiple positions of the target sentence. In this paper we are not aiming at solving any particular issue, either the discontinuous translation or the over translation. Alternatively, we address how to incorporate explicitly the source syntax to improve the NMT translation accuracy with the expectation of alleviating the issues above in general. Specifically, rather than directly assigning each source word with manually designed syntactic labels, as Sennrich and Haddow (2016) do, we linearize a phrase parse tree into a structural label sequence and let the model automatically learn useful syntactic information. On the basis, we systematically propose and compare several different approaches to incorporating the label sequence into the seq2seq NMT model. Experimentation on Chinese-to-English translation demonstrates that all proposed approaches are able to improve the translation accuracy. 2 h h1 h1 yi hm h hm si-1 h1 h1 x1 x2 ….. xm (a) encoder Atten ci MLP si RNN yi-1 (b) decoder Figure 2: Attention-based NMT model. mulated using a pair of neural networks, i.e.,"
P18-1164,D16-1162,0,0.0193314,"ng only admissible translation pairs. These data thus suggest that our improved model has a good capability of capturing the translation equivalence between source and target word embeddings. 5 ment quality by inducing the NMT model to capture more favorable pairs of words that are translation equivalents of each other under the effect of the bridging mechanism. Recently there have been also studies towards leveraging word alignments from SMT models. Mi et al. (2016) and Liu et al. (2016) use preobtained word alignments to guide the NMT attention model in the learning of favorable word pairs. Arthur et al. (2016) leverage a pre-obtained word dictionary to constrain the prediction of target words. Despite these approaches having a somewhat similar motivation of using pairs of translation equivalents to benefit the NMT translation, in our new bridging approach we do not use extra resources in the NMT model, but let the model itself learn the similarity of word pairs from the training data. 4 Besides, there exist also studies on the learning of cross-lingual embeddings for machine translation. Mikolov et al. (2013) propose to first learn distributed representation of words from large monolingual data, an"
P18-1164,P10-4002,0,0.0355367,"ry words were mapped to the special token UNK. The dimension of word embedding was 620 and the size of the hidden layer was 1000. All other settings were the same as in Bahdanau et al. (2015). The maximum length of sentences that we used to train the NMT model in our experiments was set to 50, for both the Chinese and English sides. Additionally, during decoding, we used the beam-search algorithm and set the beam size to 10. The model parameters were selected according to the maximum BLEU points on the development set. We compared our proposed models against the following two systems: • cdec (Dyer et al., 2010): this is an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target side of the training data. • Rather than using a concrete source word embedding xt∗ in Equation 3, we could also use a weighted sum of source word embeddings, P i.e. j αtj hj . However, our preliminary experiments showed that the performance gap between these two methods is very small. Therefore, we use xt∗ to calculate the new training objective as shown in Equation 3 in all experiments. 3 • RNNSearch*: this is an attention-based NMT system,"
P18-1164,P17-1064,1,0.787515,"te the respec1772 System 40 BLEU Score 35 RNNSearch* 30 25 cdec RNNSearch* Direct Link 20 (0,10] (10,20] (20,30] (30,40] (40,50] (50,100] Direct bridging Length of Source Sentence Figure 6: BLEU scores for the translation of sentences with different lengths. tive BLEU scores, which are presented in Figure 6. These results indicate that our improved system outperforms RNNSearch* for all the sentence lengths. They also reveal that the performance drops substantially when the length of the input sentence increases. This trend is consistent with the findings in (Cho et al., 2014; Tu et al., 2016; Li et al., 2017). One also observes that the NMT systems perform very badly on sentences of length over 50, when compared to the performance of the baseline SMT system (cdec). We think that the degradation of NMT systems performance over long sentences is due to the following reasons: (1) during training, the maximum source sentence length limit is set to 50, thus making the learned models not ready to cope well with sentences over this maximum length limit; (2) for long input sentences, NMT systems tend to stop early in the generation of the translation. 4.3 Analysis of Over and Under Translation To assess t"
P18-1164,C16-1291,0,0.0538199,"sformation matrix of our direct bridging method are very consistent with those obtained from the SMT lexical table, containing only admissible translation pairs. These data thus suggest that our improved model has a good capability of capturing the translation equivalence between source and target word embeddings. 5 ment quality by inducing the NMT model to capture more favorable pairs of words that are translation equivalents of each other under the effect of the bridging mechanism. Recently there have been also studies towards leveraging word alignments from SMT models. Mi et al. (2016) and Liu et al. (2016) use preobtained word alignments to guide the NMT attention model in the learning of favorable word pairs. Arthur et al. (2016) leverage a pre-obtained word dictionary to constrain the prediction of target words. Despite these approaches having a somewhat similar motivation of using pairs of translation equivalents to benefit the NMT translation, in our new bridging approach we do not use extra resources in the NMT model, but let the model itself learn the similarity of word pairs from the training data. 4 Besides, there exist also studies on the learning of cross-lingual embeddings for machin"
P18-1164,2015.iwslt-evaluation.11,0,0.0385175,"redicted word. Seeking to shorten the distance between source and target word embeddings, in what we term bridging, is the key insight for the advances presented in this paper. improve quality of both sentence translation, in general, and alignment and translation of individual source words with target words, in particular. 1 Introduction Neural machine translation (NMT) is an endto-end approach to machine translation that has achieved competitive results vis-a-vis statistical machine translation (SMT) on various language pairs (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014; Luong and Manning, 2015). In NMT, the sequence-to-sequence (seq2seq) model learns word embeddings for both source and target words synchronously. However, as illustrated in Figure 1, source and target word embeddings are at the two ends of a long information processing procedure. The individual associations between them will gradually become loose due to the separation of source-side hidden states (represented by h1 , . . . , hT in Fig. 1) and a target1767 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1767–1776 c Melbourne, Australia, July 15 - 20, 2018."
P18-1164,D15-1166,0,0.130766,"e insight of shortening the distance between source and target embeddings in the seq2seq processing chain, in the present paper we propose more strategies to bridge source and target word embeddings and with better results. 6 Related Work Since the pioneer work of Bahdanau et al. (2015) to jointly learning alignment and translation in NMT, many effective approaches have been proposed to further improve the alignment quality. The attention model plays a crucial role in the alignment quality and thus its enhancement has continuously attracted further efforts. To obtain better attention focuses, Luong et al. (2015) propose global and local attention models; and Cohn et al. (2016) extend the attentional model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. In contrast, we did not delve into the attention model or sought to redesign it in our new bridging proposal. And yet we achieve enhanced alignConclusion We have presented three models to bridge source and target word embeddings for NMT. The three models seek to shorten the distance between source and target word embeddings along the exte"
P18-1164,D16-1249,0,0.142173,"h a source word fj , a transformation matrix W is learned with the hope that the discrepancy of W xi and yj tends to be zero. Accordingly, we update 1769 the objective function of training for a single sentence with its following extended formulation: L(θ) = − Ty X (log p(yt |y<t , x) − kW xt∗ − yt k2 ) t=1 (3) where log p(yt |y<t , x) is the original objective function of the NMT model, and the term kW xt∗ − yt k2 measures and penalizes the difference between target word yt and its aligned source word xt∗ , i.e. the one with the highest attention weight, as computed in Equation 2. Similar to Mi et al. (2016), we view the two parts of the loss in Equation 3 as equally important. At this juncture, it is worth noting the following: • Our direct bridging model is an extension of the source-side bridging model, where the source word embeddings are part of the final annotation vector of the encoder. We have also tried to place the auxiliary object function directly on the NMT baseline model. However, our empirical study showed that the combined objective consistently worsens the translation quality. We blame this on that the learned word embeddings on two sides by the baseline model are too heterogeneo"
P18-1164,J03-1002,0,0.0313851,"Missing"
P18-1164,P02-1040,0,0.09988,"Missing"
P18-1164,2006.amta-papers.25,0,0.107933,"Missing"
P18-1164,N03-1033,0,0.109773,"Missing"
P18-1164,P16-1008,0,0.0277452,"length and compute the respec1772 System 40 BLEU Score 35 RNNSearch* 30 25 cdec RNNSearch* Direct Link 20 (0,10] (10,20] (20,30] (30,40] (40,50] (50,100] Direct bridging Length of Source Sentence Figure 6: BLEU scores for the translation of sentences with different lengths. tive BLEU scores, which are presented in Figure 6. These results indicate that our improved system outperforms RNNSearch* for all the sentence lengths. They also reveal that the performance drops substantially when the length of the input sentence increases. This trend is consistent with the findings in (Cho et al., 2014; Tu et al., 2016; Li et al., 2017). One also observes that the NMT systems perform very badly on sentences of length over 50, when compared to the performance of the baseline SMT system (cdec). We think that the degradation of NMT systems performance over long sentences is due to the following reasons: (1) during training, the maximum source sentence length limit is set to 50, thus making the learned models not ready to cope well with sentences over this maximum length limit; (2) for long input sentences, NMT systems tend to stop early in the generation of the translation. 4.3 Analysis of Over and Under Trans"
P18-1166,P17-1012,0,0.0972114,"rchitectures have been explored ∗ Corresponding author. Source code is available https://github.com/bzhangXMU/transformer-aan. RNN 2 CNN 3 Transformer Figure 1: Illustration of the decoding procedure under different neural architectures. We show which previous target words are required to predict the current target word yj in different NMT architectures. k indicates the filter size of the convolution layer. Introduction 1 1 at as the backbone network for translation, ranging from recurrent neural networks (RNN) (Sutskever et al., 2014; Luong et al., 2015), convolutional neural networks (CNN) (Gehring et al., 2017a,b) to full attention networks without recurrence and convolution (Vaswani et al., 2017). Particularly, the neural Transformer, relying solely on attention networks, has refreshed state-of-the-art performance on several language pairs (Vaswani et al., 2017). Most interestingly, the neural Transformer is capable of being fully parallelized at the training phase and modeling intra-/inter-dependencies of source and target sentences within a short path. The parallelization property enables training NMT very quickly, while the dependency modeling property endows the Transformer with strong ability"
P18-1166,C16-1291,0,0.0194238,"dealing with long-range dependencies. Yang et al. (2017) introduce a recurrent cycle on the attention layer to enhance the model’s memorization of previous translated source words. Zhang et al. (2017a) observe the weak discrimination ability of the attention-generated context vectors and propose a GRU-gated attention network. Kim et al. (2017) further model intrinsic structures inside attention through graphical models. Shen et al. (2017) introduce a direction structure into a selfattention network to integrate both long-range dependencies and temporal order information. Mi et al. (2016) and Liu et al. (2016) employ standard word alignment to supervise the automatically generated attention weights. Our work also focus on the evolution of attention network, but unlike previous work, we seek to simplify the selfattention network so as to accelerate the decoding procedure. The design of our model is partially inspired by the highway network (Srivastava et al., 2015) and the residual network (He et al., 2015). In the respect of speeding up the decoding of the neural Transformer, Gu et al. (2018) change the auto-regressive architecture to speed up translation by directly generating target words without"
P18-1166,E17-2061,0,0.0205918,"we have mentioned in Section 1, it suffers from decoding inefficiency. The attention mechanism is originally proposed to induce translation-relevant source information for predicting next target word in NMT. It contributes a lot to make NMT outperform SMT. Recently, a variety of efforts are made to further improve its accuracy and capability. Luong et al. 1790 (2015) explore several attention formulations and distinguish local attention from global attention. Zhang et al. (2016) treat RNN as an alternative to the attention to improve model’s capability in dealing with long-range dependencies. Yang et al. (2017) introduce a recurrent cycle on the attention layer to enhance the model’s memorization of previous translated source words. Zhang et al. (2017a) observe the weak discrimination ability of the attention-generated context vectors and propose a GRU-gated attention network. Kim et al. (2017) further model intrinsic structures inside attention through graphical models. Shen et al. (2017) introduce a direction structure into a selfattention network to integrate both long-range dependencies and temporal order information. Mi et al. (2016) and Liu et al. (2016) employ standard word alignment to super"
P18-1166,P16-5005,0,0.0184881,"and short dependency path significantly improve the training speed as well as model performance for the Transformer. Unfortunately, as we have mentioned in Section 1, it suffers from decoding inefficiency. The attention mechanism is originally proposed to induce translation-relevant source information for predicting next target word in NMT. It contributes a lot to make NMT outperform SMT. Recently, a variety of efforts are made to further improve its accuracy and capability. Luong et al. 1790 (2015) explore several attention formulations and distinguish local attention from global attention. Zhang et al. (2016) treat RNN as an alternative to the attention to improve model’s capability in dealing with long-range dependencies. Yang et al. (2017) introduce a recurrent cycle on the attention layer to enhance the model’s memorization of previous translated source words. Zhang et al. (2017a) observe the weak discrimination ability of the attention-generated context vectors and propose a GRU-gated attention network. Kim et al. (2017) further model intrinsic structures inside attention through graphical models. Shen et al. (2017) introduce a direction structure into a selfattention network to integrate both"
P18-1166,D15-1166,0,0.0854619,"l., 2015). Under this framework, various advanced neural architectures have been explored ∗ Corresponding author. Source code is available https://github.com/bzhangXMU/transformer-aan. RNN 2 CNN 3 Transformer Figure 1: Illustration of the decoding procedure under different neural architectures. We show which previous target words are required to predict the current target word yj in different NMT architectures. k indicates the filter size of the convolution layer. Introduction 1 1 at as the backbone network for translation, ranging from recurrent neural networks (RNN) (Sutskever et al., 2014; Luong et al., 2015), convolutional neural networks (CNN) (Gehring et al., 2017a,b) to full attention networks without recurrence and convolution (Vaswani et al., 2017). Particularly, the neural Transformer, relying solely on attention networks, has refreshed state-of-the-art performance on several language pairs (Vaswani et al., 2017). Most interestingly, the neural Transformer is capable of being fully parallelized at the training phase and modeling intra-/inter-dependencies of source and target sentences within a short path. The parallelization property enables training NMT very quickly, while the dependency m"
P18-1166,D16-1249,0,0.0355611,"Missing"
P18-1166,P02-1040,0,0.103169,"pendencies with previous predicted words:   ˜sl = AAN sl−1 (8) Carrying these dependencies, the decoder stacks another two sub-layers to seek translation-relevant source semantics for bridging the gap between the 5.1 Experiments WMT14 English-German Translation We examine various aspects of our AAN on this translation task. The training data consist of 4.5M sentence pairs, involving about 116M English words and 110M German words. We used newstest2013 as the development set for model selection, and newstest2014 as the test set. We evaluated translation quality via case-sensitive BLEU metric (Papineni et al., 2002). 5.1.1 Model Settings We applied byte pair encoding algorithm (Sennrich et al., 2016) to encode all sentences and limited the vocabulary size to 32K. All out-ofvocabulary words were mapped to an unique token “unk”. We set the dimensionality d of all input and output layers to 512, and that of innerFFN layer to 2048. We employed 8 parallel attention heads in both encoder and decoder layers. We batched sentence pairs together so that they were approximately of the same length, and each batch had roughly 25000 source and target tokens. During training, we used label smoothing with value ls = 0."
P18-1166,P16-1162,0,0.219561,"encies, the decoder stacks another two sub-layers to seek translation-relevant source semantics for bridging the gap between the 5.1 Experiments WMT14 English-German Translation We examine various aspects of our AAN on this translation task. The training data consist of 4.5M sentence pairs, involving about 116M English words and 110M German words. We used newstest2013 as the development set for model selection, and newstest2014 as the test set. We evaluated translation quality via case-sensitive BLEU metric (Papineni et al., 2002). 5.1.1 Model Settings We applied byte pair encoding algorithm (Sennrich et al., 2016) to encode all sentences and limited the vocabulary size to 32K. All out-ofvocabulary words were mapped to an unique token “unk”. We set the dimensionality d of all input and output layers to 512, and that of innerFFN layer to 2048. We employed 8 parallel attention heads in both encoder and decoder layers. We batched sentence pairs together so that they were approximately of the same length, and each batch had roughly 25000 source and target tokens. During training, we used label smoothing with value ls = 0.1, attention dropout and residual dropout with a rate of p = 0.1. During decoding, we"
W03-1730,W02-1817,1,\N,Missing
W07-0706,2005.iwslt-1.8,0,0.0933759,"Missing"
W07-0706,W06-1606,0,0.375434,"ng Correspondence A dependency treelet string correspondence π is a triple &lt; D, S, A &gt; which describes a translation pair &lt; D, S &gt; and their alignment A, where D is the dependency treelet on the source side and S is the translation string on the target side. &lt; D, S &gt; must be consistent with the word alignment M of the corresponding sentence pair ∀(i, j) ∈ M, i ∈ D ↔ j ∈ S A treelet is defined to be any connected subgraph, which is similar to the definition in (Quirk et al., 2005). Treelet is more representatively flexible than subtree which is widely used in models based on phrase structures (Marcu et al., 2006; Liu et al., 2006). The most important distinction between the treelet in (Quirk et al., 2005) and ours is that we allow variables at positions of subnodes. In our definition, the root node must be lexicalized but the subnodes can be replaced with a wild card. The target counterpart of a wildcard node in S is also replaced with a wild card. The wildcards introduced in this way generalize DTSC to match dependency structures with the same head word but with different modifiers or arguments. Another unique feature of our DTSC is that we allow target strings with gaps between words or wildcards."
W07-0706,2004.tmi-1.5,0,0.136619,"− LM (slr ) (3) where LM is the logarithm of the language model probability. We only need to compute the increment of the language model score: 4LM = LM (srl slr ) − LM (srl ) − LM (slr ) for each node n of the input tree T , in bottom-up order do Get all matched DTSCs rooted at n for each matched DTSC π do for each wildcard node n∗ in π do Substitute the corresponding wildcard on the target side with translations from the stack of n∗ end for for each uncovered node n@ by π do Attach the translations from the stack of n@ to the target side at the attaching point end for end for end for (4) 44 Melamed (2004) also used a similar way to integrate the language model. 5 Decoding Our decoding algorithm is similar to the bottom-up chart parsing. The distinction is that the input is a tree rather than a string and therefore the chart is indexed by nodes of the tree rather than spans of the string. Also, several other tree-based decoding algorithms introduced by Eisner (2003), Quirk et al. (2005) and Liu et al. (2006) can be classified as the chart-style parsing algorithm too. Our decoding algorithm is shown in Figure 4. Given an input dependency tree, firstly we generate the bottom-up order by postorder"
W07-0706,C04-1090,0,0.377464,", generalization, and handling discontinuous phrases which are very desirable for machine translation. We finally evaluate our current implementation of a simplified version of DTSC for statistical machine translation. 1 Introduction Over the last several years, various statistical syntaxbased models were proposed to extend traditional In this paper, we propose a novel model based on dependency structures: Dependency Treelet String Correspondence Model (DTSC). The DTSC model maps source dependency structures to target strings. It just needs a source language parser. In contrast to the work by Lin (2004) and by Quirk et al. (2005), the DTSC model does not need to generate target language dependency structures using source structures and word alignments. On the source side, we extract treelets which are any connected subgraphs and consistent with word alignments. While on the target side, we allow the aligned target sequences to be generalized and discontinuous by introducing variables and gaps. The variables on the target side are aligned to the corresponding variables of treelets, while gaps between words or variables are corresponding to the uncovered nodes which are not included by treelet"
W07-0706,P06-1077,1,0.950964,"dependency treelet string correspondence π is a triple &lt; D, S, A &gt; which describes a translation pair &lt; D, S &gt; and their alignment A, where D is the dependency treelet on the source side and S is the translation string on the target side. &lt; D, S &gt; must be consistent with the word alignment M of the corresponding sentence pair ∀(i, j) ∈ M, i ∈ D ↔ j ∈ S A treelet is defined to be any connected subgraph, which is similar to the definition in (Quirk et al., 2005). Treelet is more representatively flexible than subtree which is widely used in models based on phrase structures (Marcu et al., 2006; Liu et al., 2006). The most important distinction between the treelet in (Quirk et al., 2005) and ours is that we allow variables at positions of subnodes. In our definition, the root node must be lexicalized but the subnodes can be replaced with a wild card. The target counterpart of a wildcard node in S is also replaced with a wild card. The wildcards introduced in this way generalize DTSC to match dependency structures with the same head word but with different modifiers or arguments. Another unique feature of our DTSC is that we allow target strings with gaps between words or wildcards. Since source treele"
W07-0706,P03-1021,0,0.0595132,"Missing"
W07-0706,P00-1056,0,0.206099,"Missing"
W07-0706,P05-1034,0,0.657821,"and handling discontinuous phrases which are very desirable for machine translation. We finally evaluate our current implementation of a simplified version of DTSC for statistical machine translation. 1 Introduction Over the last several years, various statistical syntaxbased models were proposed to extend traditional In this paper, we propose a novel model based on dependency structures: Dependency Treelet String Correspondence Model (DTSC). The DTSC model maps source dependency structures to target strings. It just needs a source language parser. In contrast to the work by Lin (2004) and by Quirk et al. (2005), the DTSC model does not need to generate target language dependency structures using source structures and word alignments. On the source side, we extract treelets which are any connected subgraphs and consistent with word alignments. While on the target side, we allow the aligned target sequences to be generalized and discontinuous by introducing variables and gaps. The variables on the target side are aligned to the corresponding variables of treelets, while gaps between words or variables are corresponding to the uncovered nodes which are not included by treelets. To complete the translat"
W07-0706,W06-1608,0,0.0319377,"Missing"
W07-0706,P06-1066,1,0.90218,"Missing"
W07-0706,I05-1007,1,0.798107,"Missing"
W07-0706,P05-1033,0,0.550552,"Missing"
W07-0706,P05-1067,0,0.207272,"Missing"
W07-0706,P03-2041,0,0.441956,"ing wildcard on the target side with translations from the stack of n∗ end for for each uncovered node n@ by π do Attach the translations from the stack of n@ to the target side at the attaching point end for end for end for (4) 44 Melamed (2004) also used a similar way to integrate the language model. 5 Decoding Our decoding algorithm is similar to the bottom-up chart parsing. The distinction is that the input is a tree rather than a string and therefore the chart is indexed by nodes of the tree rather than spans of the string. Also, several other tree-based decoding algorithms introduced by Eisner (2003), Quirk et al. (2005) and Liu et al. (2006) can be classified as the chart-style parsing algorithm too. Our decoding algorithm is shown in Figure 4. Given an input dependency tree, firstly we generate the bottom-up order by postorder transversal. This order guarantees that any subnodes of node n have been translated before node n is done. For each node n in the bottom-up order, all matched DTSCs rooted at n are found, and a stack is also built for it to store the candidate translations. A DTSC π is said to match the input dependency subtree T rooted at n if and only if there is a treelet roote"
W07-0706,H05-1095,0,\N,Missing
W07-0706,zhang-etal-2004-interpreting,0,\N,Missing
