2021.emnlp-main.113,Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning,2021,-1,-1,2,1,8862,ruben branco,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Commonsense is a quintessential human capacity that has been a core challenge to Artificial Intelligence since its inception. Impressive results in Natural Language Processing tasks, including in commonsense reasoning, have consistently been achieved with Transformer neural language models, even matching or surpassing human performance in some benchmarks. Recently, some of these advances have been called into question: so called data artifacts in the training data have been made evident as spurious correlations and shallow shortcuts that in some cases are leveraging these outstanding results. In this paper we seek to further pursue this analysis into the realm of commonsense related language processing tasks. We undertake a study on different prominent benchmarks that involve commonsense reasoning, along a number of key stress experiments, thus seeking to gain insight on whether the models are learning transferable generalizations intrinsic to the problem at stake or just taking advantage of incidental shortcuts in the data items. The results obtained indicate that most datasets experimented with are problematic, with models resorting to non-robust features and appearing not to be learning and generalizing towards the overall tasks intended to be conveyed or exemplified by the datasets."
2020.lrec-1.106,The {BDC}am{\\~o}es Collection of {P}ortuguese Literary Documents: a Research Resource for Digital Humanities and Language Technology,2020,-1,-1,5,0,16829,sara grilo,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper presents the BDCam{\~o}es Collection of Portuguese Literary Documents, a new corpus of literary texts written in Portuguese that in its inaugural version includes close to 4 million words from over 200 complete documents from 83 authors in 14 genres, covering a time span from the 16th to the 21st century, and adhering to different orthographic conventions. Many of the texts in the corpus have also been automatically parsed with state-of-the-art language processing tools, forming the BDCam{\~o}es Treebank subcorpus. This set of characteristics makes of BDCam{\~o}es an invaluable resource for research in language technology (e.g. authorship detection, genre classification, etc.) and in language science and digital humanities (e.g. comparative literature, diachronic linguistics, etc.)."
2020.lrec-1.407,The {E}uropean Language Technology Landscape in 2020: Language-Centric and Human-Centric {AI} for Cross-Cultural Communication in Multilingual {E}urope,2020,4,1,24,0.137101,60,georg rehm,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Multilingualism is a cultural cornerstone of Europe and firmly anchored in the European treaties including full language equality. However, language barriers impacting business, cross-lingual and cross-cultural communication are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of approaches and technologies tailored to Europe{'}s specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, AI {--} including many opportunities, synergies but also misconceptions {--} has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions."
2020.lrec-1.598,"The {MWN}.{PT} {W}ord{N}et for {P}ortuguese: Projection, Validation, Cross-lingual Alignment and Distribution",2020,-1,-1,1,1,8863,antonio branco,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The objective of the present paper is twofold, to present the MWN.PT WordNet and to report on its construction and on the lessons learned with it. The MWN.PT WordNet for Portuguese includes 41,000 concepts, expressed by 38,000 lexical units. Its synsets were manually validated and are linked to semantically equivalent synsets of the Princeton WordNet of English, and thus transitively to the many wordnets for other languages that are also linked to this English wordnet. To the best of our knowledge, it is the largest high quality, manually validated and cross-lingually integrated, wordnet of Portuguese distributed for reuse. Its construction was initiated more than one decade ago and its description is published for the first time in the present paper. It follows a three step {\textless}projection, validation with alignment, completion{\textgreater} methodology consisting on the manual validation and expansion of the outcome of an automatic projection procedure of synsets and their hypernym relations, followed by another automatic procedure that transferred the relations of remaining semantic types across wordnets of different languages."
2020.lrec-1.622,Reproduction and Revival of the Argument Reasoning Comprehension Task,2020,-1,-1,4,1,8864,joao rodrigues,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Reproduction of scientific findings is essential for scientific development across all scientific disciplines and reproducing results of previous works is a basic requirement for validating the hypothesis and conclusions put forward by them. This paper reports on the scientific reproduction of several systems addressing the Argument Reasoning Comprehension Task of SemEval2018. Given a recent publication that pointed out spurious statistical cues in the data set used in the shared task, and that produced a revised version of it, we also evaluated the reproduced systems with this new data set. The exercise reported here shows that, in general, the reproduction of these systems is successful with scores in line with those reported in SemEval2018. However, the performance scores are worst than those, and even below the random baseline, when the reproduced systems are run over the revised data set expunged from data artifacts. This demonstrates that this task is actually a much harder challenge than what could have been perceived from the inflated, close to human-level performance scores obtained with the data set used in SemEval2018. This calls for a revival of this task as there is much room for improvement until systems may come close to the upper bound provided by human performance."
2020.lrec-1.680,"A Shared Task of a New, Collaborative Type to Foster Reproducibility: A First Exercise in the Area of Language Science and Technology with {REPROLANG}2020",2020,-1,-1,1,1,8863,antonio branco,Proceedings of the 12th Language Resources and Evaluation Conference,0,"n this paper, we introduce a new type of shared task {---} which is collaborative rather than competitive {---} designed to support and fosterthe reproduction of research results. We also describe the first event running such a novel challenge, present the results obtained, discussthe lessons learned and ponder on future undertakings."
2020.iwltp-1.1,Infrastructure for the Science and Technology of Language {PORTULAN} {CLARIN},2020,-1,-1,1,1,8863,antonio branco,Proceedings of the 1st International Workshop on Language Technology Platforms,0,"This paper presents the PORTULAN CLARIN Research Infrastructure for the Science and Technology of Language, which is part of the European research infrastructure CLARIN ERIC as its Portuguese national node, and belongs to the Portuguese National Roadmap of Research Infrastructures of Strategic Relevance. It encompasses a repository, where resources and metadata are deposited for long-term archiving and access, and a workbench, where Language Technology tools and applications are made available through different modes of interaction, among many other services. It is an asset of utmost importance for the technological development of natural languages and for their preparation for the digital age, contributing to ensure the citizenship of their speakers in the information society."
2020.iwltp-1.6,"{ELRI}: A Decentralised Network of National Relay Stations to Collect, Prepare and Share Language Resources",2020,-1,-1,20,0,17606,thierry etchegoyhen,Proceedings of the 1st International Workshop on Language Technology Platforms,0,"We describe the European Language Resource Infrastructure (ELRI), a decentralised network to help collect, prepare and share language resources. The infrastructure was developed within a project co-funded by the Connecting Europe Facility Programme of the European Union, and has been deployed in the four Member States participating in the project, namely France, Ireland, Portugal and Spain. ELRI provides sustainable and flexible means to collect and share language resources via National Relay Stations, to which members of public institutions can freely subscribe. The infrastructure includes fully automated data processing engines to facilitate the preparation, sharing and wider reuse of useful language resources that can help optimise human and automated translation services in the European Union."
2020.coling-main.354,Comparative Probing of Lexical Semantics Theories for Cognitive Plausibility and Technological Usefulness,2020,-1,-1,1,1,8863,antonio branco,Proceedings of the 28th International Conference on Computational Linguistics,0,"Lexical semantics theories differ in advocating that the meaning of words is represented as an inference graph, a feature mapping or a cooccurrence vector, thus raising the question: is it the case that one of these approaches is superior to the others in representing lexical semantics appropriately? Or in its non antagonistic counterpart: could there be a unified account of lexical semantics where these approaches seamlessly emerge as (partial) renderings of (different) aspects of a core semantic knowledge base? In this paper, we contribute to these research questions with a number of experiments that systematically probe different lexical semantics theories for their levels of cognitive plausibility and of technological usefulness. The empirical findings obtained from these experiments advance our insight on lexical semantics as the feature-based approach emerges as superior to the other ones, and arguably also move us closer to finding answers to the research questions above."
R19-1120,Whom to Learn From? Graph- vs. Text-based Word Embeddings,2019,0,0,2,0,21444,malgorzata salawa,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Vectorial representations of meaning can be supported by empirical data from diverse sources and obtained with diverse embedding approaches. This paper aims at screening this experimental space and reports on an assessment of word embeddings supported (i) by data in raw texts vs. in lexical graphs, (ii) by lexical information encoded in association- vs. inference-based graphs, and obtained (iii) by edge reconstruction- vs. matrix factorisation vs. random walk-based graph embedding methods. The results observed with these experiments indicate that the best solutions with graph-based word embeddings are very competitive, consistently outperforming mainstream text-based ones."
2019.gwc-1.32,Assessing Wordnets with {W}ord{N}et Embeddings,2019,-1,-1,4,1,8862,ruben branco,Proceedings of the 10th Global Wordnet Conference,0,"An effective conversion method was proposed in the literature to obtain a lexical semantic space from a lexical semantic graph, thus permitting to obtain WordNet embeddings from WordNets. In this paper, we propose the exploitation of this conversion methodology as the basis for the comparative assessment of WordNets: given two WordNets, their relative quality in terms of capturing the lexical semantics of a given language, can be assessed by (i) converting each WordNet into the corresponding semantic space (i.e. into WordNet embeddings), (ii) evaluating the resulting WordNet embeddings under the typical semantic similarity prediction task used to evaluate word embeddings in general; and (iii) comparing the performance in that task of the two word embeddings, extracted from the two WordNets. A better performance in that evaluation task results from the word embeddings that are better at capturing the semantic similarity of words, which, in turn, result from the WordNet that is of higher quality at capturing the semantics of words."
W18-4602,Computational Complexity of Natural Languages: A Reasoned Overview,2018,0,0,1,1,8863,antonio branco,Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing,0,"There has been an upsurge of research interest in natural language complexity. As this interest will benefit from being informed by established contributions in this area, this paper presents a reasoned overview of central results concerning the computational complexity of natural language parsing. This overview also seeks to help to understand why, contrary to recent and widespread assumptions, it is by no means sufficient that an agent handles sequences of items under a pattern $a^n b^n$ or under a pattern $a^n b^m c^n d^m$ to ascertain ipso facto that this is the result of at least an underlying context-free grammar or an underlying context-sensitive grammar, respectively. In addition, it seeks to help to understand why it is also not sufficient that an agent handles sequences of items under a pattern $a^n b^n$ for it to be deemed as having a cognitive capacity of higher computational complexity."
W18-3016,{W}ord{N}et Embeddings,2018,-1,-1,2,1,14538,chakaveh saedi,Proceedings of The Third Workshop on Representation Learning for {NLP},0,"Semantic networks and semantic spaces have been two prominent approaches to represent lexical semantics. While a unified account of the lexical meaning relies on one being able to convert between these representations, in both directions, the conversion direction from semantic networks into semantic spaces started to attract more attention recently. In this paper we present a methodology for this conversion and assess it with a case study. When it is applied over WordNet, the performance of the resulting embeddings in a mainstream semantic similarity task is very good, substantially superior to the performance of word embeddings based on very large collections of texts like word2vec."
W18-2801,Predicting Brain Activation with {W}ord{N}et Embeddings,2018,0,2,5,1,8864,joao rodrigues,Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing,0,"The task of taking a semantic representation of a noun and predicting the brain activity triggered by it in terms of fMRI spatial patterns was pioneered by Mitchell et al. 2008. That seminal work used word co-occurrence features to represent the meaning of the nouns. Even though the task does not impose any specific type of semantic representation, the vast majority of subsequent approaches resort to feature-based models or to semantic spaces (aka word embeddings). We address this task, with competitive results, by using instead a semantic network to encode lexical semantics, thus providing further evidence for the cognitive plausibility of this approach to model lexical meaning."
P18-1164,Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings,2018,0,14,3,0,29163,shaohui kuang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In neural machine translation, a source sequence of words is encoded into a vector from which a target sequence is generated in the decoding phase. Differently from statistical machine translation, the associations between source words and their possible target counterparts are not explicitly stored. Source and target words are at the two ends of a long information processing procedure, mediated by hidden states at both the source encoding and the target decoding phases. This makes it possible that a source word is incorrectly translated into a target word that is not any of its admissible equivalent counterparts in the target language. In this paper, we seek to somewhat shorten the distance between source and target words in that procedure, and thus strengthen their association, by means of a method we term bridging source and target word embeddings. We experiment with three strategies: (1) a source-side bridging model, where source word embeddings are moved one step closer to the output target sequence; (2) a target-side bridging model, which explores the more relevant source word embeddings for the prediction of the target sequence; and (3) a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation of ones by the others. Experiments and analysis presented in this paper demonstrate that the proposed bridging models are able to significantly improve quality of both sentence translation, in general, and alignment and translation of individual source words with target words, in particular."
L18-1022,"We Are Depleting Our Research Subject as We Are Investigating It: In Language Technology, more Replication and Diversity Are Needed",2018,0,1,1,1,8863,antonio branco,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1382,"Finely Tuned, 2 Billion Token Based Word Embeddings for {P}ortuguese",2018,0,0,2,1,8864,joao rodrigues,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1513,Semantic Equivalence Detection: Are Interrogatives Harder than Declaratives?,2018,0,2,3,1,8864,joao rodrigues,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1722,"Browsing and Supporting Pluricentric Global {W}ordnet, or just your {W}ordnet of Interest",2018,0,0,1,1,8863,antonio branco,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
2018.gwc-1.22,{W}ordnet{L}oom {--} a Multilingual {W}ordnet Editing System Focused on Graph-based Presentation,2018,-1,-1,5,0,17480,tomasz naskrket,Proceedings of the 9th Global Wordnet Conference,0,"The paper presents a new re-built and expanded, version 2.0 of WordnetLoom {--} an open wordnet editor. It facilitates work on a multilingual system of wordnets, is based on efficient software architecture of thin client, and offers more flexibility in enriching wordnet representation. This new version is built on the experience collected during the use of the previous one for more than 10 years of plWordNet development. We discuss its extensions motivated by the collected experience. A special focus is given to the development of a variant for the needs of MultiWordnet of Portuguese, which is based on a very different wordnet development model."
S17-1030,Ways of Asking and Replying in Duplicate Question Detection,2017,5,10,5,1,8864,joao rodrigues,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"This paper presents the results of systematic experimentation on the impact in duplicate question detection of different types of questions across both a number of established approaches and a novel, superior one used to address this language processing task. This study permits to gain a novel insight on the different levels of robustness of the diverse detection methods with respect to different conditions of their application, including the ones that approximate real usage scenarios."
W16-6405,Adding syntactic structure to bilingual terminology for improved domain adaptation,2016,3,1,6,0,10622,mikel artetxe,Proceedings of the 2nd Deep Machine Translation Workshop,0,None
W16-2332,{SMT} and Hybrid systems of the {QTL}eap project in the {WMT}16 {IT}-task,2016,26,3,15,1,17856,rosa gaudio,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper presents the description of 12 systems submitted to the WMT16 IT-task, covering six different languages, namely Basque, Bulgarian, Dutch, Czech, Portuguese and Spanish. All these systems were developed under the scope of the QTLeap project, presenting a common strategy. For each language two different systems were submitted, namely a phrasebased MT system built using Moses, and a system exploiting deep language engineering approaches, that in all the languages but Bulgarian was implemented using TectoMT. For 4 of the 6 languages, the TectoMT-based system performs better than the Moses-based one."
L16-1001,Evaluating Machine Translation in a Usage Scenario,2016,0,4,3,1,17856,rosa gaudio,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this document we report on a user-scenario-based evaluation aiming at assessing the performance of machine translation (MT) systems in a real context of use. We describe a sequel of experiments that has been performed to estimate the usefulness of MT and to test if improvements of MT technology lead to better performance in the usage scenario. One goal is to find the best methodology for evaluating the eventual benefit of a machine translation system in an application. The evaluation is based on the QTLeap corpus, a novel multilingual language resource that was collected through a real-life support service via chat. It is composed of naturally occurring utterances produced by users while interacting with a human technician providing answers. The corpus is available in eight different languages: Basque, Bulgarian, Czech, Dutch, English, German, Portuguese and Spanish."
L16-1094,Use of Domain-Specific Language Resources in Machine Translation,2016,0,3,5,0.450733,24998,sanja vstajner,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we address the problem of Machine Translation (MT) for a specialised domain in a language pair for which only a very small domain-specific parallel corpus is available. We conduct a series of experiments using a purely phrase-based SMT (PBSMT) system and a hybrid MT system (TectoMT), testing three different strategies to overcome the problem of the small amount of in-domain training data. Our results show that adding a small size in-domain bilingual terminology to the small in-domain training corpus leads to the best improvements of a hybrid MT system, while the PBSMT system achieves the best results by adding a combination of in-domain bilingual terminology and a larger out-of-domain corpus. We focus on qualitative human evaluation of the output of two best systems (one for each approach) and perform a systematic in-depth error analysis which revealed advantages of the hybrid MT system over the pure PBSMT system for this specific task."
L16-1246,{CINTIL} {D}ependency{B}ank {PREMIUM} - A Corpus of Grammatical Dependencies for {P}ortuguese,2016,14,1,6,0,17855,rita carvalho,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents a new linguistic resource for the study and computational processing of Portuguese. CINTIL DependencyBank PREMIUM is a corpus of Portuguese news text, accurately manually annotated with a wide range of linguistic information (morpho-syntax, named-entities, syntactic function and semantic roles), making it an invaluable resource specially for the development and evaluation of data-driven natural language processing tools. The corpus is under active development, reaching 4,000 sentences in its current version. The paper also reports on the training and evaluation of a dependency parser over this corpus. CINTIL DependencyBank PREMIUM is freely-available for research purposes through META-SHARE."
L16-1438,Bootstrapping a Hybrid {MT} System to a New Language Pair,2016,8,3,5,1,8864,joao rodrigues,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The usual concern when opting for a rule-based or a hybrid machine translation (MT) system is how much effort is required to adapt the system to a different language pair or a new domain. In this paper, we describe a way of adapting an existing hybrid MT system to a new language pair, and show that such a system can outperform a standard phrase-based statistical machine translation system with an average of 10 persons/month of work. This is specifically important in the case of domain-specific MT for which there is not enough parallel data for training a statistical machine translation system."
L16-1441,Word Sense-Aware Machine Translation: Including Senses as Contextual Features for Improved Translation Models,2016,16,9,5,1,24253,steven neale,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Although it is commonly assumed that word sense disambiguation (WSD) should help to improve lexical choice and improve the quality of machine translation systems, how to successfully integrate word senses into such systems remains an unanswered question. Some successful approaches have involved reformulating either WSD or the word senses it produces, but work on using traditional word senses to improve machine translation have met with limited success. In this paper, we build upon previous work that experimented on including word senses as contextual features in maxent-based translation models. Training on a large, open-domain corpus (Europarl), we demonstrate that this aproach yields significant improvements in machine translation from English to Portuguese."
L16-1483,{QTL}eap {WSD}/{NED} Corpora: Semantic Annotation of Parallel Corpora in Six Languages,2016,9,5,3,0,16264,arantxa otegi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This work presents parallel corpora automatically annotated with several NLP tools, including lemma and part-of-speech tagging, named-entity recognition and classification, named-entity disambiguation, word-sense disambiguation, and coreference. The corpora comprise both the well-known Europarl corpus and a domain-specific question-answer troubleshooting corpus on the IT domain. English is common in all parallel corpora, with translations in five languages, namely, Basque, Bulgarian, Czech, Portuguese and Spanish. We describe the annotated corpora and the tools used for annotation, as well as annotation statistics for each language. These new resources are freely available and will help research on semantic processing for machine translation and cross-lingual transfer."
W15-5708,First Steps in Using Word Senses as Contextual Features in Maxent Models for Machine Translation,2015,-1,-1,3,1,24253,steven neale,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-5713,Machine Translation for Multilingual Troubleshooting in the {IT} Domain: A Comparison of Different Strategies,2015,23,1,4,0.450733,24998,sanja vstajner,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-5503,"Small in Size, Big in Precision: A Case for Using Language-Specific Lexical Resources for Word Sense Disambiguation",2015,-1,-1,3,1,24253,steven neale,Proceedings of the Second Workshop on Natural Language Processing and Linked Open Data,0,None
W15-4101,Bootstrapping a hybrid deep {MT} system,2015,9,4,4,0.954257,8865,joao silva,Proceedings of the Fourth Workshop on Hybrid Approaches to Translation ({H}y{T}ra),0,"We present a Portuguesexe2x86x94English hybrid deep MT system based on an analysistransfer-synthesis architecture, with transfer being done at the level of deep syntax, a level that already includes a great deal of semantic information. The system received a few months of development, but its performance is already similar to that of baseline phrase-based MT, when evaluated using BLEU, and surpasses the baseline under human qualitative assessment."
W15-0208,A Flexible Tool for Manual Word Sense Annotation,2015,0,0,3,1,24253,steven neale,Proceedings of the 11th Joint {ACL}-{ISO} Workshop on Interoperable Semantic Annotation ({ISA}-11),0,None
rehm-etal-2014-strategic,"The Strategic Impact of {META}-{NET} on the Regional, National and International Level",2014,47,2,7,0.137101,60,georg rehm,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This article provides an overview of the dissemination work carried out in META-NET from 2010 until early 2014; we describe its impact on the regional, national and international level, mainly with regard to politics and the situation of funding for LT topics. This paper documents the initiativeÂs work throughout Europe in order to boost progress and innovation in our field."
E14-2021,Answering List Questions using Web as a corpus,2014,11,1,2,1,40064,patricia gonccalves,Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper supports the demo of LXListQuestion, a Web Question Answering System that exploit the redundancy of information available in the Web to answer List Questions in the form of Word Cloud."
W13-0106,Temporal Relation Classification Based on Temporal Reasoning,2013,30,2,2,1,41166,francisco costa,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,"The area of temporal information extraction has recently focused on temporal relation classification. This task is about classifying the temporal relation (precedence, overlap, etc.) holding between two given entities (events, dates or times) mentioned in a text. This interest has largely been driven by the two recent TempEval competitions. Even though logical constraints on the structure of possible sets of temporal relations are obvious, this sort of information deserves more exploration in the context of temporal relation classification. In this paper, we show that logical inference can be used to improvexe2x80x94sometimes dramaticallyxe2x80x94 existing machine learned classifiers for the problem of temporal relation classification."
W12-3409,Assigning Deep Lexical Types Using Structured Classifier Features for Grammatical Dependencies,2012,20,1,2,0.954257,8865,joao silva,Proceedings of the {ACL} 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages,0,"Deep linguistic grammars are able to provide rich and highly complex grammatical representations of sentences, capturing, for instance, long-distance dependencies and returning a semantic representation. These grammars lack robustness in the sense that they do not gracefully handle words missing from their lexicon. Several approaches have been explored to handle this problem, many of which consist in pre-annotating the input to the grammar with shallow processing machine-learning tools. Most of these tools, however, use features based on a fixed window of context, such as n-grams. We investigate whether the use of features that encode discrete structures, namely grammatical dependencies, can improve the performance of a machine learning classifier that assigns deep lexical types. In this paper we report on the design and evaluation of this classifier."
costa-branco-2012-timebankpt,{T}ime{B}ank{PT}: A {T}ime{ML} Annotated Corpus of {P}ortuguese,2012,17,15,2,1,41166,francisco costa,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we introduce TimeBankPT, a TimeML annotated corpus of Portuguese. It has been produced by adapting an existing resource for English, namely the data used in the first TempEval challenge. TimeBankPT is the first corpus of Portuguese with rich temporal annotations (i.e. it includes annotations not only of temporal expressions but also about events and temporal relations). In addition, it was subjected to an automated error mining procedure that checks the consistency of the annotated temporal relations based on their logical properties. This procedure allowed for the detection of some errors in the annotations, that also affect the original English corpus. The Portuguese language is currently undergoing a spelling reform, and several countries where Portuguese is official are in a transitional period where old and new orthographies are valid. TimeBankPT adopts the recent spelling reform. This decision is to preserve its future usefulness. TimeBankPT is freely available for download."
branco-etal-2012-propbank,A {P}rop{B}ank for {P}ortuguese: the {CINTIL}-{P}rop{B}ank,2012,12,5,1,1,8863,antonio branco,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"With the CINTIL-International Corpus of Portuguese, an ongoing corpus annotated with fully flegded grammatical representation, sentences get not only a high level of lexical, morphological and syntactic annotation but also a semantic analysis that prepares the data to a manual specification step and thus opens the way for a number of tools and resources for which there is a great research focus at the present. This paper reports on the construction of a propbank that builds on CINTIL-DeepGramBank, with nearly 10 thousand sentences, on the basis of a deep linguistic grammar and on the process and the linguistic criteria guiding that construction, which makes possible to obtain a complete PropBank with both syntactic and semantic levels of linguistic annotation. Taking into account this and the promising scores presented in this study for inter-annotator agreement, CINTIL-PropBank presents itself as a great resource to train a semantic role labeller, one of our goals with this project."
goncalves-etal-2012-treebanking,Treebanking by Sentence and Tree Transformation: Building a Treebank to support Question Answering in {P}ortuguese,2012,6,4,3,1,40064,patricia gonccalves,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents CINTIL-QATreebank, a treebank composed of Portuguese sentences that can be used to support the development of Question Answering systems. To create this treebank, we use declarative sentences from the pre-existing CINTIL-Treebank and manually transform their syntactic structure into a non-declarative sentence. Our corpus includes two clause types: interrogative and imperative clauses. CINTIL-QATreebank can be used in language science and techology general research, but it was developed particularly for the development of automatic Question Answering systems. The non-declarative entences are annotated with several layers of linguistic information, namely (i) trees with information on constituency and grammatical function; (ii) sentence type; (iii) interrogative pronoun; (iv) question type; and (v) semantic type of expected answer. Moreover, these non-declarative sentences are paired with their declarative counterparts and associated with the expected answer snippets."
E12-1027,Aspectual Type and Temporal Relation Classification,2012,33,9,2,1,41166,francisco costa,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper we investigate the relevance of aspectual type for the problem of temporal information processing, i.e. the problems of the recent TempEval challenges.n n For a large list of verbs, we obtain several indicators about their lexical aspect by querying the web for expressions where these verbs occur in contexts associated with specific aspectual types.n n We then proceed to extend existing solutions for the problem of temporal information processing with the information extracted this way. The improved performance of the resulting models shows that (i) aspectual type can be data-mined with unsupervised methods with a level of noise that does not prevent this information from being useful and that (ii) temporal information processing can profit from information about aspectual type."
W11-4524,Uma abordagem de classifica{\\c{c}}{\\~a}o autom{\\'a}tica para Tipo de Pergunta e Tipo de Resposta (An Automatic Approach for Classification of Question Type and Answer Type) [in {P}ortuguese],2011,-1,-1,2,1,40064,patricia gonccalves,Proceedings of the 8th {B}razilian Symposium in Information and Human Language Technology,0,None
P10-1069,Temporal Information Processing of a New Language: Fast Porting with Minimal Resources,2010,8,9,2,1,41166,francisco costa,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We describe the semi-automatic adaptation of a TimeML annotated corpus from English to Portuguese, a language for which TimeML annotated data was not available yet. In order to validate this adaptation, we use the obtained data to replicate some results in the literature that used the original English data. The fact that comparable results are obtained indicates that our approach can be used successfully to rapidly create semantically annotated resources for new languages."
silva-etal-2010-top,Top-Performing Robust Constituency Parsing of {P}ortuguese: Freely Available in as Many Ways as you Can Get it,2010,8,6,2,0.954257,8865,joao silva,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper we present LX-Parser, a probabilistic, robust constituency parser for Portuguese. This parser achieves ca. 88{\%} f-score in the labeled bracketing task, thus reaching a state-of-the-art performance score that is in line with those that are currently obtained by top-ranking parsers for English, the most studied natural language. To the best of our knowledge, LX-Parser is the first state-of-the-art, robust constituency parser for Portuguese that is made freely available. This parser is being distributed in a variety of ways, each suited for a different type of usage. More specifically, LX-Parser is being made available (i) as a downloadable, stand-alone parsing tool that can be run locally by its users; (ii) as a Web service that exposes an interface that can be invoked remotely and transparently by client applications; and finally (iii) as an on-line parsing service, aimed at human users, that can be accessed through any common Web browser."
branco-etal-2010-developing,Developing a Deep Linguistic Databank Supporting a Collection of Treebanks: the {CINTIL} {D}eep{G}ram{B}ank,2010,11,18,1,1,8863,antonio branco,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Corpora of sentences annotated with grammatical information have been deployed by extending the basic lexical and morphological data with increasingly complex information, such as phrase constituency, syntactic functions, semantic roles, etc. As these corpora grow in size and the linguistic information to be encoded reaches higher levels of sophistication, the utilization of annotation tools and, above all, supporting computational grammars appear no longer as a matter of convenience but of necessity. In this paper, we report on the design features, the development conditions and the methodological options of a deep linguistic databank, the CINTIL DeepGramBank. In this corpus, sentences are annotated with fully fledged linguistically informed grammatical representations that are produced by a deep linguistic processing grammar, thus consistently integrating morphological, syntactic and semantic information. We also report on how such corpus permits to straightforwardly obtain a whole range of past generation annotated corpora (POS, NER and morphology), current generation treebanks (constituency treebanks, dependency banks, propbanks) and next generation databanks (logical form banks) simply by means of a very residual selection/extraction effort to get the appropriate ''``views'''' exposing the relevant layers of information."
W09-4406,Language Independent System for Definition Extraction: First Results Using Learning Algorithms,2009,28,4,2,1,17856,rosa gaudio,Proceedings of the 1st Workshop on Definition Extraction,0,"In this paper we report on the performance of different learning algorithms and different sampling technique applied to a definition extraction task, using data sets in different language. We compare our results with those obtained by handcrafted rules to extract definitions. When Definition Extraction is handled with machine learning algorithms, two different issues arise. On the one hand, in most cases the data set used to extract definitions is unbalanced, and this means that it is necessary to deal with this characteristic with specific techniques. On the other hand it is possible to use the same methods to extract definitions from documents in different corpus, making the classifier language independent."
P09-4002,{LX}-Center: a center of online linguistic services,2009,2,2,1,1,8863,antonio branco,Proceedings of the {ACL}-{IJCNLP} 2009 Software Demonstrations,0,"This is a paper supporting the demonstration of the LX-Center at ACL-IJCNLP-09. LX-Center is a web center of online linguistic services aimed at both demonstrating a range of language technology tools and at fostering the education, research and development in natural language science and technology."
W08-2204,High Precision Analysis of {NP}s with a Deep Processing Grammar,2008,14,1,1,1,8863,antonio branco,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"In this paper we present LXGram, a general purpose grammar for the deep linguistic processing of Portuguese that aims at delivering detailed and high precision meaning representations. LXGram is grounded on the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG). HPSG is a declarative formalism resorting to unification and a type system with multiple inheritance. The semantic representations that LXGram associates with linguistic expressions use the Minimal Recursion Semantics (MRS) format, which allows for the underspecification of scope effects. LXGram is developed in the Linguistic Knowledge Builder (LKB) system, a grammar development environment that provides debugging tools and efficient algorithms for parsing and generation. The implementation of LXGram has focused on the structure of Noun Phrases, and LXGram accounts for many NP related phenomena. Its coverage continues to be increased with new phenomena, and there is active work on extending the grammar's lexicon. We have already integrated, or plan to integrate, LXGram in a few applications, namely paraphrasing, treebanking and language variant detection. Grammar coverage has been tested on newspaper text."
W08-2224,{LXG}ram in the Shared Task {``}{C}omparing {S}emantic {R}epresentations{''} of {STEP} 2008,2008,10,4,1,1,8863,antonio branco,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"LXGram is a hand-built Portuguese computational grammar based on HPSG (syntax) and MRS (semantics). The LXGram system participated in the STEP 2008 shared task which aims at comparing semantic representations produced by NLP systems such as LXGram. Every participating team had to contribute a small text. The text that we submitted for the shared task was originally in Portuguese (an excerpt from a newspaper) and translated into English, to make a meaningful comparison at the shared task possible. Likewise, the English texts contributed by the other participating teams were translated into Portuguese. Because the LXGram generates many different analyses (mainly due to PP attachment ambiguities), the preferred analysis was selected manually. It was required to extend LXGram's lexicon and inventory of syntax rules to be able to get a reasonable performance on the shared task data. Eventually, our system was able to produce an analysis for 20 out of the 30 sentences of the shared task data."
orasan-etal-2008-anaphora,Anaphora Resolution Exercise: an Overview,2008,-1,-1,4,0,25217,constantin oruasan,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Evaluation campaigns have become an established way to evaluate automatic systems which tackle the same task. This paper presents the first edition of the Anaphora Resolution Exercise (ARE) and the lessons learnt from it. This first edition focused only on English pronominal anaphora and NP coreference, and was organised as an exploratory exercise where various issues were investigated. ARE proposed four different tasks: pronominal anaphora resolution and NP coreference resolution on a predefined set of entities, pronominal anaphora resolution and NP coreference resolution on raw texts. For each of these tasks different inputs and evaluation metrics were prepared. This paper presents the four tasks, their input data and evaluation metrics used. Even though a large number of researchers in the field expressed their interest to participate, only three institutions took part in the formal evaluation. The paper briefly presents their results, but does not try to interpret them because in this edition of ARE our aim was not about finding why certain methods are better, but to prepare the ground for a fully-fledged edition."
branco-etal-2008-lx,{LX}-Service: Web Services of Language Technology for {P}ortuguese,2008,3,3,1,1,8863,antonio branco,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In the present paper we report on the development of a cluster of web services of language technology for Portuguese that we named as LXService. These web services permit the direct interaction of client applications with language processing tools via the Internet. This way of making available language technology was motivated by the need of its integration in an eLearning environment. In particular, it was motivated by the development of new multilingual functionalities that were aimed at extending a Learning Management System and that needed to resort to the outcome of some of those tools in a distributed and remote fashion. This specific usage situation happens however to be representative of a typical and recurrent set up in the utilization of language processing tools in different settings and projects. Therefore, the approach reported here offers not only a solution for this specific problem, which immediately motivated it, but contributes also some first steps for what we see as an important paradigm shift in terms of the way language technology can be distributed and find a better way to unleash its full potential and impact."
W07-1208,Self- or Pre-Tuning? Deep Linguistic Processing of Language Variants,2007,7,3,1,1,8863,antonio branco,{ACL} 2007 Workshop on Deep Linguistic Processing,0,"This paper proposes a design strategy for deep language processing grammars to appropriately handle language variants. It allows a grammar to be restricted as to what language variant it is tuned to, but also to detect the variant a given input pertains to. This is evaluated and compared to results obtained with an alternative strategy by which the relevant variant is detected with current language identification methods in a preprocessing step."
barreto-etal-2006-open,Open Resources and Tools for the Shallow Processing of {P}ortuguese: The {T}ag{S}hare Project,2006,6,22,2,0,50267,florbela barreto,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper presents the TagShare project and the linguistic resources and tools for the shallow processing of Portuguese developed in its scope. These resources include a 1 million token corpus that has been accurately hand annotated with a variety of linguistic information, as well as several state of the art shallow processing tools capable of automatically producing that type of annotation. At present, the linguistic annotations in the corpus are sentence and paragraph boundaries, token boundaries, morphosyntactic POS categories, values of inflection features, lemmas and namedentities. Hence, the set of tools comprise a sentence chunker, a tokenizer, a POS tagger, nominal and verbal analyzers and lemmatizers, a verbal conjugator, a nominal ÂinflectorÂ, and a namedentity recognizer, some of which underline several online services."
E06-2024,A Suite of Shallow Processing Tools for {P}ortuguese: {LX}-Suite,2006,4,31,1,1,8863,antonio branco,Demonstrations,0,"In this paper we present LX-Suite, a set of tools for the shallow processing of Portuguese. This suite comprises several modules, namely: a sentence chunker, a tokenizer, a POS tagger, featurizers and lemmatizers."
branco-silva-2004-evaluating,Evaluating Solutions for the Rapid Development of State-of-the-Art {POS} Taggers for {P}ortuguese,2004,0,31,1,1,8863,antonio branco,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
branco-etal-2002-nexing,Nexing Corpus: a corpus of verbal protocols on syllogistic reasoning,2002,4,0,1,1,8863,antonio branco,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In this paper, we describe the Nexing Corpus and report on the tools implemented and the tasks undertaken for its development. The Nexing Corpus includes (i) a collection of written transcriptions of verbal data elicited during a psycholinguistic experiment on syllogistic reasoning; and (ii) performance data concerning that experiment, such as latencies, confidence levels and accuracy of answers provided. The verbal productions recorded in the corpus are of a specific linguistic type that is seldom, if at all, represented in corpora. These data are relevant for the development of human language technologies aimed at modeling this type of linguistic behavior, which is not uncommon in evolved interactions of cooperative agents. This corpus with thinking aloud data on syllogistic reasoning is also an important source of material for cognitive science, in particular for research on the nature of human deductive reasoning."
J02-1001,Binding Machines,2002,24,23,1,1,8863,antonio branco,Computational Linguistics,0,"Binding constraints form one of the most robust modules of grammatical knowledge. Despite their crosslinguistic generality and practical relevance for anaphor resolution, they have resisted full integration into grammar processing. The ultimate reason for this is to be found in the original exhaustive coindexation rationale for their specification and verification. As an alternative, we propose an approach which, while permitting a unification-based specification of binding constraints, allows for a verification methodology that helps to overcome previous drawbacks. This alternative approach is based on the rationale that anaphoric nominals can be viewed as binding machines."
C00-1016,Binding Constraints as Instructions of Binding Machines,2000,16,0,1,1,8863,antonio branco,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,Binding constraints have resisted to be fully integrated into the course of grammatical processing despite its practical relevance and cross-linguistic generality. The ultimate root for this is to be found in the exponential overgenerate & filter procedure of the mainstream rationale for their satisfaction. In this paper we design an alternative approach based on the view that nominals are binding machines.
P98-1027,The Logical Structure of Binding,1998,12,1,1,1,8863,antonio branco,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,A logical recasting of Binding Theory is performed as an enhancing step for the purpose of its full and lean declarative implementation. A new insight on sentential anaphoric processes is presented which may suggestively be captured by the slogan binding conditions are the effect of phase quantification on the universe of discourse referents.
C98-1027,The Logical Structure of Binding,1998,12,1,1,1,8863,antonio branco,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,A logical recasting of Binding Theory is performed as an enhancing step for the purpose of its full and lean declarative implementation. A new insight on sentential anaphoric processes is presented which may suggestively be captured by the slogan binding conditions are the effect of phase quantification on the universe of discourse referents.
Y96-1003,Subject-oriented and non Subject-oriented Long-distance Anaphora : an Integrated Approach,1996,8,3,1,1,8863,antonio branco,"Proceedings of the 11th Pacific Asia Conference on Language, Information and Computation",0,"We discuss data showing that, unlike other long-distance anaphors widely documented in the literature (e.g. ziji from Chinese, caki from Korean, zibun, from Japanese, etc.), the Portuguese ele prOprio is not subject-oriented. This supports a reformulation of Principle Z, encompassing subject-oriented and non subject-oriented long-distance anaphora, which shows up as the fourth binding principle. The striking internal congruence of the resulting four principle based Binding Theory cogently makes it apparent that the binding symmetries are far more richer than the problematic single distributional symmetry between anaphors and pronouns that has been continuously assumed to hold by most of the research of the last three decades. We also discuss how the data involving the Portuguese long-distance anaphor add to the growing evidence that the generalization assumed in mainstream GB approaches about the universal correlation between simplex anaphors, longdistance anaphora, subject-orientedness and intermediate blocking effects is most likely not to be empirically grounded."
C96-1027,Branching Split Obliqueness at the Syntax-Semantics Interface,1996,6,4,1,1,8863,antonio branco,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,In this paper it is argued that the accuracy of the syntax-semantics interface is improved by adopting a non-linear obliqueness hierarchy for subcategorized arguments.
