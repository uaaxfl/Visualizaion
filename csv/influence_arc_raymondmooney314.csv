2020.aacl-main.49,P16-5005,0,0.0230902,"d world’s embedding at multiple scales, as done by Wang and Lake (2019). The final grid world encoding is as follows: s H = [H1s ; H2s ; H3s ] , His s0 = Convi (X ) (10) where edi is the embedding of the previous output action token yi−1 , cci is the instruction context computed with attention over textual encoder’s hidden states [hc1 , hc2 , ..., hcS ], and csi is the grid world context computed with attention over all locations in the grid world embedding H S . We set the decoder’s hidden size to 64 so that it aligns with the textual encoder, and use the attention implementation proposed by Bahdanau et al. (2016). The instruction context is computed as: ecij = vcT tanhWc (hdi−1 + hcj ) (11) exp(ecij ) c αij = PS c j=1 exp(eij ) (12) cci = S X j=1 (9) 495 c c αij hj , ∀j ∈ {1, 2, ..., S} (13) Similarly, the grid world context is computed as: esij = vsT tanhWs (hdi−1 + cci ) (14) exp(esij ) Pd2 s j=1 exp(eij ) (15) s αij = 2 csi = d X s s αij hj , ∀j ∈ {1, 2, ..., d2 } (16) j=1 where vs , vc , Wc , Ws are learnable parameters, and hsj is the embedding of grid j obtained from H S . The distribution of next action token can then be computed as p(yi |x, y1 , y2 , ..., yi−1 ) = sof tmax(Wo hdi ). 4 Experime"
2020.aacl-main.49,N18-2017,0,0.0476617,"Missing"
2020.aacl-main.49,D17-1215,0,0.0334958,"al question answering (VQA), image captioning, and vision-and-language navigation (Hudson and Manning, 2018; Anderson et al., 2018a,b). Despite all the success, recent literature shows that current deep learning approaches are exploiting statistical patterns discovered in the datasets to achieve high performance, an approach that does not achieve systematic generalization. Gururangan et al. (2018) discovered that annotation artifacts like negation words or purpose clauses in natural language inference data can be used by simple text classification categorization model to solve the given task. Jia and Liang (2017) demonstrated that adversarial examples can fool reading comprehension systems. Indeed, deep learning models often fail to achieve systematic generalizations even on tasks on which they are claimed to perform well. As shown by Bahdanau et al. (2018), state-of-the-art Visual Questioning Answering (VQA) (Hudson and Manning, 2018; Perez et al., 2018) models fail dramatically even on a synthetic VQA dataset designed with systematic difference between training and test sets. In this work, we focus on approaching systematic generalization in grounded natural language understanding tasks. We experime"
2020.acl-main.168,D19-3009,0,0.0256445,"Missing"
2020.acl-main.168,D19-1435,0,0.146389,"for disregarding Keep and the span of tokens to which it applies is that we can simply copy the content that is retained between Cold and Cnew , instead of generating it anew. By doing posthoc copying, we simplify learning for the model since it has to only learn what to change rather than also having to learn what to keep. We design a method to deterministically place edits in their correct positions in the absence of 4 Preliminary experiments showed that this performed better than structuring edits at the token-level as in other tasks (Shin et al., 2018; Li et al., 2018; Dong et al., 2019; Awasthi et al., 2019). 1855 spans. For the example in Figure 1, the raw sequence <Insert>in degrees<InsertEnd> does not encode information as to where “in degrees” should be inserted. To address this, we bind an insert sequence with the minimum number of words (aka “anchors”) such that the place of insertion can be uniquely identified. This results in the structure that is shown for Cedit in Figure 2. Here “angle” serves as the anchor point, identifying the insert location. Following the structure of Replace, this sequence indicates that “angle” should be replaced with “angle in degrees,” effectively inserting “in"
2020.acl-main.168,W05-0909,0,0.202241,"C’new ). This model gives preference to comments that are more likely for Mnew and are more consistent with the general style of comments.5 Similarity to Cold . So far, our model is mainly trained to produce accurate edits; however, we also follow intuitions that edits should be minimal (as an analogy, the use of Levenshtein distance in spelling correction). To give preference to predictions that accurately update the comment with minimal modifications, we use similarity to Cold as a heuristic for reranking. We measure similarity between the parsed candidate prediction and Cold using METEOR (Banerjee and Lavie, 2005). Reranking score. The reranking score for each candidate is a linear combination of the original beam score, the generation likelihood, and the similarity to Cold with coefficients 0.5, 0.3, and 0.2 respectively (tuned on validation data). 7 Data We extracted examples from popular, open-source Java projects using GitHub’s commit history. We extract pairs of the form (method, comment) for the same method across two consecutive commits where there is a simultaneous change to both the code and comment. This creates somewhat noisy data for the task of comment update; Appendix B describes filterin"
2020.acl-main.168,D12-1091,0,0.0163893,"text-editing metrics to measure how well our system learns to edit: SARI (Xu et al., 2016), originally proposed to evaluate text simplification, is essentially the average of N-gram F1 scores corresponding to add, delete, and keep edit operations;7 GLEU (Napoles et al., 2015), used in grammatical error correction and style transfer, takes into account the source sentence and deviates from BLEU by giving more importance to n-grams that have been correctly changed. Results: We report automatic metrics averaged across three random initializations for all learned models, and use bootstrap tests (Berg-Kirkpatrick et al., 2012) for statistical significance. Table 2 presents the results. While reranking using Cold appears to help the generation model, it still substantially underperforms all other models, across all metrics. Although this model is trained on considerably more data, it does not have access to Cold during training and uses fewer inputs and consequently has less context than the edit model. Reranking slightly deteriorates the edit model’s 7 Although the original formulation only used precision for the delete operation, more recent work computes F1 for this as well (Dong et al., 2019; Alva-Manchego et al"
2020.acl-main.168,P19-1331,0,0.135615,"ence. The intuition for disregarding Keep and the span of tokens to which it applies is that we can simply copy the content that is retained between Cold and Cnew , instead of generating it anew. By doing posthoc copying, we simplify learning for the model since it has to only learn what to change rather than also having to learn what to keep. We design a method to deterministically place edits in their correct positions in the absence of 4 Preliminary experiments showed that this performed better than structuring edits at the token-level as in other tasks (Shin et al., 2018; Li et al., 2018; Dong et al., 2019; Awasthi et al., 2019). 1855 spans. For the example in Figure 1, the raw sequence <Insert>in degrees<InsertEnd> does not encode information as to where “in degrees” should be inserted. To address this, we bind an insert sequence with the minimum number of words (aka “anchors”) such that the place of insertion can be uniquely identified. This results in the structure that is shown for Cedit in Figure 2. Here “angle” serves as the anchor point, identifying the insert location. Following the structure of Replace, this sequence indicates that “angle” should be replaced with “angle in degrees,” ef"
2020.acl-main.168,Q18-1031,0,0.0330691,"ang et al., 2017; Xu et al., 2019). Instead of generating natural language content from scratch as done in their work, we focus on applying edits to existing natural language text. We also show that generating a comment from scratch does not perform as well as our proposed edit model for the comment update setting. Editing natural language text: Approaches for editing natural language text have been studied extensively through tasks such as sentence simplification (Dong et al., 2019), style transfer (Li et al., 2018), grammatical error correction (Awasthi et al., 2019), and language modeling (Guu et al., 2018). The focus of this prior work is to revise sentences to conform to stylistic and grammatical conventions, and it does not generally consider broader contextual constraints. On the contrary, our goal is not to make cosmetic revisions to a given span of text, but rather amend its semantic meaning to be in sync with the content of a separate body of information on which it is dependent. More recently, Shah et al. (2020) proposed an approach for rewriting an outdated sentence based on a sentence stating a new factual claim, which is more closely aligned with our task. However, in our case, the se"
2020.acl-main.168,passonneau-2006-measuring,0,\N,Missing
2020.acl-main.168,D15-1166,0,\N,Missing
2020.acl-main.168,P02-1040,0,\N,Missing
2020.acl-main.168,P13-2007,0,\N,Missing
2020.acl-main.168,P15-2097,0,\N,Missing
2020.acl-main.168,D16-1230,0,\N,Missing
2020.acl-main.168,P16-1195,0,\N,Missing
2020.acl-main.168,Q16-1029,0,\N,Missing
2020.acl-main.168,N19-1317,0,\N,Missing
2020.acl-main.168,N19-1349,1,\N,Missing
2021.findings-acl.53,W19-5301,0,0.0427087,"Missing"
2021.findings-acl.53,2020.coling-main.52,1,0.738743,"Missing"
2021.findings-acl.53,2020.coling-main.210,0,0.0609817,"Missing"
2021.findings-acl.53,P09-1068,1,0.771513,"Missing"
2021.findings-acl.53,D19-5817,0,0.0571439,"Missing"
2021.findings-acl.53,L18-1438,0,0.0462471,"Missing"
2021.findings-acl.53,W13-2305,0,0.0125685,"t all. StrategyQA (Geva et al., 2021) is a new dataset focusing on performing better implicit reasoning for multi-hop question answering tasks. We summarize the different why-questions corpora in Table 1. None of them represent a large dataset focused on answering why-questions about actions in a narrative. 2.2 Human evaluation for NLG tasks Among language generation tasks, machine translation has received the most attention in terms of human evaluation. Qualified crowd workers score output translations given the source or reference text to calibrate MT systems (Sakaguchi and Van Durme, 2018; Graham et al., 2013, 2014). WMT conducts annual evaluation of outputs of systems submitted to the shared task and uses it as one of the primary metrics (along with BLEU) to rank systems (Bojar et al., 2016, 2017, 2018; Barrault et al., 2019, 2020). ChatEval (Sedoc et al., 2019) is an evaluation platform for chatbots. Zellers et al. (2020) present a leaderboard for their advice generation task. These platforms incorporate some manual analysis, but focus on very different tasks. None of their Mechanical Turk interfaces can be used for our task. We were unable to find a consistent interface for human evaluation of"
2021.findings-acl.53,E14-1047,0,0.0437022,"Missing"
2021.findings-acl.53,I08-1055,0,0.0948131,"Missing"
2021.findings-acl.53,2020.inlg-1.23,0,0.0332554,"Missing"
2021.findings-acl.53,2020.findings-emnlp.171,0,0.020028,"s can be found in Appendix A. We finetuned a pretrained T5-base model from HuggingFace (Wolf et al., 2020) on TellMeWhy. Since it is a natural language generation task related to a story, we use the SQuAD format specified in Appendix D.15 of Raffel et al. (2020) to format our inputs. Our narrative serves as the ‘context’ and the why-question is used as the ‘question’ in the selected input format. We train the model with batch size 16, learning rate 5e-5, maximum source length 75 and maximum answer length 30. The model is trained until the dev loss fails to improve for 3 iterations. UnifiedQA (Khashabi et al., 2020) is a single pretrained model that performs well across 20 different question answering datasets. It is built on top of a T5 model and simplifies finetuning by unifying the various formats used by T5. Its ability to perform both extractive and abstractive QA tasks makes it a suitable candidate for calibrating this task. A pretrained version of this model is available via HuggingFace (Wolf et al., 2020) under the name “allenai/unifiedqa-t5-base&quot;. The input format for this model is simple, just requiring the question and the narrative to be separated by a newline symbol. We train this model usin"
2021.findings-acl.53,Q18-1023,0,0.0495884,"Missing"
2021.findings-acl.53,D17-1082,0,0.0222405,"Missing"
2021.findings-acl.53,W04-1013,0,0.0948406,"d version of this model is available via HuggingFace (Wolf et al., 2020) under the name “allenai/unifiedqa-t5-base&quot;. The input format for this model is simple, just requiring the question and the narrative to be separated by a newline symbol. We train this model using learning rate 1e-5 (same as the original paper) and retain other hyperparameters from finetuning T5 as described above. 5.2 Automatic Evaluation We evaluate all of the above models on both the test set and the hidden test set (questions from CATERS data). For automatic evaluation, we report BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), BLEURT (Sellam et al., 2020) scores using the bluert-base-128 checkpoint, and BertScore (Zhang* et al., 2020) using the default roberta-large checkpoint. These numbers are presented in Table 4. Evaluated on Model BLEU RG-L F1 BLEURT BertScore Full Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.96 9.89 24.53 21.97 0.07 0.13 0.13 0.24 0.25 -1.23 -0.75 -0.963 -0.28 -0.30 -0.55 0.18 0.23 0.48 0.43 Implicit-Answer Qs in Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.89 8.14 16.31 14.6 0.06 0.12 0.11 0.17 0.18 -1.22 -0.805 -0.99 -0.51 -0.50 -0.54 0.17 0.22 0.38 0.34 Table 4: Perfor"
2021.findings-acl.53,W19-5302,0,0.0309291,"Missing"
2021.findings-acl.53,W16-1007,1,0.940516,"re external to the narrative, thus providing a challenge for future QA and narrative understanding research. 1 Introduction The actions people perform are steps of plans to achieve their desired goals. When interpreting language, humans naturally understand the reasons behind described actions, even when the reasons are left unstated (Schank and Abelson, 1975). For NLP systems, answering questions about why people perform actions in a narrative can test this ability. Answering such questions often requires filling the implicit gaps in the story itself. Consider this narrative from ROCStories (Mostafazadeh et al., 2016b): Rudy was convinced that bottled waters all tasted the same. He went to the store and bought several popular brands. He went back home and set them all on a table. He spent several hours tasting them one by one. He came to the conclusion that they actually did taste different. Now try to answer the question, “Why did he go to the store and buy several popular brands?” The answer “he wanted to taste test” is not explicit in the narrative and requires us to read between the lines to fill in the gaps (Norvig, 1987). While humans can visualise and process the events in a story to hypothesize wh"
2021.findings-acl.53,2020.emnlp-main.370,0,0.0743723,"Missing"
2021.findings-acl.53,P08-1051,0,0.0615989,"Missing"
2021.findings-acl.53,W19-4113,0,0.0603227,"Missing"
2021.findings-acl.53,N18-2012,0,0.0209232,"Missing"
2021.findings-acl.53,P19-1414,0,0.0541863,"Missing"
2021.findings-acl.53,N16-1098,1,0.828283,"re external to the narrative, thus providing a challenge for future QA and narrative understanding research. 1 Introduction The actions people perform are steps of plans to achieve their desired goals. When interpreting language, humans naturally understand the reasons behind described actions, even when the reasons are left unstated (Schank and Abelson, 1975). For NLP systems, answering questions about why people perform actions in a narrative can test this ability. Answering such questions often requires filling the implicit gaps in the story itself. Consider this narrative from ROCStories (Mostafazadeh et al., 2016b): Rudy was convinced that bottled waters all tasted the same. He went to the store and bought several popular brands. He went back home and set them all on a table. He spent several hours tasting them one by one. He came to the conclusion that they actually did taste different. Now try to answer the question, “Why did he go to the store and buy several popular brands?” The answer “he wanted to taste test” is not explicit in the narrative and requires us to read between the lines to fill in the gaps (Norvig, 1987). While humans can visualise and process the events in a story to hypothesize wh"
2021.findings-acl.53,P02-1040,0,0.114084,"alibrating this task. A pretrained version of this model is available via HuggingFace (Wolf et al., 2020) under the name “allenai/unifiedqa-t5-base&quot;. The input format for this model is simple, just requiring the question and the narrative to be separated by a newline symbol. We train this model using learning rate 1e-5 (same as the original paper) and retain other hyperparameters from finetuning T5 as described above. 5.2 Automatic Evaluation We evaluate all of the above models on both the test set and the hidden test set (questions from CATERS data). For automatic evaluation, we report BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), BLEURT (Sellam et al., 2020) scores using the bluert-base-128 checkpoint, and BertScore (Zhang* et al., 2020) using the default roberta-large checkpoint. These numbers are presented in Table 4. Evaluated on Model BLEU RG-L F1 BLEURT BertScore Full Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.96 9.89 24.53 21.97 0.07 0.13 0.13 0.24 0.25 -1.23 -0.75 -0.963 -0.28 -0.30 -0.55 0.18 0.23 0.48 0.43 Implicit-Answer Qs in Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.89 8.14 16.31 14.6 0.06 0.12 0.11 0.17 0.18 -1.22 -0.805 -0.99 -0.51 -0.50 -0.54 0.17 0.22 0.38"
2021.findings-acl.53,E14-1024,1,0.870934,"Missing"
2021.findings-acl.53,W18-6319,0,0.0116462,".45 3.96 9.89 24.53 21.97 0.07 0.13 0.13 0.24 0.25 -1.23 -0.75 -0.963 -0.28 -0.30 -0.55 0.18 0.23 0.48 0.43 Implicit-Answer Qs in Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.89 8.14 16.31 14.6 0.06 0.12 0.11 0.17 0.18 -1.22 -0.805 -0.99 -0.51 -0.50 -0.54 0.17 0.22 0.38 0.34 Table 4: Performance of models on the full test set and on implicit-answer questions in the test set using automated metrics. RG-L denotes ROUGE-L. The OO suffix denotes the vanilla version of the model while the FT version denotes the finetuned version. We select one human answer at a time and (using SacreBLEU (Post, 2018)) calculate the BLEU scores for model output with all three references, and select the maximum. Since BLEURT is a sentence level metric, to calculate the reported BLEURT, we average all the (output, reference) scores to obtain a corpus score for each reference. We then select the maximum BLEURT corpus score over all 3 human references. It is important to note that BLEURT was proposed as a metric for relative comparison, not absolute calibration. We also report BertScore F13 (Zhang* et al., 2020) as another semantic automatic evaluation metric. We report a max BertScore in the same way as BLEUR"
2021.findings-acl.53,P18-1020,0,0.0423673,"Missing"
2021.findings-acl.53,W19-8610,0,0.026832,"Missing"
2021.findings-acl.53,N19-4011,0,0.0164576,"ering why-questions about actions in a narrative. 2.2 Human evaluation for NLG tasks Among language generation tasks, machine translation has received the most attention in terms of human evaluation. Qualified crowd workers score output translations given the source or reference text to calibrate MT systems (Sakaguchi and Van Durme, 2018; Graham et al., 2013, 2014). WMT conducts annual evaluation of outputs of systems submitted to the shared task and uses it as one of the primary metrics (along with BLEU) to rank systems (Bojar et al., 2016, 2017, 2018; Barrault et al., 2019, 2020). ChatEval (Sedoc et al., 2019) is an evaluation platform for chatbots. Zellers et al. (2020) present a leaderboard for their advice generation task. These platforms incorporate some manual analysis, but focus on very different tasks. None of their Mechanical Turk interfaces can be used for our task. We were unable to find a consistent interface for human evaluation of an open-ended question answering task. To address this flaw, we propose a standard human intelligence task (HIT) evaluation scheme for our dataset. 3 Dataset Creation We want to test the abilities of models to understand the reasoning behind actions in a stor"
2021.findings-acl.53,2020.acl-main.704,0,0.0178184,"del is available via HuggingFace (Wolf et al., 2020) under the name “allenai/unifiedqa-t5-base&quot;. The input format for this model is simple, just requiring the question and the narrative to be separated by a newline symbol. We train this model using learning rate 1e-5 (same as the original paper) and retain other hyperparameters from finetuning T5 as described above. 5.2 Automatic Evaluation We evaluate all of the above models on both the test set and the hidden test set (questions from CATERS data). For automatic evaluation, we report BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), BLEURT (Sellam et al., 2020) scores using the bluert-base-128 checkpoint, and BertScore (Zhang* et al., 2020) using the default roberta-large checkpoint. These numbers are presented in Table 4. Evaluated on Model BLEU RG-L F1 BLEURT BertScore Full Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.96 9.89 24.53 21.97 0.07 0.13 0.13 0.24 0.25 -1.23 -0.75 -0.963 -0.28 -0.30 -0.55 0.18 0.23 0.48 0.43 Implicit-Answer Qs in Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.89 8.14 16.31 14.6 0.06 0.12 0.11 0.17 0.18 -1.22 -0.805 -0.99 -0.51 -0.50 -0.54 0.17 0.22 0.38 0.34 Table 4: Performance of models on the full te"
2021.findings-acl.53,verberne-etal-2006-data,0,0.170228,"Missing"
C10-1062,A97-1024,0,0.0490301,"o predict readability: syntactic features, language-model features, and lexical features, as described below. 5.1 We want to answer the question whether a machine can accurately estimate readability as judged by a human. Therefore, we built a machine-learning system that predicts the readFeatures Based on Syntax Many times, a document is found to be unreadable due to unusual linguistic constructs or ungram548 1 http://www.cs.waikato.ac.nz/ml/weka/ matical language that tend to manifest themselves in the syntactic properties of the text. Therefore, syntactic features have been previously used (Bernth, 1997) to gauge the “clarity” of written text, with the goal of helping writers improve their writing skills. Here too, we use several features based on syntactic analyses. Syntactic analyses are obtained from the Sundance shallow parser (Riloff and Phillips, 2004) and from the English Slot Grammar (ESG) (McCord, 1989). Sundance features: The Sundance system is a rule-based system that performs a shallow syntactic analysis of text. We expect that this analysis over readable text would be “well-formed”, adhering to grammatical rules of the English language. Deviations from these rules can be indicati"
C10-1062,N04-1025,0,0.76587,"ability. The evaluation was then designed to compare how well machine and naive human judges predict expert human judgements. In order to make the machine’s predicted score comparable to a human judge’s score (details about our evaluation metrics are in Section 6.1), we also restricted the machine scores to integers. Hence, the task is to predict an integer score from 1 to 5 that measures the readability of the document. This task could be modeled as a multi-class classification problem treating each integer score as a separate class, as done in some of the previous work (Si and Callan, 2001; Collins-Thompson and Callan, 2004). However, since the classes are numerical and not unrelated (for example, the score 2 is in between scores 1 and 3), we decided to model the task as a regression problem and then round the predicted score to obtain the closest integer value. Preliminary results verified that regression performed better than classification. Heilman et al. (2008) also found that it is better to treat the readability scores as ordinal than as nominal. We take the average of the expert judge scores for each document as its goldstandard score. Regression was also used by Kanungo and Orr (2009), although their eval"
C10-1062,N07-1058,0,0.796875,"he documents. Some later methods use pre-determined lists of words to determine the grade level of a document, for example the Lexile measure (Stenner et al., 1988), the Fry Short Passage measure (Fry, 1990) and the Revised Dale-Chall formula (Chall and Dale, 1995). The word lists these methods use may be thought of as very simple language models. More recently, language models have been used for predicting the grade level of documents. Si and Callan (2001) and CollinsThompson and Callan (2004) train unigram language models to predict grade levels of documents. In addition to language models, Heilman et al. (2007) and Schwarm and Ostendorf (2005) also use some syntactic features to estimate the grade level of texts. Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. Their system predicts readability of texts from Wall Street Journal using lexical, syntactic and discourse features. Kanungo and Orr (2009) consider the task of predicting readability of web summary snippets produced by search engines. Using simple surface level features like the number of characters and syllables per word, capitalization, punctuation, ellipses etc. they train a re"
C10-1062,P05-1065,0,0.720084,"ethods use pre-determined lists of words to determine the grade level of a document, for example the Lexile measure (Stenner et al., 1988), the Fry Short Passage measure (Fry, 1990) and the Revised Dale-Chall formula (Chall and Dale, 1995). The word lists these methods use may be thought of as very simple language models. More recently, language models have been used for predicting the grade level of documents. Si and Callan (2001) and CollinsThompson and Callan (2004) train unigram language models to predict grade levels of documents. In addition to language models, Heilman et al. (2007) and Schwarm and Ostendorf (2005) also use some syntactic features to estimate the grade level of texts. Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. Their system predicts readability of texts from Wall Street Journal using lexical, syntactic and discourse features. Kanungo and Orr (2009) consider the task of predicting readability of web summary snippets produced by search engines. Using simple surface level features like the number of characters and syllables per word, capitalization, punctuation, ellipses etc. they train a regression model to predict readabi"
C10-1062,P96-1041,0,0.0157412,"ed features in categorizing text (McCallum and Nigam, 1998; Yang and Liu, 1999) and evaluating readability (Collins-Thompson and Callan, 2004; Heilman et al., 2007) has been investigated in previous work. In our experiments, however, since documents were acquired through several different channels, such as machine translation or web logs, 549 we also build models that try to predict the genre of a document. Since the genre information for many English documents is readily available, we trained a series of genre-specific 5-gram LMs using the modified Kneser-Ney smoothing (Kneser and Ney, 1995; Stanley and Goodman, 1996). Table 1 contains a list of a base LM and genrespecific LMs. Given a document D consisting of tokenized word sequence {wi : i = 1, 2, · · · , |D|}, its perplexity L(D|Mj ) with respect to a LM Mj is computed as: ¢ ¡ 1 P|D| − |D| i=1 log P (wi |hi ;Mj ) , (2) L(D|Mj ) = e where |D |is the number of words in D and hi are the history words for wi , and P (wi |hi ; Mj ) is the probability Mj assigns to wi , when it follows the history words hi . Posterior perplexities from genre-specific language models: While perplexities computed from genre-specific LMs reflect the absolute probability that a d"
C10-1062,W08-0909,0,0.545949,"score from 1 to 5 that measures the readability of the document. This task could be modeled as a multi-class classification problem treating each integer score as a separate class, as done in some of the previous work (Si and Callan, 2001; Collins-Thompson and Callan, 2004). However, since the classes are numerical and not unrelated (for example, the score 2 is in between scores 1 and 3), we decided to model the task as a regression problem and then round the predicted score to obtain the closest integer value. Preliminary results verified that regression performed better than classification. Heilman et al. (2008) also found that it is better to treat the readability scores as ordinal than as nominal. We take the average of the expert judge scores for each document as its goldstandard score. Regression was also used by Kanungo and Orr (2009), although their evaluation did not constrain machine scores to be integers. We tested several regression algorithms available in the Weka1 machine learning package, and in Section 6.2 we report results for several which performed best. The next section describes the numerically-valued features that we used as input for regression. 5 Features for Predicting Readabil"
C10-1062,W02-1028,0,0.00647861,"retations. Sometimes ESG’s grammar rules fail to produce a single complete interpretation of a sentence, in which case it generates partial parses. This typically happens in cases when sentences are ungrammatical, and possibly, less readable. Thus, we use the proportion of such incomplete parses within a document as a readability feature. In case of extremely short documents, this proportion of incomplete parses can be misleading. To account for such short documents, we introduce a variation of the above incomplete parse feature, by weighting it with a log factor as was done in (Riloff, 1996; Thelen and Riloff, 2002). We also experimented with some other syntactic features such as average sentence parse scores from Stanford parser and an in-house maximum entropy statistical parer, average constituent scores etc., however, they slightly degraded the performance in combination with the rest of the features and hence we did not include them in the final set. One possible explanation could be that averaging diminishes the effect of low scores caused by ungrammaticality. 5.2 Features Based on Language Models A probabilistic language model provides a prediction of how likely a given sentence was generated by th"
C10-1062,D08-1020,0,0.778707,"enner et al., 1988), the Fry Short Passage measure (Fry, 1990) and the Revised Dale-Chall formula (Chall and Dale, 1995). The word lists these methods use may be thought of as very simple language models. More recently, language models have been used for predicting the grade level of documents. Si and Callan (2001) and CollinsThompson and Callan (2004) train unigram language models to predict grade levels of documents. In addition to language models, Heilman et al. (2007) and Schwarm and Ostendorf (2005) also use some syntactic features to estimate the grade level of texts. Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. Their system predicts readability of texts from Wall Street Journal using lexical, syntactic and discourse features. Kanungo and Orr (2009) consider the task of predicting readability of web summary snippets produced by search engines. Using simple surface level features like the number of characters and syllables per word, capitalization, punctuation, ellipses etc. they train a regression model to predict readability values. Our work differs from this previous research in several ways. Firstly, the task we h"
C10-2062,W05-0602,1,0.403701,"erative alignment model that includes a full semantic parsing model proposed by Lu et al. (2008). Our approach is capable of disambiguating the mapping between language and meanings while also learning a complete semantic parser for mapping sentences to logical form. Experimental results on Robocup sportscasting show that our approach outperforms all previous results on the NL–MR matching (alignment) task and also produces competitive performance on semantic parsing and improved language generation. 2 Related Work The conventional approach to learning semantic parsers (Zelle and Mooney, 1996; Ge and Mooney, 2005; Kate and Mooney, 2006; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007b; Lu et al., 2008) requires detailed supervision unambiguously pairing each sentence with its logical form. However, developing training corpora for these methods requires expensive expert human labor. Chen and Mooney (2008) presented methods for grounded language learning from ambiguous supervision that address three related tasks: NL–MR alignment, semantic parsing, and natural language generation. They solved the problem of aligning sentences and meanings by iteratively retraining an"
C10-2062,P06-1115,1,0.938645,"aneously learning a semantic parser that maps sentences into logical form. Compared to a previous generative model for semantic alignment, it also supports full semantic parsing. Experimental results on the Robocup sportscasting corpora in both English and Korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators. 1 Introduction Most approaches to learning semantic parsers that map sentences into complete logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007b; Lu et al., 2008) require fullysupervised corpora that provide full formal logical representations for each sentence. Such corpora are expensive and difficult to construct. Several recent projects on “grounded” language learning (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010; Liang et al., 2009) exploit more easily and naturally available training data consisting of sentences paired with world states In particular, Chen and Mooney (2008) introduced the problem of learning to sportscast by simply observing natural language commentary on simulated Roboc"
C10-2062,P09-1011,0,0.0891028,"isting methods and also produces competitive semantic parsers and improved language generators. 1 Introduction Most approaches to learning semantic parsers that map sentences into complete logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007b; Lu et al., 2008) require fullysupervised corpora that provide full formal logical representations for each sentence. Such corpora are expensive and difficult to construct. Several recent projects on “grounded” language learning (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010; Liang et al., 2009) exploit more easily and naturally available training data consisting of sentences paired with world states In particular, Chen and Mooney (2008) introduced the problem of learning to sportscast by simply observing natural language commentary on simulated Robocup robot soccer games. The training data consists of natural language (NL) sentences ambiguously paired with logical meaning representations (MRs) describing recent events in the game extracted from the simulator. Most sentences describe one of the extracted recent events; however, the specific event to which it refers is unknown. Theref"
C10-2062,D08-1082,0,0.125451,"entences into logical form. Compared to a previous generative model for semantic alignment, it also supports full semantic parsing. Experimental results on the Robocup sportscasting corpora in both English and Korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators. 1 Introduction Most approaches to learning semantic parsers that map sentences into complete logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007b; Lu et al., 2008) require fullysupervised corpora that provide full formal logical representations for each sentence. Such corpora are expensive and difficult to construct. Several recent projects on “grounded” language learning (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010; Liang et al., 2009) exploit more easily and naturally available training data consisting of sentences paired with world states In particular, Chen and Mooney (2008) introduced the problem of learning to sportscast by simply observing natural language commentary on simulated Robocup robot soccer games. The training data c"
C10-2062,D09-1042,0,0.108562,"rectly mapped to their meaning representations. Results are presented in Table 3.5 6 For our model, we report results using the parser learned directly from the ambiguous supervision, as well 5 The best result of Chen and Mooney (2008) is for WASPER - GEN, and that of Chen et al. (2010) is for WASPER with Liang et al.’s matching initialization for English and for WASER - GEN - IGSL - METEOR with Liang et al.’s initialization for Korean. 6 Our semantic parsing results are based on our best matching results with IGSL initialization. 548 as results for training a supervised parser (both WASP and Lu et al. (2009)’s) on the NL–MR matching produced by our model. We also present results for training Lu et al.’s parser and WASP on Liang et al.’s NL–MR matchings. Our initial learned semantic parser does not perform better than the best results reported by Chen et al. (2010), but it is clearly better than the initial results of Chen and Mooney (2008). Training WASP and Lu et al.’s supervised parser on our method’s highly accurate set of disambiguated NL–MR pairs improved the results. Retraining Lu et al.’s parser gave the best overall results for English, and retraining WASP gave the second highest results"
C10-2062,P02-1040,0,0.0867795,"for semantic parsing and tactical generation. Matching performance is measured in training data, since the goal is to disambiguate this data. All results are averaged across these 4 folds. We also use the same performance metrics as Chen and Mooney (2008). The accuracy of matching and semantic parsing are measured using F-measure, the harmonic mean of precision and recall, where precision is the fraction of the system’s annotations that are correct, and recall is the fraction of the annotations from the goldstandard that the system correctly produces. Generation is evaluated using BLEU score (Papineni et al., 2002) between generated sentences and reference NL sentences in the test set. We compare our results to previous results from Chen and Mooney (2008) and Chen et al. (2010) and to matching results on Robocup data from Liang et al. (2009). 7.1 NL–MR Matching The goal of matching is to find the most probable NL–MR alignment for ambiguous examples consisting of an NL sentence and multiple potential MR logical forms. In Robocup sportscasting, the MRs for a given sentence correspond to all game events that occur within a 5-second window prior to the NL comment. Not all NL sentences have a matching MR in"
C10-2062,N07-1022,1,0.917657,"antic parser that maps sentences into logical form. Compared to a previous generative model for semantic alignment, it also supports full semantic parsing. Experimental results on the Robocup sportscasting corpora in both English and Korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators. 1 Introduction Most approaches to learning semantic parsers that map sentences into complete logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007b; Lu et al., 2008) require fullysupervised corpora that provide full formal logical representations for each sentence. Such corpora are expensive and difficult to construct. Several recent projects on “grounded” language learning (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010; Liang et al., 2009) exploit more easily and naturally available training data consisting of sentences paired with world states In particular, Chen and Mooney (2008) introduced the problem of learning to sportscast by simply observing natural language commentary on simulated Robocup robot soccer games."
C10-2062,P07-1121,1,0.896959,"antic parser that maps sentences into logical form. Compared to a previous generative model for semantic alignment, it also supports full semantic parsing. Experimental results on the Robocup sportscasting corpora in both English and Korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators. 1 Introduction Most approaches to learning semantic parsers that map sentences into complete logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007b; Lu et al., 2008) require fullysupervised corpora that provide full formal logical representations for each sentence. Such corpora are expensive and difficult to construct. Several recent projects on “grounded” language learning (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010; Liang et al., 2009) exploit more easily and naturally available training data consisting of sentences paired with world states In particular, Chen and Mooney (2008) introduced the problem of learning to sportscast by simply observing natural language commentary on simulated Robocup robot soccer games."
C10-2062,D07-1071,0,0.0516062,"semantic parsing model proposed by Lu et al. (2008). Our approach is capable of disambiguating the mapping between language and meanings while also learning a complete semantic parser for mapping sentences to logical form. Experimental results on Robocup sportscasting show that our approach outperforms all previous results on the NL–MR matching (alignment) task and also produces competitive performance on semantic parsing and improved language generation. 2 Related Work The conventional approach to learning semantic parsers (Zelle and Mooney, 1996; Ge and Mooney, 2005; Kate and Mooney, 2006; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007b; Lu et al., 2008) requires detailed supervision unambiguously pairing each sentence with its logical form. However, developing training corpora for these methods requires expensive expert human labor. Chen and Mooney (2008) presented methods for grounded language learning from ambiguous supervision that address three related tasks: NL–MR alignment, semantic parsing, and natural language generation. They solved the problem of aligning sentences and meanings by iteratively retraining an existing supervised semantic parser, WASP (Wong and Mo"
C14-1115,P11-1020,0,0.0258326,"deos (1297 training, 670 testing), recognizing sentential subjects out of 45 candidate entities, objects out of 218 candidate objects, verbs out of 218 candidate activities, and places out of 12 candidate scenes. 3 Approach Our overall approach uses a probabilistic graphical model to integrate the visual detection of entities, activities, and scenes with language statistics to determine the best subject, verb, object, and place to describe a given video. A descriptive English sentence is generated from the selected sentential components. 3.1 Video Dataset We use the video dataset collected by Chen and Dolan (2011). The dataset contains 1,967 short YouTube video clips paired with multiple human-generated natural-language descriptions. The video clips are 10 to 25 seconds in duration and typically consist of a single activity. Portions of this dataset have been used in previous work on video description (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013). We use 1,297 randomly selected videos for training and evaluate predictions on the remaining 670 test videos. 1219 3.2 Visual Recognition of Subject, Verb, and Object We utilize the visual recognition techniques employed by"
C14-1115,P96-1041,0,0.0395199,"Missing"
C14-1115,W13-1302,1,0.90221,"ge descriptions and Google queries, and the work of Yang et al. (2011) which uses corpus statistics to aid the description of objects and scenes. We go beyond the scope of these previous works by also selecting verbs through the integration of activity recognition from video and statistics from parsed corpora. With regard to video description, the work of Barbu et al. (2012) uses a small, hand-coded grammar to describe a sparse set of prescribed activities. In contrast, we utilize corpus statistics to aid the description of a wide range of naturally-occurring videos. The most similar work is (Krishnamoorthy et al., 2013; Guadarrama et al., 2013) which uses an n-gram language model to help determine the best subject-verbobject for describing a video. Krishnamoorthy et al. (2013) used a limited set of videos containing a small set of 20 entities, and the work of Guadarrama et al. (2013) showed an advantage of using linguistic knowledge only for the case of “zero shot activity recognition,” in which the appropriate verb for describing the activity was never seen during training. Compared to this prior work, we explore a much larger set of entities and activities (see Section 3.2) and add scene recognition (see"
C14-1115,W11-0326,0,0.0164649,"the detection of some individual components when compared to using the vision system alone, as well as over a previous n-gram language-modeling approach. The joint detection allows us to automatically generate more accurate, richer sentential descriptions of videos with a wide array of possible content. 1 Introduction Integrating language and vision is a topic that is attracting increasing attention in computational linguistics (Berg and Hockenmaier, 2013). Although there is a fair bit of research on generating naturallanguage descriptions of images (Feng and Lapata, 2013; Yang et al., 2011; Li et al., 2011; Ordonez et al., 2011), there is significantly less work on describing videos (Barbu et al., 2012; Guadarrama et al., 2013; Das et al., 2013; Rohrbach et al., 2013; Senina et al., 2014). In particular, much of the research on videos utilizes artificially constructed videos with prescribed sets of objects and actions (Barbu et al., 2012; Yu and Siskind, 2013). Generating natural-language descriptions of videos in the wild, such as those posted on YouTube, is a very challenging task. In this paper, we focus on selecting content for generating sentences to describe videos. Due to the large numbe"
C14-1115,P11-1027,0,0.00914095,"scribed in Section 3.4. Given an SVOP tuple, our objective is to generate a rich sentence using the subject, verb, object, and place information. However, it is not prudent to add the object and place to the description of all videos since some verbs may be intransitive and the place information may be redundant. In order to achieve the best set of components to include, we use the above template to first generate a set of candidate sentences based on the SVO triple, SVP triple and the SVOP quadruple. Then, each sentence type (SVO, SVP, and SVOP) is ranked using the BerkeleyLM language model (Pauls and Klein, 2011) trained on the GoogleNgram corpus. Finally, we output the sentence with the highest average 5-gram probability in order to normalize for sentence length. 4 Experimental Results We compared using the vision system alone to our model, which augments that system with linguistic knowledge. Specifically, we consider the Highest Vision Confidence (HVC) model, which takes for each sentence component the word with the highest confidence from the state-of-the-art vision detectors described in Sections 3.2 and 3.3. We compare the results of this model on the 670 test videos to those of our Factor Graph"
C14-1115,D11-1041,0,0.219108,"Missing"
C14-1115,P13-1006,0,0.0221297,"a topic that is attracting increasing attention in computational linguistics (Berg and Hockenmaier, 2013). Although there is a fair bit of research on generating naturallanguage descriptions of images (Feng and Lapata, 2013; Yang et al., 2011; Li et al., 2011; Ordonez et al., 2011), there is significantly less work on describing videos (Barbu et al., 2012; Guadarrama et al., 2013; Das et al., 2013; Rohrbach et al., 2013; Senina et al., 2014). In particular, much of the research on videos utilizes artificially constructed videos with prescribed sets of objects and actions (Barbu et al., 2012; Yu and Siskind, 2013). Generating natural-language descriptions of videos in the wild, such as those posted on YouTube, is a very challenging task. In this paper, we focus on selecting content for generating sentences to describe videos. Due to the large numbers of video actions and objects and scarcity of training data, we introduce a graphical model for integrating statistical linguistic knowledge mined from large text corpora with noisy computer vision detections. This integration allows us to infer which vision detections to trust given prior linguistic knowledge. Using a large, realistic collection of YouTube"
C14-1115,N13-2000,0,\N,Missing
D10-1114,N09-1003,0,0.0253098,"Missing"
D10-1114,J02-2003,0,0.0103822,"features, and each prototype captures one thematic usage of the word. For example, wizard is broken up into a background cluster describing features common to all usages of the word (e.g., magic and evil) and several genrespecific usages (e.g. Merlin, fairy tales and Harry Potter). are then computed separately for each cluster, producing a distributed representation for each word. Distributional methods have also proven to be a powerful approach to modeling selectional preference (Pad´o et al., 2007; Pantel et al., 2007), rivaling methods based on existing semantic resources such as WordNet (Clark and Weir, 2002; Resnik, 1997) and FrameNet (Pad´o, 2007) and performing nearly as well as supervised methods (Herdaˇgdelen and Baroni, 2009). Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to vector-space lexical semantics (Pantel, 2003; Pantel et al., 2007; Reisinger and Mooney, 2010). In this section we briefl"
D10-1114,D08-1094,0,0.299025,"ectional preference of verbs relies on identifying commonalities shared by their typical arguments. Tiered clustering can also be viewed as a form of soft feature selection, where features that do not contribute meaningfully to the clustering can be excluded. We demonstrate the applicability of tiered clustering, highlighting particular cases where modeling shared structure is beneficial and where it can be detrimental. 1 Introduction Word meaning can be represented as highdimensional vectors inhabiting a common space whose dimensions capture semantic or syntactic properties of interest (e.g. Erk and Pado, 2008; Lowe, 2001). Such vector-space representations of meaning induce measures of word similarity that can be tuned to correlate well with judgements made by humans. Previous work has focused on designing feature representations and semantic spaces that capture salient properties of word meaning (e.g. Curran, 2004; Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997), often leveraging the distributional hypothesis, i.e. that similar words appear in similar contexts (Miller and Charles, 1991; Pereira et al., 1993). Since vector-space representations are constructed at the lexical level, th"
D10-1114,J02-3001,0,0.0144024,"ted representation for each word. Distributional methods have also proven to be a powerful approach to modeling selectional preference (Pad´o et al., 2007; Pantel et al., 2007), rivaling methods based on existing semantic resources such as WordNet (Clark and Weir, 2002; Resnik, 1997) and FrameNet (Pad´o, 2007) and performing nearly as well as supervised methods (Herdaˇgdelen and Baroni, 2009). Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to vector-space lexical semantics (Pantel, 2003; Pantel et al., 2007; Reisinger and Mooney, 2010). In this section we briefly introduce a version of the multiprototype model based on the Dirichlet Process Mixture Model (DPMM), capable of inferring automatically the number of prototypes necessary for each word (Rasmussen, 2000). Similarity between two DPMM word-representations is then computed as a function of their cluster centroids (§5), instead of the centroid of all the wor"
D10-1114,P06-1046,0,0.0092772,"only implemented using vector-spaces derived from (1) word collocations (Sch¨utze, 1998), directly leveraging the distributional hypothesis (Miller and Charles, 1991), (2) syntactic relations (Pad´o and Lapata, 2007), (3) structured corpora (e.g. Gabrilovich and Markovitch (2007)) or (4) latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997). Such models can be evaluated based on their correlation with human-reported lexical similarity judgements using e.g. the WordSim-353 collection (Finkelstein et al., 2001). Distributional methods exhibit a high degree of scalability (Gorman and Curran, 2006) and have been applied broadly in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Reisinger and Mooney (2010) introduced a multiprototype approach to vector-space lexical semantics where individual words are represented as collections of “prototype” vectors. This representation is capable of accounting for homonymy and polysemy, as well as other forms of variation in word usage, like similar context-dependent methods (Erk and Pado, 2008). The set of vectors for a word is determined by unsu"
D10-1114,W09-0205,0,0.029934,"Missing"
D10-1114,N07-1071,0,0.0353677,"accounts for features commonly shared by all occurrences (i.e. context-independent feature variation), while the clustering model accounts for variation in word usage (i.e. context-dependent variation, or word senses; Table 1). Using the tiered clustering model, we derive a multi-prototype representation capable of capturing varying degrees of sharing between word senses, and demonstrate its effectiveness in lexical semantic tasks where such sharing is desirable. In particular we show that tiered clustering outperforms the multi-prototype approach for (1) selectional preference (Resnik, 1997; Pantel et al., 2007), i.e. predict1173 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1173–1182, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics LIFE ing the typical filler of an argument slot of a verb, and (2) word-relatedness in the presence of highly polysemous words. The former case exhibits a high degree of explicit structure, especially for more selectionally restrictive verbs (e.g. the set of things that can be eaten or can shoot). The remainder of the paper is organized as follows: Section 2 gives relevant backgro"
D10-1114,P91-1030,0,0.0492124,". Merlin, fairy tales and Harry Potter). are then computed separately for each cluster, producing a distributed representation for each word. Distributional methods have also proven to be a powerful approach to modeling selectional preference (Pad´o et al., 2007; Pantel et al., 2007), rivaling methods based on existing semantic resources such as WordNet (Clark and Weir, 2002; Resnik, 1997) and FrameNet (Pad´o, 2007) and performing nearly as well as supervised methods (Herdaˇgdelen and Baroni, 2009). Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to vector-space lexical semantics (Pantel, 2003; Pantel et al., 2007; Reisinger and Mooney, 2010). In this section we briefly introduce a version of the multiprototype model based on the Dirichlet Process Mixture Model (DPMM), capable of inferring automatically the number of prototypes necessary for each word (Rasmussen, 2000). Similarity between two DPMM word-represe"
D10-1114,P93-1024,0,0.237984,"ace whose dimensions capture semantic or syntactic properties of interest (e.g. Erk and Pado, 2008; Lowe, 2001). Such vector-space representations of meaning induce measures of word similarity that can be tuned to correlate well with judgements made by humans. Previous work has focused on designing feature representations and semantic spaces that capture salient properties of word meaning (e.g. Curran, 2004; Gabrilovich and Markovitch, 2007; Landauer and Dumais, 1997), often leveraging the distributional hypothesis, i.e. that similar words appear in similar contexts (Miller and Charles, 1991; Pereira et al., 1993). Since vector-space representations are constructed at the lexical level, they conflate multiple word meanings into the same vector, e.g. collapsing occurrences of bankinstitution and bankriver . Methods such as Clustering by Committee (Pantel, 2003) and multi-prototype representations (Reisinger and Mooney, 2010) address this issue by performing word-sense disambiguation across word occurrences, and then building meaning vectors from the disambiguated words. Such approaches can readily capture the structure of homonymous words with several unrelated meanings (e.g. bat and club), but are not"
D10-1114,J03-4004,0,0.0137993,"mputed separately for each cluster, producing a distributed representation for each word. Distributional methods have also proven to be a powerful approach to modeling selectional preference (Pad´o et al., 2007; Pantel et al., 2007), rivaling methods based on existing semantic resources such as WordNet (Clark and Weir, 2002; Resnik, 1997) and FrameNet (Pad´o, 2007) and performing nearly as well as supervised methods (Herdaˇgdelen and Baroni, 2009). Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). 3 Multi-Prototype Models Representing words as mixtures over several prototypes has proven to be a powerful approach to vector-space lexical semantics (Pantel, 2003; Pantel et al., 2007; Reisinger and Mooney, 2010). In this section we briefly introduce a version of the multiprototype model based on the Dirichlet Process Mixture Model (DPMM), capable of inferring automatically the number of prototypes necessary for each word (Rasmussen, 2000). Similarity between two DPMM word-representations is then computed as a function of their cluster"
D10-1114,J07-2002,0,0.0702765,"Missing"
D10-1114,D07-1042,0,0.230069,"Missing"
D10-1114,N10-1013,1,0.92022,"corpora (e.g. Gabrilovich and Markovitch (2007)) or (4) latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997). Such models can be evaluated based on their correlation with human-reported lexical similarity judgements using e.g. the WordSim-353 collection (Finkelstein et al., 2001). Distributional methods exhibit a high degree of scalability (Gorman and Curran, 2006) and have been applied broadly in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Reisinger and Mooney (2010) introduced a multiprototype approach to vector-space lexical semantics where individual words are represented as collections of “prototype” vectors. This representation is capable of accounting for homonymy and polysemy, as well as other forms of variation in word usage, like similar context-dependent methods (Erk and Pado, 2008). The set of vectors for a word is determined by unsupervised word sense discovery (Sch¨utze, 1998), which clusters the contexts in which a word appears. Average prototype vectors 1174 all, about, life, would, death my, you, real, your, about spent, years, rest, lived"
D10-1114,W97-0209,0,0.570595,"kground model accounts for features commonly shared by all occurrences (i.e. context-independent feature variation), while the clustering model accounts for variation in word usage (i.e. context-dependent variation, or word senses; Table 1). Using the tiered clustering model, we derive a multi-prototype representation capable of capturing varying degrees of sharing between word senses, and demonstrate its effectiveness in lexical semantic tasks where such sharing is desirable. In particular we show that tiered clustering outperforms the multi-prototype approach for (1) selectional preference (Resnik, 1997; Pantel et al., 2007), i.e. predict1173 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1173–1182, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics LIFE ing the typical filler of an argument slot of a verb, and (2) word-relatedness in the presence of highly polysemous words. The former case exhibits a high degree of explicit structure, especially for more selectionally restrictive verbs (e.g. the set of things that can be eaten or can shoot). The remainder of the paper is organized as follows: Section 2"
D10-1114,J98-1004,0,0.65084,"Missing"
D10-1114,P06-1101,0,0.0802233,"Missing"
D10-1114,J06-3003,0,0.0188396,"background on the methods compared, Section 3 outlines the multiprototype model based on the Dirichlet Process mixture model, Section 4 derives the tiered clustering model, Section 5 discusses similarity metrics, Section 6 details the experimental setup and includes a micro-analysis of feature selection, Section 7 presents results applying tiered clustering to word relatedness and selectional preference, Section 8 discusses future work, and Section 9 concludes. 2 Background Models of the attributional similarity of concepts, i.e. the degree to which concepts overlap based on their attributes (Turney, 2006), are commonly implemented using vector-spaces derived from (1) word collocations (Sch¨utze, 1998), directly leveraging the distributional hypothesis (Miller and Charles, 1991), (2) syntactic relations (Pad´o and Lapata, 2007), (3) structured corpora (e.g. Gabrilovich and Markovitch (2007)) or (4) latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997). Such models can be evaluated based on their correlation with human-reported lexical similarity judgements using e.g. the WordSim-353 collection (Finkelstein et al., 2001). Distributional methods exhibit a high degree of sca"
D10-1114,P06-2118,0,0.0261837,"ize T 10 centered around the occurrence, represented using tf-idf weighting. Feature vectors are pruned to a fixed length f , discarding all but the highest-weight features (f is selected via empirical validation, as described in the next section). Finally, semantic similarity between word pairs is computed using cosine distance (`2 -normalized dot-product).2 6.4 Feature pruning is one of the most significant factors in obtaining high correlation with human similarity judgements using vector-space models, and has been suggested as one way to improve sense disambiguation for polysemous verbs (Xue et al., 2006). In this section, we calibrate the single prototype and multiprototype methods on WS-353, reaching the limit of human and oracle performance and demonstrating robust performance gains even with semantically impoverished features. In particular we obtain ρ0.75 correlation on WS-353 using only unigram collocations and ρ0.77 using a fixed-K multiprototype representation (Figure 3; Reisinger and Mooney, 2010). This result rivals average human performance, obtaining correlation near that of the supervised oracle approach of Agirre et al. (2009). The optimal pruning cutoff depends on the feature"
D10-1114,J93-1005,0,\N,Missing
D11-1130,P07-1028,0,0.152201,"cCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and club to association is less than the distance from bat to association (Griffiths et al., 2007; Tversky and Gati, 1982).1 Erk (2007) circumvents this problem by representing words as multiple exemplars derived directly from word occurrences and embedded in a common vector space to capture context-dependent usage. Likewise Reisinger and Mooney (2010) take a similar approach using mixture modeling combined with a background variation model to generate multiple prototype vectors for polysemous words. Both of these approaches still ultimately embed all words in a single metric space and hence argue for globally consistent metrics that capture human 1 Similarity also has been shown to violate symmetry (e.g. people have the intu"
D11-1130,P06-1046,0,0.0283395,"model word meaning by embedding words in a common metric space, whose dimensions are derived from, e.g., word collocations (Sch¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and club to association is less than the distance from bat to association (Griffiths et al., 2007; Tversky and Gati, 1982).1 Erk (2007) circumvents this problem by representing words as multiple exemplars derived direct"
D11-1130,S07-1009,0,0.0205325,"Missing"
D11-1130,J07-2002,0,0.133568,"Missing"
D11-1130,D10-1114,1,0.899059,"Missing"
D11-1130,W97-0209,0,0.146552,"re n-gram contexts. 2 Mixture Modeling and Lexical Semantics Distributional, or vector space methods attempt to model word meaning by embedding words in a common metric space, whose dimensions are derived from, e.g., word collocations (Sch¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is not a globally consistent metric. It violates, e.g., the triangle inequality: the sum of distances from bat to club and club to association is less than the distance from bat to association (Griffiths et al., 2007"
D11-1130,J98-1004,0,0.136649,"Missing"
D11-1130,P06-1101,0,0.282755,"r capturing relations between words that are intuitive to humans, outperforming related models such as Latent Dirichlet Allocation. 1 Introduction Humans categorize objects using multiple orthogonal taxonomic systems, where category generalization depends critically on what features are relevant to one particular system. For example, foods can be organized in terms of their nutritional value (high in fiber) or situationally (commonly eaten for Thanks1405 Raymond Mooney Department of Computer Sciences The University of Texas at Austin Austin, TX 78712 mooney@cs.utexas.edu giving; Shafto et al. (2006)). Human knowledgebases such as Wikipedia also exhibit such multiple clustering structure (e.g. people are organized by occupation or by nationality). The effects of these overlapping categorization systems manifest themselves at the lexical semantic level (Murphy, 2002), implying that lexicographical word senses and traditional computational models of word-sense based on clustering or exemplar activation are too impoverished to capture the rich dynamics of word usage. In this work, we introduce a novel probabilistic clustering method, Multi-View Mixture (MVM), based on cross-cutting categoriz"
D11-1130,P10-1040,0,0.00917073,"human evaluation tasks, measuring its ability to find meaningful syntagmatic and paradigmatic structure. We find that MVM finds more semantically and syntactically coherent fine-grained structure, using both common and rare n-gram contexts. 2 Mixture Modeling and Lexical Semantics Distributional, or vector space methods attempt to model word meaning by embedding words in a common metric space, whose dimensions are derived from, e.g., word collocations (Sch¨utze, 1998), syntactic relations (Pad´o and Lapata, 2007), or latent semantic spaces (Finkelstein et al., 2001; Landauer and Dumais, 1997; Turian et al., 2010). The distributional hypothesis addresses the problem of modeling word similarity (Curran, 2004; Miller and Charles, 1991; Sch¨utze, 1998; Turney, 2006), and can be extended to selectional preference (Resnik, 1997) and lexical substitution (McCarthy and Navigli, 2007) as well. Such methods are highly scalable (Gorman and Curran, 2006) and have been applied in information retrieval (Manning et al., 2008), large-scale taxonomy induction (Snow et al., 2006), and knowledge acquisition (Van Durme and Pas¸ca, 2008). Vector space models fail to capture the richness of word meaning since similarity is"
D11-1130,J06-3003,0,0.322849,"overlapping categorization systems manifest themselves at the lexical semantic level (Murphy, 2002), implying that lexicographical word senses and traditional computational models of word-sense based on clustering or exemplar activation are too impoverished to capture the rich dynamics of word usage. In this work, we introduce a novel probabilistic clustering method, Multi-View Mixture (MVM), based on cross-cutting categorization (Shafto et al., 2006) that generalizes traditional vector-space or distributional models of lexical semantics (Curran, 2004; Pad´o and Lapata, 2007; Sch¨utze, 1998; Turney, 2006). Cross-cutting categorization finds multiple feature subsets (categorization systems) that produce high quality clusterings of the data. For example words might be clustered based on their part of speech, or based on their thematic usage. Contextdependent variation in word usage can be accounted for by leveraging multiple latent categorization systems. In particular, cross-cutting models can be used to capture both syntagmatic and paradigmatic notions of word relatedness, breaking up word features into multiple categorization systems and then computing similarity separately for each system. M"
D11-1130,J92-4003,0,\N,Missing
D12-1040,D11-1131,0,0.11845,"Missing"
D12-1040,P09-1010,0,0.286031,"parsing model that first chooses which MRs to describe and then generates a hybrid tree structure (Lu et al., 2008) containing both the MR and NL sentence. They train this model on ambiguous data using EM. As previously discussed, B¨orschinger et al. (2011) use a PCFG generative model and also train it on ambiguous data using EM. Liang et al. (2009) assume each sentence maps to one or more semantic records (i.e. MRs) and trains a hierarchical semi-Markov generative model using EM, and then finds a Viterbi alignment between NL words and records and their constituents. Several recent projects (Branavan et al., 2009; Vogel and Jurafsky, 2010) use NL instructions to guide reinforcement learning from independent exploration with delayed rewards. These systems do not even need the ambiguous supervision obtained from observing humans follow instructions; however, they do not learn semantic parsers that map sentences to complex, structural representations of their meaning. Interpreting and executing NL navigation instructions is our primary task, and several other recent projects have studied related problems. Shimizu and Haas (2009) present a system that parses natural language instructions into actions. How"
D12-1040,J00-4006,0,0.00466837,"with m in the lexicon, or individual words that appear in this phrase. The words not covered by W ordm also can be generated by W ord∅ which has rules for every word. P hm and P hXm ensure that P hrasem produces at least one W ordm , where P hXm indicates that one or more W ordm ’s have already been generated, and P hm indicates that no W ordm has yet been generated. 3.3 Parsing Novel NL Sentences To learn the parameters of the resulting PCFG, we use the Inside-Outside algorithm.5 Then, the standard probabilistic CKY algorithm is used to produce the most probable parse for novel NL sentences (Jurafsky and Martin, 2000). B¨orschinger et al. (2011) simply read the MR, m, for a sentence off the top Sm nonterminal of the most probable parse tree. However, in our approach, the correct MR is constructed by properly composing the appropriate subset of lexeme MRs from the most-probable parse tree. This allows the system to produce a wide variety of novel MRs for novel sentences, as long as the correct MR is a subgraph of the complete context (ci ) for at least one of the training sentences. First, the parse tree is pruned to remove all subtrees starting with P hrasex nodes. This leaves a tree consisting of the Root"
D12-1040,P06-1115,1,0.577792,"ther language-grounding tasks where NL sentences potentially refer to some subset of states, events, or actions in the world, as long as this overall context can be represented as a semantic graph or logical form. Since the semantic lexicon is an input to our system, other approaches to lexicon learning are also easily incorporated. 6 Related Work Most work on learning semantic parsers that map natural-language sentences to formal representations of their meaning have relied upon totally supervised training data consisting of NL/MR pairs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Lu et al., 2008; Zettlemoyer and Collins, 2009). Several recent approaches have investigated grounded learning from ambiguous supervision extracted from perceptual context. A number of approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010; Kim and Mooney, 2010; B¨orschinger et al., 2011) assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence. Many of these approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen"
D12-1040,C10-2062,1,0.534207,"n 2 reviews B¨orschinger et al.’s PCFG approach as well as the navigation task and data. Section 3 describes our enhanced PCFG approach and Section 4 presents an experimental evaluation of it. Then, Section 5 discusses the unique aspects of our approach and Section 6 describes additional related work. Finally, Section 7 presents future research directions and Section 8 gives our conclusions. 2 2.1 Background Existing PCFG Approach Our approach extends that of B¨orschinger et al. (2011), which in turn was inspired by a series of previous techniques (Lu et al., 2008; Liang et al., 2009; Kim and Mooney, 2010) following the idea of constructing correspondences between NL and MR in a single probabilistic generative framework. Particularly, their approach automatically constructs a PCFG that generates NL sentences from MRs, which indicates how atomic MR constituents are probabilistically related to NL words. The nonterminals in the grammar correspond to complete MRs, MR constituents, and NL phrases. The nonterminal for a composite MR generates each of its MR constituents, and each atomic MR, x, generates an NL phrase, P hrasex . Each P hrasex then generates a sequence of W ordx ’s for describing x, a"
D12-1040,P09-1011,0,0.585278,"ible MRs, which in turn is inevitably exponential in the number of objects and actions in the domain. Introduction The ultimate goal of “grounded” language learning is to develop computational systems that can acquire language more like a human child. Given only supervision in the form of sentences paired with relevant but ambiguous perceptual contexts, a system should learn to interpret and/or generate language describing situations and events in the world. For example, systems have learned to commentate simulated robot soccer games by learning from sample sportscasts (Chen and Mooney, 2008; Liang et al., 2009; B¨orschinger et al., 2011), or understand navigation instructions by learning from action traces The navigation task studied by Chen and Mooney (2011) provides much more ambiguous supervision. In this task, each instructional sentence is paired with a formal landmarks plan (represented as a large graph) that includes a full description of the observed actions and world-states that result when 433 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 433–444, Jeju Island, Korea, 12–14 July 2012. 2012 A"
D12-1040,D08-1082,0,0.182842,"of the paper is organized as follows. Section 2 reviews B¨orschinger et al.’s PCFG approach as well as the navigation task and data. Section 3 describes our enhanced PCFG approach and Section 4 presents an experimental evaluation of it. Then, Section 5 discusses the unique aspects of our approach and Section 6 describes additional related work. Finally, Section 7 presents future research directions and Section 8 gives our conclusions. 2 2.1 Background Existing PCFG Approach Our approach extends that of B¨orschinger et al. (2011), which in turn was inspired by a series of previous techniques (Lu et al., 2008; Liang et al., 2009; Kim and Mooney, 2010) following the idea of constructing correspondences between NL and MR in a single probabilistic generative framework. Particularly, their approach automatically constructs a PCFG that generates NL sentences from MRs, which indicates how atomic MR constituents are probabilistically related to NL words. The nonterminals in the grammar correspond to complete MRs, MR constituents, and NL phrases. The nonterminal for a composite MR generates each of its MR constituents, and each atomic MR, x, generates an NL phrase, P hrasex . Each P hrasex then generates"
D12-1040,P10-1083,0,0.217994,"st chooses which MRs to describe and then generates a hybrid tree structure (Lu et al., 2008) containing both the MR and NL sentence. They train this model on ambiguous data using EM. As previously discussed, B¨orschinger et al. (2011) use a PCFG generative model and also train it on ambiguous data using EM. Liang et al. (2009) assume each sentence maps to one or more semantic records (i.e. MRs) and trains a hierarchical semi-Markov generative model using EM, and then finds a Viterbi alignment between NL words and records and their constituents. Several recent projects (Branavan et al., 2009; Vogel and Jurafsky, 2010) use NL instructions to guide reinforcement learning from independent exploration with delayed rewards. These systems do not even need the ambiguous supervision obtained from observing humans follow instructions; however, they do not learn semantic parsers that map sentences to complex, structural representations of their meaning. Interpreting and executing NL navigation instructions is our primary task, and several other recent projects have studied related problems. Shimizu and Haas (2009) present a system that parses natural language instructions into actions. However, they limit the number"
D12-1040,P07-1121,1,0.0606016,"tasks where NL sentences potentially refer to some subset of states, events, or actions in the world, as long as this overall context can be represented as a semantic graph or logical form. Since the semantic lexicon is an input to our system, other approaches to lexicon learning are also easily incorporated. 6 Related Work Most work on learning semantic parsers that map natural-language sentences to formal representations of their meaning have relied upon totally supervised training data consisting of NL/MR pairs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Lu et al., 2008; Zettlemoyer and Collins, 2009). Several recent approaches have investigated grounded learning from ambiguous supervision extracted from perceptual context. A number of approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010; Kim and Mooney, 2010; B¨orschinger et al., 2011) assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence. Many of these approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010) disambigu"
D12-1040,D07-1071,0,0.0349739,"es potentially refer to some subset of states, events, or actions in the world, as long as this overall context can be represented as a semantic graph or logical form. Since the semantic lexicon is an input to our system, other approaches to lexicon learning are also easily incorporated. 6 Related Work Most work on learning semantic parsers that map natural-language sentences to formal representations of their meaning have relied upon totally supervised training data consisting of NL/MR pairs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Lu et al., 2008; Zettlemoyer and Collins, 2009). Several recent approaches have investigated grounded learning from ambiguous supervision extracted from perceptual context. A number of approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010; Kim and Mooney, 2010; B¨orschinger et al., 2011) assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence. Many of these approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010) disambiguate the data and match NL sente"
D12-1040,P09-1110,0,0.0392081,"vents, or actions in the world, as long as this overall context can be represented as a semantic graph or logical form. Since the semantic lexicon is an input to our system, other approaches to lexicon learning are also easily incorporated. 6 Related Work Most work on learning semantic parsers that map natural-language sentences to formal representations of their meaning have relied upon totally supervised training data consisting of NL/MR pairs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Lu et al., 2008; Zettlemoyer and Collins, 2009). Several recent approaches have investigated grounded learning from ambiguous supervision extracted from perceptual context. A number of approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010; Kim and Mooney, 2010; B¨orschinger et al., 2011) assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence. Many of these approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al., 2010) disambiguate the data and match NL sentences to their correct MR by iteratively retrainin"
D13-1190,baccianella-etal-2010-sentiwordnet,0,0.0738996,"s per sentence and Number of references per section Table 2: Structural Features of a Wikipedia Article Wikipedia Network Features Number of Internal Wikilinks (to other Wikipedia pages) Number of External Links (to other websites) Number of Backlinks (i.e. Number of wikilinks from other Wikipedia articles to an article) Number of Language Links (i.e. Number of links to the same article in other languages) Table 3: Network Features of a Wikipedia Article added a new feature, “Overall Sentiment Score” for an article. This feature is the average of the sentiment scores assigned by SentiWordnet (Baccianella et al., 2010) to all positive and negative sentiment bearing words in an article. In total, this results in 58 basic document features. 4.2 N-Gram Language Models Language models are commonly used to measure stylistic differences in language usage between authors. For this work, we employed them to model the difference in style of neutral vs. promotional Wikipedia articles. We trained trigram word language models and trigram character language models5 with Witten-Bell smoothing to produce probabilistic models of both classes. 4.3 PCFG Language Models Probabilistic Context Free Grammars (PCFG) capture the s"
D13-1190,P11-1030,0,0.0675475,"Missing"
D13-1190,C04-1088,0,0.103311,"Missing"
D13-1190,P11-2015,0,0.40916,"trained using a set of lexical, structural, network and edit-history related features of Wikipedia articles. However, they used no features capturing syntactic structure, at a level deeper than Part-OfSpeech (POS) tags. A related area is that of vandalism detection in Wikipedia. Several systems have been developed to detect vandalizing edits in Wikipedia. These fall into two major categories: those analyzing author information and edit metadata (Wilkinson and Huberman, 2007; Stein and Hess, 2007); and those using NLP techniques such as n-gram language models and PCFGs (Wang and McKeown, 2010; Harpalani et al., 2011). We combine relevant features from both these categories to train a classifier that distinguishes promotional content from normal Wikipedia articles. 3 Dataset Collection We extracted a set of about 13,000 articles from English Wikipedia’s category, “Category:All arti2 “Advert” is the flaw-type of majority of the articles in the Category ‘Articles with a promotional tone’. 1851 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1851–1857, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Content Features N"
D13-1190,W08-0909,0,0.0607124,"Missing"
D13-1190,P03-1054,0,0.0524765,"Missing"
D13-1190,P10-1056,0,0.0288124,"Missing"
D13-1190,P10-2008,1,0.857294,"Missing"
D13-1190,W00-1308,0,0.338054,"Missing"
D13-1190,N03-1033,0,0.0843873,"Missing"
D13-1190,C10-1129,0,0.0357209,"Missing"
D16-1201,W03-0425,0,0.0735916,"2015 competition, several ensembling baselines, as well as a state-of-the-art stacking approach. The success of our technique on two different and challenging problems demonstrates the power and generality of our combined approach to ensembling. 1 Introduction Ensembling multiple systems is a well known standard approach to improving accuracy in several machine learning applications (Dietterich, 2000). Ensembles have been applied to parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), sentiment analysis (Whitehead and Yaeger, 2010) and information extraction (IE) (Florian et al., 2003; McClosky et al., 2012). Recently, using stacking (Wolpert, 1992) to ensemble systems was shown to give state-of-the-art results on slot-filling and entity linking for Knowledge Base Population (KBP) (Viswanathan et al., 2015; Rajani and Mooney, 2016). Stacking uses supervised learning to train a meta-classifier to combine multiple system outputs; therefore, it requires historical data on the performance of each system. Rajani and Mooney (2016) use data from the 2014 iteration of the KBP competition for training and then test on the data from the 2015 competition, therefore they can only ense"
D16-1201,W99-0623,0,0.16007,"d Start Slot Filling (CSSF) and Tri-lingual Entity Discovery and Linking (TEDL). We demonstrate that it outperforms the best system for both tasks in the 2015 competition, several ensembling baselines, as well as a state-of-the-art stacking approach. The success of our technique on two different and challenging problems demonstrates the power and generality of our combined approach to ensembling. 1 Introduction Ensembling multiple systems is a well known standard approach to improving accuracy in several machine learning applications (Dietterich, 2000). Ensembles have been applied to parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), sentiment analysis (Whitehead and Yaeger, 2010) and information extraction (IE) (Florian et al., 2003; McClosky et al., 2012). Recently, using stacking (Wolpert, 1992) to ensemble systems was shown to give state-of-the-art results on slot-filling and entity linking for Knowledge Base Population (KBP) (Viswanathan et al., 2015; Rajani and Mooney, 2016). Stacking uses supervised learning to train a meta-classifier to combine multiple system outputs; therefore, it requires historical data on the performance of each system. Rajani and Mooney (2016) use"
D16-1201,A00-2009,0,0.110796,"overy and Linking (TEDL). We demonstrate that it outperforms the best system for both tasks in the 2015 competition, several ensembling baselines, as well as a state-of-the-art stacking approach. The success of our technique on two different and challenging problems demonstrates the power and generality of our combined approach to ensembling. 1 Introduction Ensembling multiple systems is a well known standard approach to improving accuracy in several machine learning applications (Dietterich, 2000). Ensembles have been applied to parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), sentiment analysis (Whitehead and Yaeger, 2010) and information extraction (IE) (Florian et al., 2003; McClosky et al., 2012). Recently, using stacking (Wolpert, 1992) to ensemble systems was shown to give state-of-the-art results on slot-filling and entity linking for Knowledge Base Population (KBP) (Viswanathan et al., 2015; Rajani and Mooney, 2016). Stacking uses supervised learning to train a meta-classifier to combine multiple system outputs; therefore, it requires historical data on the performance of each system. Rajani and Mooney (2016) use data from the 2014 iteration of the KBP com"
D16-1201,P15-1018,1,0.806151,"approach to ensembling. 1 Introduction Ensembling multiple systems is a well known standard approach to improving accuracy in several machine learning applications (Dietterich, 2000). Ensembles have been applied to parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), sentiment analysis (Whitehead and Yaeger, 2010) and information extraction (IE) (Florian et al., 2003; McClosky et al., 2012). Recently, using stacking (Wolpert, 1992) to ensemble systems was shown to give state-of-the-art results on slot-filling and entity linking for Knowledge Base Population (KBP) (Viswanathan et al., 2015; Rajani and Mooney, 2016). Stacking uses supervised learning to train a meta-classifier to combine multiple system outputs; therefore, it requires historical data on the performance of each system. Rajani and Mooney (2016) use data from the 2014 iteration of the KBP competition for training and then test on the data from the 2015 competition, therefore they can only ensemble the shared systems that participated in both years. However, we would sometimes like to ensemble systems for which we have no historical performance data. For example, due to privacy, some companies may be unwilling to sh"
D16-1204,P11-1020,0,0.0983417,"Missing"
D16-1204,W14-3348,0,0.0366496,"Missing"
D16-1204,P02-1040,0,0.0983615,"Missing"
D16-1204,D14-1162,0,0.0803043,"Missing"
D16-1204,N15-1173,1,0.772055,"Missing"
D16-1204,P14-5010,0,\N,Missing
D16-1204,W14-4012,0,\N,Missing
D16-1204,C14-1115,1,\N,Missing
D16-1204,W13-1302,1,\N,Missing
D18-1165,D17-1063,0,0.108302,"ive learning in understanding natural-language object descriptions has shown that an agent following an opportunistic policy, that queries for labels not necessarily relevant to the current interaction, learns to perform better at identifying objects correctly over time (Thomason et al., 2017). However, this work only compares static policies that select actions based on manually-engineered heuristics. In this work, we focus on learning an optimal policy for this task using reinforcement learning, in the spirit of other recent attempts to learn policies for different types of active learning (Fang et al., 2017; Woodward and Finn, 2017). This allows an agent to choose whether or not to be opportunistic based on the specific interaction as well as the overall statistics of the dataset. Our learned policy outperforms a static baseline by improving its success rate on object retrieval while asking fewer questions on average. The learned policy also learns to distribute queries more uniformly across concepts than the baseline. 2 Related Work Active learning methods aim to identify examples that are likely to be the most useful in improving a supervised model. A number of metrics have been proposed to ev"
D18-1165,D17-1106,0,0.0615061,"Missing"
D18-1165,E17-1052,1,0.851739,"estricted vocabulary (Cakmak et al., 2010; Kulick et al., 2013), or additional knowledge of predicates (for example that “red” is a color) (Mohan et al., 2012). Others do not use active learning (Kollar et al., 2013; Parde et al., 2015; De Vries et al., 2017; Yu et al., 2017), or do not learn a policy that guides the interaction (Vogel et al., 2010; Thomason et al., 2016, 2017). Also related to our work is the use of reinforcement learning in dialog tasks, such as slotfilling and recommendation (Wen et al., 2015; Pietquin et al., 2011), understanding natural language instructions or commands (Padmakumar et al., 2017; Misra et al., 2017), and open domain conversation (Serban et al., 2016; Das et al., 2017). These typically do not use active learning. In our task, the policy needs to trade-off model improvement against task completion. 3 Given a set of objects OA and a natural language description l, MA would be the set of classifiers corresponding to perceptual predicates present in l. The decision made by the agent is a guess about which object is being described by l. The agent receives a score or reward based on this decision, and needs to maximize expected reward across a series of such interactions."
D18-1165,D08-1112,0,0.398295,"ades off task completion with model improvement that would benefit future tasks. 1 Introduction In machine learning tasks where obtaining labeled examples is expensive, active learning is used to lower the cost of annotation without sacrificing model performance. Active learning allows a learner to iteratively query for labels of unlabeled data points that are expected to maximally improve the existing model. It has been used in a number of natural language processing tasks such as text categorization (Lewis and Gale, 1994), semantic parsing (Thompson et al., 1999) and information extraction (Settles and Craven, 2008). The most commonly used framework for active learning is pool-based active learning, where the learner has access to the entire pool of unlabeled data at once, and can iteratively query for examples. In contrast, sequential active learning is a framework in which unlabeled examples are presented to the learner in a stream (Lewis and Gale, 1994). For every example, the learner can decide whether to query for its label or not. This results in an additional challenge – since the learner cannot compare all unlabeled data points before choosing queries, each query must be chosen based on local inf"
D18-1165,W17-2802,0,0.0288459,"f an interaction, which further increases the trade-off between model improvement and exploitation. Further, we consider a multilabel setting, which increases the number of actions at each decision step. There are other works that employ various types of turn-taking interaction to learn models for language grounding. Some of these use a restricted vocabulary (Cakmak et al., 2010; Kulick et al., 2013), or additional knowledge of predicates (for example that “red” is a color) (Mohan et al., 2012). Others do not use active learning (Kollar et al., 2013; Parde et al., 2015; De Vries et al., 2017; Yu et al., 2017), or do not learn a policy that guides the interaction (Vogel et al., 2010; Thomason et al., 2016, 2017). Also related to our work is the use of reinforcement learning in dialog tasks, such as slotfilling and recommendation (Wen et al., 2015; Pietquin et al., 2011), understanding natural language instructions or commands (Padmakumar et al., 2017; Misra et al., 2017), and open domain conversation (Serban et al., 2016; Das et al., 2017). These typically do not use active learning. In our task, the policy needs to trade-off model improvement against task completion. 3 Given a set of objects OA an"
D18-1165,D15-1199,0,0.0316803,"Missing"
E12-1061,J93-2004,0,\N,Missing
E12-1061,J98-1001,0,\N,Missing
E12-1061,J95-2002,0,\N,Missing
E12-1061,P00-1061,0,\N,Missing
E12-1061,N07-2010,0,\N,Missing
E12-1061,H05-1042,0,\N,Missing
E12-1061,J93-2003,0,\N,Missing
E12-1061,H05-1037,0,\N,Missing
E12-1061,W05-0614,0,\N,Missing
E12-1061,J90-2002,0,\N,Missing
E12-1061,N07-1022,1,\N,Missing
E12-1061,P02-1040,0,\N,Missing
E12-1061,P95-1034,0,\N,Missing
E12-1061,N06-1056,1,\N,Missing
E12-1061,C88-2128,0,\N,Missing
E12-1061,D07-1071,0,\N,Missing
E12-1061,P01-1067,0,\N,Missing
E12-1061,P09-1011,0,\N,Missing
E12-1061,P02-1034,0,\N,Missing
E12-1061,D08-1082,0,\N,Missing
E12-1061,W98-1107,1,\N,Missing
E12-1061,P06-1115,1,\N,Missing
E12-1061,W05-0602,1,\N,Missing
E12-1061,P05-1033,0,\N,Missing
E12-1061,J03-1002,0,\N,Missing
E12-1061,W09-0628,0,\N,Missing
E12-1061,P09-1010,0,\N,Missing
E14-1024,P11-1098,0,0.0419087,"Manshadi et al. (2008) and Balasubramanian et al. (2012; 2013) give approaches similar to the current work for modeling sequences of events as n-grams. These methods differ from the current work in that they do not model entities directly, instead modeling co-occurrence of particular nouns standing as arguments to particular verbs. Lewis and Steedman (2013) build clusters of relations similar to these events, finding such clusters helpful to question answering and textual inference. There has also been recent interest in the related problem of automatically learning event frames (Bejan, 2008; Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013). These approaches focus on identifying frames for information extraction tasks, as opposed to inferring events directly. Balasubramanian et al. (2013) give an event frame identification method, developed in parallel with the current work, using sequences of tuples similar to our multi-argument events, noting coherence issues with pair events. Their formulation differs from ours primarily in that they do not incorporate coreference information into their event co-occurrence distribution, and evaluate us6 Future Work We have evaluated only one type of multi"
E14-1024,W12-3019,0,0.491793,"duced a method for learning statistical scripts that, using a much simpler event representation that allows for efficient learning and inference. Jans et al. (2012) use the same simple event representation, but introduce a new model that more accurately predicts test data. These methods only model the actions of a single participant, called the protagonist. Chambers and Jurafsky (2009) extended their approach to the multi-participant case, modeling the events in which all of the entities in a document are involved; however, their method cannot represent interactions between multiple entities. Balasubramanian et al. (2012; 2013) describe the Rel-gram system, a Markov model similar to that of Jans et al. (2012), but with tuples instead of (verb, dependency) pairs. Our approach is simIntroduction Scripts encode knowledge of stereotypical events, including information about their typical ordered sequences of sub-events and corresponding arguments (Schank and Abelson, 1977). The classic example is the “restaurant script,” which encodes knowledge about what normally happens when dining out. Such knowledge can be used to improve text understanding by supporting inference of missing actions and events, as well as res"
E14-1024,D13-1185,0,0.290796,"(2012; 2013) give approaches similar to the current work for modeling sequences of events as n-grams. These methods differ from the current work in that they do not model entities directly, instead modeling co-occurrence of particular nouns standing as arguments to particular verbs. Lewis and Steedman (2013) build clusters of relations similar to these events, finding such clusters helpful to question answering and textual inference. There has also been recent interest in the related problem of automatically learning event frames (Bejan, 2008; Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013). These approaches focus on identifying frames for information extraction tasks, as opposed to inferring events directly. Balasubramanian et al. (2013) give an event frame identification method, developed in parallel with the current work, using sequences of tuples similar to our multi-argument events, noting coherence issues with pair events. Their formulation differs from ours primarily in that they do not incorporate coreference information into their event co-occurrence distribution, and evaluate us6 Future Work We have evaluated only one type of multiargument event inference, in which a s"
E14-1024,D13-1178,0,0.470404,"nt work in that they do not model entities directly, instead modeling co-occurrence of particular nouns standing as arguments to particular verbs. Lewis and Steedman (2013) build clusters of relations similar to these events, finding such clusters helpful to question answering and textual inference. There has also been recent interest in the related problem of automatically learning event frames (Bejan, 2008; Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013). These approaches focus on identifying frames for information extraction tasks, as opposed to inferring events directly. Balasubramanian et al. (2013) give an event frame identification method, developed in parallel with the current work, using sequences of tuples similar to our multi-argument events, noting coherence issues with pair events. Their formulation differs from ours primarily in that they do not incorporate coreference information into their event co-occurrence distribution, and evaluate us6 Future Work We have evaluated only one type of multiargument event inference, in which a script infers an event given a set of entities and the events involving those entities. We claim that this is the most natural adaptation of the cloze e"
E14-1024,N13-1104,0,0.0424919,"lasubramanian et al. (2012; 2013) give approaches similar to the current work for modeling sequences of events as n-grams. These methods differ from the current work in that they do not model entities directly, instead modeling co-occurrence of particular nouns standing as arguments to particular verbs. Lewis and Steedman (2013) build clusters of relations similar to these events, finding such clusters helpful to question answering and textual inference. There has also been recent interest in the related problem of automatically learning event frames (Bejan, 2008; Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013). These approaches focus on identifying frames for information extraction tasks, as opposed to inferring events directly. Balasubramanian et al. (2013) give an event frame identification method, developed in parallel with the current work, using sequences of tuples similar to our multi-argument events, noting coherence issues with pair events. Their formulation differs from ours primarily in that they do not incorporate coreference information into their event co-occurrence distribution, and evaluate us6 Future Work We have evaluated only one type of multiargument event infere"
E14-1024,P13-1035,0,0.0200144,"Missing"
E14-1024,de-marneffe-etal-2006-generating,0,0.036851,"Missing"
E14-1024,P06-1026,0,0.0139621,"Missing"
E14-1024,E12-1034,0,0.578331,"Missing"
E14-1024,N09-2013,0,0.0108572,"ts. These methods 226 ing human judgments of frame coherence rather than a narrative cloze test. help alleviate the bottleneck of the knowledgeengineering required for traditionally conceived script systems. These systems are precisionoriented: they create small, highly accurate scripts for very limited scenarios. The current work, in contrast, focuses on building high-recall models of general event sequences. There are also a number of systems addressing the related problem of modeling domain-specific human-human dialog for building dialog systems (Bangalore et al., 2006; Chotimongkol, 2008; Boyer et al., 2009). There have been a number of recent approaches to learning statistical scripts. Chambers and Jurafsky (2008) and Jans et al. (2012) give methods for learning models of (verb, dependency) pairs, as described above. Manshadi et al. (2008) give an n-gram model for sequences of verbs and their patients. McIntyre and Lapata (2009; 2010) use script objects learned from corpora of fairy tales to automatically generate stories. Chambers and Jurafsky (2009) extend their previous model to incorporate multiple entities, but do not directly model the different arguments of an event. Bamman et al. (2013)"
E14-1024,Q13-1015,0,0.0209876,"s, but do not directly model the different arguments of an event. Bamman et al. (2013) learn latent character personas from film summaries, associating character types with stereotypical actions; they focus on identifying persona types, rather than event inference. Manshadi et al. (2008) and Balasubramanian et al. (2012; 2013) give approaches similar to the current work for modeling sequences of events as n-grams. These methods differ from the current work in that they do not model entities directly, instead modeling co-occurrence of particular nouns standing as arguments to particular verbs. Lewis and Steedman (2013) build clusters of relations similar to these events, finding such clusters helpful to question answering and textual inference. There has also been recent interest in the related problem of automatically learning event frames (Bejan, 2008; Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013). These approaches focus on identifying frames for information extraction tasks, as opposed to inferring events directly. Balasubramanian et al. (2013) give an event frame identification method, developed in parallel with the current work, using sequences of tuples similar to our multi-argumen"
E14-1024,P08-1090,0,0.301438,"n a large corpus using the task of inferring held-out events (the “narrative cloze evaluation”) demonstrate that modeling multi-argument events improves predictive accuracy. 1 2 Background The idea of representing stereotypical event sequences for textual inference originates in the seminal work of Schank and Abelson (1977). Early scripts were manually engineered for specific domains; however, Mooney and DeJong (1985) present an early knowledge-based method for learning scripts from a single document. These early scripts (and methods for learning them) were non-statistical and fairly brittle. Chambers and Jurafsky (2008) introduced a method for learning statistical scripts that, using a much simpler event representation that allows for efficient learning and inference. Jans et al. (2012) use the same simple event representation, but introduce a new model that more accurately predicts test data. These methods only model the actions of a single participant, called the protagonist. Chambers and Jurafsky (2009) extended their approach to the multi-participant case, modeling the events in which all of the entities in a document are involved; however, their method cannot represent interactions between multiple enti"
E14-1024,P09-1068,0,0.614006,"r, Mooney and DeJong (1985) present an early knowledge-based method for learning scripts from a single document. These early scripts (and methods for learning them) were non-statistical and fairly brittle. Chambers and Jurafsky (2008) introduced a method for learning statistical scripts that, using a much simpler event representation that allows for efficient learning and inference. Jans et al. (2012) use the same simple event representation, but introduce a new model that more accurately predicts test data. These methods only model the actions of a single participant, called the protagonist. Chambers and Jurafsky (2009) extended their approach to the multi-participant case, modeling the events in which all of the entities in a document are involved; however, their method cannot represent interactions between multiple entities. Balasubramanian et al. (2012; 2013) describe the Rel-gram system, a Markov model similar to that of Jans et al. (2012), but with tuples instead of (verb, dependency) pairs. Our approach is simIntroduction Scripts encode knowledge of stereotypical events, including information about their typical ordered sequences of sub-events and corresponding arguments (Schank and Abelson, 1977). The"
E14-1024,P09-1025,0,0.0228829,"urrent work, in contrast, focuses on building high-recall models of general event sequences. There are also a number of systems addressing the related problem of modeling domain-specific human-human dialog for building dialog systems (Bangalore et al., 2006; Chotimongkol, 2008; Boyer et al., 2009). There have been a number of recent approaches to learning statistical scripts. Chambers and Jurafsky (2008) and Jans et al. (2012) give methods for learning models of (verb, dependency) pairs, as described above. Manshadi et al. (2008) give an n-gram model for sequences of verbs and their patients. McIntyre and Lapata (2009; 2010) use script objects learned from corpora of fairy tales to automatically generate stories. Chambers and Jurafsky (2009) extend their previous model to incorporate multiple entities, but do not directly model the different arguments of an event. Bamman et al. (2013) learn latent character personas from film summaries, associating character types with stereotypical actions; they focus on identifying persona types, rather than event inference. Manshadi et al. (2008) and Balasubramanian et al. (2012; 2013) give approaches similar to the current work for modeling sequences of events as n-gra"
E14-1024,P10-1158,0,0.0193121,"Missing"
E14-1024,1985.tmi-1.17,0,0.100011,"dependent argument. We present a script learning approach that employs events with multiple arguments. Unlike previous work, we model the interactions between multiple entities in a script. Experiments on a large corpus using the task of inferring held-out events (the “narrative cloze evaluation”) demonstrate that modeling multi-argument events improves predictive accuracy. 1 2 Background The idea of representing stereotypical event sequences for textual inference originates in the seminal work of Schank and Abelson (1977). Early scripts were manually engineered for specific domains; however, Mooney and DeJong (1985) present an early knowledge-based method for learning scripts from a single document. These early scripts (and methods for learning them) were non-statistical and fairly brittle. Chambers and Jurafsky (2008) introduced a method for learning statistical scripts that, using a much simpler event representation that allows for efficient learning and inference. Jans et al. (2012) use the same simple event representation, but introduce a new model that more accurately predicts test data. These methods only model the actions of a single participant, called the protagonist. Chambers and Jurafsky (2009"
E14-1024,D10-1048,0,0.0106327,"retains the important pairwise entity relationships between the held-out event and the other events. 4.2 These metrics target a system’s most confident predicted events: we argue that a script system is best evaluated by its top inferences. In Section 4.2.1, we evaluate on the task of inferring multi-argument events. In Section 4.2.2, we evaluate on the task of guessing pair events. Experimental Evaluation For each document, we use the Stanford dependency parser (De Marneffe et al., 2006) to get syntactic information about the document; we then use the Stanford coreference resolution engine (Raghunathan et al., 2010) to get (noisy) equivalence classes of coreferent noun phrases in a document.2 We train on approximately 1.1M articles from years 1994-2006 of the NYT portion of the Gigaword Corpus, Third Edition (Graff et al., 2007), holding out a random subset of the articles from 1999 for development and test sets. Our test set consists of 10,000 randomly selected heldout events, and our development set is 500 disjoint randomly selected held-out events. To remove duplicate documents, we hash the first 500 characters of each article and remove any articles with hash collisions. We use add-one smoothing on a"
E14-1024,D12-1071,0,0.0141388,"r to that of Jans et al. (2012), but with tuples instead of (verb, dependency) pairs. Our approach is simIntroduction Scripts encode knowledge of stereotypical events, including information about their typical ordered sequences of sub-events and corresponding arguments (Schank and Abelson, 1977). The classic example is the “restaurant script,” which encodes knowledge about what normally happens when dining out. Such knowledge can be used to improve text understanding by supporting inference of missing actions and events, as well as resolution of lexical and syntactic ambiguities and anaphora (Rahman and Ng, 2012). For example, given the text “John went to Olive Garden and ordered lasagna. He left a big tip and left,” an inference that scripts would ideally allow us to make is “John ate lasagna.” There is a small body of recent research on automatically learning probabilistic models of scripts from large corpora of raw text (Manshadi et al., 2008; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012). However, 220 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 220–229, c Gothenburg, Sweden, April 26-30 2014. 2014"
E14-1024,P10-1100,0,0.281206,"hese models use quite complex representations for events, with many different relations between events. They are not statistical, and use separate models for different scenarios (e.g. the “restaurant script” is different from the “bank script”). Generally, they require humans to encode procedural information by hand; see, however, Mooney and DeJong (1985) for an early method for learning scripts automatically from a document. Miikkulainen (1990; 1993) gives a hierarchical Neural Network system which stores sequences of events from text in episodic memory, capable of simple question answering. Regneri et al. (2010) and Li et al. (2012) give methods for using crowdsourcing to create situation-specific scripts. These methods 226 ing human judgments of frame coherence rather than a narrative cloze test. help alleviate the bottleneck of the knowledgeengineering required for traditionally conceived script systems. These systems are precisionoriented: they create small, highly accurate scripts for very limited scenarios. The current work, in contrast, focuses on building high-recall models of general event sequences. There are also a number of systems addressing the related problem of modeling domain-specific"
E17-1052,D13-1160,0,0.046264,"e that a fixed and well-trained natural language understanding component is available apriori. Kollar et al. (2013) use a probabilistic parsing and grounding model to understand natural language instructions and extend their knowledge base by asking questions. However, unlike this work, they do not use semantic parsing to leverage the compositionality of language, and also use a fixed hand-coded policy for dialog. There has been considerable work in semantic parsing using both direct supervision in the form of annotated meaning representations (Wong and Mooney, 2007; Kwiatkowski et al., 2013; Berant et al., 2013) and indirect signals from downstream tasks (Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013; Thomason et al., 2015). Artzi and Zettlemoyer (2011) use clarification dialogs to train semantic parsers for an airline reservation system without explicit annotation of meaning representations. More related to our work is that of Thomason et al. (2015), who incorporated this general approach into a system for instructing a mobile robot; however, they use a simple model of dialog state and a fixed, hand-coded dialog policy. We show that learning a dialog policy in addition to this, is more be"
E17-1052,D13-1161,0,0.0042312,"2014), but these too assume that a fixed and well-trained natural language understanding component is available apriori. Kollar et al. (2013) use a probabilistic parsing and grounding model to understand natural language instructions and extend their knowledge base by asking questions. However, unlike this work, they do not use semantic parsing to leverage the compositionality of language, and also use a fixed hand-coded policy for dialog. There has been considerable work in semantic parsing using both direct supervision in the form of annotated meaning representations (Wong and Mooney, 2007; Kwiatkowski et al., 2013; Berant et al., 2013) and indirect signals from downstream tasks (Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013; Thomason et al., 2015). Artzi and Zettlemoyer (2011) use clarification dialogs to train semantic parsers for an airline reservation system without explicit annotation of meaning representations. More related to our work is that of Thomason et al. (2015), who incorporated this general approach into a system for instructing a mobile robot; however, they use a simple model of dialog state and a fixed, hand-coded dialog policy. We show that learning a dialog policy in additi"
E17-1052,P07-1121,1,0.174571,"manner (Tellex et al., 2014), but these too assume that a fixed and well-trained natural language understanding component is available apriori. Kollar et al. (2013) use a probabilistic parsing and grounding model to understand natural language instructions and extend their knowledge base by asking questions. However, unlike this work, they do not use semantic parsing to leverage the compositionality of language, and also use a fixed hand-coded policy for dialog. There has been considerable work in semantic parsing using both direct supervision in the form of annotated meaning representations (Wong and Mooney, 2007; Kwiatkowski et al., 2013; Berant et al., 2013) and indirect signals from downstream tasks (Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013; Thomason et al., 2015). Artzi and Zettlemoyer (2011) use clarification dialogs to train semantic parsers for an airline reservation system without explicit annotation of meaning representations. More related to our work is that of Thomason et al. (2015), who incorporated this general approach into a system for instructing a mobile robot; however, they use a simple model of dialog state and a fixed, hand-coded dialog policy. We show that learning"
E17-1052,P15-2130,0,0.0153038,"Missing"
E17-1052,W14-4313,0,0.0499381,"a domain such as ours where out-of-vocabulary utterances are fairly likely, for example, in different forms of address for a person, a semantic parser that can be incrementally updated from a small number of interactions is likely to perform better. However, an empirical comparison of the two in domains where compositional language understanding is expected to be beneficial, is an interesting direction of future work. this manner, task success is improved over cases where the components are trained individually. 2 Related Work Prior work has used dialog to facilitate robot task learning, e.g. She et al. (2014), but does not account for uncertainty or dynamic changes to the language understanding module when developing a system policy. Some works use a POMDP model and common-sense knowledge (Zhang and Stone, 2015) or generate clarification questions in a probabilistic manner (Tellex et al., 2014), but these too assume that a fixed and well-trained natural language understanding component is available apriori. Kollar et al. (2013) use a probabilistic parsing and grounding model to understand natural language instructions and extend their knowledge base by asking questions. However, unlike this work,"
E17-1052,D15-1199,0,0.00689564,"Missing"
E17-1052,D11-1039,0,\N,Missing
E17-1052,Q13-1005,0,\N,Missing
H05-1091,P04-1082,0,0.00749133,"e instance – for example, the grandparent relation is defined by a single fixed path consisting of two parent relations.” We can see this happening also in the task of relation extraction from ACE, where “important concepts” are the 5 types of relations, and the “constants” defining a positive instance are the 5 types of entities. 7 Future Work Local and non-local (deep) dependencies are equally important for finding relations. In this paper we tried extracting both types of dependencies using a CCG parser, however another approach is to recover deep dependencies from syntactic parses, as in (Campbell, 2004; Levy and Manning, 2004). This may have the advantage of preserving the quality of local dependencies while completing the representation with non-local dependencies. Currently, the method assumes that the named entities are known. A natural extension is to automatically extract both the entities and their relationships. Recent research (Roth and Yih, 2004) indicates that integrating entity recognition with relation extraction in a global model that captures the mutual influences between the two tasks can lead to significant improvements in accuracy. 8 Conclusion We have presented a new kerne"
H05-1091,P97-1003,0,0.0250925,"sm, these categories allow the extraction of long-range dependencies. Together with the local word-word dependencies, they create a directed acyclic dependency graph for each parsed sentence, as shown in Figure 1. 5.2 Extracting dependencies using a CFG parser Local dependencies can be extracted from a CFG parse tree using simple heuristic rules for finding the head child for each type of constituent. Alternatively, head-modifier dependencies can be directly output by a parser whose model is based on lexical dependencies. In our experiments, we used the full parse output from Collins’ parser (Collins, 1997), in which every non-terminal node is already annotated with head information. Because local dependencies assemble into a tree for each sentence, there is only one (shortest) path between any two entities in a dependency tree. 5.3 Experimental Results A recent approach to extracting relations is described in (Culotta and Sorensen, 2004). The authors use a generalized version of the tree kernel from (Zelenko et al., 2003) to compute a kernel over relation examples, where a relation example consists of the smallest dependency tree containing the two entities of the relation. Precision and recall"
H05-1091,P04-1054,0,0.780002,"antic parsers on natural language text from different domains limit the extent to which syntactic and semantic information can be used in real IE systems. Nevertheless, various lines of work on relation extraction have shown experimentally that the use of automatically derived syntactic information can lead to significant improvements in extraction accuracy. The amount of syntactic knowledge used in IE systems varies from partof-speech only (Ray and Craven, 2001) to chunking (Ray and Craven, 2001) to shallow parse trees (Zelenko et al., 2003) to dependency trees derived from full parse trees (Culotta and Sorensen, 2004). Even though exhaustive experiments comparing the performance of a relation extraction system based on these four levels of syntactic information are yet to be conducted, a reasonable assumption is that the extraction accuracy increases with the amount of syntactic information used. The performance however depends not only on the amount of syntactic information, but also on the details of the exact models using this information. Training a machine learning system in a setting where the information used for representing the examples is only partially relevant to the actual task often leads to"
H05-1091,P02-1043,0,0.00550225,"egory is called a functor or an argument. In the example above, ’seized’ and ’several’ are functors, while ’protesters’ and ’stations’ are arguments. Syntactic categories are combined using a small set of typed combinatory rules such as functional application, composition and type raising. In Table 3 we show a sample derivation based on three functional applications. protesters NP NP NP seized (SN P )/N P (SN P )/N P several stations N P/N P NP NP SN P S Table 3: Sample derivation. In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman, 2002)3 . This parser also outputs a list of dependencies, with each dependency represented as a 4-tuple hf, a, wf , wa i, where f is the syntactic category of the functor, a is the argument number, wf is the head word of the functor, and wa is the head word of the argument. For example, the three functional applications from Table 3 result in the functor-argument dependencies enumerated below in Table 4. 3 URL:http://www.ircs.upenn.edu/˜juliahr/Parser/ 729 Because predicates (e.g. ’seized’) and adjuncts (e.g. ’several’) are always represented as functors, while complements (e.g. ’protesters’ and ’s"
H05-1091,P04-1042,0,0.026409,"example, the grandparent relation is defined by a single fixed path consisting of two parent relations.” We can see this happening also in the task of relation extraction from ACE, where “important concepts” are the 5 types of relations, and the “constants” defining a positive instance are the 5 types of entities. 7 Future Work Local and non-local (deep) dependencies are equally important for finding relations. In this paper we tried extracting both types of dependencies using a CCG parser, however another approach is to recover deep dependencies from syntactic parses, as in (Campbell, 2004; Levy and Manning, 2004). This may have the advantage of preserving the quality of local dependencies while completing the representation with non-local dependencies. Currently, the method assumes that the named entities are known. A natural extension is to automatically extract both the entities and their relationships. Recent research (Roth and Yih, 2004) indicates that integrating entity recognition with relation extraction in a global model that captures the mutual influences between the two tasks can lead to significant improvements in accuracy. 8 Conclusion We have presented a new kernel for relation extraction"
H05-1091,W04-2401,0,0.100789,"Missing"
H05-1091,C96-1079,0,\N,Missing
I17-1059,J08-1001,0,0.220915,"stin 3 Department of Computer Science, The University of Texas at Austin elisa@ferracane.com, shrekwang@utexas.edu, mooney@cs.utexas.edu 2 Abstract Our work builds upon these prior studies by exploring an effective method to (i) featurize the discourse information, and (ii) integrate discourse features into the best text classifier (i.e., CNNbased models), in the expectation of achieving state-of-the-art results in AA. Feng and Hirst (2014) (henceforth F&H14) made the first comprehensive attempt at using discourse information for AA. They employ an entity-grid model, an approach introduced by Barzilay and Lapata (2008) for the task of ordering sentences. This model tracks how the grammatical relations of salient entities (e.g., subj, obj, etc.) change between pairs of sentences in a document, thus capturing a form of discourse coherence. The grid is summarized into a vector of transition probabilities. However, because the model only records the transition between two consecutive sentences at a time, the coherence is local. Feng (2015) (henceforth F15) further extends the entity-grid model by replacing grammatical relations with discourse relations from Rhetorical Structure Theory (Mann and Thompson, 1988,"
I17-1059,D16-1245,0,0.0690291,"Missing"
I17-1059,J95-2003,0,0.848897,"Missing"
I17-1059,P17-1092,0,0.142866,"lting in a grid • Feature integration. Using a neural network architecture allows us to explore embedding the relations from the entity-grid model,3 rather than only exploiting a vector of relation probabilities. We explore these questions using two approaches to represent salient entities: grammatical relations, and RST discourse relations. We apply these models to datasets of varying sizes and genres, and find that adding any discourse information improves AA consistently on longer documents, 2 Primarily compared to previous work where discourse trees are modeled with Recursive Neural Nets (Ji and Smith, 2017). 3 Tien Nguyen and Joty (2017) are the first to propose applying embeddings in modeling local coherence (for the coherence judgment task). Our methods roughly subsume theirs, which correspond to our GR CNN2-DE (global) model (Section 3). This scheme did not come out on top in our AA tasks. 585 ot he r m (1) (2) (3) fa th er similar to Table 3. s o x s s Table 3: The entity grid for the excerpt in Table 1, where columns are salient entities and rows are sentences. Each cell contains the grammatical relation of the given entity for the given sentence (subject s, object o, another grammatical re"
I17-1059,P14-1062,0,0.105243,"Missing"
I17-1059,J99-3001,0,0.130994,"le for representing entities across a discourse. Their work is based on the entity-grid model of Barzilay and Lapata (2008) (henceforth B&L). The entity-grid model tracks the grammatical relation (subj, obj, etc.) that salient entities take on throughout a document as a way to capture local coherence . A salient entity is defined as a noun phrase that co-occurs at least twice in a document. Extensive literature has shown that subject and object relations are a strong signal for salience and it follows from Centering Theory that you want to avoid rough shifts in the center (Grosz et al., 1995; Strube and Hahn, 1999). B&L thus focus on whether a salient entity is a subject (s), object (o), other (x), or is not present (-) in a given sentence, as illustrated in Table 1. Every sentence in a document is encoded with the grammatical relation of all the salient entities, resulting in a grid • Feature integration. Using a neural network architecture allows us to explore embedding the relations from the entity-grid model,3 rather than only exploiting a vector of relation probabilities. We explore these questions using two approaches to represent salient entities: grammatical relations, and RST discourse relation"
I17-1059,P15-1150,0,0.0546886,"Missing"
I17-1059,P17-1121,0,0.0177102,"tegration. Using a neural network architecture allows us to explore embedding the relations from the entity-grid model,3 rather than only exploiting a vector of relation probabilities. We explore these questions using two approaches to represent salient entities: grammatical relations, and RST discourse relations. We apply these models to datasets of varying sizes and genres, and find that adding any discourse information improves AA consistently on longer documents, 2 Primarily compared to previous work where discourse trees are modeled with Recursive Neural Nets (Ji and Smith, 2017). 3 Tien Nguyen and Joty (2017) are the first to propose applying embeddings in modeling local coherence (for the coherence judgment task). Our methods roughly subsume theirs, which correspond to our GR CNN2-DE (global) model (Section 3). This scheme did not come out on top in our AA tasks. 585 ot he r m (1) (2) (3) fa th er similar to Table 3. s o x s s Table 3: The entity grid for the excerpt in Table 1, where columns are salient entities and rows are sentences. Each cell contains the grammatical relation of the given entity for the given sentence (subject s, object o, another grammatical relation x, or not present -). If"
I17-1059,E17-2043,0,0.19418,"(2014) and Feng (2015) go beyond the sentence level, showing that discourse information can help. However, they achieve limited performance gains and lack an in-depth analysis of discourse featurization techniques. More recently, convolutional neural networks (CNNs) have demonstrated considerable success on AA relying only on character-level n-grams (Ruder et al., 2016; Shrestha et al., 2017). The strength of these models is evidenced by findings that traditional stylometric features such as word n-grams and POS-tags do not improve, and can sometimes even hurt performance (Ruder et al., 2016; Sari et al., 2017). However, none of these CNN models make use of discourse. ∗ The first two authors contributed equally to this work. https://github.com/elisaF/authorshipattribution-discourse 1 584 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 584–593, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (1) [My father]S was a clergyman of the north of England, [who]O was deservedly respected by all who knew [him]O ; and, in his younger days, lived pretty comfortably on the joint income of a small incumbency and a snug little property of his own. (2) [My"
I17-1059,D13-1193,0,0.0738899,"Missing"
I17-1059,W11-0321,0,0.350932,"et z = hz1 , . . . , zl0 i be a sequence of discourse features9 , we treat it in a similar fashion to the charbigram sequence x, i.e. feeding it through a “parallel” convolutional net (Figure 2 right). We set the embedding size to the average number of relations, then either pad or truncate. The operation results in a pooling vector y 0 . We concatenate y 0 to the pooling vector y (which is constructed from x) then feed [y; y 0 ] to the softmax layer for the final prediction. 4 IMDB62. IMDB62 consists of 62K movie reviews from 62 users (1,000 each) from the Internet Movie dataset, compiled by Seroussi et al. (2011). Unlike the novel datasets, the reviews are considerably shorter, with a mean of 349 words per text. 4.2 Featurization As described in Section 2, in both the GR and RST variants, from each input entry we start by obtaining an entity grid. CNN2-PV. We collect the probabilities of entity role transitions (in GR) or discourse relations (in RST) for the entries. Each entry corresponds to a probability distribution vector. CNN2-DE. We employ two schemes for creating discourse feature sequences from an entity grid. While we always read the grid by column (by a salient entity), we vary whether we tr"
I17-1059,E17-2106,0,0.550172,"of identifying the author of a text, given a set of authorlabeled training texts. This task typically makes use of stylometric cues at the surface lexical and syntactic level (Stamatatos et al., 2015), although Feng and Hirst (2014) and Feng (2015) go beyond the sentence level, showing that discourse information can help. However, they achieve limited performance gains and lack an in-depth analysis of discourse featurization techniques. More recently, convolutional neural networks (CNNs) have demonstrated considerable success on AA relying only on character-level n-grams (Ruder et al., 2016; Shrestha et al., 2017). The strength of these models is evidenced by findings that traditional stylometric features such as word n-grams and POS-tags do not improve, and can sometimes even hurt performance (Ruder et al., 2016; Sari et al., 2017). However, none of these CNN models make use of discourse. ∗ The first two authors contributed equally to this work. https://github.com/elisaF/authorshipattribution-discourse 1 584 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 584–593, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (1) [My father]S was a clergyma"
I17-1059,P14-1002,0,\N,Missing
I17-2021,N15-1088,0,0.0219197,"prosodic models (Ananthakrishnan and Narayanan, 2007), phonetic postprocessing (Twiefel et al., 2014), syntactic parsing (Zechner and Waibel, 1998; Basili et al., 2013), as well as features from search engine results (Peng et al., 2013). Other work has similarly employed semantics to improve ASR performance, for example by assigning semantic category labels to entire utterances and re-ranking the ASR n-best list (Morbini et al., 2012), jointly modeling the word and semantic tag sequence (Deoras et al., 2013), and learning a semantic grammar for use by both the ASR system and semantic parser (Gaspers et al., 2015). Closest to our work is that of Erdogan et al. (2005), which uses maximum entropy modeling to combine information from the semantic parser and ASR’s language model for re-ranking. Although their method could be adapted for use with a black-box ASR, their parsing framework employs a treebanked dataset of parses (Davies et al., 1999; Jelinek et al., 1994). In contrast, the Combinatory Categorial Grammar (CCG) framework which we use in this work only requires that the root-level semantic form be given along with groundings for a small number of words (see section 2.2), significantly reducing the"
I17-2021,H94-1052,0,0.357448,"bels to entire utterances and re-ranking the ASR n-best list (Morbini et al., 2012), jointly modeling the word and semantic tag sequence (Deoras et al., 2013), and learning a semantic grammar for use by both the ASR system and semantic parser (Gaspers et al., 2015). Closest to our work is that of Erdogan et al. (2005), which uses maximum entropy modeling to combine information from the semantic parser and ASR’s language model for re-ranking. Although their method could be adapted for use with a black-box ASR, their parsing framework employs a treebanked dataset of parses (Davies et al., 1999; Jelinek et al., 1994). In contrast, the Combinatory Categorial Grammar (CCG) framework which we use in this work only requires that the root-level semantic form be given along with groundings for a small number of words (see section 2.2), significantly reducing the cost of data collection. Further, although they also experiment with an out-of-the-box language model, they only Speech is a natural channel for humancomputer interaction in robotics and consumer applications. Natural language understanding pipelines that start with speech can have trouble recovering from speech recognition errors. Black-box automatic s"
I17-2021,P98-2237,0,0.260351,"University of Texas at Austin {rcorona, jesse, mooney}@cs.utexas.edu Abstract positive user experiences (Thomason et al., 2015). We collect a dataset of spoken robot commands paired with transcriptions and semantic forms to evaluate our method.1 Given a list of ASR hypotheses, we re-rank the list to choose the hypothesis scoring best between an in-domain trained semantic parser and language model (Figure 1). This work is inspired by other re-ranking methods which have used prosodic models (Ananthakrishnan and Narayanan, 2007), phonetic postprocessing (Twiefel et al., 2014), syntactic parsing (Zechner and Waibel, 1998; Basili et al., 2013), as well as features from search engine results (Peng et al., 2013). Other work has similarly employed semantics to improve ASR performance, for example by assigning semantic category labels to entire utterances and re-ranking the ASR n-best list (Morbini et al., 2012), jointly modeling the word and semantic tag sequence (Deoras et al., 2013), and learning a semantic grammar for use by both the ASR system and semantic parser (Gaspers et al., 2015). Closest to our work is that of Erdogan et al. (2005), which uses maximum entropy modeling to combine information from the se"
I17-2021,C98-2232,0,\N,Missing
I17-2030,W15-4640,0,0.0273903,"Missing"
I17-2030,Q13-1005,0,0.26623,"formation and booking (Williams et al., 2013), as well as in non-goal oriented domains such as social-media chat-bots (Ritter et al., 2011; Shang et al., 2015). In this paper, we combine these two lines of research and propose a system that engages the user in a dialog, asking questions to elicit additional information until the system is confident that it fully understands the user’s intent and has all of the details to produce correct, complete code. An added advantage of the dialog setting is the possibility of continuous improvement of the underlying semantic parser through conversations (Artzi and Zettlemoyer, 2013; Thomason et al., 2015; Weston, 2016), which could further increase success rates for code generation and result in shorter dialogs. We focus on a restrictive, yet important class of programs that deal with conditions, i.e., if-then statements. To this end, we use the IFTTT dataset released by Quirk et al. (2015). To the best of our knowledge, this is the first attempt to use dialog for code generation from language. Generating computer code from natural language descriptions has been a longstanding problem. Prior work in this domain has restricted itself to generating code in one shot from a"
I17-2030,E17-1052,1,0.87054,"Missing"
I17-2030,P16-1069,0,0.0118227,"obability distribution over all possible values for each slot. After each user utterance, the probability distribution for one or more slots is updated based on the parse returned by the utterance parser (see Section 3.2). The system follows a hand-coded policy over the discrete state-space obtained from the belief state by assigning the values with highest probability (candidates with highest confidence) to each slot. Problem Statement Our goal is to synthesize IFTTT recipes from their natural language descriptions. Unlike prior work in this domain (Quirk et al., 2015; Dong and Lapata, 2016; Beltagy and Quirk, 2016; Liu et al., 2016), which restrict the system to synthesizing a recipe from a single description, we seek to enable the system to interact with users by engaging them in a dialog to clarify their intent when the system’s confidence in its inference is low. This is particularly crucial when there are multiple channels or functions achieving similar goals, or when the initial recipe descriptions are vague. 3 3.1.2 The dialog opens with an open-ended user utterance (a user-initiative) in which the user is expected to describe the recipe. Its parse is used to update all the slots in the belief st"
I17-2030,P15-1085,1,0.905491,"til the system is confident that it fully understands the user’s intent and has all of the details to produce correct, complete code. An added advantage of the dialog setting is the possibility of continuous improvement of the underlying semantic parser through conversations (Artzi and Zettlemoyer, 2013; Thomason et al., 2015; Weston, 2016), which could further increase success rates for code generation and result in shorter dialogs. We focus on a restrictive, yet important class of programs that deal with conditions, i.e., if-then statements. To this end, we use the IFTTT dataset released by Quirk et al. (2015). To the best of our knowledge, this is the first attempt to use dialog for code generation from language. Generating computer code from natural language descriptions has been a longstanding problem. Prior work in this domain has restricted itself to generating code in one shot from a single description. To overcome this limitation, we propose a system that can engage users in a dialog to clarify their intent until it has all the information to produce correct code. To evaluate the efficacy of dialog in code generation, we focus on synthesizing conditional statements in the form of IFTTT recip"
I17-2030,D13-1160,0,0.103338,"itself to generating code in one shot from a single description. To overcome this limitation, we propose a system that can engage users in a dialog to clarify their intent until it has all the information to produce correct code. To evaluate the efficacy of dialog in code generation, we focus on synthesizing conditional statements in the form of IFTTT recipes. 1 Introduction Building a natural language interface for programmatic tasks has long been a goal of computational linguistics. This has been explored in a plethora of domains such as generating database queries (Zelle and Mooney, 1996; Berant et al., 2013; Yin et al., 2016), building regular expressions (Manshadi et al., 2013), commanding a robot (She et al., 2014), programming on spreadsheets (Gulwani and Marron, 2014), and event-driven automation (Quirk et al., 2015), each with its own domainspecific target language. Synthesis of computer programs in general-purpose programming languages has also been explored (Ling et al., 2016; Yin and Neubig, 2017). The existing work assumes that a working program can be generated in one shot from a single natural language description. However, in many cases, users omit important details that prevents the"
I17-2030,P16-1004,0,0.0418574,"e system maintains a probability distribution over all possible values for each slot. After each user utterance, the probability distribution for one or more slots is updated based on the parse returned by the utterance parser (see Section 3.2). The system follows a hand-coded policy over the discrete state-space obtained from the belief state by assigning the values with highest probability (candidates with highest confidence) to each slot. Problem Statement Our goal is to synthesize IFTTT recipes from their natural language descriptions. Unlike prior work in this domain (Quirk et al., 2015; Dong and Lapata, 2016; Beltagy and Quirk, 2016; Liu et al., 2016), which restrict the system to synthesizing a recipe from a single description, we seek to enable the system to interact with users by engaging them in a dialog to clarify their intent when the system’s confidence in its inference is low. This is particularly crucial when there are multiple channels or functions achieving similar goals, or when the initial recipe descriptions are vague. 3 3.1.2 The dialog opens with an open-ended user utterance (a user-initiative) in which the user is expected to describe the recipe. Its parse is used to update all t"
I17-2030,D11-1054,0,0.103516,"Missing"
I17-2030,P15-1152,0,0.053694,"Missing"
I17-2030,W14-4313,0,0.0249259,"that can engage users in a dialog to clarify their intent until it has all the information to produce correct code. To evaluate the efficacy of dialog in code generation, we focus on synthesizing conditional statements in the form of IFTTT recipes. 1 Introduction Building a natural language interface for programmatic tasks has long been a goal of computational linguistics. This has been explored in a plethora of domains such as generating database queries (Zelle and Mooney, 1996; Berant et al., 2013; Yin et al., 2016), building regular expressions (Manshadi et al., 2013), commanding a robot (She et al., 2014), programming on spreadsheets (Gulwani and Marron, 2014), and event-driven automation (Quirk et al., 2015), each with its own domainspecific target language. Synthesis of computer programs in general-purpose programming languages has also been explored (Ling et al., 2016; Yin and Neubig, 2017). The existing work assumes that a working program can be generated in one shot from a single natural language description. However, in many cases, users omit important details that prevents the generation of fully executable code from their initial description. Another line of research that has recently"
I17-2030,W13-4065,0,0.0248055,"Missing"
I17-2030,W16-0105,0,0.0140829,"code in one shot from a single description. To overcome this limitation, we propose a system that can engage users in a dialog to clarify their intent until it has all the information to produce correct code. To evaluate the efficacy of dialog in code generation, we focus on synthesizing conditional statements in the form of IFTTT recipes. 1 Introduction Building a natural language interface for programmatic tasks has long been a goal of computational linguistics. This has been explored in a plethora of domains such as generating database queries (Zelle and Mooney, 1996; Berant et al., 2013; Yin et al., 2016), building regular expressions (Manshadi et al., 2013), commanding a robot (She et al., 2014), programming on spreadsheets (Gulwani and Marron, 2014), and event-driven automation (Quirk et al., 2015), each with its own domainspecific target language. Synthesis of computer programs in general-purpose programming languages has also been explored (Ling et al., 2016; Yin and Neubig, 2017). The existing work assumes that a working program can be generated in one shot from a single natural language description. However, in many cases, users omit important details that prevents the generation of full"
I17-2030,P17-1041,0,0.0277744,"ge interface for programmatic tasks has long been a goal of computational linguistics. This has been explored in a plethora of domains such as generating database queries (Zelle and Mooney, 1996; Berant et al., 2013; Yin et al., 2016), building regular expressions (Manshadi et al., 2013), commanding a robot (She et al., 2014), programming on spreadsheets (Gulwani and Marron, 2014), and event-driven automation (Quirk et al., 2015), each with its own domainspecific target language. Synthesis of computer programs in general-purpose programming languages has also been explored (Ling et al., 2016; Yin and Neubig, 2017). The existing work assumes that a working program can be generated in one shot from a single natural language description. However, in many cases, users omit important details that prevents the generation of fully executable code from their initial description. Another line of research that has recently garnered increasing attention is that of dialog systems (Singh et al., 2002; Young et al., 2013). Dialog systems have been employed for goal-directed tasks such as providing technical support (Lowe 2 2.1 Task Overview IFTTT Domain IFTTT (if-this-then-that) is a web-service that allows users to"
J16-4007,E12-1004,0,0.0613411,"vely, to identify the individual and holistic value of each feature set and systematic patterns. However, this evaluation may also be used as a framework by future lexical semantics research to see its value in end-to-end textual entailment systems. For example, we could have also included features corresponding to the many measures of distributional inclusion that were developed to predict hypernymy (Weeds, Weir, and McCarthy 2004; 795 Computational Linguistics Volume 42, Number 4 Kotlerman et al. 2010; Lenci and Benotto 2012; Santus 2013), or other supervised lexical entailment classifiers (Baroni et al. 2012; Fu et al. 2014; Weeds et al. 2014; Levy et al. 2015; Kruszewski, Paperno, and Baroni 2015). Evaluation is broken into four parts: first, we overview performance of the entire entailment rule classifier on all rules, both lexical and phrasal. We then break down these results into performance on only lexical rules and only phrasal rules. Finally, we look at only the asymmetric features to address concerns raised by Levy et al. (2015). In all sections, we evaluate the lexical rule classifier on its ability to generalize to new word pairs, as well as the full system’s performance when the entail"
J16-4007,2014.lilt-9.5,0,0.0774062,"Missing"
J16-4007,J10-4006,0,0.0369767,"e task. We use the large window size to ensure the BoW vectors captured more topical similarity, rather than syntactic similarity, which is modeled by the dependency vectors. 787 Computational Linguistics Volume 42, Number 4 Figure 4 Distribution of entailment relations on lexical items by cosine. Highly similar pairs (0.90–0.99) are less likely entailing than moderately similar pairs (0.70–0.89). Dependency Vectors. We extract (lemma/POS, relation, context/POS) tuples from each of the Stanford Collapsed CC Dependency graphs. We filter tuples with lemmas not in our 51k chosen types. Following Baroni and Lenci (2010), we model inverse relations and mark them separately. For example, “red/JJ car/NN” will generate tuples for both (car/NN, amod, red/JJ) and (red/JJ, amod−1 , car/NN). After extracting tuples, we discard all but the top 100k (relation, context/POS) pairs and build a vector space using lemma/POS as rows, and (relation, context/POS) as columns. The matrix is transformed with Positive Pointwise Mutual Information, and reduced to 300 dimensions using Singular Value Decomposition (SVD). We do not vary these parameters, but chose them as they performed best in prior work (Roller, Erk, and Boleda 201"
J16-4007,W11-2501,0,0.0761049,"Missing"
J16-4007,D10-1115,0,0.105774,"Missing"
J16-4007,S13-1002,1,0.923963,"et al. Meaning Using Logical and Distributional Models For logic-based semantics, one of the challenges is to adapt the representation to the assumptions of the probabilistic logic (Beltagy and Erk 2015). For distributional lexical and phrasal semantics, one challenge is to obtain appropriate weights for inference rules (Roller, Erk, and Boleda 2014). In probabilistic inference, the core challenge is formulating the problems to allow for efficient Markov Logic Network (MLN) inference (Beltagy and Mooney 2014). Our approach has previously been described in Garrette, Erk, and Mooney (2011) and Beltagy et al. (2013). We have demonstrated the generality of the system by applying it to both textual entailment (RTE-1 in Beltagy et al. [2013], SICK [preliminary results] and FraCas in Beltagy and Erk [2015]) and semantic textual similarity (Beltagy, Erk, and Mooney 2014), and we are investigating applications to question answering. We have demonstrated the modularity of the system by testing both MLNs (Richardson and Domingos 2006) and Probabilistic Soft Logic (Broecheler, Mihalkova, and Getoor 2010) as probabilistic inference engines (Beltagy et al. 2013; Beltagy, Erk, and Mooney 2014). The primary aim of th"
J16-4007,W15-0119,1,0.951889,"tion for a more graded representation of words and short phrases, providing information on near-synonymy and lexical entailment. Uncertainty and gradedness at the lexical and phrasal level should inform inference at all levels, so we rely on probabilistic inference to integrate logical and distributional semantics. Thus, our system has three main components, all of which present interesting challenges. 764 Beltagy et al. Meaning Using Logical and Distributional Models For logic-based semantics, one of the challenges is to adapt the representation to the assumptions of the probabilistic logic (Beltagy and Erk 2015). For distributional lexical and phrasal semantics, one challenge is to obtain appropriate weights for inference rules (Roller, Erk, and Boleda 2014). In probabilistic inference, the core challenge is formulating the problems to allow for efficient Markov Logic Network (MLN) inference (Beltagy and Mooney 2014). Our approach has previously been described in Garrette, Erk, and Mooney (2011) and Beltagy et al. (2013). We have demonstrated the generality of the system by applying it to both textual entailment (RTE-1 in Beltagy et al. [2013], SICK [preliminary results] and FraCas in Beltagy and Erk"
J16-4007,P14-1114,1,0.717939,"Missing"
J16-4007,D13-1160,0,0.145898,"in SemEval 2014, the best result was achieved by systems that did not compute a sentence representation in a compositional manner. We present a model that performs deep compositional semantic analysis and achieves state-of-the-art performance (Section 7.2). 2. Background Logical Semantics. Logical representations of meaning have a long tradition in linguistic semantics (Montague 1970; Dowty, Wall, and Peters 1981; Alshawi 1992; Kamp and Reyle 1993) and computational semantics (Blackburn and Bos 2005; van Eijck and Unger 2010), and are commonly used in semantic parsing (Zelle and Mooney 1996; Berant et al. 2013; Kwiatkowski et al. 2013). They handle many complex semantic phenomena, such as negation and quantifiers, and they identify discourse referents along with the predicates that apply to them and the relations that hold between them. However, standard first-order logic and theorem provers are binary in nature, which prevents them from capturing the graded aspects of meaning in language: Synonymy seems to come in degrees (Edmonds and Hirst 2000), as does the difference between senses in polysemous words (Brown 2008). van Eijck and Lappin (2012) write: “The case for abandoning the categorical view"
J16-4007,S14-2114,0,0.220937,"Missing"
J16-4007,W08-2222,0,0.398307,"tween them. However, standard first-order logic and theorem provers are binary in nature, which prevents them from capturing the graded aspects of meaning in language: Synonymy seems to come in degrees (Edmonds and Hirst 2000), as does the difference between senses in polysemous words (Brown 2008). van Eijck and Lappin (2012) write: “The case for abandoning the categorical view of competence and adopting a probabilistic model is at least as strong in semantics as it is in syntax.” Recent wide-coverage tools that use logic-based sentence representations include Copestake and Flickinger (2000), Bos (2008), and Lewis and Steedman (2013). We use Boxer (Bos 2008), a wide-coverage semantic analysis tool that produces logical forms, using Discourse Representation Structures (Kamp and Reyle 1993). It builds on the C&C CCG (Combinatory Categorial Grammar) parser (Clark and Curran 2004) and maps sentences into a lexically based logical form, in which the predicates are mostly words in the sentence. For example, the sentence An ogre loves a princess is mapped to: ∃x, y, z. ogre(x) ∧ agent(y, x) ∧ love(y) ∧ patient(y, z) ∧ princess(z) (2) 767 Computational Linguistics Volume 42, Number 4 As can be seen,"
J16-4007,P12-1015,0,0.0273672,"related are distributional models where the dimensions of the vectors encode model-theoretic structures rather than observed co-occurrences (Clark 2012; Grefenstette 2013; Sadrzadeh, Clark, and Coecke 2013; Herbelot and Vecchi 2015), even though they are not strictly hybrid systems as they do not include contextual distributional information. Grefenstette (2013) represents logical constructs using vectors and tensors, but concludes that they do not adequately capture logical structure, in particular, quantifiers. If, like Andrews, Vigliocco, and Vinson (2009), Silberer and Lapata (2012), and Bruni et al. (2012) (among others), we also consider perceptual context as part of distributional models, then Cooper et al. (2015) also qualifies as a hybrid logical/distributional approach. They envision a classifier that labels feature-based representations of situations (which can be viewed as perceptual distributional representations) as having a certain probability of making a proposition true, for example smile(Sandy). These propositions function as types of situations in a type-theoretic semantics. Probabilistic Logic with Markov Logic Networks. To combine logical and probabilistic information, we utiliz"
J16-4007,P04-1014,0,0.0800852,"Missing"
J16-4007,2015.lilt-10.4,0,0.509016,"than observed co-occurrences (Clark 2012; Grefenstette 2013; Sadrzadeh, Clark, and Coecke 2013; Herbelot and Vecchi 2015), even though they are not strictly hybrid systems as they do not include contextual distributional information. Grefenstette (2013) represents logical constructs using vectors and tensors, but concludes that they do not adequately capture logical structure, in particular, quantifiers. If, like Andrews, Vigliocco, and Vinson (2009), Silberer and Lapata (2012), and Bruni et al. (2012) (among others), we also consider perceptual context as part of distributional models, then Cooper et al. (2015) also qualifies as a hybrid logical/distributional approach. They envision a classifier that labels feature-based representations of situations (which can be viewed as perceptual distributional representations) as having a certain probability of making a proposition true, for example smile(Sandy). These propositions function as types of situations in a type-theoretic semantics. Probabilistic Logic with Markov Logic Networks. To combine logical and probabilistic information, we utilize MLNs (Richardson and Domingos 2006). MLNs are well suited for our approach because they provide an elegant fra"
J16-4007,copestake-flickinger-2000-open,0,0.116329,"Missing"
J16-4007,N12-1076,0,0.280892,"Missing"
J16-4007,D08-1094,1,0.83338,"n them. However, standard first-order logic and theorem provers are binary in nature, which prevents them from capturing the graded aspects of meaning in language: Synonymy seems to come in degrees (Edmonds and Hirst 2000), as does the difference between senses in polysemous words (Brown 2008). van Eijck and Lappin (2012) write: “The case for abandoning the categorical view of competence and adopting a probabilistic model is at least as strong in semantics as it is in syntax.” Recent wide-coverage tools that use logic-based sentence representations include Copestake and Flickinger (2000), Bos (2008), and Lewis and Steedman (2013). We use Boxer (Bos 2008), a wide-coverage semantic analysis tool that produces logical forms, using Discourse Representation Structures (Kamp and Reyle 1993). It builds on the C&C CCG (Combinatory Categorial Grammar) parser (Clark and Curran 2004) and maps sentences into a lexically based logical form, in which the predicates are mostly words in the sentence. For example, the sentence An ogre loves a princess is mapped to: ∃x, y, z. ogre(x) ∧ agent(y, x) ∧ love(y) ∧ patient(y, z) ∧ princess(z) (2) 767 Computational Linguistics Volume 42, Number 4 As can be seen,"
J16-4007,P14-1113,0,0.0214839,"Missing"
J16-4007,N13-1092,0,0.0993755,"Missing"
J16-4007,W11-0112,1,0.818128,"Missing"
J16-4007,P05-1014,0,0.0345764,"t to the sources of lexical and phrasal knowledge it uses, and in this article we utilize PPDB (Ganitkevitch, Van Durme, and Callison-Burch 2013) and WordNet, along with distributional models. But we are specifically interested in distributional models, in particular, in how well they can predict lexical and phrasal entailment. Our system provides a unique framework for evaluating distributional models on recognizing textual entailment (RTE) because the overall sentence representation is handled by the logic, so we can zoom in on the performance of distributional models at predicting lexical (Geffet and Dagan 2005) and phrasal entailment. The evaluation of distributional models on RTE is the third aim of our article. We build a lexical entailment classifier that exploits both task-specific features as well as distributional information, and present an in-depth evaluation of the distributional components. We now provide a brief sketch of our framework (Garrette, Erk, and Mooney 2011; Beltagy et al. 2013). Our framework is three components. The first is the logical form, which is the primary meaning representation for a sentence. The second is the distributional information, which is encoded in the form o"
J16-4007,S13-1001,0,0.567318,"c similarity of words and phrases (Landauer and Dumais 1997; Mitchell and Lapata 2010). They are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur (Lund and Burgess 1996; Landauer and Dumais 1997). Therefore, distributional models are relatively easier to build than logical representations, automatically acquire knowledge from “big data,” and capture the graded nature of linguistic meaning, but they do not adequately capture logical structure (Grefenstette 2013). Distributional models have also been extended to compute vector representations for larger phrases, for example, by adding the vectors for the individual words (Landauer and Dumais 1997) or by a component-wise product of word vectors (Mitchell and Lapata 2008, 2010), or through more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli 2010; Grefenstette and Sadrzadeh 2011). Integrating Logic-Based and Distributional Semantics. It does not seem particularly useful at this point to speculate about phenomena that either a distributional approach or a"
J16-4007,D11-1129,0,0.043449,"chutze 1998; Erk and Pado´ ¨ 2008; Thater, Furstenau, and Pinkal 2010). But at this point, fully representing structure and logical form using distributional models of phrases and sentences is still an open problem. Also, current distributional representations do not support logical inference that captures the semantics of negation, logical connectives, and quantifiers. Therefore, distributional models and logical representations of natural language meaning are complementary in their strengths, as has frequently been remarked (Coecke, Sadrzadeh, and Clark 2011; Garrette, Erk, and Mooney 2011; Grefenstette and Sadrzadeh 2011; Baroni, Bernardi, and Zamparelli 2014). Our aim has been to construct a general-purpose natural language understanding system that provides in-depth representations of sentence meaning amenable to automated inference, but that also allows for flexible and graded inferences involving word meaning. Therefore, our approach combines logical and distributional methods. Specifically, we use first-order logic as a basic representation, providing a sentence representation that can be easily interpreted and manipulated. However, we also use distributional information for a more graded representation"
J16-4007,D15-1003,0,0.0518462,"and world knowledge should be front and center in all inferential processes. Tian, Miyao, and Takuya (2014) represent sentences using Dependency-based Compositional Semantics (Liang, Jordan, and Klein 2011). They construct phrasal entailment rules based on a logic-based alignment, and use distributional similarity of aligned words to filter rules that do not surpass a given threshold. Also related are distributional models where the dimensions of the vectors encode model-theoretic structures rather than observed co-occurrences (Clark 2012; Grefenstette 2013; Sadrzadeh, Clark, and Coecke 2013; Herbelot and Vecchi 2015), even though they are not strictly hybrid systems as they do not include contextual distributional information. Grefenstette (2013) represents logical constructs using vectors and tensors, but concludes that they do not adequately capture logical structure, in particular, quantifiers. If, like Andrews, Vigliocco, and Vinson (2009), Silberer and Lapata (2012), and Bruni et al. (2012) (among others), we also consider perceptual context as part of distributional models, then Cooper et al. (2015) also qualifies as a hybrid logical/distributional approach. They envision a classifier that labels fe"
J16-4007,P88-1012,0,0.122535,"∧ princess(z) (2) 767 Computational Linguistics Volume 42, Number 4 As can be seen, Boxer uses a neo-Davidsonian framework (Parsons 1990): y is an event variable, and the semantic roles agent and patient are turned into predicates linking y to the agent x and patient z. As we discuss later, we combine Boxer’s logical form with weighted rules and perform probabilistic inference. Lewis and Steedman (2013) also integrate logical and distributional approaches, but use distributional information to create predicates for a standard binary logic and do not use probabilistic inference. Much earlier, Hobbs et al. (1988) combined logical form with weights in an abductive framework. There, the aim was to model the interpretation of a passage as its best possible explanation. Distributional Semantics. Distributional models (Turney and Pantel 2010) use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais 1997; Mitchell and Lapata 2010). They are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur"
J16-4007,Q15-1027,0,0.0421207,"Missing"
J16-4007,D13-1161,0,0.364509,"and Reyle 1993), like first-order logic, represent many linguistic phenomena like negation, quantifiers, or discourse entities. Some of these phenomena (especially negation scope and discourse entities over paragraphs) cannot be easily represented in syntax-based representations like Natural Logic (MacCartney and Manning 2009). In addition, firstorder logic has standardized inference mechanisms. Consequently, logical approaches have been widely used in semantic parsing where it supports answering complex natural language queries requiring reasoning and data aggregation (Zelle and Mooney 1996; Kwiatkowski et al. 2013; Pasupat and Liang 2015). But logic-based representations often rely on manually constructed dictionaries for lexical semantics, which can result in coverage problems. And first-order logic, being binary in nature, does not capture the graded aspect of meaning (although there are combinations of logic and probabilities). Distributional models (Turney and Pantel 2010) use contextual similarity to predict the graded semantic similarity of words and phrases (Landauer and Dumais ¨ 1997; Mitchell and Lapata 2010), and to model polysemy (Schutze 1998; Erk and Pado´ ¨ 2008; Thater, Furstenau, and Pi"
J16-4007,S14-2055,0,0.0675792,"the SemEval 2012 STS MSR-Video Description data.4 Randomly selected sentences from these two sources were first simplified to remove some linguistic phenomena that the data set was not aiming to cover. Then, additional sentences were created as variations over these sentences, by paraphrasing, negation, and reordering. RTE pairs were then created that consisted of a simplified original sentence paired with one of the transformed sentences (generated from either the same or a different original sentence). We would like to mention two particular systems that were evaluated on SICK. The first is Lai and Hockenmaier (2014), which was the top-performing system at the original shared task. It uses a linear classifier with many hand-crafted features, including alignments, word forms, POS tags, distributional similarity, WordNet, and a unique feature called Denotational Similarity. Many of these hand-crafted features are later incorporated in our lexical entailment classifier, described in Section 5.2. The Denotational Similarity uses a large database of human- and machine-generated image captions to cleverly capture some world knowledge of entailments. The second system is Bjerva et al. (2014), which also particip"
J16-4007,S12-1012,0,0.0410122,"Missing"
J16-4007,N15-1098,0,0.458345,"provide a data set of all lexical and phrasal rules needed for the SICK data set (10,211 rules). This is a valuable resource for testing lexical entailment systems on entailment relations that are actually useful in an end-to-end RTE system (Section 5.1). We evaluate a state-of-the-art compositional distributional approach (Paperno, Pham, and Baroni 2014) on the task of phrasal entailment (Section 5.2.5). We propose a simple weight learning approach to map rule weights to MLN weights (Section 6.3). The question “Do supervised distributional methods really learn lexical inference relations?” (Levy et al. 2015) has been studied before on a variety of lexical entailment data sets. For the first time, we study it on data from an actual RTE data set and show that distributional information is useful for lexical entailment (Section 7.1). Marelli et al. (2014a) report that for the SICK data set used in SemEval 2014, the best result was achieved by systems that did not compute a sentence representation in a compositional manner. We present a model that performs deep compositional semantic analysis and achieves state-of-the-art performance (Section 7.2). 2. Background Logical Semantics. Logical representat"
J16-4007,Q13-1015,0,0.110586,"he assumptions of the probabilistic logic (Beltagy and Erk 2015). For distributional lexical and phrasal semantics, one challenge is to obtain appropriate weights for inference rules (Roller, Erk, and Boleda 2014). In probabilistic inference, the core challenge is formulating the problems to allow for efficient Markov Logic Network (MLN) inference (Beltagy and Mooney 2014). Our approach has previously been described in Garrette, Erk, and Mooney (2011) and Beltagy et al. (2013). We have demonstrated the generality of the system by applying it to both textual entailment (RTE-1 in Beltagy et al. [2013], SICK [preliminary results] and FraCas in Beltagy and Erk [2015]) and semantic textual similarity (Beltagy, Erk, and Mooney 2014), and we are investigating applications to question answering. We have demonstrated the modularity of the system by testing both MLNs (Richardson and Domingos 2006) and Probabilistic Soft Logic (Broecheler, Mihalkova, and Getoor 2010) as probabilistic inference engines (Beltagy et al. 2013; Beltagy, Erk, and Mooney 2014). The primary aim of the current article is to describe our complete system in detail— all the nuts and bolts necessary to bring together the three"
J16-4007,D14-1107,0,0.0324867,"lead to erroneous entailments. If we can obtain multiple parses for a text T and query H, and hence multiple logical forms, this should increase our chances of getting a good estimate of the probability of H given T. The default CCG parser that Boxer uses is C&C (Clark and Curran 2004). This parser can be configured to produce multiple ranked parses (Ng and Curran 2012); however, we found that the top parses we get from C&C are usually not diverse enough and map to the same logical form. Therefore, in addition to the top C&C parse, we use the top parse from another recent CCG parser, EasyCCG (Lewis and Steedman 2014). Therefore, for a natural language text NT and query NH , we obtain two parses each, say ST1 and ST2 for T and SH1 and SH2 for H, which are transformed to logical forms T1 , T2 , H1 , H2 . We now compute probabilities for all possible combinations of representations of NT and NH : the probability of H1 given T1 , the probability of H1 given T2 , and conversely also the probabilities of H2 given either T1 or T2 . If the task is textual entailment with the three categories: entailment, neutral, and contradiction, then, as described in Section 4.1, we also compute the probability of ¬H1 given ei"
J16-4007,W09-3714,0,0.193266,"automatically from corpus data. There is no single representation for natural language meaning at this time that fulfills all of these requirements, but there are representations that fulfill some of them. Logic-based representations (Montague 1970; Dowty, Wall, and Peters 1981; Kamp and Reyle 1993), like first-order logic, represent many linguistic phenomena like negation, quantifiers, or discourse entities. Some of these phenomena (especially negation scope and discourse entities over paragraphs) cannot be easily represented in syntax-based representations like Natural Logic (MacCartney and Manning 2009). In addition, firstorder logic has standardized inference mechanisms. Consequently, logical approaches have been widely used in semantic parsing where it supports answering complex natural language queries requiring reasoning and data aggregation (Zelle and Mooney 1996; Kwiatkowski et al. 2013; Pasupat and Liang 2015). But logic-based representations often rely on manually constructed dictionaries for lexical semantics, which can result in coverage problems. And first-order logic, being binary in nature, does not capture the graded aspect of meaning (although there are combinations of logic a"
J16-4007,S14-2001,0,0.0594867,"build on in this article. Lewis and Steedman (2013), on the other hand, use clustering on distributional data to infer word senses, and perform standard first-order inference on the resulting logical 768 Beltagy et al. Meaning Using Logical and Distributional Models forms. The main difference between the two approaches lies in the role of gradience. Lewis and Steedman view weights and probabilities as a problem to be avoided. We believe that the uncertainty inherent in both language processing and world knowledge should be front and center in all inferential processes. Tian, Miyao, and Takuya (2014) represent sentences using Dependency-based Compositional Semantics (Liang, Jordan, and Klein 2011). They construct phrasal entailment rules based on a logic-based alignment, and use distributional similarity of aligned words to filter rules that do not surpass a given threshold. Also related are distributional models where the dimensions of the vectors encode model-theoretic structures rather than observed co-occurrences (Clark 2012; Grefenstette 2013; Sadrzadeh, Clark, and Coecke 2013; Herbelot and Vecchi 2015), even though they are not strictly hybrid systems as they do not include contextu"
J16-4007,marelli-etal-2014-sick,0,0.218007,"Missing"
J16-4007,N13-1090,0,0.0100112,"BNC, ukWaC, and a 2014-01-07 copy of Wikipedia. All corpora are preprocessed using the Stanford CoreNLP parser. We collapse particle verbs into a single token, and all tokens are annotated with a (short) POS tag so that the same lemma with a different POS is modeled separately. We keep only content words (NN, VB, RB, JJ) appearing at least 1,000 times in the corpus. The final corpus contains 50,984 types and roughly 1.5B tokens. Bag-of-Words Vectors. We filter all but the 51k chosen lemmas from the corpus, and create one sentence per line. We use Skip-Gram Negative Sampling to create vectors (Mikolov et al. 2013). We use 300 latent dimensions, a window size of 20, and 15 negative samples. These parameters were not tuned, but chosen as reasonable defaults for the task. We use the large window size to ensure the BoW vectors captured more topical similarity, rather than syntactic similarity, which is modeled by the dependency vectors. 787 Computational Linguistics Volume 42, Number 4 Figure 4 Distribution of entailment relations on lexical items by cosine. Highly similar pairs (0.90–0.99) are less likely entailing than moderately similar pairs (0.70–0.89). Dependency Vectors. We extract (lemma/POS, relat"
J16-4007,P08-1028,0,0.0399511,"n them. However, standard first-order logic and theorem provers are binary in nature, which prevents them from capturing the graded aspects of meaning in language: Synonymy seems to come in degrees (Edmonds and Hirst 2000), as does the difference between senses in polysemous words (Brown 2008). van Eijck and Lappin (2012) write: “The case for abandoning the categorical view of competence and adopting a probabilistic model is at least as strong in semantics as it is in syntax.” Recent wide-coverage tools that use logic-based sentence representations include Copestake and Flickinger (2000), Bos (2008), and Lewis and Steedman (2013). We use Boxer (Bos 2008), a wide-coverage semantic analysis tool that produces logical forms, using Discourse Representation Structures (Kamp and Reyle 1993). It builds on the C&C CCG (Combinatory Categorial Grammar) parser (Clark and Curran 2004) and maps sentences into a lexically based logical form, in which the predicates are mostly words in the sentence. For example, the sentence An ogre loves a princess is mapped to: ∃x, y, z. ogre(x) ∧ agent(y, x) ∧ love(y) ∧ patient(y, z) ∧ princess(z) (2) 767 Computational Linguistics Volume 42, Number 4 As can be seen,"
J16-4007,P12-1052,0,0.060547,"Missing"
J16-4007,P14-1009,0,0.0181022,"Missing"
J16-4007,P15-1142,0,0.0233947,"Missing"
J16-4007,D08-1068,0,0.0135133,"n them. However, standard first-order logic and theorem provers are binary in nature, which prevents them from capturing the graded aspects of meaning in language: Synonymy seems to come in degrees (Edmonds and Hirst 2000), as does the difference between senses in polysemous words (Brown 2008). van Eijck and Lappin (2012) write: “The case for abandoning the categorical view of competence and adopting a probabilistic model is at least as strong in semantics as it is in syntax.” Recent wide-coverage tools that use logic-based sentence representations include Copestake and Flickinger (2000), Bos (2008), and Lewis and Steedman (2013). We use Boxer (Bos 2008), a wide-coverage semantic analysis tool that produces logical forms, using Discourse Representation Structures (Kamp and Reyle 1993). It builds on the C&C CCG (Combinatory Categorial Grammar) parser (Clark and Curran 2004) and maps sentences into a lexically based logical form, in which the predicates are mostly words in the sentence. For example, the sentence An ogre loves a princess is mapped to: ∃x, y, z. ogre(x) ∧ agent(y, x) ∧ love(y) ∧ patient(y, z) ∧ princess(z) (2) 767 Computational Linguistics Volume 42, Number 4 As can be seen,"
J16-4007,W09-1406,0,0.00953484,"t do not surpass a given threshold. Also related are distributional models where the dimensions of the vectors encode model-theoretic structures rather than observed co-occurrences (Clark 2012; Grefenstette 2013; Sadrzadeh, Clark, and Coecke 2013; Herbelot and Vecchi 2015), even though they are not strictly hybrid systems as they do not include contextual distributional information. Grefenstette (2013) represents logical constructs using vectors and tensors, but concludes that they do not adequately capture logical structure, in particular, quantifiers. If, like Andrews, Vigliocco, and Vinson (2009), Silberer and Lapata (2012), and Bruni et al. (2012) (among others), we also consider perceptual context as part of distributional models, then Cooper et al. (2015) also qualifies as a hybrid logical/distributional approach. They envision a classifier that labels feature-based representations of situations (which can be viewed as perceptual distributional representations) as having a certain probability of making a proposition true, for example smile(Sandy). These propositions function as types of situations in a type-theoretic semantics. Probabilistic Logic with Markov Logic Networks. To com"
J16-4007,W08-2125,0,0.0353324,"for some ground clauses. For example, ogre(A) means that Anna is an ogre. Marginal inference for MLNs calculates the probability P(Q|E, R) for a query formula Q. Alchemy (Kok et al. 2005) is the most widely used MLN implementation. It is a software package that contains implementations of a variety of MLN inference and learning algorithms. However, developing a scalable, general-purpose, accurate inference method for complex MLNs is an open problem. MLNs have been used for various NLP applications, including unsupervised coreference resolution (Poon and Domingos 2008), semantic role labeling (Riedel and Meza-Ruiz 2008), and event extraction (Riedel et al. 2009). Recognizing Textual Entailment. The task that we focus on in this article is RTE (Dagan et al. 2013), the task of determining whether one natural language text, the Text T, entails, contradicts, or is not related (neutral) to another, the Hypothesis H. “Entailment” here does not mean logical entailment: The Hypothesis is entailed if a human annotator 770 Beltagy et al. Meaning Using Logical and Distributional Models judges that it plausibly follows from the Text. When using naturally occurring sentences, this is a very challenging task that should b"
J16-4007,C14-1097,1,0.8051,"Missing"
J16-4007,J98-1004,0,0.0450268,"a aggregation (Zelle and Mooney 1996; Kwiatkowski et al. 2013; Pasupat and Liang 2015). But logic-based representations often rely on manually constructed dictionaries for lexical semantics, which can result in coverage problems. And first-order logic, being binary in nature, does not capture the graded aspect of meaning (although there are combinations of logic and probabilities). Distributional models (Turney and Pantel 2010) use contextual similarity to predict the graded semantic similarity of words and phrases (Landauer and Dumais ¨ 1997; Mitchell and Lapata 2010), and to model polysemy (Schutze 1998; Erk and Pado´ ¨ 2008; Thater, Furstenau, and Pinkal 2010). But at this point, fully representing structure and logical form using distributional models of phrases and sentences is still an open problem. Also, current distributional representations do not support logical inference that captures the semantics of negation, logical connectives, and quantifiers. Therefore, distributional models and logical representations of natural language meaning are complementary in their strengths, as has frequently been remarked (Coecke, Sadrzadeh, and Clark 2011; Garrette, Erk, and Mooney 2011; Grefenstett"
J16-4007,D12-1130,0,0.00980463,"Missing"
J16-4007,P06-1101,0,0.0276372,"Missing"
J16-4007,P10-1097,0,0.0569234,"Missing"
J16-4007,P14-1008,0,0.0490958,"Missing"
J16-4007,C14-1212,0,0.0187584,"Missing"
J16-4007,C04-1146,0,0.247925,"Missing"
J16-4007,I11-1038,0,0.0728394,"Missing"
J16-4007,P08-2063,0,\N,Missing
N06-1056,P04-1083,0,0.105632,"parse. Figure 3(a) shows a possible partial semantic parse of the sample sentence based on CL ANG non-terminals (U NUM stands for uniform number). Figure 3(b) shows the corresponding CL ANG parse from which the MR is constructed. This process can be formalized as an instance of synchronous parsing (Aho and Ullman, 1972), originally developed as a theory of compilers in which syntax analysis and code generation are combined into a single phase. Synchronous parsing has seen a surge of interest recently in the machine translation community as a way of formalizing syntax-based translation models (Melamed, 2004; Chiang, 2005). According to this theory, a semantic parser defines a translation, a set of pairs of strings in which each pair is an NL sentence coupled with its MR. To finitely specify a potentially infinite translation, we use a synchronous context-free grammar (SCFG) for generating the pairs in a translation. Analogous to an ordinary CFG, each SCFG rule consists of a single non-terminal on the left-hand side (LHS). The right-hand side (RHS) of an SCFG rule is a pair of strings, hα, βi, where the non-terminals in β are a permutation of the non-terminals in α. Below are some SCFG rules that"
N06-1056,J03-1002,0,0.0999329,"ism must In this section, we focus on lexical learning, which be devised for discriminating the correct derivation is done by finding optimal word alignments between 441 If our player 4 has the ball RULE → (C ONDITION D IRECTIVE) C ONDITION → (bowner T EAM {U NUM}) T EAM → our U NUM → 4 Figure 4: Partial word alignment for the CL ANG statement and its English gloss shown in Figure 1 NL sentences and their MRs in the training set. By defining a mapping of words from one language to another, word alignments define a bilingual lexicon. Using word alignments to induce a lexicon is not a new idea (Och and Ney, 2003). Indeed, attempts have been made to directly apply machine translation systems to the problem of semantic parsing (Papineni et al., 1997; Macherey et al., 2001). However, these systems make no use of the MRL grammar, thus allocating probability mass to MR translations that are not even syntactically well-formed. Here we present a lexical induction algorithm that guarantees syntactic well-formedness of MR translations by using the MRL grammar. The basic idea is to train a statistical word alignment model on the training set, and then form a lexicon by extracting transformation rules from the K"
N06-1056,J93-2003,0,0.0572913,"yntax, although it assumes that an unambiguous, context-free grammar (CFG) of the target MRL is available. The main innovation of this al439 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 439–446, c New York, June 2006. 2006 Association for Computational Linguistics answer(count(city(loc 2(countryid(usa))))) How many cities are there in the US? Figure 2: A meaning representation in G EOQUERY gorithm is its integration with state-of-the-art statistical machine translation techniques. More specifically, a statistical word alignment model (Brown et al., 1993) is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL. Complete MRs are then formed by combining these NL substrings and their translations under a parsing framework called the synchronous CFG (Aho and Ullman, 1972), which forms the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2005). Our algorithm is called WASP, short for Word Alignment-based Semantic Parsing. In initial evaluation on several real-world data sets, we show that WASP performs favorably in terms of both accurac"
N06-1056,P00-1061,0,0.0290243,"the model can be done in cubic time with respect to sentence length using the Viterbi algorithm. An Earley chart is used for keeping track of all derivations that are consistent with the input (Stolcke, 1995). The maximum conditional likelihood criterion is used for estimating the model parameters, λi . A Gaussian prior (σ 2 = 1) is used for regularizing the model (Chen and Rosenfeld, 1999). Since gold-standard derivations are not available in the training data, correct derivations must be treated as hidden variables. Here we use a version of improved iterative scaling (IIS) coupled with EM (Riezler et al., 2000) for finding an optimal set of parameters.1 Unlike the fully-supervised case, the conditional likelihood is not concave with respect to λ, so the estimation algorithm is sensitive to initial parameters. To assume as little as possible, λ is initialized to 0. The estimation algorithm requires statistics that depend on all possible derivations for a sentence or a sentence-MR pair. While it is not feasible to enumerate all derivations, a variant of the Inside-Outside algorithm can be used for efficiently collecting the required statistics (Miyao and Tsujii, 2002). Following Zettlemoyer and Collin"
N06-1056,J95-2002,0,0.00909698,"erivation. Also for each word w there is a feature function that returns the number of times w is generated from word gaps. Generation of unseen words is modeled using an extra feature whose value is the total number of words generated from word gaps. The number of features is quite modest (less than 3,000 in our experiments). A similar feature set is used by Zettlemoyer and Collins (2005). Decoding of the model can be done in cubic time with respect to sentence length using the Viterbi algorithm. An Earley chart is used for keeping track of all derivations that are consistent with the input (Stolcke, 1995). The maximum conditional likelihood criterion is used for estimating the model parameters, λi . A Gaussian prior (σ 2 = 1) is used for regularizing the model (Chen and Rosenfeld, 1999). Since gold-standard derivations are not available in the training data, correct derivations must be treated as hidden variables. Here we use a version of improved iterative scaling (IIS) coupled with EM (Riezler et al., 2000) for finding an optimal set of parameters.1 Unlike the fully-supervised case, the conditional likelihood is not concave with respect to λ, so the estimation algorithm is sensitive to initi"
N06-1056,P05-1033,0,0.313008,"e 2: A meaning representation in G EOQUERY gorithm is its integration with state-of-the-art statistical machine translation techniques. More specifically, a statistical word alignment model (Brown et al., 1993) is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL. Complete MRs are then formed by combining these NL substrings and their translations under a parsing framework called the synchronous CFG (Aho and Ullman, 1972), which forms the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2005). Our algorithm is called WASP, short for Word Alignment-based Semantic Parsing. In initial evaluation on several real-world data sets, we show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring the same amount of supervision, and shows better robustness to variations in task complexity and word order. Section 2 provides a brief overview of the domains being considered. In Section 3, we present the semantic parsing model of WASP. Section 4 outlines the algorithm for acquiring a bilingual lexicon through the use of word alignments"
N06-1056,W05-0602,1,0.727909,"sing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (Miller et al., 1996; Papineni et al., 1997; Macherey et al., 2001), in which a typcial MR is only a single semantic frame. Learning methods have been devised that can generate MRs with a complex, nested structure (cf. Figure 1). However, these methods are mostly based on deterministic parsing (Zelle and Mooney, 1996; Kate et al., 2005), which lack the robustness that characterizes recent advances in statistical NLP. Other learning methods involve the use of fullyannotated augmented parse trees (Ge and Mooney, 2005) or prior knowledge of the NL syntax (Zettlemoyer and Collins, 2005) in training, and hence require extensive human efforts when porting to a new domain or language. In this paper, we present a novel statistical approach to semantic parsing which can handle MRs with a nested structure, based on previous work on semantic parsing using transformation rules (Kate et al., 2005). The algorithm learns a semantic parser given a set of NL sentences annotated with their correct MRs. It requires no prior knowledge of the NL syntax, although it assumes that an unambiguous, context-free grammar (CFG) of t"
N06-1056,P01-1067,0,0.00988545,"re there in the US? Figure 2: A meaning representation in G EOQUERY gorithm is its integration with state-of-the-art statistical machine translation techniques. More specifically, a statistical word alignment model (Brown et al., 1993) is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL. Complete MRs are then formed by combining these NL substrings and their translations under a parsing framework called the synchronous CFG (Aho and Ullman, 1972), which forms the basis of most existing statistical syntax-based translation models (Yamada and Knight, 2001; Chiang, 2005). Our algorithm is called WASP, short for Word Alignment-based Semantic Parsing. In initial evaluation on several real-world data sets, we show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring the same amount of supervision, and shows better robustness to variations in task complexity and word order. Section 2 provides a brief overview of the domains being considered. In Section 3, we present the semantic parsing model of WASP. Section 4 outlines the algorithm for acquiring a bilingual lexicon through the use of"
N06-1056,P96-1008,0,\N,Missing
N07-1022,J85-4002,0,0.474342,"ces to MRs. In this paper, we show how to “invert” a recent SMT-based semantic parser, WASP (Wong and Mooney, 2006), in order to produce a more effective generation system. WASP exploits the formal syntax of the MRL by learning a translator (based on a statistical synchronous contextfree grammar) that maps an NL sentence to a linearized parse-tree of its MR rather than to a flat MR string. In addition to exploiting the formal MRL grammar, our approach also allows the same learned grammar to be used for both parsing and generation, an elegant property that has been widely advocated (Kay, 1975; Jacobs, 1985; Shieber, 1988). We present experimental results in two domains previously used to test WASP’s semantic parsing ability: mapping NL queries to a formal database query language, and mapping NL soccer coaching instructions to a formal robot command language. WASP−1 is shown to produce a more accurate NL generator than P HARAOH. We also show how the idea of generating from linearized parse-trees rather than flat MRs, used effectively in WASP−1 , can also be exploited in P HARAOH. A version of P HARAOH that exploits this approach is experimentally shown to produce more accurate generators that ar"
N07-1022,T75-1004,0,0.645771,"g NL sentences to MRs. In this paper, we show how to “invert” a recent SMT-based semantic parser, WASP (Wong and Mooney, 2006), in order to produce a more effective generation system. WASP exploits the formal syntax of the MRL by learning a translator (based on a statistical synchronous contextfree grammar) that maps an NL sentence to a linearized parse-tree of its MR rather than to a flat MR string. In addition to exploiting the formal MRL grammar, our approach also allows the same learned grammar to be used for both parsing and generation, an elegant property that has been widely advocated (Kay, 1975; Jacobs, 1985; Shieber, 1988). We present experimental results in two domains previously used to test WASP’s semantic parsing ability: mapping NL queries to a formal database query language, and mapping NL soccer coaching instructions to a formal robot command language. WASP−1 is shown to produce a more accurate NL generator than P HARAOH. We also show how the idea of generating from linearized parse-trees rather than flat MRs, used effectively in WASP−1 , can also be exploited in P HARAOH. A version of P HARAOH that exploits this approach is experimentally shown to produce more accurate gene"
N07-1022,P95-1034,0,0.0153474,"of all output MRs. For generation, we need an NL grammar to ensure grammaticality, but this is not available a priori. This motivates the noisy-channel model for WASP−1 , where Pr(e|f ) is divided into two smaller 175 components: arg max Pr(e|f ) = arg max Pr(e) Pr(f |e) e (2) e Pr(e) is the language model, and Pr(f |e) is the parsing model. The generation task is to find a sentence e such that (1) e is a good sentence a priori, and (2) its meaning is the same as the input MR. For the language model, we use an n-gram model, which is remarkably useful in ranking candidate generated sentences (Knight and Hatzivassiloglou, 1995; Bangalore et al., 2000; Langkilde-Geary, 2002). For the parsing model, we re-use the one from WASP (Equation 1). Hence computing (2) means maximizing the following: max Pr(e) Pr(f |e) e ≈ max Pr(e(d)) Prλ (d|e(d)) d∈D(f ) P Pr(e(d)) · exp i λi fi (d) = max Zλ (e(d)) d∈D(f ) (3) where D(f ) is the set of derivations that are consistent with f , and e(d) is the output sentence that a derivation d yields. Compared to most existing work on generation, WASP−1 has the following characteristics: 1. It does not require any lexical information in the input MR, so lexical selection is an integral part"
N07-1022,W06-3114,0,0.0143915,"sed to construct a tactical generator. This is in con173 P HARAOH (Koehn et al., 2003) is an SMT system that uses phrases as basic translation units. During decoding, the source sentence is segmented into a sequence of phrases. These phrases are then reordered and translated into phrases in the target language, which are joined together to form the output sentence. Compared to earlier word-based methods such as IBM Models (Brown et al., 1993), phrasebased methods such as P HARAOH are much more effective in producing idiomatic translations, and are currently the best performing methods in SMT (Koehn and Monz, 2006). To use P HARAOH for NLG, we simply treat the source MRL as an NL, so that phrases in the MRL are sequences of MR tokens. Note that the grammaticality of MRs is not an issue here, as they are given as input. 3.2 WASP: The Semantic Parsing Algorithm Before showing how generation can be performed by inverting a semantic parser, we present a brief overview of WASP (Wong and Mooney, 2006), the SMT-based semantic parser on which this work is based. To describe WASP, it is best to start with an example. Consider the task of translating the English sentence in Figure 1(a) into CL ANG. To do this, we"
N07-1022,N03-1017,0,0.431572,"tical machine translation (SMT) methods in natural language generation (NLG), specifically the task of mapping statements in a formal meaning representation language (MRL) into a natural language (NL), i.e. tactical generation. Given a corpus of NL sentences each paired with a formal meaning representation (MR), it is easy to use SMT to construct a tactical generator, i.e. a statistical model that translates MRL to NL. However, there has been little, if any, research on exploiting recent SMT methods for NLG. In this paper we present results on using a recent phrase-based SMT system, P HARAOH (Koehn et al., 2003), for NLG.1 Although moderately effec1 We also tried IBM Model 4/R EWRITE (Germann, 2003), a word-based SMT system, but it gave much worse results. tive, the inability of P HARAOH to exploit the formal structure and grammar of the MRL limits its accuracy. Unlike natural languages, MRLs typically have a simple, formal syntax to support effective automated processing and inference. This MRL structure can also be used to improve language generation. Tactical generation can also be seen as the inverse of semantic parsing, the task of mapping NL sentences to MRs. In this paper, we show how to “inve"
N07-1022,W02-2103,0,0.011578,"o ensure grammaticality, but this is not available a priori. This motivates the noisy-channel model for WASP−1 , where Pr(e|f ) is divided into two smaller 175 components: arg max Pr(e|f ) = arg max Pr(e) Pr(f |e) e (2) e Pr(e) is the language model, and Pr(f |e) is the parsing model. The generation task is to find a sentence e such that (1) e is a good sentence a priori, and (2) its meaning is the same as the input MR. For the language model, we use an n-gram model, which is remarkably useful in ranking candidate generated sentences (Knight and Hatzivassiloglou, 1995; Bangalore et al., 2000; Langkilde-Geary, 2002). For the parsing model, we re-use the one from WASP (Equation 1). Hence computing (2) means maximizing the following: max Pr(e) Pr(f |e) e ≈ max Pr(e(d)) Prλ (d|e(d)) d∈D(f ) P Pr(e(d)) · exp i λi fi (d) = max Zλ (e(d)) d∈D(f ) (3) where D(f ) is the set of derivations that are consistent with f , and e(d) is the output sentence that a derivation d yields. Compared to most existing work on generation, WASP−1 has the following characteristics: 1. It does not require any lexical information in the input MR, so lexical selection is an integral part of the decoding algorithm. 2. Each predicate is"
N07-1022,W02-2106,0,0.0880069,"ion for Computational Linguistics ((bowner our {4}) (do our {6} (pos (left (half our))))) If our player 4 has the ball, then our player 6 should stay in the left side of our half. (a) CL ANG answer(state(traverse 1(riverid(’ohio’)))) What states does the Ohio run through? (b) G EOQUERY 3.1 Generation using P HARAOH Figure 1: Sample meaning representations aspects of P HARAOH’s phrase-based model can be used to improve WASP−1 , resulting in a hybrid system whose overall performance is the best. 2 MRLs and Test Domains In this work, we consider input MRs with a hierarchical structure similar to Moore (2002). The only restriction on the MRL is that it be defined by an available unambiguous context-free grammar (CFG), which is true for almost all computer languages. We also assume that the order in which MR predicates appear is relevant, i.e. the order can affect the meaning of the MR. Note that the order in which predicates appear need not be the same as the word order of the target NL, and therefore, the content planner need not know about the target NL grammar (Shieber, 1993). To ground our discussion, we consider two application domains which were originally used to demonstrate semantic parsin"
N07-1022,W05-1510,0,0.0126089,"bility can be found in Kay (1975). Shieber (1988) further noted that not only a single grammar can be used for parsing and generation, but the same language-processing architecture can be used for both tasks. Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al. (1999) for HPSG, Bangalore et al. (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. More recently, statistical chart generators have emerged, including White (2004) for CCG, Carroll and Oepen (2005) and Nakanishi et al. (2005) for HPSG. Many of these systems, however, focus on the task of surface realization—inflecting and ordering words— which ignores the problem of lexical selection. In contrast, our SMT-based methods integrate lexical selection and realization in an elegant framework and automatically learn all of their linguistic knowledge from an annotated corpus. G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. ARPA Workshop on Human Language Technology, pages 128–132, San Diego, CA. 7 M. Kay. 1996. Chart generation. In Proc. ACL-96, page"
N07-1022,J03-1002,0,0.00464571,"{U NUM 2 })i. The model, which finds an optimal mapping from words token (1) denotes a word gap of size 1, due to the unto MR predicates given a set of training sentences aligned word the that comes between has and ball. and their correct MRs. Word alignment models have It can be seen as a non-terminal that expands to at been widely used for lexical acquisition in SMT most one word, allowing for some flexibility in pat(Brown et al., 1993; Koehn et al., 2003). To use tern matching. a word alignment model in the semantic parsing scenario, we can treat the MRL simply as an NL, In WASP, G IZA++ (Och and Ney, 2003) is used and MR tokens as words, but this often leads to to obtain the best alignments from the training expoor results. First, not all MR tokens carry spe- amples. Then SCFG rules are extracted from these cific meanings. For example, in CL ANG, parenthe- alignments. The resulting SCFG, however, can be 174 If our player 4 has the ball RULE → (C ONDITION D IRECTIVE) C ONDITION → (bowner T EAM {U NUM}) T EAM → our U NUM → 4 Figure 3: Partial word alignment for the CL ANG statement and its English gloss shown in Figure 1(a) ambiguous. Therefore, a maximum-entropy model that defines the conditiona"
N07-1022,W00-1401,0,0.0177606,"we need an NL grammar to ensure grammaticality, but this is not available a priori. This motivates the noisy-channel model for WASP−1 , where Pr(e|f ) is divided into two smaller 175 components: arg max Pr(e|f ) = arg max Pr(e) Pr(f |e) e (2) e Pr(e) is the language model, and Pr(f |e) is the parsing model. The generation task is to find a sentence e such that (1) e is a good sentence a priori, and (2) its meaning is the same as the input MR. For the language model, we use an n-gram model, which is remarkably useful in ranking candidate generated sentences (Knight and Hatzivassiloglou, 1995; Bangalore et al., 2000; Langkilde-Geary, 2002). For the parsing model, we re-use the one from WASP (Equation 1). Hence computing (2) means maximizing the following: max Pr(e) Pr(f |e) e ≈ max Pr(e(d)) Prλ (d|e(d)) d∈D(f ) P Pr(e(d)) · exp i λi fi (d) = max Zλ (e(d)) d∈D(f ) (3) where D(f ) is the set of derivations that are consistent with f , and e(d) is the output sentence that a derivation d yields. Compared to most existing work on generation, WASP−1 has the following characteristics: 1. It does not require any lexical information in the input MR, so lexical selection is an integral part of the decoding algorit"
N07-1022,P03-1021,0,0.00424827,"α, βi), is in turn defined as: P (β|α)λ1 P (α|β)λ2 Pw (β|α)λ3 Pw (α|β)λ4 exp(−|α|)λ5 where P (β|α) and P (α|β) are the relative frequencies of β and α, and Pw (β|α) and Pw (α|β) are the lexical weights (Koehn et al., 2003). The word penalty, exp(−|α|), allows some control over the output sentence length. Together with the language model, the new formulation of Pr(e|f ) is a loglinear model with λi as parameters. The advantage of this model is that maximization requires no normalization and can be done exactly and efficiently. The model parameters are trained using minimum error-rate training (Och, 2003). Following the phrase extraction phase in P HARAOH, we eliminate word gaps by incorporating unaligned words as part of the extracted NL phrases (Koehn et al., 2003). The reason is that while word gaps are useful in dealing with unknown phrases during semantic parsing, for generation, using known phrases generally leads to better fluency. For the same reason, we also allow the extraction of longer phrases that correspond to multiple predicates (but no more than 5). We call the resulting hybrid system WASP−1 ++. It is similar to the syntax-based SMT system of Chiang (2005), which uses both SCFG"
N07-1022,E06-1040,0,0.0141524,"the advantage of WASP−1 ++ over P HARAOH++ was not as obvious. Our B LEU scores are not as high as those reported in Langkilde-Geary (2002) and Nakanishi et al. (2005), which are around 0.7–0.9. However, their work involves the regeneration of automatically parsed text, and the MRs that they use, which are essentially dependency parses, contain extensive lexical information of the target NL. 5.2 Human Evaluation Automatic evaluation is only an imperfect substitute for human assessment. While it is found that B LEU and N IST correlate quite well with human judgments in evaluating NLG systems (Belz and Reiter, 2006), it is best to support these figures with human evaluation, which we did on a small scale. We recruited 4 native speakers of English with no previous experience with the ROBO C UP and G EOQUERY domains. Each subject was given the same 20 sentences for each domain, randomly chosen from the test sets. For each sentence, the subjects were asked to judge the output of P HARAOH++ and WASP−1 ++ in terms of fluency and adequacy. They were presented with the following definition, adapted from Koehn and Monz (2006): Score 5 4 3 Fluency Flawless English Good English Non-native English Adequacy All mean"
N07-1022,J93-2003,0,0.0426451,"w to invert an SMT-based semantic parser, WASP, to produce a more effective generation system. Generation using SMT Methods In this section, we show how SMT methods can be used to construct a tactical generator. This is in con173 P HARAOH (Koehn et al., 2003) is an SMT system that uses phrases as basic translation units. During decoding, the source sentence is segmented into a sequence of phrases. These phrases are then reordered and translated into phrases in the target language, which are joined together to form the output sentence. Compared to earlier word-based methods such as IBM Models (Brown et al., 1993), phrasebased methods such as P HARAOH are much more effective in producing idiomatic translations, and are currently the best performing methods in SMT (Koehn and Monz, 2006). To use P HARAOH for NLG, we simply treat the source MRL as an NL, so that phrases in the MRL are sequences of MR tokens. Note that the grammaticality of MRs is not an issue here, as they are given as input. 3.2 WASP: The Semantic Parsing Algorithm Before showing how generation can be performed by inverting a semantic parser, we present a brief overview of WASP (Wong and Mooney, 2006), the SMT-based semantic parser on wh"
N07-1022,I05-1015,0,0.011911,"the notion of grammar reversability can be found in Kay (1975). Shieber (1988) further noted that not only a single grammar can be used for parsing and generation, but the same language-processing architecture can be used for both tasks. Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al. (1999) for HPSG, Bangalore et al. (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. More recently, statistical chart generators have emerged, including White (2004) for CCG, Carroll and Oepen (2005) and Nakanishi et al. (2005) for HPSG. Many of these systems, however, focus on the task of surface realization—inflecting and ordering words— which ignores the problem of lexical selection. In contrast, our SMT-based methods integrate lexical selection and realization in an elegant framework and automatically learn all of their linguistic knowledge from an annotated corpus. G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. ARPA Workshop on Human Language Technology, pages 128–132, San Diego, CA. 7 M. Kay. 1996. Chart gener"
N07-1022,P05-1033,0,0.151459,"is based. To describe WASP, it is best to start with an example. Consider the task of translating the English sentence in Figure 1(a) into CL ANG. To do this, we may first generate a parse tree of the input sentence. The meaning of the sentence is then obtained by combining the meanings of the phrases. This process can be formalized using a synchronous context-free grammar (SCFG), originally developed as a grammar formalism that combines syntax analysis and code generation in compilers (Aho and Ullman, 1972). It has been used in syntax-based SMT to model the translation of one NL to another (Chiang, 2005). A derivation for a SCFG gives rise to multiple isomorphic parse trees. Figure 2 shows a partial parse of the sample sentence and its correRULE RULE If T EAM our C ONDITION player ... C ONDITION ( U NUM has the ball (bowner T EAM 4 our (a) English { ...) U NUM }) 4 (b) CL ANG Figure 2: Partial parse trees for the CL ANG statement and its English gloss shown in Figure 1(a) sponding CL ANG parse from which an MR is constructed. Note that the two parse trees are isomorphic (ignoring terminals). Each SCFG rule consists of a non-terminal, X, on the left-hand side (LHS), and a pair of strings, hα,"
N07-1022,P02-1040,0,0.0756155,"Missing"
N07-1022,C88-2128,0,0.733799,"this paper, we show how to “invert” a recent SMT-based semantic parser, WASP (Wong and Mooney, 2006), in order to produce a more effective generation system. WASP exploits the formal syntax of the MRL by learning a translator (based on a statistical synchronous contextfree grammar) that maps an NL sentence to a linearized parse-tree of its MR rather than to a flat MR string. In addition to exploiting the formal MRL grammar, our approach also allows the same learned grammar to be used for both parsing and generation, an elegant property that has been widely advocated (Kay, 1975; Jacobs, 1985; Shieber, 1988). We present experimental results in two domains previously used to test WASP’s semantic parsing ability: mapping NL queries to a formal database query language, and mapping NL soccer coaching instructions to a formal robot command language. WASP−1 is shown to produce a more accurate NL generator than P HARAOH. We also show how the idea of generating from linearized parse-trees rather than flat MRs, used effectively in WASP−1 , can also be exploited in P HARAOH. A version of P HARAOH that exploits this approach is experimentally shown to produce more accurate generators that are more competiti"
N07-1022,J93-1008,0,0.0466032,"formance is the best. 2 MRLs and Test Domains In this work, we consider input MRs with a hierarchical structure similar to Moore (2002). The only restriction on the MRL is that it be defined by an available unambiguous context-free grammar (CFG), which is true for almost all computer languages. We also assume that the order in which MR predicates appear is relevant, i.e. the order can affect the meaning of the MR. Note that the order in which predicates appear need not be the same as the word order of the target NL, and therefore, the content planner need not know about the target NL grammar (Shieber, 1993). To ground our discussion, we consider two application domains which were originally used to demonstrate semantic parsing. The first domain is ROBO C UP. In the ROBO C UP Coach Competition (www.robocup.org), teams of agents compete in a simulated soccer game and receive coach advice written in a formal language called CL ANG (Chen et al., 2003). The task is to build a system that translates this formal advice into English. Figure 1(a) shows a piece of sample advice. The second domain is G EOQUERY, where a functional, variable-free query language is used for querying a small database on U.S. g"
N07-1022,W03-2316,0,0.00921607,"erous efforts have been made to unify the tasks of semantic parsing and tactical generation. One of the earliest espousals of the notion of grammar reversability can be found in Kay (1975). Shieber (1988) further noted that not only a single grammar can be used for parsing and generation, but the same language-processing architecture can be used for both tasks. Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al. (1999) for HPSG, Bangalore et al. (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. More recently, statistical chart generators have emerged, including White (2004) for CCG, Carroll and Oepen (2005) and Nakanishi et al. (2005) for HPSG. Many of these systems, however, focus on the task of surface realization—inflecting and ordering words— which ignores the problem of lexical selection. In contrast, our SMT-based methods integrate lexical selection and realization in an elegant framework and automatically learn all of their linguistic knowledge from an annotated corpus. G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrenc"
N07-1022,C92-2117,0,0.124795,"(www.robocup.org), teams of agents compete in a simulated soccer game and receive coach advice written in a formal language called CL ANG (Chen et al., 2003). The task is to build a system that translates this formal advice into English. Figure 1(a) shows a piece of sample advice. The second domain is G EOQUERY, where a functional, variable-free query language is used for querying a small database on U.S. geography (Kate et al., 2005). The task is to translate formal queries into NL. Figure 1(b) shows a sample query. 3 trast to existing work that focuses on the use of NLG in interlingual MT (Whitelock, 1992), in which the roles of NLG and MT are switched. We first consider using a phrase-based SMT system, P HARAOH, for NLG. Then we show how to invert an SMT-based semantic parser, WASP, to produce a more effective generation system. Generation using SMT Methods In this section, we show how SMT methods can be used to construct a tactical generator. This is in con173 P HARAOH (Koehn et al., 2003) is an SMT system that uses phrases as basic translation units. During decoding, the source sentence is segmented into a sequence of phrases. These phrases are then reordered and translated into phrases in t"
N07-1022,N06-1056,1,0.841484,"tried IBM Model 4/R EWRITE (Germann, 2003), a word-based SMT system, but it gave much worse results. tive, the inability of P HARAOH to exploit the formal structure and grammar of the MRL limits its accuracy. Unlike natural languages, MRLs typically have a simple, formal syntax to support effective automated processing and inference. This MRL structure can also be used to improve language generation. Tactical generation can also be seen as the inverse of semantic parsing, the task of mapping NL sentences to MRs. In this paper, we show how to “invert” a recent SMT-based semantic parser, WASP (Wong and Mooney, 2006), in order to produce a more effective generation system. WASP exploits the formal syntax of the MRL by learning a translator (based on a statistical synchronous contextfree grammar) that maps an NL sentence to a linearized parse-tree of its MR rather than to a flat MR string. In addition to exploiting the formal MRL grammar, our approach also allows the same learned grammar to be used for both parsing and generation, an elegant property that has been widely advocated (Kay, 1975; Jacobs, 1985; Shieber, 1988). We present experimental results in two domains previously used to test WASP’s semanti"
N07-1022,P96-1027,0,\N,Missing
N07-1022,N03-1010,0,\N,Missing
N07-2021,W05-0602,1,0.744516,"MR) which a computer program can execute to perform some task, like answering database queries or controlling a robot. These MRs are expressed in domain-specific unambiguous formal meaning representation languages (MRLs). Given a training corpus of NL sentences annotated with their correct MRs, the goal of a learning system for semantic parsing is to induce an efficient and accurate semantic parser that can map novel sentences into their correct MRs. Several learning systems have been developed for semantic parsing, many of them recently (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Kate and Mooney, 2006). These systems use supervised learning methods which only utilize annotated NL sentences. However, it requires considerable human effort to annotate sentences. In contrast, unannotated NL sentences are usually easily available. Semi-supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data (Chapelle et al., 2006). In this paper we present, to our knowledge, the first semi-supervised learning system for sema"
N07-2021,P06-1115,1,0.881242,"program can execute to perform some task, like answering database queries or controlling a robot. These MRs are expressed in domain-specific unambiguous formal meaning representation languages (MRLs). Given a training corpus of NL sentences annotated with their correct MRs, the goal of a learning system for semantic parsing is to induce an efficient and accurate semantic parser that can map novel sentences into their correct MRs. Several learning systems have been developed for semantic parsing, many of them recently (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Kate and Mooney, 2006). These systems use supervised learning methods which only utilize annotated NL sentences. However, it requires considerable human effort to annotate sentences. In contrast, unannotated NL sentences are usually easily available. Semi-supervised learning methods utilize cheaply available unannotated data during training along with annotated data and often perform better than purely supervised learning methods trained on the same amount of annotated data (Chapelle et al., 2006). In this paper we present, to our knowledge, the first semi-supervised learning system for semantic parsing. We modify"
N10-1013,N09-1003,0,0.140792,"Missing"
N10-1013,J06-1003,0,0.0467893,"e reaping many of the same benefits. Previous work on lexical semantic relatedness has focused on two approaches: (1) mining monolingual or bilingual dictionaries or other pre-existing resources to construct networks of related words (Agirre and Edmond, 2006; Ramage et al., 2009), and (2) using the distributional hypothesis to automatically infer a vector-space prototype of word meaning from large corpora (Agirre et al., 2009; Curran, 2004; Harris, 1954). The former approach tends to have greater precision, but depends on handcrafted dictionaries and cannot, in general, model sense frequency (Budanitsky and Hirst, 2006). The latter approach is fundamentally more scalable as it does not rely on specific resources and can model corpus-specific sense distributions. However, the distributional approach can suffer from poor precision, as thematically similar words (e.g., singer and actor) and antonyms often occur in similar contexts (Lin et al., 2003). Unsupervised word-sense discovery has been studied by number of researchers (Agirre and Edmond, 2006; Sch¨utze, 1998). Most work has also focused on corpus-based distributional approaches, varying the vector-space representation, e.g. by incorporating syntactic and"
N10-1013,W02-0908,0,0.0128779,"dgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models. 1 Introduction Automatically judging the degree of semantic similarity between words is an important task useful in text classification (Baker and McCallum, 1998), information retrieval (Sanderson, 1994), textual entailment, and other language processing tasks. The standard empirical approach to this task exploits the distributional hypothesis, i.e. that similar words appear in similar contexts (Curran and Moens, 2002; Lin and Pantel, 2002; Pereira et al., 1993). Traditionally, word types are represented by a single vector of contextual features derived from cooccurrence information, and semantic similarity is computed using some measure of vector distance (Lee, 1999; Lowe, 2001). However, due to homonymy and polysemy, capturing the semantics of a word with a single vector is problematic. For example, the word club is similar to both bat and association, which are not at all similar to each other. Word meaning violates the triangle inequality when viewed at the level of word types, posing a problem for vec"
N10-1013,P09-1002,0,0.011416,"Missing"
N10-1013,P07-1028,0,0.00474879,"classes: 1. Prototype models represented concepts by an abstract prototypical instance, similar to a cluster centroid in parametric density estimation. 2. Exemplar models represent concepts by a concrete set of observed instances, similar to nonparametric approaches to density estimation in statistics (Ashby and Alfonso-Reese, 1995). Tversky and Gati (1982) famously showed that conceptual similarity violates the triangle inequality, lending evidence for exemplar-based models in psychology. Exemplar models have been previously used for lexical semantics problems such as selectional preference (Erk, 2007) and thematic fit (Vandekerckhove et al., 2009). Individual exemplars can be quite noisy and the model can incur high computational overhead at prediction time since naively computing the similarity between two words using each occurrence in a textual corpus as an exemplar requires O(n2 ) comparisons. Instead, the standard 110 ... chose Zbigniew Brzezinski for the position of ... ... thus the symbol s position on his clothing was ... ... writes call options against the stock position ... ... offered a position with ... ... a position he would hold until his retirement in ... ... endanger their"
N10-1013,P99-1004,0,0.0130823,"rity between words is an important task useful in text classification (Baker and McCallum, 1998), information retrieval (Sanderson, 1994), textual entailment, and other language processing tasks. The standard empirical approach to this task exploits the distributional hypothesis, i.e. that similar words appear in similar contexts (Curran and Moens, 2002; Lin and Pantel, 2002; Pereira et al., 1993). Traditionally, word types are represented by a single vector of contextual features derived from cooccurrence information, and semantic similarity is computed using some measure of vector distance (Lee, 1999; Lowe, 2001). However, due to homonymy and polysemy, capturing the semantics of a word with a single vector is problematic. For example, the word club is similar to both bat and association, which are not at all similar to each other. Word meaning violates the triangle inequality when viewed at the level of word types, posing a problem for vector-space models (Tversky and Gati, 1982). A single “prototype” vector is simply incapable of capturing phenomena such as homonymy and polysemy. Also, most vector-space models are context independent, while the meaning of a word clearly depends on contex"
N10-1013,C02-1144,0,0.0118173,"ilarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models. 1 Introduction Automatically judging the degree of semantic similarity between words is an important task useful in text classification (Baker and McCallum, 1998), information retrieval (Sanderson, 1994), textual entailment, and other language processing tasks. The standard empirical approach to this task exploits the distributional hypothesis, i.e. that similar words appear in similar contexts (Curran and Moens, 2002; Lin and Pantel, 2002; Pereira et al., 1993). Traditionally, word types are represented by a single vector of contextual features derived from cooccurrence information, and semantic similarity is computed using some measure of vector distance (Lee, 1999; Lowe, 2001). However, due to homonymy and polysemy, capturing the semantics of a word with a single vector is problematic. For example, the word club is similar to both bat and association, which are not at all similar to each other. Word meaning violates the triangle inequality when viewed at the level of word types, posing a problem for vector-space models (Tver"
N10-1013,P93-1024,0,0.0594175,"ted words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models. 1 Introduction Automatically judging the degree of semantic similarity between words is an important task useful in text classification (Baker and McCallum, 1998), information retrieval (Sanderson, 1994), textual entailment, and other language processing tasks. The standard empirical approach to this task exploits the distributional hypothesis, i.e. that similar words appear in similar contexts (Curran and Moens, 2002; Lin and Pantel, 2002; Pereira et al., 1993). Traditionally, word types are represented by a single vector of contextual features derived from cooccurrence information, and semantic similarity is computed using some measure of vector distance (Lee, 1999; Lowe, 2001). However, due to homonymy and polysemy, capturing the semantics of a word with a single vector is problematic. For example, the word club is similar to both bat and association, which are not at all similar to each other. Word meaning violates the triangle inequality when viewed at the level of word types, posing a problem for vector-space models (Tversky and Gati, 1982). A"
N10-1013,W09-3204,0,0.0254615,"Missing"
N10-1013,J98-1004,0,0.142774,"Missing"
N10-1013,D08-1027,0,0.016205,"Missing"
N10-1013,E09-1094,0,0.0194176,"represented concepts by an abstract prototypical instance, similar to a cluster centroid in parametric density estimation. 2. Exemplar models represent concepts by a concrete set of observed instances, similar to nonparametric approaches to density estimation in statistics (Ashby and Alfonso-Reese, 1995). Tversky and Gati (1982) famously showed that conceptual similarity violates the triangle inequality, lending evidence for exemplar-based models in psychology. Exemplar models have been previously used for lexical semantics problems such as selectional preference (Erk, 2007) and thematic fit (Vandekerckhove et al., 2009). Individual exemplars can be quite noisy and the model can incur high computational overhead at prediction time since naively computing the similarity between two words using each occurrence in a textual corpus as an exemplar requires O(n2 ) comparisons. Instead, the standard 110 ... chose Zbigniew Brzezinski for the position of ... ... thus the symbol s position on his clothing was ... ... writes call options against the stock position ... ... offered a position with ... ... a position he would hold until his retirement in ... ... endanger their position as a cultural group... ... on the cha"
N15-1173,P10-1127,0,0.00370258,"ative generation results. Our network learns a joint state vector implicitly, and additionally models sequence dynamics of the language. Approach Figure 2 depicts our model for sentence generation from videos. Our framework is based on deep image description models in Donahue et al. (2014);Vinyals 1496 Convolutional Net Recurrent Net Output LSTM CNN LSTM LSTM boy LSTM LSTM is LSTM LSTM playing LSTM LSTM golf LSTM LSTM &lt;EOS&gt; mean pooling CNN LSTM ... 3 Input Video ... Predicting natural language desriptions of still images has received considerable attention, with some of the earliest works by Aker and Gaizauskas (2010), Farhadi et al. (2010), Yao et al. (2010), and Kulkarni et al. (2011) amongst others. Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014). In this work, we use deep recurrent nets (RNNs), which have recently demonstrated strong results for machine translation tasks using Long Short Term Memory (LSTM) RNNs (Sutskever et al., 2014; Cho et al., 2014). In contrast to traditional statistical"
N15-1173,W05-0909,0,0.670515,"Missing"
N15-1173,P11-1020,0,0.597873,"SVRC2012 (Russakovsky et al., 2014) image classification competition, provides a strong visual representation of objects, actions and scenes depicted in the video. Our main contributions are as follows: • We present the first end-to-end deep model for video-to-text generation that simultaneously learns a latent “meaning” state, and a fluent grammatical model of the associated language. • We leverage still image classification and caption data and transfer deep networks learned on such data to the video domain. 1495 • We provide a detailed evaluation of our model on the popular YouTube corpus (Chen and Dolan, 2011) and demonstrate a significant improvement over the state of the art. 2 Related Work Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013). For example, Rohrbach et al. (2013), Rohrbach et al. (2014) produce descriptions for videos of several people cooking in the same kitchen. These approaches generate sentences"
N15-1173,W14-4012,0,0.393961,"Missing"
N15-1173,P14-2074,0,0.0610254,"Missing"
N15-1173,Q14-1006,0,0.611034,"f supervised training data. We address the problem by transferring knowledge from auxiliary tasks. Each frame of the video is modeled by a convolutional (spatially-invariant) network pre-trained on 1.2M+ images with category labels (Krizhevsky et al., 2012). The meaning state 1494 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1494–1504, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics and sequence of words is modeled by a recurrent (temporally invariant) deep network pre-trained on 100K+ Flickr (Hodosh and Hockenmaier, 2014) and COCO (Lin et al., 2014) images with associated sentence captions. We show that such knowledge transfer significantly improves performance on the video task. Our approach is inspired by recent breakthroughs reported by several research groups in image-to-text generation, in particular, the work by Donahue et al. (2014). They applied a version of their model to video-to-text generation, but stopped short of proposing an end-to-end single network, using an intermediate role representation instead. Also, they showed results only on the narrow domain of cooking videos with a small set of pre-d"
N15-1173,W12-0504,0,0.00568186,"at simultaneously learns a latent “meaning” state, and a fluent grammatical model of the associated language. • We leverage still image classification and caption data and transfer deep networks learned on such data to the video domain. 1495 • We provide a detailed evaluation of our model on the popular YouTube corpus (Chen and Dolan, 2011) and demonstrate a significant improvement over the state of the art. 2 Related Work Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013). For example, Rohrbach et al. (2013), Rohrbach et al. (2014) produce descriptions for videos of several people cooking in the same kitchen. These approaches generate sentences by first predicting a semantic role representation, e.g., modeled with a CRF, of high-level concepts such as the actor, action and object. Then they use a template or statistical machine translation to translate the semantic representation to a sentence. Most work on “in-the-wi"
N15-1173,J10-4005,0,0.0225988,"rhadi et al. (2010), Yao et al. (2010), and Kulkarni et al. (2011) amongst others. Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014). In this work, we use deep recurrent nets (RNNs), which have recently demonstrated strong results for machine translation tasks using Long Short Term Memory (LSTM) RNNs (Sutskever et al., 2014; Cho et al., 2014). In contrast to traditional statistical MT (Koehn, 2010), RNNs naturally combine with vector-based representations, such as those for images and video. Donahue et al. (2014) and Vinyals et al. (2014) simultaneously proposed a multimodal analog of this model, with an architecture which uses a visual convnet to encode a deep state vector, and an LSTM to decode the vector into a sentence. Our approach to video to text generation is inspired by the work of Donahue et al. (2014), who also applied a variant of their model to video-to-text generation, but stopped short of training an end-toend model. Instead they converted the video to an intermediate rol"
N15-1173,W13-1302,1,0.745692,"Then they use a template or statistical machine translation to translate the semantic representation to a sentence. Most work on “in-the-wild” online video has focused on retrieval and predicting event tags rather than generating descriptive sentences; examples are tagging YouTube (Aradhye et al., 2009) and retrieving online video in the TRECVID competition (Over et al., 2012). Work on TRECVID has also included clustering both video and text features for video retrieval, e.g., (Wei et al., 2010; Huang et al., 2013). The previous work on the YouTube corpus we employ (Motwani and Mooney, 2012; Krishnamoorthy et al., 2013; Guadarrama et al., 2013; Thomason et al., 2014) used a two-step approach, first detecting a fixed tuple of role words, such as subject, verb, object, and scene, and then using a template to generate a grammatical sentence. They also utilize language models learned from large text corpora to aid visual interpretation as well as sentence generation. We compare our method to the best-performing method of Thomason et al. (2014). A recent paper by Xu et al. (2015) extracts deep features from video and a continuous vector from language, and projects both to a joint semantic space. They apply their"
N15-1173,Q14-1028,0,0.175436,"NN LSTM LSTM boy LSTM LSTM is LSTM LSTM playing LSTM LSTM golf LSTM LSTM &lt;EOS&gt; mean pooling CNN LSTM ... 3 Input Video ... Predicting natural language desriptions of still images has received considerable attention, with some of the earliest works by Aker and Gaizauskas (2010), Farhadi et al. (2010), Yao et al. (2010), and Kulkarni et al. (2011) amongst others. Propelled by successes of deep learning, several groups released record breaking results in just the past year (Donahue et al., 2014; Mao et al., 2014; Karpathy et al., 2014; Fang et al., 2014; Kiros et al., 2014; Vinyals et al., 2014; Kuznetsova et al., 2014). In this work, we use deep recurrent nets (RNNs), which have recently demonstrated strong results for machine translation tasks using Long Short Term Memory (LSTM) RNNs (Sutskever et al., 2014; Cho et al., 2014). In contrast to traditional statistical MT (Koehn, 2010), RNNs naturally combine with vector-based representations, such as those for images and video. Donahue et al. (2014) and Vinyals et al. (2014) simultaneously proposed a multimodal analog of this model, with an architecture which uses a visual convnet to encode a deep state vector, and an LSTM to decode the vector into a sentence"
N15-1173,P02-1040,0,0.120818,"Missing"
N15-1173,C14-1115,1,0.751279,"ating descriptions for “in-thewild” videos such as the YouTube domain (Figure 1) remains an open challenge. Progress in open-domain video description has been difficult in part due to large vocabularies and very limited training data consisting of videos with associated descriptive sentences. Another serious obstacle has been the lack of rich models that can capture the joint dependencies of a sequence of frames and a corresponding sequence of words. Previous work has simplified the problem by detecting a fixed set of semantic roles, such as subject, verb, and object (Guadarrama et al., 2013; Thomason et al., 2014), as an intermediate representation. This fixed representation is problematic for large vocabularies and also leads to oversimplified rigid sentence templates which are unable to model the complex structures of natural language. In this paper, we propose to translate from video pixels to natural language with a single deep neural network. Deep NNs can learn powerful features (Donahue et al., 2013; Zeiler and Fergus, 2014), but require a lot of supervised training data. We address the problem by transferring knowledge from auxiliary tasks. Each frame of the video is modeled by a convolutional ("
N15-1173,P13-1006,0,0.0074488,"cation and caption data and transfer deep networks learned on such data to the video domain. 1495 • We provide a detailed evaluation of our model on the popular YouTube corpus (Chen and Dolan, 2011) and demonstrate a significant improvement over the state of the art. 2 Related Work Most of the existing research in video description has focused on narrow domains with limited vocabularies of objects and activities (Kojima et al., 2002; Lee et al., 2008; Khan and Gotoh, 2012; Barbu et al., 2012; Ding et al., 2012; Khan and Gotoh, 2012; Das et al., 2013b; Das et al., 2013a; Rohrbach et al., 2013; Yu and Siskind, 2013). For example, Rohrbach et al. (2013), Rohrbach et al. (2014) produce descriptions for videos of several people cooking in the same kitchen. These approaches generate sentences by first predicting a semantic role representation, e.g., modeled with a CRF, of high-level concepts such as the actor, action and object. Then they use a template or statistical machine translation to translate the semantic representation to a sentence. Most work on “in-the-wild” online video has focused on retrieval and predicting event tags rather than generating descriptive sentences; examples are tagging YouTube (A"
N18-1201,D16-1203,0,0.0917993,"he fourth category of auxiliary features uses model-specific explanations. In this paper, we describe how we use these various categories of auxiliary features to improve performance for VQA. Using SWAF to effectively ensemble three recent systems, we obtain a new state-of-the-art. Our work also highlights the advantages of explainable AI models. 1 Introduction Visual Question Answering (VQA), the task of addressing open-ended questions about images (Malinowski and Fritz, 2014; Antol et al., 2015), has attracted significant attention in recent years (Andreas et al., 2016a; Goyal et al., 2016; Agrawal et al., 2016; Teney et al., 2017). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. VQA requires visual and linguistic comprehension, language grounding as well as common-sense knowledge. A variety of methods to address these challenges have been developed in recent years (Fukui et al., 2016; Xu and Saenko, 2016; Lu et al., 2016; Chen et al., 2015). The vision component of a typical VQA system extracts visual features using a deep convolutional neural network (CNN), and the linguistic component encodes the question into a semantic"
N18-1201,N16-1181,0,0.47631,"ot require querying the component models. The fourth category of auxiliary features uses model-specific explanations. In this paper, we describe how we use these various categories of auxiliary features to improve performance for VQA. Using SWAF to effectively ensemble three recent systems, we obtain a new state-of-the-art. Our work also highlights the advantages of explainable AI models. 1 Introduction Visual Question Answering (VQA), the task of addressing open-ended questions about images (Malinowski and Fritz, 2014; Antol et al., 2015), has attracted significant attention in recent years (Andreas et al., 2016a; Goyal et al., 2016; Agrawal et al., 2016; Teney et al., 2017). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. VQA requires visual and linguistic comprehension, language grounding as well as common-sense knowledge. A variety of methods to address these challenges have been developed in recent years (Fukui et al., 2016; Xu and Saenko, 2016; Lu et al., 2016; Chen et al., 2015). The vision component of a typical VQA system extracts visual features using a deep convolutional neural network (CNN), and the linguistic comp"
N18-1201,D16-1092,0,0.015938,"serves as a visual explanation for the predicted output of that model. We compare agreement between the localization-maps of the individual models to generate auxiliary features for SWAF. We take the absolute gray-scale value of the localization-maps in of each model and compute their mean rank-correlation with the localization-map of every other model. We rank the pixels according to their spatial attention and then compute the correlation between the two ranked lists. The rank correlation protocol has been used in the past to compare machinegenerated and human attention-maps as described by Das et al. (2016). We also experimented with using the Earth Mover’s Distance (EMD) in place of the rank-order correlation metric, as discussed in Section 6. We compare the localization-maps  of each pair of VQA models, generating n2 “explanation agreement” auxiliary features for SWAF, where n is the total number of models. 5 Component VQA Systems We use SWAF to combine three diverse VQA systems such that the final ensemble performs better than any individual component model even on questions with a low agreement. The three component models are trained on the VQA training set. Each of the three models is desc"
N18-1201,D16-1044,0,0.369904,"uction Visual Question Answering (VQA), the task of addressing open-ended questions about images (Malinowski and Fritz, 2014; Antol et al., 2015), has attracted significant attention in recent years (Andreas et al., 2016a; Goyal et al., 2016; Agrawal et al., 2016; Teney et al., 2017). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. VQA requires visual and linguistic comprehension, language grounding as well as common-sense knowledge. A variety of methods to address these challenges have been developed in recent years (Fukui et al., 2016; Xu and Saenko, 2016; Lu et al., 2016; Chen et al., 2015). The vision component of a typical VQA system extracts visual features using a deep convolutional neural network (CNN), and the linguistic component encodes the question into a semantic vector using a recurrent neural network (RNN). An answer is then generated conditioned on the visual features and the question vector. Most VQA systems have a single underlying method that optimizes a specific loss function and do not leverage the advantage of using multiple diverse models. One recent ensembling approach to VQA (Fukui et al., 2016) comb"
N18-1201,P15-1018,1,0.780538,"odels have been developed that combine a computer vision component with a linguistic component in order to solve the VQA challenge. Some of these models also use data-augmentation for pre-training. We discuss the VQA models we use in Section 5. Training Validation Test Images Questions 82,783 40,504 81,434 248,349 121,512 244,302 Table 1: VQA dataset splits. Stacking With Auxiliary Features (SWAF) is an ensembling technique that combines outputs from multiple systems using their confidence scores and task-relevant features. It has previously been applied effectively to information extraction (Viswanathan et al., 2015), entity linking (Rajani and Mooney, 2016) and ImageNet object detection (Rajani and Mooney, 2017). To the best of our knowledge, there has been no prior work on stacking for VQA, and we are the first to show how model-specific explanations can serve as an auxiliary feature. The auxiliary features that we use are motivated by an analysis of the VQA dataset and also inspired by related work, such as using a Bayesian framework to predict the form of the answer from the question (Kafle and Kanan, 2016). Deep learning models have been used widely on several vision and language problems. However, t"
N18-1201,D14-1162,0,0.0845999,"ingle underlying method that optimizes a specific loss function and do not leverage the advantage of using multiple diverse models. One recent ensembling approach to VQA (Fukui et al., 2016) combined multiple models that use multimodal compact bilinear pooling with attention and achieved state-of-the-art accuracy on the VQA 2016 challenge. However, their ensemble uses simple softmax averaging to combine outputs from multiple systems. Also, their model is pre-trained on the Visual Genome dataset (Krishna et al., 2017) and they concatenate learned word embeddings with pre-trained GloVe vectors (Pennington et al., 2014). Several other deep and non-deep learning approaches for solving VQA have also been proposed (Lu et al., 2016; Zhou et al., 2015; Noh et al., 2016). Although these models perform fairly well on certain image-question (IQ) pairs, they fail spectacularly on certain other IQ pairs. This led us to conclude that the various VQA models have learned to perform well on specific types of questions and images. Therefore, there is an opportunity to combine these models intelligently so as to leverage their diverse strengths. Ensembling multiple systems is a well known standard approach to improving accu"
N18-1201,D16-1201,1,0.858905,"mputer vision component with a linguistic component in order to solve the VQA challenge. Some of these models also use data-augmentation for pre-training. We discuss the VQA models we use in Section 5. Training Validation Test Images Questions 82,783 40,504 81,434 248,349 121,512 244,302 Table 1: VQA dataset splits. Stacking With Auxiliary Features (SWAF) is an ensembling technique that combines outputs from multiple systems using their confidence scores and task-relevant features. It has previously been applied effectively to information extraction (Viswanathan et al., 2015), entity linking (Rajani and Mooney, 2016) and ImageNet object detection (Rajani and Mooney, 2017). To the best of our knowledge, there has been no prior work on stacking for VQA, and we are the first to show how model-specific explanations can serve as an auxiliary feature. The auxiliary features that we use are motivated by an analysis of the VQA dataset and also inspired by related work, such as using a Bayesian framework to predict the form of the answer from the question (Kafle and Kanan, 2016). Deep learning models have been used widely on several vision and language problems. However, they frequently lack transparency and are u"
P04-1056,W03-0423,0,\N,Missing
P04-1056,J95-4004,0,\N,Missing
P04-1056,E99-1043,0,\N,Missing
P04-1056,P02-1062,0,\N,Missing
P04-1056,W03-0419,0,\N,Missing
P04-1056,N03-1028,0,\N,Missing
P06-1115,P81-1022,0,0.740451,"er than the length t. According to the equation, a production π ∈ G and a partition (p1 , .., pt ) ∈ partition(s[i..j], t) will be selected in constructing the most probable partial derivation. These will be the ones which maximize the product of the probability of π covering the substring s[i..j] with the product of probabilities of all the recursively found most probable partial derivations consistent with the partition (p1 , .., pt ). A naive implementation of the above recursion is computationally expensive, but by suitably extending the well known Earley’s context-free parsing algorithm (Earley, 1970), it can be implemented efficiently. The above task has some resemblance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways. First, the probability of a production is not independent of the sentence but depends on which substring of the sentence it covers, and second, the leaves of the tree are not individual terminals of the grammar but are substrings of words of the NL sentence. The extensions needed for Ear"
P06-1115,W05-0602,1,0.884704,"actical applications in enabling user-friendly natural language communication with computers. However, most of the research in natural language processing (NLP) has been focused on lower-level tasks like syntactic parsing, word-sense disambiguation, information extraction etc. In this paper, we have considered the important task of doing deep semantic parsing to map sentences into their computer-executable meaning representations. Previous work on learning semantic parsers either employ rule-based algorithms (Tang and Mooney, 2001; Kate et al., 2005), or use statistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006). In this paper, we present a novel kernel-based statistical method for learning semantic parsers. Kernel methods (Cristianini and Shawe-Taylor, 2000) are particularly suitable 913 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 913–920, c Sydney, July 2006. 2006 Association for Computational Linguistics NL: “If the ball is in our goal area then our player 1 should intercept it.” NL: “Which rivers run through the states bordering Texas?” CL ANG: ((bpos (goal-area our))"
P06-1115,P96-1008,0,0.408746,"productions, a SupportVector Machine (SVM) (Cristianini and ShaweTaylor, 2000) classifier is trained using string similarity as the kernel (Lodhi et al., 2002). Each classifier then estimates the probability of the production covering different substrings of the sentence. This information is used to compositionally build a complete meaning representation (MR) of the sentence. Some of the previous work on semantic parsing has focused on fairly simple domains, primarily, ATIS (Air Travel Information Service) (Price, 1990) whose semantic analysis is equivalent to filling a single semantic frame (Miller et al., 1996; Popescu et al., 2004). In this paper, we have tested K RISP on two real-world domains in which meaning representations are more complex with richer predicates and nested structures. Our experiments demonstrate that K RISP compares favorWe present a new approach for mapping natural language sentences to their formal meaning representations using stringkernel-based classifiers. Our system learns these classifiers for every production in the formal language grammar. Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these"
P06-1115,C04-1021,0,0.0233694,"tVector Machine (SVM) (Cristianini and ShaweTaylor, 2000) classifier is trained using string similarity as the kernel (Lodhi et al., 2002). Each classifier then estimates the probability of the production covering different substrings of the sentence. This information is used to compositionally build a complete meaning representation (MR) of the sentence. Some of the previous work on semantic parsing has focused on fairly simple domains, primarily, ATIS (Air Travel Information Service) (Price, 1990) whose semantic analysis is equivalent to filling a single semantic frame (Miller et al., 1996; Popescu et al., 2004). In this paper, we have tested K RISP on two real-world domains in which meaning representations are more complex with richer predicates and nested structures. Our experiments demonstrate that K RISP compares favorWe present a new approach for mapping natural language sentences to their formal meaning representations using stringkernel-based classifiers. Our system learns these classifiers for every production in the formal language grammar. Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers. Our"
P06-1115,H90-1020,0,0.0995538,"ctions of the formal MRL grammar are treated like semantic concepts. For each of these productions, a SupportVector Machine (SVM) (Cristianini and ShaweTaylor, 2000) classifier is trained using string similarity as the kernel (Lodhi et al., 2002). Each classifier then estimates the probability of the production covering different substrings of the sentence. This information is used to compositionally build a complete meaning representation (MR) of the sentence. Some of the previous work on semantic parsing has focused on fairly simple domains, primarily, ATIS (Air Travel Information Service) (Price, 1990) whose semantic analysis is equivalent to filling a single semantic frame (Miller et al., 1996; Popescu et al., 2004). In this paper, we have tested K RISP on two real-world domains in which meaning representations are more complex with richer predicates and nested structures. Our experiments demonstrate that K RISP compares favorWe present a new approach for mapping natural language sentences to their formal meaning representations using stringkernel-based classifiers. Our system learns these classifiers for every production in the formal language grammar. Meaning representations for novel na"
P06-1115,J95-2002,0,0.0126712,"ivation. These will be the ones which maximize the product of the probability of π covering the substring s[i..j] with the product of probabilities of all the recursively found most probable partial derivations consistent with the partition (p1 , .., pt ). A naive implementation of the above recursion is computationally expensive, but by suitably extending the well known Earley’s context-free parsing algorithm (Earley, 1970), it can be implemented efficiently. The above task has some resemblance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways. First, the probability of a production is not independent of the sentence but depends on which substring of the sentence it covers, and second, the leaves of the tree are not individual terminals of the grammar but are substrings of words of the NL sentence. The extensions needed for Earley’s algorithm are straightforward and are described in detail in (Kate, 2005) but due to space limitation we do not describe them here. Our extended Earley’s algorithm does a beam sear"
P06-1115,N06-1056,1,0.792792,"al language communication with computers. However, most of the research in natural language processing (NLP) has been focused on lower-level tasks like syntactic parsing, word-sense disambiguation, information extraction etc. In this paper, we have considered the important task of doing deep semantic parsing to map sentences into their computer-executable meaning representations. Previous work on learning semantic parsers either employ rule-based algorithms (Tang and Mooney, 2001; Kate et al., 2005), or use statistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006). In this paper, we present a novel kernel-based statistical method for learning semantic parsers. Kernel methods (Cristianini and Shawe-Taylor, 2000) are particularly suitable 913 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 913–920, c Sydney, July 2006. 2006 Association for Computational Linguistics NL: “If the ball is in our goal area then our player 1 should intercept it.” NL: “Which rivers run through the states bordering Texas?” CL ANG: ((bpos (goal-area our)) (do our {1} intercept)) answer(traverse(next to(statei"
P06-2034,W03-0409,0,0.0336001,"Missing"
P06-2034,W05-0620,0,0.0454797,"Missing"
P06-2034,J98-1001,0,0.018647,"Missing"
P06-2034,P05-1022,0,0.0132644,"ng representations is guided by syntax. First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. While reranking has benefited many tagging and parsing tasks (Collins, 2000; Collins, 2002c; Charniak and Johnson, 2005) including semantic role labeling (Toutanova et al., 2005), it has not yet been applied to semantic parsing. In this paper, we investigate the effect of discriminative reranking to semantic parsing. We examine if the features used in reranking syntactic parses can be adapted for semantic parsing, more concretely, for reranking the top SAPTs from the baseline model S CISSOR. The syntactic features introduced by Collins (2000) for syntactic parsing are extended with similar semantic features, based on the coupling of syntax and semantics. We present experimental results on two corpora: an interp"
P06-2034,J00-4006,0,0.00239235,"Missing"
P06-2034,P97-1003,0,0.105177,"Missing"
P06-2034,W04-0902,0,0.0678004,"Missing"
P06-2034,W02-1001,0,0.419709,"osition of meaning representations is guided by syntax. First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. While reranking has benefited many tagging and parsing tasks (Collins, 2000; Collins, 2002c; Charniak and Johnson, 2005) including semantic role labeling (Toutanova et al., 2005), it has not yet been applied to semantic parsing. In this paper, we investigate the effect of discriminative reranking to semantic parsing. We examine if the features used in reranking syntactic parses can be adapted for semantic parsing, more concretely, for reranking the top SAPTs from the baseline model S CISSOR. The syntactic features introduced by Collins (2000) for syntactic parsing are extended with similar semantic features, based on the coupling of syntax and semantics. We present experimental res"
P06-2034,P05-1073,0,0.0474282,"Missing"
P06-2034,P02-1034,0,0.599737,"osition of meaning representations is guided by syntax. First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. While reranking has benefited many tagging and parsing tasks (Collins, 2000; Collins, 2002c; Charniak and Johnson, 2005) including semantic role labeling (Toutanova et al., 2005), it has not yet been applied to semantic parsing. In this paper, we investigate the effect of discriminative reranking to semantic parsing. We examine if the features used in reranking syntactic parses can be adapted for semantic parsing, more concretely, for reranking the top SAPTs from the baseline model S CISSOR. The syntactic features introduced by Collins (2000) for syntactic parsing are extended with similar semantic features, based on the coupling of syntax and semantics. We present experimental res"
P06-2034,H05-1100,0,0.0159231,"unseen test examples, where the parameter vectors used in testing is the average of each parameter vector generated during the training process. 3 Features for Reranking SAPTs In our setting, reranking models discriminate between SAPTs that can lead to correct MRs and those that can not. Intuitively, both syntactic and semantic features describing the syntactic and semantic substructures of a SAPT would be good indicators of the SAPT’s correctness. The syntactic features introduced by Collins (2000) for reranking syntactic parse trees have been proven successfully in both English and Spanish (Cowan and Collins, 2005). We examine if these syntactic features can be adapted for semantic parsing by creating similar semantic features. In the following section, we first briefly describe the syntactic features introduced by Collins (2000), and then introduce two adapted semantic feature sets. A SAPT in CL ANG is shown in Figure 3 for illustrating the features throughout this section. 2.3 The Averaged Perceptron Reranking Model Averaged perceptron (Collins, 2002a) has been successfully applied to several tagging and parsing reranking tasks (Collins, 2002c; Collins, 2002a), and in this paper, we employed it in rer"
P06-2034,W05-0602,1,0.726245,"ive Reranking for Semantic Parsing Ruifang Ge Raymond J. Mooney Department of Computer Sciences University of Texas at Austin Austin, TX 78712 {grf,mooney}@cs.utexas.edu Abstract natural language sentences to complete formal meaning representations (MRs), where the meaning of each part of a sentence is analyzed, including noun phrases, verb phrases, negation, quantifiers and so on. Semantic parsing enables logic reasoning and is critical in many practical tasks, such as speech understanding (Zue and Glass, 2000), question answering (Lev et al., 2004) and advice taking (Kuhlmann et al., 2004). Ge and Mooney (2005) introduced an approach, S CISSOR, where the composition of meaning representations is guided by syntax. First, a statistical parser is used to generate a semanticallyaugmented parse tree (SAPT), where each internal node includes both a syntactic and semantic label. Once a SAPT is generated, an additional meaningcomposition process guided by the tree structure is used to translate it into a final formal meaning representation. The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features. While reranking has benefite"
P06-2034,J02-3001,0,\N,Missing
P06-2034,P02-1062,0,\N,Missing
P07-1073,P98-1013,0,0.00453179,"model where the features are based on words, or word sequences, is going to give too much weight to words or combinations of words that are correlated with either of individual arguments. This overweighting will adversely affect extraction performance through an increased number of errors. A method for eliminating this type of bias is introduced in Section 5.1.  [Type II Bias] While Type I bias is due to words that are correlated with the arguments of a relation instance, the Type II bias is caused by words that are specific to the relation instance itself. Using 579  FrameNet terminology (Baker et al., 1998), these correspond to instantiated frame elements. For example, the corporate acquisition frame can be seen as a subtype of the “Getting” frame in FrameNet. The core elements in this frame are the Recipient (e.g. Google) and the Theme (e.g. YouTube), which for the acquisition relationship coincide with the two arguments. They do not contribute any bias, since they are replaced with the generic tags he1 i and he2 i in all sentences from the bag. There are however other frame elements – peripheral, or extra-thematic – that can be instantiated with the same value in many sentences. In Figure 1, f"
P07-1073,P04-1054,0,0.111067,"sentences containing the pairs are extracted from the web. We extend an existing relation extraction method to handle this weaker form of supervision, and present experimental results demonstrating that our approach can reliably extract relations from web documents. 1 Introduction A growing body of recent work in information extraction has addressed the problem of relation extraction (RE), identifying relationships between entities stated in text, such as LivesIn(Person, Location) or EmployedBy(Person, Company). Supervised learning has been shown to be effective for RE (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2006); however, annotating large corpora with examples of the relations to be extracted is expensive and tedious. In this paper, we introduce a supervised learning approach to RE that requires only a handful of training examples and uses the web as a corpus. Given a few pairs of well-known entities that clearly exhibit or do not exhibit a particular relation, such as CorpAcquired(Google, YouTube) and not(CorpAcquired(Yahoo, Microsoft)), a search engine is used to find sentences on the web that mention both of the entities in each of the pairs. 576 Raymond J. Mooney Depart"
P07-1073,C92-2082,0,0.171622,"vely, implicit negative evidence can be extracted from sentences in positive bags by exploiting the fact that, besides the two relation arguments, a sentence from a positive bag may contain other entity mentions. Any pair of entities different from the relation pair is very likely to be a negative example for that relation. This is similar to the concept of negative neighborhoods introduced by Smith and Eisner (2005), and has the potential of eliminating both Type I and Type II bias. 9 Related Work One of the earliest IE methods designed to work with a reduced amount of supervision is that of Hearst (1992), where a small set of seed patterns is used in a bootstrapping fashion to mine pairs of hypernym-hyponym nouns. Bootstrapping is actually orthogonal to our method, which could be used as the pattern learner in every bootstrapping iteration. A more recent IE system that works by bootstrapping relation extraction patterns from the web is K NOW I TA LL (Etzioni et al., 2005). For a given target relation, supervision in K NOW I TA LL is provided as a rule template containing words that describe the class of the arguments (e.g. “company”), and a small set of seed extraction patterns (e.g. “has acq"
P07-1073,P05-1044,0,0.0183637,"ng Type II bias, either by modifying the word weights, or by integrating an appropriate measure of word distribution across positive bags directly in the objective function for the MIL problem. Alternatively, implicit negative evidence can be extracted from sentences in positive bags by exploiting the fact that, besides the two relation arguments, a sentence from a positive bag may contain other entity mentions. Any pair of entities different from the relation pair is very likely to be a negative example for that relation. This is similar to the concept of negative neighborhoods introduced by Smith and Eisner (2005), and has the potential of eliminating both Type I and Type II bias. 9 Related Work One of the earliest IE methods designed to work with a reduced amount of supervision is that of Hearst (1992), where a small set of seed patterns is used in a bootstrapping fashion to mine pairs of hypernym-hyponym nouns. Bootstrapping is actually orthogonal to our method, which could be used as the pattern learner in every bootstrapping iteration. A more recent IE system that works by bootstrapping relation extraction patterns from the web is K NOW I TA LL (Etzioni et al., 2005). For a given target relation, s"
P07-1073,C98-1013,0,\N,Missing
P07-1121,W05-0602,1,0.683365,"Missing"
P07-1121,J03-1002,0,0.00417815,"the F ORM nonterminal in the grandparent node in turn gives the logical form in Figure 1(a). This is the yield of the MR parse tree, since the root node of the parse tree is reached. 3.1 Lexical Acquisition Given a set of training sentences paired with their correct logical forms, {hei , fi i}, the main learning task is to find a λ-SCFG, G, that covers the training data. Like most existing work on syntax-based SMT (Chiang, 2005; Galley et al., 2006), we construct G using rules extracted from word alignments. We use the K = 5 most probable word alignments for the training set given by GIZA++ (Och and Ney, 2003), with variable names ignored to reduce sparsity. Rules are then extracted from each word alignment as follows. To ground our discussion, we use the word alignment in Figure 4 as an example. To represent the logical form in Figure 4, we use its linearized 962 parse—a list of MRL productions that generate the logical form, in top-down, left-most order (cf. Figure 2(a)). Since the MRL grammar is unambiguous, every logical form has a unique linearized parse. We assume the alignment to be n-to-1, where each word is linked to at most one MRL production. Rules are extracted in a bottom-up manner, st"
P07-1121,P03-1067,0,0.0910264,"Missing"
P07-1121,W04-0818,0,0.0118038,"al variables play an important role (Blackburn and Bos, 2005). For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the G EOQUERY functional query language in Wong and Mooney (2006)). However, development of such a functional language is non-trivial, and as we will see, logical languages can be more appropriate for certain domains. On the other hand, most existing methods for mapping NL sentences to logical forms involve substantial hand-written components that are difficult to maintain (Joshi and Vijay-Shanker, 2001; Bayer et al., 2004; Bos, 2005). Zettlemoyer and Collins (2005) present a statistical method that is considerProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 960–967, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics ably more robust, but it still relies on hand-written rules for lexical acquisition, which can create a performance bottleneck. In this work, we show that methods developed for SMT can be brought to bear on tasks where logical forms are involved, such as semantic parsing. In particular, we extend the WASP semantic parsing"
P07-1121,P05-1033,0,0.083152,"s between natural languages (NL). In generating multiple parse trees in a single derivation, synchronous grammars are ideal for modeling syntax-based translation because they describe not only the hierarchical structures of a sentence and its translation, but also the exact correspondence between their sub-parts. Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (SCFG) (Wu, 1997) and synchronous treesubstitution grammars (STSG) (Yamada and Knight, 2001). Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006). Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, 960 which is the construction of a complete, formal meaning representation (MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation. The result is a robust semantic parser that gives g"
P07-1121,J05-1003,0,0.0081443,"tity types that the m arguments can refer to: valid answers (Grice, 1975). It is also similar to Schuler’s (2003) use of model-theoretic interpretations to guide syntactic parsing. Errors that do not involve type mismatch are handled by adding new features to the maximumentropy model (Section 3.2). We only consider features that are based on the MR translations, and therefore, these features can be seen as an implicit language model of the target MRL (Papineni et al., 1997). Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. We use only the MRL part of the rules. For example, a negative weight for the combination of Q UERY → answer(x1 ,F ORM(x1 )) and F ORM → λx1 .equal(x1 , ) would discourage any parse that yields Figure 7(c). The two-level rules features, along with the features described in Section 3.2, are used in the final version of λ-WASP. 6 Experiments point( ): {(POINT)} density( , ): We evaluated the λ-WASP algorithm in the G EO {(COUNTRY, NUM), (STATE, NUM), (CITY, NUM)} QUERY domain. The larger G EOQU"
P07-1121,P06-1121,0,0.0467278,"ral languages (NL). In generating multiple parse trees in a single derivation, synchronous grammars are ideal for modeling syntax-based translation because they describe not only the hierarchical structures of a sentence and its translation, but also the exact correspondence between their sub-parts. Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (SCFG) (Wu, 1997) and synchronous treesubstitution grammars (STSG) (Yamada and Knight, 2001). Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006). Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, 960 which is the construction of a complete, formal meaning representation (MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation. The result is a robust semantic parser that gives good performance in var"
P07-1121,N06-1056,1,0.829614,"ation, but also the exact correspondence between their sub-parts. Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (SCFG) (Wu, 1997) and synchronous treesubstitution grammars (STSG) (Yamada and Knight, 2001). Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006). Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, 960 which is the construction of a complete, formal meaning representation (MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation. The result is a robust semantic parser that gives good performance in various domains. More recently, we show that our SCFG-based parser can be inverted to produce a state-of-the-art NL generator, where a formal MRL is translated into an NL (Wong and Mooney, 2007). Currently, the use of learned synchronous gr"
P07-1121,N07-1022,1,0.698825,"(MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation. The result is a robust semantic parser that gives good performance in various domains. More recently, we show that our SCFG-based parser can be inverted to produce a state-of-the-art NL generator, where a formal MRL is translated into an NL (Wong and Mooney, 2007). Currently, the use of learned synchronous grammars in semantic parsing and NL generation is limited to simple MRLs that are free of logical variables. This is because grammar formalisms such as SCFG do not have a principled mechanism for handling logical variables. This is unfortunate because most existing work on computational semantics is based on predicate logic, where logical variables play an important role (Blackburn and Bos, 2005). For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the G EOQUERY functional q"
P07-1121,J97-3002,0,0.00969738,"hronous grammars have seen a surge of interest recently in the statistical machine translation (SMT) community as a way of formalizing syntax-based translation models between natural languages (NL). In generating multiple parse trees in a single derivation, synchronous grammars are ideal for modeling syntax-based translation because they describe not only the hierarchical structures of a sentence and its translation, but also the exact correspondence between their sub-parts. Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (SCFG) (Wu, 1997) and synchronous treesubstitution grammars (STSG) (Yamada and Knight, 2001). Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006). Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, 960 which is the construction of a complete, formal meaning representation (MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our al"
P07-1121,P01-1067,0,0.0100201,"he statistical machine translation (SMT) community as a way of formalizing syntax-based translation models between natural languages (NL). In generating multiple parse trees in a single derivation, synchronous grammars are ideal for modeling syntax-based translation because they describe not only the hierarchical structures of a sentence and its translation, but also the exact correspondence between their sub-parts. Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (SCFG) (Wu, 1997) and synchronous treesubstitution grammars (STSG) (Yamada and Knight, 2001). Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006). Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, 960 which is the construction of a complete, formal meaning representation (MR) of an NL sentence. In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). Our algorithm, WASP, uses statistical models developed for syntax-based SMT for l"
P09-1069,D08-1082,0,0.226626,"ge (NL) sentence into a completely formal meaning representation (MR) or logical form. A meaning representation language (MRL) is a formal unambiguous language that supports automated inference, such as first-order predicate logic. This distinguishes it from related tasks such as semantic role labeling (SRL) (Carreras and Marquez, 2004) and other forms of “shallow” semantic analysis that do not produce completely formal representations. A number of systems for automatically learning semantic parsers have been proposed (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008). Given a training corpus of NL sentences annotated with their correct MRs, these systems induce an interpreter for mapping novel sentences into the given MRL. Previous methods for learning semantic parsers do not utilize an existing syntactic parser that provides disambiguated parse trees.1 However, accurate syntactic parsers are available for many 2 Background In this paper, we consider two domains. The first is ROBO C UP (www.robocup.org). In the ROBO C UP Coach Competition, soccer agents compete on a simulated soccer field and receive coaching instructions in a formal language called CL AN"
P09-1069,W04-2412,0,0.0161931,"erpretations. After describing the details of our approach, we present experimental results on standard corpora demonstrating improved results on learning NL interfaces for database querying and simulated robot control. Introduction Semantic parsing is the task of mapping a natural language (NL) sentence into a completely formal meaning representation (MR) or logical form. A meaning representation language (MRL) is a formal unambiguous language that supports automated inference, such as first-order predicate logic. This distinguishes it from related tasks such as semantic role labeling (SRL) (Carreras and Marquez, 2004) and other forms of “shallow” semantic analysis that do not produce completely formal representations. A number of systems for automatically learning semantic parsers have been proposed (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008). Given a training corpus of NL sentences annotated with their correct MRs, these systems induce an interpreter for mapping novel sentences into the given MRL. Previous methods for learning semantic parsers do not utilize an existing syntactic parser that provides disambiguated parse trees.1 However, accurate syntactic p"
P09-1069,J03-1002,0,0.00434612,"he ROBOCUP and G EOQUERY domains respectively. languages and could potentially be used to learn more effective semantic analyzers. This paper presents an approach to learning semantic parsers that uses parse trees from an existing syntactic analyzer to drive the interpretation process. The learned parser uses standard compositional semantics to construct alternative MRs for a sentence based on its syntax tree, and then chooses the best MR based on a trained statistical disambiguation model. The learning system first employs a word alignment method from statistical machine translation (GIZA++ (Och and Ney, 2003)) to acquire a semantic lexicon that maps words to logical predicates. Then it induces rules for composing MRs and estimates the parameters of a maximumentropy model for disambiguating semantic interpretations. After describing the details of our approach, we present experimental results on standard corpora demonstrating improved results on learning NL interfaces for database querying and simulated robot control. Introduction Semantic parsing is the task of mapping a natural language (NL) sentence into a completely formal meaning representation (MR) or logical form. A meaning representation la"
P09-1069,N06-1056,1,0.898754,"Missing"
P09-1069,P07-1121,1,0.936647,"apping a natural language (NL) sentence into a completely formal meaning representation (MR) or logical form. A meaning representation language (MRL) is a formal unambiguous language that supports automated inference, such as first-order predicate logic. This distinguishes it from related tasks such as semantic role labeling (SRL) (Carreras and Marquez, 2004) and other forms of “shallow” semantic analysis that do not produce completely formal representations. A number of systems for automatically learning semantic parsers have been proposed (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008). Given a training corpus of NL sentences annotated with their correct MRs, these systems induce an interpreter for mapping novel sentences into the given MRL. Previous methods for learning semantic parsers do not utilize an existing syntactic parser that provides disambiguated parse trees.1 However, accurate syntactic parsers are available for many 2 Background In this paper, we consider two domains. The first is ROBO C UP (www.robocup.org). In the ROBO C UP Coach Competition, soccer agents compete on a simulated soccer field and receive coaching instructions in a formal lan"
P09-1069,W05-0602,1,0.549583,"trol. Introduction Semantic parsing is the task of mapping a natural language (NL) sentence into a completely formal meaning representation (MR) or logical form. A meaning representation language (MRL) is a formal unambiguous language that supports automated inference, such as first-order predicate logic. This distinguishes it from related tasks such as semantic role labeling (SRL) (Carreras and Marquez, 2004) and other forms of “shallow” semantic analysis that do not produce completely formal representations. A number of systems for automatically learning semantic parsers have been proposed (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008). Given a training corpus of NL sentences annotated with their correct MRs, these systems induce an interpreter for mapping novel sentences into the given MRL. Previous methods for learning semantic parsers do not utilize an existing syntactic parser that provides disambiguated parse trees.1 However, accurate syntactic parsers are available for many 2 Background In this paper, we consider two domains. The first is ROBO C UP (www.robocup.org). In the ROBO C UP Coach Competition, soccer agents compete on a simulated soccer f"
P09-1069,P06-2034,1,0.815577,"Missing"
P09-1069,W01-0521,0,0.0537397,"Missing"
P09-1069,P06-1115,1,0.922194,"Missing"
P09-1069,D07-1071,0,0.159008,"Missing"
P09-1069,J04-4004,0,\N,Missing
P09-1069,J03-4003,0,\N,Missing
P10-2008,P03-1054,0,0.0304577,"rmance of the PCFG-based approach with a bag-of-words baseline, we decided to normalize the training set based on the number of words, rather than sentences. For testing, we used 15 documents per author for datasets with news articles and 5 or 10 documents per author for the Poetry dataset. More details about the datasets can be found in Table 1. corpus like the Wall Street Journal (WSJ) or Brown corpus from the Penn Treebank (http: //www.cis.upenn.edu/˜treebank/) to automatically annotate (i.e. treebank) the training documents for each author. In our experiments, we used the Stanford Parser (Klein and Manning, 2003b; Klein and Manning, 2003a) and the OpenNLP sentence segmenter (http://opennlp.sourceforge.net/). Our approach is summarized below: Input – A training set of documents labeled with author names and a test set of documents with unknown authors. 1. Train a statistical parser on a generic corpus like the WSJ or Brown corpus. 2. Treebank each training document using the parser trained in Step 1. 3. Train a PCFG Gi for each author Ai using the treebanked documents for that author. 4. For each test document, compute its likelihood for each grammar Gi by multiplying the probability of the top PCFG p"
P10-2008,C08-1065,0,0.480569,"paper articles or short stories, the author’s style could be considered a distinct “language.” Authorship attribution, also referred to as authorship identification or prediction, studies strategies for discriminating between the styles of different authors. These strategies have numerous applications, including settling disputes regarding the authorship of old and historically important documents (Mosteller and Wallace, 1984), automatic plagiarism detection, determination of document authenticity in court (Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or sentences. Peng et al. (2003) build a character-level n-gram model for each author. Stamatatos et al. (1999) and Luyckx and Daelemans 2 Authorship Attribution using PCFG We now describe our approach"
P10-2008,E99-1021,0,0.248613,"Juola and Sofko, 2004), cyber crime investigation (Zheng et al., 2009), and forensics (Luyckx and Daelemans, 2008). The general approach to authorship attribution is to extract a number of style markers from the text and use these style markers as features to train a classifier (Burrows, 1987; Binongo and Smith, 1999; Diederich et al., 2000; Holmes and Forsyth, 1995; Joachims, 1998; Mosteller and Wallace, 1984). These style markers could include the frequencies of certain characters, function words, phrases or sentences. Peng et al. (2003) build a character-level n-gram model for each author. Stamatatos et al. (1999) and Luyckx and Daelemans 2 Authorship Attribution using PCFG We now describe our approach to authorship attribution. Given a training set of documents from different authors, we build a PCFG for each author based on the documents they have written. Given a test document, we parse it using each author’s grammar and assign it to the author whose PCFG produced the highest likelihood for the document. In order to build a PCFG, a standard statistical parser takes a corpus of parse trees of sentences as training input. Since we do not have access to authors’ documents annotated with parse trees, we"
P12-1037,P11-1062,0,0.0247121,"tional facts using purely logical deduction. Sorower et al. (2011) propose a probabilistic approach to modeling implicit information as missing facts and use MLNs to infer these missing facts. They learn first-order rules for the MLN by performing exhaustive search. As mentioned earlier, inference using both these approaches, logical deduction and MLNs, have certain limitations, which BLPs help overcome. DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni, 2007) learn inference rules, also called entailment rules that capture synonymous relations and entities from text. Berant et al. (Berant et al., 2011) propose an approach that uses transitivity constraints for learning entailment rules for typed predicates. Unlike the systems described above, these systems do not learn complex first-order rules that capture common sense knowledge. Further, most of these systems do not use extractions from an IE system to learn entailment rules, thereby making them less related to our approach. 3 Bayesian Logic Programs Bayesian logic programs (BLPs) (Kersting and De Raedt, 2007; Kersting and Raedt, 2008) can be considered as templates for constructing directed graphical models (Bayes nets). Formally, a BLP"
P12-1037,N04-1001,0,0.0776068,"s in the network and P a(Xi ) represents the parents of Xi . Once a ground network is constructed, standard probabilistic inference methods can be used to answer various types of queries as reviewed by Koller and Friedman (2009). The parameters in the BLP model can be learned using the methods described by Kersting and De Raedt (2008). 351 4 4.1 Learning BLPs to Infer Implicit Facts Learning Rules from Extracted Data The first step involves learning commonsense knowledge in the form of first-order Horn rules from text. We first extract facts that are explicitly stated in the text using S IRE (Florian et al., 2004), an IE system developed by IBM. We then learn first-order rules from these extracted facts using L IME (Mccreath and Sharma, 1998), an ILP system designed for noisy training data. We first identify a set of target relations we want to infer. Typically, an ILP system takes a set of positive and negative instances for a target relation, along with a background knowledge base (in our case, other facts extracted from the same document) from which the positive instances are potentially inferable. In our task, we only have direct access to positive instances of target relations, i.e the relevant fa"
P12-1037,D08-1009,0,0.532155,"Missing"
P12-1037,D10-1106,0,0.572024,"en use to infer additional facts from text. The rest of the paper is organized as follows. Section 2 discusses related work and highlights key differences between our approach and existing work. Section 3 provides a brief background on BLPs. Section 4 describes our BLP-based approach to learning to infer implicit facts. Section 5 describes our experimental methodology and discusses the results of our evaluation. Finally, Section 6 discusses potential future work and Section 7 presents our final conclusions. 2 Related Work Several previous projects (Nahm and Mooney, 2000; Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) have mined inference rules from data automatically extracted from text by an IE system. Similar to our approach, these systems use the learned rules to infer additional information from facts directly extracted from a document. Nahm and Mooney (2000) learn propositional rules using C4.5 (Quinlan, 1993) from data extracted from computer-related job-postings, and therefore cannot learn multi-relational rules with quantified variables. Other systems (Carlson et al., 2010; Schoenmackers et al., 2010; Doppa et al., 2010; Sorower et al., 2011) learn first-"
P12-1037,N07-1016,0,0.0150943,"en and Kok, 2003), an existing ILP system, to learn firstorder rules. They propose several approaches to score the rules, which are used to infer additional facts using purely logical deduction. Sorower et al. (2011) propose a probabilistic approach to modeling implicit information as missing facts and use MLNs to infer these missing facts. They learn first-order rules for the MLN by performing exhaustive search. As mentioned earlier, inference using both these approaches, logical deduction and MLNs, have certain limitations, which BLPs help overcome. DIRT (Lin and Pantel, 2001) and RESOLVER (Yates and Etzioni, 2007) learn inference rules, also called entailment rules that capture synonymous relations and entities from text. Berant et al. (Berant et al., 2011) propose an approach that uses transitivity constraints for learning entailment rules for typed predicates. Unlike the systems described above, these systems do not learn complex first-order rules that capture common sense knowledge. Further, most of these systems do not use extractions from an IE system to learn entailment rules, thereby making them less related to our approach. 3 Bayesian Logic Programs Bayesian logic programs (BLPs) (Kersting and"
P13-1022,P05-1022,0,0.032652,"erefore leads to a more accurate model.3 ther improve the plan execution performance, and reranking using all of the feature groups (All) performs the best, as expected. However, since our model is optimizing plan execution during training, the results for parse accuracy are always worse than the baseline model. 6 Related Work Discriminative reranking is a common machine learning technique to improve the output of generative models. It has been shown to be effective for various natural language processing tasks including syntactic parsing (Collins, 2000; Collins, 2002b; Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), partof-speech tagging (Collins, 2002a), semantic role labeling (Toutanova et al., 2005), named entity recognition (Collins, 2002c). machine translation (Shen et al., 2004; Fraser and Marcu, 2006) and surface realization in generation (White and Rajkumar, 2009; Konstas and Lapata, 2012). However, to our knowledge, there has been no previous attempt to apply discriminative reranking to grounded language acquisition, where goldstandard reference parses are not typically available for training reranking models. Our use of res"
P13-1022,W10-2903,0,0.0648362,"groups helps fur3 We also tried extending Gold to use multiple reference parses in the same manner, but this actually degraded its performance for all metrics. This indicates that, unlike Multi, parses other than the best one do not have useful information in terms of optimizing normal parse accuracy. Instead, additional parses seem to add noise to the training process in this case. Therefore, updating with multiple parses does not appear to be useful in standard reranking. 225 Acknowledgments to work on learning semantic parsers from execution output such as the answers to database queries (Clarke et al., 2010; Liang et al., 2011). Although the demands of grounded language tasks, such as following navigation instructions, are different, it would be interesting to try adapting these alternative approaches to such problems. 7 We thank anonymous reviewers for their helpful comments to improve this paper. This work was funded by the NSF grant IIS-0712907 and IIS1016312. Experiments were performed on the Mastodon Cluster, provided by NSF Grant EIA0303609. Future Work References Benjamin B¨orschinger, Bevan K. Jones, and Mark Johnson. 2011. Reducing grounded learning tasks to grammatical inference. In Pr"
P13-1022,J05-1003,0,0.0322835,"of weak feedback and therefore leads to a more accurate model.3 ther improve the plan execution performance, and reranking using all of the feature groups (All) performs the best, as expected. However, since our model is optimizing plan execution during training, the results for parse accuracy are always worse than the baseline model. 6 Related Work Discriminative reranking is a common machine learning technique to improve the output of generative models. It has been shown to be effective for various natural language processing tasks including syntactic parsing (Collins, 2000; Collins, 2002b; Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), partof-speech tagging (Collins, 2002a), semantic role labeling (Toutanova et al., 2005), named entity recognition (Collins, 2002c). machine translation (Shen et al., 2004; Fraser and Marcu, 2006) and surface realization in generation (White and Rajkumar, 2009; Konstas and Lapata, 2012). However, to our knowledge, there has been no previous attempt to apply discriminative reranking to grounded language acquisition, where goldstandard reference parses are not typically available for training rera"
P13-1022,W02-1001,0,0.661687,"tment of Computer Science The University of Texas at Austin Austin, TX 78701, USA scimitar@cs.utexas.edu Abstract to the original work of Chen and Mooney (2011), it was still far from human performance. Since their system employs a generative model, discriminative reranking (Collins, 2000) could potentially improve its performance. By training a discriminative classifier that uses global features of complete parses to identify correct interpretations, a reranker can significantly improve the accuracy of a generative model. Reranking has been successfully employed to improve syntactic parsing (Collins, 2002b), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), semantic role labeling (Toutanova et al., 2005), and named entity recognition (Collins, 2002c). Standard reranking requires gold-standard interpretations (e.g. parse trees) to train the discriminative classifier. However, grounded language learning does not provide gold-standard interpretations for the training examples. Only the ambiguous perceptual context of the utterance is provided as supervision. For the navigation task, this supervision consists of the observed sequence of actions taken by a human when following an instruction"
P13-1022,P02-1034,0,0.356152,"tment of Computer Science The University of Texas at Austin Austin, TX 78701, USA scimitar@cs.utexas.edu Abstract to the original work of Chen and Mooney (2011), it was still far from human performance. Since their system employs a generative model, discriminative reranking (Collins, 2000) could potentially improve its performance. By training a discriminative classifier that uses global features of complete parses to identify correct interpretations, a reranker can significantly improve the accuracy of a generative model. Reranking has been successfully employed to improve syntactic parsing (Collins, 2002b), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), semantic role labeling (Toutanova et al., 2005), and named entity recognition (Collins, 2002c). Standard reranking requires gold-standard interpretations (e.g. parse trees) to train the discriminative classifier. However, grounded language learning does not provide gold-standard interpretations for the training examples. Only the ambiguous perceptual context of the utterance is provided as supervision. For the navigation task, this supervision consists of the observed sequence of actions taken by a human when following an instruction"
P13-1022,D08-1082,0,0.269898,"rsity of Texas at Austin Austin, TX 78701, USA scimitar@cs.utexas.edu Abstract to the original work of Chen and Mooney (2011), it was still far from human performance. Since their system employs a generative model, discriminative reranking (Collins, 2000) could potentially improve its performance. By training a discriminative classifier that uses global features of complete parses to identify correct interpretations, a reranker can significantly improve the accuracy of a generative model. Reranking has been successfully employed to improve syntactic parsing (Collins, 2002b), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), semantic role labeling (Toutanova et al., 2005), and named entity recognition (Collins, 2002c). Standard reranking requires gold-standard interpretations (e.g. parse trees) to train the discriminative classifier. However, grounded language learning does not provide gold-standard interpretations for the training examples. Only the ambiguous perceptual context of the utterance is provided as supervision. For the navigation task, this supervision consists of the observed sequence of actions taken by a human when following an instruction. Therefore, it is impossible to dire"
P13-1022,P06-1097,0,0.0086895,"are always worse than the baseline model. 6 Related Work Discriminative reranking is a common machine learning technique to improve the output of generative models. It has been shown to be effective for various natural language processing tasks including syntactic parsing (Collins, 2000; Collins, 2002b; Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), partof-speech tagging (Collins, 2002a), semantic role labeling (Toutanova et al., 2005), named entity recognition (Collins, 2002c). machine translation (Shen et al., 2004; Fraser and Marcu, 2006) and surface realization in generation (White and Rajkumar, 2009; Konstas and Lapata, 2012). However, to our knowledge, there has been no previous attempt to apply discriminative reranking to grounded language acquisition, where goldstandard reference parses are not typically available for training reranking models. Our use of response-based training is similar Comparison of different feature groups Table 3 compares reranking results using the different feature groups described in Section 4. Compared to the baseline model (Kim and Mooney, 2012), each of the feature groups Base (base features),"
P13-1022,P06-2034,1,0.776814,"Austin Austin, TX 78701, USA scimitar@cs.utexas.edu Abstract to the original work of Chen and Mooney (2011), it was still far from human performance. Since their system employs a generative model, discriminative reranking (Collins, 2000) could potentially improve its performance. By training a discriminative classifier that uses global features of complete parses to identify correct interpretations, a reranker can significantly improve the accuracy of a generative model. Reranking has been successfully employed to improve syntactic parsing (Collins, 2002b), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), semantic role labeling (Toutanova et al., 2005), and named entity recognition (Collins, 2002c). Standard reranking requires gold-standard interpretations (e.g. parse trees) to train the discriminative classifier. However, grounded language learning does not provide gold-standard interpretations for the training examples. Only the ambiguous perceptual context of the utterance is provided as supervision. For the navigation task, this supervision consists of the observed sequence of actions taken by a human when following an instruction. Therefore, it is impossible to directly apply conventiona"
P13-1022,N04-1023,0,0.126994,"Missing"
P13-1022,P05-1073,0,0.0227551,"Missing"
P13-1022,W05-1506,0,0.0353029,"est parse trees for the baseline model do not necessarily produce the n-best distinct plans, since many parses can produce the same plan. Therefore, we adapt the G EN function to produce the n best distinct plans rather than the n best parses. This may require examining many more than the n best parses, because many parses have insignificant differences that do not affect the final plan. The score assigned to a plan is the probability of the most probable parse that generates that plan. In order to efficiently compute the n best plans, we modify the exact n-best parsing algorithm developed by Huang and Chiang (2005). The modified algorithm ensures that each plan in the computed n best list produces a new distinct plan. where E XEC(y) is the execution rate of the MR plan m derived from parse tree y. In the experiments below, we demonstrate that, by exploiting multiple reference parses, this new update rule increases the execution accuracy of the final system. Intuitively, this approach gathers additional information from all candidate parses with higher execution accuracy when learning the discriminative reranker. In addition, as shown in line 2 of the algorithm above, it uses the difference in execution"
P13-1022,D09-1043,0,0.0623689,"Missing"
P13-1022,P08-1067,0,0.0145674,"rate model.3 ther improve the plan execution performance, and reranking using all of the feature groups (All) performs the best, as expected. However, since our model is optimizing plan execution during training, the results for parse accuracy are always worse than the baseline model. 6 Related Work Discriminative reranking is a common machine learning technique to improve the output of generative models. It has been shown to be effective for various natural language processing tasks including syntactic parsing (Collins, 2000; Collins, 2002b; Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), partof-speech tagging (Collins, 2002a), semantic role labeling (Toutanova et al., 2005), named entity recognition (Collins, 2002c). machine translation (Shen et al., 2004; Fraser and Marcu, 2006) and surface realization in generation (White and Rajkumar, 2009; Konstas and Lapata, 2012). However, to our knowledge, there has been no previous attempt to apply discriminative reranking to grounded language acquisition, where goldstandard reference parses are not typically available for training reranking models. Our use of response-based tr"
P13-1022,D12-1040,1,0.11639,"parse trees. 1 Introduction Grounded language acquisition involves learning to comprehend and/or generate language by simply observing its use in a naturally occurring context in which the meaning of a sentence is grounded in perception and/or action (Roy, 2002; Yu and Ballard, 2004; Gold and Scassellati, 2007; Chen et al., 2010). B¨orschinger et al. (2011) introduced an approach that reduces grounded language learning to unsupervised probabilistic context-free grammar (PCFG) induction and demonstrated its effectiveness on the task of sportscasting simulated robot soccer games. Subsequently, Kim and Mooney (2012) extended their approach to make it tractable for the more complex problem of learning to follow natural-language navigation instructions from observations of humans following such instructions in a virtual environment (Chen and Mooney, 2011). The observed sequence of actions provides very weak, ambiguous supervision for learning instructional language since there are many possible ways to describe the same execution path. Although their approach improved accuracy on the navigation task compared 218 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages"
P13-1022,P12-1039,0,0.0134031,"ommon machine learning technique to improve the output of generative models. It has been shown to be effective for various natural language processing tasks including syntactic parsing (Collins, 2000; Collins, 2002b; Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), partof-speech tagging (Collins, 2002a), semantic role labeling (Toutanova et al., 2005), named entity recognition (Collins, 2002c). machine translation (Shen et al., 2004; Fraser and Marcu, 2006) and surface realization in generation (White and Rajkumar, 2009; Konstas and Lapata, 2012). However, to our knowledge, there has been no previous attempt to apply discriminative reranking to grounded language acquisition, where goldstandard reference parses are not typically available for training reranking models. Our use of response-based training is similar Comparison of different feature groups Table 3 compares reranking results using the different feature groups described in Section 4. Compared to the baseline model (Kim and Mooney, 2012), each of the feature groups Base (base features), Pred (predicate-only and verificationremoved features), and Desc (descended action feature"
P13-1022,P11-1060,0,\N,Missing
P13-1022,D11-1131,0,\N,Missing
P13-1022,P02-1062,0,\N,Missing
P14-1114,S12-1051,0,0.348977,"ine logical and distributional representations of natural-language meaning, where distributional information is represented in the form of weighted inference rules. We apply this framework to the task of Semantic Textual Similarity (STS) (i.e. judging the semantic similarity of naturallanguage sentences), and show that PSL gives improved results compared to a previous approach based on Markov Logic Networks (MLNs) and a purely distributional approach. 1 Introduction When will people say that two sentences are similar? This question is at the heart of the Semantic Textual Similarity task (STS)(Agirre et al., 2012). Certainly, if two sentences contain many of the same words, or many similar words, that is a good indication of sentence similarity. But that can be misleading. A better characterization would be to say that if two sentences use the same or similar words in the same or similar relations, then those two sentences will be judged similar.1 Interestingly, this characterization echoes the principle of compositionality, which states that the meaning of a phrase is uniquely determined by the meaning of its parts and the rules that connect those parts. Beltagy et al. (2013) proposed a hybrid approac"
P14-1114,S12-1059,0,0.0117816,"Missing"
P14-1114,D10-1115,0,0.0390138,"ces into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 dauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously found to be the best performing simple distributional method for STS (Beltagy et al., 2013). Distributional Semantics Distributional models (Turney and Pantel, 2010), on the other hand, use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical represe"
P14-1114,S13-1002,1,0.0702222,"tual Similarity task (STS)(Agirre et al., 2012). Certainly, if two sentences contain many of the same words, or many similar words, that is a good indication of sentence similarity. But that can be misleading. A better characterization would be to say that if two sentences use the same or similar words in the same or similar relations, then those two sentences will be judged similar.1 Interestingly, this characterization echoes the principle of compositionality, which states that the meaning of a phrase is uniquely determined by the meaning of its parts and the rules that connect those parts. Beltagy et al. (2013) proposed a hybrid approach to sentence similarity: They use a very 1 Mitchell and Lapata (2008) give an amusing example of two sentences that consist of all the same words, but are very different in their meaning: (a) It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. (b) That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious. deep representation of sentence meaning, expressed in first-order logic, to capture sentence structure, but combine it with distributional similari"
P14-1114,P11-1062,0,0.0223414,"could learn that the type of an object determined by a noun should be weighted more than a property specified by an adjective. As a result, “black dog” would be appropriately judged more similar to “white dog” than to “black cat.” One of the advantages of using a probabilistic logic is that additional sources of knowledge can easily be incorporated by adding additional soft inference rules. To complement the soft inference rules capturing distributional lexical and phrasal similarities, PSL rules could be added that encode explicit paraphrase rules, such as those mined from monolingual text (Berant et al., 2011) or multi-lingual parallel text (Ganitkevitch et al., 2013). This paper has focused on STS; however, as shown by Beltagy et al. (2013), probabilistic logic is also an effective approach to recognizing textual entailment (RTE). By using the appropriate functions to combine truth values for various logical connectives, PSL could also be adapted for RTE. Although we have shown that PSL outperforms MLNs on STS, we hypothesize that MLNs may still be a better approach for RTE. However, it would be good to experimentally confirm this intuition. In any case, the high computational complexity of MLN in"
P14-1114,D13-1160,0,0.0265957,"p and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, their binary nature prevents them from capturing the “graded” aspects of meaning in language. Also, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and semantically parsing sentences into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 dauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously found to be the best p"
P14-1114,W08-2222,0,0.0929732,"n entailment includes the logical forms for both T and H as well as soft inference rules that are constructed from distributional information. Given a similarity score for all pairs of sentences in the dataset, a regressor is trained on the training set to map the system’s output to the gold standard scores. The trained regressor is applied to the scores in the test set before calculating Pearson correlation. The regression algorithm used is Additive Regression (Friedman, 2002). To determine an entailment probability, first, the two sentences are mapped to logical representations using Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that maps a CCG (Combinatory Categorial Grammar) parse into a lexically-based logical form. Boxer uses C&C for CCG parsing (Clark and Curran, 2004). Distributional semantic knowledge is then encoded as weighted inference rules in the MLN. A rule’s weight (w) is a function of the cosine similarity (sim) between its antecedent and consequent. Rules are generated on the fly for each T and H. Let t and h be the lists of all words and phrases in T and H respectively. For all pairs (a, b), where a ∈ t, b ∈ h, it generates an inference rule: a → b |w, wher"
P14-1114,P11-1020,0,0.0187176,"rkov logic instead of PSL for probabilistic inference. MLN inference is very slow in some cases, so we use a 10 minute timeout. When MLN times out, it backs off to a simpler sentence representation as explained in section 2.6. Evaluation This section evaluates the performance of PSL on the STS task. 4.1 Datasets We evaluate our system on three STS datasets. • PSL: Our proposed PSL system for combining logical and distributional information. • msr-vid: Microsoft Video Paraphrase Corpus from STS 2012. The dataset consists of 1,500 pairs of short video descriptions collected using crowdsourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al., 2012). Half of the dataset is for training, and the second half is for testing. • PSL-no-DIR: Our PSL system without distributional inference rules(empty knowledge base). This system uses PSL to compute similarity of logical forms but does not use distributional information on lexical or phrasal similarity. It tests the impact of the probabilistic logic only • msr-par: Microsoft Paraphrase Corpus from STS 2012 task. The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al., 2004). Then, for STS 2012, 1,500 pai"
P14-1114,P04-1014,0,0.0854417,"rs of sentences in the dataset, a regressor is trained on the training set to map the system’s output to the gold standard scores. The trained regressor is applied to the scores in the test set before calculating Pearson correlation. The regression algorithm used is Additive Regression (Friedman, 2002). To determine an entailment probability, first, the two sentences are mapped to logical representations using Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that maps a CCG (Combinatory Categorial Grammar) parse into a lexically-based logical form. Boxer uses C&C for CCG parsing (Clark and Curran, 2004). Distributional semantic knowledge is then encoded as weighted inference rules in the MLN. A rule’s weight (w) is a function of the cosine similarity (sim) between its antecedent and consequent. Rules are generated on the fly for each T and H. Let t and h be the lists of all words and phrases in T and H respectively. For all pairs (a, b), where a ∈ t, b ∈ h, it generates an inference rule: a → b |w, where w = → − − f (sim(→ a , b )). Both a and b can be words or phrases. Phrases are defined in terms of Boxer’s output. A phrase is more than one unary atom sharing the same variable like “a litt"
P14-1114,C04-1051,0,0.0120882,"llected using crowdsourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al., 2012). Half of the dataset is for training, and the second half is for testing. • PSL-no-DIR: Our PSL system without distributional inference rules(empty knowledge base). This system uses PSL to compute similarity of logical forms but does not use distributional information on lexical or phrasal similarity. It tests the impact of the probabilistic logic only • msr-par: Microsoft Paraphrase Corpus from STS 2012 task. The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al., 2004). Then, for STS 2012, 1,500 pairs were selected and annotated with similarity scores. Half of the dataset is for training, and the second half is for testing. • SICK: Sentences Involving Compositional Knowledge is a dataset collected for SemEval 2014. Only the training set is available at this point, which consists of 5,000 pairs of sentences. Pairs are annotated for RTE and STS, but we only use the STS data. Training and testing was done using 10-fold cross validation. 4.2 Systems Compared We compare our PSL system with several others. In all cases, we use the distributional word vectors empl"
P14-1114,N13-1092,0,0.0538761,"Missing"
P14-1114,W11-0112,1,0.676139,"sets added in the 2013 competition since they did not contain naturally-occurring, full sentences, which is the focus of our work. 1212 2.6 Combining logical and distributional methods using probabilistic logic There are a few recent attempts to combine logical and distributional representations in order to obtain the advantages of both. Lewis and Steedman (2013) use distributional information to determine word senses, but still produce a strictly logical semantic representation that does not address the “graded” nature of linguistic meaning that is important to measuring semantic similarity. Garrette et al. (2011) introduced a framework for combining logic and distributional models using probabilistic logic. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical representation, and Markov Logic Networks are used to perform probabilistic logical inference. Beltagy et al. (2013) extended this framework by generating distributional inference rules from phrase similarity and tailoring the system to the STS task. STS is treated as computing the probability of two textual entailments T |= H and H |= T , where T and H are the two sentences who"
P14-1114,D11-1129,0,0.0746619,"utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 dauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously found to be the best performing simple distributional method for STS (Beltagy et al., 2013). Distributional Semantics Distributional models (Turney and Pantel, 2010), on the other hand, use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical representations, automatically acquire kno"
P14-1114,D13-1161,0,0.0148238,"ition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, their binary nature prevents them from capturing the “graded” aspects of meaning in language. Also, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and semantically parsing sentences into logical expressions utilizing such an ontology is very difficult. Consequently, current semantic parsers are mostly restricted to quite limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). In contrast, our system is not limited to any formal ontology and can use a wide-coverage tool for semantic analysis, as discussed below. 2.2 dauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010), or more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). We use vector addition (Landauer and Dumais, 1997), and component-wise product (Mitchell and Lapata, 2008) as baselines for STS. Vector addition was previously"
P14-1114,Q13-1015,0,0.051073,"system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics. Two of the datasets we use for evaluation are from the 2012 competition. We did not utilize the new datasets added in the 2013 competition since they did not contain naturally-occurring, full sentences, which is the focus of our work. 1212 2.6 Combining logical and distributional methods using probabilistic logic There are a few recent attempts to combine logical and distributional representations in order to obtain the advantages of both. Lewis and Steedman (2013) use distributional information to determine word senses, but still produce a strictly logical semantic representation that does not address the “graded” nature of linguistic meaning that is important to measuring semantic similarity. Garrette et al. (2011) introduced a framework for combining logic and distributional models using probabilistic logic. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical representation, and Markov Logic Networks are used to perform probabilistic logical inference. Beltagy et al. (2013) extende"
P14-1114,P08-1028,0,0.735442,"f the same words, or many similar words, that is a good indication of sentence similarity. But that can be misleading. A better characterization would be to say that if two sentences use the same or similar words in the same or similar relations, then those two sentences will be judged similar.1 Interestingly, this characterization echoes the principle of compositionality, which states that the meaning of a phrase is uniquely determined by the meaning of its parts and the rules that connect those parts. Beltagy et al. (2013) proposed a hybrid approach to sentence similarity: They use a very 1 Mitchell and Lapata (2008) give an amusing example of two sentences that consist of all the same words, but are very different in their meaning: (a) It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. (b) That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious. deep representation of sentence meaning, expressed in first-order logic, to capture sentence structure, but combine it with distributional similarity ratings at the word and phrase level. Sentence similarity is then modelled as mutual entailme"
P14-1114,S13-1001,0,0.374618,"S. Vector addition was previously found to be the best performing simple distributional method for STS (Beltagy et al., 2013). Distributional Semantics Distributional models (Turney and Pantel, 2010), on the other hand, use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010). They are relatively easier to build than logical representations, automatically acquire knowledge from “big data,” and capture the “graded” nature of linguistic meaning, but do not adequately capture logical structure (Grefenstette, 2013). Distributional models are motivated by the observation that semantically similar words occur in similar contexts, so words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Such models have also been extended to compute vector representations for larger phrases, e.g. by adding the vectors for the individual words (LanMarkov Logic Networks Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic"
P15-1018,W99-0623,0,0.145217,"t. We demonstrate that our stacking approach outperforms the best system from the 2014 KBPESF competition as well as alternative ensembling methods employed in the 2014 KBP Slot Filler Validation task and several other ensembling baselines. Additionally, we demonstrate that including provenance information further increases the performance of stacking. 1 Introduction Using ensembles of multiple systems is a standard approach to improving accuracy in machine learning (Dietterich, 2000). Ensembles have been applied to a wide variety of problems in natural language processing, including parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), and sentiment analysis (Whitehead and Yaeger, 2010). This paper presents a detailed study of ensembling methods for the TAC Knowledge Base Population (KBP) English Slot Filling (ESF) task (Surdeanu, 2013; Surdeanu and Ji, 2014). We demonstrate new state-of-the-art results on this KBP task using stacking (Wolpert, 1992), which trains a final classifier to optimally combine the results of multiple systems. We present results for stacking all systems that competed in both the 2013 and 2014 KBP-ESF tracks, training ∗ These authors contributed equally 1"
P15-1018,P09-1113,0,0.0396041,"ous other tasks as a part of the Text Analysis Conference(TAC)(Surdeanu, 2013; Surdeanu and Ji, 2014). In the ESF task, the goal is to fill specific slots of information for a given set of query entities (people or organizations) based on a supplied text corpus. The participating systems employ a variety of techniques in different stages of the slot filling pipeline, such as entity search, relevant document extraction, relation modeling and inference. In 2014, the top performing system, DeepDive with Expert Advice from Stanford University (Wazalwar et al., 2014), employed distant supervision (Mintz et al., 2009) and Markov Logic Networks (Domingos et al., 2008) in their learning and inferencing system. Another system, RPI BLENDER (Hong et al., 2014), used a restricted fuzzy matching technique in a framework that learned event triggers and employed them to extract relations from documents. Given the diverse set of slot-filling systems available, it is interesting to explore methods for ensembling these systems. In this regard, TAC also conducts a Slot Filler Validation (SFV) task who goal is to improve the slot-filling performance using the output of existing systems. The input for this task is the se"
P15-1018,C14-1149,0,0.21848,"om documents. Given the diverse set of slot-filling systems available, it is interesting to explore methods for ensembling these systems. In this regard, TAC also conducts a Slot Filler Validation (SFV) task who goal is to improve the slot-filling performance using the output of existing systems. The input for this task is the set of outputs from all slotfilling systems and the expected output is a filtered set of slot fills. As with the ESF task, participating systems employ a variety of techniques to perform validation. For instance, RPI BLENDER used a Multi-dimensional Truth Finding model (Yu et al., 2014) which is an unsupervised validation approach based on computing multidimensional credibility scores. The UI CCG system (Sammons et al., 2014) developed two different validation systems using entailment and majority voting. However, stacking (Sigletos et al., 2005; Wolpert, 1992) has not previously been employed for ensembling KBP-ESF systems. In stacking, a meta-classifier is learned from the output of multiple underlying systems. In our work, we translate this to the context of ensembling slot filling sysOverview of KBP Slot Filling Task The goal of the TAC KBP-ESF task (Surdeanu, 2013; Surd"
P15-1018,P08-1004,0,\N,Missing
P15-1018,A00-2009,0,\N,Missing
P15-1085,W99-0604,0,\N,Missing
P15-1085,N10-1083,0,\N,Missing
P15-1085,N07-1022,1,\N,Missing
P15-1085,N06-1056,1,\N,Missing
P15-1085,P07-1121,1,\N,Missing
P15-1085,P13-2009,0,\N,Missing
P15-1085,P07-2045,0,\N,Missing
P15-1085,P06-1115,1,\N,Missing
P15-1085,P05-1033,0,\N,Missing
P15-1085,N03-1017,0,\N,Missing
P15-1085,J97-3002,0,\N,Missing
P15-1085,P06-1121,1,\N,Missing
P15-1085,D13-1160,0,\N,Missing
P15-1085,P09-1010,0,\N,Missing
P16-1027,P82-1020,0,0.830643,"Missing"
P16-1027,D13-1178,0,0.0680781,"iplication. 2.3 Narrative Cloze Evaluation Sentence-Level RNN Language Models RNN sequence models have recently been shown to be extremely effective for word-level and character-level language models (Mikolov et al., 2011; Jozefowicz et al., 2016). At each timestep, these models take a word or character as input, update a hidden state vector, and predict the next 281 al., 2015), a simplex verb (Chambers and Jurafsky, 2009; Orr et al., 2014), or a verb with multiple arguments (Pichotta and Mooney, 2014). In the present work, we adopt a representation of events as verbs with multiple arguments (Balasubramanian et al., 2013; Pichotta and Mooney, 2014; Modi and Titov, 2014). Formally, we define an event to be a variadic tuple (v, s, o, p∗ ), where v is a verb, s is a noun standing in subject relation to v, o is a noun standing as a direct object to v, and p∗ denotes an arbitrary number of (pobj, prep) pairs, with prep a preposition and pobj a noun related to the verb v via the preposition prep.1 Any argument except v may be null, indicating no noun fills that slot. For example, the text verbs with coreference information about multiple arguments; and Pichotta and Mooney (2016) evaluate inference of verbs with nou"
P16-1027,E12-1034,0,0.0610312,"Missing"
P16-1027,P08-1090,0,0.0716846,"or Script Inference Karl Pichotta Department of Computer Science The University of Texas at Austin pichotta@cs.utexas.edu Raymond J. Mooney Department of Computer Science The University of Texas at Austin mooney@cs.utexas.edu Abstract learned representation of the previous sentences using no linguistic preprocessing. Some prior statistical script learning systems are focused on knowledge induction. These systems are primarily designed to induce collections of co-occurring event types involving the same entities, and their ability to infer held-out events is not their primary intended purpose (Chambers and Jurafsky, 2008; Ferraro and Van Durme, 2016, inter alia). In the present work, we instead investigate the behavior of systems trained to directly optimize performance on the task of predicting subsequent events; in other words, we are investigating statistical models of events in discourse. Much prior research on statistical script learning has also evaluated on inferring missing events from documents. However, the exact form that this task takes depends on the adopted definition of what constitutes an event: in previous work, events are defined in different ways, with differing degrees of structure. We con"
P16-1027,P09-1068,0,0.0865584,"tasks (Sutskever et al., 2014; Hermann et al., 2015; Vinyals et al., 2015, inter alia). The LSTM we use is described by: it = σ (Wx,i xt + Wz,i zt−1 + bi ) gt = tanh (Wx,m xt + Wz,m zt−1 + bg ) 3.1 mt = ft ◦ mt−1 + it ◦ gt The evaluation of inference-focused statistical script systems is not straightforward. Chambers and Jurafsky (2008) introduced the Narrative Cloze evaluation, in which a single event is held out from a document and systems are judged by the ability to infer this held-out event given the remaining events. This evaluation has been used by a number of published script systems (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015). This automated evaluation measures systems’ ability to model and predict events as they co-occur in text. The exact definition of the Narrative Cloze evaluation depends on the formulation of events used in a script system. For example, Chambers and Jurafsky (2008), Jans et al. (2012), and Rudinger et al. (2015) evaluate inference of heldout (verb, dependency) pairs from documents; Pichotta and Mooney (2014) evaluate inference of zt = ot ◦ tanh mt . The model is depicted graphically in Figure 1. The memory vector mt is a fu"
P16-1027,D13-1185,0,0.025706,"2015) apply a discriminative language model to the (verb, dependency) sequence modeling task, raising the question of to what extent event inference can be performed with standard language models applied to event sequences. Pichotta and Mooney (2014) describe a method of learning a co-occurrence based model of verbs with multiple coreference-based entity arguments. 7 There is a body of related work focused on learning models of co-occurring events to automatically induce templates of complex events comprising multiple verbs and arguments, aimed ultimately at maximizing coherency of templates (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013). Ferraro and Van Durme (2016) give a model integrating various levels of event information of increasing abstraction, evaluating both on coherence of induced templates and log-likelihood of predictions of held-out events. McIntyre and Lapata (2010) describe a system that learns a model of co-occurring events and uses this model to automatically generate stories via a Genetic Algorithm. Conclusion We have given what we believe to be the first systematic evaluation of sentence-level RNN language models on the task of predicting held-out docume"
P16-1027,N13-1104,0,0.186716,"scriminative language model to the (verb, dependency) sequence modeling task, raising the question of to what extent event inference can be performed with standard language models applied to event sequences. Pichotta and Mooney (2014) describe a method of learning a co-occurrence based model of verbs with multiple coreference-based entity arguments. 7 There is a body of related work focused on learning models of co-occurring events to automatically induce templates of complex events comprising multiple verbs and arguments, aimed ultimately at maximizing coherency of templates (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013). Ferraro and Van Durme (2016) give a model integrating various levels of event information of increasing abstraction, evaluating both on coherence of induced templates and log-likelihood of predictions of held-out events. McIntyre and Lapata (2010) describe a system that learns a model of co-occurring events and uses this model to automatically generate stories via a Genetic Algorithm. Conclusion We have given what we believe to be the first systematic evaluation of sentence-level RNN language models on the task of predicting held-out document text. We have foun"
P16-1027,P07-2045,0,0.0202731,"Missing"
P16-1027,de-marneffe-etal-2006-generating,0,0.0380451,"Missing"
P16-1027,D14-1220,0,0.0182131,"call for his capture and death. hOOVi agrees . hOOVi is killed by the hOOVi. Figure 4: Sample next-sentence text predictions. hOOVi is the out-of-vocabulary pseudo-token, which frequently replaces proper names. ral network which composes verbs and arguments into low-dimensional vectors, evaluating on a multiple-choice version of the Narrative Cloze task. Modi and Titov (2014) describe a feedforward network which is trained to predict event orderings. Kiros et al. (2015) give a method of embedding sentences in low-dimensional space such that embeddings are predictive of neighboring sentences. Li et al. (2014) and Ji and Eisenstein (2015), use RNNs for discourse parsing; Liu et al. (2016) use a Convolutional Neural Network for implicit discourse relation classification. pairs from documents; Jans et al. (2012) give a superior model in the same general framework. Chambers and Jurafsky (2009) give a method of generalizing from single sequences of pair events to collections of such sequences. Rudinger et al. (2015) apply a discriminative language model to the (verb, dependency) sequence modeling task, raising the question of to what extent event inference can be performed with standard language models"
P16-1027,P13-1045,0,0.0471145,"Missing"
P16-1027,P10-1158,0,0.0125467,"urrence based model of verbs with multiple coreference-based entity arguments. 7 There is a body of related work focused on learning models of co-occurring events to automatically induce templates of complex events comprising multiple verbs and arguments, aimed ultimately at maximizing coherency of templates (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013). Ferraro and Van Durme (2016) give a model integrating various levels of event information of increasing abstraction, evaluating both on coherence of induced templates and log-likelihood of predictions of held-out events. McIntyre and Lapata (2010) describe a system that learns a model of co-occurring events and uses this model to automatically generate stories via a Genetic Algorithm. Conclusion We have given what we believe to be the first systematic evaluation of sentence-level RNN language models on the task of predicting held-out document text. We have found that models operating on raw text perform roughly comparably to identical models operating on predicate-argument event structures when predicting the latter, and that text models provide superior predictions of raw text. This provides evidence that, for the task of held-out eve"
P16-1027,W14-1606,0,0.112562,"RNN Language Models RNN sequence models have recently been shown to be extremely effective for word-level and character-level language models (Mikolov et al., 2011; Jozefowicz et al., 2016). At each timestep, these models take a word or character as input, update a hidden state vector, and predict the next 281 al., 2015), a simplex verb (Chambers and Jurafsky, 2009; Orr et al., 2014), or a verb with multiple arguments (Pichotta and Mooney, 2014). In the present work, we adopt a representation of events as verbs with multiple arguments (Balasubramanian et al., 2013; Pichotta and Mooney, 2014; Modi and Titov, 2014). Formally, we define an event to be a variadic tuple (v, s, o, p∗ ), where v is a verb, s is a noun standing in subject relation to v, o is a noun standing as a direct object to v, and p∗ denotes an arbitrary number of (pobj, prep) pairs, with prep a preposition and pobj a noun related to the verb v via the preposition prep.1 Any argument except v may be null, indicating no noun fills that slot. For example, the text verbs with coreference information about multiple arguments; and Pichotta and Mooney (2016) evaluate inference of verbs with noun information about multiple arguments. In order t"
P16-1027,P02-1040,0,0.0973018,"nference quality, the latter also learn an encoder-decoder LSTM network for transforming verbs and noun arguments into English text to present to annotators for evaluation. We evaluate instead on the task of directly inferring sequences of words. That is, instead of defining the Narrative Cloze to be the evaluation of predictions of held-out events, we define the task to be the evaluation of predictions of held-out text; in this setup, predictions need not be mediated by noisy, automatically-extracted events. To evaluate inferred text against gold standard text, we argue that the BLEU metric (Papineni et al., 2002), commonly used to evaluate Statistical Machine Translation systems, is a natural evaluation metric. It is an n-gram-level analog to the eventlevel Narrative Cloze evaluation: whereas the Narrative Cloze evaluates a system on its ability to reconstruct events as they occur in documents, BLEU evaluates a system on how well it reconstructs the n-grams. This evaluation takes some inspiration from the evaluation of neural encoder-decoder translation models (Sutskever et al., 2014; Bahdanau et al., 2015), which use similar architectures for the task of Machine Translation. That is, the task we pres"
P16-1027,E14-1024,1,0.765925,"call into question the extent to which statistical event inference systems require linguistic preprocessing and syntactic structure. In an attempt to shed light on this issue, we compare existing script models to LSTMs trained as sentencelevel language models which try to predict the sequence of words in the next sentence from a 279 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 279–289, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics in modeling events in documents (Jans et al., 2012; Rudinger et al., 2015). Pichotta and Mooney (2014) give a cooccurrence based script system that models and infers more complex multi-argument events from text. For example, in the above example, their model would ideally be able to infer a single event like accept(Wiles, prize), as opposed to the two simpler pairs from which it is composed. They provide evidence that modeling and inferring more complex multi-argument events also yields superior performance on the task of inferring simpler (verb, dependency) pair events. These events are constructed using only coreference information; that is, the learned event co-occurrence models do not dire"
P16-1027,D15-1195,0,0.309513,"Missing"
P16-1027,Q15-1024,0,\N,Missing
P19-1348,D16-1044,0,0.206634,"Missing"
P19-1348,D18-1164,0,0.0663088,"e-trained CNN, and then trains an RNN to encode the question, using an attention mechanism to focus on specific features of the image. Finally, both question and attended image features are used to predict the final answer. However, answering visual questions requires not only information about the visual content but also common knowledge, which is usually too hard to directly learn from only a limited number of images with human annotated answers as supervision. However, comparatively little previous VQA research has worked on enriching the knowledge base. We are aware of two related papers. Li et al. (2018a) use a pre-trained captioner to generate general captions and attributes with a fixed annotator and then use them to predict answers. Therefore, the captions they generate are not necessarily relevant to the question, and they may ignore image features needed for answer prediction. Narasimhan et al. (2018) employed an out-of-thebox knowledge base and trained their model to filter out irrelevant facts. After that, graph convolutional networks use this knowledge to build connections to the relevant facts and predict the final answer. Unlike them, we generate captions to provide information tha"
P19-1348,D14-1162,0,0.0823161,"ls We train our joint model using the AdaMax optimizer (Kingma and Ba, 2015) with a batch size of 384 and a learning rate of 0.002 as suggested by Teney et al. (2017). We use the validation set for VQA v2 to tune the initial learning rate and the number of epochs, yielding the highest overall VQA score. We use 1, 280 hidden units in the question embedding and attention model in the VQA module with 36 object detection features for each image. For captioning models, the dimension of the LSTM hidden state, image feature embedding, and word embedding are all set to 512. We also use Glove vectors (Pennington et al., 2014) to initialize the word embedding matrix in the caption embedding module. We initialize the training process with human annotated captions from the COCO dataset (Chen et al., 2015) and pre-train the VQA and captiongeneration modules for 20 epochs with the final joint loss in Eq. 16. After that, we generate question-relevant captions for all question-image pairs in the COCO train, validation, and test sets. In particular, we sample 5 captions per questionimage pair. We fine-tune our model using the generated captions with 0.25 ⇥ learning-rate for another 10 epochs. 4 Experiments We perform exte"
P97-1062,P92-1024,0,0.0894746,"Missing"
P97-1062,P96-1025,0,0.00608198,"iance on contextual information. Going beyond Magerman&apos;s still relatively rigid set of 36 features, we propose a yet richer, basically unlimited feature language set. Our parse action sequences are too complex to be derived from a treebank like Penn&apos;s. Not only do our parse trees contain semantic annotations, roles and more syntactic detail, we also rely on the more informative parse action sequence. While this necessitates the involvement of a parsing supervisor for training, we are able to perform deterministic parsing and get already very good test results for only 256 training sentences. (Collins, 1996) focuses on b i g r a m lexical dependencies (BLD). Trained on the same 40,000 sentences as Spatter, it relies on a much more limited type of context than our system and needs little background knowledge. Model Labeled precision Labeled recall Crossings/sentence Sent. with 0 cr. Sent. with < 2 cr. While many limited-context statistical approaches have already reached a performance ceiling, we still expect to significantly improve our results when increasing our training base beyond the currently 256 sentences, because the learning curve hasn&apos;t flattened out yet and adding substantially more ex"
P97-1062,P97-1062,1,0.106171,"Missing"
P97-1062,P95-1037,0,0.200735,"Missing"
P97-1062,J93-2004,0,0.0628766,"lated Work Our basic parsing and interactive training paradigm is based on (Simmons and Yu, 1992). We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component. (Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER. with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993). Questioning the traditional n-grams, Magerman already advocates a heavier reliance on contextual information. Going beyond Magerman&apos;s still relatively rigid set of 36 features, we propose a yet richer, basically unlimited feature language set. Our parse action sequences are too complex to be derived from a treebank like Penn&apos;s. Not only do our parse trees contain semantic annotations, roles and more syntactic detail, we also rely on the more informative parse action sequence. While this necessitates the involvement of a parsing supervisor for training, we are able to perform deterministic pa"
P97-1062,J92-4001,0,0.268535,"perations -0.45 Operation sequence -0.39 Semantics -0.63 -0.66 -0.78 -0.65 -0.56 0.54 -0.41 -0.36 Table 5: Correlation between various parse and translation metrics. Values near -1.0 or 1.0 indicate very strong correlation, whereas values near 0.0 indicate a weak or no correlation. Most correlation values, incl. for labeled precision are negative, because a higher (better) labeled precision correlates with a numerically lower (better) translation score on the 1.0 (best) to 6.0 (worst) translation evaluation scale. 7 Related Work Our basic parsing and interactive training paradigm is based on (Simmons and Yu, 1992). We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component. (Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER. with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993). Questioning the traditional n-grams, Magerman already advocates a heavier r"
P97-1062,J96-1001,0,0.0228454,"Missing"
P97-1062,W96-0102,0,\N,Missing
P97-1062,J93-2006,0,\N,Missing
P97-1062,W96-0208,1,\N,Missing
P97-1062,W96-0211,0,\N,Missing
P97-1062,J90-2002,0,\N,Missing
P97-1062,J95-4004,0,\N,Missing
P97-1062,P93-1032,0,\N,Missing
P97-1062,P95-1017,0,\N,Missing
P97-1062,P91-1023,0,\N,Missing
P97-1062,J93-1006,0,\N,Missing
P97-1062,P94-1013,0,\N,Missing
P97-1062,P96-1006,0,\N,Missing
P97-1062,P93-1005,0,\N,Missing
P97-1062,H94-1028,0,\N,Missing
P97-1062,P96-1024,0,\N,Missing
S13-1002,S12-1051,0,0.0454478,"fidential document Here, h is not entailed. RTE directly tests whether a system can construct semantic representations that allow it to draw correct inferences. Of existing RTE approaches, the closest to ours is by Bos and Markert (2005), who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. By contrast, our approach uses Markov logic with probabilistic inference. Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations. The best performer in 2012’s competition was by B¨ar et al. (2012), an ensemble system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics. Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of Hobbs et al. (1993), who proposed viewing natural language interpretation as abductive inference. In this framework, problems like reference resolution and syntactic amb"
S13-1002,S12-1059,0,0.0221299,"Missing"
S13-1002,D10-1115,0,0.448308,"the similarity of vectors representing the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly"
S13-1002,P11-1062,0,0.0850071,"e make use of existing approaches that compute distributional representations for phrases. In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition (Landauer and Dumais, 1997) or component-wise multiplication (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). The inference-rule weight, wt(p1 , p2 ), for two phrases p1 and p2 is then determined using Eq. (5) in the same way as for words. Existing approaches that derive phrasal inference rules from distributional similarity (Lin and Pantel, 2001a; Szpektor and Dagan, 2008; Berant et al., 2011) precompile large lists of inference rules. By comparison, distributional phrase similarity can be seen as a generator of inference rules “on the fly”, as it is possible to compute distributional phrase vectors for arbitrary phrases on demand as they are needed for particular examples. Inference rules are generated for all pairs of constituents (c1 , c2 ) where c1 ∈ S1 and c2 ∈ S2 , a constituent is a single word or a phrase. The logical form provides a handy way to extract phrases, as they are generally mapped to one of two logical constructs. Either we have multiple single-variable predicate"
S13-1002,H05-1079,0,0.851584,"omputing the probability of a query literal given a set of weighted clauses as background knowledge and evidence. Tasks: RTE and STS Recognizing Textual Entailment (RTE) is the task of determining whether one natural language text, the premise, implies another, the hypothesis. Consider (1) below. (1) p: Oracle had fought to keep the forms from being released h: Oracle released a confidential document Here, h is not entailed. RTE directly tests whether a system can construct semantic representations that allow it to draw correct inferences. Of existing RTE approaches, the closest to ours is by Bos and Markert (2005), who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. By contrast, our approach uses Markov logic with probabilistic inference. Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations. The best performer in 2012’s competition was by B¨ar et al. (2012), an ensemble system that integrates many techniques including string similarity,"
S13-1002,W08-2222,0,0.249599,"nd Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models. MLNs are well suited for our app"
S13-1002,W11-2504,0,0.0394132,"Missing"
S13-1002,P11-1020,0,0.0148009,"demonstrates the advantage of using a model that operationalizes entailment between words and phrases as distributional similarity. 5 On other RTE datasets there are higher previous results. Hickl (2008) achieves 0.89 accuracy and 0.88 cws on the combined RTE-2 and RTE-3 dataset. 17 5 5.1 Task 2: Semantic Textual Similarity Dataset The dataset we use in our experiments is the MSR Video Paraphrase Corpus (MSR-Vid) subset of the STS 2012 task, consisting of 1,500 sentence pairs. The corpus itself was built by asking annotators from Amazon Mechanical Turk to describe very short video fragments (Chen and Dolan, 2011). The organizers of the STS 2012 task (Agirre et al., 2012) sampled video descriptions and asked Turkers to assign similarity scores (ranging from 0 to 5) to pairs of sentences, without access to the video. The gold standard score is the average of the Turkers’ annotations. In addition to the MSR Video Paraphrase Corpus subset, the STS 2012 task involved data from machine translation and sense descriptions. We do not use these because they do not consist of full grammatical sentences, which the parser does not handle well. In addition, the STS 2012 data included sentences from the MSR Paraphra"
S13-1002,P04-1014,0,0.100456,"(Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models. MLNs are well suited for our approach since they provide an elegant framework for assigning weights to first-order logical rules, combining a diverse set of inference rules and performing sound probabilistic inference. An MLN consists of a s"
S13-1002,J12-1002,0,0.00930197,"s very sensitive to parsing errors, and the C&C parser, on which Boxer is based, produces many errors on this dataset, even for simple sentences. When the C&C CCG parse is wrong, the resulting logical form is wrong, and the resulting similarity score is greatly affected. Dropping variable binding makes the systems more robust to parsing errors. Second, in contrast to RTE, the STS dataset does not really test the important role of syntax and logical form in deter18 Future Work rectional similarity. We plan to incorporate directed similarity measures such as those of Kotlerman et al. (2010) and Clarke (2012). A primary problem for our approach is the limitations of existing MLN inference algorithms, which do not effectively scale to large and complex MLNs. We plan to explore “coarser” logical representations such as Minimal Recursion Semantics (MRS) (Copestake et al., 2005). Another potential approach to this problem is to trade expressivity for efficiency. Domingos and Webb (2012) introduced a tractable subset of Markov Logic (TML) for which a future software release is planned. Formulating the inference problem in TML could potentially allow us to run our system on longer and more complex sente"
S13-1002,D08-1094,1,0.834937,"Missing"
S13-1002,W11-0112,1,0.929827,"970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional models (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Sch¨utze, 1998; Erk and Pad´o, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated in Figure 1. Finally, Markov Logic Networks (Richardson and Domingos, 2006) (MLNs) are used to perform weighted inference on the resulting knowledge base. However, they only employed single-word distributional similarity rules, and only ev"
S13-1002,D11-1129,0,0.45908,"based representations (Montague, 1970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional models (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Sch¨utze, 1998; Erk and Pad´o, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated in Figure 1. Finally, Markov Logic Networks (Richardson and Domingos, 2006) (MLNs) are used to perform weighted inference on the resulting knowledge base. However, they only employed single-word distributional simil"
S13-1002,C08-1043,0,0.0498596,"Missing"
S13-1002,P08-1028,0,0.223342,"to handle both RTE and STS, we do not know of any other methods that have been explicitly tested on both problems. 2 Related work Distributional semantics Distributional models define the semantic relatedness of words as the similarity of vectors representing the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we d"
S13-1002,P12-1052,0,0.0289542,"vely combining logical representations with distributional information automatically acquired from text. In this section, we discuss some of limitations of the current work and directions for future research. As noted before, parse errors are currently a significant problem. We use Boxer to obtain a logical representation for a sentence, which in turn relies on the C&C parser. Unfortunately, C&C misparses many sentences, which leads to inaccurate logical forms. To reduce the impact of misparsing, we plan to use a version of C&C that can produce the top-n parses together with parse re-ranking (Ng and Curran, 2012). As an alternative to re-ranking, one could obtain logical forms for each of the topn parses, and create an MLN that integrates all of them (together with their certainty) as an underspecified meaning representation that could then be used to directly support inferences such as STS and RTE. We also plan to exploit a greater variety of distributional inference rules. First, we intend to incorporate logical form translations of existing distributional inference rule collections (e.g., (Berant et al., 2011; Chan et al., 2011)). Another issue is obtaining improved rule weights based on distributi"
S13-1002,J98-1004,0,0.284282,"Missing"
S13-1002,C08-1107,0,0.309922,"y describe similar entities and thus there is some degree of entailment between them. At the sentence level, however, we hold that a stricter, logic-based view of entailment is beneficial, and we even model sentence similarity (in STS) as entailment. There are two main innovations in the formalism that make it possible for us to work with naturally occurring corpus data. First, we use more expressive distributional inference rules based on the similarity of phrases rather than just individual words. In comparison to existing methods for creating textual inference rules (Lin and Pantel, 2001b; Szpektor and Dagan, 2008), these rules are computed on the fly as needed, rather than pre-compiled. Second, we use more flexible probabilistic combinations of evidence in order to compute degrees of sentence similarity for STS and to help compensate for parser errors. We replace deterministic conjunction by an average combiner, which encodes causal independence (Natarajan et al., 2010). We show that our framework is able to handle both sentence similarity (STS) and textual entailment (RTE) by making some simple adaptations to the MLN when switching between tasks. The framework achieves reasonable results on both tasks"
S13-1002,P10-1097,0,0.040878,"Missing"
S14-2141,S12-1051,0,0.0737721,"Explanation) finds the overall interpretation with the maximum probability given a set of evidence. This optimization problem is a second-order cone program (SOCP) (Kimmig et al., 2012) and can be solved in polynomial time. 2.5 Recognizing Textual Entailment Recognizing Textual Entailment (RTE) is the task of determining whether one natural language text, the premise, Entails, Contradicts, or is not related (Neutral) to another, the hypothesis. 2.6 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on a scale from 1 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations and systems are evaluated using the Pearson correlation between a system’s output and gold standard scores. 3 3.1 Distributional Representation Approach Logical Representation The first component in the system is Boxer (Bos, 2008), which maps the input sentences into logical 797 phrases using vector addition across the component predicates. We also tried computing phrase vectors using component-wise vector multiplication (Mitchell and Lapata, 2010), but found it performed marginally worse than addition. 3.3 A general problem w"
S14-2141,W11-2501,0,0.029055,"n inference rule: a → b |w, where the rule weight w is → − − a function of sim(→ a , b ), and sim is a similarity → − − measure of the distributional vectors → a , b . We experimented with the symmetric similarity measure cosine, and asym, the supervised, asymmetric similarity measure of Roller et al. (2014). The asym measure uses the vector difference → − → − ( a − b ) as features in a logistic regression classifier for distinguishing between four different word relations: hypernymy, cohyponymy, meronomy, and no relation. The model is trained using the noun-noun subset of the BLESS data set (Baroni and Lenci, 2011). The final similarity weight is given by the model’s estimated probability that the word relationship is either hypernymy → − − or meronomy: asym(→ a , b ) = P (hyper(a, b))+ P (mero(a, b)). Distributional representations for words are derived by counting co-occurrences in the ukWaC, WaCkypedia, BNC and Gigaword corpora. We use the 2000 most frequent content words as basis dimensions, and count co-occurrences within a two word context window. The vector space is weighted using Positive Pointwise Mutual Information. Phrases are defined in terms of Boxer’s output to be more than one unary atom"
S14-2141,D10-1115,0,0.0521769,"d” aspects of meaning in language because they are binary by nature. 2.2 Distributional Semantics Distributional models use statistics of word cooccurrences to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on dist"
S14-2141,S13-1002,1,0.910574,"Missing"
S14-2141,P14-1114,1,0.903726,"re of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for prob"
S14-2141,W14-2402,1,0.661264,"re of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for prob"
S14-2141,W08-2222,0,0.399373,"troduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactl"
S14-2141,N13-1092,0,0.182293,"Missing"
S14-2141,S13-1001,0,0.0323607,"rated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Box"
S14-2141,C14-1097,1,0.904999,"rvation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system b"
S14-2141,S12-1012,0,0.024692,"2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical i"
S14-2141,S14-2001,0,0.0930159,"r RTE system was extremely conservative: we rarely confused the Entails and Contradicts classes, indicating we correctly predict the direction of entailment, but frequently misclassify examples as Neutral. An examination of these examples showed the errors were mostly due to missing or weakly-weighted distributional rules. On STS, our vector space baseline outperforms both PSL-based systems, but the ensemble outperforms any of its components. This is a testament to Evaluation The dataset used for evaluation is SICK: Sentences Involving Compositional Knowledge dataset, a task for SemEval 2014 (Marelli et al., 2014a; Marelli et al., 2014b). The dataset is 10,000 pairs of sentences, 5000 training and 5000 for testing. Sentences are annotated for both tasks. 799 the power of distributional models in their ability to predict word and sentence similarity. Surprisingly, we see that the PSL + Asym system slightly outperforms the PSL + Cosine system. This may indicate that even in STS, some notion of asymmetry plays a role, or that annotators may have been biased by simultaneously annotating both tasks. As with RTE, the major bottleneck of our system appears to be the knowledge base, which is built solely usin"
S14-2141,marelli-etal-2014-sick,0,0.0679359,"r RTE system was extremely conservative: we rarely confused the Entails and Contradicts classes, indicating we correctly predict the direction of entailment, but frequently misclassify examples as Neutral. An examination of these examples showed the errors were mostly due to missing or weakly-weighted distributional rules. On STS, our vector space baseline outperforms both PSL-based systems, but the ensemble outperforms any of its components. This is a testament to Evaluation The dataset used for evaluation is SICK: Sentences Involving Compositional Knowledge dataset, a task for SemEval 2014 (Marelli et al., 2014a; Marelli et al., 2014b). The dataset is 10,000 pairs of sentences, 5000 training and 5000 for testing. Sentences are annotated for both tasks. 799 the power of distributional models in their ability to predict word and sentence similarity. Surprisingly, we see that the PSL + Asym system slightly outperforms the PSL + Cosine system. This may indicate that even in STS, some notion of asymmetry plays a role, or that annotators may have been biased by simultaneously annotating both tasks. As with RTE, the major bottleneck of our system appears to be the knowledge base, which is built solely usin"
S14-2141,P08-1028,0,0.0432548,", and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. 2.2 Distributional Semantics Distributional models use statistics of word cooccurrences to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability"
W00-1317,P96-1008,0,0.0662218,") approaches to constructing database interfaces require an expert to hand-craft an appropriate semantic parser (Woods, 1970; Hendrix et al., 1978). However, such hand-crafted parsers are time consllming to develop and suffer from problems with robustness and incompleteness even for domain specific applications. Nevertheless, very little research in empirical NLP has explored the task of automatically acquiring such interfaces from annotated training examples. The only exceptions of which we are aware axe a statistical approach to map133 ping airline-information queries into SQL presented in (Miller et al., 1996), a probabilistic decision-tree method for the same task described in (Kuhn and De Mori, 1995), and an approach using relational learning (a.k.a. inductive logic programming, ILP) to learn a logic-based semantic parser described in (Zelle and Mooney, 1996). The existing empirical systems for this task employ either a purely logical or purely statistical approach. The former uses a deterministic parser, which can suffer from some of the same robustness problems as rationalist methods. The latter constructs a probabilistic grammar, which requires supplying a sytactic parse tree as well as a sema"
W05-0602,P97-1003,0,0.54661,"Prolog-based language used in a previously-developed corpus of queries to a database on U.S. geography (Zelle and Mooney, 1996). The second MRL is a coaching language for robotic soccer developed for the RoboCup Coach Competition, in which AI researchers compete to provide effective instructions to a coachable team of agents in a simulated soccer domain (et al., 2003). We present an approach based on a statistical parser that generates a semantically augmented parse tree (SAPT), in which each internal node includes both a syntactic and semantic label. We augment Collins’ head-driven model 2 (Collins, 1997) to incorporate a semantic label on each internal node. By integrating syntactic and semantic interpretation into a single statistical model and finding the globally most likely parse, an accurate combined syntactic/semantic analysis can be obtained. Once a SAPT is generated, an additional step is required to translate it into a final formal meaning representation (MR). Our approach is implemented in a system called S CISSOR (Semantic Composition that Integrates Syntax and Semantics to get Optimal Representations). Training the system requires sentences annotated with both gold-standard SAPT’s"
W05-0602,J00-4006,0,0.00292078,"us-based semantic parsing due to the availability of a hand-built natural-language interface, G EOBASE, supplied with Turbo Prolog 2.0 (Borland International, 1988). The G EOQUERY language consists of Prolog queries augmented with several meta-predicates (Zelle and Mooney, 1996). Below is a sample query with its English gloss: answer(A,count(B,(city(B),loc(B,C), const(C,countryid(usa))),A)) “How many cities are there in the US?” 3 Semantic Parsing Framework This section describes our basic framework for semantic parsing, which is based on a fairly standard approach to compositional semantics (Jurafsky and Martin, 2000). First, a statistical parser is used to construct a SAPT that captures the semantic interpretation of individual words and the basic predicate-argument structure of the sentence. Next, a recursive procedure is used to compositionally construct an MR for each node in the SAPT from the semantic label of the node and the MR’s 10 of its children. Syntactic structure provides information of how the parts should be composed. Ambiguities arise in both syntactic structure and the semantic interpretation of words and phrases. By integrating syntax and semantics in a single statistical parser that prod"
W05-0602,W04-0902,0,0.0199459,"input, goldstandard SAPT’s, not required by these other systems. Further automating the construction of training SAPT’s from sentences paired with MR’s is a subject of on-going research. P RECISE is designed to work only for the specific task of NL database interfaces. By comparison, S CISSOR is more general and can work with other MRL’s as well (e.g. CL ANG). Also, P RECISE is not a learning system and can fail to parse a query it considers ambiguous, even though it may not be considered ambiguous by a human and could potentially be resolved by learning regularities in the training data. In (Lev et al., 2004), a syntax-driven approach is used to map logic puzzles described in NL to an MRL. The syntactic structures are paired with hand-written rules. A statistical parser is used to generate syntactic parse trees, and then MR’s are built using compositional semantics. The meaning of open-category words (with only a few exceptions) is considered irrelevant to solving the puzzle and their meanings are not resolved. Further steps would be needed to generate MR’s in other domains like CL ANG and G EOQUERY. No empirical results are reported for their approach. Several machine translation systems also att"
W05-0602,P96-1008,0,0.207495,"parser on trees with only semantic labels on the nodes; however, they do not integrate syntactic and semantic parsing. History-based models of parsing were first introduced in (Black et al., 1993). Their original model also included semantic labels on parse-tree nodes, but they were not used to generate a formal MR. Also, their parsing model is impoverished compared to the history included in Collins’ more recent model. S CISSOR explores incorporating semantic labels into Collins’ model in order to produce a complete SAPT which is then used to generate a formal MR. The systems introduced in (Miller et al., 1996; Miller et al., 2000) also integrate semantic labels into parsing; however, their SAPT’s are used to produce a much simpler MR, i.e., a single semantic frame. A sample frame is A IRT RANSPORTATION which has three slots – the arrival time, origin and destination. Only one frame needs to be extracted from each sentence, which is an easier task than our problem in which multiple nested frames (predicates) must be extracted. The syntactic model in (Miller et al., 2000) is similar to Collins’, but does not use features like subcat frames and distance measures. Also, the non-terminal label X is not"
W05-0602,A00-2030,0,0.0163526,"only semantic labels on the nodes; however, they do not integrate syntactic and semantic parsing. History-based models of parsing were first introduced in (Black et al., 1993). Their original model also included semantic labels on parse-tree nodes, but they were not used to generate a formal MR. Also, their parsing model is impoverished compared to the history included in Collins’ more recent model. S CISSOR explores incorporating semantic labels into Collins’ model in order to produce a complete SAPT which is then used to generate a formal MR. The systems introduced in (Miller et al., 1996; Miller et al., 2000) also integrate semantic labels into parsing; however, their SAPT’s are used to produce a much simpler MR, i.e., a single semantic frame. A sample frame is A IRT RANSPORTATION which has three slots – the arrival time, origin and destination. Only one frame needs to be extracted from each sentence, which is an easier task than our problem in which multiple nested frames (predicates) must be extracted. The syntactic model in (Miller et al., 2000) is similar to Collins’, but does not use features like subcat frames and distance measures. Also, the non-terminal label X is not further decomposed in"
W05-0602,J04-4004,0,\N,Missing
W05-0602,J02-3001,0,\N,Missing
W05-0602,C04-1021,0,\N,Missing
W05-0602,P93-1005,0,\N,Missing
W05-1307,J95-4004,0,0.057532,"1) are new types of probabilistic models that preserve all the advantages of Maximum Entropy models and at the same time avoid the label bias problem by allowing a sequence of tagging decisions to compete against each other in a global probabilistic model. 49 In both training and testing the CRF protein-name tagger, the corresponding Medline abstracts were processed as follows. Text was tokenized using white-space as delimiters and treating all punctuation marks as separate tokens. The text was segmented into sentences, and part-of-speech tags were assigned to each token using Brill’s tagger (Brill, 1995). For each token in each sentence, a vector of binary features was generated using the feature templates employed by the Maximum Entropy approach described in (Bunescu et al., 2005). Generally, these features make use of the words occurring before and after the current position in the text, their POS tags and capitalization patterns. Each feature occurring in the training data is associated with a parameter in the CRF model. We used the CRF implementation from (McCallum, 2002). To train the CRF’s parameters, we used 750 Medline abstracts manually annotated for protein names (Bunescu et al., 20"
W06-3307,J90-1003,0,0.0477657,"nored. For sake of simplicity, we use the simpler formula from Equation 3. sP MI(p1 ; p2 ) = n12 n1  n2 (3) 4 Integrated model The sP MI(p1 ; p2 ) formula can be rewritten as: (1) The approach that we take in this paper is to constrain the two proteins to be mentioned in the same sentence, based on the assumption that if there is a reason for two protein names to co-occur in the same sentence, then in most cases that is caused by their interaction. To compute the “degree of interaction” between two proteins p1 and p2 , we use the information-theoretic measure of pointwise mutual information (Church and Hanks, 1990; Manning and Sch¨utze, 1999), which is computed based on the following quantities: 1. N 2. P (p1 ; p2 ) &apos; n12 =N : the probability that p1 and p2 co-occur in the same sentence; n12 = the : the total number of protein pairs cooccurring in the same sentence in the corpus. 51 sP MI(p1 ; p2 ) = 1 n1  n2  n12 X 1 (4) i=1 Let s1 , s2 , ..., sn12 be the sentence contexts corresponding to the n12 co-occurrences of p1 and p2 , and assume that a sentence-level relation extractor is available, with the capability of computing normalized confidence values for all extractions. Then one way of using the"
W06-3307,W05-1303,0,0.0121136,"s an interaction between the two genes. Finding the two genes collocated in the same sentence in the abstract is very likely to be due to the fact that the abstract discusses their interaction. The heuristic can be made even more accurate if a pair of genes is considered as interacting only if they cooccur in a (predefined) minimum number of sentences in the entire corpus – with the evaluation modified accordingly, as described later in Section 6. 5.2 Gene Name Annotation and Normalization For the annotation of gene names and their normalization, we use a dictionary-based approach similar to (Cohen, 2005). NCBI1 provides a comprehensive dictionary of human genes, where each gene is specified by its unique identifier, and qualified with an official name, a description, synonym names and one or more protein names, as illustrated in Table 2. All of these names, including the description, are considered as potential referential expressions for the gene entity. Each name string is reduced to a normal form by: replacing dashes with spaces, introducing spaces between sequences of letters and se1 quences of digits, replacing Greek letters with their Latin counterparts (capitalized), substituting Roman"
W06-3307,J93-1003,0,0.0422502,"D corpus, or from the entire Medline, are annotated using the dictionary-based approach described in Section 5.2. The sentence-level extraction is done with the subsequence kernel (SSK) approach from (Bunescu and Mooney, 2005), which was shown to give good results on extracting interactions from biomedical abstracts. The subsequence kernel was trained on a set of 225 Medline abstracts which were manually 2 URL: http://www.ncbi.nih.gov 53 URL: http://opennlp.sourceforge.net annotated with protein names and their interactions. It is known that PMI gives undue importance to low frequency events (Dunning, 1993), therefore the evaluation considers only pairs of genes that occur at least 5 times in the whole corpus. When evaluating corpus-level extraction on HPRD, because the “quasi-exact” list of interactions is known, we report the precision-recall (PR) graphs, where the precision (P) and recall (R) are computed as follows: #true interactions extracted #total interaction extracted #true interactions extracted R = #true interactions P = 8 Experimental Results P (DjI) P (I jD)P (:I) = ln P (Dj:I) P (:I jD)P (I) (7) where P (D jI) and P (D j:I) are the probability of observing the data D conditioned on"
W06-3307,P05-1052,0,0.0308161,"a given pair of entities. More exactly, if p1 and p2 are two entities that occur in a total of n sentences s1 , s2 , ..., sn in the entire corpus C , then the confidence P (R(p1 ; p2 )jC) that they are in a particular relationship R is defined as: P (R(p1 ; p2 )jC) = (fP (R(p1 ; p2 )jsi )ji=1:ng) Table 1 shows only four of the many possible choices for the aggregation operator . max 2 Sentence-level relation extraction Most systems that identify relations between entities mentioned in text documents consider only pair of entities that are mentioned in the same sentence (Ray and Craven, 2001; Zhao and Grishman, 2005; Bunescu and Mooney, 2005). To decide the existence and the type of a relationship, these systems generally use lexico-semantic clues inferred from the sentence context of the two entities. Much research has been focused recently on automatically identifying biologically relevant entities and their relationships such as protein-protein interactions or subcellular localizations. For example, the sentence “TR6 specifically binds Fas ligand”, states an interaction between the two proteins TR6 and Fas ligand. One of the first systems for extracting interactions between proteins is described in (B"
W10-2924,P06-1115,1,0.451023,"tractable while allowing a joint labelling. ranges of the word indices of the two entities and sub-card-pyramid1 and sub-card-pyramid2 are the sub-card-pyramids rooted at its two children. Thus, along with the two entities and the words in the sentence, information from these subcard-pyramids is also used in deciding the relation at a node. In the next section, we further specify these entity and relation classifiers and explain how they are trained. We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the K RISP semantic parser (Kate and Mooney, 2006). Given the candidate entities in a sentence, the grammar, and the entity and relation classifiers, the card-pyramid parsing algorithm tries to find the most probable joint-labeling of all of its nodes, and thus jointly extracts entities and their relations. The parsing algorithm does a beam search and maintains a beam at each node of the cardpyramid. A node is represented by l[i][j] in the pseudo-code which stands for the node in the jth position in the ith level. Note that at level i, the nodes range from l[i][0] to l[i][n − i − 1], where n is the number of leaves. The beam at each node is a"
W10-2924,D08-1042,1,0.894949,"Missing"
W10-2924,A00-2030,0,0.0135938,"d employs independent entity and relation classifiers whose outputs are used to compute a most probable consistent global set of entities and relations using linear programming. One key advantage of our cardpyramid method over their method is that the classifiers can take the output of other classifiers under its node as input features during parsing. This is not possible in their approach because all classifier outputs are determined before they are passed to the linear program solver. Thus our approach is more integrated and allows greater interaction between dependent extraction decisions. Miller et al. (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels. They thus do a joint syntactic parsing and information extraction using a fixed template. However, as designed, such a CFG approach cannot handle the cases when an entity is involved in multiple relations and when the relations crisscross each other in the sentence, as in Figure 1. These cases occur frequently in the dataset we used in our experiments and many other relationextraction tasks. Giuliano et al. (2007) thoroughly evaluate the effect of entity extract"
W10-2924,H05-1091,1,0.714361,"t between l and b and between b and r for every entity (leaf) b that exists between the two entities l and r. For example, in figure 3, We add the kernels for each part of the input to compute the final kernel for the SVM classifiers. The kernel for the second part of the input is computed by simply counting the number of common 207 pairs of relations between two examples thus implicitly considering every pair of relation (as described in the last paragraph) as a feature. For the first part of the input, we use word-subsequence kernels which have shown to be effective for relation extraction (Bunescu and Mooney, 2005b). We compute the kernel as the sum of the wordsubsequence kernels between: the words between the two entities (between pattern), k (a parameter) words before the first entity (before pattern), k words after the second entity (after pattern) and the words from the beginning of the first entity to the end of the second entity (between-and-entity pattern). The before, between and after patterns have been found useful in previous work (Bunescu and Mooney, 2005b; Giuliano et al., 2007). Sometimes the words of the entities can indicate the relations they are in, hence we also use the betweenand-en"
W10-2924,E09-1066,0,0.0193228,"Missing"
W10-2924,C08-1088,0,0.0658161,"e Work For which relates a Person and an Organization, OrgBased In which relates an Organization and a Location etc.). Figure 1 shows a sample sentence annotated with entities and relations. The application domain and requirements of the downstream tasks usually dictate the type of entities and relations that an IE system needs to extract. Most work in IE has concentrated on entity extraction alone (Tjong Kim Sang, 2002; Sang and Meulder, 2003) or on relation extraction assuming entities are either given or previously extracted (Bunescu et al., 2005; Zhang et al., 2006; Giuliano et al., 2007; Qian et al., 2008). However, these tasks are very closely inter-related. While identifying correct entities is essential for identifying relations between them, identifying correct relations can in turn improve identification of entities. 203 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 203–212, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Work_For OrgBased_In Live_In OrgBased_In Live_In Located_In Person Location Location Other Organization John lives in Los Angeles , California and works there for an American company called AB"
W10-2924,W09-2201,0,0.012497,"information extraction using a fixed template. However, as designed, such a CFG approach cannot handle the cases when an entity is involved in multiple relations and when the relations crisscross each other in the sentence, as in Figure 1. These cases occur frequently in the dataset we used in our experiments and many other relationextraction tasks. Giuliano et al. (2007) thoroughly evaluate the effect of entity extraction on relation extraction using the dataset used in our experiments. However, they employ a pipeline architecture and did not investigate joint relation and entity extraction. Carlson et al. (2009) present a method to simultaneously do semi-supervised training of entity and relation classifiers. However, their coupling method is meant to take advantage of the available unsupervised data and does not do joint inference. Riedel et al. (2009) present an approach for extracting bio-molecular events and their arguments 6 Future Work There are several possible directions for extending the current approach. The card-pyramid structure could be used to perform other languageprocessing tasks jointly with entity and relation extraction. For example, co-reference resolution between two entities wit"
W10-2924,W09-1406,0,0.0142944,"ases occur frequently in the dataset we used in our experiments and many other relationextraction tasks. Giuliano et al. (2007) thoroughly evaluate the effect of entity extraction on relation extraction using the dataset used in our experiments. However, they employ a pipeline architecture and did not investigate joint relation and entity extraction. Carlson et al. (2009) present a method to simultaneously do semi-supervised training of entity and relation classifiers. However, their coupling method is meant to take advantage of the available unsupervised data and does not do joint inference. Riedel et al. (2009) present an approach for extracting bio-molecular events and their arguments 6 Future Work There are several possible directions for extending the current approach. The card-pyramid structure could be used to perform other languageprocessing tasks jointly with entity and relation extraction. For example, co-reference resolution between two entities within a sentence can be easily incorporated in card-pyramid parsing by introducing a production like coref → P erson P erson, indicating that the two person entities are the same. In this work, and in most previous work, relations are always consid"
W10-2924,W04-2401,0,0.243229,"are considered and every combination of their beam elements are tried. To be considered further, the two possible sub-card-pyramids encoded by the two beam elements must have a consistent overlap. This is easily enforced by checking that its left child’s right child’s beam element is same as its right child’s left child’s beam element. If this condition is satisfied, then those relation productions are considered which have the left-most leaf of the left child and right-most leaf 1 These are stored in the beam elements. Note that this step enforces the consistency constraint of Roth and Yih (Roth and Yih, 2004; Roth and Yih, 2007) that a relation can only be between the entities of specific types. The grammar in our approach inherently enforces this constraint. 2 206 function Card-Pyramid-Parsing(Sentence,Grammar,entity-classifiers,relation-classifiers) n = number of candidate entities in S // Let range(r) represent the range of the indices of the words for the rth candidate entity. // Let l[i][j] represent the jth node at ith level in the card-pyramid. // For leaves // A beam element at a leaf node is (label,probability). for j = 0 to n // for every leaf for each entityLabel → candidate entity ∈ G"
W10-2924,W02-2024,0,0.00977717,"xtracting structured information from text. The two most common sub-tasks of IE are extracting entities (like Person, Location and Organization) and extracting relations between them (like Work For which relates a Person and an Organization, OrgBased In which relates an Organization and a Location etc.). Figure 1 shows a sample sentence annotated with entities and relations. The application domain and requirements of the downstream tasks usually dictate the type of entities and relations that an IE system needs to extract. Most work in IE has concentrated on entity extraction alone (Tjong Kim Sang, 2002; Sang and Meulder, 2003) or on relation extraction assuming entities are either given or previously extracted (Bunescu et al., 2005; Zhang et al., 2006; Giuliano et al., 2007; Qian et al., 2008). However, these tasks are very closely inter-related. While identifying correct entities is essential for identifying relations between them, identifying correct relations can in turn improve identification of entities. 203 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 203–212, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguisti"
W10-2924,P06-1104,0,0.0100736,"and extracting relations between them (like Work For which relates a Person and an Organization, OrgBased In which relates an Organization and a Location etc.). Figure 1 shows a sample sentence annotated with entities and relations. The application domain and requirements of the downstream tasks usually dictate the type of entities and relations that an IE system needs to extract. Most work in IE has concentrated on entity extraction alone (Tjong Kim Sang, 2002; Sang and Meulder, 2003) or on relation extraction assuming entities are either given or previously extracted (Bunescu et al., 2005; Zhang et al., 2006; Giuliano et al., 2007; Qian et al., 2008). However, these tasks are very closely inter-related. While identifying correct entities is essential for identifying relations between them, identifying correct relations can in turn improve identification of entities. 203 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 203–212, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Work_For OrgBased_In Live_In OrgBased_In Live_In Located_In Person Location Location Other Organization John lives in Los Angeles , California and wo"
W10-2924,J00-4006,0,\N,Missing
W10-2924,W03-0419,0,\N,Missing
W10-2924,P02-1062,0,\N,Missing
W11-0107,W11-0124,1,0.646467,"s done. 4. The lowest-cost proof is the best interpretation, or the best abductive proof of the goal expression. However, there are two significant problems with weighted abduction as it was originally presented. First, it required a large knowledge base of commonsense knowledge. This was not available when weighted abduction was first described, but since that time there have been substantial efforts to build up knowledge bases for various purposes, and at least two of these have been used with promising results in an abductive setting—Extended WordNet [6] for question-answering and FrameNet [11] for textual inference. The second problem with weighted abduction was that the weights and costs did not have a probabilistic semantics. This, for example, hampers automatic learning of weights from data or existing resources. That is the issue we address in the present paper. In the last decade and a half, a number of formalisms for adding uncertain reasoning to predicate logic have been developed that are well-founded in probability theory. Among the most widely investigated is Markov logic [14, 4]. In this paper we show how weighted abduction can be implemented in Markov logic. This demons"
W11-0112,C04-1180,0,0.0229644,"g weights to first-order logical rules, combining a diverse set of inference rules, and performing inference in a probabilistic way. While this is a large and complex task, this paper proposes a series of first steps toward our goal. In this paper, we focus on three natural language phenomena and their interaction: implicativity and factivity, word meaning, and coreference. Our framework parses natural language into a logical form, adds rule weights computed by external NLP modules, and performs inferences using an MLN. Our end-to-end approach integrates multiple existing tools. We use Boxer (Bos et al., 2004) to parse natural 105 language into a logical form. We use Alchemy (Kok et al., 2005) for MLN inference. Finally, we use the exemplar-based distributional model of Erk and Pad´o (2010) to produce rule weights. 2 Background Logic-based semantics. Boxer (Bos et al., 2004) is a software package for wide-coverage semantic analysis that provides semantic representations in the form of Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Bos and Markert (2005) describe a system for Recognizing Textual Entailment (RTE) that uses Boxer t"
W11-0112,H05-1079,0,0.872595,"nces that take advantage of logical concepts such as factivity as well as probabilistic information on word meaning in context. 1 Introduction Logic-based representations of natural language meaning have a long history. Representing the meaning of language in a first-order logical form is appealing because it provides a powerful and flexible way to express even complex propositions. However, systems built solely using first-order logical forms tend to be very brittle as they have no way of integrating uncertain knowledge. They, therefore, tend to have high precision at the cost of low recall (Bos and Markert, 2005). Recent advances in computational linguistics have yielded robust methods that use weighted or probabilistic models. For example, distributional models of word meaning have been used successfully to judge paraphrase appropriateness. This has been done by representing the word meaning in context as a point in a high-dimensional semantics space (Erk and Pad´o, 2008; Thater et al., 2010; Erk and Pad´o, 2010). However, these models typically handle only individual phenomena instead of providing a meaning representation for complete sentences. It is a long-standing open question how best to integr"
W11-0112,P04-1014,0,0.0375653,"erforms inferences using an MLN. Our end-to-end approach integrates multiple existing tools. We use Boxer (Bos et al., 2004) to parse natural 105 language into a logical form. We use Alchemy (Kok et al., 2005) for MLN inference. Finally, we use the exemplar-based distributional model of Erk and Pad´o (2010) to produce rule weights. 2 Background Logic-based semantics. Boxer (Bos et al., 2004) is a software package for wide-coverage semantic analysis that provides semantic representations in the form of Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Bos and Markert (2005) describe a system for Recognizing Textual Entailment (RTE) that uses Boxer to convert both the premise and hypothesis of an RTE pair into first-order logical semantic representations and then uses a theorem prover to check for logical entailment. Distributional models for lexical meaning. Distributional models describe the meaning of a word through the context in which it appears (Landauer and Dumais, 1997; Lund and Burgess, 1996), where contexts can be documents, other words, or snippets of syntactic structure. Distributional models are able to predict semantic simila"
W11-0112,D08-1094,1,0.167087,"Missing"
W11-0112,P10-2017,1,0.26339,"Missing"
W11-0112,N06-2015,0,0.00739758,"tion of hypernymy and polarity are given in (12). The rule in (12a) states that in a positive environment, the hyponym entails the hypernym while the rule in (12b) states that in a negative environment, the opposite is true: the hypernym entails the hyponym. (12) (a) ∀ l p1 p2 x.[(hypernym(p1 , p2 ) ∧ true(l) ∧ pred(l, p1 , x)) → pred(l, p2 , x)]] (b) ∀ l p1 p2 x.[(hypernym(p1 , p2 ) ∧ f alse(l) ∧ pred(l, p2 , x)) → pred(l, p1 , x)]] Making use of coreference information As a test case for incorporating additional resources into Boxer’s logical form, we used the coreference data in OntoNotes (Hovy et al., 2006). However, the same mechanism would allow us to transfer information into Boxer output from arbitrary additional NLP tools such as automatic coreference analysis tools or semantic role labelers. Our system uses coreference information into two distinct ways. The first way we make use of coreference data is to copy atoms describing a particular variable to those variables that corefer. Consider again example (4) which has a two-sentence premise. This inference requires recognizing that the “he” in the second sentence of the premise refers to “George Christopher” from the first sentence. Boxer a"
W11-0112,W09-3714,0,0.130866,"(the “arranging that”) is the theme of “forget to” and DRS x5 (the “failing”) is the theme of “arrange that”. In order to write logical rules about the truth conditions of nested propositions, the structure has to be flattened. However, it is clearly not sufficient to just conjoin all propositions at the top level. Such an approach, applied to example (2), would yield (hope(x1 ) ∧ theme(x1 , x2 ) ∧ build(x2 ) ∧ . . .), leading to the wrong inference that the stadium was built. Instead, we add a new argument to each predicate that 1 2 Examples (1) and (16) and Figure 2 are based on examples by MacCartney and Manning (2009) Examples (2), (3), (4), and (18) are modified versions of sentences from document wsj 0126 from the Penn Treebank 107 x0 x1 named(x0,ed,per) named(x1,dave,per) x2 x3 ¬ forget(x2) event(x2) agent(x2,x0) theme(x2,x3) x4 x5 x3: arrange(x4) event(x4) agent(x4,x0) theme(x4,x5) x6 x5: fail(x6) event(x6) agent(x6,x1) transforms to −−−−−−−→ named(l0, ne per ed d s0 w0, z0) named(l0, ne per dave d s0 w7, z1) not(l0, l1) pred(l1, v forget d s0 w3, e2) event(l1, e2) rel(l1, agent, e2, z0) rel(l1, theme, e2, l2) prop(l1, l2) pred(l2, v arrange d s0 w5, e4) event(l2, e4) rel(l2, agent, e4, z0) rel(l2, the"
W11-0112,P08-1028,0,0.0243781,"and then uses a theorem prover to check for logical entailment. Distributional models for lexical meaning. Distributional models describe the meaning of a word through the context in which it appears (Landauer and Dumais, 1997; Lund and Burgess, 1996), where contexts can be documents, other words, or snippets of syntactic structure. Distributional models are able to predict semantic similarity between words based on distributional similarity and they can be learned in an unsupervised fashion. Recently distributional models have been used to predict the applicability of paraphrases in context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010; Erk and Pad´o, 2010). For example, in “The wine left a stain”, “result in” is a better paraphrase for “leave” than is “entrust”, while the opposite is true in “He left the children with the nurse”. Usually, the distributional representation for a word mixes all its usages (senses). For the paraphrase appropriateness task, these representations are then reweighted, extended, or filtered to focus on contextually appropriate usages. Markov Logic. An MLN consists of a set of weighted first-order clauses. It provides a way of softening first-order logic b"
W11-0112,W06-3907,0,0.107176,"p, x)]] (6) We use three different predicate symbols to distinguish three types of atomic concepts: predicates, named entities, and relations. Predicates and named entities represent words that appear in the text. For example, named(l0, ne per ed d s0 w0, z0) indicates that variable z0 is a person named “Ed” while pred(l1, v forget d s0 w3, e2) says that e2 is a “forgetting to” event. Relations capture the relationships between words. For example, rel(l1, agent, e2, z0) indicates that z0, “Ed”, is the “agent” of the “forgetting to” event e2. 5 Handling the phenomena Implicatives and factives Nairn et al. (2006) presented an approach to the treatment of inferences involving implicatives and factives. Their approach identifies an “implication signature” for every implicative or factive verb that determines the truth conditions for the verb’s nested proposition, whether in a positive or negative environment. Implication signatures take the form “x/y” where x represents the implicativity in the the positive environment and y represents the implicativity in the negative environment. Both x and y have three possible values: “+” for positive entailment, meaning the nested proposition is entailed, “-” for n"
W11-0112,D09-1001,0,0.0278519,"d Z be the normalization constant. Then the probability of a particular truth assignment x to the variables in X is defined as:     X X X 1 1 P (X = x) = exp  g(x) = exp  wi wi ni (x) (1) Z Z fi ∈F g∈Gfi fi ∈F P where g(x) is 1 if g is satisfied and 0 otherwise, and ni (x) = g∈Gf g(x) is the number of groundings i of fi that are satisfied given the current truth assignment to the variables in X. This means that the probability of a truth assignment rises exponentially with the number of groundings that are satisfied. Markov Logic has been used previously in other NLP application (e.g. Poon and Domingos (2009)). However, this paper marks the first attempt at representing deep logical semantics in an MLN. While it is possible learn rule weights in an MLN directly from training data, our approach at this time focuses on incorporating weights computed by external knowledge sources. Weights for word meaning rules are computed from the distributional model of lexical meaning and then injected into the MLN. Rules governing implicativity and coreference are given infinite weight (hard constraints). 3 Evaluation and phenomena Textual entailment offers a good framework for testing whether a system performs"
W11-0112,P10-1097,0,0.0451223,"Missing"
W13-1302,C00-1007,0,0.0497395,"ful in suggesting that “walk” is best used to describe a man “moving” with his dog. We refer to this process as verb expansion. After describing the details of our approach, we present experiments evaluating it on a real-world corpus of YouTube videos. Using a variety of methods for judging the output of the system, we demonstrate that it frequently generates useful descriptions of videos and outperforms a purely vision-based approach that does not utilize text-mined knowledge. 2 Background and Related Work Although there has been a lot of interesting work done in natural language generation (Bangalore and Rambow, 2000; Langkilde and Knight, 1998), we use a simple template for generating our sentences as we found it to work well for our task. Most prior work on natural-language description of visual data has focused on static images (Felzenszwalb et al., 2008; Kulkarni et al., 2011; Kuznetsova et al., 2012; Laptev et al., 2008; Li et al., 11 Our work differs in that we make extensive use of text-mined knowledge to select the best SVO triple and generate coherent sentences. We also evaluate our approach on a generic, large and diverse set of challenging YouTube videos that cover a wide range of activities. M"
W13-1302,de-marneffe-etal-2006-generating,0,0.0253221,"Missing"
W13-1302,W12-0504,0,0.161668,"e annotated video corpus of similar SVO triplets (Packer et al., 2012). We are interested in annotating arbitrary short videos using off-the-shelf visual detectors, without the engineering effort required to build domain-specific activity models. Our main contribution is incorporating the pragmatics of various entities’ likelihood of being 10 Proceedings of the NAACL HLT Workshop on Vision and Language (WVL ’13), pages 10–19, c Atlanta, Georgia, 14 June 2013. 2013 Association for Computational Linguistics 2011; Yao et al., 2010). The small amount of existing work on videos (Ding et al., 2012; Khan and Gotoh, 2012; Kojima et al., 2002; Lee et al., 2008; Yao and Fei-Fei, 2010) uses hand-crafted templates or rule-based systems, works in constrained domains, and does not exploit text mining. Barbu et al. (2012) produce sentential descriptions for short video clips by using an interesting dynamic programming approach combined with Hidden Markov Models for obtaining verb labels for each video. However, they do not use any text mining to improve the quality of their visual detections. Figure 1: Content planning and surface realization the subject/object of a given activity, learned from web-scale text corpor"
W13-1302,P12-1038,0,0.0851995,"output of the system, we demonstrate that it frequently generates useful descriptions of videos and outperforms a purely vision-based approach that does not utilize text-mined knowledge. 2 Background and Related Work Although there has been a lot of interesting work done in natural language generation (Bangalore and Rambow, 2000; Langkilde and Knight, 1998), we use a simple template for generating our sentences as we found it to work well for our task. Most prior work on natural-language description of visual data has focused on static images (Felzenszwalb et al., 2008; Kulkarni et al., 2011; Kuznetsova et al., 2012; Laptev et al., 2008; Li et al., 11 Our work differs in that we make extensive use of text-mined knowledge to select the best SVO triple and generate coherent sentences. We also evaluate our approach on a generic, large and diverse set of challenging YouTube videos that cover a wide range of activities. Motwani and Mooney (2012) explore how object detection and text mining can aid activity recognition in videos; however, they do not determine a complete SVO triple for describing a video nor generate a full sentential description. With respect to static image description, Li et al. (2011) gene"
W13-1302,P98-1116,0,0.0794311,"” is best used to describe a man “moving” with his dog. We refer to this process as verb expansion. After describing the details of our approach, we present experiments evaluating it on a real-world corpus of YouTube videos. Using a variety of methods for judging the output of the system, we demonstrate that it frequently generates useful descriptions of videos and outperforms a purely vision-based approach that does not utilize text-mined knowledge. 2 Background and Related Work Although there has been a lot of interesting work done in natural language generation (Bangalore and Rambow, 2000; Langkilde and Knight, 1998), we use a simple template for generating our sentences as we found it to work well for our task. Most prior work on natural-language description of visual data has focused on static images (Felzenszwalb et al., 2008; Kulkarni et al., 2011; Kuznetsova et al., 2012; Laptev et al., 2008; Li et al., 11 Our work differs in that we make extensive use of text-mined knowledge to select the best SVO triple and generate coherent sentences. We also evaluate our approach on a generic, large and diverse set of challenging YouTube videos that cover a wide range of activities. Motwani and Mooney (2012) expl"
W13-1302,W11-0326,0,0.138091,"znetsova et al., 2012; Laptev et al., 2008; Li et al., 11 Our work differs in that we make extensive use of text-mined knowledge to select the best SVO triple and generate coherent sentences. We also evaluate our approach on a generic, large and diverse set of challenging YouTube videos that cover a wide range of activities. Motwani and Mooney (2012) explore how object detection and text mining can aid activity recognition in videos; however, they do not determine a complete SVO triple for describing a video nor generate a full sentential description. With respect to static image description, Li et al. (2011) generate sentences given visual detections of objects, visual attributes and spatial relationships; however, they do not consider actions. Farhadi et al. (2010) propose a system that maps images and the corresponding textual descriptions to a “meaning” space which consists of an object, action and scene triplet. However, they assume a single object per image and do not use text-mining to determine the likelihood of objects matching different verbs. Yang et al. (2011) is the most similar to our approach in that it uses text-mined knowledge to generate sentential descriptions of static images a"
W13-1302,P12-3029,0,0.00752578,"“back-off” to the Subject-Verb and Verb-Object bigrams to coherently estimate its probability. This results in a sophisticated statistical model for estimating triplet probabilities using the syntactic context in which the words have previously occurred. This allows us to effectively determine the real-world plausibility of any SVO using knowledge automatically mined from raw text. We call this the “SVO Language Model” approach (SVO LM). In a second approach to estimating SVO probabilities, we used BerkeleyLM (Pauls and Klein, 2011) to train an n-gram language model on the GoogleNgram corpus (Lin et al., 2012). This simple model does not consider synonyms, verb conjugations, or SVO dependencies but only looks at word sequences. Given an SVO triplet as an input sequence, it estimates its probability based on n-grams. We refer to this as the “Language Model” approach (LM). 3.5 Verb Expansion As mentioned earlier, the top activity detections are expanded with their most similar verbs in order to generate a larger set of potential words for describing the action. We used the WUP metric from WordNet::Similarity to expand each activity cluster to include all verbs with a similarity of at least 0.5. For e"
W13-1302,P11-1027,0,0.040314,". In this model, if we have not seen training data for a particular SVO trigram, we “back-off” to the Subject-Verb and Verb-Object bigrams to coherently estimate its probability. This results in a sophisticated statistical model for estimating triplet probabilities using the syntactic context in which the words have previously occurred. This allows us to effectively determine the real-world plausibility of any SVO using knowledge automatically mined from raw text. We call this the “SVO Language Model” approach (SVO LM). In a second approach to estimating SVO probabilities, we used BerkeleyLM (Pauls and Klein, 2011) to train an n-gram language model on the GoogleNgram corpus (Lin et al., 2012). This simple model does not consider synonyms, verb conjugations, or SVO dependencies but only looks at word sequences. Given an SVO triplet as an input sequence, it estimates its probability based on n-grams. We refer to this as the “Language Model” approach (LM). 3.5 Verb Expansion As mentioned earlier, the top activity detections are expanded with their most similar verbs in order to generate a larger set of potential words for describing the action. We used the WUP metric from WordNet::Similarity to expand each"
W13-1302,D11-1041,0,0.324276,"Missing"
W13-1302,C98-1112,0,\N,Missing
W13-1302,N04-3012,0,\N,Missing
W13-1302,P11-1020,0,\N,Missing
W14-2402,S13-1001,0,0.0770814,"s a complex set of relationships between them 7 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 7–11, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). synonymy relation between “man” and “guy” is represented by: ∀x. man(x) ⇔ guy(x) |w1 and the hyponym"
W14-2402,S12-1051,0,0.032773,"erall interpretation with the maximum probability given a set of evidence. It turns out that this optimization problem is second-order cone program (SOCP) (Kimmig et al., 2012) and can be solved efficiently in polynomial time. 2.5 Recognizing Textual Entailment Recognizing Textual Entailment (RTE) is the task of determining whether one natural language text, the premise, Entails, Contradicts, or not related (Neutral) to another, the hypothesis. 2.6 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on a scale from 1 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations and systems are evaluated using the Pearson correlation between a system’s output and gold standard scores. 3 Approach A semantic parser is three components, a formal language, an ontology, and an inference mechanism. This section explains the details of these components in our semantic parser. It also points out the future work related to each part of the system. 3.1 Formal Language: first-order logic Natural sentences are mapped to logical form using Boxer (Bos, 2008), which maps the input sentences into a lexically-based lo"
W14-2402,D10-1115,0,0.28562,"s 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). synonymy relation between “man” and “guy” is represented by: ∀x. man(x) ⇔ guy(x) |w1 and the hyponymy relation between “car” and “vehicle” is: ∀x. car(x) ⇒ vehicle(x) |w2 where w1 and w1 are some certainty measure estimated from the distributional semantics. For inference, we use probabilistic logic frameworks"
W14-2402,D13-1161,0,0.0210497,"he ontology. Inference is what takes a problem represented in the formal knowledge representation and the ontology and performs the target task (e.g. textual entailment, question answering, etc.). Prior work in standard semantic parsing uses a pre-defined set of predicates in a fixed ontology. However, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and very difficult to do semantic parsing based on such an ontology. Consequently, current semantic parsers are mostly restricted to fairly limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). We propose a semantic parser that is not restricted to a predefined ontology. Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology. Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words (Turney and Pantel, 2010; Landauer and Dumais, 1997). Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (Lenci and Benotto, 2012; Kotlerman et al., 2010). The collection of predicte"
W14-2402,S13-1002,1,0.924408,"e hyponymy relation between “car” and “vehicle” is: ∀x. car(x) ⇒ vehicle(x) |w2 where w1 and w1 are some certainty measure estimated from the distributional semantics. For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) (Richardson and Domingos, 2006) and Probabilistic Soft Logic (PSL) (Kimmig et al., 2012). They are Statistical Relational Learning (SRL) techniques (Getoor and Taskar, 2007) that combine logical and statistical knowledge in one uniform framework, and provide a mechanism for coherent probabilistic inference. We implemented this semantic parser (Beltagy et al., 2013; Beltagy et al., 2014) and used it to perform two tasks that require deep semantic analysis, Recognizing Textual Entailment (RTE), and Semantic Textual Similarity (STS). The rest of the paper is organized as follows: section 2 presents background material, section 3 explains the three components of the semantic parser, section 4 shows how this semantic parser can be used for RTE and STS tasks, section 5 presents the evaluation and 6 concludes. 2 2.1 2.3 Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a framework for probabilistic logic that employ weighted formulas in firstorder"
W14-2402,P14-1114,1,0.442945,"tween “car” and “vehicle” is: ∀x. car(x) ⇒ vehicle(x) |w2 where w1 and w1 are some certainty measure estimated from the distributional semantics. For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) (Richardson and Domingos, 2006) and Probabilistic Soft Logic (PSL) (Kimmig et al., 2012). They are Statistical Relational Learning (SRL) techniques (Getoor and Taskar, 2007) that combine logical and statistical knowledge in one uniform framework, and provide a mechanism for coherent probabilistic inference. We implemented this semantic parser (Beltagy et al., 2013; Beltagy et al., 2014) and used it to perform two tasks that require deep semantic analysis, Recognizing Textual Entailment (RTE), and Semantic Textual Similarity (STS). The rest of the paper is organized as follows: section 2 presents background material, section 3 explains the three components of the semantic parser, section 4 shows how this semantic parser can be used for RTE and STS tasks, section 5 presents the evaluation and 6 concludes. 2 2.1 2.3 Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly enc"
W14-2402,S12-1012,0,0.248494,"ited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). We propose a semantic parser that is not restricted to a predefined ontology. Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology. Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words (Turney and Pantel, 2010; Landauer and Dumais, 1997). Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (Lenci and Benotto, 2012; Kotlerman et al., 2010). The collection of predicted semantic relations is the “on-the-fly ontology” our semantic parser uses. A distributional semantics is relatively easy to build from a large corpus of raw text, and provides the wide coverage that formal ontologies lack. The formal language we would like to use in the semantic parser is first-order logic. However, distributional information is graded in nature, so the on-the-fly ontology and its predicted semantic relations are also graded. This means, that standard first-order logic is insufficient because it is binary by nature. Probabi"
W14-2402,D13-1160,0,0.0431878,"what takes a problem represented in the formal knowledge representation and the ontology and performs the target task (e.g. textual entailment, question answering, etc.). Prior work in standard semantic parsing uses a pre-defined set of predicates in a fixed ontology. However, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and very difficult to do semantic parsing based on such an ontology. Consequently, current semantic parsers are mostly restricted to fairly limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). We propose a semantic parser that is not restricted to a predefined ontology. Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology. Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words (Turney and Pantel, 2010; Landauer and Dumais, 1997). Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (Lenci and Benotto, 2012; Kotlerman et al., 2010). The collection of predicted semantic relations i"
W14-2402,P08-1028,0,0.0743507,"26 2014. 2014 Association for Computational Linguistics 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). synonymy relation between “man” and “guy” is represented by: ∀x. man(x) ⇔ guy(x) |w1 and the hyponymy relation between “car” and “vehicle” is: ∀x. car(x) ⇒ vehicle(x) |w2 where w1 and w1 are some certainty measure estimated from the distributional semantic"
W14-2402,W08-2222,0,0.180759,"ntal issue. Background Logical Semantics Logic-based representations of meaning have a long tradition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. Also, the logical predicates and relations do not have semantics by themselves without an accompanying ontology, which we want to replace in our semantic parser with distributional semantics. To map a sentence to logical form, we use Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that produces uninterpreted logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). 2.2 Markov Logic Network 2.4 Probabilistic Soft Logic Probabilistic Soft Logic (PSL) is another recently proposed framework for probabilistic logic (Kimmig et al., 2012). It uses logical representations to compactly define large graphical models with continuous variables, and includes methods for performing efficient probabilistic inference for the resulting models. A key distinguishing"
W14-2402,P04-1014,0,0.0414115,"lational propositions, logical operators, and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. Also, the logical predicates and relations do not have semantics by themselves without an accompanying ontology, which we want to replace in our semantic parser with distributional semantics. To map a sentence to logical form, we use Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that produces uninterpreted logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). 2.2 Markov Logic Network 2.4 Probabilistic Soft Logic Probabilistic Soft Logic (PSL) is another recently proposed framework for probabilistic logic (Kimmig et al., 2012). It uses logical representations to compactly define large graphical models with continuous variables, and includes methods for performing efficient probabilistic inference for the resulting models. A key distinguishing feature of PSL is that ground atoms have soft, continuous truth values in the interval [0, 1] rather than binary truth values as used in MLNs and most other probabilistic logics. Given a set of weighted infer"
W14-2402,N13-1092,0,0.106997,"Missing"
W14-2402,D11-1129,0,\N,Missing
W16-6003,D13-1178,0,0.0325219,"head of an NP phrase relating to the verb, the entity identity according to a coreference resolution engine, or both). For example, the sentence Smith got off the plane at the Beijing air11 Proceedings of EMNLP 2016 Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods, pages 11–16, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics port would be represented as (get off, smith, plane, (at, airport)). This event representation was investigated in Pichotta and Mooney (2014) in the context of count-based co-occurrence models. Balasubramanian et al. (2013), Modi and Titov (2014), and Granroth-Wilding and Clark (2016) describe systems for related tasks with similar event formulations. In Pichotta and Mooney (2016a), we train an RNN sequence model by inputting one component of an event tuple at each timestep, representing sequences of events as sequences of event components. Standard methods for learning RNN sequence models are applied to learning statistical models of sequences of event components. To infer probable unobserved events from documents, we input observed document events in sequence, one event component per timestep, and then search"
W16-6003,P16-1167,0,0.0151079,"Li et al., 2012; Frermann et al., 2014; Orr et al., 2014). Fourth, there is a recent body of work investigating the automatic induction of event structure in different modalities. Kim and Xing (2014) give a method of modeling sequences of images from ordered photo collections on the web, allowing them to perform, among other things, sequential image prediction. Huang et al. (2016) describe a new dataset of photos in temporal sequence scraped from web albums, along with crowdsourced story-like descriptions of the sequences (and methods for automatically generating the latter from the former). Bosselut et al. (2016) describe a system which learns a model of prototypical event co-occurrence from online photo albums with their natural language captions. Incorporating learned event co-occurrence structure from large-scale natural datasets of different modalities could be an exciting line of future research. Finally, there are a number of alternative ways of evaluating learned script models that have been proposed. Motivated by the shortcomings of evaluation via held-out event inference, Mostafazadeh et al. (2016) recently introduced a corpus of crowdsourced short stories with plausible “impostor” endings al"
W16-6003,P08-1090,0,0.173393,"co-occurrence are non-statistical and handcrafted based on appeals to the intuitions of the knowledge engineer. Mooney and DeJong (1985) give an early non-statistical method of automatically inducing models of co-occurring events from documents, but their methods are non-statistical. There is a growing body of more recent work investigating methods of learning statistical models of event sequences from large corpora of raw text. These methods admit scaling models up to be much larger than hand-engineered ones, while being more robust to noise than automatically learned nonstatistical models. Chambers and Jurafsky (2008) describe a statistical co-occurrence model of (verb, dependency) pair events that is trained on a large corpus of documents and can be used to infer implicit events from text. A number of other systems following similar paradigm have also been proposed (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015). These approaches achieve generalizability and computational tractability on large corpora, but do so at the expense of decreased representational complexity: in place of the rich event structures found in Schank and Abelson (1977), these systems model and infer structurall"
W16-6003,P09-1068,0,0.0205977,"l. There is a growing body of more recent work investigating methods of learning statistical models of event sequences from large corpora of raw text. These methods admit scaling models up to be much larger than hand-engineered ones, while being more robust to noise than automatically learned nonstatistical models. Chambers and Jurafsky (2008) describe a statistical co-occurrence model of (verb, dependency) pair events that is trained on a large corpus of documents and can be used to infer implicit events from text. A number of other systems following similar paradigm have also been proposed (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015). These approaches achieve generalizability and computational tractability on large corpora, but do so at the expense of decreased representational complexity: in place of the rich event structures found in Schank and Abelson (1977), these systems model and infer structurally simpler events. In this extended abstract, we will briefly summarize a number of statistical script-related systems we have described in previous publications (Pichotta and Mooney, 2016a; Pichotta and Mooney, 2016b), place them within the broader context of related research, and"
W16-6003,D13-1185,0,0.0187202,"systems which infer events from text (including the above work). Chambers and Jurafsky (2008) give a method of modeling and inferring simple (verb, dependency) pair-events. Jans et al. (2012) describe a model of the same sorts of events which gives superior performance on the task 13 of held-out event prediction; Rudinger et al. (2015) follow this line of inquiry, concluding that the task of inferring held-out (verb, dependency) pairs from documents is best handled as a language modeling task. Second, there is a body of work focusing on automatically inducing structured collections of events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Ferraro and Van Durme, 2016), typically motivated by Information Extraction tasks. Third, there is a body of work investigating highprecision models of situations as they occur in the world (as opposed to how they are described in text) from smaller corpora of event sequences (Regneri et al., 2010; Li et al., 2012; Frermann et al., 2014; Orr et al., 2014). Fourth, there is a recent body of work investigating the automatic induction of event structure in different modalities. Kim and Xing (2014) give a method of modeling sequences of images from order"
W16-6003,N16-1147,0,0.0266829,"Extraction tasks. Third, there is a body of work investigating highprecision models of situations as they occur in the world (as opposed to how they are described in text) from smaller corpora of event sequences (Regneri et al., 2010; Li et al., 2012; Frermann et al., 2014; Orr et al., 2014). Fourth, there is a recent body of work investigating the automatic induction of event structure in different modalities. Kim and Xing (2014) give a method of modeling sequences of images from ordered photo collections on the web, allowing them to perform, among other things, sequential image prediction. Huang et al. (2016) describe a new dataset of photos in temporal sequence scraped from web albums, along with crowdsourced story-like descriptions of the sequences (and methods for automatically generating the latter from the former). Bosselut et al. (2016) describe a system which learns a model of prototypical event co-occurrence from online photo albums with their natural language captions. Incorporating learned event co-occurrence structure from large-scale natural datasets of different modalities could be an exciting line of future research. Finally, there are a number of alternative ways of evaluating learn"
W16-6003,W14-1606,0,0.131466,"to the verb, the entity identity according to a coreference resolution engine, or both). For example, the sentence Smith got off the plane at the Beijing air11 Proceedings of EMNLP 2016 Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods, pages 11–16, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics port would be represented as (get off, smith, plane, (at, airport)). This event representation was investigated in Pichotta and Mooney (2014) in the context of count-based co-occurrence models. Balasubramanian et al. (2013), Modi and Titov (2014), and Granroth-Wilding and Clark (2016) describe systems for related tasks with similar event formulations. In Pichotta and Mooney (2016a), we train an RNN sequence model by inputting one component of an event tuple at each timestep, representing sequences of events as sequences of event components. Standard methods for learning RNN sequence models are applied to learning statistical models of sequences of event components. To infer probable unobserved events from documents, we input observed document events in sequence, one event component per timestep, and then search over the components of"
W16-6003,1985.tmi-1.17,0,0.111623,"the Beijing airport, etc. The world knowledge encoded in such event co-occurrence models is intuitively useful for a number of semantic tasks, including Question Answering, Coreference Resolution, Discourse Parsing, and Semantic Role Labeling. Script learning and inference date back to AI research from the 1970s, in particular the seminal work of Schank and Abelson (1977). In this work, events are formalized as quite complex handencoded structures, and the structures encoding event co-occurrence are non-statistical and handcrafted based on appeals to the intuitions of the knowledge engineer. Mooney and DeJong (1985) give an early non-statistical method of automatically inducing models of co-occurring events from documents, but their methods are non-statistical. There is a growing body of more recent work investigating methods of learning statistical models of event sequences from large corpora of raw text. These methods admit scaling models up to be much larger than hand-engineered ones, while being more robust to noise than automatically learned nonstatistical models. Chambers and Jurafsky (2008) describe a statistical co-occurrence model of (verb, dependency) pair events that is trained on a large corp"
W16-6003,N16-1098,0,0.00703414,"escriptions of the sequences (and methods for automatically generating the latter from the former). Bosselut et al. (2016) describe a system which learns a model of prototypical event co-occurrence from online photo albums with their natural language captions. Incorporating learned event co-occurrence structure from large-scale natural datasets of different modalities could be an exciting line of future research. Finally, there are a number of alternative ways of evaluating learned script models that have been proposed. Motivated by the shortcomings of evaluation via held-out event inference, Mostafazadeh et al. (2016) recently introduced a corpus of crowdsourced short stories with plausible “impostor” endings alongside the real endings; script systems can be evaluated on this corpus by their ability to discriminate the real ending from the impostor one. This corpus is not large enough to train a script system, but can be used to evaluate a pretrained one. Hard coreference resolution problems (so-called “Winograd schema challenge” problems (Rahman and Ng, 2012)) provide another possible Input: Gold: Predicted: Input: Gold: Predicted: Input: Gold: Predicted: Input: Gold: Predicted: As of October 1 , 2008 , h"
W16-6003,P15-1019,0,0.0609004,"Missing"
W16-6003,P02-1040,0,0.120452,"be used for low-resource languages). System Unigram Copy/paste Event LSTM Text LSTM Accuracy 0.002 0.023 0.020 BLEU 1.88 0.34 5.20 1G P 22.6 19.9 30.9 Table 2: Prediction results in Pichotta and Mooney (2016b). More results can be found in the paper. Table 2 gives a subset of results from Pichotta and Mooney (2016b), comparing an event LSTM with a text LSTM. The “Copy/paste” baseline deterministically predicts a sentence as its own successor. The “Accuracy” metric measures what percentage of argmax inferences were equal to the gold-standard held-out event. The “BLEU” column gives BLEU scores (Papineni et al., 2002) for raw text inferred by systems (either directly, or via an intermediate text-generation step in the case of the Event LSTM output). The “1G P” column gives unigram precision against the gold standard, which is one of the components of BLEU. Figure 1, reproduced from Pichotta and Mooney (2016b), gives some example next-sentence predictions. Despite the fact that it is very difficult to predict the next sentence in natural text, the text-level encoder/decoder system is capable of learning learning some aspects of event cooccurrence structure in documents. These results indicate that modeling"
W16-6003,E14-1024,1,0.911535,"th information about their syntactic arguments (either the noun identity of the head of an NP phrase relating to the verb, the entity identity according to a coreference resolution engine, or both). For example, the sentence Smith got off the plane at the Beijing air11 Proceedings of EMNLP 2016 Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods, pages 11–16, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics port would be represented as (get off, smith, plane, (at, airport)). This event representation was investigated in Pichotta and Mooney (2014) in the context of count-based co-occurrence models. Balasubramanian et al. (2013), Modi and Titov (2014), and Granroth-Wilding and Clark (2016) describe systems for related tasks with similar event formulations. In Pichotta and Mooney (2016a), we train an RNN sequence model by inputting one component of an event tuple at each timestep, representing sequences of events as sequences of event components. Standard methods for learning RNN sequence models are applied to learning statistical models of sequences of event components. To infer probable unobserved events from documents, we input observ"
W16-6003,P16-1027,1,0.81666,"nts from text. A number of other systems following similar paradigm have also been proposed (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015). These approaches achieve generalizability and computational tractability on large corpora, but do so at the expense of decreased representational complexity: in place of the rich event structures found in Schank and Abelson (1977), these systems model and infer structurally simpler events. In this extended abstract, we will briefly summarize a number of statistical script-related systems we have described in previous publications (Pichotta and Mooney, 2016a; Pichotta and Mooney, 2016b), place them within the broader context of related research, and remark on future directions for research. 2 Methods and results In Pichotta and Mooney (2016a), we present a system that uses Long Short-Term Memory (LSTM) Recurrent Neural Nets (RNNs) (Hochreiter and Schmidhuber, 1997) to model sequences of events. In this work, events are defined to be verbs with information about their syntactic arguments (either the noun identity of the head of an NP phrase relating to the verb, the entity identity according to a coreference resolution engine, or both). For examp"
W16-6003,D12-1071,0,0.0215907,"Missing"
W16-6003,P10-1100,0,0.00872239,") follow this line of inquiry, concluding that the task of inferring held-out (verb, dependency) pairs from documents is best handled as a language modeling task. Second, there is a body of work focusing on automatically inducing structured collections of events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Ferraro and Van Durme, 2016), typically motivated by Information Extraction tasks. Third, there is a body of work investigating highprecision models of situations as they occur in the world (as opposed to how they are described in text) from smaller corpora of event sequences (Regneri et al., 2010; Li et al., 2012; Frermann et al., 2014; Orr et al., 2014). Fourth, there is a recent body of work investigating the automatic induction of event structure in different modalities. Kim and Xing (2014) give a method of modeling sequences of images from ordered photo collections on the web, allowing them to perform, among other things, sequential image prediction. Huang et al. (2016) describe a new dataset of photos in temporal sequence scraped from web albums, along with crowdsourced story-like descriptions of the sequences (and methods for automatically generating the latter from the former)."
W16-6003,D15-1195,0,0.0748113,"Missing"
W16-6003,P13-1045,0,0.013917,"a and Mooney (2016b), we compare event RNN models, of the sort described above, with RNN models that operate at the raw text level. In particular, we investigate the performance of a text-level sentence encoder/decoder similar to the skip-thought system of Kiros et al. (2015) on the task. In this setup, during inference, instead of encoding events and decoding events, we encode raw text, decode raw text, and then parse inferred text to get its dependency structure.2 This system does not obviously encode event co-occurrence structure in the way that the 2 We use the Stanford dependency parser (Socher et al., 2013). previous one does, but can still in principle infer implicit events from text, and does not require a parser (and can be therefore be used for low-resource languages). System Unigram Copy/paste Event LSTM Text LSTM Accuracy 0.002 0.023 0.020 BLEU 1.88 0.34 5.20 1G P 22.6 19.9 30.9 Table 2: Prediction results in Pichotta and Mooney (2016b). More results can be found in the paper. Table 2 gives a subset of results from Pichotta and Mooney (2016b), comparing an event LSTM with a text LSTM. The “Copy/paste” baseline deterministically predicts a sentence as its own successor. The “Accuracy” metri"
W16-6003,de-marneffe-etal-2006-generating,0,\N,Missing
W16-6003,E12-1034,0,\N,Missing
W16-6003,D13-1176,0,\N,Missing
W16-6003,D14-1181,0,\N,Missing
W16-6003,Q15-1024,0,\N,Missing
W16-6003,W01-1605,0,\N,Missing
W16-6003,P10-1158,0,\N,Missing
W16-6003,D15-1263,0,\N,Missing
W16-6003,S15-1024,0,\N,Missing
W16-6003,D14-1151,0,\N,Missing
W16-6003,P11-1098,0,\N,Missing
W16-6003,P14-1062,0,\N,Missing
W16-6003,P94-1019,0,\N,Missing
W16-6003,J05-1004,0,\N,Missing
W16-6003,P13-1035,0,\N,Missing
W16-6003,P09-1025,0,\N,Missing
W16-6003,prasad-etal-2008-penn,0,\N,Missing
W16-6003,N13-1104,0,\N,Missing
W16-6003,Q14-1029,0,\N,Missing
W16-6003,P16-1004,0,\N,Missing
W16-6003,K15-2002,0,\N,Missing
W16-6003,P15-1121,0,\N,Missing
W16-6003,D14-1220,0,\N,Missing
W16-6003,D13-1203,0,\N,Missing
W16-6003,P14-1002,0,\N,Missing
W16-6003,D14-1218,0,\N,Missing
W16-6003,E14-1006,0,\N,Missing
W16-6003,N15-1082,0,\N,Missing
W16-6003,N15-1173,1,\N,Missing
W16-6003,W12-3019,0,\N,Missing
W16-6003,D10-1048,0,\N,Missing
W17-2803,E17-1052,1,0.815054,"ions dW (p, o) as dW (p, o) = X c∈C 3  |P |−1 X  κq,c w(p, q) Gp,c (o) q∈P 4 (5) Conclusions and Future Work In this work, we have demonstrated that behavior annotations can improve language grounding for a platform with multiple interaction behaviors and modalities. In the future, we would like to apply this intuition in an embodied dialog agent. If a person asks a service robot to “Get the white cup.”, the robot should be able to ask “What should I do to tell if something is ‘white’?”, a behavior annotation prompt. A human-robot POMDP dialog policy could be learned, as in previous work (Padmakumar et al., 2017), to know when this kind of follow-up question is warranted. Additionally, we will explore other methods of sharing information between predicates from lexical information. For example, choosing a maximally similar neighboring word, rather than doing a weighted average across all known words, may yield better results (e.g. the best neighbor of “narrow” is “thin”, so don’t bother considering things like “green” at all). Experimental Evaluation We calculated precision, recall, and f -measure between human labels and predicate decisions when weighting constituent sensorimotor context classifiers"
W17-2803,D15-1293,0,0.0656436,"Missing"
W17-2803,P14-2003,0,0.0279704,"ons (whether to focus on audio or haptics when performing a behavior), which improves performance, and sharing information between linguistically related predicates (if “green” is a color, “white” is a color), which improves grounding recall but at the cost of precision. 1 Introduction Connecting human language predicates like “red” and “heavy” to machine perception is part of the symbol grounding problem (Harnad, 1990), approached in machine learning as grounded language learning. For many years, grounded language learning has been performed primarily in visual space (Roy and Pentland, 2002; Liu et al., 2014; Malinowski and Fritz, 2014; Mohan et al., 20 Proceedings of the First Workshop on Language Grounding for Robotics, pages 20–24, c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics grasp lift lower Figure 2: Objects explored via interaction behaviors and for which we have sparse predicate annotations. drop press Behaviors look drop, grasp, hold, lift lower, press, push push Figure 1: Behaviors the robot used to explore objects. In addition, the hold behavior (not shown) was performed after the lift behavior by holding the object in place for half a se"
W19-4807,W09-1904,0,0.0412451,"Missing"
W19-4807,D16-1011,0,0.355584,"ssification based on Convolutional Neural Networks (CNNs) (Zhang et al., 2016). While this work alludes to improved explainability using supervised attention, it does not explicitly evaluate this claim. We extend this work by evaluating whether supervised attention using human rationales, rather than unsupervised attention, actually improves explanation. Explanations from both models are full sentences that the model has weighted as being most important to the document’s final classification. While automated evaluations of explanations (e.g. comparing them to human gold-standard explanations (Lei et al., 2016)) can be somewhat useful, we argue that because the goal of machine explanations is to help users, they should be directly evaluated by human judges. Machine explanations can be different from human ones, but still provide good justification for a decision (Das et al., 2017). This opinion is shared by other researchers in the area (Doshi-Velez, 2017), but human evaluation is often avoided due to the time required and difficulty of conducting human trials. We believe it is a Work on “learning with rationales” shows that humans providing explanations to a machine learning system can improve the"
W19-4807,D18-1216,0,0.0377193,"dricks et al., 2018), but it is not connected to work on learning with human rationales, which we review below. As discussed above, Zhang et al. (2016) demonstrate increased predictive accuracy of CNN models augmented with human rationales. Here, we first reproduce their predictive results, and then focus on extracting and evaluating explanations from the models. Lei et al. (2016) present a model that extracts rationales for predictions without training on rationales. They compare their extracted rationales to human gold-standard ones through automated evaluations, i.e., precision and recall. Bao et al. (2018) extend this work by learning a mapping from the human rationales to continuous attention. They transfer this mapping to low resource target domains as an auxiliary training signal to improve classification accuracy in the target domain. They compare their learned attention with human rationales by calculating their cosine distance to the ‘oracle’ attention. None of the above related work asks human users to evaluate the generated explanations. However, Nguyen (2018) does compare human and automatic evaluations of explanations. That work finds that human evaluation is moderately, but statistic"
W19-4807,N16-1082,0,0.111806,"e human judges evaluate both attention based machine explanations and machine explanations trained from human rationales, thus connecting learning from human explanations and machine explanations to humans. necessary element of explainability research, and in this work, we compare the explanations from the two models through human evaluation on Mechanical Turk and find that the model trained with human rationales is judged to generate explanations that better support its decisions. 2 Related Work There is a growing body of research on explainable AI (Koh and Liang, 2017; Ribeiro et al., 2016; Li et al., 2016; Hendricks et al., 2018), but it is not connected to work on learning with human rationales, which we review below. As discussed above, Zhang et al. (2016) demonstrate increased predictive accuracy of CNN models augmented with human rationales. Here, we first reproduce their predictive results, and then focus on extracting and evaluating explanations from the models. Lei et al. (2016) present a model that extracts rationales for predictions without training on rationales. They compare their extracted rationales to human gold-standard ones through automated evaluations, i.e., precision and rec"
W19-4807,N18-1097,0,0.0933195,"ey compare their extracted rationales to human gold-standard ones through automated evaluations, i.e., precision and recall. Bao et al. (2018) extend this work by learning a mapping from the human rationales to continuous attention. They transfer this mapping to low resource target domains as an auxiliary training signal to improve classification accuracy in the target domain. They compare their learned attention with human rationales by calculating their cosine distance to the ‘oracle’ attention. None of the above related work asks human users to evaluate the generated explanations. However, Nguyen (2018) does compare human and automatic evaluations of explanations. That work finds that human evaluation is moderately, but statistically significantly, correlated with the automatic metrics. However, it does not evaluate any explanations based on attention, nor do the explanations make use of any extra human supervision. As mentioned above, there has also been some recent criticism of using attention as explanation (Jain and Wallace, 2019), due to a lack of correlation between the attention weights and gradient based methods which are more “faithful” to the model’s reasoning. However, attention w"
W19-4807,N16-3020,0,0.650072,". In this work, we have human judges evaluate both attention based machine explanations and machine explanations trained from human rationales, thus connecting learning from human explanations and machine explanations to humans. necessary element of explainability research, and in this work, we compare the explanations from the two models through human evaluation on Mechanical Turk and find that the model trained with human rationales is judged to generate explanations that better support its decisions. 2 Related Work There is a growing body of research on explainable AI (Koh and Liang, 2017; Ribeiro et al., 2016; Li et al., 2016; Hendricks et al., 2018), but it is not connected to work on learning with human rationales, which we review below. As discussed above, Zhang et al. (2016) demonstrate increased predictive accuracy of CNN models augmented with human rationales. Here, we first reproduce their predictive results, and then focus on extracting and evaluating explanations from the models. Lei et al. (2016) present a model that extracts rationales for predictions without training on rationales. They compare their extracted rationales to human gold-standard ones through automated evaluations, i.e.,"
W19-4807,N16-1174,0,0.0593212,"the model, and they also impact the training of the later features. Our contribution is to measure how useful these attention based explanations are to humans in understand3 3.1 Models and Dataset Models We replicate the work of Zhang et al. (2016) and use a CNN as our underlying baseline model for document classification. To model a document, each sentence is encoded as a sentence vector using a CNN, and then the document vector is formed by summing over the sentence vectors. We use two variations of this baseline model, a rationale-augmented CNN (RA-CNN) and an attention based CNN (AT-CNN) (Yang et al., 2016). RA-CNN is trained on both the document label and the rationale labels. In this model, the document vector is a weighted sum of the composite sentence vectors, where the weight is the probability of the sentence being a rationale. In AT-CNN, the document vector is still a weighted sum of sentence CNN vectors, but the weight is not learned from rationales. Rather, a trainable context vector is introduced from scratch. We calculate the interaction between this context vector and each sentence vector to induce attention weights over the sentences. The only difference between RACNN and AT-CNN is"
W19-4807,N07-1033,0,0.436234,"t learning with rationales can also improve the quality of the machine’s explanations as evaluated by human judges. Specifically, we present experiments showing that, for CNNbased text classification, explanations generated using “supervised attention” are judged superior to explanations generated using normal unsupervised attention. 1 Introduction Recently, the need for explainable artificial intelligence (XAI) has become a major concern due to the increased use of machine learning in automated decision making (Gunning, 2017; Aha, 2018). On the other hand, work on “learning with rationales” (Zaidan et al., 2007; Zhang et al., 2016) has shown that humans providing explanatory information supporting their supervised classification labels can improve the accuracy of machine learning. These human annotations that can explain classification labels are called rationales. In particular, for text categorization, humans select phrases or sentences from a document that most support their decision as rationales. However, there is no work connecting “learning from rationales” with improving XAI, although they are clearly complementary problems. Contribution We explore whether learning from human explanations ac"
W19-4807,D16-1076,1,0.948788,"tions and machine explanations to humans. necessary element of explainability research, and in this work, we compare the explanations from the two models through human evaluation on Mechanical Turk and find that the model trained with human rationales is judged to generate explanations that better support its decisions. 2 Related Work There is a growing body of research on explainable AI (Koh and Liang, 2017; Ribeiro et al., 2016; Li et al., 2016; Hendricks et al., 2018), but it is not connected to work on learning with human rationales, which we review below. As discussed above, Zhang et al. (2016) demonstrate increased predictive accuracy of CNN models augmented with human rationales. Here, we first reproduce their predictive results, and then focus on extracting and evaluating explanations from the models. Lei et al. (2016) present a model that extracts rationales for predictions without training on rationales. They compare their extracted rationales to human gold-standard ones through automated evaluations, i.e., precision and recall. Bao et al. (2018) extend this work by learning a mapping from the human rationales to continuous attention. They transfer this mapping to low resource"
W19-4807,N19-1357,0,\N,Missing
W96-0208,P95-1017,0,0.0584243,"Missing"
W96-0208,J93-1005,0,0.0161519,"thms rather than the space of possible input representations. Alternative encodings which exploit positional information, syntactic word tags, syntactic parse trees, semantic information, etc. should be tested to determine the utility of more sophisticated representations. In particular, it would be interesting to see if the accuracy ranking of the seven algorithms is affected by a change in the representation. Similar comparisons of a range of algorithms should also be performed on other natural language problems such as part-of-speech tagging (Church, 1988), prepositional phrase attachment (Hindle & Rooth, 1993), anaphora resolution (Anoe ~ Bennett, 1995), etc.. Since the requirements of individual tasks vary, different algorithms may be suitable for different sub-problems in natural language processing. 87 350 I I ~. 300 250 o ""¢: 0 0 0 ® ¢/) 200 I Naive Bales 3 Nearest Neighbor Perceptron C4.5 PFOIL-DNF PFOIL-CNF PFOIL-DLIST I I o --0--- J | -m----x...... | -~---~-'-~- -- ..-'"" "" 1 ..-""1"" 150 1O0 50 I ~ .G .................... [] .......................................... '~:~:::':""-;............~""............ii""21""2...................x.2~..~?.~?.~?..~.~.~..C~..~.~:..~.~.~?..2..~..~...2.2..Z/~..2:"
W96-0208,H93-1051,0,0.545748,"Lehman, 1994). However, in a comparison of neural-network and decision-tree methods on learning to generate the past tense of an English verb, decision trees performed significantly better (Ling & Marinov, 1993; Ling, 1994). Subsequent experiments on this problem have demonstrated that an inductive logic programming method produces even better results than decision trees (Mooney & Califf, 1995). In this paper, we present direct comparisons of a fairly wide range of general learning algorithms on the problem of discriminating six senses of the word ""line"" from context, using data assembled by Leacock et al. (1993b). We compare a naive Bayesian classifier (Duda & Hart, 1973), a perceptron (Rosenblatt, 1962), a decision-tree learner (Quinlan, 1993), a k nearest-neighbor classifier (Cover & Hart, 1967), logic-based DNF (disjunctive normal form) and CNF (conjunctive normal form) learners (Mooney, 1995) and a decisionlist learner (Rivest, 1987). Tests on all methods used identical training and test sets, and ten separate random trials were run in order to measure average performance and allow statistical testing of the significance of any observed differences. On this particular task, we found that the Bay"
W96-0208,W93-0102,0,0.3918,"Lehman, 1994). However, in a comparison of neural-network and decision-tree methods on learning to generate the past tense of an English verb, decision trees performed significantly better (Ling & Marinov, 1993; Ling, 1994). Subsequent experiments on this problem have demonstrated that an inductive logic programming method produces even better results than decision trees (Mooney & Califf, 1995). In this paper, we present direct comparisons of a fairly wide range of general learning algorithms on the problem of discriminating six senses of the word ""line"" from context, using data assembled by Leacock et al. (1993b). We compare a naive Bayesian classifier (Duda & Hart, 1973), a perceptron (Rosenblatt, 1962), a decision-tree learner (Quinlan, 1993), a k nearest-neighbor classifier (Cover & Hart, 1967), logic-based DNF (disjunctive normal form) and CNF (conjunctive normal form) learners (Mooney, 1995) and a decisionlist learner (Rivest, 1987). Tests on all methods used identical training and test sets, and ten separate random trials were run in order to measure average performance and allow statistical testing of the significance of any observed differences. On this particular task, we found that the Bay"
W96-0208,J93-2004,0,0.0619482,"Missing"
W96-0208,A88-1019,0,0.0303956,"ocused on exploring the space of possible algorithms rather than the space of possible input representations. Alternative encodings which exploit positional information, syntactic word tags, syntactic parse trees, semantic information, etc. should be tested to determine the utility of more sophisticated representations. In particular, it would be interesting to see if the accuracy ranking of the seven algorithms is affected by a change in the representation. Similar comparisons of a range of algorithms should also be performed on other natural language problems such as part-of-speech tagging (Church, 1988), prepositional phrase attachment (Hindle & Rooth, 1993), anaphora resolution (Anoe ~ Bennett, 1995), etc.. Since the requirements of individual tasks vary, different algorithms may be suitable for different sub-problems in natural language processing. 87 350 I I ~. 300 250 o ""¢: 0 0 0 ® ¢/) 200 I Naive Bales 3 Nearest Neighbor Perceptron C4.5 PFOIL-DNF PFOIL-CNF PFOIL-DLIST I I o --0--- J | -m----x...... | -~---~-'-~- -- ..-'"" "" 1 ..-""1"" 150 1O0 50 I ~ .G .................... [] .......................................... '~:~:::':""-;............~""............ii""21""2...................x.2~..~?"
W96-0208,P92-1032,0,0.32957,"Missing"
W96-0208,P92-1017,0,0.00508574,"number of different methods for learning from data. Three general approaches are statistical, neural-network, and symbolic machine learning and numerous specific methods have been developed under each of these paradigms (Wermter, Riloff, & Scheler, 1996; Charniak, 1993; Reilly & Sharkey, 1992). An important question is whether some methods perform significantly better than others on particular types of problems. Unfortunately, there have been very few direct comparisons of alternative methods on identical test data. A somewhat indirect comparison of applying stochastic context-free grammars (Periera & Shabes, 1992), a transformation-based method (Brill, 1993), and inductive logic programming (Zelle & Mooney, 1994) to parsing the ATIS (Airline Travel Information Service) corpus from the Penn Treebank (Marcus, Santorini, & Marcinkiewicz, 1993) indicates fairly similar performance for these three very different methods. Also, comparisons of Bayesian, informationBackground on Machine and Bias Learning Research in machine learning over the last ten years has been particularly concerned with experimental comparisons and the relative performance of different classification methods (Shavlik & Di82 etterich, 199"
W96-0208,P94-1013,0,0.316211,"orted in empirical natural-language research present only a single or very small number of triMs. Running multiple trials also allows for statistical testing of the significance of any resulting differences in average performance. We employ a simple two-tailed, paired t-test to compare the performance of two systems for a given trainingset size, requiring significance at the 0.05 level. Even more sophisticated statistical analysis of the results is perhaps warranted. Niblett, 1989; Quinlan, 1993; Mooney & Califf, 1995) and they have already been successfully applied to lexical disambiguation (Yarowsky, 1994). All of the logic-based methods are variations of the FOIL algorithm for induction of first-order function-free Horn clauses (Quinlan, 1990), appropriately simplified for the propositional case. They are called P F o I L - D N F , PFOlL-CNF, and PFoIL-DLIsT. The algorithms are greedy covering (separate-and-conquer) methods that use an information-theoretic heuristic to guide a topdown search for a simple definition consistent with the training data. P F o l L - D N F ( P F o l L - C N F ) learns a separate DNF (CNF) description for each sense using the examples of that sense as positive insta"
W96-0208,P93-1035,0,\N,Missing
W96-0208,H93-1047,0,\N,Missing
W98-1107,P91-1027,0,0.210038,"research is to automate lexicon construction 2 Background for an integrated NLP system that acquires semantic The output produced by WOLFm can be used to assist parsers and lexicons. A subgoal is to learn a lexicon that a larger language acquisition system; in particular, it is is as good or better than a manually-built one based on currently used as part of the input to a parser acquisition performance on a chosen task. system called CHILL (Constructive Heuristics Induction Although a few others (Siskind, 1996; Hastings and for Language Learning). CHILL uses inductive logic proLytinen, 1994; Brent, 1991) have presented systems for gramming (Muggleton, 1992; Lavra~ and D~eroski, 1994) semantic lexicon acquisition, this work is unique in comto learn a deterministic shift-reduce parser written in bining several features. First, interaction with a system, Abstract This paper describes a system, WOLFIE (WOrd Learning From Interpreted Examples), that acquires a semantic lexicon from a corpus of sentences paired with representations of their meaning. The lexicon learned consists of words paired with meaning representations. WOLFIE is part of an integrated system that learns to parse novel sentences"
W98-1107,P96-1042,0,0.0153993,"nt, while ours does. 7&quot; Future Work Although the current greedy search method has performed quite well, a better search heuristic or alternative search strategy could result in improvements. A more important issue is lessening the burden of building a large annotated training corpus. We are exploring two options in this regard. One is to use active learning (COhn et al., 1994) in which the system chooses which examples are most usefully annotated from a larger corpus of unannotated data. This approach can dramatically reduce the amount of annotated data required to achieve a desired accuracy (Engelson and Dagan, 1996). Second, we are currently developing a corpus of sentences paired with SQL database queries. Extending our system to handle this representation should be a fairly simple matter. Such corpora should be easily constructed by recording queries submitted to existing SQL applications along with their original English forms, or translating existing lists of SQL queries into English (presumably an easier direction to translate). The fact that the same training data can be used to learn both a semantic lexicon and a parser also helps limit the overall burden of constructing a complete NL interface. O"
W98-1107,H91-1026,0,0.0100869,"an initial concept hierarchy, and 63 His system proceeds in two stages, first learning what symbols are part of a word's meaning, and then learning the structure of those symbols. For example, it might first learn that c a p i t a l is part of the meaning of capital, then in the second stage learn that c a p i t a l can have either one or two arguments. By using common substructures, we can combine these two stages in WOLFIE. This work also has ties to the work on automatic construction of translation lexicons (Wu and Xia, 1995; Melamed, 1995; Kumano and Hirakawa, 1994; Catizone et al., 1993: Gale and Church, 1991). While most of these methods also compute association scores between pairs (in their case, word/word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One exception is Melamed (1996); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does. 7&quot; Future Work Although the current greedy search method has performed quite well, a better search heuristic or alternative search strategy could result in improvements. A more important issue is les"
W98-1107,C94-1009,0,0.0239234,"eatures of an unknown word. They assume access to an initial concept hierarchy, and 63 His system proceeds in two stages, first learning what symbols are part of a word's meaning, and then learning the structure of those symbols. For example, it might first learn that c a p i t a l is part of the meaning of capital, then in the second stage learn that c a p i t a l can have either one or two arguments. By using common substructures, we can combine these two stages in WOLFIE. This work also has ties to the work on automatic construction of translation lexicons (Wu and Xia, 1995; Melamed, 1995; Kumano and Hirakawa, 1994; Catizone et al., 1993: Gale and Church, 1991). While most of these methods also compute association scores between pairs (in their case, word/word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One exception is Melamed (1996); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does. 7&quot; Future Work Although the current greedy search method has performed quite well, a better search heuristic or alternative search strategy could result"
W98-1107,W95-0115,0,0.01325,"and semantic features of an unknown word. They assume access to an initial concept hierarchy, and 63 His system proceeds in two stages, first learning what symbols are part of a word's meaning, and then learning the structure of those symbols. For example, it might first learn that c a p i t a l is part of the meaning of capital, then in the second stage learn that c a p i t a l can have either one or two arguments. By using common substructures, we can combine these two stages in WOLFIE. This work also has ties to the work on automatic construction of translation lexicons (Wu and Xia, 1995; Melamed, 1995; Kumano and Hirakawa, 1994; Catizone et al., 1993: Gale and Church, 1991). While most of these methods also compute association scores between pairs (in their case, word/word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One exception is Melamed (1996); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does. 7&quot; Future Work Although the current greedy search method has performed quite well, a better search heuristic or alternative s"
W98-1107,1996.amta-1.13,0,0.014669,"d stage learn that c a p i t a l can have either one or two arguments. By using common substructures, we can combine these two stages in WOLFIE. This work also has ties to the work on automatic construction of translation lexicons (Wu and Xia, 1995; Melamed, 1995; Kumano and Hirakawa, 1994; Catizone et al., 1993: Gale and Church, 1991). While most of these methods also compute association scores between pairs (in their case, word/word pairs) and use a greedy algorithm to choose the best translation(s) for each word, they do not take advantage of the constraints between pairs. One exception is Melamed (1996); however, his approach does not allow for phrases in the lexicon or for synonymy within one text segment, while ours does. 7&quot; Future Work Although the current greedy search method has performed quite well, a better search heuristic or alternative search strategy could result in improvements. A more important issue is lessening the burden of building a large annotated training corpus. We are exploring two options in this regard. One is to use active learning (COhn et al., 1994) in which the system chooses which examples are most usefully annotated from a larger corpus of unannotated data. This"
W98-1107,W97-0313,0,0.0338656,"from the possible meanings of a (unique) phrase appearing in the sentence. If such a lexicon is found, we say that the lexicon covers the corpus. We will also talk about the coverage of components of a representation (or sentence/representation pair) by a lexicon entry. Ideally, we would like to minimize the ambiguity and size of the learned lexicon, since this should ease the parser acquisition task. Note that this notion of semantic lexicon acquisition is distinct from work on learning selectional restrictions (Manning, 1993; Brent, 1991) and learning clusters of semantically similar words (Riloff and Sheperd, 1997). Note that we allow phrases to have multiple meanings (homonymy) and for multiple phrases to have the same meaning (synonymy). Also, some phrases in the sentences may have a null meaning. We make only a few fairly straightforward assumptions about the input. First is compositionality, i.e. the meaning of a sentence is c o m p o s e d from the meanings of phrases in that sentence. Since we allow multi-word phrases in the lexicon (e.g. ([kick t h e b u c k e t ] , die(_))), this assumption seems fairly unproblematic. Second, we assume each component of the representation is due to the meaning o"
W98-1107,P93-1032,0,\N,Missing
W98-1107,P89-1022,0,\N,Missing
