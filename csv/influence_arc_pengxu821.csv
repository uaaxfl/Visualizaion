2012.amta-papers.18,2010.amta-papers.16,0,0.292091,"em to handle different domains. We study this approach with two domains (generic and patent), carry out large-scale experiments for 20 language pairs, demonstrating the viability of our approach. The rest of the paper is organized as follows. Section 2 describes related work. Section 3 presents our domain adaptation approach. Section 4 explains our method to classify an input sentence into its domain. Sections 5 and 6 talk about genre-aware decoding and tuning. We present experiment results in Section 7 and conclude in Section 8. 2 Related Work The work of Xu et al. (2007) and Banerjee et al. (2010) are perhaps the most relevant to our work. Xu et al. (2007) adapt a shared, generic translation model for better web or broadcast conversation translations and use a source-document classifier to classify an input document into a domain. This work makes the translation model shared across different domains, but domain-specific training data is not used and thus its impact is not studied. Neither did they report the generic MT accuracy. Banerjee et al. (2010) use source-sentence classification to combine two separate domain models, each trained from small amounts of domain-specific data obtain"
2012.amta-papers.18,D08-1064,0,0.129598,"optimization problem, this objective does not explicitly take genre into account. However, the weights of different domains, {λdi , d = 1 . . . D, i = 1 . . . Id }, are decoupled (by the runtime feature name re-labeling (Section 3)) in the n-best lists, rather than being shared, thus the weight optimization for one domain can concentrate on the domain itself, without being constrained too much by any other domain. Since BLEU is not decomposable at the sentence level, this objective generally can not guarantee maximized BLEUs on respective domain partitions, but rather an optimal overall BLEU (Chiang et al., 2008). Another optimization objective is the max BLEU sum that maximizes the sum of BLEUs of the individual genre partitions: ! D X BLEU(Sd ) (6) max {λdi ,d=1...D,i=1...Id } d=1 When none of the features is shared in the n-best lists across different domains (which is our case), this objective is equivalent to the summation of the individually maximized BLEUs: ! D X max BLEU(Sd ) (7) d=1 {λdi ,i=1...Id } There could be other genre-aware tuning objectives that mix (or “nest”) the above two.1 But in our paper, we are mainly interested in the experimental comparison between the max joint BLEU and the"
2012.amta-papers.18,P11-2080,0,0.0600089,"is no explicit treatment on genre, but there is a subtlety: Features are decoupled in n-best lists across domains and this gives MERT the freedom to adjust weights for one domain without much constraints from any other domain. In other words, MERT is made implicitly genre-aware. Actually, we can further tailor the MERT tuning objectives to be explicitly aware of genre. We explain alternative genre-aware tuning objectives that the classical MERT can be altered to adopt in a later section and compare their effects experimentally. Daume III (2007) does domain adaptation by augumenting features. Chiang et al. (2011) improve lexical smoothing also by augmenting features refined by genres. In our approach, the n-best lists for tuning can be viewed as containing augmented features as well. But we augment features in order to decouple them across domains, rather than providing refined domain/genre bias in the model. The language model feature deserves further explanation. Even though we do not have domain specific features in the translation model, we have domain specific language models. In our approach, the generic language model is used by different domains, but a domain language model is turned on only i"
2012.amta-papers.18,P07-1033,0,0.262823,"Missing"
2012.amta-papers.18,W07-0717,0,0.271504,"n system that is capable of producing adapted domain translations and preserving the original generic accuracy at the same time. The approach unifies automatic domain detection and domain model parameterization into one system. Experiment results on 20 language pairs demonstrate its viability. 1 Introduction Research in domain adaption for machine translation (MT) has been mostly focusing on one domain. Various methods have been proposed to make a system work best on a resource-scarce domain when most of the training data is from another open, resource-rich domain, e.g., (Foster et al., 2010; Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Decent improvements have been made on domain translation accuracy, but often, accuracy improvements for one domain are obtained at the expense of accuracy losses in another (e.g., the background) domain. With these methods, it remains unaddressed how they can be generalized to work equally well with more than one domains at the same time. One could trivially build one system/model per domain, but that does not scale and will require manual domain detection if the incoming texts belong to heterogeneous domains. So far, there has been little work on better infrastru"
2012.amta-papers.18,D10-1044,0,0.18498,"em to handle different domains. We study this approach with two domains (generic and patent), carry out large-scale experiments for 20 language pairs, demonstrating the viability of our approach. The rest of the paper is organized as follows. Section 2 describes related work. Section 3 presents our domain adaptation approach. Section 4 explains our method to classify an input sentence into its domain. Sections 5 and 6 talk about genre-aware decoding and tuning. We present experiment results in Section 7 and conclude in Section 8. 2 Related Work The work of Xu et al. (2007) and Banerjee et al. (2010) are perhaps the most relevant to our work. Xu et al. (2007) adapt a shared, generic translation model for better web or broadcast conversation translations and use a source-document classifier to classify an input document into a domain. This work makes the translation model shared across different domains, but domain-specific training data is not used and thus its impact is not studied. Neither did they report the generic MT accuracy. Banerjee et al. (2010) use source-sentence classification to combine two separate domain models, each trained from small amounts of domain-specific data obtain"
2012.amta-papers.18,W07-0733,0,0.499117,"e of producing adapted domain translations and preserving the original generic accuracy at the same time. The approach unifies automatic domain detection and domain model parameterization into one system. Experiment results on 20 language pairs demonstrate its viability. 1 Introduction Research in domain adaption for machine translation (MT) has been mostly focusing on one domain. Various methods have been proposed to make a system work best on a resource-scarce domain when most of the training data is from another open, resource-rich domain, e.g., (Foster et al., 2010; Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Decent improvements have been made on domain translation accuracy, but often, accuracy improvements for one domain are obtained at the expense of accuracy losses in another (e.g., the background) domain. With these methods, it remains unaddressed how they can be generalized to work equally well with more than one domains at the same time. One could trivially build one system/model per domain, but that does not scale and will require manual domain detection if the incoming texts belong to heterogeneous domains. So far, there has been little work on better infrastructure for building and deplo"
2012.amta-papers.18,N03-1017,1,0.0230989,"Missing"
2012.amta-papers.18,W04-3250,0,0.165773,"Missing"
2012.amta-papers.18,D08-1076,1,0.891121,"Missing"
2012.amta-papers.18,P03-1021,1,0.022694,") d(f ) eˆ = arg max λi · hi (f, e) (2) e i=1 Generalizing decoding for genre awareness in turn makes tuning genre-aware. Our tuning development set consists of sentences from D domains. And we do not need to know the domain for each sentence beforehand. The genre-aware decoding on the entire set automatically classifies it into D partitions. The runtime feature re-labeling makes each partition, Sd , d = 0, . . . , D − 1, have its own set of features that are decoupled from other domains. Therefore the system has D sets of features in total. We then can use Minimum Error Rate Training (MERT) (Och, 2003) out of the box to learn the weights for all these features in a single MERT run which maximizes the overall BLEU of the entire genre-mixed development set: max {λdi ,d=1...D,i=1...Id } BLEU(∪D−1 d=0 Sd ) (3) In this formula, there is no explicit treatment on genre, but there is a subtlety: Features are decoupled in n-best lists across domains and this gives MERT the freedom to adjust weights for one domain without much constraints from any other domain. In other words, MERT is made implicitly genre-aware. Actually, we can further tailor the MERT tuning objectives to be explicitly aware of gen"
2012.amta-papers.18,2007.mtsummit-papers.68,0,0.458729,"in detector, to generalize an MT system to handle different domains. We study this approach with two domains (generic and patent), carry out large-scale experiments for 20 language pairs, demonstrating the viability of our approach. The rest of the paper is organized as follows. Section 2 describes related work. Section 3 presents our domain adaptation approach. Section 4 explains our method to classify an input sentence into its domain. Sections 5 and 6 talk about genre-aware decoding and tuning. We present experiment results in Section 7 and conclude in Section 8. 2 Related Work The work of Xu et al. (2007) and Banerjee et al. (2010) are perhaps the most relevant to our work. Xu et al. (2007) adapt a shared, generic translation model for better web or broadcast conversation translations and use a source-document classifier to classify an input document into a domain. This work makes the translation model shared across different domains, but domain-specific training data is not used and thus its impact is not studied. Neither did they report the generic MT accuracy. Banerjee et al. (2010) use source-sentence classification to combine two separate domain models, each trained from small amounts of"
2012.amta-papers.18,N09-1028,1,0.895385,"Missing"
2020.acl-main.3,Q17-1010,0,0.0209387,"ellaneous). 4.2 200, which would output the same dimension as the concatenated word-level and char-level embeddings. We use Adam optimizer with a learning rate of 0.0005. Cross-entropy loss is leveraged to train the 3-way classification in the first step, and the specific slot type predictions are used in the second step. We split 500 data samples in the target domain as the validation set for choosing the best model and the remainder are used for the test set. We implement the model in CT and RZT and follow the same setting as for our model for a fair comparison. Baselines We use word-level (Bojanowski et al., 2017) and character-level (Hashimoto et al., 2017) embeddings for our model as well as all the following baselines. 5 5.1 Cross-domain Slot Filling Quantitative Analysis As illustrated in Table 1, we can clearly see that our models are able to achieve significantly better performance than the current state-of-the-art approach (RZT). The CT framework suffers from the difficulty of capturing the whole slot entity, while our framework is able to recognize the slot entity tokens by sharing its parameters across all slot types. Based on the CT framework, the performance of RZT is still limited, and Coac"
2020.acl-main.3,P19-1236,0,0.208583,"-shot scenarios. In addition, further experiments show that our framework can be applied to cross-domain named entity recognition, and achieves better adaptation performance than other existing frameworks. Guerini et al., 2018). Coping with low-resource problems where there are zero or few existing training samples has always been an interesting and challenging task (Kingma et al., 2014; Lample et al., 2018; Liu et al., 2019a,b; Lin et al., 2020). Cross-domain adaptation addresses the data scarcity problem in low-resource target domains (Pan et al., 2010; Jaech et al., 2016; Guo et al., 2018; Jia et al., 2019; Liu et al., 2020; Winata et al., 2020). However, most research studying the cross-domain aspect has not focused on predicting unseen label types in the target domain since both source and target domains have the same label types in the considered tasks (Guo et al., 2018). In another line of work, to bypass unseen label types, Ruder and Plank (2018) and Jia et al. (2019) utilized target domain training samples, so that there was no unseen label type in the target domain. Recently, based on the framework proposed by Bapna et al. (2017) (discussed in Section 1), Lee and Jha (2019) added an atte"
2020.acl-main.3,N06-1022,0,0.0247609,"sks (Guo et al., 2018). In another line of work, to bypass unseen label types, Ruder and Plank (2018) and Jia et al. (2019) utilized target domain training samples, so that there was no unseen label type in the target domain. Recently, based on the framework proposed by Bapna et al. (2017) (discussed in Section 1), Lee and Jha (2019) added an attention layer to produce slot-aware representations, and Shah et al. (2019) leveraged slot examples to increase the robustness of cross-domain slot filling adaptation. 2 3 Related Work Coarse-to-fine methods in NLP are best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Zhang et al. (2017) reduced the search space of semantic parsers by using coarse macro grammars. Different from the previous work, we apply the idea of coarse-to-fine into cross-domain slot filling to handle unseen slot types by separating the slot filling task into two steps (Zhai et al., 2017; 3.1 Methodology Coach Framework As depicted in Figure 2, the slot filling process in our Coach framework consists of two steps. In the first step, we utilize a BiLSTM-CRF structure (Lample et al., 2016) to learn the general pattern of slot entities by having our model predict whether t"
2020.acl-main.3,N16-1030,0,0.43454,"3 Related Work Coarse-to-fine methods in NLP are best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Zhang et al. (2017) reduced the search space of semantic parsers by using coarse macro grammars. Different from the previous work, we apply the idea of coarse-to-fine into cross-domain slot filling to handle unseen slot types by separating the slot filling task into two steps (Zhai et al., 2017; 3.1 Methodology Coach Framework As depicted in Figure 2, the slot filling process in our Coach framework consists of two steps. In the first step, we utilize a BiLSTM-CRF structure (Lample et al., 2016) to learn the general pattern of slot entities by having our model predict whether tokens are slot entities or not (i.e., 20 3-way classification for each token). In the second step, our model further predicts a specific type for each slot entity based on the similarities with the description representations of all possible slot types. To generate representations of slot entities, we leverage another encoder, BiLSTM (Hochreiter and Schmidhuber, 1997), to encode the hidden states of slot entity tokens and produce representations for each slot entity. We represent the user utterance with n token"
2020.acl-main.3,D17-1169,0,0.0457974,"slot entity based on the similarities with the description representations of all possible slot types. To generate representations of slot entities, we leverage another encoder, BiLSTM (Hochreiter and Schmidhuber, 1997), to encode the hidden states of slot entity tokens and produce representations for each slot entity. We represent the user utterance with n tokens as w = [w1 , w2 , ..., wn ], and E denotes the embedding layer for utterances. The whole process can be formulated as follows: labels to generate correct and incorrect utterance templates. Then, we use BiLSTM and an attention layer (Felbo et al., 2017) to generate the utterance and template representations: [h1 , h2 , ..., hn ] = BiLSTM(E(w)), (1) n X exp(et ) et = ht wa , αt = Pn , R= αt ht , j=1 exp(ej ) t=1 (6) th where ht is the BiLSTM hidden state in the t step, wa is the weight vector in the attention layer and R is the representation for the input utterance or template. We minimize the regularization loss functions for the right and wrong templates, which can be formulated as follows: [p1 , p2 , ..., pn ] = CRF([h1 , h2 , ..., hn ]), (2) Lr = MSE(Ru , Rr ), (7) Lw = −β × MSE(Ru , Rw ), (8) where [p1 , p2 , ..., pn ] are the logits fo"
2020.acl-main.3,J82-2005,0,0.502301,"Missing"
2020.acl-main.3,N18-2118,0,0.104523,"se additional trouble for the final prediction. We emphasize that in order to capture the whole slot entity, it is pivotal for the model to share its parameters for all slot types in the source domains and learn the general pattern of slot entities. Therefore, as depicted in Figure 1b, we propose a new cross-domain slot filling framework called Coach, Introduction Slot filling models identify task-related slot types in certain domains for user utterances, and are an indispensable part of task-oriented dialog systems. Supervised approaches have made great achievements in the slot filling task (Goo et al., 2018; Zhang et al., 2019), where substantial labeled training samples are needed. However, collecting large numbers of training samples is not only expensive but also time-consuming. To cope with the data scarcity issue, we are motivated to investigate cross-domain slot filling methods, which leverage knowledge learned in the source domains and adapt the models to the target domain with a minimum number of target domain labeled training samples. A challenge in cross-domain slot filling is to handle unseen slot types, which prevents general 19 Proceedings of the 58th Annual Meeting of the Associati"
2020.acl-main.3,W18-5036,0,0.0187586,"templates to regularize the utterance representations. By doing so, the model learns to cluster the representations of semantically similar utterances (i.e., in the same or similar templates) into a similar vector space, which further improves the adaptation robustness. Experimental results show that our model surpasses the state-of-the-art methods by a large margin in both zero-shot and few-shot scenarios. In addition, further experiments show that our framework can be applied to cross-domain named entity recognition, and achieves better adaptation performance than other existing frameworks. Guerini et al., 2018). Coping with low-resource problems where there are zero or few existing training samples has always been an interesting and challenging task (Kingma et al., 2014; Lample et al., 2018; Liu et al., 2019a,b; Lin et al., 2020). Cross-domain adaptation addresses the data scarcity problem in low-resource target domains (Pan et al., 2010; Jaech et al., 2016; Guo et al., 2018; Jia et al., 2019; Liu et al., 2020; Winata et al., 2020). However, most research studying the cross-domain aspect has not focused on predicting unseen label types in the target domain since both source and target domains have t"
2020.acl-main.3,D19-1129,1,0.734398,"Missing"
2020.acl-main.3,D18-1498,0,0.0659828,"Missing"
2020.acl-main.3,2020.repl4nlp-1.1,1,0.826185,"n addition, further experiments show that our framework can be applied to cross-domain named entity recognition, and achieves better adaptation performance than other existing frameworks. Guerini et al., 2018). Coping with low-resource problems where there are zero or few existing training samples has always been an interesting and challenging task (Kingma et al., 2014; Lample et al., 2018; Liu et al., 2019a,b; Lin et al., 2020). Cross-domain adaptation addresses the data scarcity problem in low-resource target domains (Pan et al., 2010; Jaech et al., 2016; Guo et al., 2018; Jia et al., 2019; Liu et al., 2020; Winata et al., 2020). However, most research studying the cross-domain aspect has not focused on predicting unseen label types in the target domain since both source and target domains have the same label types in the considered tasks (Guo et al., 2018). In another line of work, to bypass unseen label types, Ruder and Plank (2018) and Jia et al. (2019) utilized target domain training samples, so that there was no unseen label type in the target domain. Recently, based on the framework proposed by Bapna et al. (2017) (discussed in Section 1), Lee and Jha (2019) added an attention layer to pro"
2020.acl-main.3,D17-1206,0,0.0410278,"Missing"
2020.acl-main.3,P18-1096,0,0.0174431,"g and challenging task (Kingma et al., 2014; Lample et al., 2018; Liu et al., 2019a,b; Lin et al., 2020). Cross-domain adaptation addresses the data scarcity problem in low-resource target domains (Pan et al., 2010; Jaech et al., 2016; Guo et al., 2018; Jia et al., 2019; Liu et al., 2020; Winata et al., 2020). However, most research studying the cross-domain aspect has not focused on predicting unseen label types in the target domain since both source and target domains have the same label types in the considered tasks (Guo et al., 2018). In another line of work, to bypass unseen label types, Ruder and Plank (2018) and Jia et al. (2019) utilized target domain training samples, so that there was no unseen label type in the target domain. Recently, based on the framework proposed by Bapna et al. (2017) (discussed in Section 1), Lee and Jha (2019) added an attention layer to produce slot-aware representations, and Shah et al. (2019) leveraged slot examples to increase the robustness of cross-domain slot filling adaptation. 2 3 Related Work Coarse-to-fine methods in NLP are best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Zhang et al. (2017) reduced the search space of semantic parser"
2020.acl-main.3,P19-1547,0,0.128913,"research studying the cross-domain aspect has not focused on predicting unseen label types in the target domain since both source and target domains have the same label types in the considered tasks (Guo et al., 2018). In another line of work, to bypass unseen label types, Ruder and Plank (2018) and Jia et al. (2019) utilized target domain training samples, so that there was no unseen label type in the target domain. Recently, based on the framework proposed by Bapna et al. (2017) (discussed in Section 1), Lee and Jha (2019) added an attention layer to produce slot-aware representations, and Shah et al. (2019) leveraged slot examples to increase the robustness of cross-domain slot filling adaptation. 2 3 Related Work Coarse-to-fine methods in NLP are best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Zhang et al. (2017) reduced the search space of semantic parsers by using coarse macro grammars. Different from the previous work, we apply the idea of coarse-to-fine into cross-domain slot filling to handle unseen slot types by separating the slot filling task into two steps (Zhai et al., 2017; 3.1 Methodology Coach Framework As depicted in Figure 2, the slot filling process in ou"
2020.acl-main.3,P19-1519,0,0.0458009,"ble for the final prediction. We emphasize that in order to capture the whole slot entity, it is pivotal for the model to share its parameters for all slot types in the source domains and learn the general pattern of slot entities. Therefore, as depicted in Figure 1b, we propose a new cross-domain slot filling framework called Coach, Introduction Slot filling models identify task-related slot types in certain domains for user utterances, and are an indispensable part of task-oriented dialog systems. Supervised approaches have made great achievements in the slot filling task (Goo et al., 2018; Zhang et al., 2019), where substantial labeled training samples are needed. However, collecting large numbers of training samples is not only expensive but also time-consuming. To cope with the data scarcity issue, we are motivated to investigate cross-domain slot filling methods, which leverage knowledge learned in the source domains and adapt the models to the target domain with a minimum number of target domain labeled training samples. A challenge in cross-domain slot filling is to handle unseen slot types, which prevents general 19 Proceedings of the 58th Annual Meeting of the Association for Computational"
2020.acl-main.3,D17-1125,0,0.0171551,"e of work, to bypass unseen label types, Ruder and Plank (2018) and Jia et al. (2019) utilized target domain training samples, so that there was no unseen label type in the target domain. Recently, based on the framework proposed by Bapna et al. (2017) (discussed in Section 1), Lee and Jha (2019) added an attention layer to produce slot-aware representations, and Shah et al. (2019) leveraged slot examples to increase the robustness of cross-domain slot filling adaptation. 2 3 Related Work Coarse-to-fine methods in NLP are best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Zhang et al. (2017) reduced the search space of semantic parsers by using coarse macro grammars. Different from the previous work, we apply the idea of coarse-to-fine into cross-domain slot filling to handle unseen slot types by separating the slot filling task into two steps (Zhai et al., 2017; 3.1 Methodology Coach Framework As depicted in Figure 2, the slot filling process in our Coach framework consists of two steps. In the first step, we utilize a BiLSTM-CRF structure (Lample et al., 2016) to learn the general pattern of slot entities by having our model predict whether tokens are slot entities or not (i.e."
2020.acl-main.3,W03-0419,0,\N,Missing
2020.acl-main.348,P13-2037,0,0.0337159,"uces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic features on recurrent neural networks (RNNs). Baheti et al. (2017) adapts effective curriculum learning by training a network Algorithm 1 Meta-Transfer Learning Require: Dsrc , Dtgt Require: α, β: step size hyperparameters 1: Randomly initialize θ 2: while not done do 3: Sample batch data Dtra ∼ (Dsrc , Dtgt ), Dval ∼ Dtgt 4: for all DTtra ∈ Dtra do i 5: Evaluate ∇θ LDTtra (fθ ) using DTtra i i 6: Compute adapted parameters with gradient descent: θT0 i = θ − α∇θ LDTtra (fθ ) i 7: end for P   8: θ ← θ − β i ∇θ LDval fθT0 i 9: end while with monolingual corp"
2020.acl-main.348,W17-7509,0,0.0228281,"propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic features on recurrent neural networks (RNNs). Baheti et al. (2017) adapts effective curriculum learning by training a network Algorithm 1 Meta-Transfer Learning Require: Dsrc , Dtgt Require: α, β: step size hyperparameters 1: Randomly initialize θ 2: while not done do 3: Sample batch data Dtra ∼ (Dsrc , Dtgt ), Dval ∼ Dtgt 4: for all DTtra ∈ Dtra do i 5: Evaluate ∇θ LDTtra (fθ ) using DTtra i i 6: Compute adapted parameters with gradient descent: θT0 i = θ − α∇θ LDTtra (fθ ) i 7: end for P   8: θ ← θ − β i ∇θ LDval fθT0 i 9: end while with monolingual corpora of two languages, and subsequently training on code-switched data. Pratapa et al. (2018) and Lee e"
2020.acl-main.348,D18-1346,0,0.0142325,"e ∇θ LDTtra (fθ ) using DTtra i i 6: Compute adapted parameters with gradient descent: θT0 i = θ − α∇θ LDTtra (fθ ) i 7: end for P   8: θ ← θ − β i ∇θ LDval fθT0 i 9: end while with monolingual corpora of two languages, and subsequently training on code-switched data. Pratapa et al. (2018) and Lee et al. (2019) propose to use methods to generate artificial code-switching data using a linguistic constraint. Winata et al. (2018) proposes to leverage syntactic information to improve the identification of the location of code-switching points, and improve the language model performance. Finally Garg et al. (2018) and Winata et al. (2019) propose new neural-based methods using SeqGAN and pointer-generator (Pointer-Gen) to generate diverse synthetic codeswitching sentences that are sampled from the real code-switching data distribution. 3 Meta-Transfer Learning We aim to effectively transfer knowledge from source domains to a specific target domain. We denote our model by fθ with parameters θ. Our model accepts a set of speech inputs X = {x1 , . . . , xn } and generates a set of utterances Y = {y1 , . . . , ym }. The training involves a set of speech datasets in which each dataset is treated as a task T"
2020.acl-main.348,D18-1398,0,0.0637116,"pplicable to other natural language tasks, such as code-switching language modeling tasks. 2 Related Work Meta-learning Our idea of learning knowledge transfer from source monolingual resources to a code-switching model comes from MAML (Finn et al., 2017). Probabilistic MAML (Finn et al., 2018) is an extension of MAML, which has better classification coverage. Meta-learning has been applied to natural language and speech processing (Hospedales et al., 2020). Madotto et al. (2019) extends MAML to the personalized text generation domain and successfully produces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic f"
2020.acl-main.348,C12-1102,1,0.753256,"h processing (Hospedales et al., 2020). Madotto et al. (2019) extends MAML to the personalized text generation domain and successfully produces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic features on recurrent neural networks (RNNs). Baheti et al. (2017) adapts effective curriculum learning by training a network Algorithm 1 Meta-Transfer Learning Require: Dsrc , Dtgt Require: α, β: step size hyperparameters 1: Randomly initialize θ 2: while not done do 3: Sample batch data Dtra ∼ (Dsrc , Dtgt ), Dval ∼ Dtgt 4: for all DTtra ∈ Dtra do i 5: Evaluate ∇θ LDTtra (fθ ) using DTtra i i 6: Compute adapted parameter"
2020.acl-main.348,W19-5508,1,0.781035,"uch as code-switching language modeling tasks. 2 Related Work Meta-learning Our idea of learning knowledge transfer from source monolingual resources to a code-switching model comes from MAML (Finn et al., 2017). Probabilistic MAML (Finn et al., 2018) is an extension of MAML, which has better classification coverage. Meta-learning has been applied to natural language and speech processing (Hospedales et al., 2020). Madotto et al. (2019) extends MAML to the personalized text generation domain and successfully produces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic features on recurrent neural networks (RNNs)."
2020.acl-main.348,P19-1542,1,0.842392,"veness of our approach in terms of error rate, and that our approach is also faster to converge. We also show that our approach is also applicable to other natural language tasks, such as code-switching language modeling tasks. 2 Related Work Meta-learning Our idea of learning knowledge transfer from source monolingual resources to a code-switching model comes from MAML (Finn et al., 2017). Probabilistic MAML (Finn et al., 2018) is an extension of MAML, which has better classification coverage. Meta-learning has been applied to natural language and speech processing (Hospedales et al., 2020). Madotto et al. (2019) extends MAML to the personalized text generation domain and successfully produces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorpo"
2020.acl-main.348,P18-1143,0,0.0972363,"ks (RNNs). Baheti et al. (2017) adapts effective curriculum learning by training a network Algorithm 1 Meta-Transfer Learning Require: Dsrc , Dtgt Require: α, β: step size hyperparameters 1: Randomly initialize θ 2: while not done do 3: Sample batch data Dtra ∼ (Dsrc , Dtgt ), Dval ∼ Dtgt 4: for all DTtra ∈ Dtra do i 5: Evaluate ∇θ LDTtra (fθ ) using DTtra i i 6: Compute adapted parameters with gradient descent: θT0 i = θ − α∇θ LDTtra (fθ ) i 7: end for P   8: θ ← θ − β i ∇θ LDval fθT0 i 9: end while with monolingual corpora of two languages, and subsequently training on code-switched data. Pratapa et al. (2018) and Lee et al. (2019) propose to use methods to generate artificial code-switching data using a linguistic constraint. Winata et al. (2018) proposes to leverage syntactic information to improve the identification of the location of code-switching points, and improve the language model performance. Finally Garg et al. (2018) and Winata et al. (2019) propose new neural-based methods using SeqGAN and pointer-generator (Pointer-Gen) to generate diverse synthetic codeswitching sentences that are sampled from the real code-switching data distribution. 3 Meta-Transfer Learning We aim to effectively"
2020.acl-main.348,P19-1253,0,0.0234108,"tural language tasks, such as code-switching language modeling tasks. 2 Related Work Meta-learning Our idea of learning knowledge transfer from source monolingual resources to a code-switching model comes from MAML (Finn et al., 2017). Probabilistic MAML (Finn et al., 2018) is an extension of MAML, which has better classification coverage. Meta-learning has been applied to natural language and speech processing (Hospedales et al., 2020). Madotto et al. (2019) extends MAML to the personalized text generation domain and successfully produces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic features on recurrent ne"
2020.acl-main.348,W18-3207,1,0.87279,"Dtgt Require: α, β: step size hyperparameters 1: Randomly initialize θ 2: while not done do 3: Sample batch data Dtra ∼ (Dsrc , Dtgt ), Dval ∼ Dtgt 4: for all DTtra ∈ Dtra do i 5: Evaluate ∇θ LDTtra (fθ ) using DTtra i i 6: Compute adapted parameters with gradient descent: θT0 i = θ − α∇θ LDTtra (fθ ) i 7: end for P   8: θ ← θ − β i ∇θ LDval fθT0 i 9: end while with monolingual corpora of two languages, and subsequently training on code-switched data. Pratapa et al. (2018) and Lee et al. (2019) propose to use methods to generate artificial code-switching data using a linguistic constraint. Winata et al. (2018) proposes to leverage syntactic information to improve the identification of the location of code-switching points, and improve the language model performance. Finally Garg et al. (2018) and Winata et al. (2019) propose new neural-based methods using SeqGAN and pointer-generator (Pointer-Gen) to generate diverse synthetic codeswitching sentences that are sampled from the real code-switching data distribution. 3 Meta-Transfer Learning We aim to effectively transfer knowledge from source domains to a specific target domain. We denote our model by fθ with parameters θ. Our model accepts a set of"
2020.acl-main.348,K19-1026,1,0.902121,"matrix language. This can occur within a sentence, which is known as intrasentential code-switching or between two matrix language sentences, which is called inter-sentential code-switching (Heredia and Altarriba, 2001). Learning a code-switching automatic speech recognition (ASR) model has been a challenging task for decades due to data scarcity and difficulty in capturing similar phonemes in different These two authors contributed equally. ∇ ∇ Introduction ∗ ) languages. Several approaches have focused on generating synthetic speech data from monolingual resources (Nakayama et al., 2018; Winata et al., 2019). However, these methods are not guaranteed to generate natural code-switching speech or text. Another line of work explores the feasibility of leveraging large monolingual speech data in the pre-training and applying fine-tuning on the model using a limited source of code-switching data, which has been found useful to improve the performance (Li et al., 2011; Winata et al., 2019). However, the transferability of these pretraining approaches is not optimized on extracting useful knowledge from each individual languages in the context of code-switching, and even after the finetuning step, the m"
2020.acl-main.348,2020.acl-main.336,0,0.0335127,"Our idea of learning knowledge transfer from source monolingual resources to a code-switching model comes from MAML (Finn et al., 2017). Probabilistic MAML (Finn et al., 2018) is an extension of MAML, which has better classification coverage. Meta-learning has been applied to natural language and speech processing (Hospedales et al., 2020). Madotto et al. (2019) extends MAML to the personalized text generation domain and successfully produces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic features on recurrent neural networks (RNNs). Baheti et al. (2017) adapts effective curriculum learning by training a ne"
2020.emnlp-main.226,speer-havasi-2012-representing,0,0.0180632,"stafazadeh et al., 2016) for our experiments. It consists of 98,161 stories, where each story contains five sentences. 88,344/4,908/4,909 stories are used for train/validation/test sets, respectively. Following Guan et al. (2020), for each sentence, delexicalization is performed by replacing all the names and entities in stories with special placeholders, [MALE], [FEMALE], and [NEUTRAL] for male, female and unknown names and entities, respectively. Given the first sentence of each story, our model’s task is to generate the rest of the story. For our external knowledge base, we use ConceptNet (Speer and Havasi, 2012), consists of 600k knowledge triples. the same settings. To train our contextual knowledge ranker, we set the margin to 5.0. We set the number of knowledge sentences in Ri to 10. Therefore, for a given story context, the top 10 retrieved knowledge sentences from ConceptNet according to U SE are chosen as the positive samples. We further select 40 negative samples to compute our margin loss. We then randomly sample 50 (positive, negative) pairs for each story context to train our contextual knowledge ranker. In total, we used ∼15 million pairs for training and ∼1 million pairs for validation. A"
2020.emnlp-main.226,P18-1205,0,0.0354557,"ting commonsense knowledge into story generation with attention-based models (Guan et al., 2019; Chen et al., 2019). Recently, pre-trained language models have been used to finetune on both story completion datasets and commonsense knowledge to further improve the quality of story completion (Guan et al., 2020). However, few works concern the controllability of language model generation, especially for the large pre-trained models that are common in today’s literature. Controllable Generation Controllable text generation has a wide range of applications, including controlling through persona (Zhang et al., 2018; Boyd et al., 2020), politeness (Niu and Bansal, 2018), etc. Wiseman et al. (2018) presented controlling generations by learning latent, discrete templates from data. Fu et al. (2019) discovered the importance of pivot words that determines the sentence attributes and presented a lexical analysis framework. To control large pre-trained models, Keskar et al. (2019) demonstrated the ability to control text generation through a wide range of aspects, such as domains and links. Plug-and-play language models Dathathri et al. (2019) also address whole document controllability by adding a linear cla"
2020.emnlp-main.226,P19-1139,0,0.0231779,"able 15 only mentions the keyword “realize” instead of centering around it. This is caused by the RAKE keywords extractor, which does not always extract the keywords that represent the sentence well. One way to mitigate this issue is to leverage longer context information to identify better keywords which is subject of the future work. 6 Related Work Knowledge Incorporation of knowledge into language models has shown promising results for downstream tasks, such as factual correct generation (Logan et al., 2019) , commonsense knowledge graph construction (Bosselut et al., 2019), entity typing (Zhang et al., 2019) and etc. More recently, several works have shown that inclusion of learned mechanisms for explicit or implicit knowledge can lead to the state-of-the-art results in Question Answering (Guu et al., 2020; Karpukhin et al., 2020; Lee et al., 2019; Lewis et al., 2020) and dialogue modeling (Roller et al., 2020). Storytelling There are several different storytelling tasks described throughout the literature. Storytelling can be classified into story completion (Chen et al., 2019), story ending generation (Guan et al., 2019), story generation from prompts (Fan et al., 2018) or titles (Yao et al., 2"
2020.emnlp-main.226,D18-1356,0,0.028812,"an et al., 2019; Chen et al., 2019). Recently, pre-trained language models have been used to finetune on both story completion datasets and commonsense knowledge to further improve the quality of story completion (Guan et al., 2020). However, few works concern the controllability of language model generation, especially for the large pre-trained models that are common in today’s literature. Controllable Generation Controllable text generation has a wide range of applications, including controlling through persona (Zhang et al., 2018; Boyd et al., 2020), politeness (Niu and Bansal, 2018), etc. Wiseman et al. (2018) presented controlling generations by learning latent, discrete templates from data. Fu et al. (2019) discovered the importance of pivot words that determines the sentence attributes and presented a lexical analysis framework. To control large pre-trained models, Keskar et al. (2019) demonstrated the ability to control text generation through a wide range of aspects, such as domains and links. Plug-and-play language models Dathathri et al. (2019) also address whole document controllability by adding a linear classifier on top of G PT-2 to predict whether generated text observes a particular st"
2020.emnlp-main.226,D18-1462,0,0.0447244,"al., 2020; Karpukhin et al., 2020; Lee et al., 2019; Lewis et al., 2020) and dialogue modeling (Roller et al., 2020). Storytelling There are several different storytelling tasks described throughout the literature. Storytelling can be classified into story completion (Chen et al., 2019), story ending generation (Guan et al., 2019), story generation from prompts (Fan et al., 2018) or titles (Yao et al., 2019), and story generation from a given sentence (Guan et al., 2020). Different approaches have been developed to model the structure of stories with storylines (Yao et al., 2019), skeletons (Xu et al., 2018), Conditional Variational AutoEncoders (Wang and Wan, 2019) and a coarse-to-fine framework (Fan et al., 2019). Other works focus on incorporating commonsense knowledge into story generation with attention-based models (Guan et al., 2019; Chen et al., 2019). Recently, pre-trained language models have been used to finetune on both story completion datasets and commonsense knowledge to further improve the quality of story completion (Guan et al., 2020). However, few works concern the controllability of language model generation, especially for the large pre-trained models that are common in today"
2020.emnlp-main.587,P17-1042,0,0.0235639,"he alignment of cross-lingual representations, which do not require any external resources. • Our model outperforms the previous state-ofthe-art model in both zero-shot and few-shot scenarios on the cross-lingual SLU task. • Extensive analysis and visualizations are made to illustrate the effectiveness of our approaches. 2 Related Work Cross-lingual Transfer Learning Cross-lingual transfer learning is able to circumvent the requirement of enormous training data by leveraging the learned knowledge in the source language and learning inter-connections between the source and the target language. Artetxe et al. (2017) and Conneau et al. (2018) conducted cross-lingual word embedding mapping with zero or very few supervision signals. Recently, pre-training cross-lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu"
2020.emnlp-main.587,D18-1038,0,0.0137979,"ffective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c et al. (2017) expanded Wizard of Oz (WOZ) into multilingual WOZ by annotating two additional languages. Schuster et al. (2019) introduced a multilingual SLU dataset and proposed to leverage bilingual corpus and multilingual CoVe (Yu et al., 2018) to align the representations across languages. C"
2020.emnlp-main.587,N19-1423,0,0.173683,"ing Cross-lingual transfer learning is able to circumvent the requirement of enormous training data by leveraging the learned knowledge in the source language and learning inter-connections between the source and the target language. Artetxe et al. (2017) and Conneau et al. (2018) conducted cross-lingual word embedding mapping with zero or very few supervision signals. Recently, pre-training cross-lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresou"
2020.emnlp-main.587,D17-1169,0,0.0283836,"utterance representations based on the slot labels, which increases the generalization ability in the target language. 3.1.2 Implementation Details Figure 2 (Left) illustrates an utterance encoder and a label encoder that generate the representations for utterances and labels, respectively. We denote the user utterance as w = [w https://www.mathcha.io/editor 1 , w2 , ..., wn ], where n is the length of the utterance. Similarly, we represent the slot label sequences as s = [s1 , s2 , ..., sn ]. We combine a bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) and an attention layer (Felbo et al., 2017) to encode and produce the representations for user utterances and slot label sequences. The representation generation process is defined as follows: w w [hw 1 , h2 , ..., hw ] = BiLSTMutter (E(w)), (1) [hs1 , hs2 , ..., hsn ] = BiLSTMlabel (E(s)), (2) exp(mw w w w i ) mw i = hi v , αi = Pn w) , exp(m t t=1 (3) exp(msi ) msi = hsi v s , αis = Pn s , t=1 exp(mt ) (4) u= n X αiw hw i , l = i=1 n X αis hsi , (5) i=1 where the superscript w and s represents utterance and label, respectively, v is a trainable weight vector in the attention layer, αi is the attention score for each token i, E denote"
2020.emnlp-main.587,N18-2118,0,0.0670143,"Missing"
2020.emnlp-main.587,P19-1544,0,0.0187162,"perfect, the sentence-level alignment is still imperfect owing to grammatical and syntactical variances across languages. Therefore, we emphasize that cross-lingual methods should focus on the alignments of word-level and 1 Introduction sentence-level representations, and increase the roData-driven neural-based supervised training ap- bustness for inherent imperfect alignments. proaches have shown effectiveness in spoken lanIn this paper, we concentrate on the cross-lingual guage understanding (SLU) systems (Goo et al., SLU task (as illustrated in Figure 1), and we con2018; Chen et al., 2019; Haihong et al., 2019). sider both few-shot and zero-shot scenarios. To However, collecting large amounts of high-quality improve the quality of cross-lingual alignment, we training data is not only expensive but also timefirst propose a Label Regularization (LR) method, https://www.mathcha.io/editor consuming, which makes these approaches not scal- which utilizes the slot label sequences to regularize able to low-resource languages due to the scarcity the utterance representations. We hypothesize that of training data. Cross-lingual adaptation has natu- if the slot label sequences of user utterances are rally aris"
2020.emnlp-main.587,P82-1020,0,0.669392,"Missing"
2020.emnlp-main.587,D19-1252,0,0.140298,"Missing"
2020.emnlp-main.587,D17-1302,0,0.0161335,"ss-lingual word embedding mapping with zero or very few supervision signals. Recently, pre-training cross-lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c et al. (2017) expanded Wizard of Oz (WOZ) into multilingual WOZ by annotating two ad"
2020.emnlp-main.587,N16-1030,0,0.0603963,"Missing"
2020.emnlp-main.587,D19-1129,1,0.884995,"Missing"
2020.emnlp-main.587,D17-1269,0,0.0607767,"Missing"
2020.emnlp-main.587,Q17-1022,0,0.0419006,"Missing"
2020.emnlp-main.587,P19-1493,0,0.0163719,"nsfer learning is able to circumvent the requirement of enormous training data by leveraging the learned knowledge in the source language and learning inter-connections between the source and the target language. Artetxe et al. (2017) and Conneau et al. (2018) conducted cross-lingual word embedding mapping with zero or very few supervision signals. Recently, pre-training cross-lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna e"
2020.emnlp-main.587,N19-1380,0,0.322373,"), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c et al. (2017) expanded Wizard of Oz (WOZ) into multilingual WOZ by annotating two additional languages. Schuster et al. (2019) introduced a multilingual SLU dataset and proposed to leverage bilingual corpus and multilingual CoVe (Yu et al., 2018) to align the representations across languages. Chen et al. (2018) proposed a teacherstudent framework based on a bilingual dictionary or bilingual corpus for building cross-lingual dialog state tracking. Instead of highly relying on extensive bilingual resources, Qin et al. (2020) introduced a data augmentation framework to generate multilingual code-switching data for cross-lingual tasks including the SLU task. Liu et al. (2019b) leveraged a mixed language training framewor"
2020.emnlp-main.587,D18-1270,0,0.0639289,"Missing"
2020.emnlp-main.587,P19-1078,1,0.822312,"., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c et al. (2017) expanded Wizard of Oz (WOZ) into multilingual WOZ by annotating two additional languages. Schuster et al. (2019) introduced a multilingual SLU dataset and proposed to leverage bilingual corpus and multilingual CoVe (Yu et al., 2018) to align the representations across languages. Chen et al. (2018) proposed a teacherstudent framework based on a bilingual dictionary or bilingual corpus for building cross-lingual dialog state"
2020.emnlp-main.587,2020.acl-main.3,1,0.83076,"017) and Conneau et al. (2018) conducted cross-lingual word embedding mapping with zero or very few supervision signals. Recently, pre-training cross-lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c et al. (2017) expanded Wizard of Oz (WOZ"
2020.emnlp-main.587,D18-1034,0,0.0188539,"the target language. Artetxe et al. (2017) and Conneau et al. (2018) conducted cross-lingual word embedding mapping with zero or very few supervision signals. Recently, pre-training cross-lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c e"
2020.emnlp-main.587,W18-3023,0,0.128861,"), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c et al. (2017) expanded Wizard of Oz (WOZ) into multilingual WOZ by annotating two additional languages. Schuster et al. (2019) introduced a multilingual SLU dataset and proposed to leverage bilingual corpus and multilingual CoVe (Yu et al., 2018) to align the representations across languages. Chen et al. (2018) proposed a teacherstudent framework based on a bilingual dictionary or bilingual corpus for building cross-lingual dialog state tracking. Instead of highly relying on extensive bilingual resources, Qin et al. (2020) introduced a data augmentation framework to generate multilingual code-switching data for cross-lingual tasks including the SLU task. Liu et al. (2019b) leveraged a mixed language training framework for cross-lingual task-oriented dialogue systems. And Liu et al. (2019a) proposed to refine the crosslingual word embe"
2020.emnlp-main.587,N16-1156,0,0.0651037,"Missing"
2020.findings-emnlp.298,P19-1285,0,0.0743784,"Missing"
2020.findings-emnlp.298,W18-5446,0,0.0289867,"Missing"
2020.findings-emnlp.298,P19-1441,0,0.129897,"whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative position embedding method is reasonably generalized and robust from the inductive perspective. Finally, we show that our proposed method can be adopted as a near drop-in replacement for improving the accuracy of large models with a small computational budget. 1 Introduction The introduction of BERT (Devlin et al., 2018) has lead to new state-of-the-art results on various downstream tasks such as question answering and passage ranking. Variations of BERT, including RoBERTa (Liu et al., 2019b), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) and T5 (Raffel et al., 2019) have been proposed. At its core, BERT is non-recurrent and based on self-attention; in order to model the dependency between elements at different positions in the sequence, BERT relies on position embeddings. With BERT, the input embeddings are the sum of the token embeddings, segment embeddings, and position embeddings. The position embedding encodes the absolute positions from 1 to maximum sequence length (usually 512). That is, each position has a learnable embedding 1. We argue that the relative position"
2020.findings-emnlp.298,2021.ccl-1.108,0,0.145482,"Missing"
2020.findings-emnlp.298,D16-1244,0,0.0651909,"Missing"
2020.findings-emnlp.298,D16-1264,0,0.0364048,"Missing"
2020.findings-emnlp.298,N18-2074,0,0.0629364,"tion attends to another token at a different position. Recent work suggested removing the next sentence prediction (NSP) loss with training conducted solely on individual chunks of text (Liu et al., 2019a). In this setup, the notion of absolute positions can be arbitrary depending on chunk start positions. Therefore, the association of a token to an absolute position is not well justified. Indeed, what really matters is the relative position or distance between two tokens ti and tj , which is j − i. This phenomena has been realized and the relative position representation has been proposed in Shaw et al. (2018); Huang et al. (2018), in the context of encoder decoder machine translation and music generation respectively. Shaw et al. (2018) has been modified in transformer-XL (Dai et al., 2019) and adopted in XLNet (Yang et al., 2019). The relative position embedding in (Shaw et al., 2018) has been proven to be effective and thus it is adopted in (Raffel et al., 2019; Song et al., 2020). In this paper, we review the absolute position embedding from Devlin et al. (2018) and the relative position embeddings in Shaw et al. (2018); Dai et al. (2019). Our contributions are as follows. Transformer architect"
2020.lrec-1.73,P15-1034,0,0.0888508,"y masking a small number of input source tokens into unknown tokens. 4.2. Baselines We compare our model with the following implemented baselines: the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), the pointer-generator (PG) model (See et al., 2017), and the key-value memory networks (KVMN) (Miller et al., 2016). Meanwhile, existing OpenIE models, which parse sentences and tag parts of them as output, could be an alternative. We compare our model with two state-of-the-art open information extraction (OpenIE) pre-trained models, S-OpenIE (Stanovsky et al., 2018) and LLS-OpenIE (Angeli et al., 2015). Seq2Seq, PG, and KVMN are used for internal comparison, where all the models are trained from scratch using 584 the distant supervision data. S-OpenIE and LLS-OpenIE, on the other hand, are used for external comparison, where these two models are trained on several OpenIE datasets and evaluated on the attribute extraction task. We briefly introduce the baselines: • Seq2Seq is the most common baseline for sequence generation. We use GRUs as a base model to encode a sequence of words and decode a sequence that concatenates (subject, predicate, object) by semicolons. • PG is one of the best gen"
2020.lrec-1.73,P11-1062,0,0.0282575,"et al., 2010; Alekseev and Nikolenko, 2016), occupation (Preot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would"
2020.lrec-1.73,P17-1152,0,0.0256402,"Missing"
2020.lrec-1.73,D13-1114,0,0.0202194,"ution. Lastly, the conversations in the Persona-Chat dataset are not collected naturally, with most of the users tending to ignore what the other said and just talking about themselves. Therefore, it is hard to evaluate whether “understanding your partner” helps agents speak properly. Also, since there is no publicly available data with the same user continually talking to a system, it is hard to evaluate the lifelong setting. 7. Related Work User Attributes Inference Most previous work has treated user attribute inference from social media as a classification task, such as gender prediction (Ciot et al., 2013), age prediction (Rao et al., 2010; Alekseev and Nikolenko, 2016), occupation (Preot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007"
2020.lrec-1.73,C16-1279,0,0.013158,"Therefore, it is hard to evaluate whether “understanding your partner” helps agents speak properly. Also, since there is no publicly available data with the same user continually talking to a system, it is hard to evaluate the lifelong setting. 7. Related Work User Attributes Inference Most previous work has treated user attribute inference from social media as a classification task, such as gender prediction (Ciot et al., 2013), age prediction (Rao et al., 2010; Alekseev and Nikolenko, 2016), occupation (Preot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al"
2020.lrec-1.73,Q17-1024,0,0.0174945,"Missing"
2020.lrec-1.73,P14-1016,0,0.051709,"ue systems) of such extracted user attributes, and point out current limitations to cast light on future work. Keywords: Dialogue Systems, Personalization, Information Extraction, Natural Language Processing 1. Introduction User attributes are explicit representations of a person’s identity and characteristics in a structured format. They provide a rich repository of personal information for better user understanding in many applications. High-quality user attributes are, however, hard to obtain since the information in social networks such as Facebook and Twitter is often sparsely populated (Li et al., 2014). Therefore, exploiting unstructured data sources to obtain structured user attributes is a challenging research direction. Meanwhile, there is an increasing reliance on dialogue agents to assist, inform, and entertain humans, for example, keeping the elderly company and providing customer service. Conversational data between users and systems is informative and abundant, and most of the existing deep learning approaches are trained on these large crowd-sourced corpora or scraped conversations. These models, given the current dialogue context (e.g., few previous turns), are focused on either g"
2020.lrec-1.73,P18-1136,1,0.841085,"f words in the utterance and dhdd is the hidden size of the GRU. The last hidden state henc is represented l as the final encoded vector, which will be used to query the predicate classifier and initialize the entity generator. 2 PyTorch version in github.com/huggingface/ pytorch-pretrained-BERT Predicate Classifier We use a multi-hop (K = 3 hops) end-to-end memory network (MN) (Sukhbaatar et al., 2015) as our predicate classifier because we believe its reasoning ability can benefit predicates prediction, as shown in question answering and dialogue tasks (Bordes et al., 2016; Wu et al., 2018; Madotto et al., 2018; Wu et al., 2019b). We assign the memory in the MN as all the predicate words R = {r1 , . . . , rJ }, where J is the total number of possible predicates. The predicate classifier is queried by the encoded vector henc l , and the memory attention at each hop k is computed as αk = Sof tmax(C k (P )q k ) ∈ RJ , (1) where C k and q k are the embedding matrix and query vector at hop k, respectively. Here, αk is a soft memory selector that decides the memory relevance with respect to the query vector q k . The model reads out the memory ok as X ok = αik C k+1 (ri ) ∈ Rdhdd . (2) i Then the query ve"
2020.lrec-1.73,P19-1542,1,0.82919,"der systems predict the preference a user would give to an item, which is utilized in a variety of areas. Content-based filtering (Pazzani and Billsus, 2007), knowledge-based filtering (Burke, 2000) and collaborative filtering (Sarwar et al., 1998) are the most common approaches for recommender systems. For dialogue applications, (Lucas et al., 2009) and (Joshi et al., 2017) focus on letting the agent be aware of the human pre-defined profile and so adjust the dialogue accordingly. (Zemlyanskiy and Sha, 2018) define a mutual information discovery score to re-rank system generating responses. (Madotto et al., 2019) uses meta-learning to fast adapt to unseen persona scenarios. 8. Conclusion We utilize conversational data to extract user attributes for better user understanding. Due to lacking a labeled dataset, we apply distant supervision with a natural language inference model to train our proposed two-stage attribute extractor. Our model surpasses several retrieval and generation baselines on human evaluation, and is different from existing open information extraction approaches. In the end, we discuss potential downstream applications and point out current limitations to provide suggestions for futur"
2020.lrec-1.73,D12-1048,0,0.0185552,"reot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would give to an item, which is utilized in a variety of areas."
2020.lrec-1.73,D18-1298,0,0.106348,"e is an increasing reliance on dialogue agents to assist, inform, and entertain humans, for example, keeping the elderly company and providing customer service. Conversational data between users and systems is informative and abundant, and most of the existing deep learning approaches are trained on these large crowd-sourced corpora or scraped conversations. These models, given the current dialogue context (e.g., few previous turns), are focused on either generating good responses (Serban et al., 2015), or incorporating “system attributes” to generate consistent responses (Zhang et al., 2018; Mazare et al., 2018). However, the whole dialogue history of the same person is ignored, implying that these systems are not gradually getting to know their users by extracting user information through conversations. In this paper, we demonstrate that it is feasible to automatically extract user attributes from dialogues. Given a user utterance, our goal is to predict user information that can be represented as a (Subject, Predicate, Object) triplet format, which is available for any downstream application. For example, in Table 1, (I, live in, Florida) is extracted from the second user utterance. Meanwhile, not"
2020.lrec-1.73,D16-1147,0,0.0139355,"to weight two losses is set to be 0.5. A greedy search decoding strategy is used for our entity generator since the generated phrases are usually short. In addition, to increase model generalization and simulate an out-of-vocabulary setting, a word dropout is applied to the input by randomly masking a small number of input source tokens into unknown tokens. 4.2. Baselines We compare our model with the following implemented baselines: the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), the pointer-generator (PG) model (See et al., 2017), and the key-value memory networks (KVMN) (Miller et al., 2016). Meanwhile, existing OpenIE models, which parse sentences and tag parts of them as output, could be an alternative. We compare our model with two state-of-the-art open information extraction (OpenIE) pre-trained models, S-OpenIE (Stanovsky et al., 2018) and LLS-OpenIE (Angeli et al., 2015). Seq2Seq, PG, and KVMN are used for internal comparison, where all the models are trained from scratch using 584 the distant supervision data. S-OpenIE and LLS-OpenIE, on the other hand, are used for external comparison, where these two models are trained on several OpenIE datasets and evaluated on the attr"
2020.lrec-1.73,P02-1040,0,0.106403,"Missing"
2020.lrec-1.73,D14-1162,0,0.0918919,"Missing"
2020.lrec-1.73,P15-1169,0,0.0659842,"Missing"
2020.lrec-1.73,P19-1363,0,0.0211327,"Missing"
2020.lrec-1.73,P17-1099,0,0.0432375,"d character embeddings (100) (Hashimoto et al., 2016). The λ to weight two losses is set to be 0.5. A greedy search decoding strategy is used for our entity generator since the generated phrases are usually short. In addition, to increase model generalization and simulate an out-of-vocabulary setting, a word dropout is applied to the input by randomly masking a small number of input source tokens into unknown tokens. 4.2. Baselines We compare our model with the following implemented baselines: the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), the pointer-generator (PG) model (See et al., 2017), and the key-value memory networks (KVMN) (Miller et al., 2016). Meanwhile, existing OpenIE models, which parse sentences and tag parts of them as output, could be an alternative. We compare our model with two state-of-the-art open information extraction (OpenIE) pre-trained models, S-OpenIE (Stanovsky et al., 2018) and LLS-OpenIE (Angeli et al., 2015). Seq2Seq, PG, and KVMN are used for internal comparison, where all the models are trained from scratch using 584 the distant supervision data. S-OpenIE and LLS-OpenIE, on the other hand, are used for external comparison, where these two models"
2020.lrec-1.73,N18-1081,0,0.0831064,"opout is applied to the input by randomly masking a small number of input source tokens into unknown tokens. 4.2. Baselines We compare our model with the following implemented baselines: the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), the pointer-generator (PG) model (See et al., 2017), and the key-value memory networks (KVMN) (Miller et al., 2016). Meanwhile, existing OpenIE models, which parse sentences and tag parts of them as output, could be an alternative. We compare our model with two state-of-the-art open information extraction (OpenIE) pre-trained models, S-OpenIE (Stanovsky et al., 2018) and LLS-OpenIE (Angeli et al., 2015). Seq2Seq, PG, and KVMN are used for internal comparison, where all the models are trained from scratch using 584 the distant supervision data. S-OpenIE and LLS-OpenIE, on the other hand, are used for external comparison, where these two models are trained on several OpenIE datasets and evaluated on the attribute extraction task. We briefly introduce the baselines: • Seq2Seq is the most common baseline for sequence generation. We use GRUs as a base model to encode a sequence of words and decode a sequence that concatenates (subject, predicate, object) by se"
2020.lrec-1.73,D18-1157,0,0.011451,"tributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would give to an item, which is utilized in a variety of areas. Content-based filtering (Pazzani and Billsus, 2007), knowledge-based filtering (Burke, 2000) and collaborative filtering (Sarwar et al., 1998) are the most common approaches"
2020.lrec-1.73,P16-1123,0,0.0464071,"Missing"
2020.lrec-1.73,W18-5713,0,0.0202127,"ly introduce these datasets and discuss some of their limitations. Persona-Chat This is a multi-turn chit-chat corpus with annotation of the participants’ personal profiles (e.g., preferences about food, movies). It is collected by asking two crowd-workers to talk to each other freely but conditioned on their artificial personas, which are established by four to six persona sentences. An example from the dataset is provided in Table 2. In total there are 1155 personas with over 5,000 persona sentences, and 162,064 utterances over 10,907 dialogues. Most of the related works using this dataset (Weston et al., 2018; Semih Yavuz, 2018; Wolf et al., 2019; Dinan et al., 2019) focus on adapting systems to a given persona, i.e., learning to generate responses that are consistent with the persona. 1 The code is released at https://github.com/ jasonwu0731/GettingToKnowYou Although the dataset contains pre-defined personas and the corresponding conversations, it cannot be applied directly to the attribute extraction task for the following two reasons: 1) The mapping between utterances and the persona is missing. Which persona sentence is related to which utterance remains unknown. 2) All the personas are writte"
2020.lrec-1.73,P10-1013,0,0.0605996,"age prediction (Rao et al., 2010; Alekseev and Nikolenko, 2016), occupation (Preot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the pr"
2020.lrec-1.73,P19-1078,1,0.921683,"ce and dhdd is the hidden size of the GRU. The last hidden state henc is represented l as the final encoded vector, which will be used to query the predicate classifier and initialize the entity generator. 2 PyTorch version in github.com/huggingface/ pytorch-pretrained-BERT Predicate Classifier We use a multi-hop (K = 3 hops) end-to-end memory network (MN) (Sukhbaatar et al., 2015) as our predicate classifier because we believe its reasoning ability can benefit predicates prediction, as shown in question answering and dialogue tasks (Bordes et al., 2016; Wu et al., 2018; Madotto et al., 2018; Wu et al., 2019b). We assign the memory in the MN as all the predicate words R = {r1 , . . . , rJ }, where J is the total number of possible predicates. The predicate classifier is queried by the encoded vector henc l , and the memory attention at each hop k is computed as αk = Sof tmax(C k (P )q k ) ∈ RJ , (1) where C k and q k are the embedding matrix and query vector at hop k, respectively. Here, αk is a soft memory selector that decides the memory relevance with respect to the query vector q k . The model reads out the memory ok as X ok = αik C k+1 (ri ) ∈ Rdhdd . (2) i Then the query vector is updated f"
2020.lrec-1.73,D15-1206,0,0.0120966,"Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would give to an item, which is utilized in a variety of areas. Content-based filtering (Pazzani and Billsus, 2007), knowledge-based filtering (Burk"
2020.lrec-1.73,K18-1053,0,0.0150253,"alogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would give to an item, which is utilized in a variety of areas. Content-based filtering (Pazzani and Billsus, 2007), knowledge-based filtering (Burke, 2000) and collaborative filtering (Sarwar et al., 1998) are the most common approaches for recommender systems. For dialogue applications, (Lucas et al., 2009) and (Joshi et al., 2017) focus on letting the agent be aware of the human pre-defined profile and so adjust the dialogue accordingly. (Zemlyanskiy and Sha, 2018) define a mutual information discovery score to re-rank system generating responses. (Madotto et al., 2019) uses meta-learning to fast adapt to unseen persona scenarios. 8. Conclusion We utilize conversational data to extract user attributes for better user understanding. Due to lacking a labeled dataset, we apply distant supervision with a natural language inference model to train our proposed two-stage attribute extractor. Our model surpasses several retrieval and generation baselines on human evaluation, and is different from existing open information extraction approaches. In the end, we d"
2020.lrec-1.73,C14-1220,0,0.08916,"Missing"
2020.lrec-1.73,P18-1205,0,0.166608,"ion. Meanwhile, there is an increasing reliance on dialogue agents to assist, inform, and entertain humans, for example, keeping the elderly company and providing customer service. Conversational data between users and systems is informative and abundant, and most of the existing deep learning approaches are trained on these large crowd-sourced corpora or scraped conversations. These models, given the current dialogue context (e.g., few previous turns), are focused on either generating good responses (Serban et al., 2015), or incorporating “system attributes” to generate consistent responses (Zhang et al., 2018; Mazare et al., 2018). However, the whole dialogue history of the same person is ignored, implying that these systems are not gradually getting to know their users by extracting user information through conversations. In this paper, we demonstrate that it is feasible to automatically extract user attributes from dialogues. Given a user utterance, our goal is to predict user information that can be represented as a (Subject, Predicate, Object) triplet format, which is available for any downstream application. For example, in Table 1, (I, live in, Florida) is extracted from the second user utte"
2021.acl-demo.36,2020.acl-main.187,0,0.0526201,"Missing"
2021.acl-demo.36,P18-1033,0,0.0282367,"mn label smoothing and weighted beam search are also reflected by the Exec accuracy on Spider. Furthermore, simply adding more hypotheses in the beam can significantly boost the coverage of the correct predictions, 303 Related Work Executable Cross-database Semantic Parsing. Early NLDB systems use rule-based parsing (Zelle and Mooney, 1996; Li and Jagadish, 2014) and cannot handle the diversity of natural language in practice. Neural semantic parsing is more promising for coverage but is still brittle in real-world applications where queries can involve novel compositions of learned patterns (Finegan-Dollak et al., 2018; Shaw et al., 2020). Furthermore, to allow plug-and-play on new databases, the underlying semantic parser may not be trained on in-domain parallel corpus but needs to transfer across domains in a zero-shot fashion. Executable cross-database semantic parsing is even more challenging. Many of the previous work only tackle the cross-domain part, omitting the value prediction problem required for executable queries (Guo et al., 2019; Wang et al., 2019; Choi et al., 2020; Xu et al., 2020). Unlike the output space of predicting the SQL sketch or columns, 3 Rubin and Berant (2020) updated a version"
2021.acl-demo.36,P19-1444,0,0.258674,"where the users interact with the parser. Figure 1 caption describes the typical user interaction using an example. Behind the front-end interface, T URING consists of an executable cross-domain semantic parser trained on Spider that maps user utterances to SQL query hypotheses, the SQL execution engine that runs the queries to obtain answers, and the explanation generation module that produces the explanation text and the meta-data powering explanation highlighting. The next sections will describe the semantic parsing and explanation modules. 3 like many other top systems (Wang et al., 2019; Guo et al., 2019) on the Spider leaderboard. On the high-level, our SP adopts the grammarbased framework following TranX (Yin and Neubig, 2018) with an encoder-decoder neural architecture. A grammar-based transition system is designed to turn the generation process of the SQL abstract syntax tree (AST) into a sequence of tree-constructing actions to be predicted by the parser. The encoder fenc jointly encodes both the user question Q = q1 . . . q|Q |and database schema S = {s1 , . . . , s|S |} consisting of tables and columns in the database. The decoder fdec is a transition-based abstract syntax decoder, whic"
2021.acl-demo.36,D18-2002,0,0.104846,"the front-end interface, T URING consists of an executable cross-domain semantic parser trained on Spider that maps user utterances to SQL query hypotheses, the SQL execution engine that runs the queries to obtain answers, and the explanation generation module that produces the explanation text and the meta-data powering explanation highlighting. The next sections will describe the semantic parsing and explanation modules. 3 like many other top systems (Wang et al., 2019; Guo et al., 2019) on the Spider leaderboard. On the high-level, our SP adopts the grammarbased framework following TranX (Yin and Neubig, 2018) with an encoder-decoder neural architecture. A grammar-based transition system is designed to turn the generation process of the SQL abstract syntax tree (AST) into a sequence of tree-constructing actions to be predicted by the parser. The encoder fenc jointly encodes both the user question Q = q1 . . . q|Q |and database schema S = {s1 , . . . , s|S |} consisting of tables and columns in the database. The decoder fdec is a transition-based abstract syntax decoder, which uses the encoded representation H to predict the target SQL query T . The decoder also relies on the transition system to co"
2021.acl-demo.36,2020.findings-emnlp.438,0,0.125487,"stem with the previous systems. First, our transition system omits the action type SelectTable used by other transition-based SP systems (Wang et al., 2019; Guo et al., 2019). This is made possible by attaching the corresponding table to each column, so that the tables in the target SQL query can be deterministically inferred from the predicted columns. Second, we simplify the value prediction by always trying to copy from the user question, instead of applying the GenToken[v] action (Yin and Neubig, 2018) which generates tokens from a large vocabulary or choose from a pre-processed picklist (Lin et al., 2020). Both of the changes constrain the output space of the decoder to ease the 299 Figure 1: T URING system in action: the user selected database “Dog kennels”; the left and top panels show the database schema and table content. The user then entered “What is the average age of the dogs who have gone through any treatments?” in the search box. This question is run through the semantic parser producing multiple SQL hypotheses from beam-search, which are then explained step-by-step as shown. The differences across the hypotheses are highlighted. The tokens corresponding to table and columns are in"
2021.acl-demo.36,D18-1425,0,0.0660295,"Missing"
2021.acl-demo.36,2020.acl-main.742,0,0.0735809,"Missing"
2021.acl-demo.36,D18-1112,0,0.0196422,"provides a complementary way to resolve mistakes and ambiguities in NLDB. Acknowledgments We appreciate the ACL demo anonymous reviewers for their valuable inputs. We would like to thank Mehrsa Golestaneh and April Cooper for their work on the improved front-end version, https://turing-app.borealisai.com, which is not in the scope of this publication. We also would like to thank Wendy Tay and Simon J.D. Prince for their general support. Query Explanation. Explaining structured query language has been studied in the past (Simitsis and Ioannidis, 2009; Koutrika et al., 2010; Ngomo et al., 2013; Xu et al., 2018). Full NLDB systems can leverage explanations to correct mistakes with user feedback (Elgohary et al., 2020), or to prevent mistakes by giving clarifications (Zeng et al., 2020). However, these methods can only handle cases where the mistake or ambiguity is about the table, column, or value prediction. There is no easy way to resolve structural mistakes or ambiguities if the query sketch is wrong. T URING, on the other hand, offers the potential to recover from such mistakes if the correct query is among the top beam results. This is an orthogonal contribution that could be integrated with oth"
2021.acl-demo.36,2020.acl-demos.24,0,0.357857,"the hypotheses to select which one reflects their intention if any. The English explanations of SQL queries in T UR ING are produced by our high-precision natural language generation system based on synchronous grammars. 1 Introduction Today a vast amount of knowledge is hidden in structured datasets, not directly accessible to nontechnical users who are not familiar with the corresponding database query language like SQL or SPARQL. Natural language database interfaces (NLDB) enable everyday users to interact with databases (Zelle and Mooney, 1996; Popescu et al., 2003; Li and Jagadish, 2014; Zeng et al., 2020). However, correctly translating natural language to executable queries is challenging, as it requires resolving all the ambiguities and subtleties of natural utterances for precise mapping. Furthermore, ∗ Equal contribution System demo at https://turing.borealisai. com/; video at https://vimeo.com/537429187/ 9a5d41f446 1 quick deployment and adoption for NLDB require zero-shot transfer to new databases without an indomain text-to-SQL parallel corpus, i.e. crossdatabase semantic parsing (SP), making the translation accuracy even lower. Finally, unlike in other NLP applications where partially"
2021.acl-demo.36,2020.emnlp-main.558,0,0.0592236,"Missing"
2021.acl-long.163,2020.emnlp-main.463,0,0.134706,"oorer generalization results are often observed (Keskar et al., 2016), especially when the dataset size is only several times larger than the batch size. Furthermore, many recent works noticed a performance gap in this training approach due to layer normalization 2089 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2089–2102 August 1–6, 2021. ©2021 Association for Computational Linguistics (Xu et al., 2019; Nguyen and Salazar, 2019; Zhang et al., 2019a; Wang et al., 2019b; Liu et al., 2020; Huang et al., 2020). Inspired by the recent T-Fixup by Huang et al. (2020), which eliminates the need for learning rate warm-up and layer normalization to train vanilla transformers, we derive a data-dependent initialization strategy by applying different analyses to address several key limitations of T-Fixup. We call our method the Data-dependent Transformer Fixed-update initialization scheme, DT-Fixup. In the mixed setup of additional yet-to-be-trained transformers on top of pre-trained models, DTFixup enables the training of significantly deeper transformers, and is generally applicable t"
2021.acl-long.163,2021.ccl-1.108,0,0.068026,"Missing"
2021.acl-long.315,P17-1171,0,0.0259057,"e is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover, Chen et al. (2020b) proposed a closed-domain hybrid QA dataset where each table is linked to on average 44 passages. Different from ours, their purpose is to study multi-hop reasoning over both forms of information, and each question is still given the associated table. 3 Related Work Open Domain Question Answering ODQA has been extensively studied recently including extractive models (Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2019; Min et al., 2019; Yang et al., 2019) that predict spans from evidence passages, and generative models (Raffel et al., 2020; Method In this section, we describe our method for hybrid open-domain question answering. It mainly consists of three components: (1) a retrieval system; (2) a joint reranker and (3) a dual Seq2Seq model that uses fusion-in-decoder (Izacard and Grave, 2020) to generate direct answer or SQL query. 4079 Figure 1: The pipeline of our proposed hybrid model. The candidates are retrieved from knowledge source such as Wikipedia inclu"
2021.acl-long.315,2020.findings-emnlp.91,0,0.140003,"Missing"
2021.acl-long.315,P18-1078,0,0.0211319,"ng retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover, Chen et al. (2020b) proposed a closed-domain hybrid QA dataset where each table is linked to on average 44 passages. Different from ours, their purpose is to study multi-hop reasoning over both forms of information, and each question is still given the associated table. 3 Related Work Open Domain Question Answering ODQA has been extensively studied recently including extractive models (Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2019; Min et al., 2019; Yang et al., 2019) that predict spans from evidence passages, and generative models (Raffel et al., 2020; Method In this section, we describe our method for hybrid open-domain question answering. It mainly consists of three components: (1) a retrieval system; (2) a joint reranker and (3) a dual Seq2Seq model that uses fusion-in-decoder (Izacard and Grave, 2020) to generate direct answer or SQL query. 4079 Figure 1: The pipeline of our proposed hybrid model. The candidates are retrieved from knowledge source such as Wikipedia including both paragraphs and"
2021.acl-long.315,N19-1423,0,0.0365709,". 3.2 Joint Reranking The purpose of our reranking model is to produce a score si of how relevant a candidate (either an unstructured passage or table) is to a question. Specifically, the reranker input is the concatenation of question, a retrieved candidate-content, and its corresponding title if available2 , separated by special tokens shown in Figure 1. The candidate content can be either the unstructured 2 Wikipedia passages have page titles, and tables have table titles. text or flattened table. We use BERTbase model in this paper. Following Nogueira and Cho (2019), we finetune the BERT (Devlin et al., 2019) model using the following loss: X X L= log(si ) log(1 si ). (1) i2Ipos i2Ineg The Ipos is sampled from all relevant BM25 candidates, and the set Ineg is sampled from all non-relevant BM25 candidates. Different from Nogueira and Cho (2019), during training, for each question, we sample 64 candidates including one positive candidate and 63 negative candidates, that is, |Ipos |= 1 and |Ineg |= 63. If none of the 200 candidates is relevant, we skip the question. During inference, we use the hybrid reranker to assign a score to each of the 200 candidates, and choose the top 50 candidates as the in"
2021.acl-long.315,P19-1444,0,0.110496,"reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured textual and structured tabular information sources, as some questions are better answered by tabular evidence from databases. For example, the current state-of-the-art ODQA models struggle on questions that involve aggregation operations such as counting or averaging. One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020). These methods all rely on knowing the associated table for each question in advance, and hence are not trivially applicable to the open-domain setting, where the relevant evidence might come from millions of tables. In this paper, we provide a solution to the aforementioned problem by empowering the current generative ODQA models with the Text2SQL ability. More specifically, we propose a dual readerparser (D U R E PA) framework that can take both textual and tabular data as input, and generate either direct an"
2021.acl-long.315,2020.acl-main.398,0,0.0322855,"which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover,"
2021.acl-long.315,2020.emnlp-main.550,0,0.0848972,"Missing"
2021.acl-long.315,Q19-1026,0,0.157349,"s and the 11th International Joint Conference on Natural Language Processing, pages 4078–4088 August 1–6, 2021. ©2021 Association for Computational Linguistics the question, and finally we use a fusion-in-decoder model (Izacard and Grave, 2020) for our readerparser, which takes all the reranked candidates in addition to the question to generate direct answers or SQL queries. To evaluate the effectiveness of our D U R E PA, we construct a hybrid dataset that combines SQuAD (Rajpurkar et al., 2016) and WikiSQL (Zhong et al., 2017) questions. We also conduct experiments on NaturalQuestions (NQ) (Kwiatkowski et al., 2019) and OTT-QA (Chen et al., 2020a) to evaluate DuRePa performance. As textual and tabular open-domain knowledge, we used textual and tabular data from Wikipedia via Wikidumps (from Dec. 21, 2016) and Wikitables (Bhagavatula et al., 2015). We study the model performance on different kinds of questions, where some of them only need one supporting evidence type while others need both textual and tabular evidence. On all question types, D U R E PA performs significantly better than baseline models that were trained on a single evidence type. We also demonstrate that D U R E PA can generate humaninte"
2021.acl-long.315,W16-0105,0,0.0296435,"d SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and"
2021.acl-long.315,D19-1284,0,0.055593,"rectly generate the answers. Wang et al. (2018b,c); Nogueira and Cho (2019) proposed to rerank the retrieved passages to get higher top-n recall. Table Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQ"
2021.acl-long.315,2020.emnlp-main.466,0,0.0494121,"QA datasets, the hybrid methods consistently outperforms the baseline models that only take homogeneous input by a large margin. Specifically we achieve state-of-theart performance on OpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate that the being able to generate structural SQL queries can always bring gains, especially for those questions that requires complex reasoning. 1 Introduction Open-domain question answering (ODQA) is a task to answer factoid questions without a prespecified domain. Recently, generative models (Roberts et al., 2020; Lewis et al., 2020; Min et al., 2020; Izacard and Grave, 2020) have achieved the state-of-the-art performance on many ODQA tasks. These approaches all share the common pipeline where the first stage is retrieving evidence from the free-form text in Wikipedia. However, a large amount of world’s knowledge is not stored as plain text but in structured databases, and need to be accessed using query languages such as SQL. Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured"
2021.acl-long.315,D16-1264,0,0.309104,"AlexanderYogurt/Hybrid-Open-QA 4078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4078–4088 August 1–6, 2021. ©2021 Association for Computational Linguistics the question, and finally we use a fusion-in-decoder model (Izacard and Grave, 2020) for our readerparser, which takes all the reranked candidates in addition to the question to generate direct answers or SQL queries. To evaluate the effectiveness of our D U R E PA, we construct a hybrid dataset that combines SQuAD (Rajpurkar et al., 2016) and WikiSQL (Zhong et al., 2017) questions. We also conduct experiments on NaturalQuestions (NQ) (Kwiatkowski et al., 2019) and OTT-QA (Chen et al., 2020a) to evaluate DuRePa performance. As textual and tabular open-domain knowledge, we used textual and tabular data from Wikipedia via Wikidumps (from Dec. 21, 2016) and Wikitables (Bhagavatula et al., 2015). We study the model performance on different kinds of questions, where some of them only need one supporting evidence type while others need both textual and tabular evidence. On all question types, D U R E PA performs significantly better"
2021.acl-long.315,2020.emnlp-main.437,0,0.0258695,"Missing"
2021.acl-long.315,2020.acl-main.677,0,0.0265935,"e answers. Wang et al. (2018b,c); Nogueira and Cho (2019) proposed to rerank the retrieved passages to get higher top-n recall. Table Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In additio"
2021.acl-long.315,D19-1599,1,0.892273,"Missing"
2021.acl-long.315,N19-4013,0,0.0351031,"Missing"
2021.acl-long.315,2020.acl-main.745,0,0.0660488,"29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover, Chen et al. (2020b)"
2021.acl-long.315,N18-2093,0,0.144196,"at require complex reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured textual and structured tabular information sources, as some questions are better answered by tabular evidence from databases. For example, the current state-of-the-art ODQA models struggle on questions that involve aggregation operations such as counting or averaging. One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020). These methods all rely on knowing the associated table for each question in advance, and hence are not trivially applicable to the open-domain setting, where the relevant evidence might come from millions of tables. In this paper, we provide a solution to the aforementioned problem by empowering the current generative ODQA models with the Text2SQL ability. More specifically, we propose a dual readerparser (D U R E PA) framework that can take both textual and tabular data as input, and genera"
2021.acl-long.315,D18-1193,0,0.122604,"at require complex reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured textual and structured tabular information sources, as some questions are better answered by tabular evidence from databases. For example, the current state-of-the-art ODQA models struggle on questions that involve aggregation operations such as counting or averaging. One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020). These methods all rely on knowing the associated table for each question in advance, and hence are not trivially applicable to the open-domain setting, where the relevant evidence might come from millions of tables. In this paper, we provide a solution to the aforementioned problem by empowering the current generative ODQA models with the Text2SQL ability. More specifically, we propose a dual readerparser (D U R E PA) framework that can take both textual and tabular data as input, and genera"
2021.acl-long.315,D19-1204,0,0.0281337,"ns that require complex reasoning in the ODQA setting. 2 Roberts et al., 2020; Min et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020) that directly generate the answers. Wang et al. (2018b,c); Nogueira and Cho (2019) proposed to rerank the retrieved passages to get higher top-n recall. Table Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA"
2021.acl-long.315,D18-1425,0,0.0537764,"Missing"
2021.acl-long.315,D19-1537,0,0.0277089,"Missing"
2021.acl-long.315,2020.emnlp-main.558,0,0.027544,"sages to get higher top-n recall. Table Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g"
2021.dialdoc-1.6,2020.acl-main.703,0,0.0176165,"ng rate # of ckpt Testdev Phase Inference with Knowledge Evidence. During the testdev and test phase, we leverage the predictions from the KI process as the knowledge evidence components for the dialogue queries. The model generates responses based on a concatenation of the knowledge evidence and the dialogue context. Response Generation To obtain natural and relevant responses, we take advantage of the evidence to the query identified from § 3.1 and focusing on paraphrasing the corresponding knowledge sentences based on the dialogue context. We leverage the large pre-trained model BARTlarge (Lewis et al., 2020). The process of training and inference can be summarized as three steps: Post-processing To avoid serious information loss in the generations compared to the knowledge evidence for the OOD data samples, we compare the lengths of the knowledge evidence and the responses (denoted as Lkn and Lresp ). The generated response will be replaced by the raw knowledge evidence as the final output if Lresp ≤ αLkn , where α is set as 0.4. Pre-training on WoW dataset. We first pretrain the BART model on the WoW dataset for better initialization because of its similarity with the RG task. In the training pr"
2021.dialdoc-1.6,2020.emnlp-main.275,0,0.0405936,"nses (Choi et al., 2018; Reddy et al., 2019; Campos et al., 2020). In addition to the works to enrich the contents of open-domain conversations by controllable generation (Lin et al., 2020; Madotto et al., 2020b), the knowledge grounded dialogue task aims to offer more informative conversation by leveraging an external knowledge source (Dinan et al., 2018; Xu et al., 2020). Relevant knowledge selection is the key to improving the whole system, and very recently, latent variable models have been attracting more attention for this purpose (Lian et al., 2019; Liu et al., 2019b; Kim et al., 2020; Chen et al., 2020; Xu et al., 2021). Results and Discussion Results The results are shown in Table 3 and Table 4. For both subtasks, we observe gaps between the testdev phase and the test phase. For some of the models in subtask 1, multiple random seeds are applied in the training process. The performance gap may result from the domain difference of the partial data samples in the test phase, where the corresponding documents are unseen in the training set. In Table 3, without post-processing on the predictions, the model performance consistently drops to a certain extent, which indicates that postprocessing i"
2021.dialdoc-1.6,2021.ccl-1.108,0,0.0263752,"Missing"
2021.dialdoc-1.6,D19-1129,1,0.882377,"Missing"
2021.dialdoc-1.6,2020.findings-emnlp.215,1,0.833504,"yxucb,eishii,giwinata}@connect.ust.hk, pascale@ece.ust.hk Abstract To tackle this problem, we leverage the pretrained language models from Liu et al. (2019a) and Lewis et al. (2020) and explore data augmentation methods with several training techniques so as to avoid over-fitting to the DialDoc datasets and to teach the model the general pattern of the task. Ensemble and post-processing are conducted to further improve the model performance. Experimental results show that data augmentation is a simple but effective approach for knowledge identification in information-seeking dialogue systems (Madotto et al., 2020a), while bringing improvement to response generation at the same time. In the DialDoc21 competition, our system achieved 74.95 of F1 score and 60.74 of Exact Match in subtask 1, and 37.72 SacreBLEU score (Post, 2018) in subtask 21 . Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users’ needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the"
2021.dialdoc-1.6,2020.findings-emnlp.219,1,0.784817,"yxucb,eishii,giwinata}@connect.ust.hk, pascale@ece.ust.hk Abstract To tackle this problem, we leverage the pretrained language models from Liu et al. (2019a) and Lewis et al. (2020) and explore data augmentation methods with several training techniques so as to avoid over-fitting to the DialDoc datasets and to teach the model the general pattern of the task. Ensemble and post-processing are conducted to further improve the model performance. Experimental results show that data augmentation is a simple but effective approach for knowledge identification in information-seeking dialogue systems (Madotto et al., 2020a), while bringing improvement to response generation at the same time. In the DialDoc21 competition, our system achieved 74.95 of F1 score and 60.74 of Exact Match in subtask 1, and 37.72 SacreBLEU score (Post, 2018) in subtask 21 . Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users’ needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the"
2021.dialdoc-1.6,W18-6319,0,0.0144952,"everal training techniques so as to avoid over-fitting to the DialDoc datasets and to teach the model the general pattern of the task. Ensemble and post-processing are conducted to further improve the model performance. Experimental results show that data augmentation is a simple but effective approach for knowledge identification in information-seeking dialogue systems (Madotto et al., 2020a), while bringing improvement to response generation at the same time. In the DialDoc21 competition, our system achieved 74.95 of F1 score and 60.74 of Exact Match in subtask 1, and 37.72 SacreBLEU score (Post, 2018) in subtask 21 . Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users’ needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the task and thus achieve promising performance. In DialDoc21 competition, our system achieved 74.95 F1 score and 60.74 Exact Match score in subtask 1, and 37.72 SacreBLEU score in subtask 2. Empirical analysis is provid"
2021.dialdoc-1.6,P17-1147,0,0.0183514,"QA dataset and CQA datasets using MTL method. For better readability, we summarize the model settings in Table 1. We also explore more combinations of the experimental settings, such as other combinations of the datasets and other pre-trained language models. However, those fail to bring the improvements as much as those we mentioned above. FT Table 1: The combinations of the experimental settings for the KI subtask. Two-stage training consists of two stages: pre-training (PT) and fine-tuning (FT). which is not included in the evaluation. Among them, SearchQA (Dunn et al., 2017) and TriviaQA (Joshi et al., 2017) differ from the others by the data resource and have the least generalization ability compared to the other four datasets as reported in (Su et al., 2019). In this shared task, we consider two settings when leveraging the MRQA dataset: MRQA and MRQAsmall which excludes SearchQA and TriviaQA. Conversational QA (CQA) datasets We also introduce three CQA datasets, CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), and DoQA (Campos et al., 2020), in the shared task because of their similar settings to the KI process. Wizard-of-Wikipedia (WoW) is a commonlyused knowledge-grounded dialogue datase"
2021.dialdoc-1.6,Q19-1016,0,0.062586,"l settings for the KI subtask. Two-stage training consists of two stages: pre-training (PT) and fine-tuning (FT). which is not included in the evaluation. Among them, SearchQA (Dunn et al., 2017) and TriviaQA (Joshi et al., 2017) differ from the others by the data resource and have the least generalization ability compared to the other four datasets as reported in (Su et al., 2019). In this shared task, we consider two settings when leveraging the MRQA dataset: MRQA and MRQAsmall which excludes SearchQA and TriviaQA. Conversational QA (CQA) datasets We also introduce three CQA datasets, CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), and DoQA (Campos et al., 2020), in the shared task because of their similar settings to the KI process. Wizard-of-Wikipedia (WoW) is a commonlyused knowledge-grounded dialogue dataset (Dinan et al., 2018). It aims at providing content-full responses to user utterances based on Wikipedia documents. 3 Knowledge Identification Methodology We utilize a series of data-augmentation approaches to enable the model to obtain better representations on both dialogue context and document context and learn a general pattern of the task with less domain bias. Namely, we have a tw"
2021.dialdoc-1.6,D19-5827,1,0.846457,"erimental settings, such as other combinations of the datasets and other pre-trained language models. However, those fail to bring the improvements as much as those we mentioned above. FT Table 1: The combinations of the experimental settings for the KI subtask. Two-stage training consists of two stages: pre-training (PT) and fine-tuning (FT). which is not included in the evaluation. Among them, SearchQA (Dunn et al., 2017) and TriviaQA (Joshi et al., 2017) differ from the others by the data resource and have the least generalization ability compared to the other four datasets as reported in (Su et al., 2019). In this shared task, we consider two settings when leveraging the MRQA dataset: MRQA and MRQAsmall which excludes SearchQA and TriviaQA. Conversational QA (CQA) datasets We also introduce three CQA datasets, CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), and DoQA (Campos et al., 2020), in the shared task because of their similar settings to the KI process. Wizard-of-Wikipedia (WoW) is a commonlyused knowledge-grounded dialogue dataset (Dinan et al., 2018). It aims at providing content-full responses to user utterances based on Wikipedia documents. 3 Knowledge Identification Methodology"
2021.dialdoc-1.6,2020.emnlp-main.226,1,0.655395,"ly the question but also the previous conversation turns. Various datasets have been introduced in recent years, and many of them restrict answers to be extraction of a span from the reference document, while the others allow free-form responses (Choi et al., 2018; Reddy et al., 2019; Campos et al., 2020). In addition to the works to enrich the contents of open-domain conversations by controllable generation (Lin et al., 2020; Madotto et al., 2020b), the knowledge grounded dialogue task aims to offer more informative conversation by leveraging an external knowledge source (Dinan et al., 2018; Xu et al., 2020). Relevant knowledge selection is the key to improving the whole system, and very recently, latent variable models have been attracting more attention for this purpose (Lian et al., 2019; Liu et al., 2019b; Kim et al., 2020; Chen et al., 2020; Xu et al., 2021). Results and Discussion Results The results are shown in Table 3 and Table 4. For both subtasks, we observe gaps between the testdev phase and the test phase. For some of the models in subtask 1, multiple random seeds are applied in the training process. The performance gap may result from the domain difference of the partial data sample"
2021.findings-emnlp.327,2020.emnlp-main.20,0,0.0413657,"5 0.4 10 0.3 10 0.3 15 0.2 15 0.2 0 20 0.1 25 0.5 20 0.1 25 0 5 10 15 20 25 (a) fully-connected graph 0 5 10 15 20 25 0.0 (b) section graph Figure 7: An example of the graph attention pattern on graph-roberta models. The axes are graph node index. Node index 0 is the document node and the rest are passage nodes. Contrastive Learning Contrastive learning used as a self-supervised pretraining method has been widely used in NLP models (Rethmeier and Augenstein, 2021). Token or sentence-level contrastive learning tasks have been shown to be very useful in learning better contextual presentations (Clark et al., 2020; Giorgi et al., 2020; Meng et al., 2021). There also have been works that propose data augmentations for contrastive learning. Fang et al. (2020) proposed to use back-translation to construct positive sentence pairs in their contrastive learning framework. Wu et al. (2020); Qu et al. (2020) proposed multiple sentence-level augmentations strategies to do sentence contrastive learning. Most of these work still focus on either local token-level tasks or short sentence-level tasks. In our work, we directly work on document-level contrastive learning task. More recently Luo et al. (2021) proposed"
2021.findings-emnlp.327,P19-1285,0,0.0590152,"Missing"
2021.findings-emnlp.327,N19-1423,0,0.0736125,"Missing"
2021.findings-emnlp.327,2020.emnlp-main.550,0,0.0393059,"Missing"
2021.findings-emnlp.327,S19-2145,0,0.0274588,"Missing"
2021.findings-emnlp.327,P19-1612,0,0.0513504,"Missing"
2021.findings-emnlp.327,2021.ccl-1.108,0,0.0612724,"Missing"
2021.findings-emnlp.327,P11-1015,0,0.275739,"Missing"
2021.findings-emnlp.327,H89-1033,0,0.719438,"Missing"
2021.findings-emnlp.327,D19-1410,0,0.0421102,"Missing"
2021.repl4nlp-1.13,2021.naacl-main.236,0,0.0428486,"hat, Rongali et al. (2020) showed that leveraging a sequence-to-sequence model based on a copy mechanism (See et al., 2017) to directly generate the hierarchical representations was effective at parsing the nested queries. Taking this further, Chen et al. (2020) and Li et al. (2020a) extended the TOP dataset into multiple domains and multiple languages, and Li et al. (2020a) conducted zero-shot cross-lingual experiments using the combination of the multilingual pre-trained models (Conneau et al., 2020; Tran et al., 2020) and the copy mechanism method proposed in Rongali et al. (2020). Lately, Babu et al. (2021) and Shrivastava et al. (2021), which are concurrent works of X2Parser, proposed to tackled the TCSP task in a non-autoregressive way. Different from them, we propose to flatten the hierarchical representations and cast the problem into several sequence labeling tasks. 2.2 Language and Domain Adaptation Recently, cross-lingual and cross-domain models that aim to tackle low-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017),"
2021.repl4nlp-1.13,2020.lrec-1.674,0,0.0380175,"Missing"
2021.repl4nlp-1.13,D18-1038,0,0.0234911,"hich are concurrent works of X2Parser, proposed to tackled the TCSP task in a non-autoregressive way. Different from them, we propose to flatten the hierarchical representations and cast the problem into several sequence labeling tasks. 2.2 Language and Domain Adaptation Recently, cross-lingual and cross-domain models that aim to tackle low-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017), task-oriented semantic parsing (Chen et al., 2018; Schuster et al., 2019; Liu et al., 2019; Wu et al., 2019; Liu et al., 2020a; Chen et al., 2020; Liu et al., 2020b), named entity recognition (Ni et al., 2017; Xie et al., 2018; Jia et al., 2019; Liu et al., 2020c), speech recognition (Mimura et al., 2017; Winata et al., 2020), abstractive summarization (Zhu et al., 2019; Ouyang et al., 2019; Yu et al., 2021), etc. Despite numerous studies related to the cross-lingual and cross-domain areas, only a few of them have explored how to effectively adapt models to the target languages in target domains, and the investigated tasks are limited to sen"
2021.repl4nlp-1.13,2020.emnlp-main.413,0,0.0372946,"8) 113 Figure 3: The architecture of X2Parser. We consider the TCSP task as a combination of the coarse-grained intent classification, fine-grained intent prediction, and slot filling tasks. introduced a new dataset, called TOP, annotated with complex nested intents and slots and proposed to use the hierarchical representations to model the task. After that, Rongali et al. (2020) showed that leveraging a sequence-to-sequence model based on a copy mechanism (See et al., 2017) to directly generate the hierarchical representations was effective at parsing the nested queries. Taking this further, Chen et al. (2020) and Li et al. (2020a) extended the TOP dataset into multiple domains and multiple languages, and Li et al. (2020a) conducted zero-shot cross-lingual experiments using the combination of the multilingual pre-trained models (Conneau et al., 2020; Tran et al., 2020) and the copy mechanism method proposed in Rongali et al. (2020). Lately, Babu et al. (2021) and Shrivastava et al. (2021), which are concurrent works of X2Parser, proposed to tackled the TCSP task in a non-autoregressive way. Different from them, we propose to flatten the hierarchical representations and cast the problem into several"
2021.repl4nlp-1.13,D19-1252,0,0.0478506,"Missing"
2021.repl4nlp-1.13,P19-1236,0,0.0273528,"em into several sequence labeling tasks. 2.2 Language and Domain Adaptation Recently, cross-lingual and cross-domain models that aim to tackle low-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017), task-oriented semantic parsing (Chen et al., 2018; Schuster et al., 2019; Liu et al., 2019; Wu et al., 2019; Liu et al., 2020a; Chen et al., 2020; Liu et al., 2020b), named entity recognition (Ni et al., 2017; Xie et al., 2018; Jia et al., 2019; Liu et al., 2020c), speech recognition (Mimura et al., 2017; Winata et al., 2020), abstractive summarization (Zhu et al., 2019; Ouyang et al., 2019; Yu et al., 2021), etc. Despite numerous studies related to the cross-lingual and cross-domain areas, only a few of them have explored how to effectively adapt models to the target languages in target domains, and the investigated tasks are limited to sentiment analysis (Fern´andez et al., 2016; Li et al., 2020b), abusive language detection (Pamungkas and Patti, 2019), and machine reading comprehension (Charlet et al., 2020). To the best of our k"
2021.repl4nlp-1.13,N18-1131,0,0.0583594,"Missing"
2021.repl4nlp-1.13,D19-1129,1,0.836304,"Missing"
2021.repl4nlp-1.13,2020.acl-main.3,1,0.811576,"non-autoregressive way. Different from them, we propose to flatten the hierarchical representations and cast the problem into several sequence labeling tasks. 2.2 Language and Domain Adaptation Recently, cross-lingual and cross-domain models that aim to tackle low-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017), task-oriented semantic parsing (Chen et al., 2018; Schuster et al., 2019; Liu et al., 2019; Wu et al., 2019; Liu et al., 2020a; Chen et al., 2020; Liu et al., 2020b), named entity recognition (Ni et al., 2017; Xie et al., 2018; Jia et al., 2019; Liu et al., 2020c), speech recognition (Mimura et al., 2017; Winata et al., 2020), abstractive summarization (Zhu et al., 2019; Ouyang et al., 2019; Yu et al., 2021), etc. Despite numerous studies related to the cross-lingual and cross-domain areas, only a few of them have explored how to effectively adapt models to the target languages in target domains, and the investigated tasks are limited to sentiment analysis (Fern´andez et al., 2016; Li et al., 2020b), abusive languag"
2021.repl4nlp-1.13,P17-1135,0,0.038426,"Missing"
2021.repl4nlp-1.13,N19-1204,0,0.0164537,"-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017), task-oriented semantic parsing (Chen et al., 2018; Schuster et al., 2019; Liu et al., 2019; Wu et al., 2019; Liu et al., 2020a; Chen et al., 2020; Liu et al., 2020b), named entity recognition (Ni et al., 2017; Xie et al., 2018; Jia et al., 2019; Liu et al., 2020c), speech recognition (Mimura et al., 2017; Winata et al., 2020), abstractive summarization (Zhu et al., 2019; Ouyang et al., 2019; Yu et al., 2021), etc. Despite numerous studies related to the cross-lingual and cross-domain areas, only a few of them have explored how to effectively adapt models to the target languages in target domains, and the investigated tasks are limited to sentiment analysis (Fern´andez et al., 2016; Li et al., 2020b), abusive language detection (Pamungkas and Patti, 2019), and machine reading comprehension (Charlet et al., 2020). To the best of our knowledge, we are the first to study the combination of cross-lingual and cross-domain adaptations in the TCSP task. 3 Task Decomposition In this sect"
2021.repl4nlp-1.13,2021.findings-emnlp.161,0,0.0350375,"20) showed that leveraging a sequence-to-sequence model based on a copy mechanism (See et al., 2017) to directly generate the hierarchical representations was effective at parsing the nested queries. Taking this further, Chen et al. (2020) and Li et al. (2020a) extended the TOP dataset into multiple domains and multiple languages, and Li et al. (2020a) conducted zero-shot cross-lingual experiments using the combination of the multilingual pre-trained models (Conneau et al., 2020; Tran et al., 2020) and the copy mechanism method proposed in Rongali et al. (2020). Lately, Babu et al. (2021) and Shrivastava et al. (2021), which are concurrent works of X2Parser, proposed to tackled the TCSP task in a non-autoregressive way. Different from them, we propose to flatten the hierarchical representations and cast the problem into several sequence labeling tasks. 2.2 Language and Domain Adaptation Recently, cross-lingual and cross-domain models that aim to tackle low-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017), task-oriented semantic parsin"
2021.repl4nlp-1.13,P19-1078,1,0.676161,"he TCSP task in a non-autoregressive way. Different from them, we propose to flatten the hierarchical representations and cast the problem into several sequence labeling tasks. 2.2 Language and Domain Adaptation Recently, cross-lingual and cross-domain models that aim to tackle low-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017), task-oriented semantic parsing (Chen et al., 2018; Schuster et al., 2019; Liu et al., 2019; Wu et al., 2019; Liu et al., 2020a; Chen et al., 2020; Liu et al., 2020b), named entity recognition (Ni et al., 2017; Xie et al., 2018; Jia et al., 2019; Liu et al., 2020c), speech recognition (Mimura et al., 2017; Winata et al., 2020), abstractive summarization (Zhu et al., 2019; Ouyang et al., 2019; Yu et al., 2021), etc. Despite numerous studies related to the cross-lingual and cross-domain areas, only a few of them have explored how to effectively adapt models to the target languages in target domains, and the investigated tasks are limited to sentiment analysis (Fern´andez et al., 2016; Li et al., 2020b"
D07-1090,J93-2003,0,0.0604968,"to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases. 1 Introduction Given a source-language (e.g., French) sentence f , the problem of machine translation is to automatically produce a target-language (e.g., English) transˆ. The mathematics of the problem were forlation e malized by (Brown et al., 1993), and re-formulated by (Och and Ney, 2004) in terms of the optimization ˆ = arg max e e M X λm hm (e, f ) (1) m=1 where {hm (e, f )} is a set of M feature functions and {λm } a set of weights. One or more feature functions may be of the form h(e, f ) = h(e), in which case it is referred to as a language model. We focus on n-gram language models, which are trained on unlabeled monolingual text. As a general rule, more data tends to yield better language models. Questions that arise in this context include: (1) How might one build a language model that allows scaling to very large amounts of tra"
D07-1090,W04-3250,0,0.0773316,"are kept the same. Results are shown in Figure 5. The first part of the curve uses target data for training the language model. With Kneser-Ney smoothing (KN), the BLEU score improves from 0.3559 for 13 million tokens to 0.3832 for 237 million tokens. At such data sizes, Stupid Backoff (SB) with a constant backoff parameter α = 0.4 is around 1 BP worse than KN. On average, one gains 0.62 BP for each doubling of the training data with KN, and 0.66 BP per doubling with SB. Differences of more than 0.51 BP are statistically significant at the 0.05 level using bootstrap resampling (Noreen, 1989; Koehn, 2004). We then add a second language model using ldcnews data. The first point for ldcnews shows a large improvement of around 1.4 BP over the last point for target for both KN and SB, which is approximately twice the improvement expected from doubling the amount of data. This seems to be caused by adding a new domain and combining two models. After that, we find an improvement of 0.56–0.70 BP for each doubling of the ldcnews data. The gap between Kneser-Ney Smoothing and Stupid Backoff narrows, starting with a difference of 0.85 BP and ending with a not significant difference of 0.24 BP. Adding a"
D07-1090,J04-4002,1,0.289967,"models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases. 1 Introduction Given a source-language (e.g., French) sentence f , the problem of machine translation is to automatically produce a target-language (e.g., English) transˆ. The mathematics of the problem were forlation e malized by (Brown et al., 1993), and re-formulated by (Och and Ney, 2004) in terms of the optimization ˆ = arg max e e M X λm hm (e, f ) (1) m=1 where {hm (e, f )} is a set of M feature functions and {λm } a set of weights. One or more feature functions may be of the form h(e, f ) = h(e), in which case it is referred to as a language model. We focus on n-gram language models, which are trained on unlabeled monolingual text. As a general rule, more data tends to yield better language models. Questions that arise in this context include: (1) How might one build a language model that allows scaling to very large amounts of training data? (2) How much does translation"
D07-1090,P02-1040,0,0.10668,"ing. We focused on machine translation when describing the queued language model access. However, it is general enough that it may also be applicable to speech decoders and optical character recognition systems. 7 Experiments We trained 5-gram language models on amounts of text varying from 13 million to 2 trillion tokens. The data is divided into four sets; language models are trained for each set separately 4 . For each training data size, we report the size of the resulting language model, the fraction of 5-grams from the test data that is present in the language model, and the BLEU score (Papineni et al., 2002) obtained by the machine translation system. For smaller training sizes, we have also computed test-set perplexity using Kneser-Ney Smoothing, and report it for comparison. 7.1 Data Sets We compiled four language model training data sets, listed in order of increasing size: 3 One additional round for the sentence end marker. Experience has shown that using multiple, separately trained language models as feature functions in Eq (1) yields better results than using a single model trained on all data. 4 1e+12 x1.6/x2 1000 x1.8/x2 100 1e+10 x1.8/x2 10 1e+09 1 x1.8/x2 target +ldcnews +webnews +web"
D07-1090,W06-1626,0,0.0216258,"01) for a discussion of n-gram models and smoothing. In principle, the predictive accuracy of the language model can be improved by increasing the order of the n-gram. However, doing so further exacerbates the sparse data problem. The present work addresses the challenges of processing an amount of training data sufficient for higher-order n-gram models and of storing and managing the resulting values for efficient use by the decoder. 3 Related Work on Distributed Language Models The topic of large, distributed language models is relatively new. Recently a two-pass approach has been proposed (Zhang et al., 2006), wherein a lowerorder n-gram is used in a hypothesis-generation phase, then later the K-best of these hypotheses are re-scored using a large-scale distributed language model. The resulting translation performance was shown to improve appreciably over the hypothesis deemed best by the first-stage system. The amount of data used was 3 billion words. More recently, a large-scale distributed language model has been proposed in the contexts of speech recognition and machine translation (Emami et al., 2007). The underlying architecture is similar to (Zhang et al., 2006). The difference is that they"
D12-1089,D07-1090,1,0.269774,"• The 109 -French-English bilingual corpus with about one billion tokens from the Workshop on Statistical Machine Translation (WMT).2 These enormous data sets yield translation models that are expensive to store and process. Even with 1 2 LDC catalog No. LDC2006T13 http://www.statmt.org/wmt11/translation-task.html The most resource-intensive components of a statistical machine translation system are the language model and the phrase table. Recently, compact representations of the language model have attracted the attention of the research community, for instance in Talbot and Osborne (2007), Brants et al. (2007), Pauls and Klein (2011) or Heafield (2011), to name a few. In this paper, we address the other problem of any statistical machine translation system: large phrase tables. Johnson et al. (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. This motivated us to perform a systematic comparison of different pruning methods. However, we found that many existing methods employ ad-hoc heuristics without theoretical foundation. The pruning criterion introduced in this work is inspired by the very successful and still state-of-theart language mod"
D12-1089,J93-2003,0,0.0650262,"ine translation system similar to (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(˜ e|f˜) and p(f˜|˜ e), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Zens et al., 2004). The feature weights were tuned on a development set by applying minimum error rate training (MERT) under the Bleu criterion (Och, 2003; Macherey et al., 2008). We ran MERT once with the full phrase table and then kept the feature weights fixed, i. e., we did not rerun MERT after pruning to avoid adding unnecessary noise. We extract phrases up to a length of six words. The baseline system already includes phrase table pruning by removing singletons and keeping up to 30 target languag"
D12-1089,chen-etal-2008-improving,0,0.0154414,"translation model quality. Furthermore, a comparison to other methods is missing. Here we close this gap and perform a systematic comparison. The same idea of significance-based pruning was exploited in (Yang and Zheng, 2009; Tomeh et al., 2009) for hierarchical statistical machine translation. 973 A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce the phrase table size. The work in this paper is independent of the way the phrase extraction is done, so those approaches are complementary to our work. 3 Pruning Using Simple Statistics In this section, we will review existing pruning"
D12-1089,N09-1015,0,0.139572,"uality. Furthermore, a comparison to other methods is missing. Here we close this gap and perform a systematic comparison. The same idea of significance-based pruning was exploited in (Yang and Zheng, 2009; Tomeh et al., 2009) for hierarchical statistical machine translation. 973 A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce the phrase table size. The work in this paper is independent of the way the phrase extraction is done, so those approaches are complementary to our work. 3 Pruning Using Simple Statistics In this section, we will review existing pruning methods based on sim"
D12-1089,J07-2003,0,0.0368358,"Missing"
D12-1089,2011.mtsummit-papers.20,0,0.15905,"hierarchical statistical machine translation. 973 A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce the phrase table size. The work in this paper is independent of the way the phrase extraction is done, so those approaches are complementary to our work. 3 Pruning Using Simple Statistics In this section, we will review existing pruning methods based on simple phrase table statistics. There are two common classes of these methods: absolute phrase table pruning and relative phrase table pruning. 3.1 Absolute pruning Absolute pruning methods rely only on the statistics of a single phra"
D12-1089,2007.mtsummit-papers.22,0,0.303356,"t large parts of the phrase table can be removed without affecting translation quality. Their pruning criterion relies on statistical significance tests. However, it is unclear how this significance-based pruning criterion is related to translation model quality. Furthermore, a comparison to other methods is missing. Here we close this gap and perform a systematic comparison. The same idea of significance-based pruning was exploited in (Yang and Zheng, 2009; Tomeh et al., 2009) for hierarchical statistical machine translation. 973 A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce"
D12-1089,N07-2006,0,0.486677,"Missing"
D12-1089,W11-2123,0,0.0225503,"about one billion tokens from the Workshop on Statistical Machine Translation (WMT).2 These enormous data sets yield translation models that are expensive to store and process. Even with 1 2 LDC catalog No. LDC2006T13 http://www.statmt.org/wmt11/translation-task.html The most resource-intensive components of a statistical machine translation system are the language model and the phrase table. Recently, compact representations of the language model have attracted the attention of the research community, for instance in Talbot and Osborne (2007), Brants et al. (2007), Pauls and Klein (2011) or Heafield (2011), to name a few. In this paper, we address the other problem of any statistical machine translation system: large phrase tables. Johnson et al. (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. This motivated us to perform a systematic comparison of different pruning methods. However, we found that many existing methods employ ad-hoc heuristics without theoretical foundation. The pruning criterion introduced in this work is inspired by the very successful and still state-of-theart language model pruning criterion based on entropy measu"
D12-1089,D07-1103,0,0.871256,"e expensive to store and process. Even with 1 2 LDC catalog No. LDC2006T13 http://www.statmt.org/wmt11/translation-task.html The most resource-intensive components of a statistical machine translation system are the language model and the phrase table. Recently, compact representations of the language model have attracted the attention of the research community, for instance in Talbot and Osborne (2007), Brants et al. (2007), Pauls and Klein (2011) or Heafield (2011), to name a few. In this paper, we address the other problem of any statistical machine translation system: large phrase tables. Johnson et al. (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. This motivated us to perform a systematic comparison of different pruning methods. However, we found that many existing methods employ ad-hoc heuristics without theoretical foundation. The pruning criterion introduced in this work is inspired by the very successful and still state-of-theart language model pruning criterion based on entropy measures (Stolcke, 1998). We motivate its derivation by stating the desiderata for a good phrase table pruning criterion: • Soundness: The criterion should"
D12-1089,N03-1017,0,0.0121617,"65 M 232 M 210 M 962 M 827 M 20 18 16 14 Table 3: Training data statistics. Number of words in the training data (M=millions). 12 Prob Thres Hist 10 8 1 • French-English For each pair, we train two separate system, one for each direction. Thus it can happen that a phrase is pruned for X-to-Y, but not for Y-to-X. These four language pairs represent a nice range of training corpora sizes, as shown in Table 3. 6.2 Baseline System Pruning experiments were performed on top of the following baseline system. We used a phrasebased statistical machine translation system similar to (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(˜ e|f˜) and p(f˜|˜ e), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel e"
D12-1089,P07-2045,1,0.0199804,"are commonly deficient in at least one of them. We thus designed a novel pruning criterion that not only meets these objectives, it also performs very well in empirical evaluations. The novel contributions of this paper are: 1. a systematic description of existing phrase table pruning methods. 2. a new, theoretically sound phrase table pruning criterion. 3. an experimental comparison of several pruning methods for several language pairs. 2 Related Work The most basic pruning methods rely on probability and count cutoffs. We will cover the techniques that are implemented in the Moses toolkit (Koehn et al., 2007) and the Pharaoh decoder (Koehn, 2004) in Section 3. We are not aware of any work that analyzes their efficacy in a systematic way. It is thus not surprising that some of them perform poorly, as our experimental results will show. The work of Johnson et al. (2007) is promising as it shows that large parts of the phrase table can be removed without affecting translation quality. Their pruning criterion relies on statistical significance tests. However, it is unclear how this significance-based pruning criterion is related to translation model quality. Furthermore, a comparison to other methods"
D12-1089,koen-2004-pharaoh,0,0.0554099,"em. We thus designed a novel pruning criterion that not only meets these objectives, it also performs very well in empirical evaluations. The novel contributions of this paper are: 1. a systematic description of existing phrase table pruning methods. 2. a new, theoretically sound phrase table pruning criterion. 3. an experimental comparison of several pruning methods for several language pairs. 2 Related Work The most basic pruning methods rely on probability and count cutoffs. We will cover the techniques that are implemented in the Moses toolkit (Koehn et al., 2007) and the Pharaoh decoder (Koehn, 2004) in Section 3. We are not aware of any work that analyzes their efficacy in a systematic way. It is thus not surprising that some of them perform poorly, as our experimental results will show. The work of Johnson et al. (2007) is promising as it shows that large parts of the phrase table can be removed without affecting translation quality. Their pruning criterion relies on statistical significance tests. However, it is unclear how this significance-based pruning criterion is related to translation model quality. Furthermore, a comparison to other methods is missing. Here we close this gap and"
D12-1089,D08-1076,0,0.0173581,"l language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(˜ e|f˜) and p(f˜|˜ e), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Zens et al., 2004). The feature weights were tuned on a development set by applying minimum error rate training (MERT) under the Bleu criterion (Och, 2003; Macherey et al., 2008). We ran MERT once with the full phrase table and then kept the feature weights fixed, i. e., we did not rerun MERT after pruning to avoid adding unnecessary noise. We extract phrases up to a length of six words. The baseline system already includes phrase table pruning by removing singletons and keeping up to 30 target language phrases per source phrase. We found that this does not affect transla977 2 4 Number of Phrases [millions] 8 Figure 1: Comparison of probability-based pruning methods for German-English. tion quality significantly4 . All pruning experiments are done on top of this. 6.3"
D12-1089,W04-3243,0,0.0373733,"ility of the contingency table via the hypergeometric distribution:     N (f˜) N −N (f˜) · N (f˜,˜ e) N (˜ e)−N (f˜,˜ e)   ph (N (f˜, e˜)) = (4) N N (˜ e) The p-value is then calculated as the sum of all probabilities that are at least as extreme. The lower the p-value, the less likely this phrase pair occurred with the observed frequency by chance; we thus prune a phrase pair (f˜, e˜) if:   ∞ X  ph (k) &gt; τF (5) k=N (f˜,˜ e) for some pruning threshold τF . More details of this approach can be found in Johnson et al. (2007). The idea of using Fisher’s exact test was first explored by Moore (2004) in the context of word alignment. 5 In this section, we will derive a novel entropy-based pruning criterion. 5.1 Significance Pruning In this section, we briefly review significance pruning following Johnson et al. (2007). The idea of significance pruning is to test whether a source phrase f˜ and a target phrase e˜ co-occur more frequently in a bilingual corpus than they should just by chance. Using some simple statistics derived from the bilingual corpus, namely • N (f˜) the count of the source phrase f˜ • N (˜ e) the count of the target phrase e˜ • N (f˜, e˜) the co-occurence count of the s"
D12-1089,J04-4002,0,0.0312374,"2 M 827 M 20 18 16 14 Table 3: Training data statistics. Number of words in the training data (M=millions). 12 Prob Thres Hist 10 8 1 • French-English For each pair, we train two separate system, one for each direction. Thus it can happen that a phrase is pruned for X-to-Y, but not for Y-to-X. These four language pairs represent a nice range of training corpora sizes, as shown in Table 3. 6.2 Baseline System Pruning experiments were performed on top of the following baseline system. We used a phrasebased statistical machine translation system similar to (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(˜ e|f˜) and p(f˜|˜ e), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using"
D12-1089,P03-1021,0,0.0216351,"ws data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(˜ e|f˜) and p(f˜|˜ e), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Zens et al., 2004). The feature weights were tuned on a development set by applying minimum error rate training (MERT) under the Bleu criterion (Och, 2003; Macherey et al., 2008). We ran MERT once with the full phrase table and then kept the feature weights fixed, i. e., we did not rerun MERT after pruning to avoid adding unnecessary noise. We extract phrases up to a length of six words. The baseline system already includes phrase table pruning by removing singletons and keeping up to 30 target language phrases per source phrase. We found that this does not affect transla977 2 4 Number of Phrases [millions] 8 Figure 1: Comparison of probability-based pruning methods for German-English. tion quality significantly4 . All pruning experiments are d"
D12-1089,P02-1040,0,0.103712,"of six words. The baseline system already includes phrase table pruning by removing singletons and keeping up to 30 target language phrases per source phrase. We found that this does not affect transla977 2 4 Number of Phrases [millions] 8 Figure 1: Comparison of probability-based pruning methods for German-English. tion quality significantly4 . All pruning experiments are done on top of this. 6.3 Results In this section, we present the experimental results. Translation results are reported on the WMT’07 news commentary blind set. We will show translation quality measured with the Bleu score (Papineni et al., 2002) as a function of the phrase table size (number of phrases). Being in the upper left corner of these figures is desirable. First, we show a comparison of several probability-based pruning methods in Figure 1. We compare • Prob. Absolute pruning based on Eq. (2). • Thres. Threshold pruning based on Eq. (3). • Hist. Histogram pruning as described in Section 3.2.5 We observe that these three methods perform equally well. There is no difference between absolute and relative pruning methods, except that the two relative methods (Thres and Hist) are limited by 4 The Bleu score drops are as follows:"
D12-1089,P11-1027,0,0.00914444,"lish bilingual corpus with about one billion tokens from the Workshop on Statistical Machine Translation (WMT).2 These enormous data sets yield translation models that are expensive to store and process. Even with 1 2 LDC catalog No. LDC2006T13 http://www.statmt.org/wmt11/translation-task.html The most resource-intensive components of a statistical machine translation system are the language model and the phrase table. Recently, compact representations of the language model have attracted the attention of the research community, for instance in Talbot and Osborne (2007), Brants et al. (2007), Pauls and Klein (2011) or Heafield (2011), to name a few. In this paper, we address the other problem of any statistical machine translation system: large phrase tables. Johnson et al. (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. This motivated us to perform a systematic comparison of different pruning methods. However, we found that many existing methods employ ad-hoc heuristics without theoretical foundation. The pruning criterion introduced in this work is inspired by the very successful and still state-of-theart language model pruning criterion bas"
D12-1089,2011.eamt-1.35,0,0.125555,"ical machine translation. 973 A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce the phrase table size. The work in this paper is independent of the way the phrase extraction is done, so those approaches are complementary to our work. 3 Pruning Using Simple Statistics In this section, we will review existing pruning methods based on simple phrase table statistics. There are two common classes of these methods: absolute phrase table pruning and relative phrase table pruning. 3.1 Absolute pruning Absolute pruning methods rely only on the statistics of a single phrase pair (f˜, e˜). Hence, they a"
D12-1089,D07-1049,0,0.011536,"rillion words of web data.1 • The 109 -French-English bilingual corpus with about one billion tokens from the Workshop on Statistical Machine Translation (WMT).2 These enormous data sets yield translation models that are expensive to store and process. Even with 1 2 LDC catalog No. LDC2006T13 http://www.statmt.org/wmt11/translation-task.html The most resource-intensive components of a statistical machine translation system are the language model and the phrase table. Recently, compact representations of the language model have attracted the attention of the research community, for instance in Talbot and Osborne (2007), Brants et al. (2007), Pauls and Klein (2011) or Heafield (2011), to name a few. In this paper, we address the other problem of any statistical machine translation system: large phrase tables. Johnson et al. (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. This motivated us to perform a systematic comparison of different pruning methods. However, we found that many existing methods employ ad-hoc heuristics without theoretical foundation. The pruning criterion introduced in this work is inspired by the very successful and still state-"
D12-1089,2009.mtsummit-papers.17,0,0.54556,"g that some of them perform poorly, as our experimental results will show. The work of Johnson et al. (2007) is promising as it shows that large parts of the phrase table can be removed without affecting translation quality. Their pruning criterion relies on statistical significance tests. However, it is unclear how this significance-based pruning criterion is related to translation model quality. Furthermore, a comparison to other methods is missing. Here we close this gap and perform a systematic comparison. The same idea of significance-based pruning was exploited in (Yang and Zheng, 2009; Tomeh et al., 2009) for hierarchical statistical machine translation. 973 A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to gener"
D12-1089,2011.iwslt-papers.10,0,0.248205,"fferent approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or would be costly to generate. Duan et al. (2011), Sanchis-Trilles et al. (2011) and Tomeh et al. (2011) modify the phrase extraction methods in order to reduce the phrase table size. The work in this paper is independent of the way the phrase extraction is done, so those approaches are complementary to our work. 3 Pruning Using Simple Statistics In this section, we will review existing pruning methods based on simple phrase table statistics. There are two common classes of these methods: absolute phrase table pruning and relative phrase table pruning. 3.1 Absolute pruning Absolute pruning methods rely only on the statistics of a single phrase pair (f˜, e˜). Hence, they are independent of other"
D12-1089,C96-2141,0,0.185429,"., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(˜ e|f˜) and p(f˜|˜ e), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Zens et al., 2004). The feature weights were tuned on a development set by applying minimum error rate training (MERT) under the Bleu criterion (Och, 2003; Macherey et al., 2008). We ran MERT once with the full phrase table and then kept the feature weights fixed, i. e., we did not rerun MERT after pruning to avoid adding unnecessary noise. We extract phrases up to a length of six words. The baseline system already includes phrase table pruning by removing singletons and keeping up to 30 target language phrases per source phrase. We found that this does not affect t"
D12-1089,P09-2060,0,0.0457469,"is thus not surprising that some of them perform poorly, as our experimental results will show. The work of Johnson et al. (2007) is promising as it shows that large parts of the phrase table can be removed without affecting translation quality. Their pruning criterion relies on statistical significance tests. However, it is unclear how this significance-based pruning criterion is related to translation model quality. Furthermore, a comparison to other methods is missing. Here we close this gap and perform a systematic comparison. The same idea of significance-based pruning was exploited in (Yang and Zheng, 2009; Tomeh et al., 2009) for hierarchical statistical machine translation. 973 A different approach to phrase table pruning was undertaken by Eck et al. (2007a; 2007b). They rely on usage statistics from translating sample data, so it is not self-contained. However, it could be combined with the methods proposed here. Another approach to phrase table pruning is triangulation (Chen et al., 2008; Chen et al., 2009). This requires additional bilingual corpora, namely from the source language as well as from the target language to a third bridge language. In many situations this does not exist or wou"
D12-1089,W06-3108,1,0.466934,"erformed on top of the following baseline system. We used a phrasebased statistical machine translation system similar to (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(˜ e|f˜) and p(f˜|˜ e), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Zens et al., 2004). The feature weights were tuned on a development set by applying minimum error rate training (MERT) under the Bleu criterion (Och, 2003; Macherey et al., 2008). We ran MERT once with the full phrase table and then kept the feature weights fixed, i. e., we did not rerun MERT after pruning to avoid adding unnecessary noise. We extract phrases up to a length of six words. The baseline system already"
D12-1089,2008.iwslt-papers.8,1,0.856044,"14 Table 3: Training data statistics. Number of words in the training data (M=millions). 12 Prob Thres Hist 10 8 1 • French-English For each pair, we train two separate system, one for each direction. Thus it can happen that a phrase is pruned for X-to-Y, but not for Y-to-X. These four language pairs represent a nice range of training corpora sizes, as shown in Table 3. 6.2 Baseline System Pruning experiments were performed on top of the following baseline system. We used a phrasebased statistical machine translation system similar to (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(˜ e|f˜) and p(f˜|˜ e), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon ("
D12-1089,2002.tmi-tutorials.2,0,0.0380388,"lish 42 M 45 M 56 M 65 M 232 M 210 M 962 M 827 M 20 18 16 14 Table 3: Training data statistics. Number of words in the training data (M=millions). 12 Prob Thres Hist 10 8 1 • French-English For each pair, we train two separate system, one for each direction. Thus it can happen that a phrase is pruned for X-to-Y, but not for Y-to-X. These four language pairs represent a nice range of training corpora sizes, as shown in Table 3. 6.2 Baseline System Pruning experiments were performed on top of the following baseline system. We used a phrasebased statistical machine translation system similar to (Zens et al., 2002; Koehn et al., 2003; Och and Ney, 2004; Zens and Ney, 2008). We trained a 4gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(˜ e|f˜) and p(f˜|˜ e), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alig"
D12-1089,C04-1006,1,0.80557,". We trained a 4gram language model on the target side of the bilingual corpora and a second 4-gram language model on the provided monolingual news data. All language models used Kneser-Ney smoothing. The baseline system uses the common phrase translation models, such as p(˜ e|f˜) and p(f˜|˜ e), lexical models, word and phrase penalty, distortion penalty as well as a lexicalized reordering model (Zens and Ney, 2006). The word alignment was trained with six iterations of IBM model 1 (Brown et al., 1993) and 6 iterations of the HMM alignment model (Vogel et al., 1996) using a symmetric lexicon (Zens et al., 2004). The feature weights were tuned on a development set by applying minimum error rate training (MERT) under the Bleu criterion (Och, 2003; Macherey et al., 2008). We ran MERT once with the full phrase table and then kept the feature weights fixed, i. e., we did not rerun MERT after pruning to avoid adding unnecessary noise. We extract phrases up to a length of six words. The baseline system already includes phrase table pruning by removing singletons and keeping up to 30 target language phrases per source phrase. We found that this does not affect transla977 2 4 Number of Phrases [millions] 8 F"
D19-1012,S19-2005,0,0.0266457,"l., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017); Wang and Wan (2018); Zhou and Wang (2018) successfully introduce a framework of controlling the sentiment and emotion of the generated response, while (Zhou and Wang, 2018) also introduces a new Twitter conversation dataset and propose to distantly supervised the generative model with emojis. Meanwhile, (Lubis et al., 2018; Rashkin et al., 2018) also introduce new datasets for empathetic dialogues and train multi-task models on it. 3 Mixture of Empathetic Listeners The dialogue context is an alternating set of utter"
D19-1012,P18-5002,0,0.0254446,"Missing"
D19-1012,P19-1358,0,0.0329288,"erban et al., 2016; Vinyals and Le, 2015; Wolf et al., 2019). A recent trend is to produce personalized responses by conditioning the generation on a persona profile to make the response more consistent through the dialogue (Li et al., 2016b). In particular, PersonaChat (Zhang et al., 2018b; Kulikov et al., 2018) dataset was created, and then extended in ConvAI 2 challenge (Dinan et al., 2019), to show that by adding persona information as input to the model, the produced responses elicit more consistent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et"
D19-1012,D16-1230,0,0.213158,"Missing"
D19-1012,P19-1542,1,0.807543,"es by conditioning the generation on a persona profile to make the response more consistent through the dialogue (Li et al., 2016b). In particular, PersonaChat (Zhang et al., 2018b; Kulikov et al., 2018) dataset was created, and then extended in ConvAI 2 challenge (Dinan et al., 2019), to show that by adding persona information as input to the model, the produced responses elicit more consistent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017)"
D19-1012,P18-1136,1,0.896748,"Missing"
D19-1012,D18-1298,0,0.414457,"rsation models have shown to be successful in scalable training and generating fluent and relevant responses (Vinyals and Le, 2015). However, it has been pointed out by Li et al. (2016a,b,c); Wu et al. (2018b) that only using Maximum Likelihood Estimation as the objective function tends to lead to generic and repetitive responses like “I am sorry”. Furthermore, many others have shown that the incorporation of additional inductive bias leads to a more engaging chatbot, such as understanding commonsense (Dinan et al., 2018), or modeling consistent persona (Li et al., 2016b; Zhang et al., 2018a; Mazare et al., 2018a). Meanwhile, another important aspect of an engaging human conversation that received relaTable 1 shows an conversation from the empathetic-dialogues dataset (Rashkin et al., 2018) about how an empathetic person would respond to the stressful situation the Speaker has been through. However, despite the importance of empathy and emotional understanding in human conversations, it is still very challenging to train a dialogue agent able to recognize and respond with the correct emotion. So far, to solve the problem of empathetic dialogue response generation, which is to understand the user emot"
D19-1012,S19-2184,1,0.802043,"follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017); Wang and Wan (2018); Zhou and Wang (2018) successfully introduce a framework of controlling the sentiment and emotion of the generated response, while (Zhou and Wang, 2018) also introduces a new Twitter conversation dataset and propose to distantly supervised the generative model with emojis. Meanwhile, (Lubis et al., 2018; Rashkin et al., 2018) also introduce new datasets for empathetic dialogues and train multi-task mode"
D19-1012,D16-1147,0,0.0241154,"for each token. i where T RSDec refers to the i-th listener, including the shared one. Conceptually, we expect that 0 , to be the output from the shared listener, T RSDec a general representation which can help the model to capture the dialogue context. On the other hand, we expect that each empathetic listener learns how to respond to a particular emotion. To model this behavior, we assign different weights to each empathetic listener according to the user emotion distribution, while assigning a fixed weight of 1 to the shared listener. To elaborate, we construct a Key-Value Memory Network (Miller et al., 2016) and represent each memory slot as a vector pair (ki , Vi ), where ki ∈ Rdmodel denotes the key vector and Vi is from Equation 4. Then, the encoder informed query q is used to address the key vectors k by performing a dot product followed by a Softmax function. Thus, we have: tracker. We first flatten all dialogue turns in C, and map each token into its vectorial representation using the context embedding E C . Then the encoder encodes the context sequence into a context representation. We add a query token QRY at the beginning of each input sequence as in BERT (Devlin et al., 2018), to comput"
D19-1012,N16-1014,0,0.269283,"y less focus is emotional understanding and empathy (Rashkin et al., 2018; Dinan et al., 2019; Wolf et al., 2019). Intuitively, ordinary social conversations between two humans are often about their daily lives that revolve around happy or sad experiences. In such scenarios, people generally tend to respond in a way that acknowledges the feelings of their conversational partners. Introduction Neural network approaches for conversation models have shown to be successful in scalable training and generating fluent and relevant responses (Vinyals and Le, 2015). However, it has been pointed out by Li et al. (2016a,b,c); Wu et al. (2018b) that only using Maximum Likelihood Estimation as the objective function tends to lead to generic and repetitive responses like “I am sorry”. Furthermore, many others have shown that the incorporation of additional inductive bias leads to a more engaging chatbot, such as understanding commonsense (Dinan et al., 2018), or modeling consistent persona (Li et al., 2016b; Zhang et al., 2018a; Mazare et al., 2018a). Meanwhile, another important aspect of an engaging human conversation that received relaTable 1 shows an conversation from the empathetic-dialogues dataset (Rash"
D19-1012,P02-1040,0,0.104181,"Missing"
D19-1012,P16-1094,0,0.266273,"y less focus is emotional understanding and empathy (Rashkin et al., 2018; Dinan et al., 2019; Wolf et al., 2019). Intuitively, ordinary social conversations between two humans are often about their daily lives that revolve around happy or sad experiences. In such scenarios, people generally tend to respond in a way that acknowledges the feelings of their conversational partners. Introduction Neural network approaches for conversation models have shown to be successful in scalable training and generating fluent and relevant responses (Vinyals and Le, 2015). However, it has been pointed out by Li et al. (2016a,b,c); Wu et al. (2018b) that only using Maximum Likelihood Estimation as the objective function tends to lead to generic and repetitive responses like “I am sorry”. Furthermore, many others have shown that the incorporation of additional inductive bias leads to a more engaging chatbot, such as understanding commonsense (Dinan et al., 2018), or modeling consistent persona (Li et al., 2016b; Zhang et al., 2018a; Mazare et al., 2018a). Meanwhile, another important aspect of an engaging human conversation that received relaTable 1 shows an conversation from the empathetic-dialogues dataset (Rash"
D19-1012,D14-1162,0,0.0820006,"l MoEL vs TRS MoEL vs Multi-TRS 4 4.1 p(r1:t |C, r0:t−1 ) = softmax(O&gt; W ) (9) where O ∈ Rdmodel ×t is the output of meta listener and p(r1:t |C, r0:t−1 ) is a distribution over the vocabulary for the next tokens. We then use a standard maximum likelihood estimator (MLE) to optimize the response prediction: L2 = − log p (St |C) 4.2 Experiment Dataset Training We train our model using Adam optimizer (Kingma and Ba, 2014) and varied the learning rate during training following (Vaswani et al., 2017). The weight of both losses α and β are set to 1 for simplicity. We use pre-trained Glove vectors (Pennington et al., 2014) to initialize the word embedding and we share it across the encoder and the decoder. The rest of the parameters are randomly initialized. In the early training stage, emotion tracker randomly assign weights to the listeners, and may send noisy gradient flow back to the wrong listeners, which can make the model convergence harder. To stabilize the learning process, we replace the distribution p of the listeners with the or(10) Lastly, all the parameters are jointly trained endto-end to optimize the listener selection and response generation by minimizing the weightedsum of two losses: L = αL1"
D19-1012,N19-1126,0,0.045197,"Missing"
D19-1012,W18-5713,0,0.0125101,"019), to show that by adding persona information as input to the model, the produced responses elicit more consistent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017); Wang and Wan (2018); Zhou and Wang (2018) successfully introduce a framework of controlling the sentiment and emotion of the generated response, while (Zhou and Wang, 2018) also introduces a new Twitter conversation dataset and propose to distantly supervised the generative mo"
D19-1012,S19-2021,1,0.875815,"Missing"
D19-1012,W18-6243,1,0.763174,"istent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017); Wang and Wan (2018); Zhou and Wang (2018) successfully introduce a framework of controlling the sentiment and emotion of the generated response, while (Zhou and Wang, 2018) also introduces a new Twitter conversation dataset and propose to distantly supervised the generative model with emojis. Meanwhile, (Lubis et al., 2018; Rashkin et al., 2018) also introduce new datasets for empa"
D19-1012,K18-1053,0,0.0147919,"roduce personalized responses by conditioning the generation on a persona profile to make the response more consistent through the dialogue (Li et al., 2016b). In particular, PersonaChat (Zhang et al., 2018b; Kulikov et al., 2018) dataset was created, and then extended in ConvAI 2 challenge (Dinan et al., 2019), to show that by adding persona information as input to the model, the produced responses elicit more consistent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dial"
D19-1012,P18-1205,0,0.0948744,"Missing"
D19-1012,P18-1104,0,0.0734934,"lly, our analysis demonstrates that not only MoEL effectively attends to the right listener, but also each listener learns how to properly react to its corresponding emotion, hence allowing a more interpretable generative process. of work. The first is a multi-task approach that jointly trains a model to predict the current emotional state of the user and generate an appropriate response based on the state (Lubis et al., 2018; Rashkin et al., 2018). Instead, the second line of work focuses on conditioning the response generation to a certain fixed emotion (Hu et al., 2017; Wang and Wan, 2018; Zhou and Wang, 2018; Zhou et al., 2018). Both cases have succeeded in generating empathetic and emotional responses, but have neglected some crucial points in empathetic dialogue response generation. 1) The first assumes that by understanding the emotion, the model implicitly learns how to respond appropriately. However, without any additional inductive bias, a single decoder learning to respond for all emotions will not only lose interpretability in the generation process, but will also promote more generic responses. 2) The second assumes that the emotion to condition the generation on is given as input, but w"
D19-1012,D16-1127,0,\N,Missing
D19-1012,D16-1110,1,\N,Missing
D19-1012,W19-5917,0,\N,Missing
D19-1129,D16-1250,0,0.0328322,"mains (weather, alarm, and reminder) and translate them using bilingual lexicons. We refine the embeddings by leveraging the framework proposed in Artetxe et al. (2017). Let X and Z be the aligned cross-lingual word embeddings between two languages. Xi∗ and Zj∗ are the embeddings for the ith source word and j th target word. We denote a binary dictionary matrix D: Dij = 1 if the ith source language word is aligned with the j th target language word and Dij = 0 otherwise. The goal is to find the optimal mapping matrix W∗ by minimizing: W∗ = arg min W X Dij ||Xi∗ W − Zj∗ ||2 . (1) i,j Following Artetxe et al. (2016), with orthogonal constraints, mean centering, and length normaliza1298 1 The embeddings are available in https://fasttext.cc tion, we can maximize the following instead: W∗ = arg max Tr(XWZT DT ). (2) W We iteratively optimize Equation 2 until distances between domain-related seed words are closer than a certain threshold after refinement. Figure 1 illustrates better alignment for domainrelated words after refinement. 3.2 Gaussian Noise Injection To cope with the noise in alignments, we inject Gaussian noise to English embeddings, so the trained model will be more robust to variance. This is"
D19-1129,P17-1042,0,0.119995,"the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lingual meta-representations for code-switching name entity recognition by combining multiple monolingual word embeddings. Chen et al. (2018) proposed a teacher-student frame"
D19-1129,Q17-1010,0,0.0433367,"r, forecast, temperature, rain, hot, cold, remind, forget, alarm, cancel, tomorrow, which are related to the three dialogue domains (weather, alarm, and reminder). We translate them by leveraging bilingual dictionaries2 . The corresponding translations in Spanish and Thai are clima, pron´ostico, temperatura, lluvia, caliente, fr´ıo, recordar, olvidar, alarma, cancelar, ma˜nana and อากาศ, พยากรณ, อุณหภูมิ, ฝน, รอน, หนาว, 2 https://github.com/facebookresearch/MUSE 4.4 Evaluation We implement and evaluate the following models: Zero-shot SLU Upadhyay et al. (2018) used cross-lingual embeddings (Bojanowski et al., 2017) to do zero-shot transfer learning. Conditional Random Fields (CRF) We reproduce the baseline model in Schuster et al. (2019), and also add embedding noise, cross-lingual refinement, and delexicalization. Latent Variable Model (LVM) - Ours We replace the CRF module with latent variables and also apply it to intent prediction. Besides, we directly compare with the baseline models illustrated in Schuster et al. (2019): Multi. CoVe w/ auto They combined Multilingual CoVe (Yu et al., 2018) with an auto-encoder objective and then used the trained encoder with the CRF model. 1300 noche เ น What will"
D19-1129,D18-1038,0,0.197164,"Cortana) as a virtual agent to tend to the needs of the users. However, these agents have mostly been trained with the monolingual dataset that is often expensive to build or acquire. In order to cope with the scarcity of low-resource language dialogue data, we are motivated to look into cross-lingual dialogue systems which can adapt with very little or no training data in the target language. This task of zero-shot adaptation of dialogue systems to different languages is relatively new and has not been explored thoroughly enough yet. The main approach of previous work (Upadhyay et al., 2018; Chen et al., 2018; Schuster et al., 2019) in this task is using aligned cross-lingual word embeddings between source and target languages. However, this method suffers from imperfect alignments between the source and target language embeddings. This can be attributed not only to the noise in aligning two different embeddings, but also to the inherent discrepancies in different languages such as Thai and English which come from entirely different roots. To address such variance in the alignment, we turn to probabilistic modeling with latent variables as it has been successfully used in several recent taskorient"
D19-1129,D17-1169,0,0.0396427,"but not negligible differences across languages. Instead, using latent variables will allow us to model the distribution that captures the variance of semantically similar sentences across different languages. The whole training process is defined as follows: [h1 ...ht ...hT ] = BiLSTM(e∗ ), (3) T X exp(mt ) mt = ht wa , at = PT , v= at ht , exp(m ) j j=1 t=1 (4)     S I µt µ = WrS ht , = WrI v, log σtS )2 log σ I )2 (5) ztS ∼ qtS (z|ht ), z I ∼ q I (z|v), (6) pSt (st |ztS ) = Softmax(WgS ztS ), (7) I I p (I|z ) = Softmax(WgI z I ), (8) where attention vector (v) is obtained by following Felbo et al. (2017) and wa is the weight {S,I} matrix for the attention layer, W{r,g} are trainable parameters, superscripts S and I refer to slot prediction and intent detection respectively, subscript “r” refers to “recognition” for obtaining the LI = Ez I [log pI (I|z I )], LSt = EztS [log pSt (st |ztS )], LS = T X LSt , (9) (10) (11) t=1 hence, the final objective function to minimize is, L = LS + LI . (12) The model prediction is not deterministic since the latent variables ztS and z I are sampled from the Gaussian distributions. Therefore, in the inference time, we use the true mean µSt and µI to replace z"
D19-1129,N18-1032,0,0.0164652,"d achieve state-of-the-art results in zero-shot adaptation of English to Spanish and Thai for the natural language understanding task (i.e., the intent prediction and slot filling) on the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to"
D19-1129,D18-1330,0,0.0233275,"Missing"
D19-1129,S19-2184,1,0.762048,"f-the-art results in zero-shot adaptation of English to Spanish and Thai for the natural language understanding task (i.e., the intent prediction and slot filling) on the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lin"
D19-1129,W19-5327,1,0.850579,"in zero-shot adaptation of English to Spanish and Thai for the natural language understanding task (i.e., the intent prediction and slot filling) on the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lingual meta-represen"
D19-1129,N19-1380,0,0.291983,"al agent to tend to the needs of the users. However, these agents have mostly been trained with the monolingual dataset that is often expensive to build or acquire. In order to cope with the scarcity of low-resource language dialogue data, we are motivated to look into cross-lingual dialogue systems which can adapt with very little or no training data in the target language. This task of zero-shot adaptation of dialogue systems to different languages is relatively new and has not been explored thoroughly enough yet. The main approach of previous work (Upadhyay et al., 2018; Chen et al., 2018; Schuster et al., 2019) in this task is using aligned cross-lingual word embeddings between source and target languages. However, this method suffers from imperfect alignments between the source and target language embeddings. This can be attributed not only to the noise in aligning two different embeddings, but also to the inherent discrepancies in different languages such as Thai and English which come from entirely different roots. To address such variance in the alignment, we turn to probabilistic modeling with latent variables as it has been successfully used in several recent taskoriented dialogue systems (Wen"
D19-1129,W19-4320,1,0.821524,"of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lingual meta-representations for code-switching name entity recognition by combining multiple monolingual word embeddings. Chen et al. (2018) proposed a teacher-student framework leveraging bilingual data for crosslingual transfer learning in dialogue state trackOur model consists of a refined cross-lingual embedding layer followed by a BiLSTM (Hochreiter and Schmidhuber, 1997) which parameterizes the Latent Variable Model, as illustrated in Figure 2. We jointly train our model to predict both slots and user intents. We denote w = [w1 , . . . , wT ] as th"
D19-1129,W18-6243,1,0.833712,"ation of English to Spanish and Thai for the natural language understanding task (i.e., the intent prediction and slot filling) on the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lingual meta-representations for code-s"
D19-1129,W18-3023,0,0.0445921,"nd evaluate the following models: Zero-shot SLU Upadhyay et al. (2018) used cross-lingual embeddings (Bojanowski et al., 2017) to do zero-shot transfer learning. Conditional Random Fields (CRF) We reproduce the baseline model in Schuster et al. (2019), and also add embedding noise, cross-lingual refinement, and delexicalization. Latent Variable Model (LVM) - Ours We replace the CRF module with latent variables and also apply it to intent prediction. Besides, we directly compare with the baseline models illustrated in Schuster et al. (2019): Multi. CoVe w/ auto They combined Multilingual CoVe (Yu et al., 2018) with an auto-encoder objective and then used the trained encoder with the CRF model. 1300 noche เ น What will the weather be like this evening weather อากาศ clima Cancel tuesday alarm clock evening English Spanish Thai English Spanish Thai Figure 3: Visualization of latent variables on words (left) and sentences (right). Left: We choose “weather-climaอากาศ” and “evening-noche- เยน” from parallel sentences. English: “What will the weather be like this evening”, Spanish: “C´omo ser´a el clima esta noche”, Thai: “ตอน เยน นี อากาศ จะ เปน อยางไร”. Right: We choose two English sentences and sh"
D19-1129,W18-5001,0,0.039756,"gual word embeddings between source and target languages. However, this method suffers from imperfect alignments between the source and target language embeddings. This can be attributed not only to the noise in aligning two different embeddings, but also to the inherent discrepancies in different languages such as Thai and English which come from entirely different roots. To address such variance in the alignment, we turn to probabilistic modeling with latent variables as it has been successfully used in several recent taskoriented dialogue systems (Wen et al., 2017; Zhao et al., 2017, 2018; Zhao and Eskenazi, 2018; Le et al., 2018). However, we notice that naively using latent variables does not help the model improve much in slot filling and intent prediction. We hypothesize that the variance of the cross-lingual word embeddings is too large for the model to learn any meaningful latent variables. Hence, we propose to first refine the cross-lingual embeddings with ∼10 seed word-pairs related to the dialogue domains. We then add Gaussian noise (Zheng et al., 2016) to further compensate the imperfect alignment of cross-lingual embeddings. As a result, a combination of these methods allows us to build a t"
D19-1129,P18-1101,0,0.0456593,"Missing"
D19-1129,P17-1061,0,0.0291931,"s using aligned cross-lingual word embeddings between source and target languages. However, this method suffers from imperfect alignments between the source and target language embeddings. This can be attributed not only to the noise in aligning two different embeddings, but also to the inherent discrepancies in different languages such as Thai and English which come from entirely different roots. To address such variance in the alignment, we turn to probabilistic modeling with latent variables as it has been successfully used in several recent taskoriented dialogue systems (Wen et al., 2017; Zhao et al., 2017, 2018; Zhao and Eskenazi, 2018; Le et al., 2018). However, we notice that naively using latent variables does not help the model improve much in slot filling and intent prediction. We hypothesize that the variance of the cross-lingual word embeddings is too large for the model to learn any meaningful latent variables. Hence, we propose to first refine the cross-lingual embeddings with ∼10 seed word-pairs related to the dialogue domains. We then add Gaussian noise (Zheng et al., 2016) to further compensate the imperfect alignment of cross-lingual embeddings. As a result, a combination of these"
D19-1303,N18-1150,0,0.0156643,"t applied to two sentence-level abstractive summarization tasks on the DUC-2004 and Gigaword datasets (Rush et al., 2015). This model was later extended by selective encoding (Zhou et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summarie"
D19-1303,D17-1169,0,0.0359551,"ansfer the style by directly identifying style related keywords and modifying them. However, sensationalism is not always restricted to keywords, but the full sentence. By leveraging small human labeled English dataset, clickbait detection has been well investigated (Chakraborty et al., 2016; Shu et al., 2018; Potthast et al., 2018). However, these human labeled dataset are not available for other languages, such as Chinese. Modeling sensationalism is also related to modeling emotion. Emotion has been well investigated in both word level(Tang et al., 2016; Xu et al., 2018b) and sentence level(Felbo et al., 2017; Winata et al., 2019, 2018; Park et al., 2018; Lee et al., 2019). It has also been considered an important factor in engaging interactive systems(Lin et al., 2019b; Winata et al., 2017; Zhou et al., 2018a). Although we observe that sensational headlines contain emotion, it is still not clear which emotion and how emotions will influence the sensationalism. 6 Conclusion and Future Work In this paper, we propose a model that generates sensational headlines without labeled data using Reinforcement Learning. Firstly, we propose a distant supervision strategy to train the sensationalism scorer. As"
D19-1303,P16-1154,0,0.023977,"that our 6 https://www.figure-eight.com/ Table 2: Generated Chinese headlines from different models. Our model (Pointer-Gen+ARL-SEN) sensationalized the headline with the phrase “In Serious Trouble!”. model produces relevant headlines and we leave the sensationalism for human evaluation. Note that we only compare our models to commonly used strong summarization baselines, to validate that our implementation achieves comparable performance to existing work. In our implementation, Pointer-Gen achieves a 34.51 RG-1 score, 22.21 RG-2 score, and 31.68 RG-L score, which is similar to the results of Gu et al. (2016). PointerGen+ARL-SEN, although optimized for the sensationalism reward, achieves similar performance to our Pointer-Gen baseline, which means that Pointer-Gen+ARL-SEN still keeps its summarization ability. An example of headlines generated from different models in Table 2 shows that Pointer-Gen and Pointer-Gen+RL-ROUGE learns to summarize the main point of the article: “The Nikon D600 camera is reported to have black spots when taking photos”. Pointer-Gen+RL-SEN 3070 Model Pointer-Gen Pointer-Gen-Pos Pointer-Gen+Same-FT Pointer-Gen+Pos-FT Pointer-Gen+RL-ROUGE Pointer-Gen+RL-SEN Pointer-Gen+ARL"
D19-1303,S18-1039,1,0.900112,"Missing"
D19-1303,D15-1229,0,0.198555,"ARL loss function becomes: LARL-SEN = (1 − αsen (y ∗ )) LRL + αsen (y ∗ ) LMLE (15) If αsen (y ∗ ) is high, meaning the training headline is sensational, our loss function encourages our model to imitate the sample more using the MLE training. If αsen (y ∗ ) is low, our loss function replies on RL training to improve the sensationalism. Note that the weight αsen (y ∗ ) is different from our sensationalism reward αsen (y s ) and we call the loss function Auto-tuned Reinforcement Learning, because the ratio between MLE and RL are well “tuned” towards different samples. 3.3 Dataset We use LCSTS (Hu et al., 2015) as our dataset to train the summarization model. The dataset is collected from the Chinese microblogging website Sina Weibo. It contains over 2 million Chinese short texts with corresponding headlines given by the author of each text. The dataset is split into 2,400,591 samples for training, 10,666 samples for validation and 725 samples for testing. We tokenize each sentence with Jieba 4 and a vocabulary size of 50000 is saved. 3068 4 https://github.com/fxsjy/jieba 3.5 Figure 2: The probability density function (pdf) of predicted sensationalism score in log scale. Low sensationalism score has"
D19-1303,N18-2102,0,0.0208419,"attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achieved better performance. Niu and Bansal (2018) tackles the problem of polite generation with politeness reward. Our work is different in that we propose a novel function to balance RL and MLE. Our task is also related to text style transfer. Implicit methods (Shen et al., 2017b; Fu"
D19-1303,D18-1207,0,0.0344404,"Missing"
D19-1303,S19-2184,1,0.872022,"Missing"
D19-1303,D15-1166,0,0.0385727,"Pointer-Gen Headline Generator We choose Pointer Generator (Pointer-Gen) (See et al., 2017), a widely used summarization model, as our headline generator for its ability to copy words from the input article. It takes a news article as input and generates a headline. Firstly, the tokens of each article, {x1 , x2 , x3 , · · · , xM }, are fed into the encoder one-by-one and the encoder generates a sequence of hidden states hi . For each decoding step t, the decoder receives the embedding for each token of a headline yt as input and updates its hidden states st . An attention mechanism following Luong et al. (2015) is used: Figure 1: The loss function of Auto-tuned Reinforcement Learning is a weighted sum of LRL and LMLE , where the weight is decided by our sensationalism scorer. eti = v T tanh(Wh hi + Ws st + battn ) t t a = softmax(e ) X h∗t = ati hi (2) (3) i 0.50 averaged F1 score. This confirms that the predicted sensationalism score can partially capture the sensationalism of headlines. On the other hand, a more natural choice is to take headlines with few comments as negative examples. Thus, we train another baseline classifier on a crawled balanced sensationalism corpus of 84k headlines where th"
D19-1303,K16-1028,0,0.0832857,"Missing"
D19-1303,Q18-1027,0,0.0194518,"se extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achieved better performance. Niu and Bansal (2018) tackles the problem of polite generation with politeness reward. Our work is different in that we propose a novel function to balance RL and MLE. Our task is also related to text style transfer. Implicit methods (Shen et al., 2017b; Fu et al., 2018; Prabhumoye et al., 2018) transfer the styles by separating sentence representations into content and style, for example using backtranslation(Prabhumoye et al., 2018). However, these methods cannot guarantee the content consistency between the original sentence and transferred output (Xu et al., 2018a). Explicit methods (Zhang et al., 2018b; Xu et"
D19-1303,P18-1080,0,0.170636,"eadline, we collect a sensationalism dataset and then train a sensationalism scorer. For the sensationalism dataset collection, we choose headlines with many comments from popular online websites as positive samples. For the negative samples, we propose to use the generated headlines from a sentence summarization model. Intuitively, the summarization model, which is trained to preserve the semantic meaning, will lose the sensationalization ability and thus the generated negative samples will be less sensational than the original one, similar to the obfuscation of style after back-translation (Prabhumoye et al., 2018). For example, an original headline like “一 趟 挣10万？铁总增开申通、顺丰专列” (One trip to earn 100 thousand? China Railway opens new 3 https://github.com/HLTCHKUST/ sensational_headline Shentong and Shunfeng special lines) will become “中铁总将增开京广两列快递专列” (China Railway opens two special lines for express) from the baseline model, which loses the sensational phrases of “一 趟 挣10万 ？” (One trip to earn 100 thousand?) . We then train the sensationalism scorer by classifying sensational and nonsensational headlines using a one-layer CNN with a binary cross entropy loss Lsen . Firstly, 1-D convolution is used to ext"
D19-1303,P18-2025,0,0.0678051,"Missing"
D19-1303,D15-1044,0,0.0549155,"will hurt the performance, both in sensationalism and fluency. After manually checking the outputs, we observe that our model is able to generate sensational headlines using diverse sensationalization strategies. These strategies include, but are not limited to, creating a curiosity gap, asking questions, highlighting numbers, being emotional and emphasizing the user. Examples can be found in Table 4. 5 Related Work Our work is related to summarization tasks. An encoder-decoder model was first applied to two sentence-level abstractive summarization tasks on the DUC-2004 and Gigaword datasets (Rush et al., 2015). This model was later extended by selective encoding (Zhou et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summar"
D19-1303,P17-1099,0,0.0703767,"ur work is related to summarization tasks. An encoder-decoder model was first applied to two sentence-level abstractive summarization tasks on the DUC-2004 and Gigaword datasets (Rush et al., 2015). This model was later extended by selective encoding (Zhou et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al."
D19-1303,P17-1108,0,0.0603074,"Missing"
D19-1303,P18-1090,0,0.108045,"Missing"
D19-1303,W18-6243,1,0.839564,"ion task and achieved better performance. Niu and Bansal (2018) tackles the problem of polite generation with politeness reward. Our work is different in that we propose a novel function to balance RL and MLE. Our task is also related to text style transfer. Implicit methods (Shen et al., 2017b; Fu et al., 2018; Prabhumoye et al., 2018) transfer the styles by separating sentence representations into content and style, for example using backtranslation(Prabhumoye et al., 2018). However, these methods cannot guarantee the content consistency between the original sentence and transferred output (Xu et al., 2018a). Explicit methods (Zhang et al., 2018b; Xu et al., 2018a) transfer the style by directly identifying style related keywords and modifying them. However, sensationalism is not always restricted to keywords, but the full sentence. By leveraging small human labeled English dataset, clickbait detection has been well investigated (Chakraborty et al., 2016; Shu et al., 2018; Potthast et al., 2018). However, these human labeled dataset are not available for other languages, such as Chinese. Modeling sensationalism is also related to modeling emotion. Emotion has been well investigated in both word"
D19-1303,D18-1088,0,0.0168206,"arse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achieved better performan"
D19-1303,D18-1138,0,0.025251,"arse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achieved better performan"
D19-1303,P18-1061,0,0.0231592,"et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achi"
D19-1303,P17-1101,0,0.018251,"ally checking the outputs, we observe that our model is able to generate sensational headlines using diverse sensationalization strategies. These strategies include, but are not limited to, creating a curiosity gap, asking questions, highlighting numbers, being emotional and emphasizing the user. Examples can be found in Table 4. 5 Related Work Our work is related to summarization tasks. An encoder-decoder model was first applied to two sentence-level abstractive summarization tasks on the DUC-2004 and Gigaword datasets (Rush et al., 2015). This model was later extended by selective encoding (Zhou et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2"
D19-1303,S19-2021,1,0.844097,"directly identifying style related keywords and modifying them. However, sensationalism is not always restricted to keywords, but the full sentence. By leveraging small human labeled English dataset, clickbait detection has been well investigated (Chakraborty et al., 2016; Shu et al., 2018; Potthast et al., 2018). However, these human labeled dataset are not available for other languages, such as Chinese. Modeling sensationalism is also related to modeling emotion. Emotion has been well investigated in both word level(Tang et al., 2016; Xu et al., 2018b) and sentence level(Felbo et al., 2017; Winata et al., 2019, 2018; Park et al., 2018; Lee et al., 2019). It has also been considered an important factor in engaging interactive systems(Lin et al., 2019b; Winata et al., 2017; Zhou et al., 2018a). Although we observe that sensational headlines contain emotion, it is still not clear which emotion and how emotions will influence the sensationalism. 6 Conclusion and Future Work In this paper, we propose a model that generates sensational headlines without labeled data using Reinforcement Learning. Firstly, we propose a distant supervision strategy to train the sensationalism scorer. As a result, we achieve"
D19-5827,buck-etal-2014-n,0,0.0699868,"Missing"
D19-5827,N18-1143,0,0.0462149,"Missing"
D19-5827,P17-1055,0,0.116098,"ore of 68.98, which significantly improves the BERT-Large baseline by 8.39 and 7.22, respectively. 1 Introduction Reading comprehension (RC) is a fundamental human skills needed to answer questions that require knowledge of the world and understanding of natural language. This task is essential for intelligent dialogue systems to quickly respond in a search engine or a product recommendation system. Recently, we have witnessed several breakthroughs in question answering (QA) systems, such as bidirectional attention flow (BiDAF) (Seo et al., 2017), the attention over attention mechanism (AoA) (Cui et al., 2017), and a multi-hop architecture using gated-attention readers (Dhingra et al., 2017). A large number of QA datasets have been proposed in recent years for single-hop and multi-hop reasoning applications (Rajpurkar et al., 2016; Lai et al., 2017; Saha et al., 2018; Trischler et al., ∗ These two authors contributed equally. 203 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 203–211 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics ing. Multi-task learning explores the relationships between different tasks by capitalizing on r"
D19-5827,P19-1285,0,0.0283993,"n’s Book Test, and SQuAD datasets. Baseline MRQA organizers have released the BERT-base and BERT-large models as baselines implemented using the AllenNLP (Gardner et al., 2018) platform. 1 The BERT transformer receives a passage and a question that is separated by an [SEP] token. On top of this, the baseline models deploys a linear layer to find the corresponding span which answers the question from the passage. 3.2 XLNet 4 Model XLNet (Yang et al., 2019) is a recently proposed generalized autoregressive pre-training model for language understanding which naively follows the Transformer(-XL) (Dai et al., 2019) architecture. Instead of the bidirectional encoding structure used in BERT (Devlin et al., 2019), XLNet leverages a permutation language modeling objective and target-aware representations with a two-stream attention mechanism to enable the model to capture the context on both sides. Besides the datasets which are also used in the pre-training procedure of BERT (Devlin et al., 2019), XLNet involves Giga5 (Parker et al., 2011), ClueWeb 2012-B (an extension version of Callan et al. (2009)) and Common Crawl (Buck et al., 1 Attention-over-Attention 4.1 Experiments Preprocessing The original setti"
D19-5827,N19-1423,0,0.485599,"ention (Chung et al., 2017; Sun et al., 2018) has been paid towards generalization, i.e., building QA systems that can generalize well on different datasets and transfer to new domains quickly. One major factor that could contribute to generalization, is effective contextual representation (Talmor and Berant, 2019). Recently, models pretrained on a large unlabeled corpus, by adding an extra final layer and fine-tuning on task-specific supervised data, obtained breakthrough performances on many language understanding tasks such as the GLUE benchmark and the SQuAD QA task (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). This indicates the power of pre-trained language models in representing contextual information. Thus, we adopt XLNet (Yang et al., 2019), the state-of-the-art pre-trained language model as our language representation. Another critical issue related to generalization is how to adapt to new QA tasks using few or even no prior training examples. McCann et al. (2018); Liu et al. (2019); Talmor and Berant (2019) show that promising results can be obtained in transferring to new domains by training models on multiple tasks simultaneously using multi-task learnWith a large numbe"
D19-5827,P17-1168,0,0.0262056,"7.22, respectively. 1 Introduction Reading comprehension (RC) is a fundamental human skills needed to answer questions that require knowledge of the world and understanding of natural language. This task is essential for intelligent dialogue systems to quickly respond in a search engine or a product recommendation system. Recently, we have witnessed several breakthroughs in question answering (QA) systems, such as bidirectional attention flow (BiDAF) (Seo et al., 2017), the attention over attention mechanism (AoA) (Cui et al., 2017), and a multi-hop architecture using gated-attention readers (Dhingra et al., 2017). A large number of QA datasets have been proposed in recent years for single-hop and multi-hop reasoning applications (Rajpurkar et al., 2016; Lai et al., 2017; Saha et al., 2018; Trischler et al., ∗ These two authors contributed equally. 203 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 203–211 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics ing. Multi-task learning explores the relationships between different tasks by capitalizing on relatedness while mitigating interference from dissimilarities, thus forcing models"
D19-5827,D18-2012,0,0.0285063,"XLNet involves Giga5 (Parker et al., 2011), ClueWeb 2012-B (an extension version of Callan et al. (2009)) and Common Crawl (Buck et al., 1 Attention-over-Attention 4.1 Experiments Preprocessing The original setting of the sequence length is 512 in the XLNet-large model, but because of the constraint on the computational ability of a single GPU, a trade-off is made between the size of the context and the performance of the model. The sequence length is set as 340 when fine-tuning on the GPU but kept at 512 on the tensor processing unit (TPU). All the datasets are tokenized with SentencePiece (Kudo and Richardson, 2018) and uniformed in lower cases. 4.2 Data Analysis Datasets Under the scenario of this task, the model should be trained on six training datasets. https://github.com/mrqa/MRQA-Shared-Task-2019 205 datasets separately, and then evaluate the model on all the in-domain and out-of-domain development sets. More details about fine-tuning the XLNet model on the GPU are mentioned in §4.4. The evaluation results can be found in Table 3. When evaluating the in-domain datasets, the similarity can be computed as Similarity = GPU 12 - 24 (13) 16 4 340 512, 384, 1 (1) where Pij refers to the F1 score when fin"
D19-5827,N19-1246,0,0.0373277,"to leverage the bidirectional context and overcome the drawbacks of BERT due to its autoregressive nature. XLNet-based models have already achieved better performance than BERTbased models on many NLP tasks. 2.2 Unlike traditional knowledge-based QA (Kalyanpur et al., 2012), nowadays, many QA systems involve natural language understanding and knowledge of the world. Many datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), DROP (Dua et al., 2019), RACE (Lai et al., 2017), DueRC (Saha et al., 2018), BioASQ (Tsatsaronis et al., 2012), TextbookQA (Kembhavi et al., 2017), and RelationExtraction (Levy et al., 2017), have been published for specific QA tasks. Among all these tasks, one of the most widely studied one is extractive QA, which is to find a directly mentioned span in the article which answers the particular question. Although many studies on extractive QA have achieved significant improvements by leveraging attention-based models and pre-trained language representations, QA models might still perform poorly in unseen domains due"
D19-5827,Q19-1026,0,0.0167794,"on language model, was introduced to leverage the bidirectional context and overcome the drawbacks of BERT due to its autoregressive nature. XLNet-based models have already achieved better performance than BERTbased models on many NLP tasks. 2.2 Unlike traditional knowledge-based QA (Kalyanpur et al., 2012), nowadays, many QA systems involve natural language understanding and knowledge of the world. Many datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), DROP (Dua et al., 2019), RACE (Lai et al., 2017), DueRC (Saha et al., 2018), BioASQ (Tsatsaronis et al., 2012), TextbookQA (Kembhavi et al., 2017), and RelationExtraction (Levy et al., 2017), have been published for specific QA tasks. Among all these tasks, one of the most widely studied one is extractive QA, which is to find a directly mentioned span in the article which answers the particular question. Although many studies on extractive QA have achieved significant improvements by leveraging attention-based models and pre-trained language representations, QA models might still perform poo"
D19-5827,D17-1082,0,0.155324,"tanding of natural language. This task is essential for intelligent dialogue systems to quickly respond in a search engine or a product recommendation system. Recently, we have witnessed several breakthroughs in question answering (QA) systems, such as bidirectional attention flow (BiDAF) (Seo et al., 2017), the attention over attention mechanism (AoA) (Cui et al., 2017), and a multi-hop architecture using gated-attention readers (Dhingra et al., 2017). A large number of QA datasets have been proposed in recent years for single-hop and multi-hop reasoning applications (Rajpurkar et al., 2016; Lai et al., 2017; Saha et al., 2018; Trischler et al., ∗ These two authors contributed equally. 203 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 203–211 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics ing. Multi-task learning explores the relationships between different tasks by capitalizing on relatedness while mitigating interference from dissimilarities, thus forcing models to learn useful representations more generally by unifying tasks under a single perspective. Thus, a model, which is trained on multiple source QA datasets, can"
D19-5827,K17-1034,0,0.0169416,"e than BERTbased models on many NLP tasks. 2.2 Unlike traditional knowledge-based QA (Kalyanpur et al., 2012), nowadays, many QA systems involve natural language understanding and knowledge of the world. Many datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), DROP (Dua et al., 2019), RACE (Lai et al., 2017), DueRC (Saha et al., 2018), BioASQ (Tsatsaronis et al., 2012), TextbookQA (Kembhavi et al., 2017), and RelationExtraction (Levy et al., 2017), have been published for specific QA tasks. Among all these tasks, one of the most widely studied one is extractive QA, which is to find a directly mentioned span in the article which answers the particular question. Although many studies on extractive QA have achieved significant improvements by leveraging attention-based models and pre-trained language representations, QA models might still perform poorly in unseen domains due to the data scarcity. MLP ... /2 XLNet /2−1 ... 1 Batch Generation ... ... ... SQuAD NewsQA ... ... 2.3 2.1 Multi-task Learning Liu et al. (2019) proposed a multi-tas"
D19-5827,P19-1441,0,0.124321,"d fine-tuning on task-specific supervised data, obtained breakthrough performances on many language understanding tasks such as the GLUE benchmark and the SQuAD QA task (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). This indicates the power of pre-trained language models in representing contextual information. Thus, we adopt XLNet (Yang et al., 2019), the state-of-the-art pre-trained language model as our language representation. Another critical issue related to generalization is how to adapt to new QA tasks using few or even no prior training examples. McCann et al. (2018); Liu et al. (2019); Talmor and Berant (2019) show that promising results can be obtained in transferring to new domains by training models on multiple tasks simultaneously using multi-task learnWith a large number of datasets being released and new techniques being proposed, Question answering (QA) systems have witnessed great breakthroughs in reading comprehension (RC) tasks. However, most existing methods focus on improving in-domain performance, leaving open the research question of how these models and techniques can generalize to out-ofdomain and unseen RC tasks. To enhance the generalization ability, we p"
D19-5827,P17-1147,0,0.0437326,"ge model and a next-sentence prediction objective. Recently, XLNet (Yang et al., 2019), a permutation language model, was introduced to leverage the bidirectional context and overcome the drawbacks of BERT due to its autoregressive nature. XLNet-based models have already achieved better performance than BERTbased models on many NLP tasks. 2.2 Unlike traditional knowledge-based QA (Kalyanpur et al., 2012), nowadays, many QA systems involve natural language understanding and knowledge of the world. Many datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), DROP (Dua et al., 2019), RACE (Lai et al., 2017), DueRC (Saha et al., 2018), BioASQ (Tsatsaronis et al., 2012), TextbookQA (Kembhavi et al., 2017), and RelationExtraction (Levy et al., 2017), have been published for specific QA tasks. Among all these tasks, one of the most widely studied one is extractive QA, which is to find a directly mentioned span in the article which answers the particular question. Although many studies on extractive QA have achieved significant improvements by leve"
D19-5827,D16-1264,0,0.457475,"of the world and understanding of natural language. This task is essential for intelligent dialogue systems to quickly respond in a search engine or a product recommendation system. Recently, we have witnessed several breakthroughs in question answering (QA) systems, such as bidirectional attention flow (BiDAF) (Seo et al., 2017), the attention over attention mechanism (AoA) (Cui et al., 2017), and a multi-hop architecture using gated-attention readers (Dhingra et al., 2017). A large number of QA datasets have been proposed in recent years for single-hop and multi-hop reasoning applications (Rajpurkar et al., 2016; Lai et al., 2017; Saha et al., 2018; Trischler et al., ∗ These two authors contributed equally. 203 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 203–211 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics ing. Multi-task learning explores the relationships between different tasks by capitalizing on relatedness while mitigating interference from dissimilarities, thus forcing models to learn useful representations more generally by unifying tasks under a single perspective. Thus, a model, which is trained on multiple sourc"
D19-5827,P19-1485,0,0.123317,"xam writers, etc.), and the relationship of the question to the passage are different among datasets (e.g., collected as independent vs. dependent on evidence, multi-hop, etc). The availability of such datasets promotes the development of models that work well for only a specific domain. However, little attention (Chung et al., 2017; Sun et al., 2018) has been paid towards generalization, i.e., building QA systems that can generalize well on different datasets and transfer to new domains quickly. One major factor that could contribute to generalization, is effective contextual representation (Talmor and Berant, 2019). Recently, models pretrained on a large unlabeled corpus, by adding an extra final layer and fine-tuning on task-specific supervised data, obtained breakthrough performances on many language understanding tasks such as the GLUE benchmark and the SQuAD QA task (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). This indicates the power of pre-trained language models in representing contextual information. Thus, we adopt XLNet (Yang et al., 2019), the state-of-the-art pre-trained language model as our language representation. Another critical issue related to generalization is how t"
D19-5827,W17-2623,0,0.190951,"e Model Fine-tuning Dan Su∗, Yan Xu∗ , Genta Indra Winata, Peng Xu, Hyeondey Kim, Zihan Liu, Pascale Fung Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong {dsu, yxucb, giwinata, pxuab}@connect.ust.hk, {hdkimaa, zliucr}@connect.ust.hk, pascale@ece.ust.hk Abstract 2017; Joshi et al., 2017). However, each QA dataset is built for a particular domain and focus (Talmor and Berant, 2019). Dataset passages cover different topics, such as movies (Saha et al., 2018), news (Trischler et al., 2017), and biomedicine (Tsatsaronis et al., 2012). Also, the styles of questions (e.g., entity-centric, relational, other tasks reformulated as QA, etc.), the sources (e.g., crowd-workers, domain experts, exam writers, etc.), and the relationship of the question to the passage are different among datasets (e.g., collected as independent vs. dependent on evidence, multi-hop, etc). The availability of such datasets promotes the development of models that work well for only a specific domain. However, little attention (Chung et al., 2017; Sun et al., 2018) has been paid towards generalization, i.e., b"
D19-5827,D18-1259,0,0.0269959,"XLNet (Yang et al., 2019), a permutation language model, was introduced to leverage the bidirectional context and overcome the drawbacks of BERT due to its autoregressive nature. XLNet-based models have already achieved better performance than BERTbased models on many NLP tasks. 2.2 Unlike traditional knowledge-based QA (Kalyanpur et al., 2012), nowadays, many QA systems involve natural language understanding and knowledge of the world. Many datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), DROP (Dua et al., 2019), RACE (Lai et al., 2017), DueRC (Saha et al., 2018), BioASQ (Tsatsaronis et al., 2012), TextbookQA (Kembhavi et al., 2017), and RelationExtraction (Levy et al., 2017), have been published for specific QA tasks. Among all these tasks, one of the most widely studied one is extractive QA, which is to find a directly mentioned span in the article which answers the particular question. Although many studies on extractive QA have achieved significant improvements by leveraging attention-based models and pre-trained language repre"
D19-6109,P13-2119,0,0.0614421,"Missing"
D19-6109,P18-1031,0,0.0347485,"a base network is trained with the source data, and then the first n layers of the 76 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 76–83 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 to the complex attention mechanisms and large parameter size, it is hard to train BERT for domain adaptation using the domain-adversarial approach. Our initial experiments demonstrated the unsteadiness of this approach when applied to BERT. Unsupervised language model (LM) finetuning method (Howard and Ruder, 2018) consisting of general-domain LM pre-training and target task LM fine-tuning is effective using a AWDLSTM language model on many text classification tasks such as sentimental analysis, question classification and topic classification. However, due to the unique objective of BERT language model pre-training (masked LM and next sentence prediction) which requires multi-sentences natural language paragraphs, unsupervised fine-tuning of BERT LM does not apply to many sentence-pair classification datasets. In this work, we propose a novel domain adaptation framework, in which the idea of domainadve"
D19-6109,P07-1034,0,0.166658,"n classifier is trained via a gradient reversal layer that multiplies the gradient by a certain negative constant during the backpropagation. As the training progresses, the approach promotes the emergence of a representation that is discriminative for the main learning task and indiscriminate with respect to the shift between the domains. However, such type of models are usually hard to train since the optimization problem involves a minimization with respect to some parameters, as well as a maximization with respect to the others. Very early approaches in NLP utilized instance re-weighting (Jiang and Zhai, 2007) and target data co-training (Chen et al., 2011) to achieve domain adaptation. Recently, Denoising Autoencoders (Glorot et al., 2011), domain discrepancy regularization and domain adversarial training (Shah et al., 2019; Shen et al., 2017) have been employed to learn a domain invariant representation for neural network models. Many domain adaptation studies have focused on tasks such as sentiment analysis (Glorot et al., 2011; Shen et al., 2017) , Part-Of-Speech (POS) tagging (Ruder et al., 2017a) and paraphrase detection (Shah et al., 2019), and tested on neural network models such as multila"
D19-6109,D11-1033,0,0.202275,"Missing"
D19-6109,D16-1264,0,0.038806,"ents, in order to determine the optimal number of data points selected from the source domain, we set aside a small target domain dataset for validation. Starting from only a hundred examples, we double the training data size every time we observe a significant change in transfer performance evaluated on the validation set. 4.1 matched (in-domain) section. Similar as in SNLI, we convert the three-label classification task into a binary classification task. QNLI The Question-answering Natural Language Inference (QNLI) is a dataset converted from the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). Although its name contains “natural language inference”, the text domain and task type of QNLI are fundamentally different from those of SNLI and MNLI. The original SQuAD dataset consists of questionparagraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). GLUE converts the task into sentence pair classification by forming a pair between each question and each sentence in the corresponding context and filtering out pairs with low lexical overlap between the question and the context sentence"
D19-6109,D15-1075,0,0.0265892,"much smaller variance in evaluation metrics compared with smaller datasets. We used the pre-processed datasets from GLUE natural language understanding benchmark (Wang et al., 2018). A summary of the dataset statistics and the details of the experiment setup are presented in Table 1. Task Category Natural Language Inference Natural Language Inference Answer Sentence Selection Paraphrase Detection Dataset SNLI MNLI QNLI Quora Train Size 510,711 392,702 108,436 363,847 Dev Size 9,831 9,815 5,732 40,430 Table 1: Summary of the datasets SNLI The Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) is a collection of 570k human-written English sentence pairs supporting the task of natural language inference. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). In order to make the label set the same across all the datasets, we convert the original three-label classification task into a binary classification task with “entailment” as the positive label, and “contradiction” and “neutral” as negative. MNLI The Multi-Genre Natural Language Inferenc"
D19-6109,D17-1038,0,0.0887119,"Missing"
D19-6109,P16-1013,0,0.067069,"Missing"
D19-6109,W18-5446,0,0.0330477,"mples are pairs of “related questions” which, although pertaining to similar topics, are not truly semantically equivalent. Due to community nature, the ground-truth labels contain some amount of noise. Datasets We tested our framework on four large public datasets across three task categories: natural language inference (SNLI and MNLI), answer sentence selection (QNLI) and paraphrase detection (Quora). Large datasets usually have a much smaller variance in evaluation metrics compared with smaller datasets. We used the pre-processed datasets from GLUE natural language understanding benchmark (Wang et al., 2018). A summary of the dataset statistics and the details of the experiment setup are presented in Table 1. Task Category Natural Language Inference Natural Language Inference Answer Sentence Selection Paraphrase Detection Dataset SNLI MNLI QNLI Quora Train Size 510,711 392,702 108,436 363,847 Dev Size 9,831 9,815 5,732 40,430 Table 1: Summary of the datasets SNLI The Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) is a collection of 570k human-written English sentence pairs supporting the task of natural language inference. Given a premise sentence and a hypothesis sentenc"
H05-1027,W02-1001,0,0.107516,"he least CER. Therefore, the best estimators are those which minimize the expected error rate on unseen test data. Since the distribution of test data is unknown, we can approximately minimize the error rate on some given training data (Vapnik 1999). Toward this end, we have developed a very simple heuristic training procedure called minimum sample risk, as presented in the next section. 3 Minimum Sample Risk 3.1 Problem Definition We follow the general framework of linear discriminant models described in (Duda et al. 2001). In the rest of the paper we use the following notation, adapted from Collins (2002). • Training data is a set of example input/output pairs. In LM for IME, training samples are represented as {Ai, WiR}, for i = 1…M, where each Ai is an input phonetic string and WiR is the reference transcript of Ai. • We assume some way of generating a set of candidate word strings given A, denoted by GEN(A). In our experiments, GEN(A) consists of top N word strings converted from A using a baseline IME system that uses only a word trigram model. • We assume a set of D+1 features fd(W), for d = 0…D. The features could be arbitrary functions that map W to real values. Using vector notation, w"
H05-1027,W02-1032,1,0.740514,"CER. Therefore, the best estimators are those which minimize the expected error rate on unseen test data. Since the distribution of test data is unknown, we can approximately minimize the error rate on some given training data (Vapnik 1999). Toward this end, we have developed a very simple heuristic training procedure called minimum sample risk, as presented in the next section. 3 Minimum Sample Risk 3.1 Problem Definition We follow the general framework of linear discriminant models described in (Duda et al. 2001). In the rest of the paper we use the following notation, adapted from Collins (2002). • Training data is a set of example input/output pairs. In LM for IME, training samples are represented as {Ai, WiR}, for i = 1…M, where each Ai is an input phonetic string and WiR is the reference transcript of Ai. • We assume some way of generating a set of candidate word strings given A, denoted by GEN(A). In our experiments, GEN(A) consists of top N word strings converted from A using a baseline IME system that uses only a word trigram model. • We assume a set of D+1 features fd(W), for d = 0…D. The features could be arbitrary functions that map W to real values. Using vector notation, w"
H05-1027,P03-1021,0,0.052351,"quation (4) is a step function of λ, thus cannot be optimized directly by regular gradientbased procedures – a grid search has to be used instead. However, there are problems with simple grid search: using a large grid could miss the optimal solution whereas using a fine-grained grid would lead to a very slow algorithm. Secondly, in 211 the case of LM, there are millions of candidate features, some of which are highly correlated. We address these issues respectively in the next two subsections. 3.3 Grid Line Search Our implementation of a grid search is a modified version of that proposed in (Och 2003). The modifications are made to deal with the efficiency issue due to the fact that there is a very large number of features and training samples in our task, compared to only 8 features used in (Och 2003). Unlike a simple grid search where the intervals between any two adjacent grids are equal and fixed, we determine for each feature a sequence of grids with differently sized intervals, each corresponding to a different value of sample risk. As shown in Equation (4), the loss function (i.e. sample risk) over all training samples is the sum of the loss function (i.e. Er(.)) of each training sa"
H05-1027,P05-1034,0,0.036735,"Missing"
H05-1027,H05-1034,1,0.792962,"e 3, but also better generalization properties (fewer test errors), as shown in Figure 4. 4.4 Domain Adaptation Results Though MSR achieves impressive performance in CER reduction over the comparison methods, as described in Section 4.2, the experiments are all performed using newspaper text for both training and testing, which is not a realistic scenario if we are to deploy the model in an application. This section reports the results of additional experiments in which we adapt a model trained on one domain to a different domain, i.e., in a so-called cross-domain LM adaptation paradigm. See (Suzuki and Gao 2005) for a detailed report. The data sets we used stem from five distinct sources of text. The Nikkei newspaper corpus described in Section 4.1 was used as the background domain, on which the word trigram model was trained. We used four adaptation domains: Yomiuri (newspaper corpus), TuneUp (balanced corpus containing newspapers and other sources of text), Encarta (encyclopedia) and Shincho (collection of novels). For each of the four domains, we used an 72,000-sentence subset as adaptation training data, a 5,000-sentence subset as held-out data and another 5,000-sentence subset as test data. Simi"
N09-1028,P06-1067,0,0.123367,"ecomes one of the key weaknesses. Many reordering methods have been proposed in recent years to address this problem in different aspects. 245 The first class of approaches tries to explicitly model phrase reordering distances. Distance based distortion model (Och, 2002; Koehn et.al., 2003) is a simple way of modeling phrase level reordering. It penalizes non-monotonicity by applying a weight to the number of words between two source phrases corresponding to two consecutive target phrases. Later on, this model was extended to lexicalized phrase reordering (Tillmann, 2004; Koehn, et.al., 2005; Al-Onaizan and Papineni, 2006) by applying different weights to different phrases. Most recently, a hierarchical phrase reordering model (Galley and Manning, 2008) was proposed to dynamically determine phrase boundaries using efficient shift-reduce parsing. Along this line of research, discriminative reordering models based on a maximum entropy classifier (Zens and Ney, 2006; Xiong, et.al., 2006) also showed improvements over the distance based distortion model. None of these reordering models changes the word alignment step in SMT systems, therefore, they can not recover from the word alignment errors. These models are al"
N09-1028,N04-4026,0,\N,Missing
N09-1028,C04-1073,0,\N,Missing
N09-1028,W08-0406,0,\N,Missing
N09-1028,C08-1027,0,\N,Missing
N09-1028,D08-1089,0,\N,Missing
N09-1028,P02-1040,0,\N,Missing
N09-1028,P07-1002,0,\N,Missing
N09-1028,P06-1077,0,\N,Missing
N09-1028,P01-1067,0,\N,Missing
N09-1028,P06-1066,0,\N,Missing
N09-1028,J04-4002,1,\N,Missing
N09-1028,C08-1144,1,\N,Missing
N09-1028,C04-1010,0,\N,Missing
N09-1028,P07-1091,0,\N,Missing
N09-1028,P05-1033,0,\N,Missing
N09-1028,N03-1017,1,\N,Missing
N09-1028,P05-1034,0,\N,Missing
N09-1028,W06-3108,0,\N,Missing
N09-1028,J97-3002,0,\N,Missing
N09-1028,P07-1019,0,\N,Missing
N09-1028,P05-1066,0,\N,Missing
N09-1028,P06-1121,0,\N,Missing
N09-1028,W06-3119,0,\N,Missing
N09-1028,W04-3250,0,\N,Missing
N09-1028,D07-1077,0,\N,Missing
N09-1028,2005.iwslt-1.8,0,\N,Missing
N09-1028,2007.iwslt-1.3,0,\N,Missing
N09-1028,W02-2018,0,\N,Missing
N09-1028,D08-1076,1,\N,Missing
N09-1028,P03-1021,1,\N,Missing
N18-1002,C02-1150,0,0.0969825,"single-label that jointly learns representations for entity mentions and their context. We show experimentally that our approach is robust against noise and consistently outperforms the state-of-theart on established benchmarks for the task. 1 Introduction Fine-grained Entity Type Classification (FETC) aims at labeling entity mentions in context with one or more specific types organized in a hierarchy (e.g., actor as a subtype of artist, which in turn is a subtype of person). Fine-grained types help in many applications, including relation extraction (Mintz et al., 2009), question answering (Li and Roth, 2002), entity linking (Lin et al., 2012), knowledge base completion (Dong et al., 2014) and entity recommendation (Yu et al., 2014). Because of the high cost in labeling large training corpora with fine-grained types, current FETC systems resort to distant supervision (Mintz et al., 2009) and annotate mentions in the training corpus with all types associated with the entity in a knowledge graph. This is illustrated in 16 Proceedings of NAACL-HLT 2018, pages 16–25 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: With distant supervision, all the th"
N18-1002,D12-1082,0,0.168587,"Missing"
N18-1002,C16-1017,0,0.151433,"tion context, Gillick et al. (2014) introduced the concept of context-dependent FETC where the types of a mention are constrained to what can be deduced from its context and introduced a new OntoNotes-derived (Weischedel et al., 2011) manually annotated evaluation dataset. In addition, they addressed the problem of label noise induced by distant supervision and proposed three label cleaning heuristics. Yogatama et al. (2015) proposed an embedding-based model where userdefined features and labels were embedded into a low dimensional feature space to facilitate information sharing among labels. Ma et al. (2016) presented a label embedding method that incor17 no hand-crafted features uses attentive neural network adopts single label setting handles out-of-context noise handles overly-specifc noise Attentive — AFET — — — — — — LNR — — — AAA NFETC — — — Table 1: Summary comparison to related FETC work. FETC systems listed in the table: (1) Attentive (Shimaoka et al., 2017); (2) AFET (Ren et al., 2016a); (3) LNR (Ren et al., 2016b); (4) AAA (Abhishek et al., 2017). hinge loss function and gain great performance improvement on FIGER(GOLD). However, their work overlooks the effect of overly-specific noise"
N18-1002,P09-1113,0,0.611158,"we use public word embeddings to train a single-label that jointly learns representations for entity mentions and their context. We show experimentally that our approach is robust against noise and consistently outperforms the state-of-theart on established benchmarks for the task. 1 Introduction Fine-grained Entity Type Classification (FETC) aims at labeling entity mentions in context with one or more specific types organized in a hierarchy (e.g., actor as a subtype of artist, which in turn is a subtype of person). Fine-grained types help in many applications, including relation extraction (Mintz et al., 2009), question answering (Li and Roth, 2002), entity linking (Lin et al., 2012), knowledge base completion (Dong et al., 2014) and entity recommendation (Yu et al., 2014). Because of the high cost in labeling large training corpora with fine-grained types, current FETC systems resort to distant supervision (Mintz et al., 2009) and annotate mentions in the training corpus with all types associated with the entity in a knowledge graph. This is illustrated in 16 Proceedings of NAACL-HLT 2018, pages 16–25 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure"
N18-1002,P15-2048,0,0.140196,"dataset FIGER (GOLD). They used a linear classifier perceptron for multi-label classification. While initial work largely assumed that mention assignments could be done independently of the mention context, Gillick et al. (2014) introduced the concept of context-dependent FETC where the types of a mention are constrained to what can be deduced from its context and introduced a new OntoNotes-derived (Weischedel et al., 2011) manually annotated evaluation dataset. In addition, they addressed the problem of label noise induced by distant supervision and proposed three label cleaning heuristics. Yogatama et al. (2015) proposed an embedding-based model where userdefined features and labels were embedded into a low dimensional feature space to facilitate information sharing among labels. Ma et al. (2016) presented a label embedding method that incor17 no hand-crafted features uses attentive neural network adopts single label setting handles out-of-context noise handles overly-specifc noise Attentive — AFET — — — — — — LNR — — — AAA NFETC — — — Table 1: Summary comparison to related FETC work. FETC systems listed in the table: (1) Attentive (Shimaoka et al., 2017); (2) AFET (Ren et al., 2016a); (3) LNR (Ren e"
N18-1002,D14-1162,0,0.0805196,"Missing"
N18-1002,D16-1144,0,0.49227,"y. While only part of the types are correct: person and coach for S1, person and athlete for S2, and just person for S3. marks that shows that our model can adapt to noise in training data and consistently outperform previous methods. In summary, we describe a single, much simpler and more elegant neural network model that attempts FETC “end-to-end” without post-processing or ad-hoc features and improves on the state-of-the-art for the task. with scores above a threshold (Mintz et al., 2009; Gillick et al., 2014; Shimaoka et al., 2017) or perform a top-down search in the given type hierarchy (Ren et al., 2016a; Abhishek et al., 2017). Contributions: We propose a neural network based model to overcome the drawbacks of existing FETC systems mentioned above. With publicly available word embeddings as input, we learn two different entity representations and use bidirectional long-short term memory (LSTM) with attention to learn the context representation. We propose a variant of cross entropy loss function to handle out-of-context labels automatically during the training phase. Also, we introduce hierarchical loss normalization to adjust the penalties for correlated types, allowing our model to unders"
N18-1002,C14-1220,0,0.0632523,"mentions in context. The input is a knowledge graph Ψ with schema YΨ , whose types are organized into a type hierarchy Y, and an automatically labeled training corpus D obtained by distant supervision with Y. The output is a type-path in Y for each named entity mentioned in a test sentence from a corpus Dt . More precisely, a labeled corpus for entity type classification consists of a set of extracted entity mentions {mi }N i=1 (i.e., token spans representing entities in text), the context (e.g., sentence, paragraph) of each mention {ci }N i=1 , and the candidate 18 word position embeddings (Zeng et al., 2014) to reflect relative distances between the i-th word to the entity mention. Every relative distance is mapped to a randomly initialized position vector in Rdp , where dp is the size of position embedding. For a given word, we obtain the position vector wip . The overall embedding for the i-th word is wiE = [(wid )&gt; , (wip )&gt; ]&gt; . type sets {Yi }N i=1 automatically generated for each mention. We represent the training corpus using a set of mention-based triples D = {(mi , ci , Yi )}N i=1 . If Yi is free of out-of-context noise, the type labels for each mi should form a single type-path in Yi ."
N18-1002,W16-1313,0,0.122312,"ghts to control the hierarchical loss would be solicited from domain experts, which is inapplicable for FETC. Instead, we propose a method called hierarchical loss normalization which can overcome the above limitations and be incorporated with cross entropy loss used in our neural architecture. Table 1 provides a summary comparison of our work against the previous state-of-the-art in fine grained entity typing. porates prototypical and hierarchical information to learn pre-trained label embeddings and adpated a zero-shot framework that can predict both seen and previously unseen entity types. Shimaoka et al. (2016) proposed an attentive neural network model that used LSTMs to encode the context of an entity mention and used an attention mechanism to allow the model to focus on relevant expressions in such context. Shimaoka et al. (2017) summarizes many neural architectures for FETC task. These models ignore the outof-context noise, that is, they assume that all labels obtained via distant supervision are “correct” and appropriate for every context in the training corpus. In our paper, a simple yet effective variant of cross entropy loss function is proposed to handle the problem of out-of-context noise."
N18-1002,P16-2034,0,0.0134886,"ameters θ. In this paper, we adopt bidirectional LSTM with ds hidden units as f (ci ; θ). The network contains two sub-networks for the forward pass and the backward pass respectively. Here, we use element-wise sum to combine the forward and backward pass outputs. The output of the i-th word in shown in the following equation: → − ← − hi = [ hi ⊕ hi ] Definition 1 Given an entity mention mi = (wp , . . . , wt ) (p, t ∈ [1, T ], p ≤ t) and its context ci = (w1 , . . . , wT ) where T is the context length, our task is to predict its most specific type yˆi depending on the context. (1) Following Zhou et al. (2016), we employ word-level attention mechanism, which makes our model able to softly select the most informative words during training. Let H be a matrix consisting of output vectors [h1 , h2 , . . . , hT ] that the LSTM produced. The context representation r is formed by a weighted sum of these output vectors: In practice, ci is generated by truncating the original context with words beyond the context window size C both to the left and to the right of mi . Specifically, we compute a probability distribution over all the K = |Y |types in the target type hierarchy Y. The type with the highest prob"
N18-1002,E17-1119,0,0.428901,"r shown are labeled with the same types in oval boxes in the target type hierarchy. While only part of the types are correct: person and coach for S1, person and athlete for S2, and just person for S3. marks that shows that our model can adapt to noise in training data and consistently outperform previous methods. In summary, we describe a single, much simpler and more elegant neural network model that attempts FETC “end-to-end” without post-processing or ad-hoc features and improves on the state-of-the-art for the task. with scores above a threshold (Mintz et al., 2009; Gillick et al., 2014; Shimaoka et al., 2017) or perform a top-down search in the given type hierarchy (Ren et al., 2016a; Abhishek et al., 2017). Contributions: We propose a neural network based model to overcome the drawbacks of existing FETC systems mentioned above. With publicly available word embeddings as input, we learn two different entity representations and use bidirectional long-short term memory (LSTM) with attention to learn the context representation. We propose a variant of cross entropy loss function to handle out-of-context labels automatically during the training phase. Also, we introduce hierarchical loss normalization"
N18-1002,E17-1075,0,\N,Missing
N19-1323,P16-1200,0,0.0603339,"dimensional space, where dw is the size of word embedding and dp is the size of position embedding. Sentence Encoder. For each sentence si , we apply a non-linear transformation to the vector representation of si to derive a feature vector zi = f (si ; θ) given a set of parameters θ. In this paper, we adopt bidirectional LSTM with ds hidden units as f (si ; θ) (Zhou et al., 2016). Multi-level Attention Mechanisms. We employ attention mechanisms at both word-level and sentence-level to allow the model to softly select the most informative words and sentences during training (Zhou et al., 2016; Lin et al., 2016). With the learned language representation sL , the conditional probability p(r|S; Θ(L) ) is computed through a softmax layer, where Θ(L) is the parameters of the model to learn language representation. 4.2 bility p(r|(h, t); Θ(G) ) for each relation r: Knowledge Representation Following the score function φ and training procedure of Trouillon et al. (2016), we can get the knowledge representations eh , wr , et ∈ Cdk . With the knowledge representations and the scoring function, we can obtain the conditional probaJG = − N 1 X log p(ri |Si ; Θ(L) ) N (1) 1 N (2) i=1 N X log p(ri |(hi , ti ); Θ("
N19-1323,P09-1113,0,0.48762,"contain structured information about the world and are used in support of many natural language processing applications such as semantic search and question answering. Building KBs is a never-ending challenge because, as the world changes, new knowledge needs to be harvested while old knowledge needs to be revised. This motivates the work on the Relation Extraction (RE) task, whose goal is to assign a KB relation to a phrase connecting a pair of entities, which in turn can be used for updating the KB. The state-of-the-art in RE builds on neural models using distant (a.k.a. weak) supervision (Mintz et al., 2009) on large-scale corpora for training. A task related to RE is that of Knowledge Base Embedding (KBE), which is concerned with representing KB entities and relations in a vector space for predicting missing links in the graph. Aiming to leverage the similarities between these tasks, Weston et al. (2013) were the first to show that combining predictions from RE and KBE models was beneficial for RE. However, the way in which they combine RE and KBE predictions is rather naive (namely, by adding those scores). To the best of our knowledge, there have been no systematic attempts to further unify RE"
N19-1323,D14-1162,0,0.0859887,"Missing"
N19-1323,D17-1187,0,0.0150342,"igure 1: Workflow of the proposed framework. representations for RE and KBE tasks that uses a cross-entropy loss function to ensure both representations are learned together, resulting in significant improvements over the current state-of-theart for the RE task. 2 Related Work Recent neural models have been shown superior to approaches using hand-crafted features for the RE task. Among the pioneers, Zeng et al. (2015) proposed a piecewise convolutional network with multi-instance learning to handle weakly labeled text mentions. Recurrent neural networks (RNN) are another popular architecture (Wu et al., 2017). Similar fast progress has been seen for the KBE task for representing entities and relations in KBs with vectors or matrices. Bordes et al. (2013) introduced the influential translation-based embeddings (TransE), while Yang et al. (2014) leveraged latent matrix factorization in their DistMult method. We build on ComplEx (Trouillon et al., 2016), which extends DistMult into the complex space and has been shown significantly better on several benchmarks. Weston et al. (2013) were the first to connect RE and KBE models for the RE task. Their simple idea was to train the two models independently"
N19-1323,D15-1203,0,0.0830968,"s a novel neural framework for jointly learning 3201 Proceedings of NAACL-HLT 2019, pages 3201–3206 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Figure 1: Workflow of the proposed framework. representations for RE and KBE tasks that uses a cross-entropy loss function to ensure both representations are learned together, resulting in significant improvements over the current state-of-theart for the RE task. 2 Related Work Recent neural models have been shown superior to approaches using hand-crafted features for the RE task. Among the pioneers, Zeng et al. (2015) proposed a piecewise convolutional network with multi-instance learning to handle weakly labeled text mentions. Recurrent neural networks (RNN) are another popular architecture (Wu et al., 2017). Similar fast progress has been seen for the KBE task for representing entities and relations in KBs with vectors or matrices. Bordes et al. (2013) introduced the influential translation-based embeddings (TransE), while Yang et al. (2014) leveraged latent matrix factorization in their DistMult method. We build on ComplEx (Trouillon et al., 2016), which extends DistMult into the complex space and has b"
N19-1323,C14-1220,0,0.096093,"this paper seeks an elegant way of connecting language and knowledge representations for the RE task. In order to achieve that, we use separate loss functions (recall Figure 1) to guide the language and knowledge representation learning and a third loss function that ties the predictions of these models thus nudging the parameters towards agreement. The cross-entropy losses based on the language and knowledge representations are defined as: Language Representation JL = − Input Representation. For each word token, we use pretrained word embeddings and randomly initialized position embeddings (Zeng et al., 2014) to project it into (dw + dp )-dimensional space, where dw is the size of word embedding and dp is the size of position embedding. Sentence Encoder. For each sentence si , we apply a non-linear transformation to the vector representation of si to derive a feature vector zi = f (si ; θ) given a set of parameters θ. In this paper, we adopt bidirectional LSTM with ds hidden units as f (si ; θ) (Zhou et al., 2016). Multi-level Attention Mechanisms. We employ attention mechanisms at both word-level and sentence-level to allow the model to softly select the most informative words and sentences durin"
N19-1323,P16-2034,0,0.0592558,"representations are defined as: Language Representation JL = − Input Representation. For each word token, we use pretrained word embeddings and randomly initialized position embeddings (Zeng et al., 2014) to project it into (dw + dp )-dimensional space, where dw is the size of word embedding and dp is the size of position embedding. Sentence Encoder. For each sentence si , we apply a non-linear transformation to the vector representation of si to derive a feature vector zi = f (si ; θ) given a set of parameters θ. In this paper, we adopt bidirectional LSTM with ds hidden units as f (si ; θ) (Zhou et al., 2016). Multi-level Attention Mechanisms. We employ attention mechanisms at both word-level and sentence-level to allow the model to softly select the most informative words and sentences during training (Zhou et al., 2016; Lin et al., 2016). With the learned language representation sL , the conditional probability p(r|S; Θ(L) ) is computed through a softmax layer, where Θ(L) is the parameters of the model to learn language representation. 4.2 bility p(r|(h, t); Θ(G) ) for each relation r: Knowledge Representation Following the score function φ and training procedure of Trouillon et al. (2016), we c"
N19-1323,D17-1188,0,0.0473412,"E models for the RE task. Their simple idea was to train the two models independently and only combine them at inference time. While they showed that combining the two models is better than using the RE model alone, newer and better models since then have obviated the net gains of such a simple strategy (Xu and Barbosa, 2018). We propose a much tighter integration of RE and KBE models: we not only use them for prediction, but also train them together, thus mutually reinforcing one another. Recently, many methods have been proposed to use information from KBs to facilitate relation extraction. Sorokin and Gurevych (2017) considered other relations in the sentential context while predicting the target relation. Vashishth et al. (2018) utilized additional side information from KBs for improved RE. However, these methods didn’t leverage KBE method to unify RE and KBE in a principled way. Han et al. (2018) used a mutual attention between KBs and text to perform better on both RE and KBE, but their method was still based on TransE (Bordes et al., 2013) which can not fully exploit the advantage of the information from KBs. 3 Background and Problem The goal in the task of Relation Extraction is to predict a KB relat"
N19-1323,D18-1157,0,0.135292,"Missing"
N19-1323,D13-1136,0,0.0941794,"to be revised. This motivates the work on the Relation Extraction (RE) task, whose goal is to assign a KB relation to a phrase connecting a pair of entities, which in turn can be used for updating the KB. The state-of-the-art in RE builds on neural models using distant (a.k.a. weak) supervision (Mintz et al., 2009) on large-scale corpora for training. A task related to RE is that of Knowledge Base Embedding (KBE), which is concerned with representing KB entities and relations in a vector space for predicting missing links in the graph. Aiming to leverage the similarities between these tasks, Weston et al. (2013) were the first to show that combining predictions from RE and KBE models was beneficial for RE. However, the way in which they combine RE and KBE predictions is rather naive (namely, by adding those scores). To the best of our knowledge, there have been no systematic attempts to further unify RE and KBE, particularly during model training. We seek to close this gap with H RERE (Heterogeneous REpresentations for neural Relation Extraction), a novel neural RE framework that learns language and knowledge representations jointly. Figure 1 gives an overview. H RERE’s backbone is a bi-directional l"
P02-1025,A00-2018,0,0.0900877,"Missing"
P02-1025,P01-1017,0,0.484036,"entence, as it is sought by a regular statistical parser. Nevertheless, it is expected that techniques developed in the statistical parsing community that aim at recovering the best parse for an entire sentence, i.e. as judged by a human annotator, should also be productive in enhancing the performance of a language model that uses syntactic structure. The statistical parsing community has used various ways of enriching the dependency structure underlying the parametrization of the probabilistic model used for scoring a given parse tree (Charniak, 2000) (Collins, 1999). Recently, such models (Charniak, 2001) (Roark, 2001) have been shown to outperform the SLM in terms of both PPL and WER on the UPenn Treebank and WSJ corpora, respectively. In (Chelba and Xu, 2001), a simple way of enriching the probabilistic dependencies in the CONSTRUCTOR component of the SLM also showed better PPL and WER performance; the simple modification to the training procedure brought the WER performance of the SLM to the same level with the best as reported in (Roark, 2001). In this paper, we present three simple ways of enriching the syntactic dependency structure in the SLM, extending the work in (Chelba and Xu, 2001)"
P02-1025,W97-0301,0,0.025321,"Missing"
P02-1025,J98-4004,0,\N,Missing
P02-1025,J93-2004,0,\N,Missing
P02-1025,H92-1073,0,\N,Missing
P02-1025,J03-4003,0,\N,Missing
P02-1025,J96-1002,0,\N,Missing
P11-1084,W10-1703,0,0.0364522,"Missing"
P11-1084,P05-1033,0,0.832123,"Combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks. 1 Introduction In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the ta"
P11-1084,J07-2003,0,0.635373,"nslation forest generated by all applicable translation rules, which is not necessarily binary, we apply the synchronous binarization algorithm (Zhang et al., 2006) to generate a binary translation forest. Finally, we use a bottom-up de836 coding algorithm with intergrated LM intersection using the cube pruning technique (Chiang, 2005). The rest of the paper is organized as follows. In Section 2, we give an overview of the forest-tostring models. In Section 2.1, we introduce a more efficient and flexible algorithm for extracting composed GHKM rules based on the same principle as cube pruning (Chiang, 2007). In Section 3, we introduce our source tree binarization algorithm for producing binarized forests. In Section 4, we explain how to do synchronous rule factorization in a forest-to-string decoder. Experimental results are in Section 5. 2 Forest-to-string Translation Forest-to-string models can be described as e = Y( arg max P (d|T ) ) (1) d∈D(T ), T ∈F (f ) where f stands for a source string, e stands for a target string, F stands for a forest, D stands for a set of synchronous derivations on a given tree T , and Y stands for the target side yield of a derivation. The search problem is findin"
P11-1084,D07-1079,0,0.0930875,"). The pruning parameters that control the size of forests are normally handtuned. Such forests encode both syntactic variants and structural variants. By syntactic variants, we refer to the fact that a parser can parse a substring into either a noun phrase or verb phrase in certain cases. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 835–845, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics We believe that structural variants which allow more source spans to be explored during translation are more important (DeNeefe et al., 2007), while syntactic variants might improve word sense disambiguation but also introduce more spurious ambiguities (Chiang, 2005) during decoding. To focus on structural variants, we propose a family of binarization algorithms to expand one single constituent tree into a packed forest of binary trees containing combinations of adjacent tree nodes. We control the freedom of tree node binary combination by restricting the distance to the lowest common ancestor of two tree nodes. We show that the best results are achieved when the distance is two, i.e., when combining tree nodes sharing a common gra"
P11-1084,P03-2041,0,0.278399,"e syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 20"
P11-1084,P08-1109,0,0.0959059,"Missing"
P11-1084,N04-1035,0,0.730036,"e lowest common ancestor of two tree nodes. We show that the best results are achieved when the distance is two, i.e., when combining tree nodes sharing a common grand-parent. In contrast to conventional parser-produced-forestto-string models, in our model: • Forests are not generated by a parser but by combining sub-structures using a tree binarizer. • Instead of using arbitary pruning parameters, we control forest size by an integer number that defines the degree of tree structure violation. • There is at most one nonterminal per span so that the grammar constant is small. Since GHKM rules (Galley et al., 2004) can cover multi-level tree fragments, a synchronous grammar extracted using the GHKM algorithm can have synchronous translation rules with more than two nonterminals regardless of the branching factor of the source trees. For the first time, we show that similar to string-to-tree decoding, synchronous binarization significantly reduces search errors and improves translation quality for forest-to-string decoding. To summarize, the whole pipeline is as follows. First, a parser produces the highest-scored tree for an input sentence. Second, the parse tree is restructured using our binarization a"
P11-1084,P06-1121,0,0.628031,"n the English to German, French, Spanish and Czech tracks. 1 Introduction In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are tak"
P11-1084,J99-4004,0,0.0528737,"ization forest that allows combining any two nodes with common ancestors. The ancestor chain labeled at each node licenses the node to only combine with nodes having common ancestors in the past n generations. The algorithm creates new tree nodes on the fly. 838 New tree nodes need to have their own states indicated by a node label representing what is covered internally by the node and an ancestor chain representing which nodes the node attaches to externally. Line 22 and Line 23 of Algorithm 1 update the label and ancestor annotations of new tree nodes. Using the parsing semiring notations (Goodman, 1999), the ancestor computation can be summarized by the (∩, ∪) pair. ∩ produces the ancestor chain of a hyper-edge. ∪ produces the ancestor chain of a hyper-node. The node label computation can be summarized by the (concatenate, min) pair. concatenate produces a concatenation of node labels. min yields the label with the shortest length. A tree-sequence (Liu et al., 2007) is a sequence of sub-trees covering adjacent spans. It can be proved that the final label of each new node in the forest corresponds to the tree sequence which has the minimum length among all sequences covered by the node span."
P11-1084,N04-1014,0,0.117909,"ow that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks. 1 Introduction In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search o"
P11-1084,2006.amta-papers.8,0,0.0613912,"explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and"
P11-1084,W07-0405,0,0.0168931,"is the closest to our work. But their goal was to augment a k-best forest. They did not binarize the tree sequences. They also did not put constraint on the tree-sequence nodes according to how many brackets are crossed. Wang et al. (2007) used target tree binarization to improve rule extraction for their string-to-tree system. Their binarization forest is equivalent to our cyk-1 forest. In contrast to theirs, our binarization scheme affects decoding directly because we match tree-to-string rules on a binarized forest. Different methods of translation rule binarization have been discussed in Huang (2007). Their argument is that for tree-to-string decoding target side binarization is simpler than synchronous binarization and works well because creating discontinous source spans does not explode the state space. The forest-to-string senario is more similar to string-totree decoding in which state-sharing is important. Our experiments show that synchronous binarization helps significantly in the forest-to-string case. 7 Conclusion We have presented a new approach to tree-to-string translation. It involves a source tree binarization step and a standard forest-to-string translation step. The metho"
P11-1084,P08-1067,0,0.483355,"al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 2008; Mi and Huang, 2008) use a compact representation of exponentially many trees to improve tree-to-string models. Traditionally, such forests are obtained through hyper-edge pruning in the k-best search space of a monolingual parser (Huang, 2008). The pruning parameters that control the size of forests are normally handtuned. Such forests encode both syntactic variants and structural variants. By syntactic variants, we refer to the fact that a parser can parse a substring into either a noun phrase or verb phrase in certain cases. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistic"
P11-1084,N03-1017,0,0.0665333,"Missing"
P11-1084,W04-3250,0,0.47233,"Missing"
P11-1084,P09-1019,0,0.0530621,"Missing"
P11-1084,C04-1090,0,0.0760463,"1 Introduction In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000),"
P11-1084,P06-1077,0,0.182452,", researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To bal"
P11-1084,P07-1089,0,0.0653768,"rnally by the node and an ancestor chain representing which nodes the node attaches to externally. Line 22 and Line 23 of Algorithm 1 update the label and ancestor annotations of new tree nodes. Using the parsing semiring notations (Goodman, 1999), the ancestor computation can be summarized by the (∩, ∪) pair. ∩ produces the ancestor chain of a hyper-edge. ∪ produces the ancestor chain of a hyper-node. The node label computation can be summarized by the (concatenate, min) pair. concatenate produces a concatenation of node labels. min yields the label with the shortest length. A tree-sequence (Liu et al., 2007) is a sequence of sub-trees covering adjacent spans. It can be proved that the final label of each new node in the forest corresponds to the tree sequence which has the minimum length among all sequences covered by the node span. The ancestor chain of a new node is the common ancestors of the nodes in its minimum tree sequence. For clarity, we do full CYK loops over all O(|w|2 ) spans and O(|w|3 ) potential hyper-edges, where |w| is the length of a source string. In reality, only descendants under a shared ancestor can combine. If we assume trees have a bounded branching factor b, the number o"
P11-1084,D08-1022,0,0.572915,"ang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 2008; Mi and Huang, 2008) use a compact representation of exponentially many trees to improve tree-to-string models. Traditionally, such forests are obtained through hyper-edge pruning in the k-best search space of a monolingual parser (Huang, 2008). The pruning parameters that control the size of forests are normally handtuned. Such forests encode both syntactic variants and structural variants. By syntactic variants, we refer to the fact that a parser can parse a substring into either a noun phrase or verb phrase in certain cases. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistic"
P11-1084,P08-1023,0,0.437881,"ctrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-t"
P11-1084,C04-1010,0,0.0431539,"Missing"
P11-1084,J04-4002,0,0.219564,"Missing"
P11-1084,P03-1021,0,0.0827367,"Missing"
P11-1084,P02-1040,0,0.0813322,"Missing"
P11-1084,C00-2092,0,0.0302934,"ring (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 2008; Mi and Huang, 2008) use a compact representation of exponentially many trees to improve tree-to-string models. Traditionally, such forests are obtained through hyper-edge pruning in the k-best search space of a monolingual parser (Huang, 2008). The pruning parameters that control the size of forests are normally handtuned. Such forests encode both syntactic variants and structural variants. By syntactic variants, we refer to the fact that a parser can parse a substring into either a no"
P11-1084,P05-1034,0,0.113255,"tion In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling"
P11-1084,P08-1066,0,0.279402,"an, French, Spanish and Czech tracks. 1 Introduction In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for"
P11-1084,D07-1078,0,0.247228,"Missing"
P11-1084,W06-3108,0,0.0974778,"Missing"
P11-1084,N06-1033,1,0.965494,"duces search errors and improves translation quality for forest-to-string decoding. To summarize, the whole pipeline is as follows. First, a parser produces the highest-scored tree for an input sentence. Second, the parse tree is restructured using our binarization algorithm, resulting in a binary packed forest. Third, we apply the forest-based variant of the GHKM algorithm (Mi and Huang, 2008) on the new forest for rule extraction. Fourth, on the translation forest generated by all applicable translation rules, which is not necessarily binary, we apply the synchronous binarization algorithm (Zhang et al., 2006) to generate a binary translation forest. Finally, we use a bottom-up de836 coding algorithm with intergrated LM intersection using the cube pruning technique (Chiang, 2005). The rest of the paper is organized as follows. In Section 2, we give an overview of the forest-tostring models. In Section 2.1, we introduce a more efficient and flexible algorithm for extracting composed GHKM rules based on the same principle as cube pruning (Chiang, 2007). In Section 3, we introduce our source tree binarization algorithm for producing binarized forests. In Section 4, we explain how to do synchronous rul"
P11-1084,P08-1064,0,0.222725,"tructure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source side which means larger contexts are taken into account for translation (Poutsma, 2000), which is a modeling advantage. To balance efficiency and accuracy, forest-tostring models (Mi et al., 2008; Mi and Huang, 200"
P11-1084,P09-1020,0,0.206536,"do the forest pruning on a forest generated by a k-best algorithm, while we do the forest-pruning on the full CYK chart. As a result, we need more aggressive pruning to control forest size. 842 Related Work The idea of concatenating adjacent syntactic categories has been explored in various syntax-based models. Zollmann and Venugopal (2006) augmented hierarchial phrase based systems with joint syntactic categories. Liu et al. (2007) proposed treesequence-to-string translation rules but did not provide a good solution to place joint subtrees into connection with the rest of the tree structure. Zhang et al. (2009) is the closest to our work. But their goal was to augment a k-best forest. They did not binarize the tree sequences. They also did not put constraint on the tree-sequence nodes according to how many brackets are crossed. Wang et al. (2007) used target tree binarization to improve rule extraction for their string-to-tree system. Their binarization forest is equivalent to our cyk-1 forest. In contrast to theirs, our binarization scheme affects decoding directly because we match tree-to-string rules on a binarized forest. Different methods of translation rule binarization have been discussed in"
P11-1084,W06-3119,0,0.538983,"d significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks. 1 Introduction In recent years, researchers have explored a wide spectrum of approaches to incorporate syntax and structure into machine translation models. The unifying framework for these models is synchronous grammars (Chiang, 2005) or tree transducers (Graehl and Knight, 2004). Depending on whether or not monolingual parsing is carried out on the 835 source side or the target side for inference, there are four general categories within the framework: • string-to-string (Chiang, 2005; Zollmann and Venugopal, 2006) • string-to-tree (Galley et al., 2006; Shen et al., 2008) • tree-to-string (Lin, 2004; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008) • tree-to-tree (Eisner, 2003; Zhang et al., 2008) In terms of search, the string-to-x models explore all possible source parses and map them to the target side, while the tree-to-x models search over the subspace of structures of the source side constrained by an input tree or trees. Hence, tree-to-x models are more constrained but more efficient. Models such as Huang et al. (2006) can match multilevel tree fragments on the source si"
P11-1084,D08-1076,0,\N,Missing
P19-1067,P18-1152,0,0.0232516,"transfer to unseen categories of discourse on Wikipedia articles. 1 Introduction Coherence is a discourse property that is concerned with the logical and semantic organization of a passage, such that the overall meaning of the passage is expressed fluidly and clearly. It is an important quality measure for text generated by humans or machines, and modelling coherence can benefit many applications, including summarization, question answering (Verberne et al., 2007), essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010) and text generation (Park and Kim, 2015; Kiddon et al., 2016; Holtzman et al., 2018). The ability to generalize to new domains of text is desirable for NLP models in general. Besides the practical reason of avoiding costly retraining ∗ Work done while the author was an intern at Borealis AI. 678 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 678–687 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics that there are n! possible sentence orderings for a passage with n sentences, thus the sampled negative instances can only cover a tiny proportion of this space, limiting the performance of su"
P19-1067,D16-1032,0,0.0146629,"allenging settings of transfer to unseen categories of discourse on Wikipedia articles. 1 Introduction Coherence is a discourse property that is concerned with the logical and semantic organization of a passage, such that the overall meaning of the passage is expressed fluidly and clearly. It is an important quality measure for text generated by humans or machines, and modelling coherence can benefit many applications, including summarization, question answering (Verberne et al., 2007), essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010) and text generation (Park and Kim, 2015; Kiddon et al., 2016; Holtzman et al., 2018). The ability to generalize to new domains of text is desirable for NLP models in general. Besides the practical reason of avoiding costly retraining ∗ Work done while the author was an intern at Borealis AI. 678 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 678–687 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics that there are n! possible sentence orderings for a passage with n sentences, thus the sampled negative instances can only cover a tiny proportion of this space, limiti"
P19-1067,J08-1001,0,0.491209,"lize well for cross-domain coherence scoring, with a novel local discriminative neural model. 2. We propose a set of cross-domain coherence datasets with increasingly difficult evaluation protocols. 3. Our new method outperforms previous methods by a significant margin on both the previous closed domain WSJ dataset as well as on all open-domain ones, setting the new stateof-the-art for coherence modelling. 4. Even with the simplest sentence encoder, averaged GloVe, our method frequently outperforms previous methods, while it can gain further accuracy by using stronger encoders. 2 Related Work Barzilay and Lapata (2008) introduced the entity grid representation of a document, which uses the local syntactic transitions of entity mentions to model discourse coherence. Three tasks for evaluation were introduced for evaluation: discrimination, summary coherence rating, and readability assessment. Many models were proposed to extend this model (Eisner and Charniak, 2011; Feng and Hirst, 2012; Guinaudeau and Strube, 2013), including models relying on HMMs (Louis and Nenkova, 2012) to model document structure. Driven by the success of deep neural networks, many neural models were proposed in the past few years. Li"
P19-1067,N10-1099,0,0.20968,"Missing"
P19-1067,D17-1070,0,0.125838,"effectively covered by a sampling procedure. In practice, we sample a new set of negatives each time we see a document, hence after many epochs, we can effectively cover the space for even very long documents. Section 5.7 discusses further details on sampling. 4.1 Figure 1: Generic architecture for our proposed model. 4.2 Pre-trained Generative Model as the Sentence Encoder Our model can work with any pre-trained sentence encoder, ranging from the most simplistic average GloVe (Pennington et al., 2014) embeddings to more sophisticated supervised or unsupervised pre-trained sentence encoders (Conneau et al., 2017). As mentioned in the introduction, since generative models can often be turned into sentence encoder, generative coherence model can be leveraged by our model to benefit from the advantages of both generative and discriminative training, similar to (Kiros et al., 2015; Peters et al., 2018). After initialization, we freeze the generative model parameters to avoid overfitting. In Section 5, we will experimentally show that while we do benefit from strong pre-trained encoders, the fact that our local discriminative model improves over previous methods is independent of the choice of sentence enc"
P19-1067,D14-1218,0,0.512794,"08) introduced the entity grid representation of a document, which uses the local syntactic transitions of entity mentions to model discourse coherence. Three tasks for evaluation were introduced for evaluation: discrimination, summary coherence rating, and readability assessment. Many models were proposed to extend this model (Eisner and Charniak, 2011; Feng and Hirst, 2012; Guinaudeau and Strube, 2013), including models relying on HMMs (Louis and Nenkova, 2012) to model document structure. Driven by the success of deep neural networks, many neural models were proposed in the past few years. Li and Hovy (2014) proposed a neural clique-based discriminative model to compute the coherence score of a document by estimating a coherence probability for each clique of L sentences. Nguyen and Joty (2017) proposed a neural entity 679 sky, 2017), or all previous sentences (Logeswaran et al., 2018). There are two hidden assumptions behind this maximum likelihood approach to coherence. First, it assumes that conditional log likelihood is a good proxy for coherence. Second, it assumes that training can well capture the long-range dependencies implied by the generative model. Conditional log likelihood essential"
P19-1067,P11-2022,0,0.348976,"setting the new stateof-the-art for coherence modelling. 4. Even with the simplest sentence encoder, averaged GloVe, our method frequently outperforms previous methods, while it can gain further accuracy by using stronger encoders. 2 Related Work Barzilay and Lapata (2008) introduced the entity grid representation of a document, which uses the local syntactic transitions of entity mentions to model discourse coherence. Three tasks for evaluation were introduced for evaluation: discrimination, summary coherence rating, and readability assessment. Many models were proposed to extend this model (Eisner and Charniak, 2011; Feng and Hirst, 2012; Guinaudeau and Strube, 2013), including models relying on HMMs (Louis and Nenkova, 2012) to model document structure. Driven by the success of deep neural networks, many neural models were proposed in the past few years. Li and Hovy (2014) proposed a neural clique-based discriminative model to compute the coherence score of a document by estimating a coherence probability for each clique of L sentences. Nguyen and Joty (2017) proposed a neural entity 679 sky, 2017), or all previous sentences (Logeswaran et al., 2018). There are two hidden assumptions behind this maximum"
P19-1067,D17-1019,0,0.313058,"tly ordered list of sentences and a random permutation thereof. Earlier work focused on feature engineering, drawing on theories such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Thompson and Mann, 1987) to propose features based on local entity and lexical transitions, as well as more global concerns regarding topic transitions (Elsner et al., 2007). With the popularization of deep learning, the focus has shifted towards specifying model architectures, including a number of recent models that rely on distributed word representations used in a deep neural network (Li and Jurafsky, 2017; Nguyen and Joty, 2017; Logeswaran et al., 2018). One key decision which forms the foundation of a model is whether it is discriminative or generative. Discriminative models depend on contrastive learning; they use automatic corruption methods to generate incoherent passages of text, then learn to distinguish coherent passages from incoherent ones. By contrast, generative approaches aim at maximising the likelihood of the training text, which is assumed to be coherent, without seeing incoherent text or explicitly incorporating coherence into the optimization objective. It has been argued that"
P19-1067,N07-1055,0,0.676238,"e semantic relationships between sentences, rather than simply overfit to the structural cues of a specific domain. The standard task used to test a coherence model in NLP is sentence ordering, for example, to distinguish between a coherently ordered list of sentences and a random permutation thereof. Earlier work focused on feature engineering, drawing on theories such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Thompson and Mann, 1987) to propose features based on local entity and lexical transitions, as well as more global concerns regarding topic transitions (Elsner et al., 2007). With the popularization of deep learning, the focus has shifted towards specifying model architectures, including a number of recent models that rely on distributed word representations used in a deep neural network (Li and Jurafsky, 2017; Nguyen and Joty, 2017; Logeswaran et al., 2018). One key decision which forms the foundation of a model is whether it is discriminative or generative. Discriminative models depend on contrastive learning; they use automatic corruption methods to generate incoherent passages of text, then learn to distinguish coherent passages from incoherent ones. By contr"
P19-1067,D12-1106,0,0.162649,"Ve, our method frequently outperforms previous methods, while it can gain further accuracy by using stronger encoders. 2 Related Work Barzilay and Lapata (2008) introduced the entity grid representation of a document, which uses the local syntactic transitions of entity mentions to model discourse coherence. Three tasks for evaluation were introduced for evaluation: discrimination, summary coherence rating, and readability assessment. Many models were proposed to extend this model (Eisner and Charniak, 2011; Feng and Hirst, 2012; Guinaudeau and Strube, 2013), including models relying on HMMs (Louis and Nenkova, 2012) to model document structure. Driven by the success of deep neural networks, many neural models were proposed in the past few years. Li and Hovy (2014) proposed a neural clique-based discriminative model to compute the coherence score of a document by estimating a coherence probability for each clique of L sentences. Nguyen and Joty (2017) proposed a neural entity 679 sky, 2017), or all previous sentences (Logeswaran et al., 2018). There are two hidden assumptions behind this maximum likelihood approach to coherence. First, it assumes that conditional log likelihood is a good proxy for coheren"
P19-1067,E12-1032,0,0.0246755,"-art for coherence modelling. 4. Even with the simplest sentence encoder, averaged GloVe, our method frequently outperforms previous methods, while it can gain further accuracy by using stronger encoders. 2 Related Work Barzilay and Lapata (2008) introduced the entity grid representation of a document, which uses the local syntactic transitions of entity mentions to model discourse coherence. Three tasks for evaluation were introduced for evaluation: discrimination, summary coherence rating, and readability assessment. Many models were proposed to extend this model (Eisner and Charniak, 2011; Feng and Hirst, 2012; Guinaudeau and Strube, 2013), including models relying on HMMs (Louis and Nenkova, 2012) to model document structure. Driven by the success of deep neural networks, many neural models were proposed in the past few years. Li and Hovy (2014) proposed a neural clique-based discriminative model to compute the coherence score of a document by estimating a coherence probability for each clique of L sentences. Nguyen and Joty (2017) proposed a neural entity 679 sky, 2017), or all previous sentences (Logeswaran et al., 2018). There are two hidden assumptions behind this maximum likelihood approach t"
P19-1067,J95-2003,0,0.927768,"realisai.com 2 McGill University {joey.bose,jcheung}@cs.mcgill.ca 3 Canada CIFAR Chair, Mila Abstract on every new domain, for coherence modelling, we would also like our model to make decisions based on the semantic relationships between sentences, rather than simply overfit to the structural cues of a specific domain. The standard task used to test a coherence model in NLP is sentence ordering, for example, to distinguish between a coherently ordered list of sentences and a random permutation thereof. Earlier work focused on feature engineering, drawing on theories such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Thompson and Mann, 1987) to propose features based on local entity and lexical transitions, as well as more global concerns regarding topic transitions (Elsner et al., 2007). With the popularization of deep learning, the focus has shifted towards specifying model architectures, including a number of recent models that rely on distributed word representations used in a deep neural network (Li and Jurafsky, 2017; Nguyen and Joty, 2017; Logeswaran et al., 2018). One key decision which forms the foundation of a model is whether it is discriminative or generative."
P19-1067,P18-1052,0,0.153447,"a coherent sentence does not need to have high conditional loglikelihood, as log likelihood can also be influenced by other factors such as fluency, grammaticality, sentence length, and the frequency of words in a sentence. Second, capturing long-range dependencies in neural sequence models is still an active area of research with many challenges (Trinh et al., 2018), hence there is no guarantee that maximum likelihood learning can faithfully capture the inductive bias behind the first assumption. grid model with convolutional neural network that operates over the entity grid representation. Mohiuddin et al. (2018) extended this model for written asynchronous conversations. Both methods rely on hand-crafted features derived from NLP preprocessing tools to enhance the original entity grid representation. We take a different approach to feature engineering in our work, focusing on the effect of supervised or unsupervised pre-training. Li and Jurafsky (2017) was the first work to use generative models to model coherence and proposed to evaluate the performance of coherence models in an open-domain setting. Most recently, Logeswaran et al. (2018) used an RNN based encoder-decoder architecture to model the c"
P19-1067,P13-1010,0,0.125197,"elling. 4. Even with the simplest sentence encoder, averaged GloVe, our method frequently outperforms previous methods, while it can gain further accuracy by using stronger encoders. 2 Related Work Barzilay and Lapata (2008) introduced the entity grid representation of a document, which uses the local syntactic transitions of entity mentions to model discourse coherence. Three tasks for evaluation were introduced for evaluation: discrimination, summary coherence rating, and readability assessment. Many models were proposed to extend this model (Eisner and Charniak, 2011; Feng and Hirst, 2012; Guinaudeau and Strube, 2013), including models relying on HMMs (Louis and Nenkova, 2012) to model document structure. Driven by the success of deep neural networks, many neural models were proposed in the past few years. Li and Hovy (2014) proposed a neural clique-based discriminative model to compute the coherence score of a document by estimating a coherence probability for each clique of L sentences. Nguyen and Joty (2017) proposed a neural entity 679 sky, 2017), or all previous sentences (Logeswaran et al., 2018). There are two hidden assumptions behind this maximum likelihood approach to coherence. First, it assumes"
P19-1067,P17-1121,0,0.716253,"tences and a random permutation thereof. Earlier work focused on feature engineering, drawing on theories such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Thompson and Mann, 1987) to propose features based on local entity and lexical transitions, as well as more global concerns regarding topic transitions (Elsner et al., 2007). With the popularization of deep learning, the focus has shifted towards specifying model architectures, including a number of recent models that rely on distributed word representations used in a deep neural network (Li and Jurafsky, 2017; Nguyen and Joty, 2017; Logeswaran et al., 2018). One key decision which forms the foundation of a model is whether it is discriminative or generative. Discriminative models depend on contrastive learning; they use automatic corruption methods to generate incoherent passages of text, then learn to distinguish coherent passages from incoherent ones. By contrast, generative approaches aim at maximising the likelihood of the training text, which is assumed to be coherent, without seeing incoherent text or explicitly incorporating coherence into the optimization objective. It has been argued that neural discriminative"
P19-1067,D14-1162,0,0.0865179,"er of negatives provides a rich enough learning signal, while at the same time, is not too prohibitively large to be effectively covered by a sampling procedure. In practice, we sample a new set of negatives each time we see a document, hence after many epochs, we can effectively cover the space for even very long documents. Section 5.7 discusses further details on sampling. 4.1 Figure 1: Generic architecture for our proposed model. 4.2 Pre-trained Generative Model as the Sentence Encoder Our model can work with any pre-trained sentence encoder, ranging from the most simplistic average GloVe (Pennington et al., 2014) embeddings to more sophisticated supervised or unsupervised pre-trained sentence encoders (Conneau et al., 2017). As mentioned in the introduction, since generative models can often be turned into sentence encoder, generative coherence model can be leveraged by our model to benefit from the advantages of both generative and discriminative training, similar to (Kiros et al., 2015; Peters et al., 2018). After initialization, we freeze the generative model parameters to avoid overfitting. In Section 5, we will experimentally show that while we do benefit from strong pre-trained encoders, the fac"
P19-1067,N18-1202,0,0.0109268,"ture for our proposed model. 4.2 Pre-trained Generative Model as the Sentence Encoder Our model can work with any pre-trained sentence encoder, ranging from the most simplistic average GloVe (Pennington et al., 2014) embeddings to more sophisticated supervised or unsupervised pre-trained sentence encoders (Conneau et al., 2017). As mentioned in the introduction, since generative models can often be turned into sentence encoder, generative coherence model can be leveraged by our model to benefit from the advantages of both generative and discriminative training, similar to (Kiros et al., 2015; Peters et al., 2018). After initialization, we freeze the generative model parameters to avoid overfitting. In Section 5, we will experimentally show that while we do benefit from strong pre-trained encoders, the fact that our local discriminative model improves over previous methods is independent of the choice of sentence encoder. Model Architecture The specific neural architecture that we use for fθ is illustrated in Figure 1. We assume the use of some pre-trained sentence encoder, which is discussed in the next section. Given an input sentence pair, the sentence encoder maps the sentences to real-valued vecto"
S18-1039,D14-1162,0,0.0853705,"nsfer the emotional knowledge by exploiting neural network models as feature extractors and use these representations for traditional machine learning models such as support vector regression (SVR) and logistic regression to solve the competition tasks. Our system is placed among the Top3 for all subtasks we participated. 1 Introduction Finding a good representation of texts is very challenging since texts are sequences of words which are represented in a discrete space of the vocabulary. For this reason, many past works have investigated in finding the mapping of words (Mikolov et al., 2013; Pennington et al., 2014) or sentences (Kiros et al., 2015) to continuous spaces, so that each text can be represented by a fixed-size, realvalued N-dimensional vector. This vector representation then can be applied to machine learning models to solve problems like classification and regression. A good representation should contain essential information inside each text and be a useful input for statistical models. Emotions in texts further deepen the complexity of modeling natural language since they not only depend on the semantics of a language but also are inherently subjective and ambiguous. Despite the difficult"
S18-1039,S16-1010,0,0.049169,"Missing"
S18-1039,D17-1169,0,0.0439512,"we group the 34 emojis into 11 clusters according to the distance on the correlation matrix of the hierarchical clustering from Felbo et al. (2017) and use them as categorical labels tags and emoticons (Suttles and Ide, 2013; Wang et al., 2012), and found them useful to distantly label an emotion of each text. Furthermore, the recent popular culture of using emojis (Wood and Ruder, 2016) inside social media posts and messages provides us even richer evidence of different emotions, and they have been proven to be very effective in learning rich representations for various affect-related tasks (Felbo et al., 2017). 2.1 Methodology & Emoji Dataset One thing i dislike is laggers man I hate inconsistency The paper is irritating me As of right now i hate dre im sick of crying im tired of trying why body pain why uuugh i really have nothing to do right now i dont wanna go back to mex looking forward to holiday well today am on lake garda enjoying the life perfect time to read book im feeling great enjoying my holiday In this paper, we compare two models using two different emoji dataset to transform the competition data into robust sentence representations. First model is the pre-trained DeepMoji model (Fel"
S18-1039,D14-1181,0,0.00882238,"Missing"
S18-1039,P14-1146,0,0.109383,"n labels, we focus on four emotion categories: joy, sadness, anger, and fear, since the competition tasks are only limited to those categories. In total, our hashtag dataset consists of 1.9 million tweets (Table 2). 3.2 Hashtag Dataset Emotional word vectors (EVEC) We also explore word-level representations, along with emoji sentence representations. Although sentence-level representations already build up from word representations (in particular we use pretrained Glove vectors (Pennington et al., 2014)), they may not be enough to attend to the valence that each word contains. Previous works (Tang et al., 2014) examine the significance of using sentiment-specific word embedding for related tasks. For this reason, we train emotional word vectors that not only cluster together direct emotion words such as anger and joy, but also capture emotions inside indirect emotion words, such as anger inside headache and joy inside beach. We learn these vectors by training a Convolutional Neural Network (CNN) from another separate Twitter corpus distantly labeled with hashtags. 3.1 Methodology Our intuition to learn effective emotion word vectors is that given a document labeled with emotion there exists one or m"
S18-1039,S18-1001,0,0.131685,"Missing"
S18-1039,L18-1030,0,0.0438488,"Missing"
S18-1039,S16-1040,0,0.0447858,"Missing"
S19-2021,D16-1110,1,0.844814,"Missing"
S19-2021,S19-2005,0,0.0485243,"Missing"
S19-2021,P15-1162,0,0.0559911,"Missing"
S19-2021,S18-1019,0,0.0419734,"Missing"
S19-2021,D17-1054,0,0.0171696,"ji (Felbo et al., 2017a), train a biLSTM model to encode the whole sentence to predict the corresponding emoji of the sentence. The learned model achieves stateof-the-art results on eight datasets. Sentiment lexicons from Taboada et al. (2011) show that word lexicons annotated with sentiment/emotion labels are effective in sentiment classification. This method is further developed using both supervised and unsupervised approaches in Wang and Xia (2017). Also, other models, such as a deep averaging network (Iyyer et al., 2015), attention-based network (Winata et al., 2018), and memory network (Dou, 2017), have been investigated to improve the classification performance. Practically, the application of emotion classification has been investigated on interactive dialogue systems (Bertero et al., 2016; Winata et al., 2017; Siddique et al., 2017; Fung et al., 2018). 5 Conclusion In this paper, we compare different pre-trained word embedding features by using Logistic Regression and XGBoost along with flat and hierarchical architectures trained in end-to-end models. We further explore a GP for faster hyperparameter search. Our experiments show that hierarchical architectures give significant impro"
S19-2021,W16-6208,0,0.0375742,"Missing"
S19-2021,S18-1039,1,0.819812,"Missing"
S19-2021,D17-1169,0,0.268995,"ecifically by contextual emotion detection in text. Given a textual dialogue with two turns of context, the system has to classify the emotion of the next utterance into one of the following emotion classes: Happy, Sad, Angry, or Others. The training dataset contains *Equal contribution. 15K records for emotion classes, and contains 15K records not belonging to any of the aforementioned emotion classes. The most naive first step would be to recognize emotion from a given flattened sequence, which has been researched extensively despite the very abstract nature of emotion (Socher et al., 2013; Felbo et al., 2017a; McCann et al., 2017; Xu et al., 2018; Chatterjee et al., 2019a). However, these flat models do not work very well on dialogue data as we have to merely concatenate the turns and flatten the hierarchical information. Not only does the sequence get too long, but the hierarchy between sentences will also be destroyed (Hsu and Ku, 2018; Kim et al., 2018). We believe that the natural flow of emotion exists in dialogue, and using such hierarchical information will allow us to predict the last utterance’s emotion better. Naturally, the next step is to be able to detect emotion with a hierarchical"
S19-2021,N18-1202,0,0.275659,"tep is to be able to detect emotion with a hierarchical structure. To the best of our knowledge, this task of extracting emotional knowledge in a hierarchical setting has not yet been extensively explored in the literature. Therefore, in this paper, we investigate this problem in depth with several strong hierarchical baselines and by using a large variety of pre-trained word embeddings. 2 Methodology In this task, we focus on two main approaches: 1) feature-based and 2) end-to-end. The former compares several well-known pre-trained embeddings, including GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2018), as well as emotional embeddings. We combine these pre-trained features with a simple Logistic Regression (LR) and XGBoost (Chen and Guestrin, 2016) model as the classifier to compare their effectiveness. The latter approach is to train Emotion a) b) Softmax T2 c) Emotion Voting Softmax Encoder (E) T1 Emotion T3 E E E M1 T1 T2 T3 T1 T2 T3 ... Mn−1 Mn T1 T2 T3 T1 T2 T3 Figure 1: a) Flat model; b) Hierarchical model; c) Voting scheme a model fully end-to-end with back-propagation. We mainly compare the performances of flat models and hierarchical models, which al"
S19-2021,P17-4021,1,0.890635,"Missing"
S19-2021,D13-1170,0,0.00394712,"to be empathetic, specifically by contextual emotion detection in text. Given a textual dialogue with two turns of context, the system has to classify the emotion of the next utterance into one of the following emotion classes: Happy, Sad, Angry, or Others. The training dataset contains *Equal contribution. 15K records for emotion classes, and contains 15K records not belonging to any of the aforementioned emotion classes. The most naive first step would be to recognize emotion from a given flattened sequence, which has been researched extensively despite the very abstract nature of emotion (Socher et al., 2013; Felbo et al., 2017a; McCann et al., 2017; Xu et al., 2018; Chatterjee et al., 2019a). However, these flat models do not work very well on dialogue data as we have to merely concatenate the turns and flatten the hierarchical information. Not only does the sequence get too long, but the hierarchy between sentences will also be destroyed (Hsu and Ku, 2018; Kim et al., 2018). We believe that the natural flow of emotion exists in dialogue, and using such hierarchical information will allow us to predict the last utterance’s emotion better. Naturally, the next step is to be able to detect emotion"
S19-2021,J11-2001,0,0.00541946,"t ways. Word-level emotional representations, inspired from word embeddings, learn a vector for each word, and have shown effectiveness in different emotion related tasks, such as sentiment classification (Tang et al., 2016), emotion classification (Xu et al., 2018), and emotion intensity prediction (Park et al., 2018). Sentence-level emotional representations, such as DeepMoji (Felbo et al., 2017a), train a biLSTM model to encode the whole sentence to predict the corresponding emoji of the sentence. The learned model achieves stateof-the-art results on eight datasets. Sentiment lexicons from Taboada et al. (2011) show that word lexicons annotated with sentiment/emotion labels are effective in sentiment classification. This method is further developed using both supervised and unsupervised approaches in Wang and Xia (2017). Also, other models, such as a deep averaging network (Iyyer et al., 2015), attention-based network (Winata et al., 2018), and memory network (Dou, 2017), have been investigated to improve the classification performance. Practically, the application of emotion classification has been investigated on interactive dialogue systems (Bertero et al., 2016; Winata et al., 2017; Siddique et"
S19-2021,D17-1052,0,0.0309174,"l., 2016), emotion classification (Xu et al., 2018), and emotion intensity prediction (Park et al., 2018). Sentence-level emotional representations, such as DeepMoji (Felbo et al., 2017a), train a biLSTM model to encode the whole sentence to predict the corresponding emoji of the sentence. The learned model achieves stateof-the-art results on eight datasets. Sentiment lexicons from Taboada et al. (2011) show that word lexicons annotated with sentiment/emotion labels are effective in sentiment classification. This method is further developed using both supervised and unsupervised approaches in Wang and Xia (2017). Also, other models, such as a deep averaging network (Iyyer et al., 2015), attention-based network (Winata et al., 2018), and memory network (Dou, 2017), have been investigated to improve the classification performance. Practically, the application of emotion classification has been investigated on interactive dialogue systems (Bertero et al., 2016; Winata et al., 2017; Siddique et al., 2017; Fung et al., 2018). 5 Conclusion In this paper, we compare different pre-trained word embedding features by using Logistic Regression and XGBoost along with flat and hierarchical architectures trained i"
S19-2021,W18-6243,1,0.925306,"in text. Given a textual dialogue with two turns of context, the system has to classify the emotion of the next utterance into one of the following emotion classes: Happy, Sad, Angry, or Others. The training dataset contains *Equal contribution. 15K records for emotion classes, and contains 15K records not belonging to any of the aforementioned emotion classes. The most naive first step would be to recognize emotion from a given flattened sequence, which has been researched extensively despite the very abstract nature of emotion (Socher et al., 2013; Felbo et al., 2017a; McCann et al., 2017; Xu et al., 2018; Chatterjee et al., 2019a). However, these flat models do not work very well on dialogue data as we have to merely concatenate the turns and flatten the hierarchical information. Not only does the sequence get too long, but the hierarchy between sentences will also be destroyed (Hsu and Ku, 2018; Kim et al., 2018). We believe that the natural flow of emotion exists in dialogue, and using such hierarchical information will allow us to predict the last utterance’s emotion better. Naturally, the next step is to be able to detect emotion with a hierarchical structure. To the best of our knowledge"
S19-2021,D14-1162,0,\N,Missing
S19-2021,W18-3505,0,\N,Missing
W03-1021,J96-1002,0,0.00518108,"ining the Neural Network Model Standard back-propagation is used to train the parameters of the neural network as well as the feature vectors. See (Haykin, 1999) for details about neural networks and back-propagation. The function we try to maximize is the log-likelihood of the training data given by equation 1. It is straightforward to compute the gradient of the likelihood function for the feature vectors and the neural network parameters, and hence compute their updates. We should note from equation 4 that the neural network model is similar in functional form to the maximum entropy model (Berger et al., 1996) except that the neural network learns the feature functions by itself from the training data. However, unlike the G/IIS algorithm for the maximum entropy model, the training algorithm (usually stochastic gradient descent) for the neural network models is not guaranteed to find even a local maximum of the objective function. It is very important to mention that one of the great advantages of this model is that the number of inputs can be increased causing only sub-linear increase in the number of model parameters, as opposed to exponential growth in n-gram models. This makes the parameter esti"
W03-1021,P01-1017,0,0.0599228,"he often prohibitively large hypothesis space. Most of the state-of-the-art systems use n-gram language models, which are simple and effective most of the time. Many smoothing techniques that improve language model probability estimation have been proposed and studied in the n-gram literature (Chen and Goodman, 1998). Recent efforts have studied various ways of using information from a longer context span than that usually captured by normal n-gram language models, as well as ways of using syntactical information that is not available to the word-based n-gram models (Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001; Uystel et al., 2001). All these language models are based on stochastic parsing techniques that build up parse trees for the input word sequence and condition the generation of words on syntactical and lexical information available in the parse trees. Since these language models capture useful hierarchical characteristics of language, they can improve the PPL significantly for various tasks. Although more improvement can be achieved by enriching the syntactical dependencies in the structured language model (SLM) (Xu et al., 2002), a severe data sparseness problem was observed in"
W03-1021,E03-1002,0,0.0184181,"available in the parse trees. Since these language models capture useful hierarchical characteristics of language, they can improve the PPL significantly for various tasks. Although more improvement can be achieved by enriching the syntactical dependencies in the structured language model (SLM) (Xu et al., 2002), a severe data sparseness problem was observed in (Xu et al., 2002) when the number of conditioning features was increased. There has been recent promising work in using distributional representation of words and neural networks for language modeling (Bengio et al., 2001) and parsing (Henderson, 2003). One great advantage of this approach is its ability to fight data sparseness. The model size grows only sub-linearly with the number of predicting features used. It has been shown that this method improves significantly on regular n-gram models in perplexity (Bengio et al., 2001). The ability of the method to accommodate longer contexts is most appealing, since experiments have shown consistent improvements in PPL when the context of one of the components of the SLM is increased in length (Emami et al., 2003). Moreover, because the SLM provides an EM training procedure for its components, th"
W03-1021,P02-1025,1,0.934174,"word-based n-gram models (Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001; Uystel et al., 2001). All these language models are based on stochastic parsing techniques that build up parse trees for the input word sequence and condition the generation of words on syntactical and lexical information available in the parse trees. Since these language models capture useful hierarchical characteristics of language, they can improve the PPL significantly for various tasks. Although more improvement can be achieved by enriching the syntactical dependencies in the structured language model (SLM) (Xu et al., 2002), a severe data sparseness problem was observed in (Xu et al., 2002) when the number of conditioning features was increased. There has been recent promising work in using distributional representation of words and neural networks for language modeling (Bengio et al., 2001) and parsing (Henderson, 2003). One great advantage of this approach is its ability to fight data sparseness. The model size grows only sub-linearly with the number of predicting features used. It has been shown that this method improves significantly on regular n-gram models in perplexity (Bengio et al., 2001). The ability o"
W04-3242,P01-1017,0,\N,Missing
W04-3242,P02-1025,1,\N,Missing
W18-6243,P15-1166,0,0.036932,"rious methods are developed for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et al. (2017) to train a large model on a huge emoji tweet corpus, which boosts the performance of affectrelated tasks. Multi-task training has achieved great success in various natural language tasks, such as machine translation (Dong et al., 2015; Malaviya et al., 2017), multilingual tasks (Duong et al., 2015; Gillick et al., 2016), semantic parsIn this paper, we propose Emo2Vec to represent emotion with vectors using a multi-task training framework. Six affect-related tasks are utilized, including emotion/sentiment analysis, sarcasm classification, stress detection, abusive language classification, insult detection, and personality recognition. We empirically show how Emo2Vec leverages multi-task training to learn a generalized emotion representation. In addition, Emo2Vec outperforms existing affect-related embeddings on more than te"
W18-6243,D17-1268,0,0.0190674,"eveloped for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et al. (2017) to train a large model on a huge emoji tweet corpus, which boosts the performance of affectrelated tasks. Multi-task training has achieved great success in various natural language tasks, such as machine translation (Dong et al., 2015; Malaviya et al., 2017), multilingual tasks (Duong et al., 2015; Gillick et al., 2016), semantic parsIn this paper, we propose Emo2Vec to represent emotion with vectors using a multi-task training framework. Six affect-related tasks are utilized, including emotion/sentiment analysis, sarcasm classification, stress detection, abusive language classification, insult detection, and personality recognition. We empirically show how Emo2Vec leverages multi-task training to learn a generalized emotion representation. In addition, Emo2Vec outperforms existing affect-related embeddings on more than ten different datasets. By"
W18-6243,D17-1054,0,0.0408305,", more attention needs to be paid to words. ing (Peng et al., 2017). Hashimoto et al. (2017) jointly learns POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment by considering linguistic hierarchy and achieves state-of-the-results on five datasets. For sentiment analysis, Balikas et al. (2017) jointly trains ternary and fine-grained classification with a recurrent neural network and achieves new stateof-the-art results. 5 6 Conclusion and Future Work Related work For sentiment analysis, numerous classification models (Kalchbrenner et al.; Iyyer et al., 2015; Dou, 2017) have been explored. Multi-modal sentiment analysis (Zadeh et al., 2017; Poria et al., 2017) extends text-based model to the combination of visual, acoustic and language, which achieves better results than the single modality. Various methods are developed for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et"
W18-6243,P17-1186,0,0.0314113,"Missing"
W18-6243,uryupina-etal-2014-sentube,0,0.0275266,"ed is illustrated in Figure 2. Firstly, 1-D convolution is used to extract n293 personality recognition are included. The reason why we include many datasets is to 1) leverage different aspects of words emotion knowledge, which may not be present in single domain dataset; 2) create a more general embedding emotional space that can generalize well across different tasks and domains. To avoid over-fitting, L2 regularization penalty is added from the weights of all logistic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Suppleme"
W18-6243,D17-1052,0,0.0637837,"-grained classification with a recurrent neural network and achieves new stateof-the-art results. 5 6 Conclusion and Future Work Related work For sentiment analysis, numerous classification models (Kalchbrenner et al.; Iyyer et al., 2015; Dou, 2017) have been explored. Multi-modal sentiment analysis (Zadeh et al., 2017; Poria et al., 2017) extends text-based model to the combination of visual, acoustic and language, which achieves better results than the single modality. Various methods are developed for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et al. (2017) to train a large model on a huge emoji tweet corpus, which boosts the performance of affectrelated tasks. Multi-task training has achieved great success in various natural language tasks, such as machine translation (Dong et al., 2015; Malaviya et al., 2017), multilingual tasks (Duong et al., 2015; Gillick et al., 2016), semantic parsIn this paper, we prop"
W18-6243,D14-1162,0,0.0828235,"sive language classification, insult detection, and personality recognition. Our evaluation of Emo2Vec shows that it outperforms existing affect-related representations, such as Sentiment-Specific Word Embedding and DeepMoji embeddings with much smaller training corpora. When concatenated with GloVe, Emo2Vec achieves competitive performances to state-of-the-art results on several tasks using a simple logistic regression classifier. 1 Introduction Recent work on word representation has been focusing on embedding syntactic and semantic information into fixed-sized vectors (Mikolov et al., 2013; Pennington et al., 2014) based on the distributional hypothesis, and have proven to be useful in many natural language tasks (Collobert et al., 2011). However, despite the rising popularity regarding the use of word embeddings, they often fail to capture the emotional semantics the words convey. For example, the GloVe vector captures the semantic meaning of “headache”, as it is closer to words of ill symptoms like “fever” and “toothache”, but misses the emotional association that the word carries. The word “headache” in the sentence “You are giving me a headache” does not really mean that the speaker will get a heada"
W18-6243,D16-1058,0,0.0571363,"Missing"
W18-6243,W16-5618,0,0.0119463,"ts of all logistic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Supplemental Material. ∑ 1∑ Lj + λ ∥LRϕj ∥2 n n n j=1 j=1 Where Lj is the negative log likelihood (NLL) between yˆj and y j , and λ an hyper-parameter for the regularization terms. 3 3.2 Pre-training Emo2Vec Emo2Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alone. Parameters of T and CNN are randomly initialized and Adam is used for optimization. Best parameter settings are tuned on the validation set. For the best model,"
W18-6243,W13-1603,0,0.02274,"nt in single domain dataset; 2) create a more general embedding emotional space that can generalize well across different tasks and domains. To avoid over-fitting, L2 regularization penalty is added from the weights of all logistic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Supplemental Material. ∑ 1∑ Lj + λ ∥LRϕj ∥2 n n n j=1 j=1 Where Lj is the negative log likelihood (NLL) between yˆj and y j , and λ an hyper-parameter for the regularization terms. 3 3.2 Pre-training Emo2Vec Emo2Vec embedding matrix and the CNN model a"
W18-6243,N16-2013,0,0.0172717,"stic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Supplemental Material. ∑ 1∑ Lj + λ ∥LRϕj ∥2 n n n j=1 j=1 Where Lj is the negative log likelihood (NLL) between yˆj and y j , and λ an hyper-parameter for the regularization terms. 3 3.2 Pre-training Emo2Vec Emo2Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alone. Parameters of T and CNN are randomly initialized and Adam is used for optimization. Best parameter settings are tuned on the validation set. For the best model, we use the batch size o"
W18-6243,D13-1170,0,0.00939142,"ation Baselines: We use 50-dimension Sentimentspecific Word Embedding (SSWE) (Tang et al., 2016) as our baseline, which is an embedding model trained with 10 millions of tweets by encoding both semantic and sentiment information into vectors. Also, lots of work about the detection/classification in sentiment analysis implicitly encodes emotion inside the word vectors. For example, Felbo et al. (2017) trains a two-layer bidirectional Long Short-Term Memory (bi-LSTM) model, named DeepMoji, to predict emoji of the Small datasets For sentiment, we include 8 datasets. (1,2) SSTfine and SST-binary (Socher et al., 2013) (3) OpeNER (Agerri et al., 2013) (4,5) tube auto 2 http://hci.epfl.ch/sharing-emotion-lexicons-anddata#emo-hash-data 294 model SSWE DeepMoji embedding CNN embedding Emo2Vec SS-T 0.815 0.788 0.803 0.801 SS-Y 0.835 0.841 0.862 0.859 SS-binary 0.698 0.751 0.734 0.812 SS-fine 0.365 0.369 0.369 0.416 OpeNER 0.701 0.754 0.713 0.744 tube auto 0.620 0.628 0.605 0.629 tube tablet 0.654 0.675 0.667 0.688 SemEval 0.629 0.676 0.622 0.638 average 0.665 0.685 0.672 0.698 Table 1: Comparison between different emotion representations on sentiment datasets, all results are reported with accuracy. The best res"
W18-6243,P14-2070,0,0.0448146,"ate a more general embedding emotional space that can generalize well across different tasks and domains. To avoid over-fitting, L2 regularization penalty is added from the weights of all logistic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Supplemental Material. ∑ 1∑ Lj + λ ∥LRϕj ∥2 n n n j=1 j=1 Where Lj is the negative log likelihood (NLL) between yˆj and y j , and λ an hyper-parameter for the regularization terms. 3 3.2 Pre-training Emo2Vec Emo2Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alo"
W18-6243,D17-1056,0,0.0363301,"by Multi-task Training Peng Xu, Andrea Madotto, Chien-Sheng Wu, Ji Ho Park and Pascale Fung Center for Artificial Intelligence Research (CAiRE) The Hong Kong University of Science and Technology, Clear Water Bay [pxuab,eeandreamad,cwuak,jhpark,pascale]@ust.hk Abstract and syntactic contextual information in a vector space. This work demonstrates the effectiveness of incorporating sentiment labels in a wordlevel information for sentiment-related tasks compared to other word embeddings. However, they only focus on binary labels, which weakens their generalization ability on other affect tasks. Yu et al. (2017) instead uses emotion lexicons to tune the vector space, which gives them better results. Nevertheless, this method requires human-labeled lexicons and cannot scale to large amounts of data. Felbo et al. (2017) achieves good results on affect tasks by training a two-layer bidirectional Long Short-Term Memory (bi-LSTM) model, named DeepMoji, to predict emoji of the input document using a huge dataset of 1.2 billions of tweets. However, collecting billions of tweets is expensive and time consuming for researchers. Furthermore, most works in sentiment and emotion analysis have focused solely on a"
W18-6243,D17-1115,0,0.0312324,"017). Hashimoto et al. (2017) jointly learns POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment by considering linguistic hierarchy and achieves state-of-the-results on five datasets. For sentiment analysis, Balikas et al. (2017) jointly trains ternary and fine-grained classification with a recurrent neural network and achieves new stateof-the-art results. 5 6 Conclusion and Future Work Related work For sentiment analysis, numerous classification models (Kalchbrenner et al.; Iyyer et al., 2015; Dou, 2017) have been explored. Multi-modal sentiment analysis (Zadeh et al., 2017; Poria et al., 2017) extends text-based model to the combination of visual, acoustic and language, which achieves better results than the single modality. Various methods are developed for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et al. (2017) to train a large model on a huge emoji tweet corpus, which"
