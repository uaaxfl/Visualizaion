2020.acl-srw.41,P14-5010,0,0.00326272,"eeking questions, and to examine the Subjectivity Features: Rhetorical questions role of prior context for this task. often express an opinion (e.g., criticize), agreeFirst, we applied the CMU Twokenizer (Gimment/disagreement, etc. So we hypothesized that pel et al., 2011), removed URLs and hashtags, and recognizing subjective language may be a helpful replaced acronyms with their corresponding full 4 words or phrases using a Twitter acronym list . clue for identifying rhetorical questions. We extracted 5 features associated with subjectivity: (1) Next, we applied the Stanford CoreNLP parser (Manning et al., 2014) to obtain lemmas and part- the number of elongated words (e.g., “looooove”), (2) the number of entirely upper case words (e.g., of-speech tags. For the embedding features, we used GloVe vectors (Pennington et al., 2014) pre- “YAY”), (3) the number of exclamation marks, (4) the number of strongly subjective words found in the trained on 2B tweets. We experimented with both MPQA lexicon (Wilson et al., 2005), and (5) the 25 and 100 dimensional vectors, and show the best number of weakly subjective words in the MPQA results in Section 5. We then extracted three sets lexicon. of features: word fe"
2020.acl-srw.41,W17-5537,1,0.81735,"Missing"
2020.acl-srw.41,W16-3604,1,0.900699,"Missing"
2020.acl-srw.41,D14-1162,0,0.0835013,"greement, etc. So we hypothesized that pel et al., 2011), removed URLs and hashtags, and recognizing subjective language may be a helpful replaced acronyms with their corresponding full 4 words or phrases using a Twitter acronym list . clue for identifying rhetorical questions. We extracted 5 features associated with subjectivity: (1) Next, we applied the Stanford CoreNLP parser (Manning et al., 2014) to obtain lemmas and part- the number of elongated words (e.g., “looooove”), (2) the number of entirely upper case words (e.g., of-speech tags. For the embedding features, we used GloVe vectors (Pennington et al., 2014) pre- “YAY”), (3) the number of exclamation marks, (4) the number of strongly subjective words found in the trained on 2B tweets. We experimented with both MPQA lexicon (Wilson et al., 2005), and (5) the 25 and 100 dimensional vectors, and show the best number of weakly subjective words in the MPQA results in Section 5. We then extracted three sets lexicon. of features: word features, question features, and topic features. 4.3 Topic Features Word Features We explored both unigrams and embedding vectors to capture the meaning of the words in a tweet. Unigrams: Each word is a feature with a TFID"
2020.acl-srw.41,H05-1044,0,0.0649161,"or phrases using a Twitter acronym list . clue for identifying rhetorical questions. We extracted 5 features associated with subjectivity: (1) Next, we applied the Stanford CoreNLP parser (Manning et al., 2014) to obtain lemmas and part- the number of elongated words (e.g., “looooove”), (2) the number of entirely upper case words (e.g., of-speech tags. For the embedding features, we used GloVe vectors (Pennington et al., 2014) pre- “YAY”), (3) the number of exclamation marks, (4) the number of strongly subjective words found in the trained on 2B tweets. We experimented with both MPQA lexicon (Wilson et al., 2005), and (5) the 25 and 100 dimensional vectors, and show the best number of weakly subjective words in the MPQA results in Section 5. We then extracted three sets lexicon. of features: word features, question features, and topic features. 4.3 Topic Features Word Features We explored both unigrams and embedding vectors to capture the meaning of the words in a tweet. Unigrams: Each word is a feature with a TFIDF value. We only include unigrams that occur ≥ 3 times in the training set. Embedding (Embed): We create an embedding vector for a tweet by averaging the embedding vectors for all words in t"
2020.emnlp-main.452,D18-1066,0,0.0230126,"et al., 2016), which infer connotative polarities for a verb’s arguments from the writer’s and entity’s perspective. These efforts associated polarity with individual verbs, not event phrases. Saito et al. (2019) used discourse relations to propagate affective polarity from seeds using a Japanese web corpus. They extracted events that co-occur with seeds in a large corpus, then used discourse relations as constraints in the learning process. Another line of related work is Emotion Cause Extraction, which links emotion expressions to the events that caused the emotion (Gui et al., 2016, 2017; Chen et al., 2018; Li et al., 2018; Xia and Ding, 2019). This research uses datasets created from Chinese news and microblogs that contain an explicitly mentioned emotion. This work assigns polarity to events in the context of a specific text passage. In contrast, our work aims to identify the prior affective polarity of an event, irrespective of context. Consequently, our classifier can be used to predict the affective polarity of events in contexts that do not contain any explicit emotion or sentiment indicators. Our research is most closely related to the work by (Ding and Riloff, 2016, 2018), which identif"
2020.emnlp-main.452,D14-1125,0,0.0195338,"cts the affective polarity for unlabeled events using both the classifier’s prediction for the event phrase as well as the associated sentiment expressions. We show that our discourse-enhanced self-training method improves both recall and precision for affective event classification. 2 Related Work Several lines of research have focused on the problem of recognizing events that have implicit affective states. Research on narrative understanding used bootstrapped learning to identify patient polarity verbs, which impart affective polarity to their patients (Goyal et al., 2010, 2013). Vu et al. (2014) extracted “emotion-provoking events” using the seed pattern “I am &lt; EMOTION &gt; that &lt; EVENT &gt;, pattern expansion, and clustering. Reed et al. (2017) learned lexico-syntactic patterns associated with first-person affect to improve affective sentence classification alongside supervised learners. Li et al. (2014) extracted “major life events” from Twitter by clustering tweets that occurred with speech act words, such as “congratulations” or “condolences”. But their work did not assign affective polarity to events, and focused only on major life events that prompt expressive speech acts. Our work"
2020.emnlp-main.452,E14-1040,0,0.0155355,"ts, and focused only on major life events that prompt expressive speech acts. Our work has a broader scope, aiming to recognize everyday events as well (e.g., being hungry is negative, and seeing a rainbow is positive). Work in opinion analysis created a +/EffectWordNet (Choi and Wiebe, 2014) to recognize the effects of events on entities, although the effects are not necessarily “affective” because the entities need not be animate (e.g., baking a cake has a positive effect on the cake because it is created). Subsequent work developed implicature rules to use +/- effects for opinion analysis (Deng and Wiebe, 2014, 2015). There has also been work on recognizing the connotation of words and senses (Kang et al., 2014) and connotation frames (Rashkin et al., 2016), which infer connotative polarities for a verb’s arguments from the writer’s and entity’s perspective. These efforts associated polarity with individual verbs, not event phrases. Saito et al. (2019) used discourse relations to propagate affective polarity from seeds using a Japanese web corpus. They extracted events that co-occur with seeds in a large corpus, then used discourse relations as constraints in the learning process. Another line of r"
2020.emnlp-main.452,D15-1018,0,0.223017,"Missing"
2020.emnlp-main.452,N19-1423,0,0.014919,"large size, the AEKB cannot recognize many affective events for two reasons: (1) the AEKB’s precision is not perfect, so some positive and negative events are labeled as neutral, and (2) many affective events are not present in the knowledge base. Our research addresses these limitations by exploring whether classification models can achieve better coverage and accuracy by generalizing across events. 3.2 A BERT-based Affective Event Classifier Our goal is to design a classifier that can label an event tuple with affective polarity. Representations produced by the transformer-based BERT model (Devlin et al., 2019) have achieved state-ofthe-art performance across a variety of NLP tasks, so we used the pre-trained BERTBASE as the basis for our classifier and performed fine-tuning during the training. The input is the sequence of tokens that comprise an event tuple. For example, hI, ride, bike, -i is converted into the sequence “I ride bike”. We use the uncased version of the BERT base model as our encoder. We use the 768-dimension output embedding of the special token [CLS], and pass the output vector of the special token [CLS] to a fully connected layer with softmax to produce a probability distribution"
2020.emnlp-main.452,D10-1008,1,0.833761,"Missing"
2020.emnlp-main.452,D17-1167,0,0.0664512,"Missing"
2020.emnlp-main.452,D16-1170,0,0.0266072,"notation frames (Rashkin et al., 2016), which infer connotative polarities for a verb’s arguments from the writer’s and entity’s perspective. These efforts associated polarity with individual verbs, not event phrases. Saito et al. (2019) used discourse relations to propagate affective polarity from seeds using a Japanese web corpus. They extracted events that co-occur with seeds in a large corpus, then used discourse relations as constraints in the learning process. Another line of related work is Emotion Cause Extraction, which links emotion expressions to the events that caused the emotion (Gui et al., 2016, 2017; Chen et al., 2018; Li et al., 2018; Xia and Ding, 2019). This research uses datasets created from Chinese news and microblogs that contain an explicitly mentioned emotion. This work assigns polarity to events in the context of a specific text passage. In contrast, our work aims to identify the prior affective polarity of an event, irrespective of context. Consequently, our classifier can be used to predict the affective polarity of events in contexts that do not contain any explicit emotion or sentiment indicators. Our research is most closely related to the work by (Ding and Riloff, 2"
2020.emnlp-main.452,P14-1145,0,0.0269288,", aiming to recognize everyday events as well (e.g., being hungry is negative, and seeing a rainbow is positive). Work in opinion analysis created a +/EffectWordNet (Choi and Wiebe, 2014) to recognize the effects of events on entities, although the effects are not necessarily “affective” because the entities need not be animate (e.g., baking a cake has a positive effect on the cake because it is created). Subsequent work developed implicature rules to use +/- effects for opinion analysis (Deng and Wiebe, 2014, 2015). There has also been work on recognizing the connotation of words and senses (Kang et al., 2014) and connotation frames (Rashkin et al., 2016), which infer connotative polarities for a verb’s arguments from the writer’s and entity’s perspective. These efforts associated polarity with individual verbs, not event phrases. Saito et al. (2019) used discourse relations to propagate affective polarity from seeds using a Japanese web corpus. They extracted events that co-occur with seeds in a large corpus, then used discourse relations as constraints in the learning process. Another line of related work is Emotion Cause Extraction, which links emotion expressions to the events that caused the e"
2020.emnlp-main.452,N04-4009,0,0.23383,"Missing"
2020.emnlp-main.452,D14-1214,0,0.0259603,"everal lines of research have focused on the problem of recognizing events that have implicit affective states. Research on narrative understanding used bootstrapped learning to identify patient polarity verbs, which impart affective polarity to their patients (Goyal et al., 2010, 2013). Vu et al. (2014) extracted “emotion-provoking events” using the seed pattern “I am &lt; EMOTION &gt; that &lt; EVENT &gt;, pattern expansion, and clustering. Reed et al. (2017) learned lexico-syntactic patterns associated with first-person affect to improve affective sentence classification alongside supervised learners. Li et al. (2014) extracted “major life events” from Twitter by clustering tweets that occurred with speech act words, such as “congratulations” or “condolences”. But their work did not assign affective polarity to events, and focused only on major life events that prompt expressive speech acts. Our work has a broader scope, aiming to recognize everyday events as well (e.g., being hungry is negative, and seeing a rainbow is positive). Work in opinion analysis created a +/EffectWordNet (Choi and Wiebe, 2014) to recognize the effects of events on entities, although the effects are not necessarily “affective” bec"
2020.emnlp-main.452,D18-1506,0,0.0281165,"ch infer connotative polarities for a verb’s arguments from the writer’s and entity’s perspective. These efforts associated polarity with individual verbs, not event phrases. Saito et al. (2019) used discourse relations to propagate affective polarity from seeds using a Japanese web corpus. They extracted events that co-occur with seeds in a large corpus, then used discourse relations as constraints in the learning process. Another line of related work is Emotion Cause Extraction, which links emotion expressions to the events that caused the emotion (Gui et al., 2016, 2017; Chen et al., 2018; Li et al., 2018; Xia and Ding, 2019). This research uses datasets created from Chinese news and microblogs that contain an explicitly mentioned emotion. This work assigns polarity to events in the context of a specific text passage. In contrast, our work aims to identify the prior affective polarity of an event, irrespective of context. Consequently, our classifier can be used to predict the affective polarity of events in contexts that do not contain any explicit emotion or sentiment indicators. Our research is most closely related to the work by (Ding and Riloff, 2016, 2018), which identifies stereotypical"
2020.emnlp-main.452,N06-1020,0,0.0843913,") produced by (Ding and Riloff, 2018) contains over half a million event phrases coupled with polarity labels. These events were extracted from nearly 1.4 million personal blog posts in the ICWSM 2009 and 2011 Spinn3r datasets1 . The polarity labels were generated automatically using a weakly supervised method. Their approach optimizes for semantic consistency over a graph of event nodes that are linked by edges capturing three types of semantic relations. Our discourse-enhanced self-training algorithm adds a new twist to traditional self-training methods (Mihalcea, 2004; Kehler et al., 2004; McClosky et al., 2006). The approach is also reminiscent of co-training (Blum and Mitchell, 1998), which trains two classifiers based on independent views of the data. However in co-training, each classifier must be able to make reliable predictions on its own. We do not expect the coreferent sentiment expressions used by our approach to be sufficient by themselves because they are quite noisy (e.g., due to imperfect coreference, imperfect sentiment 5609 1 http://www.icwsm.org/data/ Method Blogs Twitter-found Twitter-all F1 71.4 65.2 50.8 POS Pre Rec 75.7 55.1 72.2 40.6 72.2 26.2 NEG Pre Rec 70.4 63.3 78.7 60.8 78."
2020.emnlp-main.452,W04-2405,0,0.153902,"Affective Event Knowledge Base (AEKB) produced by (Ding and Riloff, 2018) contains over half a million event phrases coupled with polarity labels. These events were extracted from nearly 1.4 million personal blog posts in the ICWSM 2009 and 2011 Spinn3r datasets1 . The polarity labels were generated automatically using a weakly supervised method. Their approach optimizes for semantic consistency over a graph of event nodes that are linked by edges capturing three types of semantic relations. Our discourse-enhanced self-training algorithm adds a new twist to traditional self-training methods (Mihalcea, 2004; Kehler et al., 2004; McClosky et al., 2006). The approach is also reminiscent of co-training (Blum and Mitchell, 1998), which trains two classifiers based on independent views of the data. However in co-training, each classifier must be able to make reliable predictions on its own. We do not expect the coreferent sentiment expressions used by our approach to be sufficient by themselves because they are quite noisy (e.g., due to imperfect coreference, imperfect sentiment 5609 1 http://www.icwsm.org/data/ Method Blogs Twitter-found Twitter-all F1 71.4 65.2 50.8 POS Pre Rec 75.7 55.1 72.2 40.6"
2020.emnlp-main.452,N18-1202,0,0.0166451,"ce “I ride bike”. We use the uncased version of the BERT base model as our encoder. We use the 768-dimension output embedding of the special token [CLS], and pass the output vector of the special token [CLS] to a fully connected layer with softmax to produce a probability distribution over the three polarity classes. Each input event is then assigned the polarity with the highest probability value. We will refer to this model as Aff-BERT. 3.3 Experimental Results for Blogs Data Baselines We developed two baselines to compare with Aff-BERT. The first model is a 1-layer LSTM. We first use ELMo (Peters et al., 2018) to encode an event sequence and feed the last layer of ELMo’s outputs into the LSTM. The LSTM outputs a polarity distribution for the event. The 5610 Method AEKB Aff-BERT(AEKB) ELMo+Linear(Gold) ELMo+LSTM(Gold) Aff-BERT(Gold) F1 71.4 73.6 62.3 70.5 77.4 POS Pre Rec 75.7 55.1 73.2 56.6 56.0 53.7 71.4 60.8 71.7 66.2 NEG Pre Rec 70.4 63.3 75.6 69.5 56.2 51.3 70.8 57.3 78.2 77.2 NEU Pre Rec 79.3 88.5 80.9 88.5 78.2 81.4 81.3 88.5 85.0 87.4 produced only a small improvement, but we developed a new discourse-enhanced self-training algorithm that achieved bigger performance gains. In the next sectio"
2020.emnlp-main.452,D19-1581,0,0.0206284,"are not necessarily “affective” because the entities need not be animate (e.g., baking a cake has a positive effect on the cake because it is created). Subsequent work developed implicature rules to use +/- effects for opinion analysis (Deng and Wiebe, 2014, 2015). There has also been work on recognizing the connotation of words and senses (Kang et al., 2014) and connotation frames (Rashkin et al., 2016), which infer connotative polarities for a verb’s arguments from the writer’s and entity’s perspective. These efforts associated polarity with individual verbs, not event phrases. Saito et al. (2019) used discourse relations to propagate affective polarity from seeds using a Japanese web corpus. They extracted events that co-occur with seeds in a large corpus, then used discourse relations as constraints in the learning process. Another line of related work is Emotion Cause Extraction, which links emotion expressions to the events that caused the emotion (Gui et al., 2016, 2017; Chen et al., 2018; Li et al., 2018; Xia and Ding, 2019). This research uses datasets created from Chinese news and microblogs that contain an explicitly mentioned emotion. This work assigns polarity to events in t"
2020.emnlp-main.452,E14-4025,0,0.0217691,"then predicts the affective polarity for unlabeled events using both the classifier’s prediction for the event phrase as well as the associated sentiment expressions. We show that our discourse-enhanced self-training method improves both recall and precision for affective event classification. 2 Related Work Several lines of research have focused on the problem of recognizing events that have implicit affective states. Research on narrative understanding used bootstrapped learning to identify patient polarity verbs, which impart affective polarity to their patients (Goyal et al., 2010, 2013). Vu et al. (2014) extracted “emotion-provoking events” using the seed pattern “I am &lt; EMOTION &gt; that &lt; EVENT &gt;, pattern expansion, and clustering. Reed et al. (2017) learned lexico-syntactic patterns associated with first-person affect to improve affective sentence classification alongside supervised learners. Li et al. (2014) extracted “major life events” from Twitter by clustering tweets that occurred with speech act words, such as “congratulations” or “condolences”. But their work did not assign affective polarity to events, and focused only on major life events that prompt expressive speech acts. Our work"
2020.emnlp-main.452,H05-1044,0,0.230069,"Missing"
2020.emnlp-main.452,P19-1096,0,0.0118593,"ive polarities for a verb’s arguments from the writer’s and entity’s perspective. These efforts associated polarity with individual verbs, not event phrases. Saito et al. (2019) used discourse relations to propagate affective polarity from seeds using a Japanese web corpus. They extracted events that co-occur with seeds in a large corpus, then used discourse relations as constraints in the learning process. Another line of related work is Emotion Cause Extraction, which links emotion expressions to the events that caused the emotion (Gui et al., 2016, 2017; Chen et al., 2018; Li et al., 2018; Xia and Ding, 2019). This research uses datasets created from Chinese news and microblogs that contain an explicitly mentioned emotion. This work assigns polarity to events in the context of a specific text passage. In contrast, our work aims to identify the prior affective polarity of an event, irrespective of context. Consequently, our classifier can be used to predict the affective polarity of events in contexts that do not contain any explicit emotion or sentiment indicators. Our research is most closely related to the work by (Ding and Riloff, 2016, 2018), which identifies stereotypically affective events a"
2020.emnlp-main.452,P16-1030,0,0.0948931,"negative polarity with respect to an implicit affective state. Research has shown that recognizing affective events is important for a variety of natural language processing tasks, including narrative text comprehension and summarization (Lehnert, 1981; Goyal et al., 2013), dialogue systems (Andr´e et al., 2004), response generation (Ritter et al., 2011), and sarcasm detection (Riloff et al., 2013). Much of the prior work on recognizing affective events has focused on producing lexical resources of verbs or event phrases with corresponding affective polarity values (Goyal et al., 2010, 2013; Rashkin et al., 2016; Ding and Riloff, 2016, 2018). These resources reflect substantial progress toward recognizing affective events in text, but their coverage is limited by their fixed content. We hypothesized that deep learning architectures that encode rich meaning representations could lead to a more effective approach for identifying affective events. Specifically, neural classification models have the capacity to generalize across lexically and syntactically different phrases that are semantically similar, and similar events are usually associated with the same affective polarity. To explore this approach,"
2020.emnlp-main.452,P17-2022,0,0.0175562,"ntiment expressions. We show that our discourse-enhanced self-training method improves both recall and precision for affective event classification. 2 Related Work Several lines of research have focused on the problem of recognizing events that have implicit affective states. Research on narrative understanding used bootstrapped learning to identify patient polarity verbs, which impart affective polarity to their patients (Goyal et al., 2010, 2013). Vu et al. (2014) extracted “emotion-provoking events” using the seed pattern “I am &lt; EMOTION &gt; that &lt; EVENT &gt;, pattern expansion, and clustering. Reed et al. (2017) learned lexico-syntactic patterns associated with first-person affect to improve affective sentence classification alongside supervised learners. Li et al. (2014) extracted “major life events” from Twitter by clustering tweets that occurred with speech act words, such as “congratulations” or “condolences”. But their work did not assign affective polarity to events, and focused only on major life events that prompt expressive speech acts. Our work has a broader scope, aiming to recognize everyday events as well (e.g., being hungry is negative, and seeing a rainbow is positive). Work in opinion"
2020.emnlp-main.452,D13-1066,1,0.843817,"Missing"
2020.emnlp-main.452,D11-1054,0,0.0333193,"e about events and how they impact people is sufficient for humans to infer the affective state of someone who experiences such an event, even if that person does not explicitly express an emotion. Consequently, we will refer to these events as having positive or negative polarity with respect to an implicit affective state. Research has shown that recognizing affective events is important for a variety of natural language processing tasks, including narrative text comprehension and summarization (Lehnert, 1981; Goyal et al., 2013), dialogue systems (Andr´e et al., 2004), response generation (Ritter et al., 2011), and sarcasm detection (Riloff et al., 2013). Much of the prior work on recognizing affective events has focused on producing lexical resources of verbs or event phrases with corresponding affective polarity values (Goyal et al., 2010, 2013; Rashkin et al., 2016; Ding and Riloff, 2016, 2018). These resources reflect substantial progress toward recognizing affective events in text, but their coverage is limited by their fixed content. We hypothesized that deep learning architectures that encode rich meaning representations could lead to a more effective approach for identifying affective event"
2020.emnlp-main.452,S17-2088,0,0.0773038,"essions. We show that our discourse-enhanced self-training method improves both recall and precision for affective event classification. 2 Related Work Several lines of research have focused on the problem of recognizing events that have implicit affective states. Research on narrative understanding used bootstrapped learning to identify patient polarity verbs, which impart affective polarity to their patients (Goyal et al., 2010, 2013). Vu et al. (2014) extracted “emotion-provoking events” using the seed pattern “I am &lt; EMOTION &gt; that &lt; EVENT &gt;, pattern expansion, and clustering. Reed et al. (2017) learned lexico-syntactic patterns associated with first-person affect to improve affective sentence classification alongside supervised learners. Li et al. (2014) extracted “major life events” from Twitter by clustering tweets that occurred with speech act words, such as “congratulations” or “condolences”. But their work did not assign affective polarity to events, and focused only on major life events that prompt expressive speech acts. Our work has a broader scope, aiming to recognize everyday events as well (e.g., being hungry is negative, and seeing a rainbow is positive). Work in opinion"
2020.figlang-1.20,P13-1174,0,0.155535,"erlying conceptual metaphor. For example, people are likely to use the euphemism “parted ways” to describe ending a relationship in the context of the conceptual metaphor A RELATIONSHIP IS A JOURNEY, but more likely to use the euphemism “cut their losses” in the context of the metaphor A RELATIONSHIP IS AN INVESTMENT . Our research focuses on the relationship between x-phemisms and sentiment analysis. We take advantage of several existing sentiment resources, including the NRC EmoLex, VAD, and Affective Intensity Lexicons (Mohammad and Turney, 2013; Mohammad, 2018a,b) and Connotation WordNet (Feng et al., 2013; Kang et al., 2014). We also re-implementated the NRC-Canada sentiment classifier (Mohammad et al., 2013) to use in our work. Euphemisms and dysphemisms have been studied Allan (2009) examined the connotation of color in linguistics and related disciplines (e.g., (Allan terms according to how often they appear in dysand Burridge, 1991; Pfaff et al., 1997; Rawson, phemistic, euphemistic, or neutral contexts. For 2003; Allan, 2009; Rababah, 2014)), but they have instance, “blue” is often used as a euphemism for received little attention in the NLP community. “sad”, while “yellow” can be dysphem"
2020.figlang-1.20,N13-1092,0,0.0886893,"Missing"
2020.figlang-1.20,D15-1255,0,0.0224076,"Euphemisms are related to politeness, which plays a role in applications involving dialogue and social interactions (e.g., (Danescu-Niculescu-Mizil et al., 2013)). Dysphemisms can include pejorative and offensive language, which relates to cyberbullying (Xu et al., 2012; Van Hee et al., 2015), hate speech (Magu and Luo, 2014), and abusive language (Park et al., 2018; Wiegand et al., 2018). Recognizing euphemisms and dysphemisms for controversial topics could be valuable for stance detection and argumentation in political discourse or debates (Somasundaran and Wiebe, 2010; Walker et al., 2012; Habernal and Gurevych, 2015). In medicine, researchers found that medical professionals use xphemisms when talking to patients about serious conditions, and have emphasized the importance of preserving x-phemisms across translations when treating non-English speakers (Rababah, 2014). An area of NLP that relates to x-phemisms is sentiment analysis, although the relationship is complex. A key feature of x-phemisms is that their directionality (euphemism vs. dysphemism) is relative to an underlying topic, which itself often has affective polarity. X-phemisms are usually associated with negative topics that are culturally di"
2020.figlang-1.20,P14-1145,0,0.134454,"metaphor. For example, people are likely to use the euphemism “parted ways” to describe ending a relationship in the context of the conceptual metaphor A RELATIONSHIP IS A JOURNEY, but more likely to use the euphemism “cut their losses” in the context of the metaphor A RELATIONSHIP IS AN INVESTMENT . Our research focuses on the relationship between x-phemisms and sentiment analysis. We take advantage of several existing sentiment resources, including the NRC EmoLex, VAD, and Affective Intensity Lexicons (Mohammad and Turney, 2013; Mohammad, 2018a,b) and Connotation WordNet (Feng et al., 2013; Kang et al., 2014). We also re-implementated the NRC-Canada sentiment classifier (Mohammad et al., 2013) to use in our work. Euphemisms and dysphemisms have been studied Allan (2009) examined the connotation of color in linguistics and related disciplines (e.g., (Allan terms according to how often they appear in dysand Burridge, 1991; Pfaff et al., 1997; Rawson, phemistic, euphemistic, or neutral contexts. For 2003; Allan, 2009; Rababah, 2014)), but they have instance, “blue” is often used as a euphemism for received little attention in the NLP community. “sad”, while “yellow” can be dysphemistically used 1 to"
2020.figlang-1.20,D16-1216,0,0.0223567,"o (2014) recognized code words in “euphemistic hate speech” by measuring cosine distance between word embeddings. But their code words conceal references to hate speech rather than soften them (e.g., the code word “skypes” covertly referred to Jews), which is different from the traditional definition of euphemisms that is addressed in our work. The NLP community has explored several linguistic phenomena related to x-phemisms, such as metaphor (e.g., (Shutova, 2010; Wallington et al., 2011; Shutova et al., 2010; Kesarwani et al., 2017)), politeness (e.g., (Danescu-Niculescu-Mizil et al., 2013; Aubakirova and Bansal, 2016)), and formality (e.g., (Pavlick and Tetreault, 2016)). Pfaff et al. (1997) found that people comprehend metaphorical euphemisms or dysphemisms more quickly when they share the same underlying conceptual metaphor. For example, people are likely to use the euphemism “parted ways” to describe ending a relationship in the context of the conceptual metaphor A RELATIONSHIP IS A JOURNEY, but more likely to use the euphemism “cut their losses” in the context of the metaphor A RELATIONSHIP IS AN INVESTMENT . Our research focuses on the relationship between x-phemisms and sentiment analysis. We take ad"
2020.figlang-1.20,P18-1017,0,0.0807914,"ms more quickly when they share the same underlying conceptual metaphor. For example, people are likely to use the euphemism “parted ways” to describe ending a relationship in the context of the conceptual metaphor A RELATIONSHIP IS A JOURNEY, but more likely to use the euphemism “cut their losses” in the context of the metaphor A RELATIONSHIP IS AN INVESTMENT . Our research focuses on the relationship between x-phemisms and sentiment analysis. We take advantage of several existing sentiment resources, including the NRC EmoLex, VAD, and Affective Intensity Lexicons (Mohammad and Turney, 2013; Mohammad, 2018a,b) and Connotation WordNet (Feng et al., 2013; Kang et al., 2014). We also re-implementated the NRC-Canada sentiment classifier (Mohammad et al., 2013) to use in our work. Euphemisms and dysphemisms have been studied Allan (2009) examined the connotation of color in linguistics and related disciplines (e.g., (Allan terms according to how often they appear in dysand Burridge, 1991; Pfaff et al., 1997; Rawson, phemistic, euphemistic, or neutral contexts. For 2003; Allan, 2009; Rababah, 2014)), but they have instance, “blue” is often used as a euphemism for received little attention in the NLP"
2020.figlang-1.20,L18-1027,0,0.482666,"ms more quickly when they share the same underlying conceptual metaphor. For example, people are likely to use the euphemism “parted ways” to describe ending a relationship in the context of the conceptual metaphor A RELATIONSHIP IS A JOURNEY, but more likely to use the euphemism “cut their losses” in the context of the metaphor A RELATIONSHIP IS AN INVESTMENT . Our research focuses on the relationship between x-phemisms and sentiment analysis. We take advantage of several existing sentiment resources, including the NRC EmoLex, VAD, and Affective Intensity Lexicons (Mohammad and Turney, 2013; Mohammad, 2018a,b) and Connotation WordNet (Feng et al., 2013; Kang et al., 2014). We also re-implementated the NRC-Canada sentiment classifier (Mohammad et al., 2013) to use in our work. Euphemisms and dysphemisms have been studied Allan (2009) examined the connotation of color in linguistics and related disciplines (e.g., (Allan terms according to how often they appear in dysand Burridge, 1991; Pfaff et al., 1997; Rawson, phemistic, euphemistic, or neutral contexts. For 2003; Allan, 2009; Rababah, 2014)), but they have instance, “blue” is often used as a euphemism for received little attention in the NLP"
2020.figlang-1.20,S13-2053,0,0.0359443,"cribe ending a relationship in the context of the conceptual metaphor A RELATIONSHIP IS A JOURNEY, but more likely to use the euphemism “cut their losses” in the context of the metaphor A RELATIONSHIP IS AN INVESTMENT . Our research focuses on the relationship between x-phemisms and sentiment analysis. We take advantage of several existing sentiment resources, including the NRC EmoLex, VAD, and Affective Intensity Lexicons (Mohammad and Turney, 2013; Mohammad, 2018a,b) and Connotation WordNet (Feng et al., 2013; Kang et al., 2014). We also re-implementated the NRC-Canada sentiment classifier (Mohammad et al., 2013) to use in our work. Euphemisms and dysphemisms have been studied Allan (2009) examined the connotation of color in linguistics and related disciplines (e.g., (Allan terms according to how often they appear in dysand Burridge, 1991; Pfaff et al., 1997; Rawson, phemistic, euphemistic, or neutral contexts. For 2003; Allan, 2009; Rababah, 2014)), but they have instance, “blue” is often used as a euphemism for received little attention in the NLP community. “sad”, while “yellow” can be dysphemistically used 1 to mean “cowardly”. Our paper takes the reverse Direct (“straight-talking”) references to"
2020.figlang-1.20,D18-1302,0,0.0262536,"variety of topics. Following terminology from linguistics (e.g., (Allan, 2009; Rababah, 2014)), we use the term x-phemism to refer to the general phenomenon of euphemisms and dysphemisms. Recognizing xphemisms could be valuable for many NLP tasks. Euphemisms are related to politeness, which plays a role in applications involving dialogue and social interactions (e.g., (Danescu-Niculescu-Mizil et al., 2013)). Dysphemisms can include pejorative and offensive language, which relates to cyberbullying (Xu et al., 2012; Van Hee et al., 2015), hate speech (Magu and Luo, 2014), and abusive language (Park et al., 2018; Wiegand et al., 2018). Recognizing euphemisms and dysphemisms for controversial topics could be valuable for stance detection and argumentation in political discourse or debates (Somasundaran and Wiebe, 2010; Walker et al., 2012; Habernal and Gurevych, 2015). In medicine, researchers found that medical professionals use xphemisms when talking to patients about serious conditions, and have emphasized the importance of preserving x-phemisms across translations when treating non-English speakers (Rababah, 2014). An area of NLP that relates to x-phemisms is sentiment analysis, although the relat"
2020.figlang-1.20,N12-1072,0,0.0416377,"Missing"
2020.figlang-1.20,Q16-1005,0,0.0160056,"peech” by measuring cosine distance between word embeddings. But their code words conceal references to hate speech rather than soften them (e.g., the code word “skypes” covertly referred to Jews), which is different from the traditional definition of euphemisms that is addressed in our work. The NLP community has explored several linguistic phenomena related to x-phemisms, such as metaphor (e.g., (Shutova, 2010; Wallington et al., 2011; Shutova et al., 2010; Kesarwani et al., 2017)), politeness (e.g., (Danescu-Niculescu-Mizil et al., 2013; Aubakirova and Bansal, 2016)), and formality (e.g., (Pavlick and Tetreault, 2016)). Pfaff et al. (1997) found that people comprehend metaphorical euphemisms or dysphemisms more quickly when they share the same underlying conceptual metaphor. For example, people are likely to use the euphemism “parted ways” to describe ending a relationship in the context of the conceptual metaphor A RELATIONSHIP IS A JOURNEY, but more likely to use the euphemism “cut their losses” in the context of the metaphor A RELATIONSHIP IS AN INVESTMENT . Our research focuses on the relationship between x-phemisms and sentiment analysis. We take advantage of several existing sentiment resources, incl"
2020.figlang-1.20,S12-1028,1,0.684123,"e extort loot mug pilfer plunder rob steal swindle Table 2: Seed Phrases per Topic (NPs). To collect seed terms, we identified common phrases for each topic that had high frequency in the Gigaword corpus. The seed lists are shown in Table 2. We included both active and passive voice verb phrase forms for the verbs shown in Table 2, except we excluded resign in passive voice because “was resigned to” is a common expression with a different meaning. Most previous applications of Basilisk have used lexico-syntactic patterns to represent the contexts around seed terms (e.g., (Riloff et al., 2003; Qadir and Riloff, 2012)). For example, a pattern may indicate that a phrase occurs as the syntactic subject or direct object of a specific verb. So we used the dependency relations produced by the SpaCy parser (https://spacy.io/)3 for contextual patterns. For generality, we used word lemmas both for the learned phrases and the patterns. 4.1 Representing Contextual Patterns and Verb Phrases We defined a contextual pattern as a dependency relation linked to/from a seed term, coupled with the head of the governing/dependent phrase. For example, consider the sentence “The lie spread quickly”. The contextual pattern for"
2020.figlang-1.20,W03-0404,1,0.594269,"g VPs defraud embezzle extort loot mug pilfer plunder rob steal swindle Table 2: Seed Phrases per Topic (NPs). To collect seed terms, we identified common phrases for each topic that had high frequency in the Gigaword corpus. The seed lists are shown in Table 2. We included both active and passive voice verb phrase forms for the verbs shown in Table 2, except we excluded resign in passive voice because “was resigned to” is a common expression with a different meaning. Most previous applications of Basilisk have used lexico-syntactic patterns to represent the contexts around seed terms (e.g., (Riloff et al., 2003; Qadir and Riloff, 2012)). For example, a pattern may indicate that a phrase occurs as the syntactic subject or direct object of a specific verb. So we used the dependency relations produced by the SpaCy parser (https://spacy.io/)3 for contextual patterns. For generality, we used word lemmas both for the learned phrases and the patterns. 4.1 Representing Contextual Patterns and Verb Phrases We defined a contextual pattern as a dependency relation linked to/from a seed term, coupled with the head of the governing/dependent phrase. For example, consider the sentence “The lie spread quickly”. Th"
2020.figlang-1.20,N10-1147,0,0.0403588,"olarity can be useful for identifying euphemistic and dysphemistic phrases, although this problem remains challenging. 2 Related Work Magu and Luo (2014) recognized code words in “euphemistic hate speech” by measuring cosine distance between word embeddings. But their code words conceal references to hate speech rather than soften them (e.g., the code word “skypes” covertly referred to Jews), which is different from the traditional definition of euphemisms that is addressed in our work. The NLP community has explored several linguistic phenomena related to x-phemisms, such as metaphor (e.g., (Shutova, 2010; Wallington et al., 2011; Shutova et al., 2010; Kesarwani et al., 2017)), politeness (e.g., (Danescu-Niculescu-Mizil et al., 2013; Aubakirova and Bansal, 2016)), and formality (e.g., (Pavlick and Tetreault, 2016)). Pfaff et al. (1997) found that people comprehend metaphorical euphemisms or dysphemisms more quickly when they share the same underlying conceptual metaphor. For example, people are likely to use the euphemism “parted ways” to describe ending a relationship in the context of the conceptual metaphor A RELATIONSHIP IS A JOURNEY, but more likely to use the euphemism “cut their losses”"
2020.figlang-1.20,C10-1113,0,0.0422077,"phemistic and dysphemistic phrases, although this problem remains challenging. 2 Related Work Magu and Luo (2014) recognized code words in “euphemistic hate speech” by measuring cosine distance between word embeddings. But their code words conceal references to hate speech rather than soften them (e.g., the code word “skypes” covertly referred to Jews), which is different from the traditional definition of euphemisms that is addressed in our work. The NLP community has explored several linguistic phenomena related to x-phemisms, such as metaphor (e.g., (Shutova, 2010; Wallington et al., 2011; Shutova et al., 2010; Kesarwani et al., 2017)), politeness (e.g., (Danescu-Niculescu-Mizil et al., 2013; Aubakirova and Bansal, 2016)), and formality (e.g., (Pavlick and Tetreault, 2016)). Pfaff et al. (1997) found that people comprehend metaphorical euphemisms or dysphemisms more quickly when they share the same underlying conceptual metaphor. For example, people are likely to use the euphemism “parted ways” to describe ending a relationship in the context of the conceptual metaphor A RELATIONSHIP IS A JOURNEY, but more likely to use the euphemism “cut their losses” in the context of the metaphor A RELATIONSHIP"
2020.figlang-1.20,W10-0214,0,0.051735,"ng xphemisms could be valuable for many NLP tasks. Euphemisms are related to politeness, which plays a role in applications involving dialogue and social interactions (e.g., (Danescu-Niculescu-Mizil et al., 2013)). Dysphemisms can include pejorative and offensive language, which relates to cyberbullying (Xu et al., 2012; Van Hee et al., 2015), hate speech (Magu and Luo, 2014), and abusive language (Park et al., 2018; Wiegand et al., 2018). Recognizing euphemisms and dysphemisms for controversial topics could be valuable for stance detection and argumentation in political discourse or debates (Somasundaran and Wiebe, 2010; Walker et al., 2012; Habernal and Gurevych, 2015). In medicine, researchers found that medical professionals use xphemisms when talking to patients about serious conditions, and have emphasized the importance of preserving x-phemisms across translations when treating non-English speakers (Rababah, 2014). An area of NLP that relates to x-phemisms is sentiment analysis, although the relationship is complex. A key feature of x-phemisms is that their directionality (euphemism vs. dysphemism) is relative to an underlying topic, which itself often has affective polarity. X-phemisms are usually ass"
2020.figlang-1.20,W02-1028,1,0.73695,"chuck Table 1: Examples of Euphemisms and Dysphemisms vomiting is unpleasant no matter how gently it is referred to). This paper presents the first effort to identify euphemistic and dysphemistic language in text. Since affective polarity clearly plays a role in this phenomenon, our research explores whether sentiment analysis can be useful for recognizing x-phemisms. We deconstructed the problem into two subtasks. First, we identify phrases that refer to three sensitive topics: LYING, STEALING, and FIRING (job termination). We use a weakly supervised algorithm for semantic lexicon induction (Thelen and Riloff, 2002) to semi-automatically generate lists of near-synonym phrases for each topic. Second, we investigate two methods to classify phrases as euphemistic, dysphemistic, or neutral1 . (1) We use dictionary-based methods to explore the value of several types of information found in sentiment lexicons: affective polarity, connotation, intensity, arousal, and dominance. (2) We use contextual sentiment analysis to classify x-phemism phrases. We collect sentence contexts around instances of each candidate phrase in a large corpus, and assign each phrase to an x-phemism category based on the polarity of it"
2020.figlang-1.20,N18-1095,0,0.0651855,"Following terminology from linguistics (e.g., (Allan, 2009; Rababah, 2014)), we use the term x-phemism to refer to the general phenomenon of euphemisms and dysphemisms. Recognizing xphemisms could be valuable for many NLP tasks. Euphemisms are related to politeness, which plays a role in applications involving dialogue and social interactions (e.g., (Danescu-Niculescu-Mizil et al., 2013)). Dysphemisms can include pejorative and offensive language, which relates to cyberbullying (Xu et al., 2012; Van Hee et al., 2015), hate speech (Magu and Luo, 2014), and abusive language (Park et al., 2018; Wiegand et al., 2018). Recognizing euphemisms and dysphemisms for controversial topics could be valuable for stance detection and argumentation in political discourse or debates (Somasundaran and Wiebe, 2010; Walker et al., 2012; Habernal and Gurevych, 2015). In medicine, researchers found that medical professionals use xphemisms when talking to patients about serious conditions, and have emphasized the importance of preserving x-phemisms across translations when treating non-English speakers (Rababah, 2014). An area of NLP that relates to x-phemisms is sentiment analysis, although the relationship is complex. A k"
2020.figlang-1.20,N12-1084,0,0.034343,"under” are dysphemisms for death. Table 1 shows examples of euphemisms and dysphemisms across a variety of topics. Following terminology from linguistics (e.g., (Allan, 2009; Rababah, 2014)), we use the term x-phemism to refer to the general phenomenon of euphemisms and dysphemisms. Recognizing xphemisms could be valuable for many NLP tasks. Euphemisms are related to politeness, which plays a role in applications involving dialogue and social interactions (e.g., (Danescu-Niculescu-Mizil et al., 2013)). Dysphemisms can include pejorative and offensive language, which relates to cyberbullying (Xu et al., 2012; Van Hee et al., 2015), hate speech (Magu and Luo, 2014), and abusive language (Park et al., 2018; Wiegand et al., 2018). Recognizing euphemisms and dysphemisms for controversial topics could be valuable for stance detection and argumentation in political discourse or debates (Somasundaran and Wiebe, 2010; Walker et al., 2012; Habernal and Gurevych, 2015). In medicine, researchers found that medical professionals use xphemisms when talking to patients about serious conditions, and have emphasized the importance of preserving x-phemisms across translations when treating non-English speakers (R"
2021.acl-long.540,P98-1013,0,0.363544,"unctions” of locations by identifying activities that represent a prototypical reason why people go to a location. For example, people go to restaurants to eat, airports to catch a flight, and churches to pray. They referred to the associated activity as a prototypical goal activity and presented a semi-supervised method to iteratively learn the goal activities. Our work is also related to frame semantics, which studies how we associate words and phrases with conceptual structures called frames (Fillmore, 1976), which characterize an abstract scene or situation. The Berkeley FrameNet project (Baker et al., 1998; Ruppenhofer et al., 2016) provides an online lexical database for frame semantics and a corpus of annotated documents. There has been substantial work on frame semantic parsing (e.g., Das et al., 2014; Peng et al., 2018), which is the task of automatically extracting frame structures from sentences. Several efforts have enhanced FrameNet by mapping it to other lexicons, such as WordNet, PropBank and VerbNet (Shi and Mihalcea, 2005; Palmer, 2009; Ferr´andez et al., 2010). Pavlick et al. (2015) increased the lexical coverage of FrameNet through automatic paraphrasing and manual verification. Y"
2021.acl-long.540,P19-1470,0,0.100756,"owledge in the form of predefined relations expressed in natural language words and phrases. It was built from Open Mind Common Sense, a crowd-sourced knowledge project (Singh, 2002), Within the NLP community, a variety of recent projects have focused on trying to acquire different types of commonsense knowledge, such as Forbes and Choi (2017); Collell et al. (2018); Rashkin et al. (2018); Yang et al. (2018). Sap et al. (2019) presented a crowd-sourced commonsense reasoning data set called ATOMIC that focuses on inferential knowledge related to events, which is organized as if-then relations. Bosselut et al. (2019) later proposed COMET, a transformer-based framework for automatic construction of commonsense knowledge bases that was trained from ATOMIC and ConceptNet. Both ConceptNet and COMET include a UsedFor relation that is relevant to our task, and we evaluate their performance on our data set in Section 6. Of relevance to our work, Jiang and Riloff (2018) learned the prototypical “functions” of locations by identifying activities that represent a prototypical reason why people go to a location. For example, people go to restaurants to eat, airports to catch a flight, and churches to pray. They refe"
2021.acl-long.540,P79-1013,0,0.582684,", we present transformer-based models for this task that exploit both masked sentence patterns and the definitions of physical artifacts and frames. Experiments show that our best model yields substantially better results than the baseline methods. 2 Related Work Researchers have known for a long time that commonsense knowledge is essential for natural language understanding (Charniak, 1972; Schank and Abelson, 1977). Some of this work specifically argued that commonsense knowledge about physical objects, including functional knowledge, plays an important role in narrative text understanding (Burstein, 1979; Lehnert and Burstein, 1979). These observations have led to considerable work toward constructing commonsense knowledge repositories. The Cyc project (Lenat, 1995) built a large ontology of commonsense concepts and facts over many years. More recently, ConceptNet (Speer et al., 2017) captures commonsense knowledge in the form of predefined relations expressed in natural language words and phrases. It was built from Open Mind Common Sense, a crowd-sourced knowledge project (Singh, 2002), Within the NLP community, a variety of recent projects have focused on trying to acquire different types o"
2021.acl-long.540,D19-1109,0,0.0269272,"erview of the PFmask model. Each pink block that is fed into BERT represents a sentence template for a given artifact. frame based on the extracted verbs and select the best frame. 5.4 Masked Language Model (MLM) Baseline Co-occurrence in text is a strong signal of correlation. But an activity that is highly correlated with an artifact may not be its prototypical use. For example, cut frequently co-occurs with rope, but the purpose of a rope is not to be cut – its prototypical use is for attaching things. Recent work has successfully used masked language models to learn commonsense knowledge (Davison et al., 2019), so we explored whether masked language models could be beneficial for our task. We use the BERT (Devlin et al., 2019) masked language model to get prediction scores for every (ai , lk ) pair, where ai is one of our physical artifacts and lk is a lexical unit linked to one of our 42 candidate frames. We defined 6 sentence templates that represent expressions describing what an object is used for, which are shown below. The first blank space is for artifact ai and the second blank space is for action lk . (1) can be used to . (2) I used to . (3) can be used for . (4) I used for . (5) The purpo"
2021.acl-long.540,N19-1423,0,0.189144,"rame based on the extracted verbs and select the best frame. 5.4 Masked Language Model (MLM) Baseline Co-occurrence in text is a strong signal of correlation. But an activity that is highly correlated with an artifact may not be its prototypical use. For example, cut frequently co-occurs with rope, but the purpose of a rope is not to be cut – its prototypical use is for attaching things. Recent work has successfully used masked language models to learn commonsense knowledge (Davison et al., 2019), so we explored whether masked language models could be beneficial for our task. We use the BERT (Devlin et al., 2019) masked language model to get prediction scores for every (ai , lk ) pair, where ai is one of our physical artifacts and lk is a lexical unit linked to one of our 42 candidate frames. We defined 6 sentence templates that represent expressions describing what an object is used for, which are shown below. The first blank space is for artifact ai and the second blank space is for action lk . (1) can be used to . (2) I used to . (3) can be used for . (4) I used for . (5) The purpose of is to (6) If I had , I could . . Next, we produced a probability distribution over all of the lexical units based"
2021.acl-long.540,ferrandez-etal-2010-aligning,0,0.0406407,"Missing"
2021.acl-long.540,P17-1025,0,0.023943,"ese observations have led to considerable work toward constructing commonsense knowledge repositories. The Cyc project (Lenat, 1995) built a large ontology of commonsense concepts and facts over many years. More recently, ConceptNet (Speer et al., 2017) captures commonsense knowledge in the form of predefined relations expressed in natural language words and phrases. It was built from Open Mind Common Sense, a crowd-sourced knowledge project (Singh, 2002), Within the NLP community, a variety of recent projects have focused on trying to acquire different types of commonsense knowledge, such as Forbes and Choi (2017); Collell et al. (2018); Rashkin et al. (2018); Yang et al. (2018). Sap et al. (2019) presented a crowd-sourced commonsense reasoning data set called ATOMIC that focuses on inferential knowledge related to events, which is organized as if-then relations. Bosselut et al. (2019) later proposed COMET, a transformer-based framework for automatic construction of commonsense knowledge bases that was trained from ATOMIC and ConceptNet. Both ConceptNet and COMET include a UsedFor relation that is relevant to our task, and we evaluate their performance on our data set in Section 6. Of relevance to our"
2021.acl-long.540,P18-1120,1,0.838249,"ell et al. (2018); Rashkin et al. (2018); Yang et al. (2018). Sap et al. (2019) presented a crowd-sourced commonsense reasoning data set called ATOMIC that focuses on inferential knowledge related to events, which is organized as if-then relations. Bosselut et al. (2019) later proposed COMET, a transformer-based framework for automatic construction of commonsense knowledge bases that was trained from ATOMIC and ConceptNet. Both ConceptNet and COMET include a UsedFor relation that is relevant to our task, and we evaluate their performance on our data set in Section 6. Of relevance to our work, Jiang and Riloff (2018) learned the prototypical “functions” of locations by identifying activities that represent a prototypical reason why people go to a location. For example, people go to restaurants to eat, airports to catch a flight, and churches to pray. They referred to the associated activity as a prototypical goal activity and presented a semi-supervised method to iteratively learn the goal activities. Our work is also related to frame semantics, which studies how we associate words and phrases with conceptual structures called frames (Fillmore, 1976), which characterize an abstract scene or situation. The"
2021.acl-long.540,2021.eacl-main.206,1,0.738545,"this information is often described in the dictionary definition of an artifact, although it can be expressed in many different ways. For example, the first sense definition in WordNet for knife is “edge tool used as a cutting instrument...”, and for bus it is “a vehicle carrying many passengers...”. The definition often provides a short and precise sentence that describes what the artifact is as well as what it is typically used for. FrameNet also provides a definition for each frame. For example, the definition of the Cutting frame is “An Agent cuts a Item into Pieces using an Instrument”. Jiang and Riloff (2021) exploited both frame and lexical unit definitions for the frame identification task in a model that assesses the semantic coherence between the meaning of a target word in a sentence and a candidate frame. Similarly, we hypothesized that a model could potentially learn the semantic relatedness between the definitions of a physical artifact and the frame that describes its typical function. To investigate this idea, we used the BERT model (Devlin et al., 2019) as the base of our architecture and fine-tuned BERT for our task using both dictionary definitions of artifacts and frame definitions f"
2021.acl-long.540,P15-2067,0,0.0291759,"Missing"
2021.acl-long.540,N18-1135,0,0.0119513,"ed to the associated activity as a prototypical goal activity and presented a semi-supervised method to iteratively learn the goal activities. Our work is also related to frame semantics, which studies how we associate words and phrases with conceptual structures called frames (Fillmore, 1976), which characterize an abstract scene or situation. The Berkeley FrameNet project (Baker et al., 1998; Ruppenhofer et al., 2016) provides an online lexical database for frame semantics and a corpus of annotated documents. There has been substantial work on frame semantic parsing (e.g., Das et al., 2014; Peng et al., 2018), which is the task of automatically extracting frame structures from sentences. Several efforts have enhanced FrameNet by mapping it to other lexicons, such as WordNet, PropBank and VerbNet (Shi and Mihalcea, 2005; Palmer, 2009; Ferr´andez et al., 2010). Pavlick et al. (2015) increased the lexical coverage of FrameNet through automatic paraphrasing and manual verification. Yatskar et al. (2016) introduced situation recognition, which is the problem of producing a concise summary of the situation that an image depicts. Similar to our work, they selected a subset of frames from FrameNet to repr"
2021.acl-long.540,P18-1043,0,0.0244433,"toward constructing commonsense knowledge repositories. The Cyc project (Lenat, 1995) built a large ontology of commonsense concepts and facts over many years. More recently, ConceptNet (Speer et al., 2017) captures commonsense knowledge in the form of predefined relations expressed in natural language words and phrases. It was built from Open Mind Common Sense, a crowd-sourced knowledge project (Singh, 2002), Within the NLP community, a variety of recent projects have focused on trying to acquire different types of commonsense knowledge, such as Forbes and Choi (2017); Collell et al. (2018); Rashkin et al. (2018); Yang et al. (2018). Sap et al. (2019) presented a crowd-sourced commonsense reasoning data set called ATOMIC that focuses on inferential knowledge related to events, which is organized as if-then relations. Bosselut et al. (2019) later proposed COMET, a transformer-based framework for automatic construction of commonsense knowledge bases that was trained from ATOMIC and ConceptNet. Both ConceptNet and COMET include a UsedFor relation that is relevant to our task, and we evaluate their performance on our data set in Section 6. Of relevance to our work, Jiang and Riloff (2018) learned the prot"
2021.acl-long.540,P18-2102,0,0.0168343,"monsense knowledge repositories. The Cyc project (Lenat, 1995) built a large ontology of commonsense concepts and facts over many years. More recently, ConceptNet (Speer et al., 2017) captures commonsense knowledge in the form of predefined relations expressed in natural language words and phrases. It was built from Open Mind Common Sense, a crowd-sourced knowledge project (Singh, 2002), Within the NLP community, a variety of recent projects have focused on trying to acquire different types of commonsense knowledge, such as Forbes and Choi (2017); Collell et al. (2018); Rashkin et al. (2018); Yang et al. (2018). Sap et al. (2019) presented a crowd-sourced commonsense reasoning data set called ATOMIC that focuses on inferential knowledge related to events, which is organized as if-then relations. Bosselut et al. (2019) later proposed COMET, a transformer-based framework for automatic construction of commonsense knowledge bases that was trained from ATOMIC and ConceptNet. Both ConceptNet and COMET include a UsedFor relation that is relevant to our task, and we evaluate their performance on our data set in Section 6. Of relevance to our work, Jiang and Riloff (2018) learned the prototypical “functions”"
2021.eacl-main.206,P98-1013,0,0.738171,"arget word in a sentence based on the semantic coherence of their meanings. We evaluate our model on three data sets and show that it consistently achieves better performance than previous systems. 1 The pandemic has sparked a lot of problems for the economy. Introduction Research on frame semantics has grown within the fields of natural language processing and cognitive science since the 1970s as the study of how we associate words and phrases with cognitive structures called frames, which characterize a small abstract scene or situation (Fillmore, 1976, 1982). The Berkeley FrameNet project (Baker et al., 1998) provides an online lexical database for frame semantics together with a corpus of annotated documents. Frame semantic parsing is the task of automatically extracting frame semantic structures from sentences. The process typically consists of three steps: target identification, which identifies frame-evoking predicates in the sentence; frame identification, which identifies the evoked frame for each target; and argument identification, which identifies arguments of a frame and labels them with semantic roles (frame elements). In this work, we focus on the frame identification problem. FrameNet"
2021.eacl-main.206,2020.acl-main.95,0,0.0631383,"al., 2014). Syntactic information, typically dependency paths, has consistently played an important role in frame identification (Das et al., 2014; Peng et al., 2018). Our work is motivated by the rich lexicographic information about frames and lexical units provided by the FrameNet database, which has not been fully utilized for the frame identification task. Recent advances in large pre-trained transformer models (Devlin et al., 2019) have demonstrated the ability to capture semantic meaning in dictionary definitions for the related problem of word sense disambiguation (Huang et al., 2019; Blevins and Zettlemoyer, 2020). 2429 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2429–2434 April 19 - 23, 2021. ©2021 Association for Computational Linguistics [CLS] [Target ] [SEP] Sentence [SEP] Lexical Unit Definition Frame Definition [Target ] Sentence [SEP] ... ... [CLS] BERT Linear ℒ [SEP] Lexical Unit Definition Frame Definition Figure 1: Overview of the FIDO architecture. Each green block represents a different candidate pair (lexical unit, frame) for the same Targeti . Our model uses the definitions of frames and lexical units in FrameNet as a"
2021.eacl-main.206,D18-1181,0,0.0232517,"ultimodal representations grounded in images to improve frame identification. Peng et al. (2018) 1 https://github.com/tyjiangU/fido proposed a joint inference formulation that learns semantic parsers from multiple datasets. In contrast to the previous models, our model does not rely on syntactic features. We assess semantic coherence directly from the input sentence and definitions in FrameNet. Another line of related work is learning embeddings from dictionary definitions. It has been shown that neural networks can extract semantic information from dictionary definitions (Kumar et al., 2019; Bosc and Vincent, 2018). Recent work in word sense disambiguation (Huang et al., 2019; Blevins and Zettlemoyer, 2020) has demonstrated that providing pre-trained language models with sense definitions (glosses) can be effective. Yong and Torrent (2020) also used the sense definitions of lexical units for their research on frame induction. Our model adopts a similar architecture as Huang et al. (2019), but we focus on the frame identification task and we explore the use of both lexical unit and frame definitions for this task. 3 Method Given a sentence and a target word or phrase, the frame identification task assign"
2021.eacl-main.206,N18-1134,0,0.0345326,"Missing"
2021.eacl-main.206,N10-1138,0,0.0988773,"obj. ignite” and “provide the stimulus for”. The former sense is associated with the Setting fire frame and the latter one is associated with the Cause to start frame. The Setting fire frame is defined as “this frame describes the creation of a flame...”, and the Cause to start frame is defined as “a cause, animate or inanimate, causes a process, the effect, to begin”. So Cause to start is the correct frame for this sentence. Previous work has shown the success of using feature engineering with linear classification models (Johansson and Nugues, 2007) and discriminative probabilistic models (Das et al., 2010), which were later improved by applying distributed word representations and deep neural network models (Hermann et al., 2014). Syntactic information, typically dependency paths, has consistently played an important role in frame identification (Das et al., 2014; Peng et al., 2018). Our work is motivated by the rich lexicographic information about frames and lexical units provided by the FrameNet database, which has not been fully utilized for the frame identification task. Recent advances in large pre-trained transformer models (Devlin et al., 2019) have demonstrated the ability to capture se"
2021.eacl-main.206,N19-1423,0,0.00961421,"2007) and discriminative probabilistic models (Das et al., 2010), which were later improved by applying distributed word representations and deep neural network models (Hermann et al., 2014). Syntactic information, typically dependency paths, has consistently played an important role in frame identification (Das et al., 2014; Peng et al., 2018). Our work is motivated by the rich lexicographic information about frames and lexical units provided by the FrameNet database, which has not been fully utilized for the frame identification task. Recent advances in large pre-trained transformer models (Devlin et al., 2019) have demonstrated the ability to capture semantic meaning in dictionary definitions for the related problem of word sense disambiguation (Huang et al., 2019; Blevins and Zettlemoyer, 2020). 2429 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2429–2434 April 19 - 23, 2021. ©2021 Association for Computational Linguistics [CLS] [Target ] [SEP] Sentence [SEP] Lexical Unit Definition Frame Definition [Target ] Sentence [SEP] ... ... [CLS] BERT Linear ℒ [SEP] Lexical Unit Definition Frame Definition Figure 1: Overview of the FIDO a"
2021.eacl-main.206,E17-1045,0,0.152953,"rk on the frame identification problem with respect to FrameNet, especially since the SemEval 2007 shared task (Baker et al., 2007). Johansson and Nugues (2007) used a SVM classifier to disambiguate frames with hand-crafted features. Das et al. (2010) applied feature-based discriminative probabilistic (log-linear) models for frame identification. Hermann et al. (2014) presented a method using distributed representations of predicates and their syntactic context by mapping input representations and frame representations to a common latent space using the WSABIE algorithm (Weston et al., 2011). Hartmann et al. (2017) built a simplified model based on Hermann et al. (2014) and achieved comparable results. They also released a new FrameNet-annotated test set based on user-generated web text from Yahoo! Answers. Yang and Mitchell (2017) integrated a bidirectional LSTM neural network and a relational network to jointly decode frames. More recently, Botschen et al. (2018) brought in multimodal representations grounded in images to improve frame identification. Peng et al. (2018) 1 https://github.com/tyjiangU/fido proposed a joint inference formulation that learns semantic parsers from multiple datasets. In con"
2021.eacl-main.206,P14-1136,0,0.36202,"e is associated with the Cause to start frame. The Setting fire frame is defined as “this frame describes the creation of a flame...”, and the Cause to start frame is defined as “a cause, animate or inanimate, causes a process, the effect, to begin”. So Cause to start is the correct frame for this sentence. Previous work has shown the success of using feature engineering with linear classification models (Johansson and Nugues, 2007) and discriminative probabilistic models (Das et al., 2010), which were later improved by applying distributed word representations and deep neural network models (Hermann et al., 2014). Syntactic information, typically dependency paths, has consistently played an important role in frame identification (Das et al., 2014; Peng et al., 2018). Our work is motivated by the rich lexicographic information about frames and lexical units provided by the FrameNet database, which has not been fully utilized for the frame identification task. Recent advances in large pre-trained transformer models (Devlin et al., 2019) have demonstrated the ability to capture semantic meaning in dictionary definitions for the related problem of word sense disambiguation (Huang et al., 2019; Blevins and"
2021.eacl-main.206,D17-1128,0,0.116859,"fted features. Das et al. (2010) applied feature-based discriminative probabilistic (log-linear) models for frame identification. Hermann et al. (2014) presented a method using distributed representations of predicates and their syntactic context by mapping input representations and frame representations to a common latent space using the WSABIE algorithm (Weston et al., 2011). Hartmann et al. (2017) built a simplified model based on Hermann et al. (2014) and achieved comparable results. They also released a new FrameNet-annotated test set based on user-generated web text from Yahoo! Answers. Yang and Mitchell (2017) integrated a bidirectional LSTM neural network and a relational network to jointly decode frames. More recently, Botschen et al. (2018) brought in multimodal representations grounded in images to improve frame identification. Peng et al. (2018) 1 https://github.com/tyjiangU/fido proposed a joint inference formulation that learns semantic parsers from multiple datasets. In contrast to the previous models, our model does not rely on syntactic features. We assess semantic coherence directly from the input sentence and definitions in FrameNet. Another line of related work is learning embeddings f"
2021.eacl-main.206,2020.lrec-1.431,0,0.0851127,"Missing"
2021.eacl-main.206,D19-1355,0,0.0680281,"k models (Hermann et al., 2014). Syntactic information, typically dependency paths, has consistently played an important role in frame identification (Das et al., 2014; Peng et al., 2018). Our work is motivated by the rich lexicographic information about frames and lexical units provided by the FrameNet database, which has not been fully utilized for the frame identification task. Recent advances in large pre-trained transformer models (Devlin et al., 2019) have demonstrated the ability to capture semantic meaning in dictionary definitions for the related problem of word sense disambiguation (Huang et al., 2019; Blevins and Zettlemoyer, 2020). 2429 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2429–2434 April 19 - 23, 2021. ©2021 Association for Computational Linguistics [CLS] [Target ] [SEP] Sentence [SEP] Lexical Unit Definition Frame Definition [Target ] Sentence [SEP] ... ... [CLS] BERT Linear ℒ [SEP] Lexical Unit Definition Frame Definition Figure 1: Overview of the FIDO architecture. Each green block represents a different candidate pair (lexical unit, frame) for the same Targeti . Our model uses the definitions of frames and"
2021.eacl-main.206,S07-1048,0,0.349692,"be triggered. The word lemma spark has two senses in FrameNet: “with obj. ignite” and “provide the stimulus for”. The former sense is associated with the Setting fire frame and the latter one is associated with the Cause to start frame. The Setting fire frame is defined as “this frame describes the creation of a flame...”, and the Cause to start frame is defined as “a cause, animate or inanimate, causes a process, the effect, to begin”. So Cause to start is the correct frame for this sentence. Previous work has shown the success of using feature engineering with linear classification models (Johansson and Nugues, 2007) and discriminative probabilistic models (Das et al., 2010), which were later improved by applying distributed word representations and deep neural network models (Hermann et al., 2014). Syntactic information, typically dependency paths, has consistently played an important role in frame identification (Das et al., 2014; Peng et al., 2018). Our work is motivated by the rich lexicographic information about frames and lexical units provided by the FrameNet database, which has not been fully utilized for the frame identification task. Recent advances in large pre-trained transformer models (Devli"
2021.eacl-main.206,P19-1568,0,0.0212231,"(2018) brought in multimodal representations grounded in images to improve frame identification. Peng et al. (2018) 1 https://github.com/tyjiangU/fido proposed a joint inference formulation that learns semantic parsers from multiple datasets. In contrast to the previous models, our model does not rely on syntactic features. We assess semantic coherence directly from the input sentence and definitions in FrameNet. Another line of related work is learning embeddings from dictionary definitions. It has been shown that neural networks can extract semantic information from dictionary definitions (Kumar et al., 2019; Bosc and Vincent, 2018). Recent work in word sense disambiguation (Huang et al., 2019; Blevins and Zettlemoyer, 2020) has demonstrated that providing pre-trained language models with sense definitions (glosses) can be effective. Yong and Torrent (2020) also used the sense definitions of lexical units for their research on frame induction. Our model adopts a similar architecture as Huang et al. (2019), but we focus on the frame identification task and we explore the use of both lexical unit and frame definitions for this task. 3 Method Given a sentence and a target word or phrase, the frame i"
2021.eacl-main.206,N18-1135,0,0.579126,"rame is defined as “a cause, animate or inanimate, causes a process, the effect, to begin”. So Cause to start is the correct frame for this sentence. Previous work has shown the success of using feature engineering with linear classification models (Johansson and Nugues, 2007) and discriminative probabilistic models (Das et al., 2010), which were later improved by applying distributed word representations and deep neural network models (Hermann et al., 2014). Syntactic information, typically dependency paths, has consistently played an important role in frame identification (Das et al., 2014; Peng et al., 2018). Our work is motivated by the rich lexicographic information about frames and lexical units provided by the FrameNet database, which has not been fully utilized for the frame identification task. Recent advances in large pre-trained transformer models (Devlin et al., 2019) have demonstrated the ability to capture semantic meaning in dictionary definitions for the related problem of word sense disambiguation (Huang et al., 2019; Blevins and Zettlemoyer, 2020). 2429 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2429–2434 April"
C02-1070,J95-4004,0,0.010277,"reated, we apply an English IE system to the English texts and transfer the IE annotations to the target language as follows: 1. Sentence align the parallel corpus.1 2. Word-align the parallel corpus using the Giza++ system (Och and Ney, 2000). 3. Transfer English IE annotations and nounphrase boundaries to French via the mechanism described in Yarowsky et al. (2001), yielding annotated sentence pairs as illustrated in Figure 1. 4. Train a stand-alone IE tagger on these projected annotations (described in Section 4). 4 Transformation-Based Learning We used transformation-based learning (TBL) (Brill, 1995) to learn information extraction rules for French. TBL is well-suited for this task because it uses rule templates as the basis for learning, which can be easily modeled after English extraction patterns. However, information extraction systems typically rely on a shallow parser to identify syntactic elements (e.g., subjects and direct objects) and verb 1 This is trivial because each sentence has a numbered anchor preserved by the MT system. Rule Condition Rule Effect 1. w1 =crashed w2 =in w3 is LOC . 2. w1 =wreckage w2 =of w3 is VEH . 3. w1 =injuring w2 is VIC . 4. w1 =NOUN w2 =crashed w1 is"
C02-1070,P00-1035,1,0.88979,"Missing"
C02-1070,P98-1067,0,0.0265457,"or a specific domain, and the types of facts to be extracted are defined in advance. In this paper, we will focus on the domain of plane crashes and will try to extract descriptions of the vehicle involved in the crash, victims of the crash, and the location of the crash. Most IE systems use some form of extraction patterns to recognize and extract relevant information. Many techniques have been developed to generate extraction patterns for a new domain automatically, including PALKA (Kim & Moldovan, 1993), AutoSlog (Riloff, 1993), CRYSTAL (Soderland et al., 1995), RAPIER (Califf, 1998), SRV (Freitag, 1998), meta-bootstrapping (Riloff & Jones, 1999), and ExDisco (Yangarber et al., 2000). For this work, we will use AutoSlog-TS (Riloff, 1996b) to generate IE patterns for the plane crash domain. AutoSlog-TS is a derivative of AutoSlog that automatically generates extraction patterns by gathering statistics from a corpus of relevant texts (within the domain) and irrelevant texts (outside the domain). Each extraction pattern represents a linguistic expression that can extract noun phrases from one of three syntactic positions: subject, direct object, or object of a prepositional phrase. For example,"
C02-1070,N01-1006,0,0.0351997,"Missing"
C02-1070,P00-1056,0,0.0248368,"ause it frees cross-language projection research from the relatively few large existing bilingual corpora (such as the Canadian Hansards). MT allows projection to be performed on any corpus, such as the domain-specific planecrash news stories employed here. Section 5 gives the details of the MT system and corpora that we used. Once the artificial parallel corpus has been created, we apply an English IE system to the English texts and transfer the IE annotations to the target language as follows: 1. Sentence align the parallel corpus.1 2. Word-align the parallel corpus using the Giza++ system (Och and Ney, 2000). 3. Transfer English IE annotations and nounphrase boundaries to French via the mechanism described in Yarowsky et al. (2001), yielding annotated sentence pairs as illustrated in Figure 1. 4. Train a stand-alone IE tagger on these projected annotations (described in Section 4). 4 Transformation-Based Learning We used transformation-based learning (TBL) (Brill, 1995) to learn information extraction rules for French. TBL is well-suited for this task because it uses rule templates as the basis for learning, which can be easily modeled after English extraction patterns. However, information extra"
C02-1070,C00-2136,0,0.0314821,"in advance. In this paper, we will focus on the domain of plane crashes and will try to extract descriptions of the vehicle involved in the crash, victims of the crash, and the location of the crash. Most IE systems use some form of extraction patterns to recognize and extract relevant information. Many techniques have been developed to generate extraction patterns for a new domain automatically, including PALKA (Kim & Moldovan, 1993), AutoSlog (Riloff, 1993), CRYSTAL (Soderland et al., 1995), RAPIER (Califf, 1998), SRV (Freitag, 1998), meta-bootstrapping (Riloff & Jones, 1999), and ExDisco (Yangarber et al., 2000). For this work, we will use AutoSlog-TS (Riloff, 1996b) to generate IE patterns for the plane crash domain. AutoSlog-TS is a derivative of AutoSlog that automatically generates extraction patterns by gathering statistics from a corpus of relevant texts (within the domain) and irrelevant texts (outside the domain). Each extraction pattern represents a linguistic expression that can extract noun phrases from one of three syntactic positions: subject, direct object, or object of a prepositional phrase. For example, the following patterns could extract vehicles involved in a plane crash: “<subjec"
C02-1070,H01-1035,1,0.939542,"ish, annotated corpora and text analysis tools are readily available. However, for the large majority of the world’s languages, resources such as treebanks, part-of-speech taggers, and parsers do not exist. And even for many of the better-supported languages, cutting edge analysis tools in areas such as information extraction are not readily available. One solution to this NLP-resource disparity is to transfer linguistic resources, tools, and domain knowledge from resource-rich languages to resource-impoverished ones. In recent years, there has been a burst of projects based on this paradigm. Yarowsky et al. (2001) developed cross-language projection models for part-of-speech tags, base noun phrases, named-entity tags, and morphological analysis (lemmatization) for four languages. Resnik et al. (2001) developed related models for projecting dependency parsers from English to Chinese. There has also been extensive work on the cross-language transfer and development of ontologies and WordNets (e.g., (Atserias et al., 1997)). 3.2 Mechanics of Projection The cross-language projection methodology employed in this paper is based on Yarowsky et al. (2001), with one important exception. Given the absence of ava"
C02-1070,C98-1064,0,\N,Missing
D07-1075,P04-1056,0,0.122506,"m is to extract facts associated with domain-specific events from unstructured text. Many different approaches to information extraction have been developed, but generally speaking they fall into two categories: classifier-based approaches and rule/patternbased approaches. Classifier-based IE systems use machine learning techniques to train a classifier that sequentially processes a document looking for words to be extracted. Examples of classifier-based IE systems are SRV (Freitag, 1998), HMM approaches (Freitag and McCallum, 2000), ALICE (Chieu et al., 2003), and Relational Markov Networks (Bunescu and Mooney, 2004). The classifier typically decides whether a word should be extracted by considering features associated with that word as well as features of the words around it. Another common approach to information extraction uses a set of explicit patterns or rules to find relevant information. Some older systems relied on hand-crafted patterns, while more recent systems learn them automatically or semiautomatically. Examples of rule/pattern-based approaches to information extraction are FASTUS (Hobbs et al., 1997), PALKA (Kim and Moldovan, 1993), LIEP (Huffman, 1996), CRYSTAL (Soderland et al., 1995), A"
D07-1075,P03-1028,0,0.524195,"us and ProMed disease outbreak stories. This approach requires only a few seed extraction patterns and a collection of relevant and irrelevant documents for training. 1 Introduction Many information extraction (IE) systems rely on rules or patterns to extract words and phrases based on their surrounding context (e.g., (Soderland et al., 1995; Riloff, 1996; Califf and Mooney, 1999; Yangarber et al., 2000)). For example, a pattern like “<subject> was assassinated” can reliably identify a victim of a murder event. Classification-based IE systems (e.g., (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003)) also generally decide whether to extract words based on properties of the words themselves as well as properties associated with their surrounding context. In this research, we propose an alternative approach to IE that decouples the tasks of finding a relevant region of text and finding a desired extraction. This decoupled approach to IE has several potential advantages. First, even seemingly good patterns can produce false hits due to metaphor and idiomatic expressions. However, by restricting their use to relevant regions of text, we could avoid such false positives. For example, “John Ke"
D07-1075,P98-1067,0,0.686932,"ood performance on the MUC-4 terrorism corpus and ProMed disease outbreak stories. This approach requires only a few seed extraction patterns and a collection of relevant and irrelevant documents for training. 1 Introduction Many information extraction (IE) systems rely on rules or patterns to extract words and phrases based on their surrounding context (e.g., (Soderland et al., 1995; Riloff, 1996; Califf and Mooney, 1999; Yangarber et al., 2000)). For example, a pattern like “<subject> was assassinated” can reliably identify a victim of a murder event. Classification-based IE systems (e.g., (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003)) also generally decide whether to extract words based on properties of the words themselves as well as properties associated with their surrounding context. In this research, we propose an alternative approach to IE that decouples the tasks of finding a relevant region of text and finding a desired extraction. This decoupled approach to IE has several potential advantages. First, even seemingly good patterns can produce false hits due to metaphor and idiomatic expressions. However, by restricting their use to relevant regions of text, we could"
D07-1075,W06-0208,1,0.820319,"classifier as our relevant sentence classifier in the IE experiments described in Section 6.3. 4 Learning Semantic Affinity-based Extraction Patterns One motivation for creating a relevant region classifier is to reduce the responsibilities of the extraction patterns. Once we know that we are in a domainrelevant area of text, patterns that simply identify words and phrases belonging to a relevant semantic class may be sufficient. In this section, we describe a method to automatically identify semantically appropriate extraction patterns for use with the sentence classifier. In previous work (Patwardhan and Riloff, 2006), we introduced a metric called semantic affinity which was used to automatically assign event roles to extraction patterns. Semantic affinity measures the tendency of a pattern to extract noun phrases that belong to a specific set of semantic categories. To use this metric for information extraction, a mapping must be defined between semantic categories and the event roles that are relevant to the IE task. For example, one role in the terrorism domain is physical target, which refers to physical objects that are the target of an attack. Most physical targets fall into one of two general seman"
D07-1075,P03-1029,0,0.288099,"f explicit patterns or rules to find relevant information. Some older systems relied on hand-crafted patterns, while more recent systems learn them automatically or semiautomatically. Examples of rule/pattern-based approaches to information extraction are FASTUS (Hobbs et al., 1997), PALKA (Kim and Moldovan, 1993), LIEP (Huffman, 1996), CRYSTAL (Soderland et al., 1995), AutoSlog/AutoSlog-TS (Riloff, 1993; Riloff, 1996), RAPIER (Califf and Mooney, 1999), WHISK (Soderland, 1999), ExDisco (Yangarber et al., 2000), SNOWBALL (Agichtein and Gravano, 2000), (LP)2 (Ciravegna, 2001), subtree patterns (Sudo et al., 2003), predicate-argument rules (Yakushiji et al., 2006) and KnowItAll 718 (Popescu et al., 2004). One commonality behind all of these approaches is that they simultaneously decide whether a context is relevant and whether a word or phrase is a desirable extraction. Classifier-based systems rely on features that consider both the word and its surrounding context, and rule/pattern-based systems typically use patterns or rules that match both the words around a candidate extraction and (sometimes) properties of the candidate extraction itself. There is a simplicity and elegance to having a single mod"
D07-1075,M92-1001,0,0.0638172,"y first removing from our extraction pattern collection all patterns with probability less than θl . For each event role, we then sort the remaining patterns based on their semantic affinity score for that role and select the top N patterns. Next, we use the θu probability threshold to separate these N patterns into two subsets. Patterns with a probability above θu are considered to be Primary patterns for that role, and those below become the Secondary patterns. 6 Experiments and Results 6.1 Data Sets We evaluated the performance of our IE system on two data sets: the MUC-4 terrorism corpus (Sundheim, 1992), and a ProMed disease outbreaks corpus. The MUC-4 IE task is to extract information about Latin American terrorist events. We focused our analysis on five MUC-4 string roles: perpetrator individuals, perpetrator organizations, physical targets, victims, and weapons. The disease outbreaks corpus consists of electronic reports about disease outbreak events. For this domain we focused on two string roles: diseases and victims5 . The MUC-4 data set consists of 1700 documents, divided into 1300 development (DEV) texts, and four test sets of 100 texts each (TST1, TST2, TST3, and TST4). We used 1300"
D07-1075,W06-1634,0,0.0129451,"information. Some older systems relied on hand-crafted patterns, while more recent systems learn them automatically or semiautomatically. Examples of rule/pattern-based approaches to information extraction are FASTUS (Hobbs et al., 1997), PALKA (Kim and Moldovan, 1993), LIEP (Huffman, 1996), CRYSTAL (Soderland et al., 1995), AutoSlog/AutoSlog-TS (Riloff, 1993; Riloff, 1996), RAPIER (Califf and Mooney, 1999), WHISK (Soderland, 1999), ExDisco (Yangarber et al., 2000), SNOWBALL (Agichtein and Gravano, 2000), (LP)2 (Ciravegna, 2001), subtree patterns (Sudo et al., 2003), predicate-argument rules (Yakushiji et al., 2006) and KnowItAll 718 (Popescu et al., 2004). One commonality behind all of these approaches is that they simultaneously decide whether a context is relevant and whether a word or phrase is a desirable extraction. Classifier-based systems rely on features that consider both the word and its surrounding context, and rule/pattern-based systems typically use patterns or rules that match both the words around a candidate extraction and (sometimes) properties of the candidate extraction itself. There is a simplicity and elegance to having a single model that handles both of these problems at the same"
D07-1075,C00-2136,0,0.714602,"raction patterns. We then distinguish primary patterns from secondary patterns and apply the patterns selectively in the relevant regions. The resulting IE system achieves good performance on the MUC-4 terrorism corpus and ProMed disease outbreak stories. This approach requires only a few seed extraction patterns and a collection of relevant and irrelevant documents for training. 1 Introduction Many information extraction (IE) systems rely on rules or patterns to extract words and phrases based on their surrounding context (e.g., (Soderland et al., 1995; Riloff, 1996; Califf and Mooney, 1999; Yangarber et al., 2000)). For example, a pattern like “<subject> was assassinated” can reliably identify a victim of a murder event. Classification-based IE systems (e.g., (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003)) also generally decide whether to extract words based on properties of the words themselves as well as properties associated with their surrounding context. In this research, we propose an alternative approach to IE that decouples the tasks of finding a relevant region of text and finding a desired extraction. This decoupled approach to IE has several potential advantages. First, even"
D07-1075,C02-1154,0,0.0430397,"Missing"
D07-1075,C98-1064,0,\N,Missing
D09-1016,D07-1075,1,0.73697,"to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. In contrast, our approach simply creates a richer IE model for individual extractions by expanding the “field of view” to include the surrounding sentence. The two components of the unified model presented in this paper are somewhat similar to our previous work (Patwardhan and Riloff, 2007), where we employ a relevant region identification phase prior to pattern-based extraction. In that work we adopted a pipeline paradigm, where a classifier identifies relevant sentences and only those sentences are fed to the extraction module. Our unified probabilistic model described in this paper does not draw a hard line between relevant and irrelevant sentences, but gently balances (1) appears in an event sentence, and (2) is a legitimate filler for the event role. Thus, G LACIER is designed for noun phrase extraction and, mathematically, its decisions are based on the following joint pro"
D09-1016,P04-1056,0,0.193815,"ence in a unified framework. Our unified probabilistic model, called G LACIER, consists of two components: a model for sentential event recognition and a model for recognizing plausible role fillers. The Sentential Event Recognizer offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event. The Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995), Riloff (1996), Yangarber et al. (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al. (2003), Bunescu and Mooney (2004)) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together. Most IE systems are designed to identify role fillers that appear as arguments to event verbs or nouns, either explicitly via syntactic relations or implicitly via proximity (e.g., John murdered Tom or the murder of Tom by John). But ma"
D09-1016,P03-1028,0,0.113762,"l and sentential evidence in a unified framework. Our unified probabilistic model, called G LACIER, consists of two components: a model for sentential event recognition and a model for recognizing plausible role fillers. The Sentential Event Recognizer offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event. The Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995), Riloff (1996), Yangarber et al. (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al. (2003), Bunescu and Mooney (2004)) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together. Most IE systems are designed to identify role fillers that appear as arguments to event verbs or nouns, either explicitly via syntactic relations or implicitly via proximity (e.g., John murdered Tom or the mur"
D09-1016,P05-1045,0,0.206819,"liff and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. In contrast, our approach simply creates a richer IE model for individual extractions by expanding the “field of view” to include the surrounding sentence. The two components of the unified model presented in this paper are somewhat similar to our previous work (Patwardhan and Riloff, 2007), where we employ a relevant region identification phase prior to pattern-based extraction. In that work we adopted a pipeline paradigm, where a classifier iden"
D09-1016,M92-1001,0,0.0329601,"Event Template Both the MUC-4 and ProMed data sets have separate answer keys rather than annotated source documents. Figure 1 shows an example of a document and its corresponding answer key template. To train the baseline NB system, we identify all instances of each answer key string in the source document and consider every instance a positive training example. This produces noisy training data, however, because some instances occur in 2 http://www.promedmail.org The “victims” can be people, animals, or plants. 4 With respect to the definition of terrorist incidents in the MUC-4 guidelines (Sundheim, 1992). 5 Pronouns were discarded from both the system responses and the answer keys since we do not perform coreference resolution. Duplicate extractions (e.g., the same string extracted multiple times from the same document) were conflated before being scored, so they count as just one hit or one miss. 3 155 AutoSlog-TS Sem Affinity NB .50 NB .70 NB .90 P .33 .48 .36 .41 .51 PerpInd R F .49 .40 .39 .43 .34 .35 .25 .31 .17 .25 P .52 .36 .35 .43 .56 PerpOrg R F .33 .41 .58 .45 .46 .40 .31 .36 .15 .24 P .54 .56 .53 .58 .67 Target R .59 .46 .49 .42 .30 F .56 .50 .51 .48 .41 P .49 .46 .50 .58 .75 Victi"
D09-1016,P98-1067,0,0.60409,"tion extraction that incorporates both phrasal and sentential evidence in a unified framework. Our unified probabilistic model, called G LACIER, consists of two components: a model for sentential event recognition and a model for recognizing plausible role fillers. The Sentential Event Recognizer offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event. The Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995), Riloff (1996), Yangarber et al. (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al. (2003), Bunescu and Mooney (2004)) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together. Most IE systems are designed to identify role fillers that appear as arguments to event verbs or nouns, either explicitly via syntactic relations or implicitly"
D09-1016,P08-1030,0,0.231327,"hich represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. In contrast, our approach simply creates a richer IE model for individual extractions by expanding the “field of view” to include the surrounding sentence. The two components of the unified model presented in this paper are somewhat similar to our previous work (Patwardhan and Riloff, 2007), where we employ a relevant region identification phase prior to pattern-based extraction. In that work we adopted a pipeline paradigm, where a classifier identifies relevant sentences a"
D09-1016,C00-2136,0,0.65102,"pe of event that they are looking for. We propose a new model for information extraction that incorporates both phrasal and sentential evidence in a unified framework. Our unified probabilistic model, called G LACIER, consists of two components: a model for sentential event recognition and a model for recognizing plausible role fillers. The Sentential Event Recognizer offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event. The Introduction Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al. (1995), Riloff (1996), Yangarber et al. (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al. (2003), Bunescu and Mooney (2004)) to extract role fillers for events. Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it. For tasks such as named entity recognition, immediate context is usually sufficient. But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together. Most IE systems are designed to identify role fillers that appear as arguments to event verbs o"
D09-1016,P07-1075,0,0.166473,"s that are candidates for extraction. Some systems use extraction patterns (Soderland et al., 1995; Riloff, 1996; Yangarber et al., 2000; Califf and Mooney, 2003), which represent the immediate contexts surrounding candidate extractions. Similarly, classifierbased approaches (Freitag, 1998; Freitag and McCallum, 2000; Chieu et al., 2003; Bunescu and Mooney, 2004) rely on features in the immediate context of the candidate extractions. Our work seeks to incorporate additional context into IE. Indeed, several recent approaches have shown the need for global information to improve IE performance. Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Finkel et al. (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. In contrast, our approach simply creates a richer IE model for individual extractions by expanding the “field of view” to include the surrounding sentence. The two components of the unified model presented in this paper are somewhat similar to our previous work (Patwardhan and Riloff, 2007), where we employ a relevan"
D09-1016,C98-1064,0,\N,Missing
D09-1099,P99-1008,0,0.0373775,"ut considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query, and kept only the unique ones. In total, we collected 1.1 GB of snippets for"
D09-1099,P99-1016,0,0.035074,"performance of our approach not by measuring the top951 ranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retriev"
D09-1099,C02-1130,1,0.200375,"passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the cont"
D09-1099,N03-1011,0,0.526242,"hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query, and kept only the unique ones. In total, we collected 1.1 GB of snippets for Animals and 1.5 GB f"
D09-1099,C92-2082,0,0.155172,"l the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for learning. KnowItAll (Etzioni et al., 2005) applies the hyponym pattern"
D09-1099,P08-1119,1,0.498746,"xample of (when the first argument is an instance/example of the second). Section 2 describes our method for harvesting; Section 3 discusses related work; and Section 4 describes the experiments and the results. 2 Term and Relation Extraction using the Doubly-Anchored Pattern Our goal is to develop a technique that automatically ‘fills in’ the concept space in the taxonomy below any root concept, by harvesting terms through repeated web queries. We perform this in two alternating stages. 949 Stage 1: Basic-level/Instance concept collection: We use the Doubly-Anchored Pattern DAP developed in (Kozareva et al., 2008): DAP: [SeedTerm1] such as [SeedTerm2] and <X> which learns a list of basic-level concepts or instances (depending on whether SeedTerm2 expresses a basic-level concept or an instance).1 DAP is very reliable because it is instantiated with examples at both ‘ends’ of the space to be filled (the higher-level (root) concept SeedTerm1 and a basiclevel term or instance (SeedTerm2)), which mutually disambiguate each other. For example, “presidents” for SeedTerm1 can refer to the leader of a country, corporation, or university, and “Ford” for SeedTerm2 can refer to a car company, an automobile pioneer"
D09-1099,W02-1111,0,0.0154932,"g the top951 ranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query,"
D09-1099,I08-2112,0,0.018117,"Missing"
D09-1099,N04-1041,0,0.165862,"ncept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers have followed up on this idea using the web as a corpus."
D09-1099,W97-0313,1,0.420074,"ept such as Concept and <X> If (b) returns more web hits than (a), then the concept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent y"
D09-1099,P98-2182,0,0.111571,"If (b) returns more web hits than (a), then the concept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers h"
D09-1099,W02-1028,1,0.342189,"seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for learn"
D09-1099,C02-1114,0,0.0176554,"sion the basic-level concepts for given root concept. Thus, we almost entirely eliminate the need for humans to provide hyponym seeds. Second, we evaluate the performance of our approach not by measuring the top951 ranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for"
D09-1099,C98-2177,0,\N,Missing
D10-1008,P98-1013,0,0.567785,"s. We also present two corpus-based methods to automatically produce a new resource for affect state recognition: a patient polarity verb lexicon. 4.1 4.1.1 Plot Unit Creation Recognizing Affect States The basic building blocks of plot units are affect states which come in three flavors: positive, negative, and mental. In recent years, many publicly available resources have been created for sentiment analysis and other types of semantic knowledge. We considered a wide variety of resources and ultimately decided to experiment with five resources that most closely matched our needs: • FrameNet (Baker et al., 1998): We manually identified 87 frame classes that seem to be associated with affect: 43 mental classes (e.g., COMMU NICATION and NEEDING ), 22 positive classes (e.g., ACCOMPLISHMENT and SUPPORTING ), and 22 negative classes (e.g., CAUSE HARM and PROHIBITING ). We use the verbs listed for these classes to produce M, +, and - affect states. • MPQA Lexicon (Wilson et al., 2005b): We used the words listed as having positive or negative sentiment polarity to produce +/- states, when they occur with the designated part-of-speech. • OpinionFinder (Wilson et al., 2005a) (Version 1.4) : We used the +/- la"
D10-1008,P08-1090,0,0.0739261,"Missing"
D10-1008,P09-1068,0,0.349667,"Missing"
D10-1008,W06-1651,0,0.0453128,"and SUPPORTING ), and 22 negative classes (e.g., CAUSE HARM and PROHIBITING ). We use the verbs listed for these classes to produce M, +, and - affect states. • MPQA Lexicon (Wilson et al., 2005b): We used the words listed as having positive or negative sentiment polarity to produce +/- states, when they occur with the designated part-of-speech. • OpinionFinder (Wilson et al., 2005a) (Version 1.4) : We used the +/- labels assigned by its contextual polarity classifier (Wilson et al., 2005b) to create +/- states and the MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) to produce mental (M) states. • Semantic Orientation Lexicon (Takamura et al., 2005): We used the words listed as having positive or negative polarity to produce +/- affect states, when they occur with the designated part-of-speech. • Speech Act Verbs: We used 228 speech act verbs from (Wierzbicka, 1987) to produce M states. 4.1.2 Identifying the Characters For the purposes of this work, we made two simplifying assumptions: (1) There are only two characters per fable1 , and (2) Both characters are mentioned in the fable’s title. The problem of coreference resolution for fables is somewhat dif"
D10-1008,E03-1061,0,0.0686685,"Missing"
D10-1008,P97-1023,0,0.04484,"ify verbs that co-occur with these words as probable agents. For each agent term, we applied the pattern “* by [a,an,the] AGENT” and extracted the matching N-grams. Then we applied a part-ofspeech tagger to each N-gram and saved the words that were tagged as verbs (i.e., the words in the * position).3 This process produced 811 negative (evil agent) PPVs and 1362 positive (kind agent) PPVs. 4.2.2 PPV Bootstrapping over Conjunctions Our second approach for acquiring PPVs is based on an observation from sentiment analysis research that conjoined adjectives typically have the same polarity (e.g. (Hatzivassiloglou and McKeown, 1997)). 3 The POS tagging quality is undoubtedly lower than if tagging complete sentences but it seemed reasonable. Our hypothesis was that conjoined verbs often share the same polarity as well (e.g., “abducted and killed” or “rescued and rehabilitated”). We exploit this idea inside a bootstrapping algorithm to iteratively learn verbs that co-occur in conjunctions. Bootstrapping begins with 10 negative and 10 positive PPV seeds. First, we extracted triples of the form “w1 and w2” from the Google Web 1T N -gram corpus that had frequency ≥ 100 and were lower case. We separated each conjunction into t"
D10-1008,W10-0905,0,0.0241283,"Missing"
D10-1008,W06-0301,0,0.0311767,"Missing"
D10-1008,1985.tmi-1.17,0,0.12358,"Missing"
D10-1008,W03-0404,1,0.740643,"RB (“w1”) and a CONTEXT (“and w2”), and created a copy of the conjunction with the roles of w1 and w2 reversed. For example, “rescued and adopted” produces: VERB =“rescued” CONTEXT =“and adopted” VERB =“adopted” CONTEXT =“and rescued” Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. Basilisk identifies semantically similar words based on their co-occurrence with seeds in contextual patterns. Basilisk was originally designed for semantic class induction using lexico-syntactic patterns, but has also been used to learn subjective and objective nouns (Riloff et al., 2003). Basilisk first identifies the pattern contexts that are most strongly associated with the seed words. Words that occur in those contexts are labeled as candidates and scored based on the strength of their contexts. The top 5 candidates are selected and the bootstrapping process repeats. Basilisk produces a lexicon of learned words as well as a ranked list of pattern contexts. Since we bootstrapped over verb conjunctions, we also extracted new PPVs from the contexts. We ran the bootstrapping process to create a lexicon of 500 words, and we collected verbs from the top 500 contexts as well. 5"
D10-1008,C08-1103,0,0.0249138,"Missing"
D10-1008,P05-1017,0,0.15278,"use the verbs listed for these classes to produce M, +, and - affect states. • MPQA Lexicon (Wilson et al., 2005b): We used the words listed as having positive or negative sentiment polarity to produce +/- states, when they occur with the designated part-of-speech. • OpinionFinder (Wilson et al., 2005a) (Version 1.4) : We used the +/- labels assigned by its contextual polarity classifier (Wilson et al., 2005b) to create +/- states and the MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) to produce mental (M) states. • Semantic Orientation Lexicon (Takamura et al., 2005): We used the words listed as having positive or negative polarity to produce +/- affect states, when they occur with the designated part-of-speech. • Speech Act Verbs: We used 228 speech act verbs from (Wierzbicka, 1987) to produce M states. 4.1.2 Identifying the Characters For the purposes of this work, we made two simplifying assumptions: (1) There are only two characters per fable1 , and (2) Both characters are mentioned in the fable’s title. The problem of coreference resolution for fables is somewhat different than for other genres, primarily because the characters are often animals (e.g"
D10-1008,W02-1028,1,0.759532,"atively learn verbs that co-occur in conjunctions. Bootstrapping begins with 10 negative and 10 positive PPV seeds. First, we extracted triples of the form “w1 and w2” from the Google Web 1T N -gram corpus that had frequency ≥ 100 and were lower case. We separated each conjunction into two parts: a primary VERB (“w1”) and a CONTEXT (“and w2”), and created a copy of the conjunction with the roles of w1 and w2 reversed. For example, “rescued and adopted” produces: VERB =“rescued” CONTEXT =“and adopted” VERB =“adopted” CONTEXT =“and rescued” Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. Basilisk identifies semantically similar words based on their co-occurrence with seeds in contextual patterns. Basilisk was originally designed for semantic class induction using lexico-syntactic patterns, but has also been used to learn subjective and objective nouns (Riloff et al., 2003). Basilisk first identifies the pattern contexts that are most strongly associated with the seed words. Words that occur in those contexts are labeled as candidates and scored based on the strength of their contexts. The top 5 candidates are selected and the bootstrapping process repeats. Basi"
D10-1008,H05-2018,1,0.286447,"ave been created for sentiment analysis and other types of semantic knowledge. We considered a wide variety of resources and ultimately decided to experiment with five resources that most closely matched our needs: • FrameNet (Baker et al., 1998): We manually identified 87 frame classes that seem to be associated with affect: 43 mental classes (e.g., COMMU NICATION and NEEDING ), 22 positive classes (e.g., ACCOMPLISHMENT and SUPPORTING ), and 22 negative classes (e.g., CAUSE HARM and PROHIBITING ). We use the verbs listed for these classes to produce M, +, and - affect states. • MPQA Lexicon (Wilson et al., 2005b): We used the words listed as having positive or negative sentiment polarity to produce +/- states, when they occur with the designated part-of-speech. • OpinionFinder (Wilson et al., 2005a) (Version 1.4) : We used the +/- labels assigned by its contextual polarity classifier (Wilson et al., 2005b) to create +/- states and the MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) to produce mental (M) states. • Semantic Orientation Lexicon (Takamura et al., 2005): We used the words listed as having positive or negative polarity to produce +/- affect st"
D10-1008,H05-1044,0,0.168083,"ave been created for sentiment analysis and other types of semantic knowledge. We considered a wide variety of resources and ultimately decided to experiment with five resources that most closely matched our needs: • FrameNet (Baker et al., 1998): We manually identified 87 frame classes that seem to be associated with affect: 43 mental classes (e.g., COMMU NICATION and NEEDING ), 22 positive classes (e.g., ACCOMPLISHMENT and SUPPORTING ), and 22 negative classes (e.g., CAUSE HARM and PROHIBITING ). We use the verbs listed for these classes to produce M, +, and - affect states. • MPQA Lexicon (Wilson et al., 2005b): We used the words listed as having positive or negative sentiment polarity to produce +/- states, when they occur with the designated part-of-speech. • OpinionFinder (Wilson et al., 2005a) (Version 1.4) : We used the +/- labels assigned by its contextual polarity classifier (Wilson et al., 2005b) to create +/- states and the MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) to produce mental (M) states. • Semantic Orientation Lexicon (Takamura et al., 2005): We used the words listed as having positive or negative polarity to produce +/- affect st"
D10-1008,C98-1013,0,\N,Missing
D11-1069,J96-2004,0,0.360933,"Missing"
D11-1069,W06-3406,0,0.0136195,"most of the previous work has focused on email classification. Cohen et al. (2004) introduced the notion of “email speech acts” defined as specific verb-noun pairs following a pre-designed ontology. They approached the problem as a document classification task. Goldstein and Sabin (2006) adopted this notion of email acts (Cohen et al., 2004) but focused on verb lexicons to classify them. Carvalho and Cohen (2005) presented a classification scheme using a dependency network, capturing the sequential correlations with the context emails using transition probabilities from or to a target email. Carvalho and Cohen (2006) later employed N-gram sequence features to determine which N-grams are meaningfully related to different email speech acts with a goal towards improving their earlier email classification based on the writer’s intention. Lampert et al. (2006) performed speech act classification in email messages following a verbal re749 sponse modes (VRM) speech act taxonomy. They also provided a comparison of VRM taxonomy with Searle’s taxonomy (Searle, 1976) of speech act classes. They evaluated several machine learning algorithms using syntactic, morphological, and lexical features. Mildinhall and Noyes (2"
D11-1069,W04-3240,0,0.843751,"Missing"
D11-1069,P10-1029,1,0.759131,"veterinary consultant explained to us that the same term (e.g., diabetes) may be considered a symptom in one context if it is secondary to another condition (e.g., pancreatitis) but a disease in a different context if it is the primary diagnosis. and hypotheses, rather than individual symptoms. In the end, we only used the DRUG and TEST semantic lexicon in our classifiers. We used all 1000 terms in the DRUG lexicon, but only used the top 200 TEST words because the quality of the lexicon seemed questionable after that point. Semantic Tags: We also used bootstrapped contextual semantic taggers (Huang and Riloff, 2010) that had been previously trained for the domain of veterinary medicine. These taggers assign semantic class labels to noun phrase instances based on the surrounding context in a sentence. The taggers were trained on 4,629 veterinary message board posts using 10 seed words for each semantic category (see (Huang and Riloff, 2010) for details). To ensure good precision, only tags that have a confidence value ≥ 1.0 were used. Our speech act classifiers used the tags associated with two semantic categories: DRUG and TEST. 3.3 Classification To create our classifiers, we used the Weka (Hall et al.,"
D11-1069,D09-1130,0,0.0291517,"rsations in instant messages and chat rooms. Nastri et al. (2006) performed an empirical analysis of speech acts in the away messages of instant messenger services to achieve a better understanding of the communication goals of such services. Ravi and Kim (2007) employed speech act profiling in online threaded discussions to determine message roles and to identify threads with questions, answers, and unanswered questions. They designed their own speech act categories based on their analysis of student interactions in discussion threads. The work most closely related to ours is the research of Jeong et al. (2009) on semi-supervised speech act recognition in both emails and forums. Like our work, their research also classifies individual sentences, as opposed to entire documents. However, they trained their classifier on spoken telephone (SWBD-DAMSL corpus) and meeting (MRDA corpus) conversations and mapped the labelled dialog act classes of these corpora to 12 dialog act classes that they found suitable for email and forum text genres. These dialog act classes (addressed as speech acts by them) are somewhat different from Searle’s original speech act classes. They also used substantially different typ"
D11-1069,U06-1007,0,0.38647,"ification task. Goldstein and Sabin (2006) adopted this notion of email acts (Cohen et al., 2004) but focused on verb lexicons to classify them. Carvalho and Cohen (2005) presented a classification scheme using a dependency network, capturing the sequential correlations with the context emails using transition probabilities from or to a target email. Carvalho and Cohen (2006) later employed N-gram sequence features to determine which N-grams are meaningfully related to different email speech acts with a goal towards improving their earlier email classification based on the writer’s intention. Lampert et al. (2006) performed speech act classification in email messages following a verbal re749 sponse modes (VRM) speech act taxonomy. They also provided a comparison of VRM taxonomy with Searle’s taxonomy (Searle, 1976) of speech act classes. They evaluated several machine learning algorithms using syntactic, morphological, and lexical features. Mildinhall and Noyes (2008) presented a stochastic speech act model based on verbal response modes (VRM) to classify email intentions. Some research has considered speech act classes in other means of online conversations. Twitchell and Jr. (2004) and Twitchell et a"
D11-1069,D10-1035,0,0.0468069,"omains. Representative speech acts may involve diagnoses and hypotheses regarding diseases and symptoms. Similarly, Commissive speech acts may reveal a doctor’s plan or intention regarding the administration of drugs or tests. Thus, it may be beneficial for a classifier to know whether a sentence contains certain semantic entities. We experimented with two different sources of semantic information. Semantic Lexicon: Basilisk (Thelen and Riloff, 2002) is a bootstrapping algorithm that has been used to induce semantic lexicons for terrorist events (Thelen and Riloff, 2002), biomedical concepts (McIntosh, 2010), and subjective/objective nouns for opinion analysis (Riloff et al., 2003). We ran Basilisk over our collection of 15,383 veterinary message board posts to create a semantic lexicon for veterinary medicine. As input, Basilisk requires seed words for each semantic category. To obtain seeds, we parsed the corpus using a noun phrase chunker, sorted the head nouns by frequency, and manually identified the 20 most frequent nouns belonging to four semantic categories: DISEASE / SYMPTOM , DRUG , TEST , and TREATMENT . However, the induced TREATMENT lexicon was of relatively poor quality so we did no"
D11-1069,W03-0404,1,0.805186,"Missing"
D11-1069,W02-1028,1,0.693677,"sentences across many domains. However, we hypothesized that semantic entities may correlate with speech acts within a particular domain. For example, consider medical domains. Representative speech acts may involve diagnoses and hypotheses regarding diseases and symptoms. Similarly, Commissive speech acts may reveal a doctor’s plan or intention regarding the administration of drugs or tests. Thus, it may be beneficial for a classifier to know whether a sentence contains certain semantic entities. We experimented with two different sources of semantic information. Semantic Lexicon: Basilisk (Thelen and Riloff, 2002) is a bootstrapping algorithm that has been used to induce semantic lexicons for terrorist events (Thelen and Riloff, 2002), biomedical concepts (McIntosh, 2010), and subjective/objective nouns for opinion analysis (Riloff et al., 2003). We ran Basilisk over our collection of 15,383 veterinary message board posts to create a semantic lexicon for veterinary medicine. As input, Basilisk requires seed words for each semantic category. To obtain seeds, we parsed the corpus using a noun phrase chunker, sorted the head nouns by frequency, and manually identified the 20 most frequent nouns belonging"
D11-1069,N03-1033,0,0.00419335,"formation Network (VIN), which is a web site (www.vin.com) for professionals in veterinary medicine. Among other things, VIN hosts message board forums where veterinarians and other veterinary professionals can discuss issues and pose questions to each other. Over half of the small animal veterinarians in the U.S. and Canada use the VIN web site. We obtained 15,383 VIN message board threads representing three topics: cardiology, endocrinology, and feline internal medicine. We did basic cleaning, removing html tags and tokenizing numbers. We then applied the Stanford part-of-speech 753 tagger (Toutanova et al., 2003) to each sentence to obtain part-of-speech tags for the words. For our experiments, we randomly selected 150 message board threads from this collection. Since the goal of our work was to study speech acts in sentences, and not the conversational dialogue between different writers, we used only the initial post of each thread. These 150 message board posts contained a total of 1,956 sentences, with an average of 13.04 sentences per post. In the next section, we explain how we manually annotated each sentence in our data set to create gold standard speech act labels. 4.2 Gold Standard Annotation"
D13-1066,D12-1091,0,0.0212027,"Missing"
D13-1066,W10-2914,0,0.141457,"sions, represented using punctuation and other keyboard characters, to be more predictive of irony1 in contrast to features representing structured linguistic knowledge in Por1 They adopted the term ‘irony’ instead of ‘sarcasm’ to refer to the case when a word or expression with prior positive polarity is figuratively used to express a negative opinion. 705 tuguese. Filatova (2012) presented a detailed description of sarcasm corpus creation with sarcasm annotations of Amazon product reviews. Their annotations capture sarcasm both at the document level and the text utterance level. Tsur et al. (2010) presented a semi-supervised learning framework that exploits syntactic and pattern based features in sarcastic sentences of Amazon product reviews. They observed correlated sentiment words such as “yay!” or “great!” often occurring in their most useful patterns. Davidov et al. (2010) used sarcastic tweets and sarcastic Amazon product reviews to train a sarcasm classifier with syntactic and pattern-based features. They examined whether tweets with a sarcasm hashtag are reliable enough indicators of sarcasm to be used as a gold standard for evaluation, but found that sarcasm hashtags are noisy"
D13-1066,filatova-2012-irony,0,0.509927,"ial dialogue to learn lexical N-gram cues associated with sarcasm (e.g., “oh really”, “I get it”, “no way”, etc.) as well as lexico-syntactic patterns. In opinionated user posts, Carvalho et al. (2009) found oral or gestural expressions, represented using punctuation and other keyboard characters, to be more predictive of irony1 in contrast to features representing structured linguistic knowledge in Por1 They adopted the term ‘irony’ instead of ‘sarcasm’ to refer to the case when a word or expression with prior positive polarity is figuratively used to express a negative opinion. 705 tuguese. Filatova (2012) presented a detailed description of sarcasm corpus creation with sarcasm annotations of Amazon product reviews. Their annotations capture sarcasm both at the document level and the text utterance level. Tsur et al. (2010) presented a semi-supervised learning framework that exploits syntactic and pattern based features in sarcastic sentences of Amazon product reviews. They observed correlated sentiment words such as “yay!” or “great!” often occurring in their most useful patterns. Davidov et al. (2010) used sarcastic tweets and sarcastic Amazon product reviews to train a sarcasm classifier wit"
D13-1066,P11-2102,0,0.64887,"c sentences of Amazon product reviews. They observed correlated sentiment words such as “yay!” or “great!” often occurring in their most useful patterns. Davidov et al. (2010) used sarcastic tweets and sarcastic Amazon product reviews to train a sarcasm classifier with syntactic and pattern-based features. They examined whether tweets with a sarcasm hashtag are reliable enough indicators of sarcasm to be used as a gold standard for evaluation, but found that sarcasm hashtags are noisy and possibly biased towards the hardest form of sarcasm (where even humans have difficulty). Gonz´alez-Ib´an˜ ez et al. (2011) explored the usefulness of lexical and pragmatic features for sarcasm detection in tweets. They used sarcasm hashtags as gold labels. They found positive and negative emotions in tweets, determined through fixed word dictionaries, to have a strong correlation with sarcasm. Liebrecht et al. (2013) explored Ngram features from 1 to 3-grams to build a classifier to recognize sarcasm in Dutch tweets. They made an interesting observation from their most effective Ngram features that people tend to be more sarcastic towards specific topics such as school, homework, weather, returning from vacation,"
D13-1066,W07-0101,0,0.680534,"follow a positive sentiment (initially, the seed word “love”). Second, we learn positive sentiment phrases that occur near a negative situation phrase. The bootstrapping process iterates, alternately learning new negative situations and new positive sentiment phrases. Finally, we use the learned lists of sentiment and situation phrases to recognize sarcasm in new tweets by identifying contexts that contain a positive sentiment in close proximity to a negative situation phrase. 2 Related Work Researchers have investigated the use of lexical and syntactic features to recognize sarcasm in text. Kreuz and Caucci (2007) studied the role that different lexical factors play, such as interjections (e.g., “gee” or “gosh”) and punctuation symbols (e.g., ‘?’) in recognizing sarcasm in narratives. Lukin and Walker (2013) explored the potential of a bootstrapping method for sarcasm classification in social dialogue to learn lexical N-gram cues associated with sarcasm (e.g., “oh really”, “I get it”, “no way”, etc.) as well as lexico-syntactic patterns. In opinionated user posts, Carvalho et al. (2009) found oral or gestural expressions, represented using punctuation and other keyboard characters, to be more predictiv"
D13-1066,W13-1605,0,0.583115,"Missing"
D13-1066,W13-1104,0,0.489692,"ly learning new negative situations and new positive sentiment phrases. Finally, we use the learned lists of sentiment and situation phrases to recognize sarcasm in new tweets by identifying contexts that contain a positive sentiment in close proximity to a negative situation phrase. 2 Related Work Researchers have investigated the use of lexical and syntactic features to recognize sarcasm in text. Kreuz and Caucci (2007) studied the role that different lexical factors play, such as interjections (e.g., “gee” or “gosh”) and punctuation symbols (e.g., ‘?’) in recognizing sarcasm in narratives. Lukin and Walker (2013) explored the potential of a bootstrapping method for sarcasm classification in social dialogue to learn lexical N-gram cues associated with sarcasm (e.g., “oh really”, “I get it”, “no way”, etc.) as well as lexico-syntactic patterns. In opinionated user posts, Carvalho et al. (2009) found oral or gestural expressions, represented using punctuation and other keyboard characters, to be more predictive of irony1 in contrast to features representing structured linguistic knowledge in Por1 They adopted the term ‘irony’ instead of ‘sarcasm’ to refer to the case when a word or expression with prior"
D13-1066,N13-1039,0,0.209974,"egative situations and new positive sentiment phrases. Finally, we use the learned lists of sentiment and situation phrases to recognize sarcasm in new tweets by identifying contexts that contain a positive sentiment in close proximity to a negative situation phrase. 2 Related Work Researchers have investigated the use of lexical and syntactic features to recognize sarcasm in text. Kreuz and Caucci (2007) studied the role that different lexical factors play, such as interjections (e.g., “gee” or “gosh”) and punctuation symbols (e.g., ‘?’) in recognizing sarcasm in narratives. Lukin and Walker (2013) explored the potential of a bootstrapping method for sarcasm classification in social dialogue to learn lexical N-gram cues associated with sarcasm (e.g., “oh really”, “I get it”, “no way”, etc.) as well as lexico-syntactic patterns. In opinionated user posts, Carvalho et al. (2009) found oral or gestural expressions, represented using punctuation and other keyboard characters, to be more predictive of irony1 in contrast to features representing structured linguistic knowledge in Por1 They adopted the term ‘irony’ instead of ‘sarcasm’ to refer to the case when a word or expression with prior"
D13-1066,N13-1059,0,0.0107603,"that the negative situation phrase is preceded by a positive sentiment. We harvest the n-grams that precede the negative situation phrases as positive sentiment candidates, score and select the best candidates, and add them to a list of positive sentiment phrases. The bootstrapping process then iterates, alternately learning more positive sentiment phrases and more negative situation phrases. We also observed that positive sentiments are frequently expressed as predicative phrases (i.e., predicate adjectives and predicate nominals). For example: “I’m taking calculus. It is awesome. #sarcasm”. Wiegand et al. (2013) offered a related observation that adjectives occurring in predicate adjective constructions are more likely to convey subjectivity than adjectives occurring in non-predicative structures. Therefore we also include a step in the learning process to harvest predicative phrases that occur in close proximity to a negative situation phrase. In the following sections, we explain each step of the bootstrapping process in more detail. 3.4 3.2 We extract three n-grams as candidate negative situation phrases: Bootstrapping Data For the learning process, we used Twitter’s streaming API to obtain a larg"
D13-1066,H05-2018,1,0.557344,"Missing"
D13-1066,H05-1044,0,0.175212,"Missing"
D13-1162,D12-1091,0,0.0223871,"Missing"
D13-1162,P08-1119,1,0.790288,"nt-based text classification (Riloff and Lehnert, 1994) and web page classification (Furnkranz et al., 1998). Semantic information has also been incorporated for text classification. However, most previous work relies on existing semantic resources, such as Wordnet (Scott and Stan, 1998; Bloehdorn and Hotho, 2006) or Wikipedia (Wang et al., 2009). There is also a rich history of automatic lexicon induction from text corpora (e.g., (Roark and Charniak, 1998; Riloff and Jones, 1999; McIntosh and Curran, 2009)), Wikipedia (e.g., (Vyas and Pantel, 2009)), and the Web (e.g., (Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2010)). The novel aspects of our work are in using an IE tagger to harvest a domain-specific lexicon from unannotated texts, and using the induced lexicon to encode domain-specific features for text classification. existing resources such as Wordnet (Miller, 1990), our veterinary message board posts are filled with informal and unconventional vocabulary. For example, one might naively assume that “male” and “female” are sufficient to identify gender. But the gender of animals is often revealed by describing their spayed/neutered status, often indicated with shorthand notation"
D13-1162,P09-1045,0,0.0152593,"Lodhi et al., 2001). Information extraction techniques have been used previously to create richer features for event-based text classification (Riloff and Lehnert, 1994) and web page classification (Furnkranz et al., 1998). Semantic information has also been incorporated for text classification. However, most previous work relies on existing semantic resources, such as Wordnet (Scott and Stan, 1998; Bloehdorn and Hotho, 2006) or Wikipedia (Wang et al., 2009). There is also a rich history of automatic lexicon induction from text corpora (e.g., (Roark and Charniak, 1998; Riloff and Jones, 1999; McIntosh and Curran, 2009)), Wikipedia (e.g., (Vyas and Pantel, 2009)), and the Web (e.g., (Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2010)). The novel aspects of our work are in using an IE tagger to harvest a domain-specific lexicon from unannotated texts, and using the induced lexicon to encode domain-specific features for text classification. existing resources such as Wordnet (Miller, 1990), our veterinary message board posts are filled with informal and unconventional vocabulary. For example, one might naively assume that “male” and “female” are sufficient to identify gender. But the gender of"
D13-1162,P98-2182,0,0.0992692,"ation (LDA) (Br et al., 2008) and string kernels (Lodhi et al., 2001). Information extraction techniques have been used previously to create richer features for event-based text classification (Riloff and Lehnert, 1994) and web page classification (Furnkranz et al., 1998). Semantic information has also been incorporated for text classification. However, most previous work relies on existing semantic resources, such as Wordnet (Scott and Stan, 1998; Bloehdorn and Hotho, 2006) or Wikipedia (Wang et al., 2009). There is also a rich history of automatic lexicon induction from text corpora (e.g., (Roark and Charniak, 1998; Riloff and Jones, 1999; McIntosh and Curran, 2009)), Wikipedia (e.g., (Vyas and Pantel, 2009)), and the Web (e.g., (Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2010)). The novel aspects of our work are in using an IE tagger to harvest a domain-specific lexicon from unannotated texts, and using the induced lexicon to encode domain-specific features for text classification. existing resources such as Wordnet (Miller, 1990), our veterinary message board posts are filled with informal and unconventional vocabulary. For example, one might naively assume that “male” and “female” a"
D13-1162,W98-0706,0,0.0998684,"d features. Researchers have also investigated clustering (Baker and McCallum, 1998), Latent Semantic Indexing (LSI) (Zelikovitz and Hirsh, 2001), Latent Dirichlet Allocation (LDA) (Br et al., 2008) and string kernels (Lodhi et al., 2001). Information extraction techniques have been used previously to create richer features for event-based text classification (Riloff and Lehnert, 1994) and web page classification (Furnkranz et al., 1998). Semantic information has also been incorporated for text classification. However, most previous work relies on existing semantic resources, such as Wordnet (Scott and Stan, 1998; Bloehdorn and Hotho, 2006) or Wikipedia (Wang et al., 2009). There is also a rich history of automatic lexicon induction from text corpora (e.g., (Roark and Charniak, 1998; Riloff and Jones, 1999; McIntosh and Curran, 2009)), Wikipedia (e.g., (Vyas and Pantel, 2009)), and the Web (e.g., (Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2010)). The novel aspects of our work are in using an IE tagger to harvest a domain-specific lexicon from unannotated texts, and using the induced lexicon to encode domain-specific features for text classification. existing resources such as Wordne"
D13-1162,N09-1033,0,0.0136629,"hniques have been used previously to create richer features for event-based text classification (Riloff and Lehnert, 1994) and web page classification (Furnkranz et al., 1998). Semantic information has also been incorporated for text classification. However, most previous work relies on existing semantic resources, such as Wordnet (Scott and Stan, 1998; Bloehdorn and Hotho, 2006) or Wikipedia (Wang et al., 2009). There is also a rich history of automatic lexicon induction from text corpora (e.g., (Roark and Charniak, 1998; Riloff and Jones, 1999; McIntosh and Curran, 2009)), Wikipedia (e.g., (Vyas and Pantel, 2009)), and the Web (e.g., (Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2010)). The novel aspects of our work are in using an IE tagger to harvest a domain-specific lexicon from unannotated texts, and using the induced lexicon to encode domain-specific features for text classification. existing resources such as Wordnet (Miller, 1990), our veterinary message board posts are filled with informal and unconventional vocabulary. For example, one might naively assume that “male” and “female” are sufficient to identify gender. But the gender of animals is often revealed by describing the"
D13-1162,C98-2177,0,\N,Missing
D14-1127,C10-2005,0,0.0279236,"shtag and use it to recognize emotion in the body of tweets. However, unlike hashtags, which are selfcontained, the words surrounding a phrase in a tweet must also be considered. For example, negation can toggle polarity (“don’t love life” may suggest S AD NESS, not J OY ) and the aspectual context may indicate that no emotion is being expressed (e.g., “I would love life if ...”). Consequently, we train classifiers to determine if a tweet contains an emotion based on both an emotion phrase and its context. 2 Related Work In addition to sentiment analysis, which has been widely studied (e.g., (Barbosa and Feng, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011; Mitchell et al., 2013)), recognizing emotions in social media text has also become a popular research topic in recent years. Researchers have studied feature sets and linguistic styles (Roberts et al., 2012), emotion influencing behaviors (Kim et al., 2012), sentence contexts (Yang et al., 2007b), hierarchical emotion classification (Ghazi et al., 2010; Esmin et al., 2012) and emotion lexicon creation (Yang et al., 2007a; Mohammad, 2012a; Staiano and Guerini, 2014). Researchers have also started to utilize the hashtags of tweets, but prim"
D14-1127,D11-1052,0,0.0108301,"gnize emotion in the body of tweets. However, unlike hashtags, which are selfcontained, the words surrounding a phrase in a tweet must also be considered. For example, negation can toggle polarity (“don’t love life” may suggest S AD NESS, not J OY ) and the aspectual context may indicate that no emotion is being expressed (e.g., “I would love life if ...”). Consequently, we train classifiers to determine if a tweet contains an emotion based on both an emotion phrase and its context. 2 Related Work In addition to sentiment analysis, which has been widely studied (e.g., (Barbosa and Feng, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011; Mitchell et al., 2013)), recognizing emotions in social media text has also become a popular research topic in recent years. Researchers have studied feature sets and linguistic styles (Roberts et al., 2012), emotion influencing behaviors (Kim et al., 2012), sentence contexts (Yang et al., 2007b), hierarchical emotion classification (Ghazi et al., 2010; Esmin et al., 2012) and emotion lexicon creation (Yang et al., 2007a; Mohammad, 2012a; Staiano and Guerini, 2014). Researchers have also started to utilize the hashtags of tweets, but primarily to collect labeled data"
D14-1127,J96-2004,0,0.0571017,"Missing"
D14-1127,W10-2914,0,0.0759725,"Missing"
D14-1127,W10-0217,0,0.0210875,"iers to determine if a tweet contains an emotion based on both an emotion phrase and its context. 2 Related Work In addition to sentiment analysis, which has been widely studied (e.g., (Barbosa and Feng, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011; Mitchell et al., 2013)), recognizing emotions in social media text has also become a popular research topic in recent years. Researchers have studied feature sets and linguistic styles (Roberts et al., 2012), emotion influencing behaviors (Kim et al., 2012), sentence contexts (Yang et al., 2007b), hierarchical emotion classification (Ghazi et al., 2010; Esmin et al., 2012) and emotion lexicon creation (Yang et al., 2007a; Mohammad, 2012a; Staiano and Guerini, 2014). Researchers have also started to utilize the hashtags of tweets, but primarily to collect labeled data (e.g., for sarcasm (Davi1203 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203–1209, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Bootstrapped Learning. (HT = hashtag; HP = hashtag pattern) Emotion Classes A FFECTION dov et al., 2010; Riloff et al., 2013) and for sentiment/e"
D14-1127,D13-1171,0,0.0118917,"Missing"
D14-1127,S13-2053,0,0.0349948,"n lexicon creation (Yang et al., 2007a; Mohammad, 2012a; Staiano and Guerini, 2014). Researchers have also started to utilize the hashtags of tweets, but primarily to collect labeled data (e.g., for sarcasm (Davi1203 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203–1209, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Bootstrapped Learning. (HT = hashtag; HP = hashtag pattern) Emotion Classes A FFECTION dov et al., 2010; Riloff et al., 2013) and for sentiment/emotion data (Wang et al., 2012; Mohammad et al., 2013; Choudhury et al., 2012; Purver and Battersby, 2012; Mohammad, 2012a)). Wang et al. (2011) investigated several graph based algorithms to collectively classify hashtag sentiments, but their work is focused on positive versus negative polarity classification. Our research extends the preliminary work on bootstrapped learning of emotion hashtags (Qadir and Riloff, 2013) to additionally learn patterns corresponding to hashtag prefix expressions and to extract emotion phrases from the hashtags, which are used to train phrase-based emotion classifiers. 3 Learning Emotion Hashtags, Hashtag Patterns"
D14-1127,S12-1033,0,0.195942,"ontext. 2 Related Work In addition to sentiment analysis, which has been widely studied (e.g., (Barbosa and Feng, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011; Mitchell et al., 2013)), recognizing emotions in social media text has also become a popular research topic in recent years. Researchers have studied feature sets and linguistic styles (Roberts et al., 2012), emotion influencing behaviors (Kim et al., 2012), sentence contexts (Yang et al., 2007b), hierarchical emotion classification (Ghazi et al., 2010; Esmin et al., 2012) and emotion lexicon creation (Yang et al., 2007a; Mohammad, 2012a; Staiano and Guerini, 2014). Researchers have also started to utilize the hashtags of tweets, but primarily to collect labeled data (e.g., for sarcasm (Davi1203 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203–1209, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Bootstrapped Learning. (HT = hashtag; HP = hashtag pattern) Emotion Classes A FFECTION dov et al., 2010; Riloff et al., 2013) and for sentiment/emotion data (Wang et al., 2012; Mohammad et al., 2013; Choudhury et al., 2012; Purver"
D14-1127,N12-1071,0,0.192676,"ontext. 2 Related Work In addition to sentiment analysis, which has been widely studied (e.g., (Barbosa and Feng, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011; Mitchell et al., 2013)), recognizing emotions in social media text has also become a popular research topic in recent years. Researchers have studied feature sets and linguistic styles (Roberts et al., 2012), emotion influencing behaviors (Kim et al., 2012), sentence contexts (Yang et al., 2007b), hierarchical emotion classification (Ghazi et al., 2010; Esmin et al., 2012) and emotion lexicon creation (Yang et al., 2007a; Mohammad, 2012a; Staiano and Guerini, 2014). Researchers have also started to utilize the hashtags of tweets, but primarily to collect labeled data (e.g., for sarcasm (Davi1203 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203–1209, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Bootstrapped Learning. (HT = hashtag; HP = hashtag pattern) Emotion Classes A FFECTION dov et al., 2010; Riloff et al., 2013) and for sentiment/emotion data (Wang et al., 2012; Mohammad et al., 2013; Choudhury et al., 2012; Purver"
D14-1127,N13-1039,0,0.0239171,"Missing"
D14-1127,E12-1049,0,0.0476478,"d, 2012a; Staiano and Guerini, 2014). Researchers have also started to utilize the hashtags of tweets, but primarily to collect labeled data (e.g., for sarcasm (Davi1203 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203–1209, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Bootstrapped Learning. (HT = hashtag; HP = hashtag pattern) Emotion Classes A FFECTION dov et al., 2010; Riloff et al., 2013) and for sentiment/emotion data (Wang et al., 2012; Mohammad et al., 2013; Choudhury et al., 2012; Purver and Battersby, 2012; Mohammad, 2012a)). Wang et al. (2011) investigated several graph based algorithms to collectively classify hashtag sentiments, but their work is focused on positive versus negative polarity classification. Our research extends the preliminary work on bootstrapped learning of emotion hashtags (Qadir and Riloff, 2013) to additionally learn patterns corresponding to hashtag prefix expressions and to extract emotion phrases from the hashtags, which are used to train phrase-based emotion classifiers. 3 Learning Emotion Hashtags, Hashtag Patterns and Phrases For our research, we collapsed Parrot’s"
D14-1127,W13-1602,1,0.459676,"4 Association for Computational Linguistics Figure 1: Bootstrapped Learning. (HT = hashtag; HP = hashtag pattern) Emotion Classes A FFECTION dov et al., 2010; Riloff et al., 2013) and for sentiment/emotion data (Wang et al., 2012; Mohammad et al., 2013; Choudhury et al., 2012; Purver and Battersby, 2012; Mohammad, 2012a)). Wang et al. (2011) investigated several graph based algorithms to collectively classify hashtag sentiments, but their work is focused on positive versus negative polarity classification. Our research extends the preliminary work on bootstrapped learning of emotion hashtags (Qadir and Riloff, 2013) to additionally learn patterns corresponding to hashtag prefix expressions and to extract emotion phrases from the hashtags, which are used to train phrase-based emotion classifiers. 3 Learning Emotion Hashtags, Hashtag Patterns and Phrases For our research, we collapsed Parrot’s emotion taxonomy (Parrott, 2001)1 into 5 emotion classes that frequently occur in tweets and minimally overlap with each other: A FFECTION, A NGER /R AGE, F EAR /A NXIETY, J OY, and S AD NESS /D ISAPPOINTMENT. We also used a N ONE OF THE A BOVE class for tweets that do not express any emotion or express an emotion di"
D14-1127,D13-1066,1,0.200569,"Missing"
D14-1127,roberts-etal-2012-empatweet,0,0.0458455,"ot J OY ) and the aspectual context may indicate that no emotion is being expressed (e.g., “I would love life if ...”). Consequently, we train classifiers to determine if a tweet contains an emotion based on both an emotion phrase and its context. 2 Related Work In addition to sentiment analysis, which has been widely studied (e.g., (Barbosa and Feng, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011; Mitchell et al., 2013)), recognizing emotions in social media text has also become a popular research topic in recent years. Researchers have studied feature sets and linguistic styles (Roberts et al., 2012), emotion influencing behaviors (Kim et al., 2012), sentence contexts (Yang et al., 2007b), hierarchical emotion classification (Ghazi et al., 2010; Esmin et al., 2012) and emotion lexicon creation (Yang et al., 2007a; Mohammad, 2012a; Staiano and Guerini, 2014). Researchers have also started to utilize the hashtags of tweets, but primarily to collect labeled data (e.g., for sarcasm (Davi1203 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203–1209, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1"
D14-1127,P14-2070,0,0.051536,"Work In addition to sentiment analysis, which has been widely studied (e.g., (Barbosa and Feng, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011; Mitchell et al., 2013)), recognizing emotions in social media text has also become a popular research topic in recent years. Researchers have studied feature sets and linguistic styles (Roberts et al., 2012), emotion influencing behaviors (Kim et al., 2012), sentence contexts (Yang et al., 2007b), hierarchical emotion classification (Ghazi et al., 2010; Esmin et al., 2012) and emotion lexicon creation (Yang et al., 2007a; Mohammad, 2012a; Staiano and Guerini, 2014). Researchers have also started to utilize the hashtags of tweets, but primarily to collect labeled data (e.g., for sarcasm (Davi1203 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203–1209, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Bootstrapped Learning. (HT = hashtag; HP = hashtag pattern) Emotion Classes A FFECTION dov et al., 2010; Riloff et al., 2013) and for sentiment/emotion data (Wang et al., 2012; Mohammad et al., 2013; Choudhury et al., 2012; Purver and Battersby, 2012; Mohammad"
D14-1127,S07-1013,0,0.115934,"Missing"
D14-1127,P07-2034,0,0.0143241,"I would love life if ...”). Consequently, we train classifiers to determine if a tweet contains an emotion based on both an emotion phrase and its context. 2 Related Work In addition to sentiment analysis, which has been widely studied (e.g., (Barbosa and Feng, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011; Mitchell et al., 2013)), recognizing emotions in social media text has also become a popular research topic in recent years. Researchers have studied feature sets and linguistic styles (Roberts et al., 2012), emotion influencing behaviors (Kim et al., 2012), sentence contexts (Yang et al., 2007b), hierarchical emotion classification (Ghazi et al., 2010; Esmin et al., 2012) and emotion lexicon creation (Yang et al., 2007a; Mohammad, 2012a; Staiano and Guerini, 2014). Researchers have also started to utilize the hashtags of tweets, but primarily to collect labeled data (e.g., for sarcasm (Davi1203 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203–1209, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Bootstrapped Learning. (HT = hashtag; HP = hashtag pattern) Emotion Classes A FFECTION"
D14-1127,N03-1031,0,\N,Missing
D14-1127,L12-1000,0,\N,Missing
D15-1019,P11-1016,0,0.0254261,"d “gift box” may represent two different concepts with different polarities in similes. We replaced personal pronouns (e.g., he, she) with a general PERSON token and other pronouns (e.g., it, this, that) with a general IT token. Table 2 presents examples of positive and negative similes in the annotated data set. timent, research has also focused on understanding people’s sentiment during specific events such as stock market fluctuations, presidential elections, Oscars, tsunamis, or toward entities such as movies, companies, or aspects of a product (Bollen et al., 2011; Thelwall et al., 2011; Jiang et al., 2011; Hu and Liu, 2004; Jo and Oh, 2011). To our knowledge, we are the first to explore recognition of affective polarity in similes as a whole, where the polarity relates to an act or state of the tenor. Unlike previous work, we do not rely on the presence of explicit properties. We also present a data set annotated with affective polarity in similes, and experiment with both manually annotated and automatically acquired training data. 3 Simile Data Set Creation One of the major challenges of supervised classification is acquiring sufficient labeled data for training, since manual annotation is t"
D15-1019,J96-2004,0,0.206256,"ire clause in place of the vehicle (e.g., “I feel like im gonna puke”). Other times, the informal text of Twitter makes the tweet hard to parse (e.g., “he is like whatttt”) or a verb occurs after “like” (e.g., “he is like hyperventilating”). The invalid label covers these types of erroneously extracted similes. The annotation task was first conducted on a small sample of 50 similes, to select workers that had high annotation agreement with each other and gold standard labels we prepared. The best three workers then all annotated the official set of 1500 similes. The average Cohen’s Kappa (κ) (Carletta, 1996) between each pair of annotators was 0.69. We then assigned the final label through majority vote. However, none of the annotators agreed on the same label for 78 of the 1500 similes, and 303 instances were labeled as invalid similes by the annotators. So we removed these 381 instances from the annotated data set. Finally, we randomly divided the remaining similes into an evaluation (Eval) set of 741 similes, and a development (Dev) set of 378 similes. Table 3 shows the label distribution of these sets. 3.2 # of Similes (Dev Data) 164 181 33 378 # of Similes (Eval Data) 312 343 86 741 Table 3:"
D15-1019,C10-2028,0,0.0318407,"Li et al. (2012) used similar patterns to retrieve similes and determine basic sentiment toward simile vehicles across different languages using the compared properties. One major difference with their work and ours is that they determine sentiment or affective perception toward entities or concepts extracted from simile vehicles. In contrast, our work is focused Our work is also related to sentiment analysis in general. The most common approach applies supervised classification with features such as ngrams, parts-of-speech, punctuation, lexicon features, etc. (e.g., (Kouloumpis et al., 2011; Davidov et al., 2010; Mohammad et al., 2013)). To overcome the challenge of acquiring manually labeled data, some work automatically collects noisy training data using emoticons and hashtags (e.g., (Go et al., 2009; Purver and Battersby, 2012)). In addition to determining overall sen191 longer tweet, and repeated the process. We used the UIUC Chunker (Punyakanok and Roth, 2001) to identify phrase sequences with the syntax of similes (e.g., NP1 + VP + PP-like + NP2 , or NP1 + VP + ADJP + PP-like + NP2 , where NP1 is the tenor, NP2 is the vehicle, VP is the event, and ADJP is any explicitly mentioned property). We"
D15-1019,P14-5010,0,0.0031431,"etermining overall sen191 longer tweet, and repeated the process. We used the UIUC Chunker (Punyakanok and Roth, 2001) to identify phrase sequences with the syntax of similes (e.g., NP1 + VP + PP-like + NP2 , or NP1 + VP + ADJP + PP-like + NP2 , where NP1 is the tenor, NP2 is the vehicle, VP is the event, and ADJP is any explicitly mentioned property). We generalized over the extracted similes by removing the comparator, and the optional explicit property component. Our simile representation is thus a triple of the tenor, event and vehicle. We also lemmatized all words using Stanford CoreNLP (Manning et al., 2014). For a tenor phrase, we kept only the head noun, which is usually sufficient to understand the affective polarity target. We kept the entire noun phrase for the vehicle, since vehicles like “ice box” and “gift box” may represent two different concepts with different polarities in similes. We replaced personal pronouns (e.g., he, she) with a general PERSON token and other pronouns (e.g., it, this, that) with a general IT token. Table 2 presents examples of positive and negative similes in the annotated data set. timent, research has also focused on understanding people’s sentiment during speci"
D15-1019,P13-1174,0,0.0401469,"tive and negative properties that appear with the vehicle in our corpus. We look for the property words in the combined AFINN and MPQA sentiment lexicons. Sentiment Classifier Label: We use 2 binary features (one for positive and one for negative) to represent the label that the NRC-Canada Sentiment Classifier assigns to a simile. Simile Connotation Polarity: We use 2 binary features (one for positive and one for negative) to indicate the overall connotation of a simile. We count whether the number of positive (or negative) connotation words is greater in a simile using a Connotation Lexicon (Feng et al., 2013), which contains 30,881 words with positive connotation and 33,724 words with negative connotation. 5 5.1 Evaluation Classification Performance with Manually Annotated Data Table 4 presents the results for supervised classification with our manually annotated data set using 10-fold cross-validation. As baselines, we used existing sentiment resources as described in Section 3.2, but now applied to evaluation data. We also used the connotation lexicon (Feng et al., 2013) the same way as the MPQA sentiment lexicon (Wilson et al., 2005) to compare as an additional baseline. The top section of Tabl"
D15-1019,S13-2053,0,0.0221242,"similar patterns to retrieve similes and determine basic sentiment toward simile vehicles across different languages using the compared properties. One major difference with their work and ours is that they determine sentiment or affective perception toward entities or concepts extracted from simile vehicles. In contrast, our work is focused Our work is also related to sentiment analysis in general. The most common approach applies supervised classification with features such as ngrams, parts-of-speech, punctuation, lexicon features, etc. (e.g., (Kouloumpis et al., 2011; Davidov et al., 2010; Mohammad et al., 2013)). To overcome the challenge of acquiring manually labeled data, some work automatically collects noisy training data using emoticons and hashtags (e.g., (Go et al., 2009; Purver and Battersby, 2012)). In addition to determining overall sen191 longer tweet, and repeated the process. We used the UIUC Chunker (Punyakanok and Roth, 2001) to identify phrase sequences with the syntax of similes (e.g., NP1 + VP + PP-like + NP2 , or NP1 + VP + ADJP + PP-like + NP2 , where NP1 is the tenor, NP2 is the vehicle, VP is the event, and ADJP is any explicitly mentioned property). We generalized over the ext"
D15-1019,S15-2080,0,0.0421555,"d sentiment expressed through metaphor. Rumbell et al. (2008) presented an analysis of animals that are metaphorically used to describe a person. Rentoumi et al. (2009) determined use of figurative language by disambiguating word senses, and then determined sentiment polarity at the sense level using ngram graph similarity. Wallington et al. (2011) identified affect in metaphor and similes when a comparison is made with an animal (e.g., dog, fox) or mythical creature (e.g., dragon, angel) by analyzing WordNet sense glosses of the compared terms. More recently, the SemEval-2015 Shared Task 11 (Ghosh et al., 2015) has addressed the sentiment analysis of figurative language such as irony, metaphor and sarcasm in Twitter. Niculae and Danescu-Niculescu-Mizil (2014) created a simile data set from Amazon product reviews, and determined when comparisons are figurative. They did not identify affective polarity, but showed that sentiment and figurative comparisons are correlated. Fishelov (2007) conducted a study of 16 similes where the connection between tenor and vehicle is obvious or not obvious, and when a conventional or unconventional explicit property is present or absent. Fishelov analyzed responses fr"
D15-1019,S13-2052,0,0.0426472,"itive/negative lexicon words. This method yields 629 positive and 522 negative similes. Using Sentiment Classifiers: We create our third training data set using a state-of-the-art sentiment classifier designed for tweets. For this, we reimplemented the NRC Canada sentiment classifier (Zhu et al., 2014) using the same set of features described by the authors. We use a Java implementation2 of SVM from LIBLINEAR (Fan et al., 2008), with the original parameter values used by the NRC Canada system. We trained the sentiment classifier with all of the tweet training data from SemEval 2013 subtask B (Nakov et al., 2013). We label a simile as positive or negative if the sentiment classifier labels it as positive or negative, respectively. This method yields 1185 positive and 402 negative similes. Using Sentiment in Surrounding Words: The previous approaches for labeling training instances will primarily identify similes that contain one or more strongly affective words. This Automatically Labeled Similes For any new domain (e.g., Amazon product reviews), manual annotations for supervised training 2 193 http://liblinear.bwaldvogel.de/ 4 can potentially bias the training data and limit the classifier’s ability"
D15-1019,D14-1215,0,0.240106,"nce (Li et al., 2012; Fishelov, 2007). Sometimes, the sentiment of a simile is expressed explicitly, such as “Jane swims beautifully like a dolphin!”. But in many cases the sentiment is implicit, evoked entirely from the comparison itself. “Jane swims like a dolphin” is easily understood to be a compliment toward Jane’s swimming ability because dolphins are known to be excellent swimmers. A simile consists of four key components: the topic or tenor (subject of the comparison), the vehicle (object of the comparison), the event (act or state), and a comparator (usually “as”, “like”, or “than”) (Niculae and Danescu-Niculescu-Mizil, 2014). A property (shared attribute) can be optionally included as well (e.g., “He is as red as Simile smells like bananas stinks like bananas smells like rotten bananas smells like garbage memory like an elephant memory like a sieve looks like a celebrity acts like a celebrity Polarity neutral negative negative negative positive negative positive negative Table 1: Simile Examples with Affective Polarity. However, the affective polarity of a simile often emerges from multiple component terms. For instance, all of the words in Examples (e) and (f) have neutral polarity. But Example (e) is positive b"
D15-1019,S14-2077,0,0.015085,"raining data set is created using the 2,718 positive words and 4,910 negative words from the MPQA lexicon (Wilson et al., 2005). We applied the CMU part-of-speech tagger for tweets (Owoputi et al., 2013) to match the MPQA parts-of-speech for each word. We assign positive/negative polarity to similes with more positive/negative lexicon words. This method yields 629 positive and 522 negative similes. Using Sentiment Classifiers: We create our third training data set using a state-of-the-art sentiment classifier designed for tweets. For this, we reimplemented the NRC Canada sentiment classifier (Zhu et al., 2014) using the same set of features described by the authors. We use a Java implementation2 of SVM from LIBLINEAR (Fan et al., 2008), with the original parameter values used by the NRC Canada system. We trained the sentiment classifier with all of the tweet training data from SemEval 2013 subtask B (Nakov et al., 2013). We label a simile as positive or negative if the sentiment classifier labels it as positive or negative, respectively. This method yields 1185 positive and 402 negative similes. Using Sentiment in Surrounding Words: The previous approaches for labeling training instances will prima"
D15-1019,N13-1039,0,0.0141311,"exicon (Nielsen, 2011) containing 2,477 manually labeled words with integer values ranging from -5 (negativity) to 5 (positivity). For each simile, we sum the sentiment scores for all lexicon words in the simile components, assigning positive/negative polarity depending on whether the sum is positive/negative. This method yields 460 positive and 423 negative similes. Using MPQA Sentiment Lexicon Words: Our second training data set is created using the 2,718 positive words and 4,910 negative words from the MPQA lexicon (Wilson et al., 2005). We applied the CMU part-of-speech tagger for tweets (Owoputi et al., 2013) to match the MPQA parts-of-speech for each word. We assign positive/negative polarity to similes with more positive/negative lexicon words. This method yields 629 positive and 522 negative similes. Using Sentiment Classifiers: We create our third training data set using a state-of-the-art sentiment classifier designed for tweets. For this, we reimplemented the NRC Canada sentiment classifier (Zhu et al., 2014) using the same set of features described by the authors. We use a Java implementation2 of SVM from LIBLINEAR (Fan et al., 2008), with the original parameter values used by the NRC Canad"
D15-1019,E12-1049,0,0.0265542,"is that they determine sentiment or affective perception toward entities or concepts extracted from simile vehicles. In contrast, our work is focused Our work is also related to sentiment analysis in general. The most common approach applies supervised classification with features such as ngrams, parts-of-speech, punctuation, lexicon features, etc. (e.g., (Kouloumpis et al., 2011; Davidov et al., 2010; Mohammad et al., 2013)). To overcome the challenge of acquiring manually labeled data, some work automatically collects noisy training data using emoticons and hashtags (e.g., (Go et al., 2009; Purver and Battersby, 2012)). In addition to determining overall sen191 longer tweet, and repeated the process. We used the UIUC Chunker (Punyakanok and Roth, 2001) to identify phrase sequences with the syntax of similes (e.g., NP1 + VP + PP-like + NP2 , or NP1 + VP + ADJP + PP-like + NP2 , where NP1 is the tenor, NP2 is the vehicle, VP is the event, and ADJP is any explicitly mentioned property). We generalized over the extracted similes by removing the comparator, and the optional explicit property component. Our simile representation is thus a triple of the tenor, event and vehicle. We also lemmatized all words using"
D15-1019,R09-1067,0,0.0712736,"for different domains. on determining affective polarity of a simile as a whole, where the affective polarity typically relates to an act or state of the tenor. In many cases, a simile vehicle does not have positive or negative polarity by itself. For example, “sauna” is not a positive or negative concept, but “room feels like a sauna” is a negative simile because it suggests that the room is humid and unpleasant. 2 Previous research has also explored sentiment expressed through metaphor. Rumbell et al. (2008) presented an analysis of animals that are metaphorically used to describe a person. Rentoumi et al. (2009) determined use of figurative language by disambiguating word senses, and then determined sentiment polarity at the sense level using ngram graph similarity. Wallington et al. (2011) identified affect in metaphor and similes when a comparison is made with an animal (e.g., dog, fox) or mythical creature (e.g., dragon, angel) by analyzing WordNet sense glosses of the compared terms. More recently, the SemEval-2015 Shared Task 11 (Ghosh et al., 2015) has addressed the sentiment analysis of figurative language such as irony, metaphor and sarcasm in Twitter. Niculae and Danescu-Niculescu-Mizil (201"
D15-1019,P12-2015,0,0.197003,"les. Related Work Although similes are a popular form of comparison, there has been relatively little prior research on understanding affective polarity in similes. Veale and Hao (2007) created a large simile case-base using the pattern “as ADJ as a/an NOUN”. They collected similes by querying the web after instantiating part of the pattern with adjectives, and then had a human annotate 30,991 of the extracted similes for validity. Their focus was on extracting salient properties associated with simile vehicles, and the affective perception on vehicles that the salient properties bring about. Veale (2012) took a step further and automatically recognized the affect toward vehicles when properties reinforce each other (e.g., hot and humid). They built a support graph of properties and determined how they connect to unambiguous positive and negative words. Li et al. (2012) used similar patterns to retrieve similes and determine basic sentiment toward simile vehicles across different languages using the compared properties. One major difference with their work and ours is that they determine sentiment or affective perception toward entities or concepts extracted from simile vehicles. In contrast,"
D15-1019,H05-1044,0,0.273041,"n Words: Our first training data set is created using the AFINN sentiment lexicon (Nielsen, 2011) containing 2,477 manually labeled words with integer values ranging from -5 (negativity) to 5 (positivity). For each simile, we sum the sentiment scores for all lexicon words in the simile components, assigning positive/negative polarity depending on whether the sum is positive/negative. This method yields 460 positive and 423 negative similes. Using MPQA Sentiment Lexicon Words: Our second training data set is created using the 2,718 positive words and 4,910 negative words from the MPQA lexicon (Wilson et al., 2005). We applied the CMU part-of-speech tagger for tweets (Owoputi et al., 2013) to match the MPQA parts-of-speech for each word. We assign positive/negative polarity to similes with more positive/negative lexicon words. This method yields 629 positive and 522 negative similes. Using Sentiment Classifiers: We create our third training data set using a state-of-the-art sentiment classifier designed for tweets. For this, we reimplemented the NRC Canada sentiment classifier (Zhu et al., 2014) using the same set of features described by the authors. We use a Java implementation2 of SVM from LIBLINEAR"
D16-1005,W13-3520,0,0.0267057,"r each of the three classes, Past, Ongoing and Future. To alleviate overfitting of the CNN model, we applied dropout (Hinton et al., 2012) on the convolution layer and the following pooling layer with a keeping rate of 0.5. Our experiments used the 300-dimension English word2vec embeddings14 trained on 100 billion words of Google News. We trained our own 300dimension Spanish embeddings, running word2vec (Mikolov et al., 2013) over both Spanish Gigaword (Mendon et al., 2011)— tokenized using Stanford CoreNLP SpanishTokenizer (Manning et al., 2014)— and the pre-tokenized Spanish Wikipedia dump (Al-Rfou et al., 2013). The vectors were then tuned during backpropagation for our specific task. 13 Stanford CoreNLP has no support for generating syntactic dependencies for Spanish. 14 docs.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM. Row 1 2 3 4 5 6 7 8 Method TIPSem TIPSem with transitivity SVM with all features SVM with BOW features only +Tense/Aspect/Time +Governing Word +Future Oriented Lexicon Convolutional Neural Net PA 26/80/39 75/76/75 91/81/86 88/80/84 89/81/85 90/81/85 90/82/86 91/83/87 OG 8/32/13 14/22/17 33/47/39 37/46/41 40/50/44 43/56/48 44/56/49 46/57/51 FU 4/23/7 4/21/7 45/58/51 40/53/45 42/52/"
D16-1005,P98-1013,0,0.625075,"Missing"
D16-1005,bejan-harabagiu-2008-linguistic,0,0.332676,"Missing"
D16-1005,S13-2002,0,0.227721,"at they are much harder to label because tense and aspect are less available than for events realized as finite verbs. Fourth, the EventStatus data set is multilingual: we collected data from both English and Spanish texts, allowing us to compare events representing the same event frame across two languages that are known to differ in their typological properties for describing events (Talmy, 1985). Using the new EventStatus corpus, we investigate two approaches for recognizing the temporal status of events. We create a SVM classifier that incorporates features drawn from prior TempEval work (Bethard, 2013; Chambers et al., 2014; Llorens et al., 2010) as well as a new automatically induced lexicon of 411 English and 348 Spanish “futureoriented” matrix verbs—verbs like “threaten” and “fear” whose complement clause or nominal direct object argument is likely to describe a future event. We show that the SVM outperforms a state-of-theart TempEval system and that the induced lexicon further improves performance for both English and Spanish. We also introduce a Convolutional Neural Network (CNN) to detect the temporal status of events. Our analysis shows that it successfully models semantic compositi"
D16-1005,P07-1073,0,0.0844152,"Missing"
D16-1005,P15-2072,0,0.0288722,"riants. We then randomly selected 2954 and 14915 news stories from the English Gigaword 5th Ed. (Parker et al., 2011) and Spanish Gigaword 3rd Ed. (Mendon et al., 2011) corpora, respectively, that contain at least one civil unrest phrase. Events of a specific type are very sparsely distributed in a large corpus like the Gigaword, so we used keyword matching just as a first pass to identify candidate event mentions. 3 The English keywords are “protest”, “strike”, “march”, “rally”, “riot” and “occupy”. These correspond to the most frequent words in the relevant frame in the Media Frames corpus (Card et al., 2015). Because “march” most commonly refers to the month, we removed the word itself and only kept its other morphological variations. 4 Spanish keywords: “marchar”, “protestar”, “amotinar(se)”, “manifestar(se)”, “huelga”, “manifestaci´on”, “disturbio”, “mot´ın”, “ocupar * la calle”, “tomar * la calle”, “salir * las calles”, “lanzarse a las calles”, “cacerolas vac´ıas”, “cacerolazo”, “cacerolada”. Asterisks could be replaced by up to 4 words. The last three terms are common expressions for protest marches in many countries of Latin America and Spain. 5 46 (out of 3000) and 9 (out of 1500) stories w"
D16-1005,Q14-1022,0,0.0644833,"Missing"
D16-1005,P98-1067,0,0.168207,"Missing"
D16-1005,D14-1181,0,0.00628674,"For the Spanish data, we used Stanford CoreNLP to generate Partof-Speech tags13 and then applied the MaltParser (Nivre et al., 2004) to generate dependencies. 4 Convolutional Neural Network Model Convolutional neural networks (CNNs) have been shown to be effective in modeling natural language semantics (Collobert et al., 2011). We were especially keen to find out whether the convolution operations of CNNs can model the semantic compositionality needed to detect temporal-aspectual status. For our experiments, we trained a simple CNN with one convolution layer followed by one max pooling layer (Kim, 2014; Collobert et al., 2011), The convolution layer has 300 hidden units. In each unit, the same affine transformation is applied to every consecutive 5 words (a filter instance) in the input sequence of words. A different affine transformation is applied to each hidden unit. After each affine transformation, a Rectified Linear Units (ReLU) (Nair and Hinton, 2010) non-linearity is applied. For each hidden unit, the max pooling layer selects the maximum value from the pool of real values generated from each filter instance. After the max pooling layer, a softmax classifier predicts probabilites fo"
D16-1005,S10-1063,0,0.284343,"se tense and aspect are less available than for events realized as finite verbs. Fourth, the EventStatus data set is multilingual: we collected data from both English and Spanish texts, allowing us to compare events representing the same event frame across two languages that are known to differ in their typological properties for describing events (Talmy, 1985). Using the new EventStatus corpus, we investigate two approaches for recognizing the temporal status of events. We create a SVM classifier that incorporates features drawn from prior TempEval work (Bethard, 2013; Chambers et al., 2014; Llorens et al., 2010) as well as a new automatically induced lexicon of 411 English and 348 Spanish “futureoriented” matrix verbs—verbs like “threaten” and “fear” whose complement clause or nominal direct object argument is likely to describe a future event. We show that the SVM outperforms a state-of-theart TempEval system and that the induced lexicon further improves performance for both English and Spanish. We also introduce a Convolutional Neural Network (CNN) to detect the temporal status of events. Our analysis shows that it successfully models semantic compositionality for some challenging temporal contexts"
D16-1005,S15-2134,0,0.0138314,"English and Spanish (Verhagen et al., 2010), and can compute the relation of each event with the Document Creation 50 Time. We applied TIPSem to our test set, mapping the DCT relations to our three event status classes15 . Row 1 of Tables 6 and 7 shows TIPSem results. The columns show results for each category separately, as well as macro-average and microaverage results across the three categories. Each cell shows the Recall/Precision/F-score numbers. Since TIPSem linked relatively few event mentions to the DCT, we next leveraged the transitivity of temporal relations (UzZaman et al., 2012; Llorens et al., 2015), linking an event to a DCT if the temporal relation between another event in the same sentence and the DCT is transferable. For instance, if event A is AFTER its DCT, and event B is AFTER event A, then event B is also AFTER the DCT.16 Row 2 shows the results of TIPSem with temporal transitivity. Even augmented by transitivity, TIPSem fails to detect many Ongoing (OG) and Future (FU) events; most mislabeled OG and FU events were nominal. Confusion matrices (Table 8) show that most of the 15 We used the obvious mappings from TIPSem relations: “BEFORE” to “PA”, “AFTER” to “FU” , and “INCLUDES” ("
D16-1005,P14-5010,0,0.00419871,"the max pooling layer, a softmax classifier predicts probabilites for each of the three classes, Past, Ongoing and Future. To alleviate overfitting of the CNN model, we applied dropout (Hinton et al., 2012) on the convolution layer and the following pooling layer with a keeping rate of 0.5. Our experiments used the 300-dimension English word2vec embeddings14 trained on 100 billion words of Google News. We trained our own 300dimension Spanish embeddings, running word2vec (Mikolov et al., 2013) over both Spanish Gigaword (Mendon et al., 2011)— tokenized using Stanford CoreNLP SpanishTokenizer (Manning et al., 2014)— and the pre-tokenized Spanish Wikipedia dump (Al-Rfou et al., 2013). The vectors were then tuned during backpropagation for our specific task. 13 Stanford CoreNLP has no support for generating syntactic dependencies for Spanish. 14 docs.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM. Row 1 2 3 4 5 6 7 8 Method TIPSem TIPSem with transitivity SVM with all features SVM with BOW features only +Tense/Aspect/Time +Governing Word +Future Oriented Lexicon Convolutional Neural Net PA 26/80/39 75/76/75 91/81/86 88/80/84 89/81/85 90/81/85 90/82/86 91/83/87 OG 8/32/13 14/22/17 33/47/39 37/46/41 40/50/44"
D16-1005,de-marneffe-etal-2006-generating,0,0.174951,"Missing"
D16-1005,N15-2023,0,0.0645627,"Missing"
D16-1005,W04-2407,0,0.0618242,"wo events but not when relating an event to the Document Creation Time, for which tense, aspect, and time expression features were the most useful (Llorens et al., 2010; Bethard, 2013). 12 We did not imitate this procedure for Spanish because the quality of our generated Spanish dependencies is poor. 49 pairs the governing word of an event mention with the dependency relation in between. We used Stanford CoreNLP (Marneffe et al., 2006) to generate dependencies for the English data. For the Spanish data, we used Stanford CoreNLP to generate Partof-Speech tags13 and then applied the MaltParser (Nivre et al., 2004) to generate dependencies. 4 Convolutional Neural Network Model Convolutional neural networks (CNNs) have been shown to be effective in modeling natural language semantics (Collobert et al., 2011). We were especially keen to find out whether the convolution operations of CNNs can model the semantic compositionality needed to detect temporal-aspectual status. For our experiments, we trained a simple CNN with one convolution layer followed by one max pooling layer (Kim, 2014; Collobert et al., 2011), The convolution layer has 300 hidden units. In each unit, the same affine transformation is appl"
D16-1005,N15-1044,0,0.0295513,"Missing"
D16-1005,S13-2001,0,0.0942687,"GOING ), or may happen in the future ( FUTURE ). We introduce a new task and corpus for studying the temporal/aspectual properties of major events. The EventStatus corpus consists of 4500 English and Spanish news articles about civil unrest events, such as protests, demonstrations, marches, and strikes, in which each event is annotated as PAST, O N -G OING, or F UTURE (sublabeled as P LANNED, A LERT or P OSSIBLE). This task bridges event extraction research and temporal research in the tradition of TIMEBANK (Pustejovsky et al., 2003) and TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Previous corpora have begun this association: TIMEBANK, for example, includes temporal relations linking events with Document Creation Times (DCT). But the EventStatus task and corpus offers several new research directions. First, major societal events are often discussed before they happen, or while they are still happening, because they have the potential to impact a large number of people. News outlets frequently report on impending natural disasters (e.g., hurricanes), anticipated disease outbreaks (e.g., Zika virus), threats of terrorism, and plans or warnings of potential civil unrest"
D16-1005,S07-1014,0,0.155943,"y happened (PAST), is currently happening (ON GOING ), or may happen in the future ( FUTURE ). We introduce a new task and corpus for studying the temporal/aspectual properties of major events. The EventStatus corpus consists of 4500 English and Spanish news articles about civil unrest events, such as protests, demonstrations, marches, and strikes, in which each event is annotated as PAST, O N -G OING, or F UTURE (sublabeled as P LANNED, A LERT or P OSSIBLE). This task bridges event extraction research and temporal research in the tradition of TIMEBANK (Pustejovsky et al., 2003) and TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Previous corpora have begun this association: TIMEBANK, for example, includes temporal relations linking events with Document Creation Times (DCT). But the EventStatus task and corpus offers several new research directions. First, major societal events are often discussed before they happen, or while they are still happening, because they have the potential to impact a large number of people. News outlets frequently report on impending natural disasters (e.g., hurricanes), anticipated disease outbreaks (e.g., Zika virus), threats of terrorism, an"
D16-1005,D11-1040,0,0.0545468,"Missing"
D16-1005,C98-1013,0,\N,Missing
D16-1005,C98-1064,0,\N,Missing
E12-1029,P11-1098,0,0.301261,"action patterns in an unsupervised way (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). But these efforts target open domain information extraction. To extract domainspecific event information, domain experts are needed to select the pattern subsets to use. There have also been weakly supervised approaches that use more than just local context. (Patwardhan and Riloff, 2007) uses a semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences. The event sentence classifier is self-trained using seed patterns. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles. However, these weakly supervised systems produce substantially lower performance than the best supervised systems. 3 Overview of TIER The goal of our research is to develop a weakly supervised training process that can successfully train a state-of-the-art event extraction system for a new domain with minimal human input. We decided to focus our efforts on the TIER event extraction model because it recently produced better performance on the MU"
E12-1029,P05-1045,0,0.00660023,"the negative:positive ratio to be 10:1. Once the classifier is trained, it is applied to the unlabeled noun phrases in the relevant documents. Noun phrases that are assigned role filler labels by the classifier with high confidence (using the sliding threshold) are added to the set of positive instances. New negative instances are drawn randomly from the irrelevant documents to maintain the 10:1 (negative:positive) ratio. We extract features from each noun phrase (NP) and its surrounding context. The features include the NP head noun and its premodifiers. We also use the Stanford NER tagger (Finkel et al., 2005) to identify Named Entities within the NP. The context features include four words to the left of the NP, four words to the right of the NP, and the lexico-syntactic patterns generated by AutoSlog to capture expressions around the NP (see (Riloff, 1993) for details). 4.2.2 Event Sentence Classifier The event sentence classifier is responsible for identifying sentences that describe a relevant event. Similar to the noun phrase classifier training, positive training instances are selected from the relevant documents and negative instances are drawn from the irrelevant documents. All sentences in"
E12-1029,P98-1067,0,0.414988,"xtraction systems process stories about domain-relevant events and identify the role fillers of each event. A key challenge for event extraction is that recognizing role fillers is inherently contextual. For example, a PERSON can be a perpetrator or a victim in different contexts (e.g., “John Smith assassinated the mayor” vs. “John Smith was assassinated”). Similarly, any COM PANY can be an acquirer or an acquiree depending on the context. Many supervised learning techniques have been used to create event extraction systems using gold standard “answer key” event templates for training (e.g., (Freitag, 1998a; Chieu and Ng, The goal of our research is to use bootstrapping techniques to automatically train a state-ofthe-art event extraction system without humangenerated answer key templates. The focus of our work is the TIER event extraction model, which is a multi-layered architecture for event extraction (Huang and Riloff, 2011). TIER’s innovation over previous techniques is the use of four different classifiers that analyze a document at increasing levels of granularity. TIER progressively zooms in on event information using a pipeline of classifiers that perform document-level classification,"
E12-1029,P06-1061,0,0.186873,"3; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in t"
E12-1029,P10-1029,1,0.736206,"nouns or if the semantic class of the head noun is compatible with the corresponding event role. In the previous example, tsunami will not be extracted as a weapon because it has an incompatible semantic class (EVENT), but bomb will be extracted because it has a compatible semantic class (WEAPON). We use the semantic class labels assigned by the Sundance parser (Riloff and Phillips, 2004) in our experiments. Sundance looks up each noun in a semantic dictionary to assign the semantic class labels. As an alternative, general resources (e.g., WordNet (Miller, 1990)) or a semantic tagger (e.g., (Huang and Riloff, 2010)) could be used. 289 men = Human building = Object ... Semantic Dictionary Constraints terrorists assassins snipers ... Role−Identifying Noun Constraints was killed by <np> <subject> attacked <subject> fired shots ... Role−Identifying Patterns men John Smith was killed by two armed 1 in broad daylight this morning. The assassins attacked the mayor as he 2 left his house to go to work about 8:00 am. Police arrested the unidentified men 3 an hour later. Figure 3: Automatic Training Data Creation 4.1.3 Propagating Labels with Coreference To enrich the automatically labeled training instances, we"
E12-1029,P11-1114,1,0.328003,"ayor” vs. “John Smith was assassinated”). Similarly, any COM PANY can be an acquirer or an acquiree depending on the context. Many supervised learning techniques have been used to create event extraction systems using gold standard “answer key” event templates for training (e.g., (Freitag, 1998a; Chieu and Ng, The goal of our research is to use bootstrapping techniques to automatically train a state-ofthe-art event extraction system without humangenerated answer key templates. The focus of our work is the TIER event extraction model, which is a multi-layered architecture for event extraction (Huang and Riloff, 2011). TIER’s innovation over previous techniques is the use of four different classifiers that analyze a document at increasing levels of granularity. TIER progressively zooms in on event information using a pipeline of classifiers that perform document-level classification, sentence classification, and noun phrase classification. TIER outperformed previous event extraction systems on the MUC-4 data set, but relied heavily on a large collection of 1,300 documents coupled with answer key templates to train its four classifiers. In this paper, we present a bootstrapping solution that exploits a larg"
E12-1029,P08-1030,0,0.476072,"eenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must man"
E12-1029,W05-0610,0,0.0413844,"s comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction"
E12-1029,P10-1081,0,0.293509,"ifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean”"
E12-1029,P07-1075,0,0.0273379,"al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these appro"
E12-1029,D07-1075,1,0.961594,"Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean” the learned patterns to obtain good performance. Research has also been done to learn extraction patterns in an unsupervised way (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). But these efforts target open domain information extraction. To extract domainspecific event information, domain experts are needed to select the pattern subsets to use. There have also been weakly supervised approaches that use more than just local context. (Patwardhan and Riloff, 2007) uses a semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences. The event sentence classifier is self-trained using seed patterns. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles. However, these weakly supervised systems produce substantially lower performance than the best supervised systems. 3 Overview of TIER The goal of our research is to develop a weakly supervised train"
E12-1029,D09-1016,1,0.88382,"land et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perform best all use supervised learning techniques that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-ba"
E12-1029,P06-2094,0,0.0874073,"re a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean” the learned patterns to obtain good performance. Research has also been done to learn extraction patterns in an unsupervised way (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). But these efforts target open domain information extraction. To extract domainspecific event information, domain experts are needed to select the pattern subsets to use. There have also been weakly supervised approaches that use more than just local context. (Patwardhan and Riloff, 2007) uses a semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences. The event sentence classifier is self-trained using seed patterns. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resour"
E12-1029,N06-1039,0,0.057139,"es that require a large number of texts coupled with manually-generated annotations or answer key templates. A variety of techniques have been explored for weakly supervised training of event extraction systems, primarily in the realm of pattern or rule-based approaches (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)). In some of these approaches, a human must manually review and “clean” the learned patterns to obtain good performance. Research has also been done to learn extraction patterns in an unsupervised way (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). But these efforts target open domain information extraction. To extract domainspecific event information, domain experts are needed to select the pattern subsets to use. There have also been weakly supervised approaches that use more than just local context. (Patwardhan and Riloff, 2007) uses a semantic affinity measure to learn primary and secondary patterns, and the secondary patterns are applied only to event sentences. The event sentence classifier is self-trained using seed patterns. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resour"
E12-1029,P05-1047,0,0.332208,"trapped system, TIERlite , outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishma"
E12-1029,P03-1029,0,0.113861,"show that the bootstrapped system, TIERlite , outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslenniko"
E12-1029,C00-2136,0,0.730164,"n research. Our results show that the bootstrapped system, TIERlite , outperforms previous weakly supervised event extraction systems and achieves performance levels comparable to supervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance"
E12-1029,P05-1062,0,0.0134343,"upervised training with 700 manually annotated documents. 2 Related Work Event extraction techniques have largely focused on detecting event “triggers” with their arguments for extracting role fillers. Classical methods are either pattern-based (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005) or classifierbased (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Recently, several approaches have been proposed to address the insufficiency of using only local context to identify role fillers. Some approaches look at the broader sentential context around a potential role filler when making a decision (e.g., (Gu and Cercone, 2006; Patwardhan and Riloff, 2009)). Other systems take a more global view and consider discourse properties of the document as a whole to improve performance (e.g., (Maslennikov and Chua, 2007; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2011)). Currently, the learning-based event extraction systems that perf"
E12-1029,C98-1064,0,\N,Missing
H05-1045,P98-1013,0,0.00645926,"Missing"
H05-1045,A97-1029,0,0.0683494,"Missing"
H05-1045,C04-1018,1,0.565484,"t in methods for automatically identifying opinions, emotions, and sentiments in text. Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g., Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee S1: Taiwan-born voters favoring independence... 1 In related work, we investigate methods to identify the opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and Riloff (2005), Wilson et al. (2005)) and the nesting structure of sources (e.g., Breck and Cardie (2004)). The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus. 355 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 355–362, Vancouver, October 2005. 2005 Association for Computational Linguistics S2: According to the report, the human rights record in China is horrendous. S3: International officers believe that the EU will prevail. S4: International officers said US officials want the EU to prevail. In S1, the phrase “Taiwan-born voters”"
H05-1045,W03-0430,0,0.0131654,"rpetrator and the person who is the victim. We hypothesized that IE techniques would be wellsuited for source identification because an opinion statement can be viewed as a kind of speech event with the source as the agent. We investigate two very different learning-based methods from information extraction for the problem of opinion source identification: graphical models and extraction pattern learning. In particular, we consider Conditional Random Fields (Lafferty et al., 2001) and a variation of AutoSlog (Riloff, 1996a). CRFs have been used successfully for Named Entity recognition (e.g., McCallum and Li (2003), Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in several domains (Riloff, 1996a). While CRFs treat source identification as a sequence tagging task, AutoSlog views the problem as a pattern-matching task, acquiring symbolic patterns that rely on both the syntax and lexical semantics of a sentence. We hypothesized that a combination of the two techniques would perform better than either one alone. Section 3 describes the CRF approach to identifying opinion sources and the features that the system uses. Section 4 then presents a new variation of Aut"
H05-1045,J05-1004,0,0.0139477,"Missing"
H05-1045,W02-1011,0,0.0421764,"Missing"
H05-1045,P04-1035,0,0.246996,"Missing"
H05-1045,W03-1014,1,0.638262,"Missing"
H05-1045,P02-1053,0,0.0160953,"Missing"
H05-1045,H05-2018,1,0.453659,"troduction In recent years, there has been a great deal of interest in methods for automatically identifying opinions, emotions, and sentiments in text. Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g., Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee S1: Taiwan-born voters favoring independence... 1 In related work, we investigate methods to identify the opinion expressions (e.g., Riloff and Wiebe (2003), Wiebe and Riloff (2005), Wilson et al. (2005)) and the nesting structure of sources (e.g., Breck and Cardie (2004)). The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus. 355 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 355–362, Vancouver, October 2005. 2005 Association for Computational Linguistics S2: According to the report, the human rights record in China is horrendous. S3: International officers believe that the EU will prevail. S4: International officers said US offi"
H05-1045,W03-1017,0,0.237929,"Missing"
H05-1045,J03-4003,0,\N,Missing
H05-1045,C98-1013,0,\N,Missing
H05-2018,H05-1045,1,0.340187,"generated from a large corpus of unannotated data by two high-precision, rule-based classifiers. Speech Events and Direct Subjective Expression Classification The second component identifies speech events (e.g., “said,” “according to”) and direct subjective expressions (e.g., “fears,” “is happy”). Speech events include both speaking and writing events. Direct subjective expressions are words or phrases where an opinion, emotion, sentiment, etc. is directly described. A high-precision, rule-based classifier is used to identify these expressions. Related Work Please see (Wiebe and Riloff, 2005; Choi et al., 2005; Wilson et al., 2005) for discussions of related work in automatic opinion and sentiment analysis. 4 Acknowledgments This work was supported by the Advanced Research and Development Activity (ARDA), by the NSF under grants IIS-0208028, IIS-0208798 and IIS0208985, and by the Xerox Foundation. 2.3.2 2.3.3 Opinion Source Identification The third component is a source identifier that combines a Conditional Random Field sequence tagging model (Lafferty et al., 2001) and extraction pattern learning (Riloff, 1996) to identify the sources of speech events and subjective expressions (Choi et al., 2005"
H05-2018,P97-1003,0,0.0192817,"tering out opinionated sentences (Riloff et al., 2005). System Architecture Overview Document Processing For general document processing, OpinionFinder first runs the Sundance partial parser (Riloff and Phillips, 2004) to provide semantic class tags, identify Named Entities, and match extraction patterns that correspond to subjective language (Riloff and Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tokenize, sentence split, and part-of-speech tag the data, and the Abney stemmer2 is used to stem. In batch mode, OpinionFinder parses the data again, this time to obtain constituency parse trees (Collins, 1997), which are then converted to dependency parse trees (Xia and Palmer, 2001). Currently, this stage is only 1 2 http://opennlp.sourceforge.net/ SCOL version 1g available at http://www.vinartus.net/spa/ 34 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34–35, Vancouver, October 2005. available for batch mode processing due to the time required for parsing. Finally, a clue-finder is run to identify words and phrases from a large subjective language lexicon. 2.3 Subjectivity Analysis The subjectivity analysis has four components. The first classifier focuses on identifying sentiment"
H05-2018,W03-1014,1,0.673444,"from knowledge of subjective language include systems that summarize the various viewpoints in a document or that mine product reviews. Even typical fact-oriented applications, such as information extraction, can benefit from subjectivity analysis by filtering out opinionated sentences (Riloff et al., 2005). System Architecture Overview Document Processing For general document processing, OpinionFinder first runs the Sundance partial parser (Riloff and Phillips, 2004) to provide semantic class tags, identify Named Entities, and match extraction patterns that correspond to subjective language (Riloff and Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tokenize, sentence split, and part-of-speech tag the data, and the Abney stemmer2 is used to stem. In batch mode, OpinionFinder parses the data again, this time to obtain constituency parse trees (Collins, 1997), which are then converted to dependency parse trees (Xia and Palmer, 2001). Currently, this stage is only 1 2 http://opennlp.sourceforge.net/ SCOL version 1g available at http://www.vinartus.net/spa/ 34 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34–35, Vancouver, October 2005. available for batch mode processing due to the time requir"
H05-2018,H05-1044,1,0.169551,"rge corpus of unannotated data by two high-precision, rule-based classifiers. Speech Events and Direct Subjective Expression Classification The second component identifies speech events (e.g., “said,” “according to”) and direct subjective expressions (e.g., “fears,” “is happy”). Speech events include both speaking and writing events. Direct subjective expressions are words or phrases where an opinion, emotion, sentiment, etc. is directly described. A high-precision, rule-based classifier is used to identify these expressions. Related Work Please see (Wiebe and Riloff, 2005; Choi et al., 2005; Wilson et al., 2005) for discussions of related work in automatic opinion and sentiment analysis. 4 Acknowledgments This work was supported by the Advanced Research and Development Activity (ARDA), by the NSF under grants IIS-0208028, IIS-0208798 and IIS0208985, and by the Xerox Foundation. 2.3.2 2.3.3 Opinion Source Identification The third component is a source identifier that combines a Conditional Random Field sequence tagging model (Lafferty et al., 2001) and extraction pattern learning (Riloff, 1996) to identify the sources of speech events and subjective expressions (Choi et al., 2005). The source of a spe"
H05-2018,H01-1014,0,0.0113786,"tecture Overview Document Processing For general document processing, OpinionFinder first runs the Sundance partial parser (Riloff and Phillips, 2004) to provide semantic class tags, identify Named Entities, and match extraction patterns that correspond to subjective language (Riloff and Wiebe, 2003). Next, OpenNLP1 1.1.0 is used to tokenize, sentence split, and part-of-speech tag the data, and the Abney stemmer2 is used to stem. In batch mode, OpinionFinder parses the data again, this time to obtain constituency parse trees (Collins, 1997), which are then converted to dependency parse trees (Xia and Palmer, 2001). Currently, this stage is only 1 2 http://opennlp.sourceforge.net/ SCOL version 1g available at http://www.vinartus.net/spa/ 34 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 34–35, Vancouver, October 2005. available for batch mode processing due to the time required for parsing. Finally, a clue-finder is run to identify words and phrases from a large subjective language lexicon. 2.3 Subjectivity Analysis The subjectivity analysis has four components. The first classifier focuses on identifying sentiment expressions. The second classifier takes the sentiment expressions and iden"
H92-1043,M91-1007,0,0.0247706,"ns or civilian locations were the apparent or accidental targets in an intentional act of violence). In order to achieve highprecision information extraction, the MUC-3 text analyzers had to differentiate relevant and irrelevant texts without human assistance. A system with a high rate of false positives would tend to generate output for irrelevant texts, and this behavior would show up in both the scores for overgeneration and spurious event counts. An analysis of the MUC-3 evaluation suggests that all of the MUC-3 systems experienced significant difficulty with relevant text classification (Krupka et al. 1991). Although some texts will inevitably require in-depth natural language understanding capabilities in order to be correctly classified, we will demonstrate that skimming techniques can be used to identify subsets of a corpus that can be classified with very high levels of precision. Our algorithm automatically derives relevancy signatures from a training corpus using selective concept extraction techniques. These signatures are then used to recognize relevant texts with a high degree of accuracy. CLASSIFICATION RELEVANCY DISCRIMINATIONS Text classification is central to many information retrie"
H92-1043,M91-1033,1,0.866667,"Missing"
H92-1043,M91-1018,1,0.888208,"Missing"
H92-1043,M91-1001,0,\N,Missing
M91-1036,P89-1030,1,0.82301,"ate as of the MUC-3 test . This module is a significant departure from the discourse module in BBN &apos; s Janus [3] and the related module in BBN &apos; s Delphi [4, 5] . The discourse components of those systems, which were developed primarily for question-answering applications in limite d domains, are able to take advantage of having complete analyses of the input sentences . For example , syntactic information is used to constrain intra-sentential anaphora [19] ; complete semantic representation s (including quantification information) are used in generating discourse entities in a principled way [2] ; and centering heuristics [7, 12] are used for tracking focus, improving anaphora resolution . 3 For a more detailed characterization of the MUC-3 corpus see the introduction to these proceedings . 4 If the description includes an existing module not used for MUC-3, then a brief explanation as to why it was not used i s given . 257 A fundamental characteristic of PLUM, however, is the emphasis on fragmentary processing at all levels , and the assumption that a non-trivial amount of an input message may not be understood . This led u s to focus more on approaches that depend less on reliable"
M91-1036,H90-1047,0,0.0290048,"a model of the relevant events in the domain (e .g . , &quot;murder&quot;), it attempts to derive any information which was not already found by the semantic interpreter . The primary output of the discourse module is a sequence of frame-like event structures, which are in tur n the input to the template generator . The discourse module of PLUM is new (as are all the other PLUM components except the parser), an d in flux—what is reported here is its state as of the MUC-3 test . This module is a significant departure from the discourse module in BBN &apos; s Janus [3] and the related module in BBN &apos; s Delphi [4, 5] . The discourse components of those systems, which were developed primarily for question-answering applications in limite d domains, are able to take advantage of having complete analyses of the input sentences . For example , syntactic information is used to constrain intra-sentential anaphora [19] ; complete semantic representation s (including quantification information) are used in generating discourse entities in a principled way [2] ; and centering heuristics [7, 12] are used for tracking focus, improving anaphora resolution . 3 For a more detailed characterization of the MUC-3 corpus s"
M91-1036,H91-1033,0,0.0123249,"a model of the relevant events in the domain (e .g . , &quot;murder&quot;), it attempts to derive any information which was not already found by the semantic interpreter . The primary output of the discourse module is a sequence of frame-like event structures, which are in tur n the input to the template generator . The discourse module of PLUM is new (as are all the other PLUM components except the parser), an d in flux—what is reported here is its state as of the MUC-3 test . This module is a significant departure from the discourse module in BBN &apos; s Janus [3] and the related module in BBN &apos; s Delphi [4, 5] . The discourse components of those systems, which were developed primarily for question-answering applications in limite d domains, are able to take advantage of having complete analyses of the input sentences . For example , syntactic information is used to constrain intra-sentential anaphora [19] ; complete semantic representation s (including quantification information) are used in generating discourse entities in a principled way [2] ; and centering heuristics [7, 12] are used for tracking focus, improving anaphora resolution . 3 For a more detailed characterization of the MUC-3 corpus s"
M91-1036,P87-1022,0,0.0286572,"module is a significant departure from the discourse module in BBN &apos; s Janus [3] and the related module in BBN &apos; s Delphi [4, 5] . The discourse components of those systems, which were developed primarily for question-answering applications in limite d domains, are able to take advantage of having complete analyses of the input sentences . For example , syntactic information is used to constrain intra-sentential anaphora [19] ; complete semantic representation s (including quantification information) are used in generating discourse entities in a principled way [2] ; and centering heuristics [7, 12] are used for tracking focus, improving anaphora resolution . 3 For a more detailed characterization of the MUC-3 corpus see the introduction to these proceedings . 4 If the description includes an existing module not used for MUC-3, then a brief explanation as to why it was not used i s given . 257 A fundamental characteristic of PLUM, however, is the emphasis on fragmentary processing at all levels , and the assumption that a non-trivial amount of an input message may not be understood . This led u s to focus more on approaches that depend less on reliable complete understanding of the text"
M91-1036,C90-3023,1,0.832015,"lities . We did not specify which problems should or should not be included . 2 We consider the fifteen systems that participated in MUC-3 as a representative sample of implemented natural languag e text processing systems in the USA . 256 actions, discourse structure etc [15] [6] . Moreover, other versions of some of the text understanding system s represented in MUC-3 have incorporated such capabilities for discourse analysis in narrow domains (se e references in the following sections) . While there have been implementations of much more detailed discours e analysis for narrow domains [26] [11] no implementation of discourse analyses for broad domains exist whic h could guide efforts on the MUC-3 domain . If implemented, those missing capabilities would most certainly improve system performance . There are several reasons why the MUC-3 versions of the participating systems lack them at this point . First, given time constraints (it has been proposed to hold MUC annually) and the fact that the existing capabilities directl y contribute to the performance in the MUC-3 domain, their improvement takes priority over implementin g new ones . Second, the significant effort required to rese"
M91-1036,P83-1007,0,0.0299913,"module is a significant departure from the discourse module in BBN &apos; s Janus [3] and the related module in BBN &apos; s Delphi [4, 5] . The discourse components of those systems, which were developed primarily for question-answering applications in limite d domains, are able to take advantage of having complete analyses of the input sentences . For example , syntactic information is used to constrain intra-sentential anaphora [19] ; complete semantic representation s (including quantification information) are used in generating discourse entities in a principled way [2] ; and centering heuristics [7, 12] are used for tracking focus, improving anaphora resolution . 3 For a more detailed characterization of the MUC-3 corpus see the introduction to these proceedings . 4 If the description includes an existing module not used for MUC-3, then a brief explanation as to why it was not used i s given . 257 A fundamental characteristic of PLUM, however, is the emphasis on fragmentary processing at all levels , and the assumption that a non-trivial amount of an input message may not be understood . This led u s to focus more on approaches that depend less on reliable complete understanding of the text"
M91-1036,J86-3001,0,0.340785,"of topi c identification would have permitted an even finer-grained, more correct segmentation . This is because i n many texts a dominating topic segment can contain antecedents of anaphora, and play a role inside dominate d segments . In a proper anaphora resolution algorithm, antecedents should be sought in the current segmen t and certain dominating segments . The tree ITP constructed for message 99 does not fully recover th e discourse structure of message 99, which actually has a dominating segment consisting of sentences 1,2,3 an d 14 . Sentence 15 `pops &apos; up to the dominating segment [13] [14] . If this dominance structure is represented, it is then possible to find the correct antecedent for `the attacks &apos; in sentence 5, which refers to the attack o n the two embassies . Its antecedent can be found in sentence 1 of the dominating segment . Future research will enable the ITP discourse segmentation algorithm to recover discourse dominanc e relations and become sensitive to tense, aspect and topic . LSI&apos;S DBG MESSAGE UNDERSTANDING SYSTEM : DISCOURSE PROCESSIN G LS I &apos;s approach to discourse processing is based on the notion of text grammar, which was originally defined by Propp"
M91-1036,P88-1012,0,0.0360817,"n phrase to the individuals with which it coreferential unless these individuals appear syntacticall y conjoined in the text . In other words, it will not form new groupings of individuals . Thus, it would no t handle `X was attacked . Y was attacked . The attacks were conducted by Z .&apos; Nor is the system able to figure out that the `TWO VEHICLES&apos; mentioned in TST1-MUC3-0099 are the CAR-BOMB and USS R EMBASSY VEHICLE, since these antecedents do not appear conjoined . SRI&apos;S TACITUS : DISCOURSE PROCESSIN G The TACITUS system employs the general method of abductive explanation to understand texts [16] . This method of explanation is quite well suited to the narrative texts of the MUC-3 domain, because the text s consist almost entirely of declarative sentences that are intended to convey information to the reader . TACITUS does not have an explicit discourse processing module, and does not currently employ any theory o f discourse structure, but rather relies on the assumption that the correct resolution of anaphora and individuation of events will be a consequence of generating the best explanation for the truth of its constituen t sentences, subject to minimizing the extension of certain"
M91-1036,P89-1032,0,0.016045,"or . The discourse module of PLUM is new (as are all the other PLUM components except the parser), an d in flux—what is reported here is its state as of the MUC-3 test . This module is a significant departure from the discourse module in BBN &apos; s Janus [3] and the related module in BBN &apos; s Delphi [4, 5] . The discourse components of those systems, which were developed primarily for question-answering applications in limite d domains, are able to take advantage of having complete analyses of the input sentences . For example , syntactic information is used to constrain intra-sentential anaphora [19] ; complete semantic representation s (including quantification information) are used in generating discourse entities in a principled way [2] ; and centering heuristics [7, 12] are used for tracking focus, improving anaphora resolution . 3 For a more detailed characterization of the MUC-3 corpus see the introduction to these proceedings . 4 If the description includes an existing module not used for MUC-3, then a brief explanation as to why it was not used i s given . 257 A fundamental characteristic of PLUM, however, is the emphasis on fragmentary processing at all levels , and the assumptio"
M91-1036,P91-1008,0,0.0307206,"m at this point . First, given time constraints (it has been proposed to hold MUC annually) and the fact that the existing capabilities directl y contribute to the performance in the MUC-3 domain, their improvement takes priority over implementin g new ones . Second, the significant effort required to research and implement any of these capabilities may not be proportional to the increase in performance . Many aspects of discourse, e .g ., constructing a theoretical and computational model of the relation tha t holds between different sentences of a text, are still open research problems [18] [22] . A typical MUC- 3 message contains fairly unrestricted text with long and complex sentences . A fair amount of knowledge i s required in order to correctly interpret it . Both solving theoretical aspects of these problems and implementing them on such a broad and complex domain as MUC-3 3 is an extremely challenging goal, practicall y unachievable within the limited resources most systems have at their disposal . The purpose of this paper is to : 1. present and compare the approaches used in the systems represented here in addressing the thre e discourse understanding capabilities mentioned"
M91-1036,J88-2005,0,0.0242778,"apabilities . We did not specify which problems should or should not be included . 2 We consider the fifteen systems that participated in MUC-3 as a representative sample of implemented natural languag e text processing systems in the USA . 256 actions, discourse structure etc [15] [6] . Moreover, other versions of some of the text understanding system s represented in MUC-3 have incorporated such capabilities for discourse analysis in narrow domains (se e references in the following sections) . While there have been implementations of much more detailed discours e analysis for narrow domains [26] [11] no implementation of discourse analyses for broad domains exist whic h could guide efforts on the MUC-3 domain . If implemented, those missing capabilities would most certainly improve system performance . There are several reasons why the MUC-3 versions of the participating systems lack them at this point . First, given time constraints (it has been proposed to hold MUC annually) and the fact that the existing capabilities directl y contribute to the performance in the MUC-3 domain, their improvement takes priority over implementin g new ones . Second, the significant effort required to"
M91-1036,A88-1018,0,0.0303999,"cking of tense and time, explore further the issues of merging of events , and add a representation of ambiguity at various levels . GE&apos;S NLTOOLSET : DPM AND TRUMPET Discourse processing is the ability to understand connected text . This includes the ability to recogniz e fragments of text that describe individual events . We do discourse-related processing in two stages : before parsing, the Discourse Processing Module (DPM) [20], produces an initial segmentation of the input story int o fragments relevant for different events ; then, after parsing, the top-down expectation module (TRUMPET ) [28] uses special domain knowledge to connect the representations of individual sentences relating to th e same event . Our intuition is that discourse should drive text understanding, including parsing and interpretation . Placing DPM before parsing is the first step toward this mode of text understanding . DPM and TRUMPE T currently overlap somewhat in their discourse-related processing . We maintain them as independent modules in order to ensure fewer errors by employing multiple strategies, to evaluate the performance of the syste m in different configurations and to experiment with different"
M91-1036,J88-2002,0,\N,Missing
N04-1038,P99-1048,1,0.776185,"e and companies. Proper names are assumed to be coreferent if they match exactly, or if they closely match based on a few heuristics. For example, a person’s full name will match with just their last name (e.g., “George Bush” and “Bush”), and a company name will match with and without a corporate suffix (e.g., “IBM Corp.” and “IBM”). Proper names that match are resolved with each other. The second case involves existential noun phrases (Allen, 1995), which are noun phrases that uniquely specify an object or concept and therefore do not need a prior referent in the discourse. In previous work (Bean and Riloff, 1999), we developed an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood. For example, a story can mention “the FBI”, “the White House”, or “the weather” without any prior referent in the story. Although these existential NPs do not need a prior referent, they may occur multiple times in a document. By definition, each existential NP uniquely specifies an object or concept, so we can infer that all instances of the same existential NP are coreferent (e.g., “the FBI” always re"
N04-1038,C90-3063,0,0.091077,"hat putting all of the contextual role KSs in play at the same time produces the greatest performance gain. There are two possible reasons: (1) the knowledge sources are resolving different cases of anaphora, and (2) the knowledge sources provide multiple pieces of evidence in support of (or against) a candidate, thereby acting synergistically to push the Dempster-Shafer model over the belief threshold in favor of a single candidate. 5 Related Work Many researchers have developed coreference resolvers, so we will only discuss the methods that are most closely related to BABAR. Dagan and Itai (Dagan and Itai, 1990) experimented with co-occurrence statistics that are similar to our lexical caseframe expectations. Their work used subject-verb, verb-object, and adjective-noun relations to compare the contexts surrounding an anaphor and candidate. However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our caseframe network. (Niyu et al., 1998) used unsupervised learning to acquire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver. The learned information was recycled ba"
N04-1038,J93-1003,0,0.0394917,"Missing"
N04-1038,J95-2003,0,0.0155651,"ual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a Dempster-Shafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. 1 Introduction The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Leass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phra"
N04-1038,W97-0319,0,0.281482,"Missing"
N04-1038,J94-4002,0,0.0259029,"e whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a Dempster-Shafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. 1 Introduction The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Leass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextua"
N04-1038,P02-1014,0,0.522721,"tic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. 1 Introduction The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Leass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase. Consider the following sentences:"
N04-1038,W98-1119,0,0.10748,"le candidate. 5 Related Work Many researchers have developed coreference resolvers, so we will only discuss the methods that are most closely related to BABAR. Dagan and Itai (Dagan and Itai, 1990) experimented with co-occurrence statistics that are similar to our lexical caseframe expectations. Their work used subject-verb, verb-object, and adjective-noun relations to compare the contexts surrounding an anaphor and candidate. However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our caseframe network. (Niyu et al., 1998) used unsupervised learning to acquire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver. The learned information was recycled back into the resolver to improve its performance. This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions. (Kehler, 1997) also used a DempsterShafer model to merge evidence from different sources for template-level coreference. Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners (Aone and Bennett, 1995; McCarthy and Lehn"
N04-1038,J01-4004,0,0.80219,"olutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns. 1 Introduction The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Leass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase. Consider the following sentences: Ellen Riloff School"
N13-1005,P11-1040,0,0.114104,"Missing"
N13-1005,D12-1091,0,0.0250646,"Missing"
N13-1005,P06-1061,0,0.305779,"Missing"
N13-1005,P11-1114,1,0.908034,"Missing"
N13-1005,de-marneffe-etal-2006-generating,0,0.0185558,"that describe the reason for a civil unrest event. We identify probable event sentences by extracting all sentences that contain at least one agent term and one purpose phrase. Agents Purpose Phrases protesters, activists, demonstrators, students, groups, crowd, workers, palestinians, supporters, women demanding, to demand, protesting, to protest Table 1: Agent and Purpose Phrases Used for Seeding 3.1.2 Harvesting Event Expressions To constrain the learning process, we require event expressions and purpose phrases to match certain syntactic structures. We apply the Stanford dependency parser (Marneffe et al., 2006) to the probable event sentences to identify verb phrase candidates and to enforce syntactic constraints between the different types of event information. “xcomp” links “took to the streets” with “protesting higher fuel prices”. Figure 3: Syntactic Dependencies between Agents, Event Phrases, and Purpose Phrases Given the syntactic construction shown in Figure 3, with a known agent and purpose phrase, we extract the head verb phrase of the “xcomp” dependency relation as an event phrase candidate. The event phrases that co-occur with at least two unique agent terms and two unique purposes phrase"
N13-1005,N12-1083,0,0.0608543,"Missing"
N13-1005,D07-1075,1,0.894994,"Missing"
N13-1005,N12-1034,0,0.0275632,"1: Bootstrapped Learning of Event Dictionaries 2002) which addresses event-based organization of a stream of news stories. Event recognition is similar to New Event Detection, also called First Story Detection, which is considered the most difficult TDT task (Allan et al., 2000a). Typical approaches reduce documents to a set of features, either as a word vector (Allan et al., 2000b) or a probability distribution (Jin et al., 1999), and compare the incoming stories to stories that appeared in the past by computing similarities between their feature representations. Recently, event paraphrases (Petrovic et al., 2012) have been explored to deal with the diversity of event descriptions. However, the New Event Detection task differs from our event recognition task because we want to find all stories describing a certain type of event, not just new events. 3 Bootstrapped Learning of Event Dictionaries Our bootstrapping approach consists of two stages of learning as shown in Figure 1. The process begins with a few agent seeds, purpose phrase patterns, and unannotated articles selected from a broadcoverage corpus using event keywords. In the first stage, event expressions are harvested from the sentences that h"
N13-1005,P06-2094,0,0.0659968,"Missing"
N13-1005,P05-1047,0,0.186793,"Missing"
N13-1005,P03-1029,0,0.145943,"Missing"
N13-1005,C00-2136,0,0.372244,"Missing"
N15-1168,de-marneffe-etal-2006-generating,0,0.021436,"Missing"
N15-1168,C10-2030,0,0.0279844,"or even with small amounts of training data. Second, we design supervised classifiers for medication use categorization. We incorporate a rich set of contextual, syntactic, and sentential features as well as a semantic tagger trained for the veterinary domain with bootstrapped learning over a large set of unannotated veterinary texts. We demonstrate additional performance gains by using balanced selftraining with the unannotated texts. 2 Related Work Previous work on extracting medication information from text has primarily focused on clinical medical text, such as discharge summaries (e.g., (Doan and Xu, 2010; Halgrim et al., 2010; Doan et al., 2012; Tang et al., 2013; Segura-Bedmar et al., 1452 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1452–1458, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2013)). The Third and Fourth i2b2 Shared Tasks included medication detection from clinical texts (Uzuner et al., 2010; Uzuner et al., 2011), and the Fourth i2b2 Shared Task also included relation classification between treatments (including medications), problems, and tests. Recently, there has been growi"
N15-1168,W09-1125,0,0.0262896,"ns and rules. RxNorm (Nelson et al., 2011; Liu et al., 2005) is a large knowledge base containing generic and brand names of drugs and it is often used as a component in these systems. We compare our results with the MedEx system (Xu et al., 2010), which uses RxNorm coupled with manually defined rules. To our knowledge, classifying medication mentions with respect to administration use categories has not yet been studied. A novel aspect of our work is also the use of Web Context features for medication detection. Similar Web features have been exploited for fine-grained person classification (Giuliano, 2009), while we demonstrate that they can be highly beneficial for medical concept extraction. 3 Task Description and Data Set We divide our task into two subproblems: (1) Medication Detection aims to identify words corresponding to non-food substances used to treat patients (e.g., drugs, potassium supplements), and (2) Medication Use Categorization aims to classify medication mentions based on actions and observations related to their administration and to identify question contexts. We assign each medication mention to one of the six categories below. Rx: The text indicates that a doctor prescrib"
N15-1168,W10-1109,0,0.0680972,"Missing"
N15-1168,P10-1029,1,0.835814,"whether the medication is separated by a comma from the ending question mark (for lists). Semantic Tagging. We hypothesized that identifying semantic concepts might be beneficial. For example, the presence of an ANIMAL term suggests a patient, and a SYMPTOM term may indicate the reason for a prescription or an effect of medication use. First, we used WordNet (Miller, 1995) and identified synsets for 4 semantic classes: ANIMAL, DRUG, DISEASE / SYMPTOM , and HUMAN . We assigned any noun phrase with a head in these synsets to the corresponding semantic type. Next, we used a bootstrapping method (Huang and Riloff, 2010) to build domain-specific semantic taggers (SemTaggers) for the same four semantic classes as well as TEST, TREATMENT and OTHER . We used 10 seed words6 for each category and 10,000 unlabeled veterinary forum texts for bootstrapping. Finally, we created Semantic Features for our medication use classifier. Each noun phrase tagged with a semantic class was replaced by a semantic type. Then we constructed features from pairs of adjacent terms in a context window of size eight (+/-4) around each medication mention. For example, the word sequence “for a Boston terrier with diabetes” would be transf"
N15-1168,W10-1915,0,0.0817994,"Missing"
N15-1168,P14-5010,0,0.012823,"Missing"
N15-1168,S13-2056,0,0.0572557,"Missing"
N15-1168,W14-3415,0,0.0114142,"015 Association for Computational Linguistics 2013)). The Third and Fourth i2b2 Shared Tasks included medication detection from clinical texts (Uzuner et al., 2010; Uzuner et al., 2011), and the Fourth i2b2 Shared Task also included relation classification between treatments (including medications), problems, and tests. Recently, there has been growing interest in extracting medication information from other types of text, such as Twitter, online health forums, and drug review sites (e.g., (Leaman et al., 2010; Bian et al., 2012; Liu et al., 2013; Liu and Chen, 2013; Yates and Goharian, 2013; Segura-Bedmar et al., 2014)). Much of this research is geared toward identifying adverse drug events or drug-drug interactions. Many methods have been used for medication extraction, including rule based approaches (Levin et al., 2007; Xu et al., 2010), machine learning (Patrick and Li, 2010; Doan and Xu, 2010; Tang et al., 2013), and hybrid methods (Halgrim et al., 2010; Meystre et al., 2010). Rule based and hybrid approaches typically rely on manually created lexicons and rules. RxNorm (Nelson et al., 2011; Liu et al., 2005) is a large knowledge base containing generic and brand names of drugs and it is often used as"
N16-1146,P14-2050,0,0.0637387,"Missing"
N16-1146,D14-1215,0,0.623745,"even human annotators. We also present an analysis of the similes in our data set with respect to their interpretive diversity (intuitively, a measure of how many plausible interpretations a simile has). We show that our method performs best on similes with low diversity, as one would expect since their implicit properties are most clear to humans. 2 Problem Description and Data A simile typically consists of four key components: the topic or tenor (subject of the comparison), the vehicle (object of the comparison), the event (act or state), and a comparator (usually “as”, “like”, or “than”) (Niculae and Danescu-Niculescu-Mizil, 2014). For the simile “the room feels like Antarctica”, “room” is the tenor, “feels” is the event, and “Antarctica” is the vehicle. A property (shared attribute) can optionally be included to explicitly state how the tenor is being compared with the vehicle, (e.g., “the room is as cold as Antarctica”). Table 1 shows examples of open similes from our Twitter data set, along with several properties inferred by our human annotators (our data set will be described in Section 2.1). We represent each simile using just the head noun of the tenor and vehicle, and the lemma of the event. Veale and Hao (2007"
N16-1146,P13-3013,0,0.151977,"is given. Hanks (2005) manually categorized vehicle nouns of similes into semantic categories. Automatic approaches that use computational models for similes are relatively rare. Veale and Hao (2007) extracted salient properties of vehicles from the web using “as ADJ as a/an NOUN” extraction pattern to acquire knowledge for concept categories. Veale (2012) built a knowledge-base of affective stereotypes by characterizing simile vehicles with salient properties. Li et al. (2012) used explicit property extraction patterns to determine the sentiment that properties convey toward simile vehicles. Niculae and Yaneva (2013) and Niculae (2013) used constituency and dependency parsing-based techniques to identify similes in text. Qadir et al. (2015) classified similes into positive and negative affective polarities using supervised classification, with 1231 features derived from simile components. Niculae and Danescu-Niculescu-Mizil (2014) designed a classifier with domain specific, domain agnostic, and metaphor inspired features to determine when comparisons are figurative. Computational approaches to work on figurative language also include figurative language identification using word sense disambiguation (Rent"
N16-1146,W13-3829,0,0.332402,"y categorized vehicle nouns of similes into semantic categories. Automatic approaches that use computational models for similes are relatively rare. Veale and Hao (2007) extracted salient properties of vehicles from the web using “as ADJ as a/an NOUN” extraction pattern to acquire knowledge for concept categories. Veale (2012) built a knowledge-base of affective stereotypes by characterizing simile vehicles with salient properties. Li et al. (2012) used explicit property extraction patterns to determine the sentiment that properties convey toward simile vehicles. Niculae and Yaneva (2013) and Niculae (2013) used constituency and dependency parsing-based techniques to identify similes in text. Qadir et al. (2015) classified similes into positive and negative affective polarities using supervised classification, with 1231 features derived from simile components. Niculae and Danescu-Niculescu-Mizil (2014) designed a classifier with domain specific, domain agnostic, and metaphor inspired features to determine when comparisons are figurative. Computational approaches to work on figurative language also include figurative language identification using word sense disambiguation (Rentoumi et al., 2009),"
N16-1146,N13-1039,0,0.0174992,"Missing"
N16-1146,D15-1019,1,0.742686,"tional models for similes are relatively rare. Veale and Hao (2007) extracted salient properties of vehicles from the web using “as ADJ as a/an NOUN” extraction pattern to acquire knowledge for concept categories. Veale (2012) built a knowledge-base of affective stereotypes by characterizing simile vehicles with salient properties. Li et al. (2012) used explicit property extraction patterns to determine the sentiment that properties convey toward simile vehicles. Niculae and Yaneva (2013) and Niculae (2013) used constituency and dependency parsing-based techniques to identify similes in text. Qadir et al. (2015) classified similes into positive and negative affective polarities using supervised classification, with 1231 features derived from simile components. Niculae and Danescu-Niculescu-Mizil (2014) designed a classifier with domain specific, domain agnostic, and metaphor inspired features to determine when comparisons are figurative. Computational approaches to work on figurative language also include figurative language identification using word sense disambiguation (Rentoumi et al., 2009), harvesting metaphors by using noun and verb clustering-based techniques (Shutova et al., 2010), interpreti"
N16-1146,R09-1067,0,0.0204717,"013) and Niculae (2013) used constituency and dependency parsing-based techniques to identify similes in text. Qadir et al. (2015) classified similes into positive and negative affective polarities using supervised classification, with 1231 features derived from simile components. Niculae and Danescu-Niculescu-Mizil (2014) designed a classifier with domain specific, domain agnostic, and metaphor inspired features to determine when comparisons are figurative. Computational approaches to work on figurative language also include figurative language identification using word sense disambiguation (Rentoumi et al., 2009), harvesting metaphors by using noun and verb clustering-based techniques (Shutova et al., 2010), interpreting metaphors by generating literal paraphrases (Shutova, 2010), etc. Although previous research has extensively used explicit property extraction patterns for various tasks, none has explored the impact of multiple simile components for inferring properties. To our knowledge, we are the first to introduce the task of automatically inferring the implicit properties in open similes, which is fundamental to automatic understanding of similes. 6 Conclusion In this work, we addressed the prob"
N16-1146,C10-1113,0,0.0891833,"Missing"
N16-1146,P12-2015,0,0.194683,"and property salience have been compared by Gagn´e (2002). Fishelov (2007) experimented with affective connotation and degrees of difficulty associated with understanding a simile when a simile property is conventional or unconventional, or no property is given. Hanks (2005) manually categorized vehicle nouns of similes into semantic categories. Automatic approaches that use computational models for similes are relatively rare. Veale and Hao (2007) extracted salient properties of vehicles from the web using “as ADJ as a/an NOUN” extraction pattern to acquire knowledge for concept categories. Veale (2012) built a knowledge-base of affective stereotypes by characterizing simile vehicles with salient properties. Li et al. (2012) used explicit property extraction patterns to determine the sentiment that properties convey toward simile vehicles. Niculae and Yaneva (2013) and Niculae (2013) used constituency and dependency parsing-based techniques to identify similes in text. Qadir et al. (2015) classified similes into positive and negative affective polarities using supervised classification, with 1231 features derived from simile components. Niculae and Danescu-Niculescu-Mizil (2014) designed a c"
N16-1146,N10-1147,0,\N,Missing
N18-1174,P16-1231,0,0.0205959,"age). created for prior research (Ding and Riloff, 2018) which aims to identify affective events. We will refer to this data as the AffectEvent dataset. We will briefly describe this data and the human need category annotations that we added on top of it. The AffectEvent dataset contains events extracted from a personal story corpus that was created by applying a personal story classifier (Gordon and Swanson, 2009) to 177 million blog posts. The personal story corpus contains 1,383,425 personal story blogs. StanfordCoreNLP (Manning et al., 2014) was used for POS and NER tagging and SyntaxNet (Andor et al., 2016) for parsing. Each event is represented using a frame-like structure to capture the meanings of different types of events. Each event representation contains four components: hAgent, Predicate, Theme, PPi. The Predicate is a simple verb phrase corresponding to an action or state. The Agent is a named entity, nominal, or pronoun, and is extracted using syntactic heuristics rather than semantic role labeling. We use “Theme” loosely to allow a NP or adjective to fill this role. The PP component is composed of a preposition and a NP. All words in the event are lemmatized, and active and passive vo"
N18-1174,D14-1125,0,0.0503125,"Missing"
N18-1174,P13-2022,0,0.0540403,"o their patients. Li et al. (2015) designed methods to extract verb expressions that imply negative opinions from reviews. Rashkin et al. (2016) recently proposed connotation frames to incorporate the connotative polarities for a verb’s arguments from the writer’s and other event entities’ perspectives. Li et al. (2014) proposed a bootstrapping approach to extract major life events from tweets using congratulation and condolence speech acts. Most of these major life events are affective although their work did not identify polarity. Another group of researchers have studied +/- effect events (Deng et al., 2013; Choi and Wiebe, 2014) which they previously called benefactive/malefactive events. Their work mainly focused on inferring implicit opinions through implicature rules (Deng and Wiebe, 2014, 2015). Ding and Riloff (2016) designed an event context graph model to identify affective events using label propagation. Reed et al. (2017) demonstrated that automatically acquired patterns could benefit the recognition of first-person related affective sentences. Most recently, Ding and Riloff (2018) developed a semantic consistency model to induce a large set of affective events using three types of sem"
N18-1174,E14-1040,0,0.485042,"tive events in text, which are activities or states that positively or negatively affect the people who experience them. Recognizing affective events in text is challenging because they appear as factual expressions and their affective polarity is often implicit. For example, “I broke my arm” and “I got fired” are usually negative experiences, while “I broke a record” and “I went to a concert” are typically positive experiences. Several NLP techniques have been developed to recognize affective events, including patient polarity verb bootstrapping (Goyal et al., 2010, 2013), implicature rules (Deng and Wiebe, 2014), label propagation (Ding and Riloff, 2016), pattern-based learning (Vu et al., 2014; Reed et al., 2017), and semantic consistency optimization (Ding and Riloff, 2018). Our research aims to probe deeper and understand not just the polarity of affective events, but the reason for the polarity. Events can impact people in many ways, and understanding why an event is beneficial or detrimental is a fundamental aspect of language understanding and narrative text comprehension. Additionally, many applications could benefit from understanding the nature of affective events, including text summarizati"
N18-1174,D15-1018,0,0.16809,"Missing"
N18-1174,N15-1168,1,0.858069,"very specific to a character in a particular narrative story. However, but many types of goals originate from universal needs and desires shared by most people (Max-Neef et al., 1991). In addition, our work is also related to research on wish detection (Goldberg et al., 2009), desire fulfillment (Chaturvedi et al., 2016), and modelling protagonist goals and desires (Rahimtoroghi et al., 2017). Self-training is a semi-supervised learning method to improve performance by exploiting unlabeled data. Self-training has been successfully used in many NLP applications such as information extraction (Ding and Riloff, 2015) and syntactic parsing (McClosky et al., 2006). Co-training (Blum and Mitchell, 1998) uses both labeled and unlabeled data to train models that have two different views of the data. Co-training has been previously used for many NLP tasks including spectral clustering (Kumar and Daum´e, 2011), word sense disambiguation (Mihalcea, 2004), coreference resolution (Phillips and Riloff, 2002), and sentiment analysis (Wan, 2009; Xia et al., 2015). 3 Affective Event Data The goal of our research is to categorize affective events based on 7 categories of human needs. To facilitate this work, we build up"
N18-1174,N09-1030,0,0.272541,"vations and personalities. The second one is Fundamental Human Needs (Max-Neef et al., 1991) which was developed to help communities identify their strengths and weaknesses. The human need categories are also related to the concept of “goals”, which has been proposed by (Schank and Abelson, 1977) to understand narrative stories. Goals could be very specific to a character in a particular narrative story. However, but many types of goals originate from universal needs and desires shared by most people (Max-Neef et al., 1991). In addition, our work is also related to research on wish detection (Goldberg et al., 2009), desire fulfillment (Chaturvedi et al., 2016), and modelling protagonist goals and desires (Rahimtoroghi et al., 2017). Self-training is a semi-supervised learning method to improve performance by exploiting unlabeled data. Self-training has been successfully used in many NLP applications such as information extraction (Ding and Riloff, 2015) and syntactic parsing (McClosky et al., 2006). Co-training (Blum and Mitchell, 1998) uses both labeled and unlabeled data to train models that have two different views of the data. Co-training has been previously used for many NLP tasks including spectra"
N18-1174,D10-1008,1,0.810163,"Missing"
N18-1174,D14-1214,0,0.109039,", yielding gains in both precision and recall. 2 Related Work Recently, there has been growing interest in recognizing the affective polarity of events. For example, Goyal et al. (2013) developed a bootstrapped learning method to learn patient polarity verbs, which impart affective polarities to their patients. Li et al. (2015) designed methods to extract verb expressions that imply negative opinions from reviews. Rashkin et al. (2016) recently proposed connotation frames to incorporate the connotative polarities for a verb’s arguments from the writer’s and other event entities’ perspectives. Li et al. (2014) proposed a bootstrapping approach to extract major life events from tweets using congratulation and condolence speech acts. Most of these major life events are affective although their work did not identify polarity. Another group of researchers have studied +/- effect events (Deng et al., 2013; Choi and Wiebe, 2014) which they previously called benefactive/malefactive events. Their work mainly focused on inferring implicit opinions through implicature rules (Deng and Wiebe, 2014, 2015). Ding and Riloff (2016) designed an event context graph model to identify affective events using label prop"
N18-1174,P14-5010,0,0.00393179,"on of Human Need Categories (each cell shows the frequency and percentage). created for prior research (Ding and Riloff, 2018) which aims to identify affective events. We will refer to this data as the AffectEvent dataset. We will briefly describe this data and the human need category annotations that we added on top of it. The AffectEvent dataset contains events extracted from a personal story corpus that was created by applying a personal story classifier (Gordon and Swanson, 2009) to 177 million blog posts. The personal story corpus contains 1,383,425 personal story blogs. StanfordCoreNLP (Manning et al., 2014) was used for POS and NER tagging and SyntaxNet (Andor et al., 2016) for parsing. Each event is represented using a frame-like structure to capture the meanings of different types of events. Each event representation contains four components: hAgent, Predicate, Theme, PPi. The Predicate is a simple verb phrase corresponding to an action or state. The Agent is a named entity, nominal, or pronoun, and is extracted using syntactic heuristics rather than semantic role labeling. We use “Theme” loosely to allow a NP or adjective to fill this role. The PP component is composed of a preposition and a"
N18-1174,N06-1020,0,0.0538998,"Missing"
N18-1174,W04-2405,0,0.160484,"Missing"
N18-1174,D14-1162,0,0.0807894,"ontext classifier to learn from the unlabeled events. These two types of classifiers provide complementary views of an event, so new instances labeled by one classifier can be used as valuable new data to benefit the other classifier, in an iterative learning cycle. 4.1 Event Expression Classifiers The most obvious approach is to use the words in event expressions as features for recognizing human need categories (e.g., {ear, be, better} for the event &lt;ear, be, better>). We experimented with both lexical (string) features and pre-trained word embedding features. For the latter, we used GloVe (Pennington et al., 2014) vectors (200d) pretrained on 27B tweets. For each event expression, we compute its embedding as the average of its words’ embeddings. We also designed semantic features using the lexical categories in the LIWC lexicon (Pennebaker et al., 2007) to capture a more general meaning for each word. LIWC is a dictionary of words associated with “psychologically meaningful” lexical categories, some of which are directly relevant to our task, such as AFFECTIVE, SO CIAL, COGNITIVE , and BIOLOGICAL PROCESS. We identify the LIWC category of the head word of each phrase in the event representation and use"
N18-1174,W02-1017,1,0.69967,"Missing"
N18-1174,W17-5543,0,0.14105,"help communities identify their strengths and weaknesses. The human need categories are also related to the concept of “goals”, which has been proposed by (Schank and Abelson, 1977) to understand narrative stories. Goals could be very specific to a character in a particular narrative story. However, but many types of goals originate from universal needs and desires shared by most people (Max-Neef et al., 1991). In addition, our work is also related to research on wish detection (Goldberg et al., 2009), desire fulfillment (Chaturvedi et al., 2016), and modelling protagonist goals and desires (Rahimtoroghi et al., 2017). Self-training is a semi-supervised learning method to improve performance by exploiting unlabeled data. Self-training has been successfully used in many NLP applications such as information extraction (Ding and Riloff, 2015) and syntactic parsing (McClosky et al., 2006). Co-training (Blum and Mitchell, 1998) uses both labeled and unlabeled data to train models that have two different views of the data. Co-training has been previously used for many NLP tasks including spectral clustering (Kumar and Daum´e, 2011), word sense disambiguation (Mihalcea, 2004), coreference resolution (Phillips and"
N18-1174,P16-1030,0,0.016533,"event expression. Our results show that this co-training model effectively uses unlabeled data to substantially improve results compared to classifiers trained only with labeled data, yielding gains in both precision and recall. 2 Related Work Recently, there has been growing interest in recognizing the affective polarity of events. For example, Goyal et al. (2013) developed a bootstrapped learning method to learn patient polarity verbs, which impart affective polarities to their patients. Li et al. (2015) designed methods to extract verb expressions that imply negative opinions from reviews. Rashkin et al. (2016) recently proposed connotation frames to incorporate the connotative polarities for a verb’s arguments from the writer’s and other event entities’ perspectives. Li et al. (2014) proposed a bootstrapping approach to extract major life events from tweets using congratulation and condolence speech acts. Most of these major life events are affective although their work did not identify polarity. Another group of researchers have studied +/- effect events (Deng et al., 2013; Choi and Wiebe, 2014) which they previously called benefactive/malefactive events. Their work mainly focused on inferring imp"
N18-1174,P17-2022,0,0.591327,"erience them. Recognizing affective events in text is challenging because they appear as factual expressions and their affective polarity is often implicit. For example, “I broke my arm” and “I got fired” are usually negative experiences, while “I broke a record” and “I went to a concert” are typically positive experiences. Several NLP techniques have been developed to recognize affective events, including patient polarity verb bootstrapping (Goyal et al., 2010, 2013), implicature rules (Deng and Wiebe, 2014), label propagation (Ding and Riloff, 2016), pattern-based learning (Vu et al., 2014; Reed et al., 2017), and semantic consistency optimization (Ding and Riloff, 2018). Our research aims to probe deeper and understand not just the polarity of affective events, but the reason for the polarity. Events can impact people in many ways, and understanding why an event is beneficial or detrimental is a fundamental aspect of language understanding and narrative text comprehension. Additionally, many applications could benefit from understanding the nature of affective events, including text summarization, conversational dialogue processing, and mental health therapy or counseling systems. As an illustrat"
N18-1174,E14-4025,0,0.0501556,"he people who experience them. Recognizing affective events in text is challenging because they appear as factual expressions and their affective polarity is often implicit. For example, “I broke my arm” and “I got fired” are usually negative experiences, while “I broke a record” and “I went to a concert” are typically positive experiences. Several NLP techniques have been developed to recognize affective events, including patient polarity verb bootstrapping (Goyal et al., 2010, 2013), implicature rules (Deng and Wiebe, 2014), label propagation (Ding and Riloff, 2016), pattern-based learning (Vu et al., 2014; Reed et al., 2017), and semantic consistency optimization (Ding and Riloff, 2018). Our research aims to probe deeper and understand not just the polarity of affective events, but the reason for the polarity. Events can impact people in many ways, and understanding why an event is beneficial or detrimental is a fundamental aspect of language understanding and narrative text comprehension. Additionally, many applications could benefit from understanding the nature of affective events, including text summarization, conversational dialogue processing, and mental health therapy or counseling syst"
N18-1174,P09-1027,0,0.132317,"Missing"
N18-1174,P15-1102,0,0.0482243,"Missing"
P08-1119,P99-1008,0,0.881859,"Missing"
P08-1119,P99-1016,0,0.197953,"ar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic c"
P08-1119,P06-1038,0,0.114438,"d outperforming the results reported by others who have worked on the same classes. 2 Related Work A substantial amount of research has been done in the area of semantic class learning, under a variety of different names and with a variety of different goals. Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly sup"
P08-1119,C02-1130,1,0.642619,"ses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around the pattern. Pasca also developed a second technique (Pas¸ca, 2007b) that creates context vectors for a group of seed instances by searching web query logs, and uses them to learn similar instances. The"
P08-1119,N03-1011,0,0.750933,"c class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local an"
P08-1119,C92-2082,0,0.727441,"formation (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around the pattern. Pasca also developed a second technique (Pas¸ca, 2007b) that creates context vectors for a group of seed instances by searching web query logs, and uses them to learn similar instances. The work most closely related to ours is Hearst’s early work on hyponym learning (Hearst, 1992) and more recent work that has followed up on her idea. Hearst’s system exploited patterns that explicitly identify a hyponym relation between a semantic class and a word (e.g., “such authors as Shakespeare”). We will refer t"
P08-1119,C02-1144,0,0.168229,"ing high accuracies and outperforming the results reported by others who have worked on the same classes. 2 Related Work A substantial amount of research has been done in the area of semantic class learning, under a variety of different names and with a variety of different goals. Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH"
P08-1119,P98-2127,0,0.33935,"ses, achieving high accuracies and outperforming the results reported by others who have worked on the same classes. 2 Related Work A substantial amount of research has been done in the area of semantic class learning, under a variety of different names and with a variety of different goals. Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic clas"
P08-1119,W02-1111,0,0.0186934,"ogy learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <"
P08-1119,N04-1041,0,0.490325,"create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups"
P08-1119,W02-1017,1,0.837817,"ed on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around"
P08-1119,W97-0313,1,0.630135,"sses desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web"
P08-1119,P98-2182,0,0.755248,"th just a class name and one seed instance and then automatically generate a ranked list of new class instances. We conducted experiments on four semantic classes and consistently achieved high accuracies. 1 Introduction Knowing the semantic classes of words (e.g., “trout” is a kind of FISH) can be extremely valuable for many natural language processing tasks. Although some semantic dictionaries do exist (e.g., WordNet (Miller, 1990)), they are rarely complete, especially for large open classes (e.g., classes of people and objects) and rapidly changing categories (e.g., computer technology). (Roark and Charniak, 1998) reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in WordNet. Automatic semantic lexicon acquisition could be used to enhance existing resources such as WordNet, or to produce semantic lexicons for specialized categories or domains. A variety of methods have been developed for automatic semantic class identification, under the rubrics of lexical acquisition, hyponym acquisition, semantic lexicon induction, semantic class learning, and web-based information extraction. Many of these approaches employ surface-level patterns to identify words and their"
P08-1119,E06-1003,0,0.031708,"nstruction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic cl"
P08-1119,W02-1028,1,0.802786,"nd Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around the pattern. Pasca also developed a second technique (Pas¸ca, 2007b) that creates context vectors for a group of seed instances by searching web"
P08-1119,C02-1114,0,0.733507,", 1992) and more recent work that has followed up on her idea. Hearst’s system exploited patterns that explicitly identify a hyponym relation between a semantic class and a word (e.g., “such authors as Shakespeare”). We will refer to these as hyponym patterns. Pasca’s previously mentioned system (Pas¸ca, 2004) applies hyponym patterns to the web and acquires contexts around them. The KnowItAll system (Etzioni et al., 2005) also uses hyponym patterns to extract class instances from the web and then evaluates them further by computing mutual information scores based on web queries. The work by (Widdows and Dorow, 2002) on lexical acquisition is similar to ours because they also use graph structures to learn semantic classes. However, their graph is based entirely on syntactic relations between words, while our graph captures the ability of instances to find each other in a hyponym pattern based on web querying, without any part-ofspeech tagging or parsing. 1 Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on web pages, but used a precompiled corpus of downloaded web pages. 3 Semantic Class Learning with Hyponym Pattern Linkage Graphs 3.1 A Doubly-Anchored Hyponym Pattern Our work was motivated by"
P08-1119,C98-2177,0,\N,Missing
P08-1119,C98-2122,0,\N,Missing
P09-1074,N07-1010,0,0.183205,"resolution classes in a corpus explains (at least partially) why performance varies so much from cor662 P O MUC6 0.59 0.67 MUC7 0.59 0.61 ACE2 0.62 0.66 ACE03 0.65 0.68 ACE04 0.59 0.62 ACE05 0.62 0.67 5 Related Work The bulk of the relevant related work is described in earlier sections, as appropriate. This paper studies complexity issues for NP coreference resolution using a “good”, i.e. near state-of-the-art, system. For state-of-the-art performance on the MUC data sets see, e.g. Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (20"
P09-1074,N04-1038,1,0.917126,"Missing"
P09-1074,D08-1031,0,0.459229,"er, we describe the coreference resolver that we use for our study. 3.1 docs 60 50 159 105 128 81 CEs 4232 4297 2630 3106 3037 1991 chains 960 1081 1148 1340 1332 775 CEs/ch 4.4 3.9 2.3 2.3 2.3 2.6 tr/tst split 30/30 (st) 30/20 (st) 130/29 (st) 74/31 90/38 57/24 Table 2: Dataset characteristics including the number of documents, annotated CEs, coreference chains, annotated CEs per chain (average), and number of documents in the train/test split. We use st to indicate a standard train/test split. fairly comprehensive set of 61 features introduced in previous coreference resolution systems (see Bengtson and Roth (2008)). We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details. Lexical (9): String-based comparisons of the two CEs, such as exact string matching and head noun matching. Proximity (5): Sentence and paragraph-based measures of the distance between two CEs. Grammatical (28): A wide variety of syntactic properties of the CEs, either individually or as a pair. These features are based on part-of-speech tags, parse trees, or dependency relations. For example: one feature indicates whether both CEs are syntactic subjects; another indicates whether the CEs"
P09-1074,C02-1139,1,0.861798,"61 ACE2 0.62 0.66 ACE03 0.65 0.68 ACE04 0.59 0.62 ACE05 0.62 0.67 5 Related Work The bulk of the relevant related work is described in earlier sections, as appropriate. This paper studies complexity issues for NP coreference resolution using a “good”, i.e. near state-of-the-art, system. For state-of-the-art performance on the MUC data sets see, e.g. Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusi"
P09-1074,P02-1014,1,0.845895,"61 ACE2 0.62 0.66 ACE03 0.65 0.68 ACE04 0.59 0.62 ACE05 0.62 0.67 5 Related Work The bulk of the relevant related work is described in earlier sections, as appropriate. This paper studies complexity issues for NP coreference resolution using a “good”, i.e. near state-of-the-art, system. For state-of-the-art performance on the MUC data sets see, e.g. Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g. Bengtson and Roth (2008) and Luo (2007). While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusi"
P09-1074,J01-4006,0,0.0863922,"other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance. In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g. Mitkov (2002) and Byron (2001)). However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusions are not directly applicable to the full coreference task. Previous work has developed methods to predict system performance on NLP tasks given data set characteristics, e.g. Birch et al. (2008) does this for machine translation. Our work looks for the first time at predicting the performance of NP coreference resolvers. Table 6: Predicted (P) vs Observed (O) scores. pus to corpus. To explore this issue, we create a Coreference Performance Prediction (CPP) measure to"
P09-1074,de-marneffe-etal-2006-generating,0,0.0108758,"Missing"
P09-1074,N07-1051,0,0.0206299,"Missing"
P09-1074,W04-1217,0,0.0121339,"Missing"
P09-1074,J01-4004,0,0.979775,"Missing"
P09-1074,P04-1018,0,0.531788,"Missing"
P09-1074,P03-1023,0,0.315248,"Missing"
P09-1074,H05-1004,0,0.564694,"s work has shown that resolving coreference between proper names is relatively easy (e.g. Kameyama (1997)) because string matching functions specialized to the type of proper name (e.g. person vs. location) are quite accurate. Thus, we would expect a coreference resolution system to depend critically on its Named Entity (NE) extractor. On the other hand, state-of-the-art NE taggers are already quite good, so improving this component may not provide much additional gain. To study the influence of NE recognition, we replace the system-generated NEs of 4 We also experimented with the CEAF score (Luo, 2005), but excluded it due to difficulties dealing with the extracted, rather than annotated, CEs. CEAF assigns a zero score to each twinless extracted CE and weights all coreference chains equally, irrespective of their size. As a result, runs with extracted CEs exhibit very low CEAF precision, leading to unreliable scores. 5 All experiments sample uniformly from 1000 threshold values. 659 ReconcileACL09 1. DEFAULT THRESHOLD (0.5) 2. BASELINE = THRESHOLD ESTIMATION 3. OPTIMAL THRESHOLD 4. BASELINE with perfect NEs 5. BASELINE with perfect CEs 6. BASELINE with anaphoric CEs MUC B 3 all B30 MUC B 3"
P09-1074,M95-1005,0,\N,Missing
P09-1074,J00-4005,0,\N,Missing
P09-1074,D08-1078,0,\N,Missing
P09-1074,W97-1307,0,\N,Missing
P10-1029,N06-1020,0,0.0178997,"999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at1 Some NER systems also handle specialized constructs such as dates and monetary amounts. 276 training set. Second, we employ a cross-category bootstrapping process that simultaneously trains a suite of cla"
P10-1029,A97-1029,0,0.0201805,"on from the Web using patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been expl"
P10-1029,W09-2201,0,0.0525965,"uce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). These techniques harvest broad-coverage semantic information from the Web using patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention de"
P10-1029,W99-0613,0,0.119981,"g patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such"
P10-1029,W99-0612,0,0.0601407,"typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon inductio"
P10-1029,C02-1130,0,0.0680397,"knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002;"
P10-1029,P09-1045,0,0.482056,")). has access to contextual features and cannot see the seed. Then we apply the classifier to the corpus to automatically label new instances, and combine these new instances with the seed-based instances. This process expands and diversifies the training set to fuel subsequent bootstrapping. Another challenge is that we want to use a small set of seeds to minimize the amount of human effort, and then use bootstrapping to fully exploit the domain-specific corpus. Iterative self-training, however, often has difficulty sustaining momentum or it succumbs to semantic drift (Komachi et al., 2008; McIntosh and Curran, 2009). To address these issues, we simultaneously induce a suite of classifiers for multiple semantic categories, using the positive instances of one semantic category as negative instances for the others. As bootstrapping progresses, the classifiers gradually improve themselves, and each other, over many iterations. We also explore a onesemantic-class-per-discourse (OSCPD) heuristic that infuses the learning process with fresh training instances, which may be substantially different from the ones seen previously, and we use the labels produced by the classifiers to dynamically create semantic feat"
P10-1029,W04-2405,0,0.0273714,"ns and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at1 Some NER systems also handle specialized constructs such as dates and monetary amounts. 276 training set. Second, we employ a cross-category bootstrapping process that simultaneous"
P10-1029,P02-1045,0,0.0305929,"t associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at1 Some NER systems also handle specialized constructs such as dates and monetary amounts. 276 training set. Second, we employ a cross-category bootstrapping process that simultaneously trains a suite of classifiers for multiple semantic categories, using the positive instances for one semantic class as negative instances for the others. This cross-category training process"
P10-1029,N03-1023,0,0.00985694,"veloped (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at1 Some NER systems also handle specialized constructs such as dates and monetary amounts. 276 training set. Second, we employ a cross-category bootstrapping process t"
P10-1029,P07-1068,0,0.0136105,"nt from the ones seen previously, and we use the labels produced by the classifiers to dynamically create semantic features. We evaluate our approach by creating six semantic taggers using a collection of message board posts in the domain of veterinary medicine. Our results show this approach produces high-quality semantic taggers after a sustained bootstrapping cycle that maintains good precision while steadily increasing recall over many iterations. Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e.g., (Pas¸c"
P10-1029,P03-1043,0,0.166801,"not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. NER systems (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002) identify proper named entities, such as people, organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)"
P10-1029,W02-1017,1,0.891137,"er named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at1 Some NER systems also handle specialized constructs such as dates and monetary amounts. 276 training set. Second, we employ a cross-category bootstrapping process that simultaneously trains a suite of classifiers for multiple semantic categories, using the positive instances for one semantic class as negative instances for the others. This cross-category training process gives the learner sustained"
P10-1029,W97-0313,1,0.771743,"ning process with fresh training instances, which may be substantially different from the ones seen previously, and we use the labels produced by the classifiers to dynamically create semantic features. We evaluate our approach by creating six semantic taggers using a collection of message board posts in the domain of veterinary medicine. Our results show this approach produces high-quality semantic taggers after a sustained bootstrapping cycle that maintains good precision while steadily increasing recall over many iterations. Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically"
P10-1029,D08-1106,0,0.0127089,"ouni and Florian, 2009)). has access to contextual features and cannot see the seed. Then we apply the classifier to the corpus to automatically label new instances, and combine these new instances with the seed-based instances. This process expands and diversifies the training set to fuel subsequent bootstrapping. Another challenge is that we want to use a small set of seeds to minimize the amount of human effort, and then use bootstrapping to fully exploit the domain-specific corpus. Iterative self-training, however, often has difficulty sustaining momentum or it succumbs to semantic drift (Komachi et al., 2008; McIntosh and Curran, 2009). To address these issues, we simultaneously induce a suite of classifiers for multiple semantic categories, using the positive instances of one semantic category as negative instances for the others. As bootstrapping progresses, the classifiers gradually improve themselves, and each other, over many iterations. We also explore a onesemantic-class-per-discourse (OSCPD) heuristic that infuses the learning process with fresh training instances, which may be substantially different from the ones seen previously, and we use the labels produced by the classifiers to dyna"
P10-1029,P08-1119,1,0.391517,"here the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). These techniques harvest broad-coverage semantic information from the Web using patterns and statistics, typically for the purpose of knowledge acquisition. Importantly, our goal is to classify instances in context, rather than generate lists of terms. In addition, the goal of our research is to learn specialized terms and jargon that may not be common on the Web, as well as domain-specific usages that may differ from the norm (e.g., “mix” and “lab” are usually ANIMALS in our domain). 2 Related Work Semantic class tagging is most closely related to named entity recogn"
P10-1029,W02-1028,1,0.88766,"be substantially different from the ones seen previously, and we use the labels produced by the classifiers to dynamically create semantic features. We evaluate our approach by creating six semantic taggers using a collection of message board posts in the domain of veterinary medicine. Our results show this approach produces high-quality semantic taggers after a sustained bootstrapping cycle that maintains good precision while steadily increasing recall over many iterations. Another line of relevant work is semantic class induction (e.g., (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009), where the goal is to induce a stand-alone dictionary of words with semantic class labels. These techniques are often designed to learn specialized terminology from unannotated domain-specific texts via bootstrapping. Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. Consequently, our classifier can also assign semantic class labels to pronouns. There has also been work on extracting semantically related terms or category members from the Web (e."
P10-1029,N03-1033,0,0.0041834,"1.0, when we end the bootstrapping process. In Section 4, we show that this sliding threshold outperforms fixed threshold values. Animal 612 Dis/Sym 900 Drug 369 Test 404 Human 818 Other 1723 To select seed words, we used the procedure proposed by Roark and Charniak (1998), ranking all of the head nouns in the training corpus by frequency and manually selecting the first 10 nouns that unambiguously belong to each category.7 This process is fast, relatively objective, and guaranteed to yield high-frequency terms, which is important for bootstrapping. We used the Stanford part-ofspeech tagger (Toutanova et al., 2003) to identify nouns, and our own simple rule-based NP chunker. 4 Evaluation 4.2 Baselines 4.1 To assess the difficulty of our data set and task, we evaluated several baselines. The first baseline searches for each head noun in WordNet and labels the noun as category Ck if it has a hypernym synset corresponding to that category. We manually identified the WordNet synsets that, to the best of our ability, seem to most closely correspond Data Our data set consists of message board posts from the Veterinary Information Network (VIN), which is a web site (www.vin.com) for professionals in veterinary"
P10-1029,P03-1044,0,0.103195,"organizations, and locations. Several bootstrapping methods for NER have been previously developed (e.g., (Collins and Singer, 1999; Niu et al., 2003)). NER systems, however, do not identify nominal NP instances (e.g., “a software manufacturer” or “the beach”), or handle semantic classes that are not associated with proper named entities (e.g., symptoms).1 ACE The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). Our bootstrapping model can be viewed as a form of self-training (e.g., (Ng and Cardie, 2003; Mihalcea, 2004; McClosky et al., 2006)), and cross-category training is similar in spirit to co-training (e.g., (Blum and Mitchell, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Mueller et al., 2002; Phillips and Riloff, 2002)). But, importantly, our classifiers all use the same feature set so they do not represent independent views of the data. They do, however, offer slightly different perspectives because each is at1 Some NER systems also handle specialized constructs such as dates and"
P10-1029,P95-1026,0,0.240345,"model incrementally trains semantic class taggers, so we explored the idea of using the labels assigned by the classifiers to create enhanced feature vectors by dynamically adding semantic features. This process allows later stages of bootstrapping to directly benefit from earlier stages. For example, consider the sentence: One Semantic Class Per Discourse We also explored the idea of using a one semantic class per discourse (OSCPD) heuristic to generate additional training instances during bootstrapping. Inspired by Yarowsky’s one sense per discourse heuristic for word sense disambiguation (Yarowsky, 1995), we make the assumption that multiple instances of a word in the same discourse will nearly always correspond to the same semantic class. Since our data set consists of message board posts organized as threads, we consider all posts in the same thread to be a single discourse. After each training step, we apply the classifiers to the unlabeled data to label some new instances. For each newly labeled instance, the OSCPD heuristic collects all instances with the same head noun in the same discourse (thread) and unilaterally labels them with the same semantic class. This heuristic serves as meta"
P10-1029,P98-2182,0,\N,Missing
P10-1029,C98-2177,0,\N,Missing
P10-2029,P04-1018,0,0.369513,"array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. chitecture of contemporary state-of-the-art learning-based coreference resolution systems; • support experimentation on most of the standard coreference resolution data sets; • implement most popular coreference resolution scoring metrics; • exhibit state-of-the-art coreference resolution performance (i.e., it can be configured to create a resolver that achieves performance close to the best reported results); • can be easily extended with new methods and features;"
P10-2029,H05-1004,0,0.76397,"Missing"
P10-2029,D08-1031,0,0.799412,"2004) and BART (Versley et al., 2008) (which can be considered a successor of GuiTaR) are both modular systems that target the full coreference resolution task. As such, both systems come close to meeting the majority of the desiderata set forth in Section 1. BART, in particular, can be considered an alternative to Reconcile, although we believe that Reconcile’s approach is more flexible than BART’s. In addition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art. Coreference resolution has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum an"
P10-2029,P02-1014,1,0.937135,"n be easily extended with new methods and features; • is relatively fast and easy to configure and run; • has a set of pre-built resolvers that can be used as black-box coreference resolution systems. While several other coreference resolution systems are publicly available (e.g., Poesio and Kabadjov (2004), Qiu et al. (2004) and Versley et al. (2008)), none meets all seven of these desiderata (see Related Work). Reconcile is a modular software platform that abstracts the basic architecture of most contemporary supervised learningbased coreference resolution systems (e.g., Soon et al. (2001), Ng and Cardie (2002), Bengtson and Roth (2008)) and achieves performance comparable to the state-of-the-art on several benchmark data sets. Additionally, Reconcile can be easily reconfigured to use different algorithms, features, preprocessing elements, evaluation settings and metrics. In the rest of this paper, we review related work (Section 2), describe Reconcile’s organization and components (Section 3) and show experimental results for Reconcile on six data sets and two evaluation metrics (Section 4). 2 3 System Description Reconcile was designed to be a research testbed capable of implementing most current"
P10-2029,P05-1045,0,0.0134027,"Missing"
P10-2029,N07-1051,0,0.00913438,"Missing"
P10-2029,poesio-kabadjov-2004-general,0,0.252363,"on task (e.g., MUC-6 (1995), ACE NIST (2004)), and the data sets created for these evaluations have become standard benchmarks in the field (e.g., MUC and ACE data sets). However, it is still frustratingly difficult to compare results across different coreference resolution systems. Reported coreference resolution scores vary wildly across data sets, evaluation metrics, and system configurations. • implement the basic underlying software ar156 Proceedings of the ACL 2010 Conference Short Papers, pages 156–161, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics (Poesio and Kabadjov, 2004) and BART (Versley et al., 2008) (which can be considered a successor of GuiTaR) are both modular systems that target the full coreference resolution task. As such, both systems come close to meeting the majority of the desiderata set forth in Section 1. BART, in particular, can be considered an alternative to Reconcile, although we believe that Reconcile’s approach is more flexible than BART’s. In addition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengts"
P10-2029,qiu-etal-2004-public,0,0.58709,"the standard coreference resolution data sets; • implement most popular coreference resolution scoring metrics; • exhibit state-of-the-art coreference resolution performance (i.e., it can be configured to create a resolver that achieves performance close to the best reported results); • can be easily extended with new methods and features; • is relatively fast and easy to configure and run; • has a set of pre-built resolvers that can be used as black-box coreference resolution systems. While several other coreference resolution systems are publicly available (e.g., Poesio and Kabadjov (2004), Qiu et al. (2004) and Versley et al. (2008)), none meets all seven of these desiderata (see Related Work). Reconcile is a modular software platform that abstracts the basic architecture of most contemporary supervised learningbased coreference resolution systems (e.g., Soon et al. (2001), Ng and Cardie (2002), Bengtson and Roth (2008)) and achieves performance comparable to the state-of-the-art on several benchmark data sets. Additionally, Reconcile can be easily reconfigured to use different algorithms, features, preprocessing elements, evaluation settings and metrics. In the rest of this paper, we review rel"
P10-2029,P07-1107,0,0.113384,"rt supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art. Coreference resolution has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. chitecture of contemporary state-of-the-art learning-based coreference resolution systems; • support experimentation on most of the standard coreference resolution data sets; • implement most popular coreference resolution scoring metrics; • exhibit state-of-the-art coreference"
P10-2029,J01-4004,0,0.984178,"hough we believe that Reconcile’s approach is more flexible than BART’s. In addition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art. Coreference resolution has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. chitecture of contemporary state-of-the-art le"
P10-2029,P09-1074,1,0.696567,"Missing"
P10-2029,P08-4003,0,0.235695,"(2004)), and the data sets created for these evaluations have become standard benchmarks in the field (e.g., MUC and ACE data sets). However, it is still frustratingly difficult to compare results across different coreference resolution systems. Reported coreference resolution scores vary wildly across data sets, evaluation metrics, and system configurations. • implement the basic underlying software ar156 Proceedings of the ACL 2010 Conference Short Papers, pages 156–161, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics (Poesio and Kabadjov, 2004) and BART (Versley et al., 2008) (which can be considered a successor of GuiTaR) are both modular systems that target the full coreference resolution task. As such, both systems come close to meeting the majority of the desiderata set forth in Section 1. BART, in particular, can be considered an alternative to Reconcile, although we believe that Reconcile’s approach is more flexible than BART’s. In addition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in pe"
P10-2029,P03-1023,0,0.148118,"has received much research attention, resulting in an array of approaches, algorithms and features. Reconcile is modeled after typical supervised learning approaches to coreference resolution (e.g. the architecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems. However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. chitecture of contemporary state-of-the-art learning-based coreference resolution systems; • support experimentation on most of the standard coreference resolution data sets; • implement most popular coreference resolution scoring metrics; • exhibit state-of-the-art coreference resolution performance (i.e., it can be configured to create a resolver that achieves performance close to the best reported results); • c"
P10-2029,M95-1005,0,\N,Missing
P10-2029,J94-4002,0,\N,Missing
P11-1114,P07-1073,0,0.0111927,"96; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to exFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different documents. (L"
P11-1114,P05-1045,0,0.00556965,"represent four words to the left and four words to the right of the targeted NP, as well as the head noun and modifiers (adjectives and noun modifiers) of the targeted NP itself. Lexico-syntactic patterns: we use the AutoSlog pattern generator (Riloff, 1993) to automatically create lexico-syntactic patterns around each noun phrase in the sentence. These patterns are similar to dependency relations in that they typically represent the syntactic role of the NP with respect to other constituents (e.g., subject-of, object-of, and noun arguments). Semantic features: we use the Stanford NER tagger (Finkel et al., 2005) to determine if the targeted NP is a named entity, and we use the Sundance parser (Riloff and Phillips, 2004) to assign semantic class labels to each NP’s head noun. 4 Event Narrative Document Classification One of our goals was to explore the use of document genre to permit more aggressive strategies for extracting role fillers. In this section, we first present an analysis of the MUC-4 data set which reveals the distribution of event narratives in the corpus, and then explain how we train a classifier to automatically identify event narrative stories. 4.1 Manual Analysis We define an event"
P11-1114,P98-1067,0,0.85422,"given to the role filler extractors. This multi-layered approach creates an event extraction system that can discover role fillers in a variety of different contexts, while maintaining good precision. In the following sections, we position our research with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. 1138 2 Related Work Some event extraction data sets only include documents that describe relevant events (e.g., wellknown data sets for the domains of corporate acquisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant docum"
P11-1114,P06-1061,0,0.428326,"earch with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. 1138 2 Related Work Some event extraction data sets only include documents that describe relevant events (e.g., wellknown data sets for the domains of corporate acquisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either"
P11-1114,P08-1030,0,0.208204,"traction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to exFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 20"
P11-1114,M91-1033,1,0.350906,"ocument, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and"
P11-1114,W05-0610,0,0.268281,"tterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to exFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependenc"
P11-1114,P10-1081,0,0.254612,")), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to exFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 2007) developed a system that learns to recognize event sentences and uses patterns tha"
P11-1114,P07-1075,0,0.062688,"reitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to exFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redu"
P11-1114,D07-1075,1,0.933146,"t. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 2007) developed a system that learns to recognize event sentences and uses patterns that have a semantic affinity for an event role to extract role fillers. GLACIER (Patwardhan and Riloff, 2009) jointly considers sentential evidence and phrasal evidence in a unified probabilistic framework. Our research follows in the same spirit as these approaches by performing multiple levels of text analysis. But our event extraction model includes two novel contributions: (1) we develop a set of role-specific sentence classifiers to learn to recognize secondary contexts associated with each type of event role"
P11-1114,D09-1016,1,0.924209,"cross events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 2007) developed a system that learns to recognize event sentences and uses patterns that have a semantic affinity for an event role to extract role fillers. GLACIER (Patwardhan and Riloff, 2009) jointly considers sentential evidence and phrasal evidence in a unified probabilistic framework. Our research follows in the same spirit as these approaches by performing multiple levels of text analysis. But our event extraction model includes two novel contributions: (1) we develop a set of role-specific sentence classifiers to learn to recognize secondary contexts associated with each type of event role , and (2) we exploit text genre to incorporate a third level of analysis that enables the system to aggressively hunt for role fillers in documents that are event narratives. In Section 5,"
P11-1114,P06-2094,0,0.205751,"ed as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. How"
P11-1114,N06-1039,0,0.0519882,"e characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. How"
P11-1114,P05-1047,0,0.150983,"relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and sea"
P11-1114,P03-1029,0,0.467129,"red of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extrac"
P11-1114,C00-2136,0,0.613195,"ction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis"
P11-1114,P05-1062,0,0.228025,"utomatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to exFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-b"
P11-1114,C98-1064,0,\N,Missing
P11-2054,W07-1011,0,0.0799452,"Missing"
P13-2015,P98-1012,0,0.0418012,"Missing"
P13-2015,W11-0210,0,0.163373,"nce resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures. 1 Introduction Coreference resolvers are typically evaluated on collections of news articles that cover a wide range of topics, such as the ACE (ACE03, 2003; ACE04, 2004; ACE05, 2005) and OntoNotes (Pradhan et al., 2007) data sets. Many NLP applications, however, involve text analysis for specialized domains, such as clinical medicine (Gooch and Roudsari, 2012; Glinos, 2011), legal text analysis (Bouayad-Agha et al., 2009), and biological literature (Batista-Navarro and Ananiadou, 2011; Casta˜no et al., 2002). Learning-based coreference resolvers can be easily retrained for a specialized domain given annotated training texts for that domain. However, we found that retraining an off-the-shelf coreference resolver with domainspecific texts showed little benefit. This surprising result led us to question the nature of the feature sets used by noun phrase (NP) coreference resolvers. Nearly all of the features employed by recent systems fall into three categories: string match and word overlap, syntactic properties (e.g., appositives, predicate nominals, parse features, etc.), a"
P13-2015,N04-1038,1,0.787186,"ng and context for entity tracking in ACE. Ng (2007) used lexical information to assess the likelihood of a noun phrase being anaphoric, but this did not show clear improvements on ACE data. There has been previous work on domainspecific coreference resolution for several domains, including biological literature (Casta˜no et al., 2002; Liang and Lin, 2005; Gasperin and Briscoe, 2008; Kim et al., 2011; Batista-Navarro and Ananiadou, 2011), clinical medicine (He, 2007; Zheng et al., 2011; Glinos, 2011; Gooch and Roudsari, 2012) and legal documents (BouayadAgha et al., 2009). In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves. To the best of our knowledge, our work is the first to examine the impact of lexicalized features for domain-specific coreference resolution. vious work has evaluated the benefit of lexical features only for broad-coverage data sets. Bengston and Roth (2008) incorporated a memorization feature to learn which entities can refer to one another. They created a bi"
P13-2015,D08-1031,0,0.167931,"inos, 2011; Gooch and Roudsari, 2012) and legal documents (BouayadAgha et al., 2009). In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves. To the best of our knowledge, our work is the first to examine the impact of lexicalized features for domain-specific coreference resolution. vious work has evaluated the benefit of lexical features only for broad-coverage data sets. Bengston and Roth (2008) incorporated a memorization feature to learn which entities can refer to one another. They created a binary feature for every pair of head nouns, including pronouns. They reported no significant improvement from these features on the ACE 2004 data. Rahman and Ng (2011a) also utilized lexical features, going beyond strict memorization with methods to combat data sparseness and incorporating semantic information. They created a feature for every ordered pair of head nouns (for pronouns and nominals) or full NPs (for proper nouns). Semi-lexical features were also used when one NP was a Named Ent"
P13-2015,W11-1905,0,0.0359949,"Missing"
P13-2015,D10-1048,0,0.0250681,"ic information is often impossible using only general-purpose resources. For example, WordNet defines “tomcat” only as an animal, does not contain an entry for “UAW”, and categorizes “anthrax” and “diagnosis” very differently.1 In this paper, we evaluate the impact of lexicalized features on 4 domains: management succession (MUC-6 data), vehicle launches (MUC-7 data), disease outbreaks (ProMed texts), and terrorism (MUC-4 data). We incorporate lexicalized feature sets into two different coreference architectures: Reconcile (Stoyanov et al., 2010), a pairwise coreference classifier, and Sieve (Raghunathan et al., 2010), a rule-based system. Our results show that lexicalized features significantly improve performance in all four domains and in both types of coreference architectures. Most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words. In this paper, we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution. We show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four"
P13-2015,N04-1001,0,0.0264483,"8 76.29 75.56 P 80.22 77.35 74.52 71.76 MUC-4 R 60.81 64.19 65.65 67.37 F 69.18 70.16 69.80 69.50 Table 1: Cross-domain B3 (Bagga and Baldwin, 1998) results for Reconcile with its general feature set. The Paired Permutation test (Pesarin, 2001) was used for statistical significance testing and gray cells represent results that are not significantly different from the best result. for training. Our work aims to show the benefit of lexical features using much smaller training sets (&lt; 50 documents) focused on specific domains. Lexical features have also been used for slightly different purposes. Florian et al. (2004) utilized lexical information such as mention spelling and context for entity tracking in ACE. Ng (2007) used lexical information to assess the likelihood of a noun phrase being anaphoric, but this did not show clear improvements on ACE data. There has been previous work on domainspecific coreference resolution for several domains, including biological literature (Casta˜no et al., 2002; Liang and Lin, 2005; Gasperin and Briscoe, 2008; Kim et al., 2011; Batista-Navarro and Ananiadou, 2011), clinical medicine (He, 2007; Zheng et al., 2011; Glinos, 2011; Gooch and Roudsari, 2012) and legal docume"
P13-2015,P11-1082,0,0.0128101,"information to match the compatibility of contexts surrounding NPs, not the NPs themselves. To the best of our knowledge, our work is the first to examine the impact of lexicalized features for domain-specific coreference resolution. vious work has evaluated the benefit of lexical features only for broad-coverage data sets. Bengston and Roth (2008) incorporated a memorization feature to learn which entities can refer to one another. They created a binary feature for every pair of head nouns, including pronouns. They reported no significant improvement from these features on the ACE 2004 data. Rahman and Ng (2011a) also utilized lexical features, going beyond strict memorization with methods to combat data sparseness and incorporating semantic information. They created a feature for every ordered pair of head nouns (for pronouns and nominals) or full NPs (for proper nouns). Semi-lexical features were also used when one NP was a Named Entity, and unseen features were used when the NPs were not in the training set. Their features did yield improvements on both the ACE 2005 and OntoNotes-2 data, but the semilexical features included Named Entity classes as well as word-based features. Rahman and Ng (2011"
P13-2015,C08-1033,0,0.0269222,"f lexical features using much smaller training sets (&lt; 50 documents) focused on specific domains. Lexical features have also been used for slightly different purposes. Florian et al. (2004) utilized lexical information such as mention spelling and context for entity tracking in ACE. Ng (2007) used lexical information to assess the likelihood of a noun phrase being anaphoric, but this did not show clear improvements on ACE data. There has been previous work on domainspecific coreference resolution for several domains, including biological literature (Casta˜no et al., 2002; Liang and Lin, 2005; Gasperin and Briscoe, 2008; Kim et al., 2011; Batista-Navarro and Ananiadou, 2011), clinical medicine (He, 2007; Zheng et al., 2011; Glinos, 2011; Gooch and Roudsari, 2012) and legal documents (BouayadAgha et al., 2009). In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves. To the best of our knowledge, our work is the first to examine the impact of lexicalized features for domain-specific corefer"
P13-2015,P09-1074,1,0.839837,"nd y, L EX S ET ( X , Y ) = 1 if y ∈ Coref Set(x), or 0 otherwise. The benefit of the set-based features over a single monolithic feature is that the classifier has one set-based feature for each mention found in the training data, so it can learn to handle individual terms differently. We also tried encoding a separate feature for each distinct pair of words, analogous to the memorization feature in Bengston and Roth (2008). This did not improve performance as much as the other feature representations presented here. Extracting Coreferent Training Pairs We adopt the terminology introduced by Stoyanov et al. (2009) to define a coreference element (CE) as a noun phrase that can participate in a coreference relation based on the task definition. Each training document has manually annotated gold coreference chains corresponding to the sets of CEs that are coreferent. For each CE in a gold chain, we pair that CE with all of the other CEs in the same chain. We consider the coreference relation to be bi-directional, so we don’t retain information about which CE was the antecedent. We do not extract CE pairs that share the same head noun because they are better handled with string match. For nominal NPs, we r"
P13-2015,P10-2029,1,0.909771,"be coreferent with “diagnosis”. Capturing these types of domain-specific information is often impossible using only general-purpose resources. For example, WordNet defines “tomcat” only as an animal, does not contain an entry for “UAW”, and categorizes “anthrax” and “diagnosis” very differently.1 In this paper, we evaluate the impact of lexicalized features on 4 domains: management succession (MUC-6 data), vehicle launches (MUC-7 data), disease outbreaks (ProMed texts), and terrorism (MUC-4 data). We incorporate lexicalized feature sets into two different coreference architectures: Reconcile (Stoyanov et al., 2010), a pairwise coreference classifier, and Sieve (Raghunathan et al., 2010), a rule-based system. Our results show that lexicalized features significantly improve performance in all four domains and in both types of coreference architectures. Most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words. In this paper, we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution. We show that adding lexicalized features to off-the"
P13-2015,M95-1005,0,0.65405,"We require a frequency ≥ 2 to minimize overfitting because many cases occur only once in the training data. 83 launches and consists of 30 training texts and 20 test texts. We used these standard train/test splits to be consistent with previous work. We also created 2 new coreference data sets which we will make freely available. We manually annotated 45 ProMed-mail articles (www.promedmail.org) about disease outbreaks and 45 MUC-4 texts about terrorism, following the MUC guidelines (Hirschman, 1997). Interannotator agreement between two annotators was .77 (κ) on ProMed and .84 (MUC F Score)(Villain et al., 1995) on both ProMed and MUC-4.3 We performed 5-fold cross-validation on both data sets and report the micro-averaged results. Gold CE spans were used in all experiments to factor out issues with markable identification and anaphoricity across the different domains. 4.2 p ≤ 0.05. MUC-4 showed the largest gain for Reconcile, with the F score increasing from 69.5 to over 74. For most domains, adding the lexical features to Reconcile substantially increased precision with comparable levels of recall. The bottom half of Table 2 contains the results of adding a lexical heuristic to Sieve. The first row"
P13-2015,W11-1813,1,0.904115,"Missing"
P13-2015,I05-1065,0,0.0556417,"to show the benefit of lexical features using much smaller training sets (&lt; 50 documents) focused on specific domains. Lexical features have also been used for slightly different purposes. Florian et al. (2004) utilized lexical information such as mention spelling and context for entity tracking in ACE. Ng (2007) used lexical information to assess the likelihood of a noun phrase being anaphoric, but this did not show clear improvements on ACE data. There has been previous work on domainspecific coreference resolution for several domains, including biological literature (Casta˜no et al., 2002; Liang and Lin, 2005; Gasperin and Briscoe, 2008; Kim et al., 2011; Batista-Navarro and Ananiadou, 2011), clinical medicine (He, 2007; Zheng et al., 2011; Glinos, 2011; Gooch and Roudsari, 2012) and legal documents (BouayadAgha et al., 2009). In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters. But BABAR acquired and used lexical information to match the compatibility of contexts surrounding NPs, not the NPs themselves. To the best of our knowledge, our work is the first to examine the impact of lexicalized features"
P13-2015,P02-1014,0,0.0800724,"significantly better than without it. In Sieve’s high-precision coreference architecture, the lexical heuristic yields additional recall gains without sacrificing much precision. Reconcile +LexLookup +LexSets Sieve +LexBegin +LexEnd Coreference Resolution Models We conducted experiments using two coreference resolution architectures. Reconcile4 (Stoyanov et al., 2010) is a freely available pairwise mention classifier. For classification, we chose Weka’s (Witten and Frank, 2005) Decision Tree learner inside Reconcile. Reconcile contains roughly 60 features (none lexical), largely modeled after Ng and Cardie (2002). We modified Reconcile’s Single Link clustering scheme to enforce an additional rule that non-overlapping proper names cannot be merged into the same chain. We also conducted experiments with the Sieve coreference resolver, which applies high precision heuristic rules to incrementally build coreference chains. We implemented the L EX L OOKUP ( X , Y ) feature as an additional heuristic rule. We tried inserting this heuristic before Sieve’s other rules (LexBegin), and also after Sieve’s other rules (LexEnd). 4.3 P 70.59 71.32 71.44 90.09 86.54 87.00 ACE 2004 R 83.09 82.93 83.45 74.23 75.43 75."
P13-2015,C98-1012,0,\N,Missing
P18-1120,P08-1090,0,0.169818,"Missing"
P18-1120,P09-1068,0,0.113571,"Missing"
P18-1120,elson-mckeown-2010-building,0,0.0209272,"and go to the doctor for medical services. People travel to specific destinations to enjoy the beach, go skiing, or see historical sites. For most places, people typically go there for a common set of reasons, which we will refer to as prototypical goal activities (goal-acts) for a location. For example, a prototypical goal-act for restaurants would be “eat food” and for IKEA would be “buy furniture”. Previous research has established that recognizing people’s goals is essential for narrative text understanding and story comprehension (Schank and Abelson, 1977; Wilensky, 1978; Lehnert, 1981; Elson and McKeown, 2010; Goyal et al., 2013). In sentence (a), we infer that she retrieved chicken (e.g., from the refrigerator) but did not pay for it. In (b), we infer that she paid for the chicken but probably did not eat it at the supermarket. In (c), we infer that she ate the chicken at the restaurant. Note how the verb “got” maps to different presumed events depending on the location. Our research aims to learn the prototypical goalacts for locations using a text corpus. First, we extract activities that co-occur with locations in goaloriented syntactic patterns. Next, we construct an activity profile matrix t"
P18-1120,P13-1174,0,0.0327311,"Missing"
P18-1120,N09-1030,0,0.0527916,"Missing"
P18-1120,D10-1008,1,0.879116,"Missing"
P18-1120,E12-1034,0,0.0500299,"Missing"
P18-1120,P14-5010,0,0.00246739,"Burton et al., 2011). Since our interest is learning about the activities of ordinary people in their daily lives, we use the Weblog subset of the Spinn3r corpus, which contains over 133 million blog posts. We use the text data to identify activities that are potential goal-acts for a location. However we also need to identify locations and want to include both proper names (e.g., Disneyland) as well as nominals (e.g., store, beach), so Named Entity Recognition will not suffice. Consequently, we extract (Loc, Act) pairs using syntactic patterns. First, we apply the Stanford dependency parser (Manning et al., 2014). We then extract sentences that match the pattern “go to X to Y ” with the 1298 l1 = McDonald’s l2 = Burger King l3 = bookstore .. . ln = church a1 = buy book .10 .12 .40 a2 = eat burger .30 .50 .02 .. . .05 ... am = pray .01 .02 .04 .01 .70 Table 1: An illustration of the activity profile matrix Y . following conditions: (1) there exists a subject connecting to “go”, (2) X has an nmod (nominal modifier) relation to “go” (lemma), (3) X is a noun or noun compound, (4) Y has an xcomp relation (open clausal complement) with “go”, and (5) Y is a verb. Figure 1 depicts the intended syntactic struc"
P18-1120,D14-1162,0,0.0811427,"tween activities can often be implied by their lexical overlap, e.g., two activities sharing the same verb or noun might be related. For each word belonging to any of our activities, we use WordNet (Miller, 1995) to find its synonyms. We also include the word itself in the synonym set. If the synonym sets of two words overlap, we call these two words “match”. Then we define the lexical overlap similarity function between ai and ak :   if verb and noun match 1 O sim (ai , ak ) = 0.5 if verb or noun match   0 otherwise (8) As a third option, we can use 300-dimension word embedding vectors (Pennington et al., 2014) trained on 840 billion tokens of web data to compute semantic similarity. We compute an activity’s embedding as the average of its words’ embeddings. Let simE (ai , ak ) be the cosine value between the embedding vectors of ai and ak : simE (ai , ak ) = coshEmbed(ai ), Embed(ak )i (9) Finally, we can plug these similarity functions into Eq (6). We use AL , AO , AE to denote the corresponding matrix. We can also plug in multiple similarity metrics such as (simL + simE )/2 and use combination symbols AL+E to denote the matrix. 3.3.5 Injecting Activity Similarity Once we have a similarity matrix"
P18-1120,E14-1024,0,0.0419306,"Missing"
P18-1120,E09-1077,0,0.0522644,"Missing"
P18-1120,P10-1149,0,0.0435961,"Missing"
P99-1048,W97-0319,0,0.145617,"Missing"
P99-1048,P98-2182,0,0.0334419,"Missing"
P99-1048,C96-1021,0,\N,Missing
P99-1048,J94-4002,0,\N,Missing
P99-1048,C98-2177,0,\N,Missing
S12-1028,A97-1029,0,0.0509039,"iloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based 200 method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associate words with high-level semantic classes (e.g., a “house” is a transient structure). Semantic class tagging is closely related to named entity recognition (NER) (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002)). Some bootstrapping methods have been used for NER (e.g., (Collins and Singer, 1999; Niu et al., 2003) to learn from unannotated texts. However, most NER systems will not label nominal noun phrases (e.g., they will not identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with"
S12-1028,J96-2004,0,0.0148464,"ntext. A noun with multiple senses could get assigned different semantic class labels in different contexts. The annotators first annotated 13 of the 23 documents, and discussed the cases where they disagreed. Then they independelty annotated 12 http://www.promedmail.org/ We omit the details of the coreference annotations since it is not the focus of this research. However, the annotators measured their agreement on 10 documents and achieved MUC scores of Precision = .82, Recall = .86, F-measure = .84. 13 the remaining 10 documents and measured interannotator agreement with Cohen’s Kappa (κ) (Carletta, 1996). The κ score for these 10 documents was 0.91, indicating a high level of agreement. The annotators then adjudicated their disagreements on all 23 documents to create the gold standard. 4.4 Dictionary Evaluation To assess the quality of the lexicons, we estimated their accuracy by compiling external word lists from freely available sources such as Wikipedia14 and WordNet (Miller, 1990). Table 1 shows the sources that we used, where the bracketed items refer to WordNet hypernym categories. We searched each WordNet hypernym tree (also, instancerelationship) for all senses of the word. Additional"
S12-1028,W09-2201,0,0.0785986,"as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010) developed a bootstrapping technique that induces a semantic tagger from unannotated texts. We use their system in our ensemble. There has also been work on extracting semantic class members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). This line of research is fundamentally different from ours because these techniques benefit from the vast repository of information available on the Web and are therefore designed to harvest a wide swath of general-purpose semantic information. Our research is aimed at acquiring domain-specific semantic dictionaries using a collection of documents representing a specialized domain. 3 Ensemble-based Semantic Lexicon Induction 3.1 Motivation Our research combines three fundamentally different techniques into an ensemble-based bootstrapping framework for semantic lexicon induction: pattern-bas"
S12-1028,W99-0613,0,0.258683,"cIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g., (Collins and Singer, 1999; Niu et al., 2003; Huang and Riloff, 2010)) label word instances and focus on the local context surrounding each instance. The difference between these approaches is that semantic taggers make decisions based on a single context and can assign different labels to different instances, whereas lexicon induction algorithms compile corpus statistics from multiple instances of a word and typically assign each word to a single semantic category.1 We also hypothesize that coreference resolution can be exploited to infer semantic 1 This approach would be untenable for broad-coverage semantic knowledg"
S12-1028,W99-0612,0,0.0770518,"ll, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based 200 method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associate words with high-level semantic classes (e.g., a “house” is a transient structure). Semantic class tagging is closely related to named entity recognition (NER) (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002)). Some bootstrapping methods have been used for NER (e.g., (Collins and Singer, 1999; Niu et al., 2003) to learn from unannotated texts. However, most NER systems will not label nominal noun phrases (e.g., they will not identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010"
S12-1028,W02-1029,0,0.0427435,"sh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based 200 method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associate words with high-level semantic classes (e.g., a “house” is a transient structure). Semantic class tagging is closely related to named entity recognition (NER) (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002)). Some bootstrapping methods have been used for NER (e.g., (Collins and Singer, 1999; Niu et al., 2003) to learn from unannotated texts. However, most NER systems will not label"
S12-1028,C02-1130,0,0.140774,"mple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based 200 method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associate words with high-level semantic classes (e.g., a “house” is a transient structure). Semantic class tagging is closely related to named entity recognition (NER) (e.g., (Bikel et al., 1997; Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Fleischman and Hovy, 2002)). Some bootstrapping methods have been used for NER (e.g., (Collins and Singer, 1999; Niu et al., 2003) to learn from unannotated texts. However, most NER systems will not label nominal noun phrases (e.g., they will not identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010) developed a bootstrapping"
S12-1028,P10-1029,1,0.853471,"ut accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g., (Collins and Singer, 1999; Niu et al., 2003; Huang and Riloff, 2010)) label word instances and focus on the local context surrounding each instance. The difference between these approaches is that semantic taggers make decisions based on a single context and can assign different labels to different instances, whereas lexicon induction algorithms compile corpus statistics from multiple instances of a word and typically assign each word to a single semantic category.1 We also hypothesize that coreference resolution can be exploited to infer semantic 1 This approach would be untenable for broad-coverage semantic knowledge acquisition, but within a specialized dom"
S12-1028,P08-1119,1,0.865048,"identify “the dentist” as a person) or recognize semantic classes that are not associated with proper named entities (e.g., symptoms).2 ACE mention detection systems (e.g., (ACE, 2007; ACE, 2008)) can label noun phrases that are associated with 5-7 semantic classes and are typically trained with supervised learning. Recently, (Huang and Riloff, 2010) developed a bootstrapping technique that induces a semantic tagger from unannotated texts. We use their system in our ensemble. There has also been work on extracting semantic class members from the Web (e.g., (Pas¸ca, 2004; Etzioni et al., 2005; Kozareva et al., 2008; Carlson et al., 2009)). This line of research is fundamentally different from ours because these techniques benefit from the vast repository of information available on the Web and are therefore designed to harvest a wide swath of general-purpose semantic information. Our research is aimed at acquiring domain-specific semantic dictionaries using a collection of documents representing a specialized domain. 3 Ensemble-based Semantic Lexicon Induction 3.1 Motivation Our research combines three fundamentally different techniques into an ensemble-based bootstrapping framework for semantic lexicon"
S12-1028,U08-1013,0,0.0881974,"ry lookup on a collection of disease outbreak articles. Our results show that the induced dictionaries yield better performance than an instance-based semantic tagger, achieving higher accuracy with comparable levels of recall. 2 Related Work Several techniques have been developed for semantic class induction (also called set expansion) using bootstrapping methods that consider co-occurrence statistics based on nouns (Riloff and Shepherd, 1997), syntactic structures (Roark and Charniak, 1998; Phillips and Riloff, 2002), and contextual patterns (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2008; McIntosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based 200 method for automatic thesaurus constr"
S12-1028,P09-1045,0,0.545199,"ch as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g., (Collins and Singer, 1999"
S12-1028,D10-1035,0,0.0613054,"or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g., (Collins and Singer, 1999; Niu et al., 200"
S12-1028,P07-1068,0,0.0284172,"omains, such as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g."
S12-1028,P03-1043,0,0.154734,"McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tagging (e.g., (Collins and Singer, 1999; Niu et al., 2003; Huang and Riloff, 2010)) label word instances and focus on the local context surrounding each instance. The difference between these approaches is that semantic taggers make decisions based on a single context and can assign different labels to different instances, whereas lexicon induction algorithms compile corpus statistics from multiple instances of a word and typically assign each word to a single semantic category.1 We also hypothesize that coreference resolution can be exploited to infer semantic 1 This approach would be untenable for broad-coverage semantic knowledge acquisition, but"
S12-1028,W02-1017,1,0.931704,"esources are not always sufficient for specialized domains, such as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, cla"
S12-1028,W97-0313,1,0.790853,"e of WordNet (Miller, 1990). However, off-the-shelf resources are not always sufficient for specialized domains, such as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words"
S12-1028,P98-2182,0,0.180905,". However, off-the-shelf resources are not always sufficient for specialized domains, such as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual"
S12-1028,P10-2029,1,0.756738,"epeats. Although the coreference chains remain the same throughout the process, the lexicon grows so more words in the chains have semantic class labels as bootstrapping progresses. Bootstrapping ends when fewer than 5 words are learned for each of the semantic classes. Many noun phrases are singletons (i.e., they are not coreferent with any other NPs), which limits the set of words that can be learned using coreference chains. Furthermore, coreference resolvers make mistakes, so the accuracy of the induced lexicons depends on the quality of the chains. For our experiments, we used Reconcile (Stoyanov et al., 2010), a freely available supervised coreference resolver. 3.3 Ensemble-based Bootstrapping Framework Figure 1 shows the architecture of our ensemblebased bootstrapping framework. Initially, each lexicon only contains the seed nouns. Each component hypothesizes a set of candidate words for each semantic class, based on its own criteria. The word lists produced by the three systems are then compared, and we retain only the words that were hypothesized with the same class label by at least two of the three systems. The remaining words are discarded. The consenus words are added to the lexicon, and th"
S12-1028,W02-1028,1,0.958438,"ficient for specialized domains, such as medicine, chemistry, or microelectronics. Furthermore, in virtually every domain, texts contain lexical variations that are often missing from dictionaries, such as acronyms, abbreviations, spelling variants, informal shorthand terms (e.g., “abx” for “antibiotics”), and composite terms (e.g., “maydecember” or “virus/worm”). To address this problem, techniques have been developed to automate the construction of semantic lexicons from text corpora using bootstrapping methods (Riloff and Shepherd, 1997; Roark and Charniak, 1998; Phillips and Riloff, 2002; Thelen and Riloff, 2002; Ng, 2007; McIntosh and Curran, 2009; McIntosh, 2010), but accuracy is still far from perfect. Our research explores the use of ensemble methods to improve the accuracy of semantic lexicon induction. Our observation is that semantic class associations can be learned using several fundamentally different types of corpus analysis. Bootstrapping methods for semantic lexicon induction (e.g., (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2009)) collect corpus-wide statistics for individual words based on shared contextual patterns. In contrast, classifiers for semantic tag"
S12-1028,N09-1033,0,0.0125442,"ng higher accuracy with comparable levels of recall. 2 Related Work Several techniques have been developed for semantic class induction (also called set expansion) using bootstrapping methods that consider co-occurrence statistics based on nouns (Riloff and Shepherd, 1997), syntactic structures (Roark and Charniak, 1998; Phillips and Riloff, 2002), and contextual patterns (Riloff and Jones, 1999; Thelen and Riloff, 2002; McIntosh and Curran, 2008; McIntosh and Curran, 2009). To improve the accuracy of induced lexicons, some research has incorporated negative information from human judgements (Vyas and Pantel, 2009), automatically discovered negative classes (McIntosh, 2010), and distributional similarity metrics to recognize concept drift (McIntosh and Curran, 2009). Phillips and Riloff (2002) used co-training (Blum and Mitchell, 1998) to exploit three simple classifiers that each recognized a different type of syntactic structure. The research most closely related to ours is an ensemble-based 200 method for automatic thesaurus construction (Curran, 2002). However, that goal was to acquire finegrained semantic information that is more akin to synonymy (e.g., words similar to “house”), whereas we associa"
S12-1028,C98-2177,0,\N,Missing
S19-1022,P13-1052,0,0.0307543,"Missing"
S19-1022,D14-1214,0,0.0142525,"CAL: die hurt pain kill sick blood hospital dead drug surgery M ONEY /J OB: job pay money deal sell business price sale purchase dollar E QUIPMENT: car phone computer bike camera chair boat machine desk laptop F OOD /D RINK: eat water food dinner drink lunch breakfast cake meal chocolate P EOPLE: people friend guy girl man kid mom someone everyone family E MOTION: good love nice fun bad happy best better smile enjoy laugh hate kind beautiful wrong amazing awesome funny crazy worry OTHER: be have do get go time say make think take day come look thing tell start way try last year 2013), tweets (Li et al., 2014), news (Deng et al., 2013; Deng and Wiebe, 2014), and personal blogs (Ding and Riloff, 2016; Reed et al., 2017; Ding and Riloff, 2018b). Recently, Ding et al. (2018) further characterized affective events in terms of human needs categories: Physiological, Health, Leisure, Social, Financial, Cognition, and Freedom, which can explain why an event is positive or negative. Subsequently, Ding and Riloff (2018a) designed supervised learning models and a semisupervised co-training model to assign affective events to Human Needs categories. Our work exploits recent advances in distributed word represe"
S19-1022,P13-2022,0,0.196553,"levant semantic concepts improves both the recall and precision of Human Needs categorization for events. 1 Introduction Affective events have a positive or negative impact on the people who experience the event. For example, being hired for a job is typically a beneficial (positive) event, but being fired is usually a detrimental (negative) event. Recognizing affective events is critical to understand people’s motivations, goals, desires, and empathy in narrative stories and conversations. Previous research has proposed several methods to recognize affective events and their polarity (e.g., (Deng et al., 2013; Vu et al., 2014; Reed et al., 2017; Ding and Riloff, 2016)). To achieve a deeper level of understanding, Ding and Riloff (2018a) further classified affective events into categories associated with theories of Human Needs (Maslow et al., 1970; MaxNeef et al., 1991) in psychology: Physiological, Health, Leisure, Social, Finance, and Cognition, to characterize the reason for the event’s affective polarity. For example, breaking your arm is a negative event because it violates a need to maintain 2 Related Work Previous work in NLP on affective events has primarily focused on identifying the affe"
S19-1022,E14-1040,0,0.240103,"dead drug surgery M ONEY /J OB: job pay money deal sell business price sale purchase dollar E QUIPMENT: car phone computer bike camera chair boat machine desk laptop F OOD /D RINK: eat water food dinner drink lunch breakfast cake meal chocolate P EOPLE: people friend guy girl man kid mom someone everyone family E MOTION: good love nice fun bad happy best better smile enjoy laugh hate kind beautiful wrong amazing awesome funny crazy worry OTHER: be have do get go time say make think take day come look thing tell start way try last year 2013), tweets (Li et al., 2014), news (Deng et al., 2013; Deng and Wiebe, 2014), and personal blogs (Ding and Riloff, 2016; Reed et al., 2017; Ding and Riloff, 2018b). Recently, Ding et al. (2018) further characterized affective events in terms of human needs categories: Physiological, Health, Leisure, Social, Financial, Cognition, and Freedom, which can explain why an event is positive or negative. Subsequently, Ding and Riloff (2018a) designed supervised learning models and a semisupervised co-training model to assign affective events to Human Needs categories. Our work exploits recent advances in distributed word representations and semantic embeddings (e.g., (Mikolov"
S19-1022,P09-1045,0,0.0130471,"resentations and semantic embeddings (e.g., (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018)), and is related to domain adaptation of word embeddings (e.g., (Sarma et al., 2018)). But we aim to recognize a specific set of semantic concepts, rather than a specialized domain with domain-specific texts. The LIWC dictionary (Pennebaker et al., 2007) contains word lists for semantic categories that are similar to our targeted semantic concepts, but its coverage is insufficient. Our work is also related to semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Qadir and Riloff, 2012; De Benedictis et al., 2013; Gupta and Manning, 2015), contextual semantic tagging (Huang and Riloff, 2010), and fine-grained entity recognition (Fleischman and Hovy, 2002; Ling and Weld, 2012). Our goal, however, is not to generate a dictionary, or assign semantic meanings in sentence contexts. Instead, our classifier learns to recognize words that belong to a small set of relevant semantic classes based only on their pre-trained embedding vectors. 3 Table 1: Semantic Concepts and Seed Words (Section 5) was generated from the ICWSM 2009 and 2011 blog corpora (Burton e"
S19-1022,N18-1174,1,0.768467,"ective events have a positive or negative impact on the people who experience the event. For example, being hired for a job is typically a beneficial (positive) event, but being fired is usually a detrimental (negative) event. Recognizing affective events is critical to understand people’s motivations, goals, desires, and empathy in narrative stories and conversations. Previous research has proposed several methods to recognize affective events and their polarity (e.g., (Deng et al., 2013; Vu et al., 2014; Reed et al., 2017; Ding and Riloff, 2016)). To achieve a deeper level of understanding, Ding and Riloff (2018a) further classified affective events into categories associated with theories of Human Needs (Maslow et al., 1970; MaxNeef et al., 1991) in psychology: Physiological, Health, Leisure, Social, Finance, and Cognition, to characterize the reason for the event’s affective polarity. For example, breaking your arm is a negative event because it violates a need to maintain 2 Related Work Previous work in NLP on affective events has primarily focused on identifying the affective polarity of events in narrative fables (Goyal et al., 198 Proceedings of the Eighth Joint Conference on Lexical and Comput"
S19-1022,D14-1162,0,0.0823041,"blogs (Ding and Riloff, 2016; Reed et al., 2017; Ding and Riloff, 2018b). Recently, Ding et al. (2018) further characterized affective events in terms of human needs categories: Physiological, Health, Leisure, Social, Financial, Cognition, and Freedom, which can explain why an event is positive or negative. Subsequently, Ding and Riloff (2018a) designed supervised learning models and a semisupervised co-training model to assign affective events to Human Needs categories. Our work exploits recent advances in distributed word representations and semantic embeddings (e.g., (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018)), and is related to domain adaptation of word embeddings (e.g., (Sarma et al., 2018)). But we aim to recognize a specific set of semantic concepts, rather than a specialized domain with domain-specific texts. The LIWC dictionary (Pennebaker et al., 2007) contains word lists for semantic categories that are similar to our targeted semantic concepts, but its coverage is insufficient. Our work is also related to semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Qadir and Riloff, 2012; De Benedictis et al., 2013; Gupta and M"
S19-1022,C02-1130,0,0.169362,", (Sarma et al., 2018)). But we aim to recognize a specific set of semantic concepts, rather than a specialized domain with domain-specific texts. The LIWC dictionary (Pennebaker et al., 2007) contains word lists for semantic categories that are similar to our targeted semantic concepts, but its coverage is insufficient. Our work is also related to semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Qadir and Riloff, 2012; De Benedictis et al., 2013; Gupta and Manning, 2015), contextual semantic tagging (Huang and Riloff, 2010), and fine-grained entity recognition (Fleischman and Hovy, 2002; Ling and Weld, 2012). Our goal, however, is not to generate a dictionary, or assign semantic meanings in sentence contexts. Instead, our classifier learns to recognize words that belong to a small set of relevant semantic classes based only on their pre-trained embedding vectors. 3 Table 1: Semantic Concepts and Seed Words (Section 5) was generated from the ICWSM 2009 and 2011 blog corpora (Burton et al., 2009, 2011), so we selected seed words from these corpora as well. We used the following procedure to identify commonly used words for each category: we sorted all word lemmas by frequency"
S19-1022,N18-1202,0,0.0149361,"016; Reed et al., 2017; Ding and Riloff, 2018b). Recently, Ding et al. (2018) further characterized affective events in terms of human needs categories: Physiological, Health, Leisure, Social, Financial, Cognition, and Freedom, which can explain why an event is positive or negative. Subsequently, Ding and Riloff (2018a) designed supervised learning models and a semisupervised co-training model to assign affective events to Human Needs categories. Our work exploits recent advances in distributed word representations and semantic embeddings (e.g., (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018)), and is related to domain adaptation of word embeddings (e.g., (Sarma et al., 2018)). But we aim to recognize a specific set of semantic concepts, rather than a specialized domain with domain-specific texts. The LIWC dictionary (Pennebaker et al., 2007) contains word lists for semantic categories that are similar to our targeted semantic concepts, but its coverage is insufficient. Our work is also related to semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Qadir and Riloff, 2012; De Benedictis et al., 2013; Gupta and Manning, 2015), contex"
S19-1022,N15-1128,0,0.0214848,"t al., 2014; Peters et al., 2018; Devlin et al., 2018)), and is related to domain adaptation of word embeddings (e.g., (Sarma et al., 2018)). But we aim to recognize a specific set of semantic concepts, rather than a specialized domain with domain-specific texts. The LIWC dictionary (Pennebaker et al., 2007) contains word lists for semantic categories that are similar to our targeted semantic concepts, but its coverage is insufficient. Our work is also related to semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Qadir and Riloff, 2012; De Benedictis et al., 2013; Gupta and Manning, 2015), contextual semantic tagging (Huang and Riloff, 2010), and fine-grained entity recognition (Fleischman and Hovy, 2002; Ling and Weld, 2012). Our goal, however, is not to generate a dictionary, or assign semantic meanings in sentence contexts. Instead, our classifier learns to recognize words that belong to a small set of relevant semantic classes based only on their pre-trained embedding vectors. 3 Table 1: Semantic Concepts and Seed Words (Section 5) was generated from the ICWSM 2009 and 2011 blog corpora (Burton et al., 2009, 2011), so we selected seed words from these corpora as well. We u"
S19-1022,S12-1028,1,0.830281,"mbeddings (e.g., (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018)), and is related to domain adaptation of word embeddings (e.g., (Sarma et al., 2018)). But we aim to recognize a specific set of semantic concepts, rather than a specialized domain with domain-specific texts. The LIWC dictionary (Pennebaker et al., 2007) contains word lists for semantic categories that are similar to our targeted semantic concepts, but its coverage is insufficient. Our work is also related to semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Qadir and Riloff, 2012; De Benedictis et al., 2013; Gupta and Manning, 2015), contextual semantic tagging (Huang and Riloff, 2010), and fine-grained entity recognition (Fleischman and Hovy, 2002; Ling and Weld, 2012). Our goal, however, is not to generate a dictionary, or assign semantic meanings in sentence contexts. Instead, our classifier learns to recognize words that belong to a small set of relevant semantic classes based only on their pre-trained embedding vectors. 3 Table 1: Semantic Concepts and Seed Words (Section 5) was generated from the ICWSM 2009 and 2011 blog corpora (Burton et al., 2009, 2011), so w"
S19-1022,P10-1029,1,0.787029,", and is related to domain adaptation of word embeddings (e.g., (Sarma et al., 2018)). But we aim to recognize a specific set of semantic concepts, rather than a specialized domain with domain-specific texts. The LIWC dictionary (Pennebaker et al., 2007) contains word lists for semantic categories that are similar to our targeted semantic concepts, but its coverage is insufficient. Our work is also related to semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Qadir and Riloff, 2012; De Benedictis et al., 2013; Gupta and Manning, 2015), contextual semantic tagging (Huang and Riloff, 2010), and fine-grained entity recognition (Fleischman and Hovy, 2002; Ling and Weld, 2012). Our goal, however, is not to generate a dictionary, or assign semantic meanings in sentence contexts. Instead, our classifier learns to recognize words that belong to a small set of relevant semantic classes based only on their pre-trained embedding vectors. 3 Table 1: Semantic Concepts and Seed Words (Section 5) was generated from the ICWSM 2009 and 2011 blog corpora (Burton et al., 2009, 2011), so we selected seed words from these corpora as well. We used the following procedure to identify commonly used"
S19-1022,P17-2022,0,0.474212,"th the recall and precision of Human Needs categorization for events. 1 Introduction Affective events have a positive or negative impact on the people who experience the event. For example, being hired for a job is typically a beneficial (positive) event, but being fired is usually a detrimental (negative) event. Recognizing affective events is critical to understand people’s motivations, goals, desires, and empathy in narrative stories and conversations. Previous research has proposed several methods to recognize affective events and their polarity (e.g., (Deng et al., 2013; Vu et al., 2014; Reed et al., 2017; Ding and Riloff, 2016)). To achieve a deeper level of understanding, Ding and Riloff (2018a) further classified affective events into categories associated with theories of Human Needs (Maslow et al., 1970; MaxNeef et al., 1991) in psychology: Physiological, Health, Leisure, Social, Finance, and Cognition, to characterize the reason for the event’s affective polarity. For example, breaking your arm is a negative event because it violates a need to maintain 2 Related Work Previous work in NLP on affective events has primarily focused on identifying the affective polarity of events in narrativ"
S19-1022,W18-3407,0,0.0134514,"ve events in terms of human needs categories: Physiological, Health, Leisure, Social, Financial, Cognition, and Freedom, which can explain why an event is positive or negative. Subsequently, Ding and Riloff (2018a) designed supervised learning models and a semisupervised co-training model to assign affective events to Human Needs categories. Our work exploits recent advances in distributed word representations and semantic embeddings (e.g., (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018)), and is related to domain adaptation of word embeddings (e.g., (Sarma et al., 2018)). But we aim to recognize a specific set of semantic concepts, rather than a specialized domain with domain-specific texts. The LIWC dictionary (Pennebaker et al., 2007) contains word lists for semantic categories that are similar to our targeted semantic concepts, but its coverage is insufficient. Our work is also related to semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Qadir and Riloff, 2012; De Benedictis et al., 2013; Gupta and Manning, 2015), contextual semantic tagging (Huang and Riloff, 2010), and fine-grained entity recognition (Fleischman and Hovy, 2"
S19-1022,W02-1028,1,0.619895,"s in distributed word representations and semantic embeddings (e.g., (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2018)), and is related to domain adaptation of word embeddings (e.g., (Sarma et al., 2018)). But we aim to recognize a specific set of semantic concepts, rather than a specialized domain with domain-specific texts. The LIWC dictionary (Pennebaker et al., 2007) contains word lists for semantic categories that are similar to our targeted semantic concepts, but its coverage is insufficient. Our work is also related to semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Qadir and Riloff, 2012; De Benedictis et al., 2013; Gupta and Manning, 2015), contextual semantic tagging (Huang and Riloff, 2010), and fine-grained entity recognition (Fleischman and Hovy, 2002; Ling and Weld, 2012). Our goal, however, is not to generate a dictionary, or assign semantic meanings in sentence contexts. Instead, our classifier learns to recognize words that belong to a small set of relevant semantic classes based only on their pre-trained embedding vectors. 3 Table 1: Semantic Concepts and Seed Words (Section 5) was generated from the ICWSM 2009 and"
S19-1022,E14-4025,0,0.0264573,"cepts improves both the recall and precision of Human Needs categorization for events. 1 Introduction Affective events have a positive or negative impact on the people who experience the event. For example, being hired for a job is typically a beneficial (positive) event, but being fired is usually a detrimental (negative) event. Recognizing affective events is critical to understand people’s motivations, goals, desires, and empathy in narrative stories and conversations. Previous research has proposed several methods to recognize affective events and their polarity (e.g., (Deng et al., 2013; Vu et al., 2014; Reed et al., 2017; Ding and Riloff, 2016)). To achieve a deeper level of understanding, Ding and Riloff (2018a) further classified affective events into categories associated with theories of Human Needs (Maslow et al., 1970; MaxNeef et al., 1991) in psychology: Physiological, Health, Leisure, Social, Finance, and Cognition, to characterize the reason for the event’s affective polarity. For example, breaking your arm is a negative event because it violates a need to maintain 2 Related Work Previous work in NLP on affective events has primarily focused on identifying the affective polarity of"
W00-0603,P99-1042,0,0.0315883,"Missing"
W00-0603,W97-0313,1,0.343851,"Missing"
W00-0603,P98-2182,0,0.0215222,"Missing"
W00-0603,C98-2177,0,\N,Missing
W01-1201,breck-etal-2000-evaluate,1,0.884655,"Missing"
W01-1201,W00-0600,0,0.42007,"ems. Our results quantify the limitations of both term overlap and answer typing to distinguish between competing answer candidates. 1 Introduction When building a system to perform a task, the most important statistic is the performance on an end-to-end evaluation. For the task of opendomain question answering against text collections, there have been two large-scale end-toend evaluations: (TREC-8 Proceedings, 1999) and (TREC-9 Proceedings, 2000). In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children’s reading levels (Charniak et al., 2000; Hirschman et al., 1999; Ng et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000). The performance statistics have been useful for determining how well techniques work. However, raw performance statistics are not enough. If the score is low, we need to understand what went wrong and how to fix it. If the score is high, it is important to understand why. For example, performance may be dependent on characteristics of the current test set and would not carry over to a new domain. It would also be useful to know if there is a particular characteristic of the system that is central. If so, th"
W01-1201,P99-1042,1,0.796705,"fy the limitations of both term overlap and answer typing to distinguish between competing answer candidates. 1 Introduction When building a system to perform a task, the most important statistic is the performance on an end-to-end evaluation. For the task of opendomain question answering against text collections, there have been two large-scale end-toend evaluations: (TREC-8 Proceedings, 1999) and (TREC-9 Proceedings, 2000). In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children’s reading levels (Charniak et al., 2000; Hirschman et al., 1999; Ng et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000). The performance statistics have been useful for determining how well techniques work. However, raw performance statistics are not enough. If the score is low, we need to understand what went wrong and how to fix it. If the score is high, it is important to understand why. For example, performance may be dependent on characteristics of the current test set and would not carry over to a new domain. It would also be useful to know if there is a particular characteristic of the system that is central. If so, then the system can be str"
W01-1201,W00-1316,0,0.1246,"th term overlap and answer typing to distinguish between competing answer candidates. 1 Introduction When building a system to perform a task, the most important statistic is the performance on an end-to-end evaluation. For the task of opendomain question answering against text collections, there have been two large-scale end-toend evaluations: (TREC-8 Proceedings, 1999) and (TREC-9 Proceedings, 2000). In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children’s reading levels (Charniak et al., 2000; Hirschman et al., 1999; Ng et al., 2000; Riloff and Thelen, 2000; Wang et al., 2000). The performance statistics have been useful for determining how well techniques work. However, raw performance statistics are not enough. If the score is low, we need to understand what went wrong and how to fix it. If the score is high, it is important to understand why. For example, performance may be dependent on characteristics of the current test set and would not carry over to a new domain. It would also be useful to know if there is a particular characteristic of the system that is central. If so, then the system can be streamlined and simp"
W01-1201,W00-0605,0,\N,Missing
W01-1201,W00-0603,1,\N,Missing
W01-1201,W00-0601,0,\N,Missing
W02-1017,A97-1029,0,0.0833575,"Missing"
W02-1017,C94-2195,0,0.0178287,"Missing"
W02-1017,P99-1016,0,0.160212,"Missing"
W02-1017,W99-0612,0,0.0559567,"Missing"
W02-1017,C92-2082,0,0.0278575,"Missing"
W02-1017,P99-1042,0,0.0535073,"Missing"
W02-1017,J93-2004,0,0.048219,"Missing"
W02-1017,W98-1106,1,0.934629,"Missing"
W02-1017,W97-0313,1,0.890131,"Missing"
W02-1017,P98-2182,0,0.54969,"Missing"
W02-1017,C98-2177,0,\N,Missing
W02-1017,W99-0613,0,\N,Missing
W02-1028,C94-2195,0,0.0128536,"Missing"
W02-1028,P99-1016,0,0.281225,"Missing"
W02-1028,W98-1119,0,0.00830659,"Missing"
W02-1028,C92-2082,0,0.0719891,"Missing"
W02-1028,P99-1042,0,0.00849362,"Missing"
W02-1028,W98-1106,1,0.238569,"Missing"
W02-1028,W97-0313,1,0.831071,"Missing"
W02-1028,W99-0613,0,0.79734,"Missing"
W02-1028,P98-2182,0,0.272495,"Missing"
W02-1028,W99-0612,0,0.0471215,"Missing"
W02-1028,J93-2004,0,\N,Missing
W02-1028,C98-2177,0,\N,Missing
W03-0404,P99-1016,0,0.0739636,"Missing"
W03-0404,P97-1023,0,0.80925,"Missing"
W03-0404,C92-2082,0,0.023385,"Missing"
W03-0404,C94-2174,0,0.0211935,"Missing"
W03-0404,P97-1005,0,0.0184628,"Missing"
W03-0404,W02-1011,0,0.0665331,"Missing"
W03-0404,W97-0313,1,0.377646,"Missing"
W03-0404,P98-2182,0,0.0178579,"Missing"
W03-0404,P98-1013,0,0.0179181,"Missing"
W03-0404,P02-1053,0,0.0353155,"Missing"
W03-0404,P99-1032,1,0.642678,"Missing"
W03-0404,C98-1013,0,\N,Missing
W03-0404,W02-1028,1,\N,Missing
W03-0404,C98-2177,0,\N,Missing
W03-1014,P98-1013,0,0.104127,"Missing"
W03-1014,P98-1067,0,0.0158561,"example, you can say that a comedian bombed last night, which is a subjective statement, but you can’t express this sentiment with the passive voice of bombed. In Section 3.2, we will show examples of extraction patterns representing subjective expressions which do in fact exhibit both of these phenomena. A variety of algorithms have been developed to automatically learn extraction patterns. Most of these algorithms require special training resources, such as texts annotated with domain-specific tags (e.g., AutoSlog (Riloff, 1993), CRYSTAL (Soderland et al., 1995), RAPIER (Califf, 1998), SRV (Freitag, 1998), WHISK (Soderland, 1999)) or manually defined keywords, frames, or object recognizers (e.g., PALKA (Kim and Moldovan, 1993) and LIEP (Huffman, 1996)). AutoSlog-TS (Riloff, 1996) takes a different approach, requiring only a corpus of unannotated texts that have been separated into those that are related to the target domain (the “relevant” texts) and those that are not (the “irrelevant” texts). Most recently, two bootstrapping algorithms have been used to learn extraction patterns. Metabootstrapping (Riloff and Jones, 1999) learns both extraction patterns and a semantic lexicon using unannotat"
W03-1014,P97-1023,0,0.928172,"IMA, and NRO. Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu must recognize rants and emotional tirades, among other things. In general, nearly any system that seeks to identify information could benefit from being able to separate factual and subjective information. Some existing resources contain lists of subjective words (e.g., Levin’s desire verbs (1993)), and some empirical methods in NLP have automatically identified adjectives, verbs, and N-grams that are statistically associated with subjective language (e.g., (Turney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Wiebe et al., 2001)). However, subjective language can be exhibited by a staggering variety of words and phrases. In addition, many subjective terms occur infrequently, such as strongly subjective adjectives (e.g., preposterous, unseemly) and metaphorical or idiomatic phrases (e.g., dealt a blow, swept off one’s feet). Consequently, we believe that subjectivity learning systems must be trained on extremely large text collections before they will acquire a subjective vocabulary that is truly broad and comprehensive in scope. To address this issue, we have been exploring the use o"
W03-1014,C94-2174,0,0.0157234,"Missing"
W03-1014,P02-1053,0,0.130923,"A, DIA, NSA, NIMA, and NRO. Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu must recognize rants and emotional tirades, among other things. In general, nearly any system that seeks to identify information could benefit from being able to separate factual and subjective information. Some existing resources contain lists of subjective words (e.g., Levin’s desire verbs (1993)), and some empirical methods in NLP have automatically identified adjectives, verbs, and N-grams that are statistically associated with subjective language (e.g., (Turney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Wiebe et al., 2001)). However, subjective language can be exhibited by a staggering variety of words and phrases. In addition, many subjective terms occur infrequently, such as strongly subjective adjectives (e.g., preposterous, unseemly) and metaphorical or idiomatic phrases (e.g., dealt a blow, swept off one’s feet). Consequently, we believe that subjectivity learning systems must be trained on extremely large text collections before they will acquire a subjective vocabulary that is truly broad and comprehensive in scope. To address this iss"
W03-1014,P97-1005,0,0.0140973,"Missing"
W03-1014,P99-1032,1,0.511368,"Missing"
W03-1014,W02-1011,0,0.0681785,"Missing"
W03-1014,W03-0404,1,0.378518,"Missing"
W03-1014,W03-2102,1,0.146689,"seem subjective to a person intuitively, but that are reliable indicators of subjectivity. 4 Experimental Results 4.1 Subjectivity Data The text collection that we used consists of Englishlanguage versions of foreign news documents from FBIS, the U.S. Foreign Broadcast Information Service. The data is from a variety of countries. Our system takes unannotated data as input, but we needed annotated data to evaluate its performance. We briefly describe the manual annotation scheme used to create the gold-standard, and give interannotator agreement results. In 2002, a detailed annotation scheme (Wilson and Wiebe, 2003) was developed for a government-sponsored project. We only mention aspects of the annotation scheme relevant to this paper. The scheme was inspired by work in linguistics and literary theory on subjectivity, which focuses on how opinions, emotions, etc. are expressed linguistically in context (Banfield, 1982). The goal is to identify and characterize expressions of private states in a sentence. Private state is a general covering term for opinions, evaluations, emotions, and speculations (Quirk et al., 1985). For example, in sentence (1) the writer is expressing a negative evaluation. (1) “The"
W03-1014,C00-2136,0,0.0441251,"ned keywords, frames, or object recognizers (e.g., PALKA (Kim and Moldovan, 1993) and LIEP (Huffman, 1996)). AutoSlog-TS (Riloff, 1996) takes a different approach, requiring only a corpus of unannotated texts that have been separated into those that are related to the target domain (the “relevant” texts) and those that are not (the “irrelevant” texts). Most recently, two bootstrapping algorithms have been used to learn extraction patterns. Metabootstrapping (Riloff and Jones, 1999) learns both extraction patterns and a semantic lexicon using unannotated texts and seed words as input. ExDisco (Yangarber et al., 2000) uses a bootstrapping mechanism to find new extraction patterns using unannotated texts and some seed patterns as the initial input. For our research, we adopted a learning process very similar to that used by AutoSlog-TS, which requires only relevant texts and irrelevant texts as its input. We describe this learning process in more detail in the next section. 3 Learning and Bootstrapping Extraction Patterns for Subjectivity We have developed a bootstrapping process for subjectivity classification that explores three ideas: (1) highprecision classifiers can be used to automatically identify su"
W03-1014,C98-1013,0,\N,Missing
W03-1014,C98-1064,0,\N,Missing
W06-0208,P99-1068,0,0.139341,"ber 2005 and once in April 2005, which produced 3,496 documents and 3,309 documents, respectively. After removing duplicate articles, we were left Data Collection In this research, our goal is to automatically learn IE patterns from a large, domain-independent text collection, such as the Web. The billions of freely available documents on the World Wide Web and its ever-growing size make the Web a potential source of data for many corpus-based natural language processing tasks. Indeed, many researchers have recently tapped the Web as a data-source for improving performance on NLP tasks (e.g., Resnik (1999), Ravichandran and Hovy (2002), Keller and Lapata (2003)). Despite these successes, numerous problems exist with collecting data from the Web, such as web pages containing information that is not free text, including advertisements, embedded scripts, tables, captions, etc. Also, the documents cover many genres, and it is not easy to identify documents of a particular genre or domain. Additionally, most of the doc2 3 68 http://www.google.com http://www.google.com/apis with a total of 6,182 potentially relevant terrorism articles. 5 Learning Domain-Specific IE Patterns from Web Pages 4.2 Process"
W06-0208,P03-1028,0,0.190873,"Missing"
W06-0208,W99-0613,0,0.061462,"eral disadvantages. Because of the manual labor involved in annotating a corpus, and because a new corpus must be annotated for each domain, most annotated IE corpora are relatively small. Language is so expressive that it is practically impossible for the patterns learned from a relatively small training set to cover all the different ways of describing events. Consequently, the IE patterns learned from manually annotated training sets typically represent only a subset of the IE patterns that could be useful for the task. Many recent approaches in natural language processing (Yarowsky, 1995; Collins and Singer, 1999; Riloff and Jones, 1999; Nigam et al., 2000; Wiebe and Riloff, 2005) have recognized the need to use unannotated data to improve performance. While the Web provides a vast repository of unannotated texts, it is non-trivial to identify texts that belong to a particular domain. The difficulty is that web pages are not specifically annotated with tags categorizing their content. Nevertheless, in this paper we look to the Web as a vast dynamic resource for domain-specific IE learning. Our approach exploits an existing set of IE patterns that were learned from annotated training data to automatica"
W06-0208,M92-1001,0,0.541177,"Missing"
W06-0208,M95-1002,0,0.101269,"Missing"
W06-0208,J03-3005,0,0.190977,"ed 3,496 documents and 3,309 documents, respectively. After removing duplicate articles, we were left Data Collection In this research, our goal is to automatically learn IE patterns from a large, domain-independent text collection, such as the Web. The billions of freely available documents on the World Wide Web and its ever-growing size make the Web a potential source of data for many corpus-based natural language processing tasks. Indeed, many researchers have recently tapped the Web as a data-source for improving performance on NLP tasks (e.g., Resnik (1999), Ravichandran and Hovy (2002), Keller and Lapata (2003)). Despite these successes, numerous problems exist with collecting data from the Web, such as web pages containing information that is not free text, including advertisements, embedded scripts, tables, captions, etc. Also, the documents cover many genres, and it is not easy to identify documents of a particular genre or domain. Additionally, most of the doc2 3 68 http://www.google.com http://www.google.com/apis with a total of 6,182 potentially relevant terrorism articles. 5 Learning Domain-Specific IE Patterns from Web Pages 4.2 Processing the Texts Having created a large domain-specific cor"
W06-0208,C00-2136,0,0.0476826,"n new IE patterns from them. We compute the semantic affinity of each new pattern to automatically infer the type of information that it will extract. Experiments on the MUC-4 test set show that these new IE patterns improved recall with only a small precision loss. 1 Introduction Information Extraction (IE) is the task of identifying event descriptions in natural language text and extracting information related to those events. Many IE systems use extraction patterns or rules to identify the relevant information (Soderland et al., 1995; Riloff, 1996; Califf and Mooney, 1999; Soderland, 1999; Yangarber et al., 2000). Most of these systems use annotated training data to learn pattern matching rules based on lexical, syntactic, and/or semantic information. The learned patterns are then used to locate relevant information in new texts. 66 Proceedings of the Workshop on Information Extraction Beyond The Document, pages 66–73, c Sydney, July 2006. 2006 Association for Computational Linguistics 3 Learning IE Patterns from a Fixed Training Set the Web. These web pages are then used for additional IE training, yielding a new set of domainspecific IE patterns. Experiments on the MUC-4 test set show that the new I"
W06-0208,P95-1026,0,0.0391084,"ing data has several disadvantages. Because of the manual labor involved in annotating a corpus, and because a new corpus must be annotated for each domain, most annotated IE corpora are relatively small. Language is so expressive that it is practically impossible for the patterns learned from a relatively small training set to cover all the different ways of describing events. Consequently, the IE patterns learned from manually annotated training sets typically represent only a subset of the IE patterns that could be useful for the task. Many recent approaches in natural language processing (Yarowsky, 1995; Collins and Singer, 1999; Riloff and Jones, 1999; Nigam et al., 2000; Wiebe and Riloff, 2005) have recognized the need to use unannotated data to improve performance. While the Web provides a vast repository of unannotated texts, it is non-trivial to identify texts that belong to a particular domain. The difficulty is that web pages are not specifically annotated with tags categorizing their content. Nevertheless, in this paper we look to the Web as a vast dynamic resource for domain-specific IE learning. Our approach exploits an existing set of IE patterns that were learned from annotated t"
W06-0208,P02-1006,0,0.353845,"ce in April 2005, which produced 3,496 documents and 3,309 documents, respectively. After removing duplicate articles, we were left Data Collection In this research, our goal is to automatically learn IE patterns from a large, domain-independent text collection, such as the Web. The billions of freely available documents on the World Wide Web and its ever-growing size make the Web a potential source of data for many corpus-based natural language processing tasks. Indeed, many researchers have recently tapped the Web as a data-source for improving performance on NLP tasks (e.g., Resnik (1999), Ravichandran and Hovy (2002), Keller and Lapata (2003)). Despite these successes, numerous problems exist with collecting data from the Web, such as web pages containing information that is not free text, including advertisements, embedded scripts, tables, captions, etc. Also, the documents cover many genres, and it is not easy to identify documents of a particular genre or domain. Additionally, most of the doc2 3 68 http://www.google.com http://www.google.com/apis with a total of 6,182 potentially relevant terrorism articles. 5 Learning Domain-Specific IE Patterns from Web Pages 4.2 Processing the Texts Having created a"
W06-1652,C04-1200,0,0.0976101,"Missing"
W06-1652,J93-2004,0,0.0384676,"Missing"
W06-1652,W04-3253,0,0.0510384,"Missing"
W06-1652,P04-1035,0,0.17754,"ence (i.e., A is good enough that we are comfortable discarding B in favor of the more general feature A). Note that based on the subsumption hierarchy shown in Figure 2, all 1Grams will always survive the subsumption process because they cannot be subsumed by any other types of features. Our goal is to identify complex features that are worth adding to a set of unigram features. 3 4 Using the Subsumption Hierarchy for Analysis Data Sets We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al., 2004), the Polarity data set5 created by (Pang and Lee, 2004), and the MPQA data set created by (Wiebe et al., 2005).6 The OP and Polarity data sets involve document-level opinion classification, while the MPQA data set involves 5 Version v2.0, which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6 Available at http://www.cs.pitt.edu/mpqa/databaserelease/ 444 In this section, we illustrate how the subsumption hierarchy can be used as an analytic tool to automatically identify features that substantially outperform simpler counterparts. These features represent specialized usages and expressions that would be good candidates fo"
W06-1652,W02-1011,0,0.0182856,"a method to automatically identify features that are representationally subsumed by a simpler feature but that are better opinion indicators. These subjective expressions could then be added to a subjectivity lexicon (Esuli and Sebastiani, 2005), and used to gain understanding about which types of complex features capture meaningful expressions that are important for opinion recognition. Many opinion classifiers are created by adopting a “kitchen sink” approach that throws together a variety of features. But in many cases adding new types of features does not improve performance. For example, Pang et al. (2002) found that unigrams outperformed bigrams, and unigrams outperformed the combination of unigrams plus bigrams. Our second goal is to automatically identify features that are unnecessary because similar features provide equal or better coverage and discriminatory value. Our hypothesis is that a reduced feature set, which selectively combines unigrams with only the most valuable complex features, will perform better than a larger feature set that includes the entire “kitchen sink” of features. In this paper, we explore the use of a subsumption hierarchy to formally define the subsumption relatio"
W06-1652,H05-1043,0,0.0716947,"Missing"
W06-1652,W03-1014,1,0.127302,"&lt;np> &lt;possessive> NP The Subsumption Hierarchy 2.1 Text Representations We analyze two feature representations that have been used for opinion analysis: Ngrams and Extraction Patterns. Information extraction (IE) patterns are lexico-syntactic patterns that represent expressions which identify role relationships. For example, the pattern “&lt;subj> ActVP(recommended)” extracts the subject of active-voice instances of the verb “recommended” as the recommender. The pattern “&lt;subj> PassVP(recommended)” extracts the subject of passive-voice instances of “recommended” as the object being recommended. (Riloff and Wiebe, 2003) explored the idea of using extraction patterns to represent more complex subjective expressions that have noncompositional meanings. For example, the expression “drive (someone) up the wall” expresses the feeling of being annoyed, but the meanings of the words “drive”, “up”, and “wall” have no emotional connotations individually. Furthermore, this expression is not a fixed word sequence that can be adequately modeled by Ngrams. Any noun phrase can appear between the words “drive’ and “up”, so a flexible representation is needed to capture the general pattern “drives &lt;NP> up the wall”. This ex"
W06-1652,P02-1053,0,0.00825848,"Missing"
W06-1652,J04-3002,1,0.073394,"hat is considered to be an acceptable performance difference (i.e., A is good enough that we are comfortable discarding B in favor of the more general feature A). Note that based on the subsumption hierarchy shown in Figure 2, all 1Grams will always survive the subsumption process because they cannot be subsumed by any other types of features. Our goal is to identify complex features that are worth adding to a set of unigram features. 3 4 Using the Subsumption Hierarchy for Analysis Data Sets We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al., 2004), the Polarity data set5 created by (Pang and Lee, 2004), and the MPQA data set created by (Wiebe et al., 2005).6 The OP and Polarity data sets involve document-level opinion classification, while the MPQA data set involves 5 Version v2.0, which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6 Available at http://www.cs.pitt.edu/mpqa/databaserelease/ 444 In this section, we illustrate how the subsumption hierarchy can be used as an analytic tool to automatically identify features that substantially outperform simpler counterparts. These features represent specialized"
W06-1652,W03-1017,0,0.170187,"Missing"
W06-1652,H05-2017,0,\N,Missing
W09-1703,P99-1016,0,0.0519637,"ric for the purposes of semantic class verification. There has also been work on fully unsupervised 1 Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on Web pages, but used a precompiled corpus of downloaded Web pages. semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Davidov et al., 2007)), however clustering methods may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)). 3 Semantic Lexicon Induction with Web-based Corroboration Our approach combines a weakly supervised learning algorithm for corpus-based semantic lexicon induction with a follow-on procedure that gathers corroborating statistical evidence from the Web. In this section, we describe both of these components. First, we give a brief overview of the Basilisk bootstrapping algorithm that we use for corpus-based semantic lexicon induction. Second, we present our new strategies for acquiring and utilizing corroborating statistical evidence from the Web. 3.1 Cor"
W09-1703,P06-1038,0,0.0183016,"hyponym occurrences in these patterns. Our work builds upon Turney’s work on semantic orientation (Turney, 2002) and synonym learning (Turney, 2001), in which he used a PMI-IR algorithm to measure the similarity of words and phrases based on Web queries. We use a similar PMI (pointwise mutual information) metric for the purposes of semantic class verification. There has also been work on fully unsupervised 1 Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on Web pages, but used a precompiled corpus of downloaded Web pages. semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Davidov et al., 2007)), however clustering methods may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)). 3 Semantic Lexicon Induction with Web-based Corroboration Our approach combines a weakly supervised learning algorithm for corpus-based semantic lexicon induction with a follow-on procedure that gathers corroborating statistical evidence from the Web. In this"
W09-1703,P07-1030,0,0.275414,"patterns. Our work builds upon Turney’s work on semantic orientation (Turney, 2002) and synonym learning (Turney, 2001), in which he used a PMI-IR algorithm to measure the similarity of words and phrases based on Web queries. We use a similar PMI (pointwise mutual information) metric for the purposes of semantic class verification. There has also been work on fully unsupervised 1 Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on Web pages, but used a precompiled corpus of downloaded Web pages. semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Davidov et al., 2007)), however clustering methods may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)). 3 Semantic Lexicon Induction with Web-based Corroboration Our approach combines a weakly supervised learning algorithm for corpus-based semantic lexicon induction with a follow-on procedure that gathers corroborating statistical evidence from the Web. In this section, we describe bo"
W09-1703,C92-2082,0,0.0984327,"d Jones, 1999; Thelen and Riloff, 2002). Due to the need for POS tagging and/or parsing, these types of methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the Web. The goal of our work is to improve upon corpus-based bootstrapping algorithms by using cooccurrence statistics obtained from the Web to rerank and filter the hypothesized category members. Techniques for semantic class learning have also been developed specifically for the Web. Several Web-based semantic class learners build upon Hearst’s early work (Hearst, 1992) with hyponym patterns. Hearst exploited patterns that explicitly identify a hyponym relation between a semantic class and a word (e.g., “such authors as <X>”) to automatically acquire new hyponyms. (Pas¸ca, 2004) applied hyponym patterns to the Web and learned semantic class instances and groups by acquiring contexts around the patterns. Later, (Pasca, 2007) created context vectors for a group of seed instances by searching Web query logs, and used them to learn similar instances. The KnowItAll system (Etzioni et al., 2005) also uses hyponym patterns to extract class instances from the Web an"
W09-1703,P08-1119,1,0.595516,"lation between a semantic class and a word (e.g., “such authors as <X>”) to automatically acquire new hyponyms. (Pas¸ca, 2004) applied hyponym patterns to the Web and learned semantic class instances and groups by acquiring contexts around the patterns. Later, (Pasca, 2007) created context vectors for a group of seed instances by searching Web query logs, and used them to learn similar instances. The KnowItAll system (Etzioni et al., 2005) also uses hyponym patterns to extract class instances from the Web and evaluates them further by computing mutual information scores based on Web queries. (Kozareva et al., 2008) proposed the use of a doubly-anchored hyponym pattern and a graph to represent the links between hyponym occurrences in these patterns. Our work builds upon Turney’s work on semantic orientation (Turney, 2002) and synonym learning (Turney, 2001), in which he used a PMI-IR algorithm to measure the similarity of words and phrases based on Web queries. We use a similar PMI (pointwise mutual information) metric for the purposes of semantic class verification. There has also been work on fully unsupervised 1 Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on Web pages, but used a precomp"
W09-1703,C02-1144,0,0.20732,"ent the links between hyponym occurrences in these patterns. Our work builds upon Turney’s work on semantic orientation (Turney, 2002) and synonym learning (Turney, 2001), in which he used a PMI-IR algorithm to measure the similarity of words and phrases based on Web queries. We use a similar PMI (pointwise mutual information) metric for the purposes of semantic class verification. There has also been work on fully unsupervised 1 Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on Web pages, but used a precompiled corpus of downloaded Web pages. semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Davidov et al., 2007)), however clustering methods may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)). 3 Semantic Lexicon Induction with Web-based Corroboration Our approach combines a weakly supervised learning algorithm for corpus-based semantic lexicon induction with a follow-on procedure that gathers corroborating statistical ev"
W09-1703,W02-1111,0,0.0148924,"fication. There has also been work on fully unsupervised 1 Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on Web pages, but used a precompiled corpus of downloaded Web pages. semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Davidov et al., 2007)), however clustering methods may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)). 3 Semantic Lexicon Induction with Web-based Corroboration Our approach combines a weakly supervised learning algorithm for corpus-based semantic lexicon induction with a follow-on procedure that gathers corroborating statistical evidence from the Web. In this section, we describe both of these components. First, we give a brief overview of the Basilisk bootstrapping algorithm that we use for corpus-based semantic lexicon induction. Second, we present our new strategies for acquiring and utilizing corroborating statistical evidence from the Web. 3.1 Corpus-based Semantic Lexicon Induction vi"
W09-1703,N04-1041,0,0.0426793,"American terrorism and disease-related documents. Section 5 summarizes our results and discusses future work. 2 Related Work Our research focuses on semantic lexicon induction, where the goal is to create a list of words that belong to a desired semantic class. A substantial amount of previous work has been done on weakly supervised and unsupervised creation of semantic lexicons. Weakly supervised corpus-based 19 methods have utilized noun co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Widdows and Dorow, 2002; Phillips and Riloff, 2002; Pantel and Ravichandran, 2004; Tanev and Magnini, 2006), and lexico-syntactic contextual patterns (e.g., “resides in <location>” or “moved to <location>”) (Riloff and Jones, 1999; Thelen and Riloff, 2002). Due to the need for POS tagging and/or parsing, these types of methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the Web. The goal of our work is to improve upon corpus-based bootstrapping algorithms by using cooccurrence statistics obtained from the Web to rerank and filter the hypothesized category members. Techniques for semantic clas"
W09-1703,W02-1017,1,0.873723,"esenting two domains: Latin American terrorism and disease-related documents. Section 5 summarizes our results and discusses future work. 2 Related Work Our research focuses on semantic lexicon induction, where the goal is to create a list of words that belong to a desired semantic class. A substantial amount of previous work has been done on weakly supervised and unsupervised creation of semantic lexicons. Weakly supervised corpus-based 19 methods have utilized noun co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Widdows and Dorow, 2002; Phillips and Riloff, 2002; Pantel and Ravichandran, 2004; Tanev and Magnini, 2006), and lexico-syntactic contextual patterns (e.g., “resides in <location>” or “moved to <location>”) (Riloff and Jones, 1999; Thelen and Riloff, 2002). Due to the need for POS tagging and/or parsing, these types of methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the Web. The goal of our work is to improve upon corpus-based bootstrapping algorithms by using cooccurrence statistics obtained from the Web to rerank and filter the hypothesized category member"
W09-1703,W97-0313,1,0.773224,"ating evidence from the Web. Section 4 presents experimental results on seven semantic categories representing two domains: Latin American terrorism and disease-related documents. Section 5 summarizes our results and discusses future work. 2 Related Work Our research focuses on semantic lexicon induction, where the goal is to create a list of words that belong to a desired semantic class. A substantial amount of previous work has been done on weakly supervised and unsupervised creation of semantic lexicons. Weakly supervised corpus-based 19 methods have utilized noun co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Widdows and Dorow, 2002; Phillips and Riloff, 2002; Pantel and Ravichandran, 2004; Tanev and Magnini, 2006), and lexico-syntactic contextual patterns (e.g., “resides in <location>” or “moved to <location>”) (Riloff and Jones, 1999; Thelen and Riloff, 2002). Due to the need for POS tagging and/or parsing, these types of methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the Web. The goal of our work is to improve upon corpus-based bootstrapping algorithms by usi"
W09-1703,P98-2182,0,0.223877,". Section 4 presents experimental results on seven semantic categories representing two domains: Latin American terrorism and disease-related documents. Section 5 summarizes our results and discusses future work. 2 Related Work Our research focuses on semantic lexicon induction, where the goal is to create a list of words that belong to a desired semantic class. A substantial amount of previous work has been done on weakly supervised and unsupervised creation of semantic lexicons. Weakly supervised corpus-based 19 methods have utilized noun co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Widdows and Dorow, 2002; Phillips and Riloff, 2002; Pantel and Ravichandran, 2004; Tanev and Magnini, 2006), and lexico-syntactic contextual patterns (e.g., “resides in <location>” or “moved to <location>”) (Riloff and Jones, 1999; Thelen and Riloff, 2002). Due to the need for POS tagging and/or parsing, these types of methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the Web. The goal of our work is to improve upon corpus-based bootstrapping algorithms by using cooccurrence statistics"
W09-1703,E06-1003,0,0.0301712,"-related documents. Section 5 summarizes our results and discusses future work. 2 Related Work Our research focuses on semantic lexicon induction, where the goal is to create a list of words that belong to a desired semantic class. A substantial amount of previous work has been done on weakly supervised and unsupervised creation of semantic lexicons. Weakly supervised corpus-based 19 methods have utilized noun co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Widdows and Dorow, 2002; Phillips and Riloff, 2002; Pantel and Ravichandran, 2004; Tanev and Magnini, 2006), and lexico-syntactic contextual patterns (e.g., “resides in <location>” or “moved to <location>”) (Riloff and Jones, 1999; Thelen and Riloff, 2002). Due to the need for POS tagging and/or parsing, these types of methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the Web. The goal of our work is to improve upon corpus-based bootstrapping algorithms by using cooccurrence statistics obtained from the Web to rerank and filter the hypothesized category members. Techniques for semantic class learning have also been"
W09-1703,W02-1028,1,0.886013,"any domain probably exists in some niche or cranny, but finding the appropriate corner of the Web to tap into is a challenge. You have to know where to look to find specialized knowledge. In contrast, corpus-based methods can learn specialized terminology directly from a domain-specific corpus, but accuracy can be a problem because most corpora are relatively small. In this paper, we seek to exploit the best of both worlds by combining a weakly supervised corpusbased method for semantic lexicon induction with statistics obtained from the Web. First, we use a bootstrapping algorithm, Basilisk (Thelen and Riloff, 2002), to automatically induce a semantic lexicon from a domain-specific corpus. This produces a set of words that are hypothesized to belong to the targeted semantic category. Second, we use the Web as a source of corroborating evidence to confirm, or disconfirm, whether each term truly belongs to the semantic category. For each candidate word, we search the Web for pages that contain both the word and a semantically related term. We expect that true semantic category members will co-occur with semantically similar words more often than non-members. This paper is organized as follows. Section 2 di"
W09-1703,P02-1053,0,0.00781085,"ring contexts around the patterns. Later, (Pasca, 2007) created context vectors for a group of seed instances by searching Web query logs, and used them to learn similar instances. The KnowItAll system (Etzioni et al., 2005) also uses hyponym patterns to extract class instances from the Web and evaluates them further by computing mutual information scores based on Web queries. (Kozareva et al., 2008) proposed the use of a doubly-anchored hyponym pattern and a graph to represent the links between hyponym occurrences in these patterns. Our work builds upon Turney’s work on semantic orientation (Turney, 2002) and synonym learning (Turney, 2001), in which he used a PMI-IR algorithm to measure the similarity of words and phrases based on Web queries. We use a similar PMI (pointwise mutual information) metric for the purposes of semantic class verification. There has also been work on fully unsupervised 1 Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on Web pages, but used a precompiled corpus of downloaded Web pages. semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Davidov et al., 2007)), however clustering methods may or may not produce the types"
W09-1703,C02-1114,0,0.0969153,"semantic categories representing two domains: Latin American terrorism and disease-related documents. Section 5 summarizes our results and discusses future work. 2 Related Work Our research focuses on semantic lexicon induction, where the goal is to create a list of words that belong to a desired semantic class. A substantial amount of previous work has been done on weakly supervised and unsupervised creation of semantic lexicons. Weakly supervised corpus-based 19 methods have utilized noun co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Widdows and Dorow, 2002; Phillips and Riloff, 2002; Pantel and Ravichandran, 2004; Tanev and Magnini, 2006), and lexico-syntactic contextual patterns (e.g., “resides in <location>” or “moved to <location>”) (Riloff and Jones, 1999; Thelen and Riloff, 2002). Due to the need for POS tagging and/or parsing, these types of methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the Web. The goal of our work is to improve upon corpus-based bootstrapping algorithms by using cooccurrence statistics obtained from the Web to rerank and filter the h"
W09-1703,C98-2177,0,\N,Missing
W10-0203,W06-1651,0,0.0134263,"es to propagate affect states onto the characters, and in some cases, to infer new affect states. 4.1 Step 1: Assigning Affect Tags to Words 4.1.1 Sentiment Analysis Resources AESOP incorporates several existing sentiment analysis resources to recognize affect states associated with emotions and speech acts. • OpinionFinder2 (Wilson et al., 2005) (Version 1.4) is used to identify all three types of states. We use the +/- labels assigned by its contextual polarity classifier (Wilson, 2005) to create +/- affect tags. The MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) are used as M affect tags. • Subjectivity Lexicon3 (Wilson, 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated partof-speech (POS). • Semantic Orientation Lexicon4 (Takamura et al., 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated part-of-speech. • A list of 228 speech act verbs compiled from (Wierzbicka, 1987)5 , which are used for M states. 4.1.2 Patient Polarity Verbs As we discussed in Section 3.4, existing resources are not sufficient to identify affect states t"
W10-0203,W06-0301,0,0.016285,"fect tags from words and phrases to characters in the story via syntactic relations. During the course of our research, we came to appreciate that affect states, of the type required for plot units, can represent much more than just direct expressions of emotion. A common phenomena are affect states that result from a character being acted upon in a positive or negative way. For example, “the cat ate the mouse” produces a positive affect state for the cat and a negative affect 1 This is somewhat analogous to, but not exactly the same as, associating opinion words with their targets or topics (Kim and Hovy, 2006; Stoyanov and Cardie, 2008). Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 17–25, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics The Father and His Sons (s1) A father had a family of sons who were perpetually quarreling among themselves. (s2) When he failed to heal their disputes by his exhortations, he determined to give them a practical illustration of the evils of disunion; and for this purpose he one day told them to bring him a bundle of sticks. (s3) When they had done so,"
W10-0203,C08-1103,0,0.0131807,"and phrases to characters in the story via syntactic relations. During the course of our research, we came to appreciate that affect states, of the type required for plot units, can represent much more than just direct expressions of emotion. A common phenomena are affect states that result from a character being acted upon in a positive or negative way. For example, “the cat ate the mouse” produces a positive affect state for the cat and a negative affect 1 This is somewhat analogous to, but not exactly the same as, associating opinion words with their targets or topics (Kim and Hovy, 2006; Stoyanov and Cardie, 2008). Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 17–25, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics The Father and His Sons (s1) A father had a family of sons who were perpetually quarreling among themselves. (s2) When he failed to heal their disputes by his exhortations, he determined to give them a practical illustration of the evils of disunion; and for this purpose he one day told them to bring him a bundle of sticks. (s3) When they had done so, he placed the faggot into th"
W10-0203,P05-1017,0,0.0912897,"ognize affect states associated with emotions and speech acts. • OpinionFinder2 (Wilson et al., 2005) (Version 1.4) is used to identify all three types of states. We use the +/- labels assigned by its contextual polarity classifier (Wilson, 2005) to create +/- affect tags. The MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) are used as M affect tags. • Subjectivity Lexicon3 (Wilson, 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated partof-speech (POS). • Semantic Orientation Lexicon4 (Takamura et al., 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated part-of-speech. • A list of 228 speech act verbs compiled from (Wierzbicka, 1987)5 , which are used for M states. 4.1.2 Patient Polarity Verbs As we discussed in Section 3.4, existing resources are not sufficient to identify affect states that arise from a character being acted upon. Sentiment lexicons, for example, assign polarity to verbs irrespective of their agents or patients. To fill this gap, we tried to automatically acquire verbs that have a strong patient polarity (i.e., the p"
W10-0203,H05-2018,1,0.78312,"syntactic relations. AESOP produces affect states in a 3-step process. First, AESOP labels individual words and phrases with an M, +, or - affect tag. Second, it identifies all references to the two main characters of the story. Third, AESOP applies affect projection rules to propagate affect states onto the characters, and in some cases, to infer new affect states. 4.1 Step 1: Assigning Affect Tags to Words 4.1.1 Sentiment Analysis Resources AESOP incorporates several existing sentiment analysis resources to recognize affect states associated with emotions and speech acts. • OpinionFinder2 (Wilson et al., 2005) (Version 1.4) is used to identify all three types of states. We use the +/- labels assigned by its contextual polarity classifier (Wilson, 2005) to create +/- affect tags. The MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) are used as M affect tags. • Subjectivity Lexicon3 (Wilson, 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated partof-speech (POS). • Semantic Orientation Lexicon4 (Takamura et al., 2005): The positive/negative words in this list are assigned +/- affect tags, when t"
W10-0203,H05-1044,0,0.0338825,"Second, it identifies all references to the two main characters of the story. Third, AESOP applies affect projection rules to propagate affect states onto the characters, and in some cases, to infer new affect states. 4.1 Step 1: Assigning Affect Tags to Words 4.1.1 Sentiment Analysis Resources AESOP incorporates several existing sentiment analysis resources to recognize affect states associated with emotions and speech acts. • OpinionFinder2 (Wilson et al., 2005) (Version 1.4) is used to identify all three types of states. We use the +/- labels assigned by its contextual polarity classifier (Wilson, 2005) to create +/- affect tags. The MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) are used as M affect tags. • Subjectivity Lexicon3 (Wilson, 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated partof-speech (POS). • Semantic Orientation Lexicon4 (Takamura et al., 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated part-of-speech. • A list of 228 speech act verbs compiled from (Wierzbicka, 1987)5 , which are used for M states. 4"
W11-0206,D10-1035,0,0.0291008,"Missing"
W11-0206,W03-0404,1,0.797083,"Missing"
W11-0206,W02-1028,1,0.675165,"Missing"
W11-0206,P09-1045,0,\N,Missing
W11-1813,W11-1801,0,0.139781,"Missing"
W11-1813,J08-1002,0,0.01914,"Missing"
W11-1813,C08-1079,0,0.124054,"pproaches to coreference resolution and BioNLP 2011 Shared Task Our system was developed to participate in a Protein Coreference (COREF) task (Nguyen et al., 2011), one of the supporting tasks in the BioNLP Shared Task 2011. The COREF task is to find all mentions participating in the coreference relation and to connect the anaphora-antecedent pairs. The corpus is based on the Genia-Medco coreference corpus. The Genia-Medco corpus was produced for the biomedical domain, and some comparative analysis with this corpus and other newswire domain data have been performed (Yang et al., 2004a; 2004b; Nguyen and Kim, 2008; Nguyen et al., 2008). The COREF corpus consists of 800 text files for training, 150 for development, and 260 for testing, which all have gene/protein coreference annotations. The training set has 2,313 pairs of coreference links with 4,367 mentions. 2,117 mentions are antecedents, with an average of 4.21 tokens each (delimited by white space), and 2,301 89 Proceedings of BioNLP Shared Task 2011 Workshop, pages 89–93, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics mentions are anaphora, with an average of 1.28 tokens each. The anaphora are much shorter"
W11-1813,poesio-kabadjov-2004-general,0,0.0232828,"tial mention, protein links, and system recall mode. We witnessed that specialized mention detection is crucial for coreference resolution in the biomedical domain. 1 2 Introduction Coreference resolution is a mechanism that groups entity mentions in a text into coreference chains based on whether they refer to the same real-world entity or concept. Like other NLP applications, which must meet the need for aggressive and sophisticated methods of detecting valuable information in emerging domains, numerous coreference resolvers have been developed, including JavaRap (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004) and BART (Versley et al., 2008). Our research uses a recently released system, Reconcile (Stoyanov et al, 2009; 2010a; 2010b), which was designed as a general architecture for coreference resolution that can be used to easily create learning-based coreference resolvers. Reconcile is based on supervised learning approaches to coreference resolution and BioNLP 2011 Shared Task Our system was developed to participate in a Protein Coreference (COREF) task (Nguyen et al., 2011), one of the supporting tasks in the BioNLP Shared Task 2011. The COREF task is to find all mentions participating in the"
W11-1813,qiu-etal-2004-public,0,0.050955,"Missing"
W11-1813,P10-2029,1,0.909286,"Missing"
W11-1813,H05-1059,0,0.0284775,"Missing"
W11-1813,P08-4003,0,0.0212152,"em recall mode. We witnessed that specialized mention detection is crucial for coreference resolution in the biomedical domain. 1 2 Introduction Coreference resolution is a mechanism that groups entity mentions in a text into coreference chains based on whether they refer to the same real-world entity or concept. Like other NLP applications, which must meet the need for aggressive and sophisticated methods of detecting valuable information in emerging domains, numerous coreference resolvers have been developed, including JavaRap (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004) and BART (Versley et al., 2008). Our research uses a recently released system, Reconcile (Stoyanov et al, 2009; 2010a; 2010b), which was designed as a general architecture for coreference resolution that can be used to easily create learning-based coreference resolvers. Reconcile is based on supervised learning approaches to coreference resolution and BioNLP 2011 Shared Task Our system was developed to participate in a Protein Coreference (COREF) task (Nguyen et al., 2011), one of the supporting tasks in the BioNLP Shared Task 2011. The COREF task is to find all mentions participating in the coreference relation and to conn"
W11-1813,C04-1033,0,0.14205,"ed on supervised learning approaches to coreference resolution and BioNLP 2011 Shared Task Our system was developed to participate in a Protein Coreference (COREF) task (Nguyen et al., 2011), one of the supporting tasks in the BioNLP Shared Task 2011. The COREF task is to find all mentions participating in the coreference relation and to connect the anaphora-antecedent pairs. The corpus is based on the Genia-Medco coreference corpus. The Genia-Medco corpus was produced for the biomedical domain, and some comparative analysis with this corpus and other newswire domain data have been performed (Yang et al., 2004a; 2004b; Nguyen and Kim, 2008; Nguyen et al., 2008). The COREF corpus consists of 800 text files for training, 150 for development, and 260 for testing, which all have gene/protein coreference annotations. The training set has 2,313 pairs of coreference links with 4,367 mentions. 2,117 mentions are antecedents, with an average of 4.21 tokens each (delimited by white space), and 2,301 89 Proceedings of BioNLP Shared Task 2011 Workshop, pages 89–93, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics mentions are anaphora, with an average of 1.28 tokens each."
W11-1813,P09-1074,1,\N,Missing
W11-1813,nguyen-etal-2008-challenges,0,\N,Missing
W13-1602,P11-1040,0,0.0336605,"Missing"
W13-1602,W11-3702,0,0.0851517,"Missing"
W13-1602,D11-1052,0,0.0386699,"h each other. We took Parrott’s primary emotion Joy and Fear2 directly. We merged Parrott’s secondary emotion Affection and Lust into our Affection class and merged Parrott’s secondary emotion Sadness and Disappointment into our Sadness/Disappointment class, since these emotions are often difficult to distinguish from each other. Lastly, we mapped Parrott’s secondary emotion Rage to our Anger/Rage class directly. There were other emotions in Parrott’s taxonomy such as Surprise, Neglect, etc. that we did 1 This feature has also been found to have a strong association with sentiment polarities (Brody and Diakopoulos, 2011) 2 we renamed the Fear class as Fear/Anxiety 4 3.3 Overview of Bootstrapping Framework Figure 1: Bootstrapping Architecture Figure 1 presents the framework of our bootstrapping algorithm for learning emotion hashtags. The algorithm runs in two steps. In the first step, the bootstrapping process begins with five manually defined “seed” hashtags for each emotion class. For each seed hashtag, we search Twitter for tweets that contain the hashtag and label these tweets with the emotion class associated with the hashtag. We use these labeled tweets to train a supervised N-gram classifier for every"
W13-1602,J96-2004,0,0.101156,"not have at least one hashtag (other than the topic hashtag). To annotate tweets with respect to emotion, two annotators were given definitions of the 5 emotion classes from Collins English Dictionary7 , Parrott’s (Parrott, 2001) emotion taxonomy of these 5 emotions and additional annotation guidelines. The annotators were instructed to label each tweet with up to two emotions. The instructions specified that the emotion must be felt by the tweeter at the time the tweet was written. After several trials and discussions, the annotators reached a satisfactory agreement level of 0.79 Kappa (κ) (Carletta, 1996). The annotation disagreements in these 500 tweets were then adjudicated, and each annotator labeled an additional 2,500 tweets. Altogether this gave us an emotion annotated dataset of 5,500 tweets. We randomly separated out 1,000 tweets from this collection as a tuning set, and used the remaining 4,500 tweets as evaluation data. In Table 2, we present the emotion distribution in 6 This data collection process is similar to the emotion tweet dataset creation by Roberts et al. (2012) 7 http://www.collinsdictionary.com/ https://dev.twitter.com/docs/api/1/get/search a typical convention to mark a"
W13-1602,W10-2914,0,0.0827899,"Missing"
W13-1602,S12-1033,0,0.192672,"ags (e.g., #angry), many hashtags are multiword phrases (e.g., #LoveHimSoMuch). People also use elongated1 forms of words (e.g., #yaaaaay, #goawaaay) to put emphasis on their emotional state. In addition, words are often spelled creatively by replacing a word with a number or replacing some characters with phonetically similar characters (e.g., #only4you, #YoureDaBest). While many of these hashtags convey emotions, these stylistic variations in the use of hashtags make it very difficult to create a repository of emotion hashtags manually. While emotion word lexicons exist (Yang et al., 2007a; Mohammad, 2012), and adding a ‘#’ symbol as a prefix to these lexicon entries could potentially give us lists of emotion hashtags, it would be unlikely to find multi-word phrases or stylistic variations frequently used in tweets. This drives our motivation to automatically learn hashtags that are commonly used to express emotion in tweets. 3.2 Emotion Classes For this research, we selected 5 prominent emotion classes that are frequent in tweets: Affection, Anger/Rage, Fear/Anxiety, Joy and Sadness/Disappointment. We started by analyzing Parrott’s (Parrott, 2001) emotion taxonomy and how these emotions are ex"
W13-1602,N13-1039,0,0.0147618,"Missing"
W13-1602,E12-1049,0,0.0795154,"Missing"
W13-1602,N10-1020,0,0.0511228,"Missing"
W13-1602,roberts-etal-2012-empatweet,0,0.280669,"nto the following sections. In Section 2, we present a brief overview of previous research related to emotion classification in social media and the use of hashtags. In Section 3, we describe our bootstrapping approach for learning lists of emotion hashtags. In Section 4 we discuss the data collection process and our experimental design. In Section 5, we present the results of our experiments. Finally, we conclude by summarizing our findings and presenting directions for future work. 2 Related Work Recognizing emotions in social media texts has grown popular among researchers in recent years. Roberts et al. (2012) investigated feature sets to classify emotions in Twitter and presented an analysis of different linguistic styles people use to express emotions. The research of Kim et al. (2012a) is focused on discovering emotion influencing patterns to classify emotions in social network conversations. Esmin et al. (2012) presented a 3-level hierarchical emotion classification approach by differentiating between emotion vs. non-emotion text, positive vs. negative emotion, and then classified different emotions. Yang et al. (2007b) investigated sentence contexts to classify emotions in blogs at the documen"
W13-1602,P07-2034,0,0.028672,"st single word hashtags (e.g., #angry), many hashtags are multiword phrases (e.g., #LoveHimSoMuch). People also use elongated1 forms of words (e.g., #yaaaaay, #goawaaay) to put emphasis on their emotional state. In addition, words are often spelled creatively by replacing a word with a number or replacing some characters with phonetically similar characters (e.g., #only4you, #YoureDaBest). While many of these hashtags convey emotions, these stylistic variations in the use of hashtags make it very difficult to create a repository of emotion hashtags manually. While emotion word lexicons exist (Yang et al., 2007a; Mohammad, 2012), and adding a ‘#’ symbol as a prefix to these lexicon entries could potentially give us lists of emotion hashtags, it would be unlikely to find multi-word phrases or stylistic variations frequently used in tweets. This drives our motivation to automatically learn hashtags that are commonly used to express emotion in tweets. 3.2 Emotion Classes For this research, we selected 5 prominent emotion classes that are frequent in tweets: Affection, Anger/Rage, Fear/Anxiety, Joy and Sadness/Disappointment. We started by analyzing Parrott’s (Parrott, 2001) emotion taxonomy and how the"
W14-2714,P11-1040,0,0.052774,"Missing"
W14-2714,D12-1091,0,0.0461117,"Missing"
W14-2714,D13-1114,0,0.0048299,"named entities and historical topic distributions in tweets. In contrast, our work classifies isolated tweets into two different user types, based on their textual content. Consequently, our work can produce different user type labels for different tweets by the same user, which can help identify shared content not authored by the user. Another body of related work tries to classify Twitter users along other dimensions such as ethnicity and political orientation (Pennacchiotti and Popescu, 2011; Cohen and Ruths, 2013). Gender inference in Twitter has also garnered interest in the recent past (Ciot et al., 2013; Liu and Ruths, 2013; Fink et al., 2012). Researchers have also focused on user behaviors showcased in Twitter including the types of messages posted (Naaman et al., 2010), social connections (Wu et al., 2011), user responses to events (Popescu and Pennacchiotti, 2011) and behaviors related to demographics (Volkova et al., 2013; Mislove et al., 2011; Rao et al., 2010). Event recognition is another area that continues to attract a lot of interest in social media. Previous work has investigated event identification and extraction (Jackoway et al., 2011; Becker et al., Sample Tweets from Organiz"
W14-2714,P11-2008,0,0.0881927,"Missing"
W14-2714,P12-3005,0,0.02801,"e 1, organization-tweets are often characterized by headline-like language usage, structured style, a lack of conversation with the audience (i.e., few reply-tweets), and hyperlinks to original articles. In contrast, person-tweets show significant language variability including shorthand terms, conversational behavior, slang and profanity, expressions of emotion, and an overall relaxed usage of language. 99 3.1 Data Acquisition for User Types To create our data sets, we archived tweets (using Twitter Streaming API) for six months, beginning February 1st , 2013. We then used a language filter (Lui and Baldwin, 2012) to separate out the English and Spanish tweets. Also, in the data sets we created (see below), we removed duplicates, retweets and any tweet with less than 5 words. Given that large-scale human annotation is expensive, we explored several heuristics to reliably compile a large gold standard collection of person- and organization-tweets. 3.1.1 Acquiring Person-tweets To acquire person-tweets, we devised a person heuristic, focusing on the name and the profile description fields in each user account corresponding to a tweet. We first gathered lists of person names (first names and surnames), fo"
W14-2714,N12-1083,0,0.0330796,"Missing"
W14-2714,N10-1021,0,0.0841841,"Missing"
W14-2714,D11-1141,0,0.0277444,"ts rarely contain self-references. Also, organizations often address their audience using second-person pronouns in tweets (e.g., Will you High Five the #Bruins or #Blackhawks? Sign up for a chance to win a trip to the Cup Final: http://t.co/XQP8ZDOINV). To capture these characteristics, we included two binary features that look for 1st and 2nd person pronouns in a tweet. 4.3.4 NER Features We hypothesized that organization-tweets will carry more named entities and proper nouns. For English tweets, we identified Persons, Organizations and Locations using the Named Entity Recognizer (NER) from Ritter et al. (2011). For Spanish tweets, we used NER models trained on CoNLL 2002 shared task data for Spanish. The features were encoded as three values, representing the frequency of each entity type in a tweet. 102 ULM: Unigram Language Model BLM: Bigram Language Model NGR: SVM with N-grams OrgH: Organization Heuristic NGR + OrgH NGR + OrgH + Linguistic Features P 71.63 81.46 86.02 66.87 82.26 89.01 English R 63.18 49.17 62.76 91.08 86.82 89.40 F1 67.14 61.32 72.57 77.12 84.48 89.20† P 66.14 80.03 85.76 65.32 83.85 87.59 Spanish R 60.43 51.08 66.56 81.44 85.17 85.47 F1 63.16 62.36 74.95 72.49 84.50 86.52† Tab"
W14-2714,W02-2024,0,0.014378,"tweets. 4 User Type Classification To automatically distinguish person-tweets from organization-tweets, we trained a supervised classifier using N-gram features, an organization heuristic, and a linguistic feature set categorized into six classes. For the classification algorithm, we employed a Support Vector Machine (SVM) with a linear kernel, using the LIBSVM package (Chang and Lin, 2011). For the features that rely on part-of-speech (POS) tags, we used the English Twitter POS tagger by Gimpel et al. (2011) and another tagger trained on the CoNLL 2002 shared task data for Spanish (Tjong Kim Sang, 2002) using the OpenNLP toolkit (OpenSource, 2010). 4.1 N-gram Features We started off by introducing N-gram features to capture the words in a tweet. Specifically, we trained a supervised classifier using unigram and bigram features encoded with binary values. In selecting the N-gram features, we discarded any Ngram that appears less than five times in the training data. 4.2 Organization Heuristic Following observations by Messner et al. (2011), we combined two simple heuristic rules to flag tweets that are likely to be from an organization. The first observation is that ‘replies’ (i.e., @user men"
W14-2714,D13-1187,0,0.0149111,"y the user. Another body of related work tries to classify Twitter users along other dimensions such as ethnicity and political orientation (Pennacchiotti and Popescu, 2011; Cohen and Ruths, 2013). Gender inference in Twitter has also garnered interest in the recent past (Ciot et al., 2013; Liu and Ruths, 2013; Fink et al., 2012). Researchers have also focused on user behaviors showcased in Twitter including the types of messages posted (Naaman et al., 2010), social connections (Wu et al., 2011), user responses to events (Popescu and Pennacchiotti, 2011) and behaviors related to demographics (Volkova et al., 2013; Mislove et al., 2011; Rao et al., 2010). Event recognition is another area that continues to attract a lot of interest in social media. Previous work has investigated event identification and extraction (Jackoway et al., 2011; Becker et al., Sample Tweets from Organizations • Banking Commission Split Over EU Bonus Cap http://t.co/GSSbmHAWsQ • Apple likely to introduce smaller, cheaper iPad mini today http://t.co/TuKBHZ3z • Diet Coke may be the new #2, but U.S. soda market is shrinking http://ow.ly/1bSNnh Sample Tweets from Individual Persons • @john It’s a stress fracture. Nah, no dancing wa"
W15-0515,U08-1003,0,0.0130446,"lity FACT Phrases with “OF” and FEEL Phrases with “FOR” FACT “OF” Phrases RESULT OF ORIGIN OF THEORY OF EVIDENCE OF PARTS OF EVOLUTION OF PERCENT OF THOUSANDS OF EXAMPLE OF LAW OF 5 FEEL “FOR” Phrases MARRIAGE FOR STANDING FOR SAME FOR TREATMENT FOR DEMAND FOR ATTENTION FOR ADVOCATE FOR NO EVIDENCE FOR JUSTIFICATION FOR EXCUSE FOR Related Work Related research on argumentation has primarily worked with different genres of argument than found in IAC, such as news articles, weblogs, legal briefs, supreme court summaries, and congressional debates (Marwell and Schmitt, 1967; Thomas et al., 2006; Burfoot, 2008; Cialdini, 2000; McAlister et al., 2014; Reed and Rowe, 2004). The examples from IAC in Figure 1 illustrate that natural informal dialogues such as those found in online forums exhibit a much broader range of argumentative styles. Other work has on models of natural informal arguments have focused on stance classification (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014), and identifying the structure of arguments such as main claims and their justifications (Biran and Rambow,"
W15-0515,D10-1121,0,0.0456432,"Missing"
W15-0515,C04-1200,0,0.255455,"Missing"
W15-0515,W13-1104,1,0.300424,"Missing"
W15-0515,N15-1046,1,0.139075,"A Fact: Distinguishing Factual and Emotional Argumentation in Online Dialogue Shereen Oraby∗ , Lena Reed∗ , Ryan Compton∗ , Ellen Riloff † , Marilyn Walker∗ and Steve Whittaker∗ ∗ University of California Santa Cruz {soraby,lireed,rcompton,mawalker,swhittak}@ucsc.edu † University of Utah riloff@cs.utah.edu Abstract Schmitt, 1967; Cialdini, 2000; McAlister et al., 2014; Reed and Rowe, 2004). Recent work has begun to model different aspects of these natural informal arguments, with tasks including stance classification (Somasundaran and Wiebe, 2010; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014), and work on the detailed structure of arguments (Biran and Rambow, 2011; Purpura et al., 2008; Yang and Cardie, 2013). Successful models of these tasks have many possible applications in sentiment detection, automatic summarization, argumentative agents (Zuckerman et al., 2015), and in systems that support human argumentative behavior (Rosenfeld and Kraus, 2015). We investigate the characteristics of factual and emotional argumentation styles observed in online debates. Using an annotated set of FACTUAL and FEELING debate forum posts, we extract patter"
W15-0515,S12-1033,0,0.0604829,"Missing"
W15-0515,N12-1071,0,0.0458703,"Missing"
W15-0515,W02-1011,0,0.017406,"Missing"
W15-0515,W14-2105,0,0.0115292,"responses may try to bolster their argument by providing statistics related to a position, giving historical or scientific background, or presenting specific examples or data. There is clearly a relationship between a proposition being FACTUAL versus OBJECTIVE or VERIDICAL, although each of these different labelling tasks may elicit differences from annotators (Wiebe and Riloff, 2005; Riloff and 116 Proceedings of the 2nd Workshop on Argumentation Mining, pages 116–126, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics Wiebe, 2003; Saur´ı and Pustejovsky, 2009; Park and Cardie, 2014). Class FACT FACT FEEL FEEL Debate Forum Dialogue Quote: Even though our planet is getting warmer, it is still a lot cooler than it was 4000 years ago. Response: The average global temperature follows a sinusoidal pattern, the general consensus is we are supposed to be approaching a peak. Projections show that instead of peaking, there will be continue to be an increase in average global temperature. Quote: “When you go to war against your enemies...suppose you see a beautiful woman whom you desire...you shall take her..and she shall marry you.” - Deut. 21:10 Response: Read to the very end of"
W15-0515,W13-1602,1,0.0897573,"Missing"
W15-0515,W03-1014,1,0.711763,"Missing"
W15-0515,roberts-etal-2012-empatweet,0,0.0752252,"Missing"
W15-0515,D08-1027,0,0.0567413,"Missing"
W15-0515,P09-1026,0,0.247158,"Missing"
W15-0515,W10-0214,0,0.0611393,"Missing"
W15-0515,W06-1639,0,0.052753,"Table 4: High-Probability FACT Phrases with “OF” and FEEL Phrases with “FOR” FACT “OF” Phrases RESULT OF ORIGIN OF THEORY OF EVIDENCE OF PARTS OF EVOLUTION OF PERCENT OF THOUSANDS OF EXAMPLE OF LAW OF 5 FEEL “FOR” Phrases MARRIAGE FOR STANDING FOR SAME FOR TREATMENT FOR DEMAND FOR ATTENTION FOR ADVOCATE FOR NO EVIDENCE FOR JUSTIFICATION FOR EXCUSE FOR Related Work Related research on argumentation has primarily worked with different genres of argument than found in IAC, such as news articles, weblogs, legal briefs, supreme court summaries, and congressional debates (Marwell and Schmitt, 1967; Thomas et al., 2006; Burfoot, 2008; Cialdini, 2000; McAlister et al., 2014; Reed and Rowe, 2004). The examples from IAC in Figure 1 illustrate that natural informal dialogues such as those found in online forums exhibit a much broader range of argumentative styles. Other work has on models of natural informal arguments have focused on stance classification (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014), and identifying the structure of arguments such as main claims and their justifications (Bir"
W15-0515,walker-etal-2012-corpus,1,0.308204,"Missing"
W15-0515,J04-3002,0,0.0812307,"Missing"
W15-0515,H05-1044,0,0.129561,"Missing"
W15-0515,P13-1161,0,0.0214478,"and Steve Whittaker∗ ∗ University of California Santa Cruz {soraby,lireed,rcompton,mawalker,swhittak}@ucsc.edu † University of Utah riloff@cs.utah.edu Abstract Schmitt, 1967; Cialdini, 2000; McAlister et al., 2014; Reed and Rowe, 2004). Recent work has begun to model different aspects of these natural informal arguments, with tasks including stance classification (Somasundaran and Wiebe, 2010; Walker et al., 2012), argument summarization (Misra et al., 2015), sarcasm detection (Justo et al., 2014), and work on the detailed structure of arguments (Biran and Rambow, 2011; Purpura et al., 2008; Yang and Cardie, 2013). Successful models of these tasks have many possible applications in sentiment detection, automatic summarization, argumentative agents (Zuckerman et al., 2015), and in systems that support human argumentative behavior (Rosenfeld and Kraus, 2015). We investigate the characteristics of factual and emotional argumentation styles observed in online debates. Using an annotated set of FACTUAL and FEELING debate forum posts, we extract patterns that are highly correlated with factual and emotional arguments, and then apply a bootstrapping methodology to find new patterns in a larger pool of unannot"
W15-0515,P14-1031,0,0.0340034,"Missing"
W15-0515,W03-1017,0,0.201515,"Missing"
W15-3807,W02-1001,0,0.263133,"Missing"
W15-3807,P08-1108,0,0.0638072,"Missing"
W15-3807,W02-2031,0,0.128753,"Missing"
W15-3807,N01-1025,0,0.310018,"Missing"
W15-3807,P10-1052,0,0.0969543,"Missing"
W15-3807,P02-1060,0,0.108364,"Missing"
W15-3807,P14-5010,0,\N,Missing
W15-3807,C00-1030,0,\N,Missing
W15-3807,W02-0305,0,\N,Missing
W16-3604,P15-2124,0,0.360908,"Missing"
W16-3604,L16-1704,1,0.12245,"Missing"
W16-3604,N16-1016,0,0.0298906,"Missing"
W16-3604,P15-2122,0,0.0528141,"stic Cues Rhetorical Questions. There is no previous work on distinguishing sarcastic from non-sarcastic uses of rhetorical questions (RQs). RQs are syntactically formulated as a question, but function as an indirect assertion (Frank, 1990). The polarity of the question implies an assertion of the opposite polarity, e.g. Can you read? implies You can’t read. RQs are prevalent in persuasive discourse, and are frequently used ironically (Schaffer, 2005; Ilie, 1994; Gibbs, 2000). Previous work focuses on their formal semantic properties (Han, 1997), or distinguishing RQs from standard questions (Bhattasali et al., 2015). We hypothesized that we could find RQs in abundance by searching for questions in the middle of a post, that are followed by a statement, using the assumption that questions followed by a statement are unlikely to be standard informationTable 3: Annotation Counts for a Subset of Cues Cue annotation experiments. After running a large number of retrieval experiments with our regex pattern matcher, we select batches of the resulting posts that mix different cue classes to put out for annotation, in such a way as to not allow the annotators to determine what regex cues were used. We then success"
W16-3604,N16-1082,0,0.010216,"7: Supervised Learning Results for Generic (Gen: 3,260 posts per class), Rhetorical Questions (RQ: 851 posts per class) and Hyperbole (Hyp: 582 posts per class) Supervised Learning. We restrict our supervised experiments to a default linear SVM learner with Stochastic Gradient Descent (SGD) training and L2 regularization, available in the SciKit-Learn toolkit (Pedregosa et al., 2011). We use 10-fold cross-validation, and only two types of features: n-grams and Word2Vec word embeddings. We expect Word2Vec to be able to capture semantic generalizations that n-grams do not (Socher et al., 2013; Li et al., 2016). The n-gram features include unigrams, bigrams, and trigrams, including sequences of punctuation (for example, ellipses or “!!!”), and emoticons. We use GoogleNews Word2Vec features (Mikolov et al., 2013).4 Table 7 summarizes the results of our supervised learning experiments on our datasets using 10-fold cross validation. The data is balanced evenly between the SARCASTIC and NOTSARCASTIC classes, and the best F-Measures for each class are shown in bold. The default W2V model, (trained on Google News), gives the best overall F-measure of 0.74 on the Gen corpus for the SARCASTIC class, while n"
W16-3604,W13-1104,1,0.878413,"Missing"
W16-3604,P14-5010,0,0.00267574,"r three subcorpora with AutoSlog-TS parameters, aimed at optimizing precision 4 Linguistic Analysis Dataset # Sarc Patterns # NotSarc Patterns Generic (Gen) 1,316 3,556 Rhetorical Questions (RQ) 671 1,000 Hyperbole (Hyp) 411 527 Table 10: Total number of patterns passing threshold of Freq ≥ 2, Prob ≥ 0.75 Here we aim to provide a linguistic characterization of the differences between the sarcastic and the not-sarcastic classes. We use the AutoSlog-TS pattern learner to generate patterns automatically, and the Stanford dependency parser to examine relationships between arguments (Riloff, 1996; Manning et al., 2014). Table 10 shows the number of sarcastic patterns we extract with AutoSlog-TS, with a frequency of at least 2 and a probability of at least 0.75 for each corpus. We learn many novel lexico-syntactic cue patterns that are not the regex that we search for. We discuss specific novel learned patterns for each class below. Rhetorical Questions. We notice that while the NOT- SARCASTIC patterns generated for RQs are similar to the topic-specific NOT- SARCASTIC patterns we find in the general dataset, there are some interesting features of the SARCASTIC patterns that are more unique to the RQs. Many o"
W16-3604,filatova-2012-irony,0,0.121062,"Missing"
W16-3604,W15-0515,1,0.41263,"Missing"
W16-3604,P11-2102,0,0.396638,"Missing"
W16-3604,W03-1014,1,0.652095,"Missing"
W16-3604,D13-1066,1,0.901418,"Missing"
W16-3604,D13-1170,0,0.00263744,"the corpus. Hyp Table 7: Supervised Learning Results for Generic (Gen: 3,260 posts per class), Rhetorical Questions (RQ: 851 posts per class) and Hyperbole (Hyp: 582 posts per class) Supervised Learning. We restrict our supervised experiments to a default linear SVM learner with Stochastic Gradient Descent (SGD) training and L2 regularization, available in the SciKit-Learn toolkit (Pedregosa et al., 2011). We use 10-fold cross-validation, and only two types of features: n-grams and Word2Vec word embeddings. We expect Word2Vec to be able to capture semantic generalizations that n-grams do not (Socher et al., 2013; Li et al., 2016). The n-gram features include unigrams, bigrams, and trigrams, including sequences of punctuation (for example, ellipses or “!!!”), and emoticons. We use GoogleNews Word2Vec features (Mikolov et al., 2013).4 Table 7 summarizes the results of our supervised learning experiments on our datasets using 10-fold cross validation. The data is balanced evenly between the SARCASTIC and NOTSARCASTIC classes, and the best F-Measures for each class are shown in bold. The default W2V model, (trained on Google News), gives the best overall F-measure of 0.74 on the Gen corpus for the SARCAS"
W16-3604,swanson-etal-2014-getting,1,0.912284,"Missing"
W16-3604,W02-1028,1,0.37196,"Missing"
W16-3604,walker-etal-2012-corpus,1,0.39942,"Missing"
W16-3604,P14-2084,0,0.13863,"Missing"
W17-5537,P15-2122,0,0.054115,"Missing"
W17-5537,W10-2914,0,0.235582,"Missing"
W17-5537,filatova-2012-irony,0,0.028364,"only computational work utilizing that data is by Battasali et al. (2015), who used n-gram language models with pre- and post-context to distinguish RQs from regular questions in SWBDDAMSL. Using context improved their results to 0.83 F1 on a balanced dataset of 958 instances, demonstrating that context information could be very useful for this task. Although it has been observed in the literature that RQs are often used sarcastically (Gibbs, 2000; Ilie, 1994), previous work on sarcasm classification has not focused on RQs (Bamman and Smith, 2015; Riloff et al., 2013; Liebrecht et al., 2013; Filatova, 2012; Gonz´alez-Ib´an˜ ez et al., 2011; Davi311 S ARCASTIC dov et al., 2010; Tsur et al., 2010). Riloff et al. (2013) investigated the utility of sequential features in tweets, emphasizing a subtype of sarcasm that consists of an expression of positive emotion contrasted with a negative situation, and showed that sequential features performed much better than features that did not capture sequential information. More recent work on sarcasm has focused specifically on sarcasm identification on Twitter using neural network approaches (Poria et al, 2016; Ghosh and Veale, 2016; Zhang et al., 2016; Ami"
W17-5537,W16-0425,0,0.0291747,", 2013; Liebrecht et al., 2013; Filatova, 2012; Gonz´alez-Ib´an˜ ez et al., 2011; Davi311 S ARCASTIC dov et al., 2010; Tsur et al., 2010). Riloff et al. (2013) investigated the utility of sequential features in tweets, emphasizing a subtype of sarcasm that consists of an expression of positive emotion contrasted with a negative situation, and showed that sequential features performed much better than features that did not capture sequential information. More recent work on sarcasm has focused specifically on sarcasm identification on Twitter using neural network approaches (Poria et al, 2016; Ghosh and Veale, 2016; Zhang et al., 2016; Amir et al., 2016). Other work emphasizes features of semantic incongruity in recognizing sarcasm (Joshi et al., 2015; Reyes et al., 2012). Sarcastic RQs clearly feature semantic incongruity, in some cases by expressing the certainty of particular facts in the frame of a question, and in other cases by asking questions like “Can you read?” (Row 2 in Table 1), a competence which a speaker must have, prima facie, to participate in online discussion. To our knowledge, our previous work is the first to consider the task of distinguishing sarcastic vs. not-sarcastic RQs, where"
W17-5537,W15-4322,0,0.0560455,"Missing"
W17-5537,P11-2102,0,0.0564532,"g that data is by Battasali et al. (2015), who used n-gram language models with pre- and post-context to distinguish RQs from regular questions in SWBDDAMSL. Using context improved their results to 0.83 F1 on a balanced dataset of 958 instances, demonstrating that context information could be very useful for this task. Although it has been observed in the literature that RQs are often used sarcastically (Gibbs, 2000; Ilie, 1994), previous work on sarcasm classification has not focused on RQs (Bamman and Smith, 2015; Riloff et al., 2013; Liebrecht et al., 2013; Filatova, 2012; Gonz´alez-Ib´an˜ ez et al., 2011; Davi311 S ARCASTIC dov et al., 2010; Tsur et al., 2010). Riloff et al. (2013) investigated the utility of sequential features in tweets, emphasizing a subtype of sarcasm that consists of an expression of positive emotion contrasted with a negative situation, and showed that sequential features performed much better than features that did not capture sequential information. More recent work on sarcasm has focused specifically on sarcasm identification on Twitter using neural network approaches (Poria et al, 2016; Ghosh and Veale, 2016; Zhang et al., 2016; Amir et al., 2016). Other work emphas"
W17-5537,L16-1704,1,0.851195,"ds for both domains below. Corpus Creation Sarcasm is a prevalent discourse function of RQs. In previous work, we observe both sarcastic and not-sarcastic uses of RQs in forums, and collect a set of sarcastic and not-sarcastic RQs in debate by using a heuristic stating that an RQ is a question that occurs in the middle of a turn, and which is answered immediately by the speaker themselves (Oraby et al., 2016). RQs are thus defined intentionally: the speaker indicates that their intention is not to elicit an answer by not ceding the turn.3 Debate Forums: The Internet Argument Corpus (IAC 2.0) (Abbott et al., 2016) contains a large number of discussions about politics and social issues, making it a good source of RQs. Following our previous work (2016), we first extract RQs in 3 We acknowledge that this method may miss RQs that do not follow this heuristic, but opt to use this conservative pattern for expanding the data to avoid introducing extra noise. 312 FACTUAL /I NFO -S EEKING Q UESTIONS posts whose length varies from 10-150 words, and collect five annotations for each of the RQs paired with the context of their following statements. We ask Turkers to specify whether or not the RQ-response pair is"
W17-5537,D15-1019,1,0.82697,"off et al. (2013) investigated the utility of sequential features in tweets, emphasizing a subtype of sarcasm that consists of an expression of positive emotion contrasted with a negative situation, and showed that sequential features performed much better than features that did not capture sequential information. More recent work on sarcasm has focused specifically on sarcasm identification on Twitter using neural network approaches (Poria et al, 2016; Ghosh and Veale, 2016; Zhang et al., 2016; Amir et al., 2016). Other work emphasizes features of semantic incongruity in recognizing sarcasm (Joshi et al., 2015; Reyes et al., 2012). Sarcastic RQs clearly feature semantic incongruity, in some cases by expressing the certainty of particular facts in the frame of a question, and in other cases by asking questions like “Can you read?” (Row 2 in Table 1), a competence which a speaker must have, prima facie, to participate in online discussion. To our knowledge, our previous work is the first to consider the task of distinguishing sarcastic vs. not-sarcastic RQs, where we construct a corpus of sarcasm in three types: generic, RQ, and hyperbole, and provide simple baseline experiments using ngrams (0.70 F1"
W17-5537,J81-4005,0,0.672732,"Missing"
W17-5537,W13-1605,0,0.145752,"Missing"
W17-5537,D13-1066,1,0.934664,"Missing"
W17-5537,W16-3604,1,0.9192,"learly feature semantic incongruity, in some cases by expressing the certainty of particular facts in the frame of a question, and in other cases by asking questions like “Can you read?” (Row 2 in Table 1), a competence which a speaker must have, prima facie, to participate in online discussion. To our knowledge, our previous work is the first to consider the task of distinguishing sarcastic vs. not-sarcastic RQs, where we construct a corpus of sarcasm in three types: generic, RQ, and hyperbole, and provide simple baseline experiments using ngrams (0.70 F1 for SARC and 0.71 F1 for NOT- SARC) (Oraby et al., 2016). Here, we adopt the same heuristic for gathering RQs and expand the corpus in debate forums, also collecting a novel Twitter corpus. We show that we can distinguish between SARCASTIC and OTHER uses of RQs that we observe, such as argumentation and persuasion in forums and Twitter, respectively. We show that linguistic features aid in the classification task, and explore the effects of context, using traditional and neural models. 3 1 2 Do you even read what anyone posts? Try it, you might learn something.......maybe not....... If they haven’t been discovered yet, HOW THE BLOODY HELL DO YOU KN"
W17-5537,swanson-etal-2014-getting,1,0.901893,"Missing"
W17-5537,W15-0515,1,0.879,"Missing"
W17-5537,P15-2124,0,\N,Missing
W17-5537,N16-1146,1,\N,Missing
W17-5537,C16-1151,0,\N,Missing
W17-5537,C16-1231,0,\N,Missing
W95-0112,P79-1002,0,0.0172946,"Missing"
W95-0112,J93-2004,0,0.0285585,"r information extraction (e.g., [Riloff, 1993; Kim and Moldovan, 1993]). However, previous approaches require an annotated training corpus or some other type of manually encoded training data. A n n o t a t e d training corpora are expensive to build, both in terms of the time and the expertise required to create them. Furthermore, training corpora for information extraction are typically annotated with domain-specific tags, in contrast to general-purpose annotations such as part-of-speech tags or noun-phrase bracketing (e.g., the Brown Corpus [Francis and Kucera, 1982] and the Penn Treebank [Marcus et al., 1993]). Consequently, a new training corpus must be annotated for each domain. We have begun to explore the possibility of using an untagged corpus to automatically acquire conceptual patterns for information extraction. Our approach uses a combination of domainindependent linguistic rules and statistics. The linguistic rules are based on our previous system, AutoSlog [Riloff, 1993], which automatically constructs dictionaries for information extraction using an annotated training corpus. We have put a new spin on the original system by applying it exhaustively to an untagged but preclassified tra"
W95-0112,J93-2006,0,0.0165105,"d relies only on preclassified texts. 5 Discussion AutoSlog-TS demonstrates that conceptual patterns for information extraction can be acquired automatically from only a preclassified text corpus, thereby obviating the need for an annotated training corpus. Generating annotated corpora is time-consuming and sometimes difficult, though the payoffs are often significant. General purpose text annotations, such as part-of-speech tags and noun-phrase bracketing, are costly to obtain but have wide applicability and have been used successfully to develop statistical NLP systems (e.g., [Church, 1989; Weischedel et al., 1993]). Domain-specific text annotations, however, require a domain expert and have much narrower applicability. From a practical perspective, it is important to consider the human factor and to try to minimize the amount of time and effort required to build a training corpus. Domain-specific text annotations are expensive to obtain, so our goal has been to eliminate our dependence on them. 15As we stated in Section 3.1, it took a person only 5 hours to review the 1237 concept nodes produced by AutoSlog [Riloff, 1993]. 16The connected words represent phrases in CIRCUS&apos; lexicon. 160 We h a v e s h"
W95-0112,A88-1000,0,\N,Missing
W95-0112,M92-1038,0,\N,Missing
W97-0313,P79-1002,0,0.29143,"lding semantic lexicons will always be a subjective process, and the quality of a semantic lexicon is highly dependent on the task for which it will be used. But there is no question that semantic knowledge is essential for many problems in natural language processing. Most of the time semantic knowledge is defined manually for the target application, but several techniques have been developed for generating semantic knowledge automatically. Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994)). Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989; Cardie, 1993)). Our task orientation is a bit different because we are trying to construct a semantic lexicon for a target category, instead of classifying unknown or polysemous words in context. To our knowledge, our system is the first one aimed at building semantic lexicons from raw text without using any additional semantic knowledge. The only lexical knowledge used by our parser is a part-of-"
W97-0313,C92-2070,0,0.0420762,". 2 Generating a Semantic Lexicon Our work is based on the observation that category members are often surrounded by other category members in text, for example in conjunctions (lions and tigers and bears), lists (lions, tigers, bears...), appositives (the stallion, a white Arabian), and nominal compounds (Arabian stallion; tuna fish). Given a few category members, we wondered whether it 117 would be possible to collect surrounding contexts and use statistics to identify other words that also belong to the category. Our approach was motivated by Yarowsky&apos;s word sense disambiguation algorithm (Yarowsky, 1992) and the notion of statistical salience, although our system uses somewhat different statistical measures and techniques. We begin with a small set of seed words for a category. We experimented with different numbers of seed words, but were surprised to find that only 5 seed words per category worked quite well. As an example, the seed word lists used in our experiments are shown below. The context windows do not cut across sentence boundaries. Note that our context window is much narrower than those used by other researchers (Yarowsky, 1992). We experimented with larger window sizes and found"
W97-0313,J93-2006,0,\N,Missing
W97-0313,M92-1038,0,\N,Missing
W98-1106,W97-1002,0,0.0507356,"Missing"
W98-1106,W97-0313,1,0.928083,"oking for places where the case frames failed to recognize desired information. But this approach is extremely timeconsuming unless the answers are known in advance (i.e., the information that should have been extracted), which is unrealistic for most applications. It should be possible, however, to learn case frame structures automaticallyfrom a text corpus. Toward this end, we have been developing a corpus-b~ed approach to conceptual case frame acquisition. Our approach builds upon earlier work on corpus-based methods for generating extraction patterns (Riloff, 1996b) and semantic lexicons (Riloff and Shepherd, 1997). Our new system constructs conceptual case frames by learning semantic preferences for extraction patterns and merging syntactically compatible patterns into more complex structures. The resulting case frames can have slots for multiple role objects and each slot has a set of learned selectional restrictions for its role object. The first section of this paper begins with background about AutoSlog-TS, a corpus-based system for generating extraction patterns automatically, and the extraction patterns that it generates. The following section presents a new corpus-based algorithm that uses the e"
W98-1106,M92-1021,0,\N,Missing
W98-1106,M92-1038,0,\N,Missing
