1994.amta-1.6,P94-1005,0,0.0567189,"Missing"
1994.amta-1.6,C94-1057,0,0.0285998,"Missing"
1994.amta-1.6,C94-1038,0,0.0525107,"Missing"
2003.mtsummit-eval.6,levin-etal-2000-lessons,0,0.236732,"Missing"
2003.mtsummit-eval.6,N03-2021,0,0.0853093,"Missing"
2003.mtsummit-eval.6,2001.mtsummit-papers.68,0,0.0129763,"archy of tasks. They proposed that the ""weakest"" engines be identified as those that only enable their users to perform the least linguistically demanding task in the hierarchy, while the ""stronger"" engines would be identified as those that enable their users to perform more linguistically demanding tasks. To the best of our knowledge, this was the first proposal for systematic, linguistically motivated, task-based MT evaluation. Since then, some MT researchers have begun to examine evaluation tasks and report on these at workshops,1 while others have focused on nontask-based, n-gram metrics (Papineni et al., 2001; Doddington, 2002 on DARPA TIDES2 MTEval).3 Curiously, there appear to be only a few researchers who both work with actual, current 1 See Hovy et al. (2003) for a listing of website links to five such workshops since 2000. 2 TIDES is a research program administered by DARPA, one agency within the US Department of Defense that has funded natural language processing research for many years. 3 Indeed this interest in n-gram-based evaluation has extended beyond MT to research on summarization (Pastra et al. 2002) and headline generation (Zajic et al. 2002). 43 operational MT systems under develop"
2003.mtsummit-eval.6,taylor-white-1998-predicting,0,0.0682672,"tput that is disfluent or incomplete. Google's decision to make MT available to their users, however, suggests that this wariness does not deter all users. Indeed it may be that Google has observed empirically what Levin et al. (2000) report in evaluating the even noisier case of speech translation: users of MT output perform their tasks at a much higher rate of success than would be expected, given the accuracy of the MT output. For MT researchers, their attention to actual, practical tasks that even weak MT engines can perform, was first sparked by Church and Hovy (1993). A few years later, Taylor and White (1998) hypothesized that MT engines could be evaluated operationally in terms of a hierarchy of tasks. They proposed that the ""weakest"" engines be identified as those that only enable their users to perform the least linguistically demanding task in the hierarchy, while the ""stronger"" engines would be identified as those that enable their users to perform more linguistically demanding tasks. To the best of our knowledge, this was the first proposal for systematic, linguistically motivated, task-based MT evaluation. Since then, some MT researchers have begun to examine evaluation tasks and report on"
2004.eamt-1.13,2003.mtsummit-papers.32,0,0.0811218,"Missing"
2004.eamt-1.13,2001.mtsummit-papers.68,0,0.0392072,"Missing"
2006.amta-papers.27,E06-1032,0,\N,Missing
2006.amta-papers.27,N04-1001,0,\N,Missing
2006.amta-papers.27,lavie-etal-2004-significance,0,\N,Missing
2006.amta-papers.27,taylor-white-1998-predicting,0,\N,Missing
2006.amta-papers.27,W00-0502,0,\N,Missing
2006.amta-papers.27,H89-1032,0,\N,Missing
2006.amta-papers.27,N03-2021,0,\N,Missing
2006.amta-papers.27,P02-1040,0,\N,Missing
2006.amta-papers.27,1999.mtsummit-1.85,0,\N,Missing
2006.amta-papers.27,W05-0909,0,\N,Missing
2006.amta-papers.27,J90-3005,0,\N,Missing
2006.eamt-1.25,W05-0909,0,0.0379392,"use is suited to all people and no single MT system is suited to all people, it is also true that no MT metric is suited to everyone’s needs. While automatic metrics have been used primarily by the MT development community, we are nonetheless interested in how they may be of use to the MT user community. The pace at which new MT metrics are being introduced is invigorating the field and encouraging wider participation and challenges that will ultimately lead to metrics that will be better understood by all. Recent proposals include (i) detecting paraphrased variants of reference translations (Banerjee & Lavie, 2005), (ii) identifying output of syntactic forms that correlate highly with human subjective judgments (Liu & Gildea, 2005), and (iii) capturing more highly valued information in content words with part-of-speech re-weighting (Callison-Burch et al., 2006). planatory variables to yield an adequate model for predicting task response results (Tate 2005). Further model-testing is now underway exploring a broader range of text-based metrics, including source language complexity (Clifford et al., 2004). Acknowledgements Several individuals contributed to the task-based evaluation research project, inclu"
2006.eamt-1.25,E06-1032,0,0.150336,"Missing"
2006.eamt-1.25,2003.mtsummit-papers.9,0,0.0122411,"istically-trained engine. The statistically significant results were mixed, pre-empting an analysis with a single, across-the-board ranking of the engines. To assist users in interpreting these results for their own work environments, we are now working with the collected data to develop a single customizable metric with loss functions, weighted over these rates. While the experiment and evaluation methodology have provided the results that the MT users request, namely an analysis of a real-world task on multiple MT engines, this approach is quite costly, time-consuming and laborintensive. As Coughlin (2003) has noted, resource considerations such as these have forced the field to rely heavily on automated metrics. Thus, it is crucial in any evaluation to determine how well results with these metrics compare to the results we find in task-based analyses. In particular, we want to know whether there is a relationship between these popular, strictly textbased metrics and the end-to-end (machine and user) effectiveness metrics of concern to real users. Our next step is to explore this relationship by studying various aspects of automated metric correlation with subject responses from the information"
2006.eamt-1.25,H93-1035,0,0.0623371,"d ranking of the engines. Our next step is to incorporate the collected data in statistical models and test for their adequacy in predicting these task results from faster and less expensive, automatic metrics. The long-term goal is to understand which metrics accurately predict MT users’ task effectiveness with different MT engines on text-handling tasks of varying levels of difficulty. 1 Introduction Among machine translation (MT) developers for over a decade, there has been the assumption that MT engines are “good enough” to support people performing certain applications in the real world (Church & Hovy 1993). More recently, informal reports from operational and field settings have described successful, but carefully limited use of MT output in real-world tasks. (Fisher, et al. 1999; Holland, 2005). The research reported here was undertaken to assess how effectively people can perform one specific real-world task on the outputs of different MT engines. The paper describes the results of a one-of-a-kind, large-scale, MT evaluation experiment where nearly sixty subjects extracted who, when, and where-type essential elements of information (EIs) from output generated by three types of Arabic-English"
2006.eamt-1.25,laoudi-etal-2006-task,1,0.678102,"upper-bound for a shared limitation, that none of the selected engines could yet support. This report focuses exclusively 3 on the 2 www.darpa.mil/ipto/solicitations/closed/0528 PIP.htm 3 The choice for task (i) as topic categorization, a form of Taylor & White’s “detection,” and for task (iii) as template completion, a form of Taylor & White’s “deep extraction” were based on our previous experiments (Tate, Lee, & Voss, 2003; and Voss, 2002). The results of the pilot conducted for task (iii) indicated that the MT engines were not adequate to support users performing event-template completion (Laoudi, Tate, & Voss, 2006). experiment conducted for task (ii), that we gauged to be at an intermediate level of extraction difficulty, shown as an extra row inserted in Table 1. A small, prior pilot experiment to evaluate Arabic-English MT engines for document exploitation tasks indicated that subjects could extract some named entities and event participants from noisy MT output, but they could not readily identify relations within events (Voss, 2002). This led us to select wh-item extraction, a task between event-level analysis and named-entity recognition. Selection of MT Engines In conjunction with the project spo"
2006.eamt-1.25,lavie-etal-2004-significance,0,0.0136398,"1998; White et al., 2000). Then, with the introduction of several automatic MT metrics1 demonstrating both the vitality of MT evaluation as a research area of its own and the impact of metrics on the MT development cycle, MT stakeholders began funding research experiments in task-based assessment of MT engines, to address users’ needs. See, for example, the request for proposals that include methods for utility evaluations, in the 2005 broad agency announcement for GALE (Global Autonomous Language Exploitation), a large 1 Such as BLEU (Papineni et al. 2002), GTM (Melamed et al. 2003), METEOR (Lavie et al. 2004), and TER (Snover et al. 2005). research program, directed by DARPA, a U.S. government funding agency. 2 Publishing Gisting Extraction Deep Extraction Intermediate Extraction Wh-Item Extraction Shallow Extraction Triage Detection Filtering Produce technically correct document in fluent English Produce a summary of the document For documents of interest, capture specified key information Event identification (scenarios): determine an incident type and report all pertinent information Relationship identification: member-of, friend-of, boss-of Identification of: who-, where-, when-type informatio"
2006.eamt-1.25,W05-0904,0,0.0843735,"Missing"
2006.eamt-1.25,levin-etal-2000-lessons,0,0.0410293,"Missing"
2006.eamt-1.25,N03-2021,0,0.0123001,"ndling tasks (Taylor & White, 1998; White et al., 2000). Then, with the introduction of several automatic MT metrics1 demonstrating both the vitality of MT evaluation as a research area of its own and the impact of metrics on the MT development cycle, MT stakeholders began funding research experiments in task-based assessment of MT engines, to address users’ needs. See, for example, the request for proposals that include methods for utility evaluations, in the 2005 broad agency announcement for GALE (Global Autonomous Language Exploitation), a large 1 Such as BLEU (Papineni et al. 2002), GTM (Melamed et al. 2003), METEOR (Lavie et al. 2004), and TER (Snover et al. 2005). research program, directed by DARPA, a U.S. government funding agency. 2 Publishing Gisting Extraction Deep Extraction Intermediate Extraction Wh-Item Extraction Shallow Extraction Triage Detection Filtering Produce technically correct document in fluent English Produce a summary of the document For documents of interest, capture specified key information Event identification (scenarios): determine an incident type and report all pertinent information Relationship identification: member-of, friend-of, boss-of Identification of: who-,"
2006.eamt-1.25,P02-1040,0,0.0758661,"roposed by users for text-handling tasks (Taylor & White, 1998; White et al., 2000). Then, with the introduction of several automatic MT metrics1 demonstrating both the vitality of MT evaluation as a research area of its own and the impact of metrics on the MT development cycle, MT stakeholders began funding research experiments in task-based assessment of MT engines, to address users’ needs. See, for example, the request for proposals that include methods for utility evaluations, in the 2005 broad agency announcement for GALE (Global Autonomous Language Exploitation), a large 1 Such as BLEU (Papineni et al. 2002), GTM (Melamed et al. 2003), METEOR (Lavie et al. 2004), and TER (Snover et al. 2005). research program, directed by DARPA, a U.S. government funding agency. 2 Publishing Gisting Extraction Deep Extraction Intermediate Extraction Wh-Item Extraction Shallow Extraction Triage Detection Filtering Produce technically correct document in fluent English Produce a summary of the document For documents of interest, capture specified key information Event identification (scenarios): determine an incident type and report all pertinent information Relationship identification: member-of, friend-of, boss-o"
2006.eamt-1.25,2003.mtsummit-eval.6,1,0.851834,"Missing"
2006.eamt-1.25,taylor-white-1998-predicting,0,0.0170718,"dings and a few words about future directions for our work. 2 Background Extrinsic, task-based evaluation of MT engines has long been of interest to MT users who seek automated support tools to expedite their decision-making tasks (Spaerck-Jones & Gallier, 1996). In the late 1990’s two new MT research trends emerged, furthering interest in extrinsic metrics: task-based experiments were being conducted by MT developers on their own engines (Resnik, 1997; Levin et al., 1999), and task-based experiments assuming an ordering of task difficulty were being proposed by users for text-handling tasks (Taylor & White, 1998; White et al., 2000). Then, with the introduction of several automatic MT metrics1 demonstrating both the vitality of MT evaluation as a research area of its own and the impact of metrics on the MT development cycle, MT stakeholders began funding research experiments in task-based assessment of MT engines, to address users’ needs. See, for example, the request for proposals that include methods for utility evaluations, in the 2005 broad agency announcement for GALE (Global Autonomous Language Exploitation), a large 1 Such as BLEU (Papineni et al. 2002), GTM (Melamed et al. 2003), METEOR (Lavi"
2006.eamt-1.25,W00-0502,0,0.0224869,"about future directions for our work. 2 Background Extrinsic, task-based evaluation of MT engines has long been of interest to MT users who seek automated support tools to expedite their decision-making tasks (Spaerck-Jones & Gallier, 1996). In the late 1990’s two new MT research trends emerged, furthering interest in extrinsic metrics: task-based experiments were being conducted by MT developers on their own engines (Resnik, 1997; Levin et al., 1999), and task-based experiments assuming an ordering of task difficulty were being proposed by users for text-handling tasks (Taylor & White, 1998; White et al., 2000). Then, with the introduction of several automatic MT metrics1 demonstrating both the vitality of MT evaluation as a research area of its own and the impact of metrics on the MT development cycle, MT stakeholders began funding research experiments in task-based assessment of MT engines, to address users’ needs. See, for example, the request for proposals that include methods for utility evaluations, in the 2005 broad agency announcement for GALE (Global Autonomous Language Exploitation), a large 1 Such as BLEU (Papineni et al. 2002), GTM (Melamed et al. 2003), METEOR (Lavie et al. 2004), and T"
2007.mtsummit-papers.32,J84-3009,0,0.701915,"Missing"
2008.amta-govandcom.12,P07-2045,0,0.00442358,"Missing"
2008.amta-govandcom.12,2008.eamt-1.26,1,0.877859,"Missing"
2008.eamt-1.26,2006.eamt-1.27,0,0.0155739,"llel segments in training the post-editor, but not necessarily beyond that. Introduction In industry and government, MT developers may be asked to improve existing, operational translation systems with relatively weak, black-box MT engines because higher quality MT engines are not available and only a limited quantity of online resources is available. Recent research results show impressive performance gains in translating between Indo-European languages when chaining together mature, existing rule-based MT engines and post-MT editors built automatically with limited amounts of parallel data ([1], [2], [3]). In this paper, we show that this hybrid approach of serially composing an MT engine and automated post-MT editor---when applied to much weaker MT engines, translating across more widely divergent languages, and given only limited amounts of training data---will yield statistically significant boosts in translation quality up to the first 50K of parallel segments in training the post-editor, but not necessarily beyond that. 192 12th EAMT conference, 22-23 September 2008, Hamburg, Germany The key idea behind our approach is to have MT engines do their own translations to boost the p"
2008.eamt-1.26,W07-0732,0,0.507973,"segments in training the post-editor, but not necessarily beyond that. Introduction In industry and government, MT developers may be asked to improve existing, operational translation systems with relatively weak, black-box MT engines because higher quality MT engines are not available and only a limited quantity of online resources is available. Recent research results show impressive performance gains in translating between Indo-European languages when chaining together mature, existing rule-based MT engines and post-MT editors built automatically with limited amounts of parallel data ([1], [2], [3]). In this paper, we show that this hybrid approach of serially composing an MT engine and automated post-MT editor---when applied to much weaker MT engines, translating across more widely divergent languages, and given only limited amounts of training data---will yield statistically significant boosts in translation quality up to the first 50K of parallel segments in training the post-editor, but not necessarily beyond that. 192 12th EAMT conference, 22-23 September 2008, Hamburg, Germany The key idea behind our approach is to have MT engines do their own translations to boost the perfor"
2008.eamt-1.26,W07-0728,0,0.188977,"nts in training the post-editor, but not necessarily beyond that. Introduction In industry and government, MT developers may be asked to improve existing, operational translation systems with relatively weak, black-box MT engines because higher quality MT engines are not available and only a limited quantity of online resources is available. Recent research results show impressive performance gains in translating between Indo-European languages when chaining together mature, existing rule-based MT engines and post-MT editors built automatically with limited amounts of parallel data ([1], [2], [3]). In this paper, we show that this hybrid approach of serially composing an MT engine and automated post-MT editor---when applied to much weaker MT engines, translating across more widely divergent languages, and given only limited amounts of training data---will yield statistically significant boosts in translation quality up to the first 50K of parallel segments in training the post-editor, but not necessarily beyond that. 192 12th EAMT conference, 22-23 September 2008, Hamburg, Germany The key idea behind our approach is to have MT engines do their own translations to boost the performance"
2008.eamt-1.26,P07-2045,0,0.0106705,"roach section. The paper concludes with a discussion of open issues and notes of future work. Approach We know that human translators dislike working with MT output because---with no mechanisms built into the MT system to learn directly from human post-editing corrections--- the same errors appear over and over and the translators must make the same corrections over and over again as well. Our in-house requirement has been to determine how to boost the MT engines we already have and eliminate, where possible, known errors. In this paper, we report on leveraging the SRILM and MOSES tools ([4], [5]) without modifications to rapidly build statistical post-MT editors in just a few months. Our work follows from the insights of [2] and [3] that “post-editors” can be built as monolingual translation engines that convert “raw” target-language (TL) text produced by a baseline MT engine into higher quality TL text by correcting errors in TL word choice and order. Our approach has been to augment two in-house Urdu-to-English MT engines, one rule-based and one lexicon-based, with automated statistical post-editors built from the same corpus of parallel-aligned data to address several questions: 1"
2008.eamt-1.26,N03-2021,0,0.0225399,"gnment algorithm was restricted to comparing segments within aligned documents. Before starting the alignment, documents were binned into three groups: those containing the same number of segments (Equal), those whose segment counts were off by one (OneOff), and those whose segments counts were more than one off (MoreThanOneOff). We had expected that segment pairs within Equal document set might already be perfectly aligned. On inspection however, we found that many documents in the Equal set were not segment-aligned. The automated evaluation metrics in the algorithm were BLEU [7] and GTM 1.4 [8]. After some initial experimentation, BLEU was set to have an n-gram size of 2, to yield more of a score spread across segments. The translation engines in the algorithm were the in-house LBMT and RBMT engines. The algorithm also included postMT processing prior to segment-pair scoring to remove annotations intended for the human reader only, to boost segment scores and again create more spread across segments. Alignment Algorithm The algorithm steps, necessarily simplifying somewhat from all the details, were: 1. Split the original single files with all of the &quot;aligned&quot; data in it into separa"
2008.eamt-1.26,D07-1091,0,0.0135132,"and SMT engines, and between SMT and RBMT+APE engines (50K alignmt 2) Document score changes Bleu-4 # increased / unchanged / decreased between LBMT+APE and SMT 71 / 59 / 2 between SMT and RBMT+APE 90 / 3 / 39 Table 7 also shows that with document-level scores, there is some evidence to rank the SMT below the RBMT+APE builds on 50K alignments.7 With a carefully constructed test set, it would be possible to determine whether this RBMT provides to its APE a parsing analysis and re-ordering advantage that the SMT we have built in its current form lacks (for example, no factored translation model [9], because we lacked Urdu resources to annotate our data for lemmas, part-of-speech, morphology, word class). Though we have presented an evaluation of the alignments and hybrid builds in terms of Bleu scores, we recognize that this is but a first step in reaching a deeper understanding of the impact and effectiveness of APEs when chained with LBMT and RBMT engines, 7 We apply paired t-tests of statistical significance over document scores, rather than using BLEU’s automated confidence intervals without system to system paired comparisons. 199 12th EAMT conference, 22-23 September 2008, Hamburg"
2008.eamt-1.26,E06-1032,0,0.0235859,"gh we have presented an evaluation of the alignments and hybrid builds in terms of Bleu scores, we recognize that this is but a first step in reaching a deeper understanding of the impact and effectiveness of APEs when chained with LBMT and RBMT engines, 7 We apply paired t-tests of statistical significance over document scores, rather than using BLEU’s automated confidence intervals without system to system paired comparisons. 199 12th EAMT conference, 22-23 September 2008, Hamburg, Germany especially given the well-recognized limitations of literal matching for assessing translation quality [10]. At this stage in our work, we have begun manually assessing segment outputs across translation engines by aligning them with multiple reference translations (RTs), as shown in Table 8. The APEs for both engines made three identical substitutions while also making other distinct changes. The LBMT+APE and StatMT outputs are strikingly similar, while the RBMT+APE output quite distinct re-ordering of the Urdu original word order (compare with LBMT output). Table 8. Five MT system outputs (LBMT, LBMT+APE, RBMT, RBMT+APE, StatMT) on same input segment and four Reference Translations (RT1-4), manua"
2008.eamt-1.26,P02-1040,0,\N,Missing
2009.mtsummit-government.3,2007.mtsummit-papers.27,0,0.0995971,"Missing"
2009.mtsummit-government.3,hobbs-etal-2008-mtriage,1,0.86986,"Missing"
2009.mtsummit-government.3,W08-0511,1,0.856397,"Missing"
2020.acl-demos.11,W18-2501,0,0.0205612,"Missing"
2020.acl-demos.11,S10-1059,0,0.0339721,"2016). For types without coarse-grained type training data in ACE/ERE, we design dependency pathbased patterns instead and implement a rule-based system to detect their fine-grained relations directly from the text (Li et al., 2019b). 3.3 Text Event Extraction and Coreference We start by extracting coarse-grained events and arguments using a Bi-LSTM CRF model and a CNNbased model (Zhang et al., 2018b) for three languages, and then detect the fine-grained event types by applying verb-based rules, context-based rules, and argument-based rules (Li et al., 2019b). We also extract FrameNet frames (Chen et al., 2010) in English corpora to enrich the fine-grained events. We apply a graph-based algorithm (AlBadrashiny et al., 2017) for our languageindependent event coreference resolution. For each event type, we cast the event mentions as nodes in a graph, so that the undirected, weighted edges between these nodes represent coreference confidence scores between their corresponding events. We then apply hierarchical clustering to obtain event clusters and train a Maximum Entropy binary classifier on the cluster features (Li et al., 2019b). 4 Visual Knowledge Extraction The Visual Knowledge Extraction (VKE) b"
2020.acl-demos.11,D11-1142,0,0.490499,"at GitHub3 and DockerHub4 , with complete documentation5 . 1 Introduction Knowledge Extraction (KE) aims to find entities, relations and events involving those entities from unstructured data, and link them to existing knowledge bases. Open source KE tools are useful for many real-world applications including disaster monitoring (Zhang et al., 2018a), intelligence analysis (Li et al., 2019a) and scientific knowledge mining (Luan et al., 2017; Wang et al., 2019). Recent years have witnessed the great success and wide usage of open source Natural Language Processing tools (Manning et al., 2014; Fader et al., 2011; Gardner et al., 2018; Daniel Khashabi, 2018; Honnibal and Montani, 2017), but there is no comprehensive open source system for KE. We release ∗ These authors contributed equally to this work. System page: http://blender.cs.illinois.edu/ software/gaia-ie 2 http://tac.nist.gov/2019/SM-KBP/index.html 3 GitHub: https://github.com/GAIA-AIDA 4 DockerHub: text knoweldge extraction components are in https://hub.docker.com/orgs/blendernlp/ repositories, visual knowledge extraction components are in https://hub.docker.com/u/dannapierskitoptal 5 Video: http://blender.cs.illinois.edu/aida/ gaia.mp4 1 Fi"
2020.acl-demos.11,P16-2096,0,0.0126616,"l entities. 7 Related Work Existing knowledge extraction systems mainly focus on text (Manning et al., 2014; Fader et al., 2011; Gardner et al., 2018; Daniel Khashabi, 2018; Honnibal and Montani, 2017; Pan et al., 2017; Li et al., 2019a), and do not readily support fine-grained 13 https://trec.nist.gov/trec_eval/ LDC2018E01, LDC2018E52, LDC2018E63, LDC2018E76, LDC2019E77 15 http://blender.cs.illinois.edu/demo/video_ recommendation/index_attack_dark.html 14 Innovations in technology often face the ethical dilemma of dual use: the same advance may offer potential benefits and harms (Ehni, 2008; Hovy and Spruit, 2016; Brundage et al., 2018). We first discuss dual use,17 as it relates to this demo in particular and then discuss two other considerations for applying this technology, data bias and privacy. For our demo, the distinction between beneficial use and harmful use depends, in part, on the data. Proper use of the technology requires that input documents/images are legally and ethically obtained. Regulation and standards (e.g. GDPR18 ) provide a legal framework for ensuring that such data is properly used and that any individual whose data is used has the right to request its removal. In the absence"
2020.acl-demos.11,N19-4019,1,0.475259,"seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos. GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation2 . The system is publicly available at GitHub3 and DockerHub4 , with complete documentation5 . 1 Introduction Knowledge Extraction (KE) aims to find entities, relations and events involving those entities from unstructured data, and link them to existing knowledge bases. Open source KE tools are useful for many real-world applications including disaster monitoring (Zhang et al., 2018a), intelligence analysis (Li et al., 2019a) and scientific knowledge mining (Luan et al., 2017; Wang et al., 2019). Recent years have witnessed the great success and wide usage of open source Natural Language Processing tools (Manning et al., 2014; Fader et al., 2011; Gardner et al., 2018; Daniel Khashabi, 2018; Honnibal and Montani, 2017), but there is no comprehensive open source system for KE. We release ∗ These authors contributed equally to this work. System page: http://blender.cs.illinois.edu/ software/gaia-ie 2 http://tac.nist.gov/2019/SM-KBP/index.html 3 GitHub: https://github.com/GAIA-AIDA 4 DockerHub: text knoweldge extrac"
2020.acl-demos.11,D19-1641,1,0.720878,"und KBs (Pan et al., 2015), including Freebase (LDC2015E42) and GeoNames (LDC2019E43). For mentions that are linkable to the same Freebase entity, coreference information is added. For name mentions that cannot be linked to the KB, we apply heuristic rules (Li et al., 2019b) to same-named mentions within each document to form NIL clusters. A NIL cluster is a cluster of entity mentions referring to the same entity but do not have corresponding KB entries (Ji et al., 2014). Fine-grained Entity Typing We develop an attentive fine-grained type classification model with latent type representation (Lin and Ji, 2019). It takes as input a mention with its context sentence and predicts the most likely fine-grained type. We obtain the YAGO (Suchanek et al., 2008) fine-grained types from the results of Freebase entity linking, and map these types to the DARPA AIDA ontology. For mentions with identified, coarse-grained GPE and LOC types, we further determine their fine-grained types using GeoNames attributes feature class and feature code from the GeoNames entity linking result. Given that most nominal mentions are descriptions and thus do not link to entries in Freebase or GeoNames, we develop a nominal keywo"
2020.acl-demos.11,P19-1016,1,0.755197,"a primary event and its connected events from the knowledge graph (screenshot in Figure 2). 3 Text Knowledge Extraction As shown in Figure 3, the Text Knowledge Extraction (TKE) system extracts entities, relations, and events from input documents. Then it clusters identical entities through entity linking and coreference, and clusters identical events using event coreference. 8 https://tac.nist.gov/tracks/SM-KBP/2019/ ontologies/LDCOntology 3.1 Text Entity Extraction and Coreference Coarse-grained Mention Extraction We extract coarse-grained named and nominal entity mentions using a LSTM-CRF (Lin et al., 2019) model. We use pretrained ELMo (Peters et al., 2018) word embeddings as input features for English, and pretrain Word2Vec (Le and Mikolov, 2014) models on Wikipedia data to generate Russian and Ukrainian word embeddings. Entity Linking and Coreference We seek to link the entity mentions to pre-existing entities in the background KBs (Pan et al., 2015), including Freebase (LDC2015E42) and GeoNames (LDC2019E43). For mentions that are linkable to the same Freebase entity, coreference information is added. For name mentions that cannot be linked to the KB, we apply heuristic rules (Li et al., 2019"
2020.acl-demos.11,D17-1279,0,0.0504435,"Missing"
2020.acl-demos.11,N15-1119,1,0.814395,"using event coreference. 8 https://tac.nist.gov/tracks/SM-KBP/2019/ ontologies/LDCOntology 3.1 Text Entity Extraction and Coreference Coarse-grained Mention Extraction We extract coarse-grained named and nominal entity mentions using a LSTM-CRF (Lin et al., 2019) model. We use pretrained ELMo (Peters et al., 2018) word embeddings as input features for English, and pretrain Word2Vec (Le and Mikolov, 2014) models on Wikipedia data to generate Russian and Ukrainian word embeddings. Entity Linking and Coreference We seek to link the entity mentions to pre-existing entities in the background KBs (Pan et al., 2015), including Freebase (LDC2015E42) and GeoNames (LDC2019E43). For mentions that are linkable to the same Freebase entity, coreference information is added. For name mentions that cannot be linked to the KB, we apply heuristic rules (Li et al., 2019b) to same-named mentions within each document to form NIL clusters. A NIL cluster is a cluster of entity mentions referring to the same entity but do not have corresponding KB entries (Ji et al., 2014). Fine-grained Entity Typing We develop an attentive fine-grained type classification model with latent type representation (Lin and Ji, 2019). It take"
2020.acl-demos.11,P17-1178,1,0.874756,"f events being viewed, such as the fine-grained type, place, time, attacker, target, and instrument. The demo is publicly available15 with a user interface as shown in Figure 2, displaying extracted text entities and events across languages, visual entities, visual entity linking and coreference results from face, landmark and flag recognition, and the results of grounding text entities to visual entities. 7 Related Work Existing knowledge extraction systems mainly focus on text (Manning et al., 2014; Fader et al., 2011; Gardner et al., 2018; Daniel Khashabi, 2018; Honnibal and Montani, 2017; Pan et al., 2017; Li et al., 2019a), and do not readily support fine-grained 13 https://trec.nist.gov/trec_eval/ LDC2018E01, LDC2018E52, LDC2018E63, LDC2018E76, LDC2019E77 15 http://blender.cs.illinois.edu/demo/video_ recommendation/index_attack_dark.html 14 Innovations in technology often face the ethical dilemma of dual use: the same advance may offer potential benefits and harms (Ehni, 2008; Hovy and Spruit, 2016; Brundage et al., 2018). We first discuss dual use,17 as it relates to this demo in particular and then discuss two other considerations for applying this technology, data bias and privacy. For ou"
2020.acl-demos.11,N18-1202,0,0.0435292,"knowledge graph (screenshot in Figure 2). 3 Text Knowledge Extraction As shown in Figure 3, the Text Knowledge Extraction (TKE) system extracts entities, relations, and events from input documents. Then it clusters identical entities through entity linking and coreference, and clusters identical events using event coreference. 8 https://tac.nist.gov/tracks/SM-KBP/2019/ ontologies/LDCOntology 3.1 Text Entity Extraction and Coreference Coarse-grained Mention Extraction We extract coarse-grained named and nominal entity mentions using a LSTM-CRF (Lin et al., 2019) model. We use pretrained ELMo (Peters et al., 2018) word embeddings as input features for English, and pretrain Word2Vec (Le and Mikolov, 2014) models on Wikipedia data to generate Russian and Ukrainian word embeddings. Entity Linking and Coreference We seek to link the entity mentions to pre-existing entities in the background KBs (Pan et al., 2015), including Freebase (LDC2015E42) and GeoNames (LDC2019E43). For mentions that are linkable to the same Freebase entity, coreference information is added. For name mentions that cannot be linked to the KB, we apply heuristic rules (Li et al., 2019b) to same-named mentions within each document to fo"
2020.acl-demos.11,P14-5010,0,0.0168957,"is publicly available at GitHub3 and DockerHub4 , with complete documentation5 . 1 Introduction Knowledge Extraction (KE) aims to find entities, relations and events involving those entities from unstructured data, and link them to existing knowledge bases. Open source KE tools are useful for many real-world applications including disaster monitoring (Zhang et al., 2018a), intelligence analysis (Li et al., 2019a) and scientific knowledge mining (Luan et al., 2017; Wang et al., 2019). Recent years have witnessed the great success and wide usage of open source Natural Language Processing tools (Manning et al., 2014; Fader et al., 2011; Gardner et al., 2018; Daniel Khashabi, 2018; Honnibal and Montani, 2017), but there is no comprehensive open source system for KE. We release ∗ These authors contributed equally to this work. System page: http://blender.cs.illinois.edu/ software/gaia-ie 2 http://tac.nist.gov/2019/SM-KBP/index.html 3 GitHub: https://github.com/GAIA-AIDA 4 DockerHub: text knoweldge extraction components are in https://hub.docker.com/orgs/blendernlp/ repositories, visual knowledge extraction components are in https://hub.docker.com/u/dannapierskitoptal 5 Video: http://blender.cs.illinois.edu"
2020.acl-demos.11,N18-2002,0,0.0505647,"Missing"
2020.acl-demos.11,W03-0419,0,0.625358,"Missing"
2020.acl-demos.11,D18-1125,1,0.868466,"Missing"
2020.acl-demos.11,W15-0812,0,0.139288,"Missing"
2020.acl-demos.11,P19-1191,1,0.841816,"Missing"
2020.acl-demos.11,N18-5009,1,0.887178,"Missing"
2020.dmr-1.7,W13-2322,1,0.826233,"bject matter experts working in the space of UV inactivation of viruses. We introduce InfoForager, a proof-of-concept prototype tool that utilizes semantic understanding and search, going beyond the words in a user question to focus on its meaning. By employing a semantic search, we hypothesize that the user will more easily search through medical documents because they do not need to rephrase their questions (for example, into keywords) to conform to the system’s search limitations and capabilities. InfoForager first parses a user research question into Abstract Meaning Representation (AMR) (Banarescu et al., 2013), then compares the resulting AMR query to a collection of medical research papers already parsed into AMR. All AMR query-sentence pairs in each paper are scored for their semantic similarity, and InfoForager returns the highest-ranking answer sentence and the source document. Given the urgent nature of this research, we allotted six weeks during which four NLP researchers worked with two UV inactivation researchers to perform a shallow pass through the semantic searching problem space. Additionally, we worked with test users to obtain an understanding of the system requirements This work is l"
2020.dmr-1.7,W19-0124,1,0.827594,"sentence is returned with its source document. 3.1 Semantic Parse Using AMR AMR is a directed, acyclic graph (DAG) representation of the meaning of a sentence, in which nodes map to words in the sentence, and edges map to the relations between them. Figure 2 shows the AMR in the text-based Penman notation (a) and the graph notation (b). There is a relatively large and active body of research surrounding AMR, such that there are a variety of parsers for automatically converting natural language text into AMR, including our own work to retrain and adapt various AMR parsers for dialogue systems (Bonial et al., 2019; Bonial et al., 2020). AMR has demonstrated value in biomedical NLP applications in the past (see Section 6), so we elected to explore the use of AMR in the development of a research framework that has the potential to match not only the concepts within a research AMR query, but also the relations between those concepts for more efficient “semantic search.” To obtain AMR parses of our user questions and the medical research document collection, we leverage the parser from Lindemann et al. (2019) based on its high performance after retraining within a new domain in previous research. The parse"
2020.dmr-1.7,2020.lrec-1.86,1,0.837111,"with its source document. 3.1 Semantic Parse Using AMR AMR is a directed, acyclic graph (DAG) representation of the meaning of a sentence, in which nodes map to words in the sentence, and edges map to the relations between them. Figure 2 shows the AMR in the text-based Penman notation (a) and the graph notation (b). There is a relatively large and active body of research surrounding AMR, such that there are a variety of parsers for automatically converting natural language text into AMR, including our own work to retrain and adapt various AMR parsers for dialogue systems (Bonial et al., 2019; Bonial et al., 2020). AMR has demonstrated value in biomedical NLP applications in the past (see Section 6), so we elected to explore the use of AMR in the development of a research framework that has the potential to match not only the concepts within a research AMR query, but also the relations between those concepts for more efficient “semantic search.” To obtain AMR parses of our user questions and the medical research document collection, we leverage the parser from Lindemann et al. (2019) based on its high performance after retraining within a new domain in previous research. The parser was retrained on the"
2020.dmr-1.7,P13-2131,0,0.222367,"Missing"
2020.dmr-1.7,N12-1017,0,0.0306258,"Missing"
2020.dmr-1.7,2020.emnlp-main.49,0,0.025044,"hs, the authors transform the MRC task to a graph containment problem. The authors use a max-margin approach for subgraph matching, reasoning that the hypothesis graph aligns to the passage graph where the question is best answered, not unlike the intuition in InfoForager of applying Smatch to find the best alignment of triples from the query AMR DAG to triples from sentence AMR DAGs in a searched document. The authors obtain competitive results and point out that this approach uniquely allows them to combine evidence from multiple sentences. Although they do not use the AMR formalism per se, Du and Cardie (2020) achieve state-ofthe-art performance on the Automatic Content Extraction (ACE) 2005 benchmark event extraction task by making use of template-based questions within a BERT-based question-answering model, suggesting to us that an extension to AMR-based question templates might further improve their approach. The authors experimented with several template strategies for forming the questions targeting the event and its participants. Most relevant to our work, the authors found that the success of their approach was strongly influenced by different questioning strategies, ranging from posing a si"
2020.dmr-1.7,P14-1134,0,0.0245468,"ed model that identifies such an event subgraph given an AMR. While the method shows promising results, the authors find that improvements in AMR parsing are needed for further improvement on the task. Wang et al. (2017) were the first to make use of AMR embeddings, in this case along with word and dependency embeddings, to mine for otherwise hidden reports of drug-drug interactions (DDI) in the textual biomedical literature. The best performance the authors report was obtained by combining these three types of embeddings. They also noted that AMR embeddings alone, leveraging the JAMR parser (Flanigan et al., 2014), did not perform adequately for this DDI task, and, like others, attributed this to poor parser performance due to limited medical terms and documents in its training dataset. Other research that leverages AMR in tasks similar to ours has focused on question-answering. Mitra and Baral (2016) use AMR as an intermediate representation for question-answering tasks designed to test an agent’s understanding. The authors find that the addition of a formal reasoning layer significantly 74 increases the reasoning capability of an agent, and that AMR serves as an effective pivot from natural language"
2020.dmr-1.7,P19-1450,0,0.0139044,"e text into AMR, including our own work to retrain and adapt various AMR parsers for dialogue systems (Bonial et al., 2019; Bonial et al., 2020). AMR has demonstrated value in biomedical NLP applications in the past (see Section 6), so we elected to explore the use of AMR in the development of a research framework that has the potential to match not only the concepts within a research AMR query, but also the relations between those concepts for more efficient “semantic search.” To obtain AMR parses of our user questions and the medical research document collection, we leverage the parser from Lindemann et al. (2019) based on its high performance after retraining within a new domain in previous research. The parser was retrained on the Linguistic Data Consortium’s AMR 3.0 corpora (LDC2020T02) and the freely available Bio-AMR corpus,4 as well as our own manually annotated dataset of approximately 1,000 AMRs drawn from the Dial-AMR corpus (Bonial et al., 2020). 4 https://amr.isi.edu/download/2018-01-25/amr-release-bio-v3.0.txt 69 (a) (c / concentrate-02 :ARG1 (v / virus) :location (s / saliva) :quant (a / amr-unknown)) (a) AMR in Penman notation (b) AMR in directed, a-cyclic graph notation Figure 2: The utt"
2020.dmr-1.7,2020.tacl-1.34,0,0.0114004,"ts that were not relevant. As a keyword search system, PubMed did not offer explicit indicators for where or why a match occurred. Overall, participants spent more time, in comparison to InfoForager, in reading through the entire returned abstract to determine if it was relevant, or trying different permutations of keywords to see if they could find the right combination to get their answer. Thus, the user studies established motivating evidence for a prototype tool with semantic search, described next. 7 8 Several new AMR measures now also exist (Anchieta et al., 2019; Song and Gildea, 2019; Opitz et al., 2020). For some documents, users may be able click an additional link to access the full text. 71 4.2 Proof-of-Concept Prototype After running user studies, we put the anticipated components together into an end-to-end, proof-ofconcept system that we could begin to formally evaluate. Prior to any real-time user interaction, the medical document collection of papers are preprocessed, including conversion from PDF to text using the Poppler PDF9 rendering library, and word tokenization and sentence segmentation using a toolkit developed internally within our team. The documents in our collection had v"
2020.dmr-1.7,W17-2315,0,0.177095,"Missing"
2020.dmr-1.7,P16-2079,0,0.0517153,"ataset. Other research that leverages AMR in tasks similar to ours has focused on question-answering. Mitra and Baral (2016) use AMR as an intermediate representation for question-answering tasks designed to test an agent’s understanding. The authors find that the addition of a formal reasoning layer significantly 74 increases the reasoning capability of an agent, and that AMR serves as an effective pivot from natural language to the Answer Set Programming language used for reasoning and inductive logic. AMR is leveraged for a machine reading comprehension (MRC) task for question-answering in Sachan and Xing (2016). Here, the authors first convert a Q-A pair into a single AMR “hypothesis” graph. Additionally, the authors create an AMR for an entire passage, as opposed to the standard single-sentence AMR, by combining at coreference points. With these two AMR graphs, the authors transform the MRC task to a graph containment problem. The authors use a max-margin approach for subgraph matching, reasoning that the hypothesis graph aligns to the passage graph where the question is best answered, not unlike the intuition in InfoForager of applying Smatch to find the best alignment of triples from the query AM"
2020.dmr-1.7,P19-1446,0,0.0152846,"imes, to too many results that were not relevant. As a keyword search system, PubMed did not offer explicit indicators for where or why a match occurred. Overall, participants spent more time, in comparison to InfoForager, in reading through the entire returned abstract to determine if it was relevant, or trying different permutations of keywords to see if they could find the right combination to get their answer. Thus, the user studies established motivating evidence for a prototype tool with semantic search, described next. 7 8 Several new AMR measures now also exist (Anchieta et al., 2019; Song and Gildea, 2019; Opitz et al., 2020). For some documents, users may be able click an additional link to access the full text. 71 4.2 Proof-of-Concept Prototype After running user studies, we put the anticipated components together into an end-to-end, proof-ofconcept system that we could begin to formally evaluate. Prior to any real-time user interaction, the medical document collection of papers are preprocessed, including conversion from PDF to text using the Poppler PDF9 rendering library, and word tokenization and sentence segmentation using a toolkit developed internally within our team. The documents in"
2020.emnlp-main.50,D13-1185,1,0.75481,"Missing"
2020.emnlp-main.50,P08-1090,1,0.872582,"Missing"
2020.emnlp-main.50,P09-1068,1,0.7437,"tory provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among event"
2020.emnlp-main.50,chambers-jurafsky-2010-database,1,0.801355,"Missing"
2020.emnlp-main.50,N13-1104,0,0.133398,"Missing"
2020.emnlp-main.50,N19-1423,0,0.0311453,"Missing"
2020.emnlp-main.50,N15-1165,0,0.026451,"Missing"
2020.emnlp-main.50,W16-1701,1,0.907272,"Missing"
2020.emnlp-main.50,P16-1025,1,0.902933,"Missing"
2020.emnlp-main.50,E12-1034,0,0.126036,"Missing"
2020.emnlp-main.50,W19-3311,0,0.0837118,"Missing"
2020.emnlp-main.50,Q18-1023,0,0.0569661,"Missing"
2020.emnlp-main.50,2020.acl-main.713,1,0.645499,"Missing"
2020.emnlp-main.50,N16-1098,1,0.812451,"ream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations."
2020.emnlp-main.50,W16-1007,1,0.880849,"ream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations."
2020.emnlp-main.50,P15-1019,0,0.145444,"Missing"
2020.emnlp-main.50,S19-1012,0,0.0429448,"ent schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations. Temporal relations exist between almost all events, even those that are not semantically related; while research in identifying causal relations has been hobbled by low inter-annotator agreement (Hong et al., 2016). In this paper, we hypothesize that two events are connected when their entity arguments are coreferential or semantically related. For example, in Figure 1, (a) and (b)"
2020.emnlp-main.50,P17-1178,1,0.846469,"ument roles. We follow our recent work on ACE IE (Lin et al., 2020) to split the data. We consider the training set as historical data to train the LM, and the test set as our target data to induce schema for target scenarios. The instance graphs of the target data set are constructed from manual annotations. For historical data, we construct event instance graphs from both manual annotations (Historicalann ) and system extraction results (Historicalsys ) from the state-ofthe-art IE model (Lin et al., 2020). We perform cross-document entity coreference resolution by applying an entity linker (Pan et al., 2017) for both annotated and system generated instance graphs. Table 2 shows the data statistics. Split Historicalann Historicalsys Validation Target The cardinality for an instance graph and a schema will be the number of substructures in each, i.e., X |g|I = count(hvm , emn , vn i), hvm ,emn ,vn i∈g |s|S = 47,525 48,664 3,422 3,673 7,152 7,018 728 802 4,419 4,426 468 424 By extension, each path of length l=5 in a graph schema [φi , ψij , φj , ψjk , φk ] contains two consecutive triples hφi , ψij ,φj i, hφj , ψjk , φk i∈s, and a matched instance path contains two consecutive instance triples hvm ,"
2020.emnlp-main.50,K19-1051,0,0.111184,"Missing"
2020.emnlp-main.50,N18-1202,0,0.129853,"event but fails to extract the I NVESTIGATE C RIME triggered by “discovered” and its D EFENDANT argument “Mohammed A. Salameh”. Event graph scehmas can inform the model that a person who is arrested was usually investigated, our IE system can fix this missing error. Therefore we also conduct extrinsic evaluations and show the effectiveness of the induced schema repository in enhancing downstream end-to-end IE tasks. PART- WHOLE PLACE−1 −−−−−−−→ GPE −−−−−→ ATTACK. We train the path language model on two tasks: learning an auto-regressive language model (Ponte and Croft, 1998; Dai and Le, 2015; Peters et al., 2018; Radford et al.; Yang et al., 2019) to predict an edge or a node, given previous edges and nodes in a path, and a neighboring path classification task to predict how likely two paths co-occur. The path language model is trained from all the paths between two event instances from the same document, based on the assumption that events from the same document (especially news document) tell a coherent story. We propose two intrinsic evaluation metrics, instance coverage and instance coherence, to assess when event instance graphs are covered by each 685 In summary, we make the following novel con"
2020.emnlp-main.50,E14-1024,0,0.100189,"Missing"
2020.emnlp-main.50,D15-1195,0,0.203725,"Missing"
2020.emnlp-main.50,N16-1049,0,0.151836,"Missing"
2020.emnlp-main.50,W17-0901,0,0.0336757,"Missing"
2020.emnlp-main.50,W19-3404,0,0.0428587,"Missing"
2020.lrec-1.243,P15-1017,0,0.0888802,"for the nodes (words). We train a BiAffine Dependency Parser (Dozat and Manning, 2016) for a particular language using the Universal Dependency treebanks (Nivre et al., 2016), and then apply the dependency parser to sentences to obtain universal dependency trees. For fully connected graphs, we regard each token in a sentence as a node in the graph and there’s an edge between each pair of nodes. Then we apply Tree-LSTM encoder and Transformer encoder to generate word representations in the latent space, respectively. Tree-LSTM Encoder. We exploit the Child-Sum TreeLSTMs proposed by Tai et al. (2015). In contrast to the standard LSTM, here the memory cell updates of the TreeLSTM unit are dependent on the states of all children units. The Tree-LSTM unit selectively incorporates information from each child. Transformer Encoder. Our multi-layer bidirectional Transformer encoder is based on the architecture proposed in Vaswani et al. (2017), composed of a stack of N identical layers, where each layer has a multi-head self-attention sub-layer and a position-wise feed-forward sub-layer. Our implementation is identical to the original, except that here, crucially, we do not include positional en"
2020.lrec-1.243,doddington-etal-2004-automatic,0,0.27252,"Missing"
2020.lrec-1.243,L18-1245,0,0.141208,"n Extraction (IE) that aims to identify event triggers and arguments from unstructured texts and classify them into predefined categories. Compared to other IE tasks such as name tagging, the annotations for Event Extraction are more costly because they are structured and require a rich label space; full event structure annotation consists of its trigger span and type label as well as each of its one or more argument spans and role labels. Publicly available annotations for event extraction exist for only a few languages, such as English, Spanish, Chinese, and Arabic (Doddington et al., 2004; Getman et al., 2018). We propose a novel shareand-transfer framework to project training data for English only and test data for zero-event-resource languages into one common semantic space, so that we can train an event extractor on English annotations and apply it to target languages. Currently most successful cross-lingual transfer approaches for IE are limited to sequence labeling (Feng et al., 2018a; Xie et al., 2018; Zhang et al., 2018a; Lin et al., 2018). In contrast, event extraction requires transferring complex graph structures that contain triggers and arguments. For example, in Figure 1, the words fir"
2020.lrec-1.243,C16-1114,0,0.538714,"l., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et al. (2016) have used language-independent features for event extraction for low-resource languages. (Lu and Nguyen, 2018) show that word sense disambiguation helps event detection via neural representation matching. (Liu et al., 2018a; Zhang et al., 2018b) propose event extraction by attention mechanism, e.g. the former use a gated multi-lingual attention technique. To the best of our knowledge, this is the first work to design a cross-lingual structure transfer framework to enable event extraction for a language without any event training data. 5. Conclusions and Future Work In this paper, we propose a"
2020.lrec-1.243,P08-1030,1,0.911889,"sonnel Business Manufacture 2,613 346 539 453 2,411 301 53 52 1,956 207 116 86 1,688 275 651 225 1,340 146 58 43 279 0 14 8 158 58 9 5 Table 2: Distribution of event types in various datasets (Number of event mentions). The statistics for English are from the training split, and the statistics for Spanish, Russian and Ukrainian are from testing splits. Hyperparameter Value word embedding size 300 hidden dimension size 768 filter size 768 number of head 12 number of layer 12 dropout 0.2 learning rate 0.003 batch size 16 of event types for each language. We follow the criteria in previous work (Ji and Grishman, 2008; Li et al., 2013) for evaluation. 3.2. Training Details Treebanks. We use the Version 2.3 treebanks released by Universal Dependencies 3 to train the dependency parsers. Tokenization. We use Spacy tokenization (Honnibal and Montani, 2017) for English and Spanish and the NLTK toktok tokenizer (Dehdari, 2014) for Russian and Ukrainian. Word Embedding. We use multilingual word embeddings released by Facebook Research (Lample et al., 2017) 4 . The algorithm aligns word embeddings of various languages, which are pre-trained from Wikipedia articles (Joulin et al., 2016) 5 , in a single vector space"
2020.lrec-1.243,D12-1092,0,0.0290845,"large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et al. (2016) have used language-independent features for event extraction for low-resource languages. (Lu and Nguyen, 2018) show that word sense disambiguation helps event detection via neural representation matching. (Liu et al., 2018a; Zhang et al., 2018b) propose"
2020.lrec-1.243,P13-1008,1,0.885221,"ture 2,613 346 539 453 2,411 301 53 52 1,956 207 116 86 1,688 275 651 225 1,340 146 58 43 279 0 14 8 158 58 9 5 Table 2: Distribution of event types in various datasets (Number of event mentions). The statistics for English are from the training split, and the statistics for Spanish, Russian and Ukrainian are from testing splits. Hyperparameter Value word embedding size 300 hidden dimension size 768 filter size 768 number of head 12 number of layer 12 dropout 0.2 learning rate 0.003 batch size 16 of event types for each language. We follow the criteria in previous work (Ji and Grishman, 2008; Li et al., 2013) for evaluation. 3.2. Training Details Treebanks. We use the Version 2.3 treebanks released by Universal Dependencies 3 to train the dependency parsers. Tokenization. We use Spacy tokenization (Honnibal and Montani, 2017) for English and Spanish and the NLTK toktok tokenizer (Dehdari, 2014) for Russian and Ukrainian. Word Embedding. We use multilingual word embeddings released by Facebook Research (Lample et al., 2017) 4 . The algorithm aligns word embeddings of various languages, which are pre-trained from Wikipedia articles (Joulin et al., 2016) 5 , in a single vector space. It learns a mapp"
2020.lrec-1.243,D14-1198,1,0.831152,"when the model generates the representation for the word “ранение (wound)”, “спину (back)” also contributes besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al"
2020.lrec-1.243,R11-1002,0,0.0219009,"visualized attention weights, we can clearly see when the model generates the representation for the word “ранение (wound)”, “спину (back)” also contributes besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recen"
2020.lrec-1.243,P18-1074,1,0.785986,"icly available annotations for event extraction exist for only a few languages, such as English, Spanish, Chinese, and Arabic (Doddington et al., 2004; Getman et al., 2018). We propose a novel shareand-transfer framework to project training data for English only and test data for zero-event-resource languages into one common semantic space, so that we can train an event extractor on English annotations and apply it to target languages. Currently most successful cross-lingual transfer approaches for IE are limited to sequence labeling (Feng et al., 2018a; Xie et al., 2018; Zhang et al., 2018a; Lin et al., 2018). In contrast, event extraction requires transferring complex graph structures that contain triggers and arguments. For example, in Figure 1, the words fire/ fired combined with different arguments indicate different event types. A transfer approach to IE with a typical sequence-based Long Short Term Memory (LSTM) encoder will incorporate languagespecific characteristics, such as word order, into word representations, reducing its effectiveness in transfer between two languages with quite different word orders. In this paper, we explore cross-lingual event transfer learning in a zero-resource"
2020.lrec-1.243,D18-1156,0,0.0860644,"nt of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et al. (2016) have used language-independent features for event extr"
2020.lrec-1.243,D18-1517,0,0.0123448,"guagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et al. (2016) have used language-independent features for event extraction for low-resource languages. (Lu and Nguyen, 2018) show that word sense disambiguation helps event detection via neural representation matching. (Liu et al., 2018a; Zhang et al., 2018b) propose event extraction by attention mechanism, e.g. the former use a gated multi-lingual attention technique. To the best of our knowledge, this is the first work to design a cross-lingual structure transfer framework to enable event extraction for a language without any event training data. 5. Conclusions and Future Work In this paper, we propose a novel cross-lingual structure transfer framework for zero-resource event extraction. Experiments on three lang"
2020.lrec-1.243,P15-2060,0,0.107531,"пину (back)” also contributes besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extract"
2020.lrec-1.243,N16-1034,0,0.0723032,"es besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018b) demonstrate methods for building multilingual event extraction systems. Hsi et a"
2020.lrec-1.243,L16-1262,0,0.0919272,"Missing"
2020.lrec-1.243,P15-1150,0,0.122428,"Missing"
2020.lrec-1.243,P17-2046,0,0.0487613,"Missing"
2020.lrec-1.243,D18-1034,0,0.0245405,"re argument spans and role labels. Publicly available annotations for event extraction exist for only a few languages, such as English, Spanish, Chinese, and Arabic (Doddington et al., 2004; Getman et al., 2018). We propose a novel shareand-transfer framework to project training data for English only and test data for zero-event-resource languages into one common semantic space, so that we can train an event extractor on English annotations and apply it to target languages. Currently most successful cross-lingual transfer approaches for IE are limited to sequence labeling (Feng et al., 2018a; Xie et al., 2018; Zhang et al., 2018a; Lin et al., 2018). In contrast, event extraction requires transferring complex graph structures that contain triggers and arguments. For example, in Figure 1, the words fire/ fired combined with different arguments indicate different event types. A transfer approach to IE with a typical sequence-based Long Short Term Memory (LSTM) encoder will incorporate languagespecific characteristics, such as word order, into word representations, reducing its effectiveness in transfer between two languages with quite different word orders. In this paper, we explore cross-lingual eve"
2020.lrec-1.243,N16-1033,0,0.0142582,"nerates the representation for the word “ранение (wound)”, “спину (back)” also contributes besides the word “ранен” itself. And “спину (back)” is an argument of the event mention triggered by “ранение (wound)” here. It indicates that the model successfully transfers structural information from the source language to the target language. 4. Related Work A large number of supervised machine learning techniques have been used for event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b). These approaches incorporate languagespecific information, and thus require a substantial amount of annotations when adapted to a new language. Traditional multilingual approaches (Li et al., 2012; Wei et Figure 4: Visualization of Attention Weights for the First head of the Multi-head Attention Sublayer. al., 2017) to event extraction were all based on feature engineering. Recently, (Agerri et al., 2016; Danilova et al., 2014; Feng et al., 2018"
2020.lrec-1.243,N18-5009,1,0.761275,"and role labels. Publicly available annotations for event extraction exist for only a few languages, such as English, Spanish, Chinese, and Arabic (Doddington et al., 2004; Getman et al., 2018). We propose a novel shareand-transfer framework to project training data for English only and test data for zero-event-resource languages into one common semantic space, so that we can train an event extractor on English annotations and apply it to target languages. Currently most successful cross-lingual transfer approaches for IE are limited to sequence labeling (Feng et al., 2018a; Xie et al., 2018; Zhang et al., 2018a; Lin et al., 2018). In contrast, event extraction requires transferring complex graph structures that contain triggers and arguments. For example, in Figure 1, the words fire/ fired combined with different arguments indicate different event types. A transfer approach to IE with a typical sequence-based Long Short Term Memory (LSTM) encoder will incorporate languagespecific characteristics, such as word order, into word representations, reducing its effectiveness in transfer between two languages with quite different word orders. In this paper, we explore cross-lingual event transfer learning"
2020.lrec-1.86,2020.scil-1.31,1,0.786792,"l. and Lindemann et al. parsers to obtain the standard AMR for manual corrections, as each correctly captured several of the extremely frequent aspects of the corpus, including the mode :imperative marker. 4.2.2. Graph-to-Graph Transformation for Dialogue-AMR In order to automatically generate Dialogue-AMRs with the tense, aspect, and illocutionary force information critical to the navigation domain, we developed a graph-tograph transformation system that converts standard AMRs into our Dialogue-AMRs through a mixed-methods approach that leverages both rule-based and classifier-based systems (Abrams et al., 2020). Both the standard AMR and original natural language utterance are required as input to the graph-to-graph transformer. From the utterance, the speech act and tense are determined by employing classifiers. From the standard AMR, the relations (e.g., go-02, turn-01) corresponding to robot concepts are determined by matching the standard AMR root relation against a dictionary of keywords associated with a particular robot concept (see Table 2). Next, the aspectual information is extracted based upon speech act and tense patterns (e.g., present-tense assertions are complete ongoing +). Finally,"
2020.lrec-1.86,W13-2322,1,0.888934,"nly the content of an utterance, but the illocutionary force behind it, as well as tense and aspect. To showcase the coverage of the schema, we use both manual and automatic methods to construct the “DialAMR” corpus—a corpus of human-robot dialogue annotated with standard AMR and our enriched Dialogue-AMR schema. Our automated methods can be used to incorporate AMR into a larger NLU pipeline supporting human-robot dialogue. Keywords: Dialogue, Abstract Meaning Representation, Illocutionary Force 1. Introduction This paper describes a schema that enriches Abstract Meaning Representation (AMR) (Banarescu et al., 2013) to support Natural Language Understanding (NLU) in humanrobot dialogue systems. AMR is a formalism for sentence semantics that abstracts away many syntactic idiosyncrasies and represents sentences with rooted directed acyclic graphs (Figure 1a shows the PENMAN notation of the graph). Although AMR provides a suitable level of abstraction for representing the content of sentences in our domain, it lacks a level of representation for speaker intent, which would capture the pragmatic effect of an utterance in dialogue. Pragmatic information is critical in dialogue with a conversational agent. For"
2020.lrec-1.86,bastianelli-etal-2014-huric,0,0.0191406,"taxonomies often have to be fine-tuned to the domain of interest to be fully useful. While we adopt many of the categories of Searle’s taxonomy for our own speech act inventory, we integrate distinctions from the ISO standard and, following Traum (1999) and Poesio and Traum (1998), define our speech acts according to the effects of an utterance relating to the beliefs and obligations of the interlocutors (see Section 3.1). Our work forms part of a larger, growing interest in representing various levels of interpretation in existing meaning representation frameworks, and in AMR in particular. Bastianelli et al. (2014) present their Human Robot Interaction Corpus (HuRIC) following the format of AMR. This corpus is comprised of paired audio interactions and transcriptions. Though all text is annotated in the format of AMR, AMR is significantly altered by incorporating detailed spatial relations, frame semantics (Fillmore, 1985), and morphosyntactic information. Shen (2018) further presents a small corpus of manually annotated AMRs for spoken language to help the parsing task. The study presents similar findings to our own: while AMR offers a clean framework for the concepts and relations used in spoken langu"
2020.lrec-1.86,W19-3322,1,0.813505,"d and robust schema for representing illocutionary force in AMR called “Dialogue-AMR” (Figure 1b). This expands and refines previous work which proposed basic modifications for (a) (d / drive-01 :mode imperative :ARG0 (y / you) :destination (d2 / door)) (b) (c / command-SA :ARG0 (c2 / commander) :ARG2 (r / robot) :ARG1 (g / go-02 :completable + :ARG0 r :ARG3 (h / here) :ARG4 (d/ door) :time (a2 / after :op1 (n / now)))) Figure 1: The utterance Drive to the door represented in (a) standard AMR form, (b) Dialogue-AMR form. how to annotate speech acts and tense and aspect information within AMR (Bonial et al., 2019a). The contributions of the present research are: i) a set of speech acts finalized and situated in a taxonomy (Section 3.1); ii) the refinement of the Dialogue-AMR annotation schema to provide coverage of novel language (Sections 3.2 and 3.3); and iii) the creation of the “DialAMR” corpus, a collection of human-robot dialogues to which the new Dialogue-AMR schema has been applied (Section 4).1 DialAMR has additionally been annotated with standard AMR, thus constituting one of the first corpora of dialogue annotated with AMR (see related work in Section 5) and allowing for comparison of both"
2020.lrec-1.86,W19-0124,1,0.862276,"d and robust schema for representing illocutionary force in AMR called “Dialogue-AMR” (Figure 1b). This expands and refines previous work which proposed basic modifications for (a) (d / drive-01 :mode imperative :ARG0 (y / you) :destination (d2 / door)) (b) (c / command-SA :ARG0 (c2 / commander) :ARG2 (r / robot) :ARG1 (g / go-02 :completable + :ARG0 r :ARG3 (h / here) :ARG4 (d/ door) :time (a2 / after :op1 (n / now)))) Figure 1: The utterance Drive to the door represented in (a) standard AMR form, (b) Dialogue-AMR form. how to annotate speech acts and tense and aspect information within AMR (Bonial et al., 2019a). The contributions of the present research are: i) a set of speech acts finalized and situated in a taxonomy (Section 3.1); ii) the refinement of the Dialogue-AMR annotation schema to provide coverage of novel language (Sections 3.2 and 3.3); and iii) the creation of the “DialAMR” corpus, a collection of human-robot dialogues to which the new Dialogue-AMR schema has been applied (Section 4).1 DialAMR has additionally been annotated with standard AMR, thus constituting one of the first corpora of dialogue annotated with AMR (see related work in Section 5) and allowing for comparison of both"
2020.lrec-1.86,T75-2014,0,0.329028,"ialogue, an interlocutor must interpret the meaning of a speaker’s utterance on at least two levels, as first suggested by Austin (1962): (i) its propositional content and (ii) its illocutionary force. While semantic representations have traditionally sought to represent propositional content, speech act theory has sought to delineate and explicate the relationship between an utter690 ance and its effects on the mental and interactional states of the conversational participants. Speech acts have been used as part of the meaning representation of task-oriented dialogue systems since the 1970s (Bruce, 1975; Cohen and Perrault, 1979; Allen and Perrault, 1980). For a summary of some of the earlier work in this area, see Traum (1999). Although the refinement and extension of Austin’s (1962) hypothesized speech acts by Searle (1969) remains a canonical work on this topic, there have since been a number of widely used speech act taxonomies that differ from or augment this work, including an ISO standard (Bunt et al., 2012). Nevertheless, these taxonomies often have to be fine-tuned to the domain of interest to be fully useful. While we adopt many of the categories of Searle’s taxonomy for our own sp"
2020.lrec-1.86,bunt-etal-2012-iso,1,0.899784,"ng those same concepts. Thus, the AMR formalism smooths away many syntactic and lexical features that are unimportant to the robot. Existing AMR parsers can be utilized to obtain an initial interpretation of a user utterance, making the interpretation process easier than parsing natural language Development of Dialogue-AMR Speech Act Inventory We embrace much of the higher-level categorization and labeling of speech acts outlined by Searle (1969), including the basic categories of Assertions (termed “representatives” by Searle), Commissives, Directives, and Expressives. Additionally, based on Bunt et al. (2012), we introduce an early distinction in classifying our speech acts between Information Transfer Functions and Action-Discussion Functions (see Figure 3). In terms of dialogue function, this division allows us to monitor the status of distinct dialogue contexts. For Information Transfer Types, we can monitor the quantity and quality of general-purpose information exchanged in the dialogue that is relevant to the larger task at hand. For example, Robot, do you speak any foreign languages? may not directly impact a current task, but it introduces information into the dialogue that may be useful a"
2020.lrec-1.86,P13-2131,0,0.260444,"Missing"
2020.lrec-1.86,N15-1119,0,0.0330163,"use a lexicon (shared with PropBank (Palmer et al., 2005) comprised of numbered senses of a relation, each of which lists a set of numbered participant roles (Arg0-5). For ease of creation and manipulation, annotators work with notation from the PENMAN project (Penman Natural Language Group, 1989), which is the notation used in this paper (e.g., Figure 1a). AMR has been used to support NLU, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), as well as machine translation (Langkilde and Knight, 1998), question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). text directly into a robot-oriented representation. Standard AMR nevertheless omits certain semantic information essential to our domain. Specifically, AMR omits both tense and aspect information, assuming that some of this information may be gleaned from morphosyntactic information already well-represented in syntactic treebanks. The formalism also lacks illocutionary force, considering it distinct from core contentful meaning. We therefore add these properties to the robot’s semantic representation (Sectio"
2020.lrec-1.86,W16-6603,0,0.0140873,"r graph nodes) are introduced for entities, events, properties, and states. Leaves are labeled with concepts (e.g., (r / robot)). Relational concepts in AMR use a lexicon (shared with PropBank (Palmer et al., 2005) comprised of numbered senses of a relation, each of which lists a set of numbered participant roles (Arg0-5). For ease of creation and manipulation, annotators work with notation from the PENMAN project (Penman Natural Language Group, 1989), which is the notation used in this paper (e.g., Figure 1a). AMR has been used to support NLU, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), as well as machine translation (Langkilde and Knight, 1998), question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017). text directly into a robot-oriented representation. Standard AMR nevertheless omits certain semantic information essential to our domain. Specifically, AMR omits both tense and aspect information, assuming that some of this information may be gleaned from morphosyntactic information already well-represented in syntactic treebanks. The formalism also lacks illocut"
2020.lrec-1.86,W17-2315,0,0.484277,"Missing"
2020.lrec-1.86,L18-1017,1,0.835141,"by the DM. SCOUT also includes annotations of dialogue structure 685 Left Conversational Floor # Participant 1 2 3 4 5 6 7 8 9 proceed to the doorway ahead Right Conversational Floor DM → Participant DM → RN RN I see more than one doorway. Which doorway? the doorway closest to you processing move into Kitchen moving... done done Table 1: Navigation instruction initiated by the participant (#1), its clarification (#2-4), subsequent translation to a simplified form (Dialogue Manager (DM) to Robot Navigator (RN), #6), and acknowledgement of instructions (#5, 7, 9) and execution by the RN (#8). (Traum et al., 2018) that allow for the characterization of distinct information states (Traum and Larsson, 2003). However, this dialogue structure annotation schema does not provide a markup of the semantic content in participant instructions. 2.2. AMR AMR is a formalism for sentence semantics that abstracts away from some syntactic idiosyncrasies (Banarescu et al., 2013). Each sentence is represented by a rooted directed acyclic graph (DAG) in which variables (or graph nodes) are introduced for entities, events, properties, and states. Leaves are labeled with concepts (e.g., (r / robot)). Relational concepts in"
2020.lrec-1.86,N15-1040,0,0.0218702,"lease data, which, as mentioned previously, does not include natural dialogue, nor does it include much instruction-giving or commands. Nonetheless, we applied parsers to the SCOUT corpus to determine which could achieve the best performance with the least manually annotated in-domain training data. These experiments are ongoing, and full results will be reported in a future paper. Here, we limit our description to what is relevant for the automatic annotation pass used to efficiently create the DialAMR corpus. First, we tested two long-standing parsers, JAMR (Flanigan et al., 2014) and CAMR (Wang et al., 2015), on the Random-Commander set of gold-standard, manually annotated standard AMRs. Performance was far below reported f-scores on LDC AMR test data (Bonial et al., 2019b). Particularly problematic areas included missing mode :imperative markers on all imperative utterances, failure to include implicit subjects (e.g., the Arg0-mover in utterances such as Moving...), and failure to correctly represent the photographing semantics of the common light verb construction take a photo/picture (instead representing this as a taking event in the sense of grasping/moving). Next, we evaluated more recent s"
2020.lrec-1.86,W00-0309,0,0.46237,"guage generation, and robot concept specification. The DialogueAMR relations classify speaker intention, while the argument roles allow for flexible representation of previously unseen values (e.g., Turn left 100 degrees compared to a more typical number of degrees, such as 90) and compositional construction of referring expressions. Furthermore, the completable annotation attached to goal-oriented Dialogue-AMRs allow a dialogue management system to determine if all the arguments required for execution of the instruction are present, and, if not, the system can follow up with a clarification (Xu and Rudnicky, 2000). This structured approach is expected to be less brittle than the statistical similarity and retrieval model implemented in Lukin et al.’s (2018) NLU component in this human-robot dialogue domain, which has difficulty generalizing to novel, unseen commands. We expect promising results from integrating DialogueAMR into our human-robot dialogue architecture. Furthermore, our annotation schema and corpus will contribute to a growing set of resources supporting meaning representation that goes beyond propositional content to model speaker intention in the conversational context. Acknowledgments W"
2021.emnlp-main.422,D13-1185,0,0.0147787,"on all evaluation tasks, since it ignores the coreferential arguments and their relations, but 6 Related Work relies solely on the overly simplistic temporal order to connect events. This is especially apparent from The definition of a complex event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al.,"
2021.emnlp-main.422,P08-1090,0,0.742122,"event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between par"
2021.emnlp-main.422,chambers-jurafsky-2010-database,0,0.0434858,"Missing"
2021.emnlp-main.422,N13-1104,0,0.0305441,"n tasks, since it ignores the coreferential arguments and their relations, but 6 Related Work relies solely on the overly simplistic temporal order to connect events. This is especially apparent from The definition of a complex event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et"
2021.emnlp-main.422,P16-1154,0,0.011558,"ained the new event ei is an A RREST event, so we add LDC Schema Learning Ontology. three argument nodes for D ETAINEE, JAILOR, and 5 Compared to (Liao et al., 2019), we do not use the posiP LACE respectively. The edges between these ar- tional embedding mask because the newly generated nodes guments and event ei are also added into the graph. have distinct roles. 5206 3.5 Coreferential Argument Generation After updating the node representations, we detect the entity type of each argument, and also predict whether the argument is coreferential to existing entities. Inspired by copy mechanism (Gu et al., 2016), we classify each argument node vj to either a new entity with entity type φ(vj ), or an existing entity node in the previous graph G&lt;i . For example, in Figure 2, the D ETAINEE should be classified to the existing ATTACKER node, while JAILOR node is classified as P ERSON. Namely, p(hei , aj , vj i|ei , aj ) ( p(hei , aj , vj i, g|ei , aj ) if vj is new, = p(hei , aj , vj i, c|ei , aj ) otherwise, where p(hei , aj , vj i, g|ei , aj ) is the generation probability, classifying the new node to its entity type φ(vj ):  p(hei , aj , vj i, g|ei , aj ) = exp(W φ(vj ) v j ) Z The copy probability p"
2021.emnlp-main.422,E12-1034,0,0.0677415,"Missing"
2021.emnlp-main.422,D19-6014,0,0.0172787,"lso overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced f"
2021.emnlp-main.422,2020.findings-emnlp.340,0,0.0275404,"nt. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-dr"
2021.emnlp-main.422,2021.naacl-main.274,1,0.713586,"complex event type, such as car-bombing, we construct a set of instance graphs, where each instance graph is about one complex event, such as Kabul ambulance bombing. We first identify a cluster of documents that describes the same complex event. In this paper, we treat all documents linked to a single Wikipedia page as belonging to the same complex event, detailed in §4.1. We use OneIE, a state-of-the-art Information Extraction system (Lin et al., 2020), to extract entities, relations and events, and then perform crossdocument entity (Pan et al., 2015, 2017) and event coreference resolution (Lai et al., 2021) over the document cluster of each complex event. We further conduct event-event temporal relation extraction (Ning et al., 2019; Wen et al., 2021b) to determine the order of event pairs. We run the entire 2 For simplification purposes, we mention “schema graphs” as “schemas”, and “events” in schemas are only “event types”. pipeline following (Wen et al., 2021a) 3 , and the detailed extraction performance is reported in the paper. After extraction, we construct one instance graph for each complex event, where coreferential events or entities are merged. We consider the isolated events as irrel"
2021.emnlp-main.422,2020.emnlp-main.50,1,0.80141,"tional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et a"
2021.emnlp-main.422,2021.naacl-main.69,1,0.668757,"ns between arguments, so we only compute this metric for the IED dataset. IED Schema Learning Corpus: The same type of complex events may have many variants, which depends on the different types of conditions and participants. In order to evaluate our model’s capability at capturing uncertainty and multiple hypotheses, we decided to dive deeper into one scenario and chose the improvised explosive device (IED) as our case study. We first collected Wikipedia articles that describe 4 types of complex events, i.e., Car-bombing IED, Drone Strikes IED, Suicide IED and General IED. Then we followed (Li et al., 2021) to exploit the external links to collect the additional news documents with the corresponding complex event type. The ground-truth schemas for this IED corpus are created manually, through a schema curation 4.3 Instance Graph Perplexity Evaluation tool (Mishra et al., 2021). Only one human schema graph was created for each complex event type, To evaluate our temporal event graph model, we resulting in 4 schemas. In detail, for each com- compute the instance graph perplexity by predictplex event type, we presented example instance ing the instance graphs in the test set, graphs and the ranked"
2021.emnlp-main.422,L16-1555,0,0.0194662,"l., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-driven schema induction approach, and choose to use IE systems instead of using manual annotation, to induce schemas that are robust and can tolerate extraction errors. modeling and generation of graphs ("
2021.emnlp-main.422,2020.acl-main.713,1,0.793931,"rded as a summary abstraction of instance graphs, capturing the reoccurring structures. 3 3.1 Our Approach Instance Graph Construction To induce schemas for a complex event type, such as car-bombing, we construct a set of instance graphs, where each instance graph is about one complex event, such as Kabul ambulance bombing. We first identify a cluster of documents that describes the same complex event. In this paper, we treat all documents linked to a single Wikipedia page as belonging to the same complex event, detailed in §4.1. We use OneIE, a state-of-the-art Information Extraction system (Lin et al., 2020), to extract entities, relations and events, and then perform crossdocument entity (Pan et al., 2015, 2017) and event coreference resolution (Lai et al., 2021) over the document cluster of each complex event. We further conduct event-event temporal relation extraction (Ning et al., 2019; Wen et al., 2021b) to determine the order of event pairs. We run the entire 2 For simplification purposes, we mention “schema graphs” as “schemas”, and “events” in schemas are only “event types”. pipeline following (Wen et al., 2021a) 3 , and the detailed extraction performance is reported in the paper. After"
2021.emnlp-main.422,K16-1008,0,0.0141284,"and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the"
2021.emnlp-main.422,W16-1007,0,0.0187693,"between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-driven schema induction approach, and choose to use IE systems instead of using manual annotation, to induce schemas that are robust and can tolerate extraction errors. modeling and generation of graphs (Li et al., 2018a; Jin et al., 2018; Grover et al."
2021.emnlp-main.422,I17-2007,0,0.0262425,"hat events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applic"
2021.emnlp-main.422,P15-1019,0,0.0538438,"Missing"
2021.emnlp-main.422,D19-1642,0,0.0347696,"Missing"
2021.emnlp-main.422,N15-1119,1,0.748927,"roach Instance Graph Construction To induce schemas for a complex event type, such as car-bombing, we construct a set of instance graphs, where each instance graph is about one complex event, such as Kabul ambulance bombing. We first identify a cluster of documents that describes the same complex event. In this paper, we treat all documents linked to a single Wikipedia page as belonging to the same complex event, detailed in §4.1. We use OneIE, a state-of-the-art Information Extraction system (Lin et al., 2020), to extract entities, relations and events, and then perform crossdocument entity (Pan et al., 2015, 2017) and event coreference resolution (Lai et al., 2021) over the document cluster of each complex event. We further conduct event-event temporal relation extraction (Ning et al., 2019; Wen et al., 2021b) to determine the order of event pairs. We run the entire 2 For simplification purposes, we mention “schema graphs” as “schemas”, and “events” in schemas are only “event types”. pipeline following (Wen et al., 2021a) 3 , and the detailed extraction performance is reported in the paper. After extraction, we construct one instance graph for each complex event, where coreferential events or en"
2021.emnlp-main.422,P17-1178,1,0.891563,"Missing"
2021.emnlp-main.422,L16-1556,0,0.0245624,"iders the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events (Nguyen et al., 2017; Hu et al., 2017; Li et al., 2018b; Kiyomaru et al., 2019; Lv et al., 2019), or predict a pre-condition event given the current events (Kwon et al., 2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events. Existing script annotations (Chambers and Jurafsky, 2008, 2010; Modi et al., 2016; Wanzare et al., 2016; Mostafazadeh et al., 2016a,b; Kwon et al., 2020) cannot support a comprehensive graph schema induction due to the missing of critical event graph structures, such as argument relations. Furthermore, in real-world applications, complex event schemas are expected to be induced from large-scale historical data, which is not feasible to annotate manually. We propose a data-driven schema induction approach, and choose to use IE systems instead of using manual annotation, to induce schemas that are robust and can tolerate extraction errors. modeling and generation of graphs (Li et al., 2018a; Jin"
2021.emnlp-main.422,P16-1028,0,0.0157009,"ations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal dependency and the multi-hop argument dependency across events. Recent work on event graph schema induction (Li et al., 2020) only considers the connections between a pair of two events. Similarly, their event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single o"
2021.emnlp-main.422,E14-1024,0,0.0227831,"duction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. However, we induce a comprehensive event graph schema, capturing both the temporal depen"
2021.emnlp-main.422,2020.emnlp-main.612,0,0.0293622,"Missing"
2021.emnlp-main.422,2021.naacl-demos.16,1,0.847555,"Missing"
2021.emnlp-main.422,D15-1195,0,0.0450596,"Missing"
2021.emnlp-main.422,N16-1049,0,0.0199393,"l arguments and their relations, but 6 Related Work relies solely on the overly simplistic temporal order to connect events. This is especially apparent from The definition of a complex event schema septhe instance graph perplexity in Table 3. arates us from related lines of work, namely Learning Corpus Size. An average of 113 in- schema induction and script learning. Previous stance graphs is used for each complex event type work on schema induction aims to characterize 5210 event triggers and participants of individual atomic events (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Yuan et al., 2018), ignoring inter-event relations. Work on script learning, on the other hand, originally limited attention to event chains with a single protagonist (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015; Jans et al., 2012; Granroth-Wilding and Clark, 2016) and later extended to multiple participants (Pichotta and Mooney, 2014, 2016; Weber et al., 2018). Recent efforts rely on distributed representations encoded from the compositional nature of events (Modi, 2016; Granroth-Wilding and Clark, 2016; Weber et al., 2018, 2020; Zhang et al., 2020), and language modeling (Rudi"
2021.emnlp-main.422,2021.naacl-main.6,1,0.815069,"Missing"
2021.emnlp-main.422,2020.emnlp-main.374,0,0.0992327,"Missing"
2021.mrqa-1.7,2021.acl-long.143,0,0.0220218,"ding agents to negotiate a shared interpretation and renegotiate that interpretation as each input is received. 9. Build B-Brain mechanisms for controlling processing by understanding agents. 10. Adapt and extend existing parsers and lexicons. 11. Evolve existing commonsense databases by adding knowledge as needed. 12. Build links between existing resources, allowing multiple resources to be used.” The neural approaches in this paper are missing all of these components. However, there is some evidence that deep neural language models may rediscover a similar NLP pipeline (Tenney et al., 2019; Li et al., 2021). The knowledge-base-centric approaches suggested by McCarthy and Mueller would guarantee that if the background knowledge, passage, and questions were encoded as logical statements correctly, the inference would be valid and the answers true. When using a language model to answer the questions, we have no such guarantees. Background We present views on what would be needed to address McCarthy’s challenge, an overview of the GPT-3, and prompt-based learning. 3.1 Early Views of NLU Architecture The paper from which the passage was taken contain’s McCarthy’s thoughts on how one might build a sys"
2021.mrqa-1.7,2020.acl-main.463,0,0.0178053,"to the grammatical sentence than the ungrammatical one (Marvin and Linzen, 2018; Newman et al., 2021). Though significant caution is needed in interpreting LM results, we see diagnostic value in constructing challenge datasets to evaluate systems’ passage understanding, as carefully constructed by Dua et al. (2019). The fact that a current state-of-the-art LM dropped more than 50 absolute F1 points on their dataset reinforces our belief that challenge datasets can spur research into more comprehensive semantic analyses of what LMs know, encouraging system hill-climbing up the right hill, per Bender and Koller (2020). to knowledge bases (Petroni et al., 2019). New possibilities are emerging with methods of estimating the knowledge in LMs that can be found with automatically constructed prompts that yield better results than those manually created, demonstrating that any given prompt may be sub-optimal (Jiang et al., 2020). Given our focus on assessing what one family of prompt-based LMs can answer about a given text passage, the most recent and pertinent overview that situates these LMs in the field is the extensive, systematic survey of prompting methods and pre-trained language models using these method"
2021.mrqa-1.7,D18-1151,0,0.0289186,"d for system evaluation to establish benchmarks to gauge progress on shared tasks. In MT research, going beyond automated scores, the challenge set approach to system evaluation has provided a more fine-grained picture of the strengths of neural systems, as well as insight into which linguistic phenomena remain out of reach (Isabelle et al., 2017). In recent LM research that probes the syntactic knowledge in these models, the structured datasets of minimal grammatical/ungrammatical sentence pairs test when LMs assign a higher probability to the grammatical sentence than the ungrammatical one (Marvin and Linzen, 2018; Newman et al., 2021). Though significant caution is needed in interpreting LM results, we see diagnostic value in constructing challenge datasets to evaluate systems’ passage understanding, as carefully constructed by Dua et al. (2019). The fact that a current state-of-the-art LM dropped more than 50 absolute F1 points on their dataset reinforces our belief that challenge datasets can spur research into more comprehensive semantic analyses of what LMs know, encouraging system hill-climbing up the right hill, per Bender and Koller (2020). to knowledge bases (Petroni et al., 2019). New possibi"
2021.mrqa-1.7,clark-etal-2008-toward,0,0.0467111,"Missing"
2021.mrqa-1.7,N19-1246,0,0.0548365,"Missing"
2021.mrqa-1.7,2021.naacl-main.290,0,0.0428489,"o establish benchmarks to gauge progress on shared tasks. In MT research, going beyond automated scores, the challenge set approach to system evaluation has provided a more fine-grained picture of the strengths of neural systems, as well as insight into which linguistic phenomena remain out of reach (Isabelle et al., 2017). In recent LM research that probes the syntactic knowledge in these models, the structured datasets of minimal grammatical/ungrammatical sentence pairs test when LMs assign a higher probability to the grammatical sentence than the ungrammatical one (Marvin and Linzen, 2018; Newman et al., 2021). Though significant caution is needed in interpreting LM results, we see diagnostic value in constructing challenge datasets to evaluate systems’ passage understanding, as carefully constructed by Dua et al. (2019). The fact that a current state-of-the-art LM dropped more than 50 absolute F1 points on their dataset reinforces our belief that challenge datasets can spur research into more comprehensive semantic analyses of what LMs know, encouraging system hill-climbing up the right hill, per Bender and Koller (2020). to knowledge bases (Petroni et al., 2019). New possibilities are emerging wi"
2021.mrqa-1.7,D19-1250,0,0.0188318,"tical one (Marvin and Linzen, 2018; Newman et al., 2021). Though significant caution is needed in interpreting LM results, we see diagnostic value in constructing challenge datasets to evaluate systems’ passage understanding, as carefully constructed by Dua et al. (2019). The fact that a current state-of-the-art LM dropped more than 50 absolute F1 points on their dataset reinforces our belief that challenge datasets can spur research into more comprehensive semantic analyses of what LMs know, encouraging system hill-climbing up the right hill, per Bender and Koller (2020). to knowledge bases (Petroni et al., 2019). New possibilities are emerging with methods of estimating the knowledge in LMs that can be found with automatically constructed prompts that yield better results than those manually created, demonstrating that any given prompt may be sub-optimal (Jiang et al., 2020). Given our focus on assessing what one family of prompt-based LMs can answer about a given text passage, the most recent and pertinent overview that situates these LMs in the field is the extensive, systematic survey of prompting methods and pre-trained language models using these methods by Liu et al. (2021). As this survey note"
2021.mrqa-1.7,D17-1263,0,0.0258641,"uists ask native speakers to judge grammaticality of presented sentences or to answer questions about their language. The elicitation results are key to deriving properties of the language (Clark et al., 2008; Probst and Levin, 2002). In the latter, datasets are constructed for system evaluation to establish benchmarks to gauge progress on shared tasks. In MT research, going beyond automated scores, the challenge set approach to system evaluation has provided a more fine-grained picture of the strengths of neural systems, as well as insight into which linguistic phenomena remain out of reach (Isabelle et al., 2017). In recent LM research that probes the syntactic knowledge in these models, the structured datasets of minimal grammatical/ungrammatical sentence pairs test when LMs assign a higher probability to the grammatical sentence than the ungrammatical one (Marvin and Linzen, 2018; Newman et al., 2021). Though significant caution is needed in interpreting LM results, we see diagnostic value in constructing challenge datasets to evaluate systems’ passage understanding, as carefully constructed by Dua et al. (2019). The fact that a current state-of-the-art LM dropped more than 50 absolute F1 points on"
2021.mrqa-1.7,2002.tmi-papers.17,0,0.177524,"had been memorized, especially at temperature 0. Nevertheless, we can’t completely rule it out. 6 Related Work Challenge Datasets We have constructed a “challenge dataset” guided by approaches in the fields of linguistics and computational linguistics. In the Answers not clearly correct or incorrect begin with “??”. 79 former, datasets are developed for structured elicitation where linguists ask native speakers to judge grammaticality of presented sentences or to answer questions about their language. The elicitation results are key to deriving properties of the language (Clark et al., 2008; Probst and Levin, 2002). In the latter, datasets are constructed for system evaluation to establish benchmarks to gauge progress on shared tasks. In MT research, going beyond automated scores, the challenge set approach to system evaluation has provided a more fine-grained picture of the strengths of neural systems, as well as insight into which linguistic phenomena remain out of reach (Isabelle et al., 2017). In recent LM research that probes the syntactic knowledge in these models, the structured datasets of minimal grammatical/ungrammatical sentence pairs test when LMs assign a higher probability to the grammatic"
2021.mrqa-1.7,2020.tacl-1.28,0,0.0335471,"). The fact that a current state-of-the-art LM dropped more than 50 absolute F1 points on their dataset reinforces our belief that challenge datasets can spur research into more comprehensive semantic analyses of what LMs know, encouraging system hill-climbing up the right hill, per Bender and Koller (2020). to knowledge bases (Petroni et al., 2019). New possibilities are emerging with methods of estimating the knowledge in LMs that can be found with automatically constructed prompts that yield better results than those manually created, demonstrating that any given prompt may be sub-optimal (Jiang et al., 2020). Given our focus on assessing what one family of prompt-based LMs can answer about a given text passage, the most recent and pertinent overview that situates these LMs in the field is the extensive, systematic survey of prompting methods and pre-trained language models using these methods by Liu et al. (2021). As this survey notes, in tuning-free prompting as we have carried out with GPT LMs, the questions in the q/a task generate the answers directly. This is efficient, yet also leaves the prompts as the only method to provide the task specification. For this reason, we present the framework"
2021.mrqa-1.7,P19-1452,0,0.0352836,"Missing"
2021.mrqa-1.7,W18-5446,0,0.0291338,"to make use of these models today, this suggests sticking to applications where the information needed to answer the questions are immediately available in the passage or slowlychanging, widely known information about the world; or else creative questions where there is no wrong answer. For researchers, though, this highlights the need to discover ways of combining the deductive reasoning capabilities already present in early AI work with the context-sensitivity and ability to work with natural language that these models provide. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is of particular relevance to this work, as it is a collection of resources for both training and evaluation of various types of Natural Language Understanding tasks. It is intended to be agnostic to the system type. The evaluation suite includes tasks related to sentiment, paraphrase, natural language inference, coreference, as well as question-answering. While we took inspiration from this benchmark in our inclusion of questions overlapping with the inference tasks in GLUE, such as the Winograd Schema Challenge, we contribute a set of challenge questions that have been carefully crafted and"
2021.naacl-demos.8,W19-1909,0,0.0217165,"Missing"
2021.naacl-demos.8,2020.emnlp-demos.18,0,0.241329,"ung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease hypertension [PMID:32314699 (PMC7253125)] Past medical history was significant for hypertension, treated"
2021.naacl-demos.8,D19-1371,0,0.0246983,"89178)] To address the role of angiotensin in lung injury, there is an ongoing clinical trial to examine whether losartan treatment affects outcomes in COVID-19 associated ARDS (NCT04312009). [PMID:32439915 (PMC7242178)] Losartan was also the molecule chosen in two trials recently started in the United States by the University of Minnesota to treat patients with COVID-19 (clinical trials.gov NCT04311177 and NCT 104312009). Related Work Extensive prior research work has focused on extracting biomedical entities (Zheng et al., 2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from te"
2021.naacl-demos.8,W17-2307,0,0.0224213,"posing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease hypertension [PMID:32314699 (PMC7253125)] Past medical history was significant for hypertension, treated with Evidence amlodipine and benazepril, and chronic back pain. Sentences [PMID:32081428 (PMC7092824)] On the other hand, many ACE inhibitors are cu"
2021.naacl-demos.8,W13-2001,0,0.0375856,"sults, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Disease"
2021.naacl-demos.8,N19-1145,1,0.895006,"Missing"
2021.naacl-demos.8,Q17-1008,0,0.0252127,"s Factor, and Interleukin-10. We see all of these connections in our results, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many l"
2021.naacl-demos.8,2020.bionlp-1.22,0,0.0299439,"Missing"
2021.naacl-demos.8,D19-6204,1,0.833702,"ctions in our results, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and"
2021.naacl-demos.8,2020.acl-demos.11,1,0.877148,"2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from text and images in literature are complementary. Our framework advances state-of-the-art by extending the knowledge elements to more fine-grained types, incorporating image analysis and cross-media knowledge grounding, and KG matching into QA. 6 would be too time-consuming for manual human effort. Accordingly, the tool would be useful for stakeholders (e.g., biomedical scientists) to identify specific drug candidates and molecular targets that are relevant in their biomedical and clinical research aims. The use of our know"
2021.naacl-demos.8,W19-5006,0,0.0162201,"ults, such as the examples shown in Figure 3 and Figure 9. With further checks on these results, the scientists also indicated that many results were worth further investigation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19"
2021.naacl-demos.8,2020.acl-main.230,1,0.885325,"Missing"
2021.naacl-demos.8,D19-1410,0,0.0136688,"nowledge elements in each path in the KG. Each edge is assigned a salience score by aggregating the scores of paths passing through it. In addition to knowledge elements, we also present related sentences and source information as evidence. We use BioBert (Lee et al., 2020), a pre-trained language model to represent each sentence along with its left and right neighboring sentences as local contexts. Using the same architecture computed on all respective sentences and the user query, we aggregate the sequence embedding layer, the last hidden layer in the BERT architecture with average pooling (Reimers and Gurevych, 2019). We use the similarity between the embedding representations of each sentence and each query to identify and extract the most relevant sentences as evidence. Another common category of queries includes entity types, rather than entity instances, and requires extracting evidence sentences based on type or pattern matching. We have developed E VI DENCE M INER (Wang et al., 2020a,b), a web-based system that allows for the user’s query as a natural language statement or an inquiry about a relationship at the meta-symbol level (e.g., CHEMICAL, PROTEIN) and then automatically retrieves textual evid"
2021.naacl-demos.8,2020.bionlp-1.21,0,0.0201708,"igation. For example, in Figure 3 we can see that Lusartan is connected to tumor protein p53 which is related to lung cancer. Severe Acute Respiratory Syndrome lopinavir-ritonavir drug combination Q1 Q4 Q6 Q8 Table 1: Example Answers for Questions in Drug Repurposing Reports Manandhar and Yuret, 2013; Bui et al., 2014; Peng et al., 2016; Wei et al., 2015; Peng et al., 2017; Luo et al., 2017; Wei et al., 2019; Li and Ji, 2019; Peng et al., 2019, 2020), and events (Ananiadou et al., 2010; Van Landeghem et al., 2013; Nédellec et al., 2013; Deléger et al., 2016; Wei et al., 2019; Li et al., 2019; ShafieiBavani et al., 2020) from biomedical literature, with the most recent work focused on COVID-19 literature (Hope et al., 2020; Ilievski et al., 2020; Wolinski, 2020; Ahamed and Samad, 2020). Most of the recent biomedical QA work (Yang et al., 2015, 2016; Chandu et al., 2017; Kraus et al., 2017) is driven by the BioASQ initiative (Tsatsaronis et al., 2015), and many live QA systems, including COVIDASK11 and AUEB12 , and search enCOVID-19 Coronavirus Infections cathepsin D Figure 9: Connections Involving Coronavirus Related Diseases 5 Example Answers Drug Class angiotensin-converting enzyme (ACE) inhibitors Disease"
2021.naacl-demos.8,D18-1230,1,0.819154,"Missing"
2021.naacl-demos.8,2020.bionlp-1.3,0,0.0196518,"h COVID-19 (clinical trials.gov NCT04311177 and NCT 104312009). Related Work Extensive prior research work has focused on extracting biomedical entities (Zheng et al., 2014; Habibi et al., 2017; Crichton et al., 2017; Wang et al., 2018; Beltagy et al., 2019; Alsentzer et al., 2019; Wei et al., 2019; Wang et al., 2020c), relations (Uzuner et al., 2011; Krallinger et al., 2011; 11 10 http://blender.cs.illinois.edu/ covid19/DrugRe-purposingReport_V2.0.docx 12 71 https://covidask.korea.ac.kr/ http://cslab241.cs.aueb.gr:5000/ gines (Kricka et al., 2020; Esteva et al., 2020; Hope et al., 2020; Taub Tabib et al., 2020) have been developed. Our work is an application and extension of our recently developed multimedia knowledge extraction system for the news domain (Li et al., 2020a,b). Similar to the news domain, the knowledge elements extracted from text and images in literature are complementary. Our framework advances state-of-the-art by extending the knowledge elements to more fine-grained types, incorporating image analysis and cross-media knowledge grounding, and KG matching into QA. 6 would be too time-consuming for manual human effort. Accordingly, the tool would be useful for stakeholders (e.g., bio"
2021.naacl-demos.8,W16-3104,0,0.0689349,"Missing"
2021.naacl-demos.8,P19-1191,1,0.889692,"Missing"
2021.naacl-demos.8,C14-1149,1,0.831567,"Missing"
2021.naacl-demos.8,2020.acl-demos.8,1,0.900567,"cal contexts. Using the same architecture computed on all respective sentences and the user query, we aggregate the sequence embedding layer, the last hidden layer in the BERT architecture with average pooling (Reimers and Gurevych, 2019). We use the similarity between the embedding representations of each sentence and each query to identify and extract the most relevant sentences as evidence. Another common category of queries includes entity types, rather than entity instances, and requires extracting evidence sentences based on type or pattern matching. We have developed E VI DENCE M INER (Wang et al., 2020a,b), a web-based system that allows for the user’s query as a natural language statement or an inquiry about a relationship at the meta-symbol level (e.g., CHEMICAL, PROTEIN) and then automatically retrieves textual evidence from a background corpora of COVID-19. 4 4.1 BM1_00870 BM1_06175 BM1_16375 BM1_17125 BM1_22385 BM1_30360 BM1_33735 BM1_56245 BM1_56735 BM1_00870 BM1_06175 BM1_16375 BM1_17125 BM1_22385 BM1_30360 BM1_33735 BM1_56245 BM1_56735 CATB-10270 CATB-1418 CATB-1674 CATB-16A CATB-16D2 CATB-1852 CATB1874 CATB-2744 CATB-3098 CATB-348 CATB-3483 CATB-5880 CATB-84 CATB912 CATD CATHY CATK"
C12-1076,C10-1034,0,0.0603871,"a predefined threshold δ t t . Given its success when applied to sentence ranking for the task of extractive document summarization (Mihalcea, 2004), we choose TextRank as the baseline method to compute ranking scores in 1241 tweet-only networks where edges between tweets are determined by their cosine similarity. 2.3 Redundancy Removal Since users on Twitter can be tweeting similar information obliviously, and retweet and reply others’ tweets, redundancy has been shown to be a pervasive phenomenon (Zanzotto et al., 2011). This issue has not been considered in previous works on tweet ranking (Duan et al., 2010; Huang et al., 2011). In this work, we perform a redundancy removal step to diversify top ranked tweets. To do so, we adopt the widely used greedy procedure (Carterette and Chandar, 2009; McDonald, 2007) to apply redundancy removal after the completion of each ranking method, as follows: tweet t i in position i is removed when its cosine similarity with tweets t j ∈ [t 1 , t i−1 ] in more highly-ranked positions exceeds or equals a predefined threshold δ r ed 3 3 Motivations and Hypotheses Next, we describe the motivational aspects and hypotheses in this work, which we aim to prove. Hypothesi"
C12-1076,I11-1042,0,0.0868099,"old δ t t . Given its success when applied to sentence ranking for the task of extractive document summarization (Mihalcea, 2004), we choose TextRank as the baseline method to compute ranking scores in 1241 tweet-only networks where edges between tweets are determined by their cosine similarity. 2.3 Redundancy Removal Since users on Twitter can be tweeting similar information obliviously, and retweet and reply others’ tweets, redundancy has been shown to be a pervasive phenomenon (Zanzotto et al., 2011). This issue has not been considered in previous works on tweet ranking (Duan et al., 2010; Huang et al., 2011). In this work, we perform a redundancy removal step to diversify top ranked tweets. To do so, we adopt the widely used greedy procedure (Carterette and Chandar, 2009; McDonald, 2007) to apply redundancy removal after the completion of each ranking method, as follows: tweet t i in position i is removed when its cosine similarity with tweets t j ∈ [t 1 , t i−1 ] in more highly-ranked positions exceeds or equals a predefined threshold δ r ed 3 3 Motivations and Hypotheses Next, we describe the motivational aspects and hypotheses in this work, which we aim to prove. Hypothesis 1: Informative twee"
C12-1076,P04-3020,0,0.0190467,"homogeneous networks, which is defined as follows: s(vi ) = (1 − d) + d ∗ X v j ∈I n(vi ) w ji s(v j ) X w jk (1) vk ∈Out(v j ) where vi is a vertex with s(vi ) as the ranking score, I n(vi ) as the set of incoming edges, and Out(vi ) as the set of outgoing edges; w i j is the weight for the edge between two vertices vi and v j . An edge exists between two vertices that represent text units when their computed shared content (cosine similarity) exceeds or equals a predefined threshold δ t t . Given its success when applied to sentence ranking for the task of extractive document summarization (Mihalcea, 2004), we choose TextRank as the baseline method to compute ranking scores in 1241 tweet-only networks where edges between tweets are determined by their cosine similarity. 2.3 Redundancy Removal Since users on Twitter can be tweeting similar information obliviously, and retweet and reply others’ tweets, redundancy has been shown to be a pervasive phenomenon (Zanzotto et al., 2011). This issue has not been considered in previous works on tweet ranking (Duan et al., 2010; Huang et al., 2011). In this work, we perform a redundancy removal step to diversify top ranked tweets. To do so, we adopt the wi"
C12-1076,D11-1061,0,0.0804207,"represent text units when their computed shared content (cosine similarity) exceeds or equals a predefined threshold δ t t . Given its success when applied to sentence ranking for the task of extractive document summarization (Mihalcea, 2004), we choose TextRank as the baseline method to compute ranking scores in 1241 tweet-only networks where edges between tweets are determined by their cosine similarity. 2.3 Redundancy Removal Since users on Twitter can be tweeting similar information obliviously, and retweet and reply others’ tweets, redundancy has been shown to be a pervasive phenomenon (Zanzotto et al., 2011). This issue has not been considered in previous works on tweet ranking (Duan et al., 2010; Huang et al., 2011). In this work, we perform a redundancy removal step to diversify top ranked tweets. To do so, we adopt the widely used greedy procedure (Carterette and Chandar, 2009; McDonald, 2007) to apply redundancy removal after the completion of each ranking method, as follows: tweet t i in position i is removed when its cosine similarity with tweets t j ∈ [t 1 , t i−1 ] in more highly-ranked positions exceeds or equals a predefined threshold δ r ed 3 3 Motivations and Hypotheses Next, we descr"
C12-1076,W04-3252,0,\N,Missing
C12-1076,J92-4003,0,\N,Missing
C14-1149,R13-1051,0,0.0579518,"Missing"
C14-1149,P14-1038,1,0.758714,"rases and their links. It must contain one query entity node and one or more slot filler nodes. The annotation of a node includes its entity type, subtype, mention type, referent entities, and semantic category (though not every node has each type of annotation). The annotation of a link includes a dependency label and/or a semantic relation between the two linked nodes. The knowledge graph is constructed using the following procedure. First, we annotate the evidence text using dependency parsing (Marneffe et al., 2006) and Information Extraction (entity, relation and event) (Li et al., 2013; Li and Ji, 2014). Two nodes are linked if they are deemed related by one of the annotation methods (e.g., [Mays, 50] is labeled with the dependency type amod, and [home, Tampa] is labeled with the semantic relation located in). The annotation output is often in terms of syntactic heads. Thus, we extend the boundaries of entity, time, and value mentions (e.g., people’s titles) to include an entire phrase where possible. We then enrich each node with annotation for entity type, subtype and mention type. Entity type and subtype refer to the role played by the entity in the world, the latter being more fine-grain"
C14-1149,P13-1008,1,0.422759,"tity mentions, phrases and their links. It must contain one query entity node and one or more slot filler nodes. The annotation of a node includes its entity type, subtype, mention type, referent entities, and semantic category (though not every node has each type of annotation). The annotation of a link includes a dependency label and/or a semantic relation between the two linked nodes. The knowledge graph is constructed using the following procedure. First, we annotate the evidence text using dependency parsing (Marneffe et al., 2006) and Information Extraction (entity, relation and event) (Li et al., 2013; Li and Ji, 2014). Two nodes are linked if they are deemed related by one of the annotation methods (e.g., [Mays, 50] is labeled with the dependency type amod, and [home, Tampa] is labeled with the semantic relation located in). The annotation output is often in terms of syntactic heads. Thus, we extend the boundaries of entity, time, and value mentions (e.g., people’s titles) to include an entire phrase where possible. We then enrich each node with annotation for entity type, subtype and mention type. Entity type and subtype refer to the role played by the entity in the world, the latter bei"
C14-1149,de-marneffe-etal-2006-generating,0,0.00418774,"nd [Mays, per: age, 50]. Formally, a knowledge graph is an annotated graph of entity mentions, phrases and their links. It must contain one query entity node and one or more slot filler nodes. The annotation of a node includes its entity type, subtype, mention type, referent entities, and semantic category (though not every node has each type of annotation). The annotation of a link includes a dependency label and/or a semantic relation between the two linked nodes. The knowledge graph is constructed using the following procedure. First, we annotate the evidence text using dependency parsing (Marneffe et al., 2006) and Information Extraction (entity, relation and event) (Li et al., 2013; Li and Ji, 2014). Two nodes are linked if they are deemed related by one of the annotation methods (e.g., [Mays, 50] is labeled with the dependency type amod, and [home, Tampa] is labeled with the semantic relation located in). The annotation output is often in terms of syntactic heads. Thus, we extend the boundaries of entity, time, and value mentions (e.g., people’s titles) to include an entire phrase where possible. We then enrich each node with annotation for entity type, subtype and mention type. Entity type and su"
C14-1149,P04-3020,0,0.0192256,"between ri and tk when response ri is provided by system tk . Credibility Initialization Each source is represented as a combination of publication venue and genre. The credibility scores of sources S are initialized uniformly as n1 , where n is the number of sources. Given the set of systems T = {t1 , . . . , tl }, we initialize their credibility scores c0 (t) based on their interactions on the predicted responses. Suppose each system ti generates a set of responses Rti . The similarity between two systems ti and tj is defined as similarity(ti , tj ) = |Rti ∩Rtj | log (|Rti |)+log (|Rtj |) (Mihalcea, 2004). Then we construct a weighted undirected graph G = hT, Ei, where T (G) = {t1 , . . . , tl } and E(G) = {hti , tj i}, hti , tj i = similarity(ti , tj ), and apply the TextRank algorithm (Mihalcea, 2004) on G to obtain c0 (t). We got negative results by initializing system credibility scores uniformly. We also got negative results by initializing system credibility scores using system metadata, such as the algorithms and resources the system used at each step, its previous performance in benchmark tests, and the confidence values it produced for its responses. We found the quality of an SF syst"
C14-1149,P09-1113,0,0.0716101,"Mays amod nsubj {GPE.Population-Center.NAM, FL-USA} 【 Per:place_of_death】 aux Tampa {Death-Trigger} prep_in had located_in nn prep_of died sleep prep_at home poss {FAC.Building-Grounds.NOM} poss June,28 his {PER.Individual.PRO, Mays} {06/28/2009, TIME-WITHIN} 【 per:date_of_death】 Figure 2: Knowledge Graph Example. number of trigger phrases for each slot type by mapping various knowledge bases, including Wikipedia Infoboxes, Freebase (Bollacker et al., 2008), DBPedia (Auer et al., 2007) and YAGO (Suchanek et al., 2007), into the Gigaword corpus3 and Wikipedia articles via distant supervision (Mintz et al., 2009)4 . Each intermediate node in the knowledge graph that matches a trigger phrase is then assigned a corresponding semantic category. For example, “died” in Fig. 2 is labeled a Death-Trigger. 4.3 Knowledge Graph-Based Verification We design linguistic indicators in terms of the properties of nodes and paths that are likely to be bear on the response’s veracity. Formally, a path consists of the list of nodes and links that must be traversed along a route from a query node to a slot filler node. Node indicators contribute information about a query entity or slot filler node in isolation, that may"
C14-1149,C10-1099,0,0.021436,"Missing"
D18-1433,P16-1014,0,0.0345829,"objects into captions (Venugopalan et al., 2016), and perform open domain captioning (Tran et al., 2016). To the best of our knowledge, our dataset is the first of its kind and offers challenges in entity and activity recognition as well as the generation low probability words. Datasets with captions rich in knowledge elements, like those in our dataset, take a necessary step towards increasing the utility of video captioning systems. We employ similar approaches to those in automatic summarization, where pointer networks (Vinyals et al., 2015) and copy mechanisms (Gu et al., 2016) are used (Gulcehre et al., 2016; Nallapati et al., 2016; Miao and Blunsom, 2016; See et al., 2017), and natural language generation for dialogue systems (Wen et al., 2015; Tran and Nguyen, 2017). The KaVD network combines the copying capabilities of pointer networks (See et al., 2017) and semantic control of gating mechanisms (Wen et al., 2015; Tran and Nguyen, 2017) in a complementary fashion to address a new, multi-modal task. 3999 7 Conclusions and Future Work We collect a news video dataset with knowledgerich descriptions and present a multi-modal approach to this task that uses a novel Knowledgeaware Video Description"
D18-1433,W04-3250,0,0.0136134,"se and diverse descriptions, though it negatively affects the entity incorporation performance. The video alone is insufficient to generate the correct entities (Table 2). In Figure 5a, the VD baseline generates the correct event, but generates the incorrect location “Kabul”. We observe that when the visual evidence is ambiguous, this model may fail to generate the correct events and entities. For example, if a video depicts the destruction of buildings after a hurricane, then the VD baseline 7 8 This criterion is used for computing precision and recall. Found via paired bootstrap resampling (Koehn, 2004). may mistakenly describe the video as an explosion since the visual evidence is similar. The article-only baseline tends to mention the correct entities as shown in Figure 5a, where the description is generally on topic but provides some irrelevant information. Indeed, this model can generate descriptions unrelated to the video itself. In Figure 5b, the article-only baseline’s description contains some correct entities (e.g., “Colombia”), but is not focused on the announcement depicted in the video. As See et al. (2017) discuss, this model can be more extractive than abstractive, copying many"
D18-1433,P13-1008,1,0.940267,"events that appear in the video’s description, but these may not be specific to the video content. For example, in Figure 2b, the video discusses the “heightened security” and does not depict the arrest directly. Topically related news documents capture background knowledge about the attack that led to the “heightened security” as well as the arrest, but they may not describe the actual video content, which displays some of the increased security measures. Thus, we propose to retrieve topically related news documents from which we seek to extract named entities (Pan et al., 2017) and events (Li et al., 2013) likely relevant to the video. We then propose to use this knowledge in the generation process through an entity pointer network, which learns to dynamically incorporate extracted entities into the description, and through a new knowledge gate, which conditions the generator on the extracted event and entity types. We include the video content in the generation by learning video representations using a spatio-temporal hierarchical attention that spatially attends to regions of each frame and temporally attends to different frames. We call the combination of these generation components the Know"
D18-1433,W04-1013,0,0.0415132,"Missing"
D18-1433,D16-1031,0,0.0534703,"ns by encoding information from the preceding and subsequent frames (Yao et al., 2015). We use a LSTM decoder, which applies a temporal attention (Bahdanau et al., 2015) to the frame representations at each step. To generate each word, the decoder computes its hidden state, adjusts this hidden state with the knowledge gate output at the current time step, and determines the most probable word by utilizing the entity pointer network to decide whether to generate a named entity or vocabulary word. Pointer networks are effective at incorporating out-of-vocabulary (OOV) words in output sequences (Miao and Blunsom, 2016; See et al., 2017). In previous research, OOV words may appear in the input sequence, in which case they are copied into the output. Analogously, in our approach, named entities can be considered as OOV words that are from a separate set instead of the input sequence. In the following equations, where appropriate, we omit bias terms for brevity. Encoder. The input to the encoder is a sequence of video frames, {F1 , ..., FN }. First, we extract frame-level features by applying a Convolutional Neural Network (CNN) (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014; Ioffe and Szegedy, 2015;"
D18-1433,K16-1028,0,0.0223229,"(Venugopalan et al., 2016), and perform open domain captioning (Tran et al., 2016). To the best of our knowledge, our dataset is the first of its kind and offers challenges in entity and activity recognition as well as the generation low probability words. Datasets with captions rich in knowledge elements, like those in our dataset, take a necessary step towards increasing the utility of video captioning systems. We employ similar approaches to those in automatic summarization, where pointer networks (Vinyals et al., 2015) and copy mechanisms (Gu et al., 2016) are used (Gulcehre et al., 2016; Nallapati et al., 2016; Miao and Blunsom, 2016; See et al., 2017), and natural language generation for dialogue systems (Wen et al., 2015; Tran and Nguyen, 2017). The KaVD network combines the copying capabilities of pointer networks (See et al., 2017) and semantic control of gating mechanisms (Wen et al., 2015; Tran and Nguyen, 2017) in a complementary fashion to address a new, multi-modal task. 3999 7 Conclusions and Future Work We collect a news video dataset with knowledgerich descriptions and present a multi-modal approach to this task that uses a novel Knowledgeaware Video Description network, which can utili"
D18-1433,N04-1019,0,0.0681349,"beddings and compute entity embeddings. For visual features, we use the Conv3-512 layer response of VGGNet (Simonyan and Zisserman, 2014) pre-trained on ImageNet (Deng et al., 2009). use ROUGE-L for comparison to summarization work. These capture the coherence and relevance of the generated descriptions to the ground truth. Generating these descriptions is concerned with not only generating fluent text, but also the amount of knowledge conveyed and the accuracy of the knowledge elements (e.g., named entities or event structures). Previous work in natural language generation and summarization (Nenkova and Passonneau, 2004; Novikova et al., 2017; Wiseman et al., 2017; Pasunuru and Bansal, 2018) scores and/or assigns weights to overlapping text, salient phrases, or information units (e.g., entity relations (Wiseman et al., 2017)). However, knowledge elements cannot be simply represented as a set of isolated information units since they are inherently interconnected through some structure. Therefore, for this knowledge-centric generation task, we compute F1 scores on event and entity extraction results from the generated descriptions against the extraction results on the ground truth. For entities, we measure the"
D18-1433,P11-1020,0,0.281605,"Description network. The model learns to incorporate entities found in the topically related documents into the description via an entity pointer network and the generation procedure is guided by the event and entity types from the topically related documents through a knowledge gate, which is a gating mechanism added to the model’s decoder that takes a one-hot vector of these types. We evaluate our approach on the new dataset of news videos we have collected, establishing the first benchmark for this dataset as well as proposing a new metric to evaluate these descriptions. 1 a) Description (Chen and Dolan, 2011): A man is talking. b) Human Description: Senior army ofﬁcer and Zimbabwe Defence Forces' spokesperson, Major General S. B. Moyo, assures the public that President Robert Mugabe and his family are safe and denies that the military is staging a coup. Figure 1: Comparison of machine (a) and human (b) generated descriptions.1 Introduction Video captioning is a challenging task that seeks to automatically generate a natural language description of the content of a video. Many video captioning efforts focus on learning video representations that model the spatial and temporal dynamics of the videos"
D18-1433,D17-1238,0,0.0476198,"Missing"
D18-1433,P17-1178,1,0.924882,"contain the named entities or events that appear in the video’s description, but these may not be specific to the video content. For example, in Figure 2b, the video discusses the “heightened security” and does not depict the arrest directly. Topically related news documents capture background knowledge about the attack that led to the “heightened security” as well as the arrest, but they may not describe the actual video content, which displays some of the increased security measures. Thus, we propose to retrieve topically related news documents from which we seek to extract named entities (Pan et al., 2017) and events (Li et al., 2013) likely relevant to the video. We then propose to use this knowledge in the generation process through an entity pointer network, which learns to dynamically incorporate extracted entities into the description, and through a new knowledge gate, which conditions the generator on the extracted event and entity types. We include the video content in the generation by learning video representations using a spatio-temporal hierarchical attention that spatially attends to regions of each frame and temporally attends to different frames. We call the combination of these g"
D18-1433,W14-3348,0,0.0367102,"Missing"
D18-1433,P16-1154,0,0.0754805,"., 2017), incorporate novel objects into captions (Venugopalan et al., 2016), and perform open domain captioning (Tran et al., 2016). To the best of our knowledge, our dataset is the first of its kind and offers challenges in entity and activity recognition as well as the generation low probability words. Datasets with captions rich in knowledge elements, like those in our dataset, take a necessary step towards increasing the utility of video captioning systems. We employ similar approaches to those in automatic summarization, where pointer networks (Vinyals et al., 2015) and copy mechanisms (Gu et al., 2016) are used (Gulcehre et al., 2016; Nallapati et al., 2016; Miao and Blunsom, 2016; See et al., 2017), and natural language generation for dialogue systems (Wen et al., 2015; Tran and Nguyen, 2017). The KaVD network combines the copying capabilities of pointer networks (See et al., 2017) and semantic control of gating mechanisms (Wen et al., 2015; Tran and Nguyen, 2017) in a complementary fashion to address a new, multi-modal task. 3999 7 Conclusions and Future Work We collect a news video dataset with knowledgerich descriptions and present a multi-modal approach to this task that uses a novel K"
D18-1433,P17-1117,1,0.841562,"em mistakenly assigns a “Transport” event type instead of the correct “Demonstrate” event type. In contrast, such mistakes do not appear in the manual evaluations. 6 Related Work Most previous video captioning efforts focus on learning video representations through different encoding techniques (Venugopalan et al., 2015a,b), using spatial or temporal attentions (Yao et al., 2015; Pan et al., 2016; Yu et al., 2016; Zanfir et al., 2016), using 3D CNN features (Tran et al., 2015; Yao et al., 2015; Pan et al., 2016), or easing the learning process via multi-task learning or reinforcement rewards (Pasunuru and Bansal, 2017a,b). Compared to other hierarchical models (Pan et al., 2016; Yu et al., 2016), each level of our hierarchy encodes a different dimension of the video, leveraging global temporal features and local spatial features, which are shown to be effective for different tasks (Ballas et al., 2015; Xu et al., 2015; Yu et al., 2017). We move towards using datasets with captions that have specific knowledge rather than generic 3998 Model Article-only VD VD+Entity Pointer VD+Knowledge Gate Entity Pointer+Knowledge Gate KaVD METEOR ROUGE-L Entity F1 Auto-Entity F1 Event F1 Auto-Event F1 8.6 9.1 9.7 9.8 10."
D18-1433,D17-1103,1,0.825738,"em mistakenly assigns a “Transport” event type instead of the correct “Demonstrate” event type. In contrast, such mistakes do not appear in the manual evaluations. 6 Related Work Most previous video captioning efforts focus on learning video representations through different encoding techniques (Venugopalan et al., 2015a,b), using spatial or temporal attentions (Yao et al., 2015; Pan et al., 2016; Yu et al., 2016; Zanfir et al., 2016), using 3D CNN features (Tran et al., 2015; Yao et al., 2015; Pan et al., 2016), or easing the learning process via multi-task learning or reinforcement rewards (Pasunuru and Bansal, 2017a,b). Compared to other hierarchical models (Pan et al., 2016; Yu et al., 2016), each level of our hierarchy encodes a different dimension of the video, leveraging global temporal features and local spatial features, which are shown to be effective for different tasks (Ballas et al., 2015; Xu et al., 2015; Yu et al., 2017). We move towards using datasets with captions that have specific knowledge rather than generic 3998 Model Article-only VD VD+Entity Pointer VD+Knowledge Gate Entity Pointer+Knowledge Gate KaVD METEOR ROUGE-L Entity F1 Auto-Entity F1 Event F1 Auto-Event F1 8.6 9.1 9.7 9.8 10."
D18-1433,N18-2102,1,0.823313,"v3-512 layer response of VGGNet (Simonyan and Zisserman, 2014) pre-trained on ImageNet (Deng et al., 2009). use ROUGE-L for comparison to summarization work. These capture the coherence and relevance of the generated descriptions to the ground truth. Generating these descriptions is concerned with not only generating fluent text, but also the amount of knowledge conveyed and the accuracy of the knowledge elements (e.g., named entities or event structures). Previous work in natural language generation and summarization (Nenkova and Passonneau, 2004; Novikova et al., 2017; Wiseman et al., 2017; Pasunuru and Bansal, 2018) scores and/or assigns weights to overlapping text, salient phrases, or information units (e.g., entity relations (Wiseman et al., 2017)). However, knowledge elements cannot be simply represented as a set of isolated information units since they are inherently interconnected through some structure. Therefore, for this knowledge-centric generation task, we compute F1 scores on event and entity extraction results from the generated descriptions against the extraction results on the ground truth. For entities, we measure the F1 score of the named entities in the generated description compared to"
D18-1433,N15-1173,0,0.0413973,"te with this metric. We observe discrepancies between the manual and automatic event metrics, in part, due to errors in the automated extraction and the addition of more test points. For example, in the generated sentence, “Hundreds of people are to take to the streets of...”, the event extraction system mistakenly assigns a “Transport” event type instead of the correct “Demonstrate” event type. In contrast, such mistakes do not appear in the manual evaluations. 6 Related Work Most previous video captioning efforts focus on learning video representations through different encoding techniques (Venugopalan et al., 2015a,b), using spatial or temporal attentions (Yao et al., 2015; Pan et al., 2016; Yu et al., 2016; Zanfir et al., 2016), using 3D CNN features (Tran et al., 2015; Yao et al., 2015; Pan et al., 2016), or easing the learning process via multi-task learning or reinforcement rewards (Pasunuru and Bansal, 2017a,b). Compared to other hierarchical models (Pan et al., 2016; Yu et al., 2016), each level of our hierarchy encodes a different dimension of the video, leveraging global temporal features and local spatial features, which are shown to be effective for different tasks (Ballas et al., 2015; Xu et"
D18-1433,D15-1199,0,0.025523,"Missing"
D18-1433,D17-1239,0,0.0255935,"atures, we use the Conv3-512 layer response of VGGNet (Simonyan and Zisserman, 2014) pre-trained on ImageNet (Deng et al., 2009). use ROUGE-L for comparison to summarization work. These capture the coherence and relevance of the generated descriptions to the ground truth. Generating these descriptions is concerned with not only generating fluent text, but also the amount of knowledge conveyed and the accuracy of the knowledge elements (e.g., named entities or event structures). Previous work in natural language generation and summarization (Nenkova and Passonneau, 2004; Novikova et al., 2017; Wiseman et al., 2017; Pasunuru and Bansal, 2018) scores and/or assigns weights to overlapping text, salient phrases, or information units (e.g., entity relations (Wiseman et al., 2017)). However, knowledge elements cannot be simply represented as a set of isolated information units since they are inherently interconnected through some structure. Therefore, for this knowledge-centric generation task, we compute F1 scores on event and entity extraction results from the generated descriptions against the extraction results on the ground truth. For entities, we measure the F1 score of the named entities in the genera"
D18-1433,P17-1099,0,0.383734,"on from the preceding and subsequent frames (Yao et al., 2015). We use a LSTM decoder, which applies a temporal attention (Bahdanau et al., 2015) to the frame representations at each step. To generate each word, the decoder computes its hidden state, adjusts this hidden state with the knowledge gate output at the current time step, and determines the most probable word by utilizing the entity pointer network to decide whether to generate a named entity or vocabulary word. Pointer networks are effective at incorporating out-of-vocabulary (OOV) words in output sequences (Miao and Blunsom, 2016; See et al., 2017). In previous research, OOV words may appear in the input sequence, in which case they are copied into the output. Analogously, in our approach, named entities can be considered as OOV words that are from a separate set instead of the input sequence. In the following equations, where appropriate, we omit bias terms for brevity. Encoder. The input to the encoder is a sequence of video frames, {F1 , ..., FN }. First, we extract frame-level features by applying a Convolutional Neural Network (CNN) (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014; Ioffe and Szegedy, 2015; Szegedy et al., 201"
D18-1433,K17-1044,0,0.0998129,"ding, xt−1 . The final decoder hidden state is determined after the knowledge gate computation. The motivation for the knowledge gate is that it biases the model to generate sentences that contain specific knowledge relevant to the video and topically related documents, acting as a kind of coverage mechanism (Tu et al., 2016). For example, given the retrieved event types in Figure 3, the knowledge gate encourages the decoder to generate the event trigger “coup” due to the presence of the “Attack” event type. Inspired by the gating mechanisms from natural language generation (Wen et al., 2015; Tran and Nguyen, 2017), the knowledge gate, gt , is given by gt = σ (Wg,v [xt−1 , vt ] + Wg,sˆst ) (7) kt = gt kt−1 (8) where all W are learned parameters and [xt−1 , vt ] is the concatenation of these two vectors. This gating step determines the amount of the entity and event type features contained in kt−1 to carry to the next step. With the updated kt , we compute the decoder hidden state, st , as st = ˆst + (ot tanh (Ws,k kt )) (9) where ot is the output gate of the LSTM and Ws,k is a learned parameter. Our next step is to generate the next word. The model needs to produce named entities (e.g., “S. B. Moyo” and"
D18-1433,N16-1174,0,0.0505958,"0 corresponds to an event or entity type (e.g., “Arrest-Jail” event type or “President” entity type), so the j th element, k (j) , is 1 if the entity or event type is found in the related documents and 0 otherwise. k0 serves as the initial knowledge gate vector of the decoder (Section 2.2). The entity embeddings give the model access to semantic representations of the entities, while the knowledge gate vector aids the generation process by providing the model with the event and entity types. 2.2 KaVD Network Our model learns video representations using hierarchical, or multi-level, attention (Yang et al., 2016; Qin et al., 2017). The encoder is comprised of a spatial attention (Xu et al., 2015) and bidirectional Long Short-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997) temporal encoder. The spatial attention allows the model to attend to different locations of each frame (Figure 4), yielding frame representations 3994 S. B. Moyo g Output Distribution gin p genP v + (1 − p gen)P e st p gen vt ct Temporal Attention gin vt S. Mu ga be st Vocab Distribution B. M Zim oyo ba bw e as su r froes m Spatial Attention g hN sta h1 Entity Distribution Knowledge Gate Vector S. B. Moyo sta General"
D18-1433,P16-1008,0,0.0352559,"where st−1 is the previous decoder hidden state and atime is another scoring function. This yields a single, spatio-temporally attentive video representation, vt . We then compute an intermediate hidden state, ˆst , by applying the decoder LSTM 3995 to st−1 , vt , and previous word embedding, xt−1 . The final decoder hidden state is determined after the knowledge gate computation. The motivation for the knowledge gate is that it biases the model to generate sentences that contain specific knowledge relevant to the video and topically related documents, acting as a kind of coverage mechanism (Tu et al., 2016). For example, given the retrieved event types in Figure 3, the knowledge gate encourages the decoder to generate the event trigger “coup” due to the presence of the “Attack” event type. Inspired by the gating mechanisms from natural language generation (Wen et al., 2015; Tran and Nguyen, 2017), the knowledge gate, gt , is given by gt = σ (Wg,v [xt−1 , vt ] + Wg,sˆst ) (7) kt = gt kt−1 (8) where all W are learned parameters and [xt−1 , vt ] is the concatenation of these two vectors. This gating step determines the amount of the entity and event type features contained in kt−1 to carry to the n"
D18-1433,D16-1204,0,0.0955565,"b) Human Description: Senior army ofﬁcer and Zimbabwe Defence Forces' spokesperson, Major General S. B. Moyo, assures the public that President Robert Mugabe and his family are safe and denies that the military is staging a coup. Figure 1: Comparison of machine (a) and human (b) generated descriptions.1 Introduction Video captioning is a challenging task that seeks to automatically generate a natural language description of the content of a video. Many video captioning efforts focus on learning video representations that model the spatial and temporal dynamics of the videos (Yao et al., 2015; Venugopalan et al., 2016; Yu et al., 2017). Although the language generation component within this task is of great importance, less work has been done to enhance the contextual knowledge conveyed by the descriptions. The descriptions generated by previous methods tend to be “generic”, describing To address this problem, we collect a news video dataset, where each video is accompanied by meta-data (e.g., tags and date) and a natural language description of the content in, and/or context around, the video. We create an approach to this task that is motivated by two observations. First, the video content alone is insuf"
D19-1030,D14-1164,0,0.0220708,"f manually annotated mentions. The system extracted entity mentions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al.,"
D19-1030,D12-1092,0,0.0655205,"Missing"
D19-1030,L18-1245,0,0.0189856,"so are reported predominantly in the low-resource language data sources available to that community. For example, though English language news will occasionally discuss the Person Aung San Suu Kyi, the vast majority of Physical-Located relations and Meeting events involving her are only reported locally in Burmese news, and thus, without Burmese relation and event extraction, a knowledge graph of this person will lack this available information. Unfortunately, publiclyavailable gold-standard annotations for relation and event extraction exist for only a few languages (Doddington et al., 2004; Getman et al., 2018), and Burmese is not among them. Compared to other IE tasks such as name tagging, the annotations for Relation and Event Extraction are also more costly to obtain, because they are structured and require a rich label space. Recent research (Lin et al., 2017) has found that relational facts are typically expressed by identifiable patterns within languages and has shown Introduction Advanced Information Extraction (IE) tasks entail predicting structures, such as relations between entities, and events involving entities. Given a pair of entity mentions, Relation Extraction aims to identify the re"
D19-1030,P14-1038,1,0.937832,"ces by limiting the number of negative samples to be no more than the number of positive samples for each document. For data preprocessing, we apply the Stanford CoreNLP toolkit (Manning et al., 2014) for Chinese word segmentation and English tokenization, and the API provided by UDPipe (Straka and s m2 yij log(σ(U r · [hm1 i ; hij ; hj ])) i=1 j=1 (1) where U r is a weight matrix. 316 Straková, 2017) for Arabic tokenization. We use UDPipe1 for POS tagging and dependency parsing for all three languages. We follow the following criteria in previous work (Ji and Grishman, 2008; Li et al., 2013; Li and Ji, 2014) for evaluation: performance by applying models trained with various combinations of training and test data from these three languages, as shown in Tables 3 and 4. We can see that the results are promising. For both tasks, the models trained from English are best, followed by Chinese, and then Arabic. We find that extraction task performance degrades as the accuracy of language-dependent tools (for sentence segmentation, POS tagging, dependency parsing) degrades. • A relation mention is considered correct if its relation type is correct, and the head offsets of the two related entity mention a"
D19-1030,C16-1114,0,0.359758,"al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervise"
D19-1030,P13-1008,1,0.874134,"e training instances by limiting the number of negative samples to be no more than the number of positive samples for each document. For data preprocessing, we apply the Stanford CoreNLP toolkit (Manning et al., 2014) for Chinese word segmentation and English tokenization, and the API provided by UDPipe (Straka and s m2 yij log(σ(U r · [hm1 i ; hij ; hj ])) i=1 j=1 (1) where U r is a weight matrix. 316 Straková, 2017) for Arabic tokenization. We use UDPipe1 for POS tagging and dependency parsing for all three languages. We follow the following criteria in previous work (Ji and Grishman, 2008; Li et al., 2013; Li and Ji, 2014) for evaluation: performance by applying models trained with various combinations of training and test data from these three languages, as shown in Tables 3 and 4. We can see that the results are promising. For both tasks, the models trained from English are best, followed by Chinese, and then Arabic. We find that extraction task performance degrades as the accuracy of language-dependent tools (for sentence segmentation, POS tagging, dependency parsing) degrades. • A relation mention is considered correct if its relation type is correct, and the head offsets of the two relate"
D19-1030,P18-1074,1,0.816541,"2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et al., 2010; Qian et al., 2014) or machine translation (Faruqui and Kumar, 2015; Zou et al., 2018). Recent methods (Lin et al., 2017; Wang et al., 2018b) aggregate consistent patterns and complementary information across languages to enhance Relation Extraction, but they do so exploiting only distributional representations. Even"
D19-1030,P09-1113,0,0.169793,"Missing"
D19-1030,P16-1105,0,0.0446401,"ing techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et"
D19-1030,N19-1112,0,0.0293072,"raction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has been very successful for SRL. Early attempts relied on word alignment (Van der Plas et al., 2011) or bilingual dictionaries (Kozhevnikov and Titov, 2013). Recent work incorporates universal dependencies (Prazák and Konopík, 2017) or multilingual word embeddings for Polyglot SRL (Mulcaire et al., 2018). Liu et al. (2019) and Mulcaire et al. (2019) exploit multi-lingual contextualized word embedding for SRL and other Polyglot NLP tasks including dependency parsing and name tagging. To the best of our knowledge, our work is the first to construct a cross-lingual structure transfer framework that combines language-universal symbolic representations and distributional representations for relation and event extraction over texts written in a language without any training data. GCN has been successfully applied to several individual monolingual NLP tasks, including relation extraction (Zhang et al., 2018b), event d"
D19-1030,N19-1392,0,0.0323235,"mantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has been very successful for SRL. Early attempts relied on word alignment (Van der Plas et al., 2011) or bilingual dictionaries (Kozhevnikov and Titov, 2013). Recent work incorporates universal dependencies (Prazák and Konopík, 2017) or multilingual word embeddings for Polyglot SRL (Mulcaire et al., 2018). Liu et al. (2019) and Mulcaire et al. (2019) exploit multi-lingual contextualized word embedding for SRL and other Polyglot NLP tasks including dependency parsing and name tagging. To the best of our knowledge, our work is the first to construct a cross-lingual structure transfer framework that combines language-universal symbolic representations and distributional representations for relation and event extraction over texts written in a language without any training data. GCN has been successfully applied to several individual monolingual NLP tasks, including relation extraction (Zhang et al., 2018b), event detection (Nguyen and Grishm"
D19-1030,D18-1156,0,0.172682,"Missing"
D19-1030,P18-2106,0,0.0942397,"Missing"
D19-1030,N15-1028,0,0.0215415,"symbolic features, such as a common labeled dependency path, as well as distributional features, such as multilingual word embeddings. Based on these language-universal representations, we then project all entity mentions, event triggers and their contexts into one multilingual common space. Unlike recent work on multilingual common space construction that makes use of linear mappings (Mikolov et al., 2013; Rothe et al., 2016; Zhang et al., 2016; Lazaridou et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016; Faruqui and Dyer, 2014; Lu et al., 2015) to transfer surface features across languages, our major innovation is to convert the text data into structured representations derived from universal dependency parses and enhanced with distributional information to capture individual entities as well 314 as the relations and events involving those entities, so we can share structural representations across multiple languages. Then we construct a novel cross-lingual structure transfer learning framework to project source language (SL) training data and target language (TL) test data into the common semantic space, so that we can train a rela"
D19-1030,N16-1034,0,0.107021,"nd thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew"
D19-1030,D18-1517,0,0.0143756,"nguage (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored"
D19-1030,P14-5010,0,0.00571376,"Missing"
D19-1030,P15-2060,0,0.0927352,"e-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name"
D19-1030,D17-1159,0,0.194902,"cy parses and enhanced with distributional information to capture individual entities as well 314 as the relations and events involving those entities, so we can share structural representations across multiple languages. Then we construct a novel cross-lingual structure transfer learning framework to project source language (SL) training data and target language (TL) test data into the common semantic space, so that we can train a relation or event extractor from SL annotations and apply the resulting extractor to TL texts. We adopt graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017) to encode graph structures over the input data, applying graph convolution operations to generate entity and word representations in a latent space. In contrast to other encoders such a Tree-LSTM (Tai et al., 2015), GCN can cover more complete contextual information from dependency parses because, for each word, it captures all parse tree neighbors of the word, rather than just the child nodes of the word. Using this shared encoder, we treat the two tasks of relation extraction and event argument role labeling as mappings from the latent space to relation type and to event type and argument r"
D19-1030,W15-1506,0,0.0597738,"e-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name"
D19-1030,P17-1178,1,0.854212,"re seen in packed emergency rooms) Figure 2: Multilingual common semantic space and cross-lingual structure transfer. that the consistency in patterns observed across languages can be used to improve relation extraction. Inspired by their results, we exploit languageuniversal features relevant to relation and event argument identification and classification, by way of both symbolic and distributional representations. For example, language-universal POS tagging and universal dependency parsing is available for 76 languages (Nivre et al., 2018), entity extraction is available for 282 languages (Pan et al., 2017), and multi-lingual word embeddings are available for 44 languages (Bojanowski et al., 2017; Joulin et al., 2018). As shown in Figure 2, even for distinct pairs of entity mentions (colored pink and blue, in both English and Russian), the structures share similar language-universal symbolic features, such as a common labeled dependency path, as well as distributional features, such as multilingual word embeddings. Based on these language-universal representations, we then project all entity mentions, event triggers and their contexts into one multilingual common space. Unlike recent work on mul"
D19-1030,E17-1077,0,0.0555006,"Missing"
D19-1030,D12-1042,0,0.0476729,"y extracted by Stanford CoreNLP instead of manually annotated mentions. The system extracted entity mentions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al."
D19-1030,P11-2052,0,0.0755243,"Missing"
D19-1030,P15-1150,0,0.12426,"Missing"
D19-1030,prazak-konopik-2017-cross,0,0.0648181,"nhance Relation Extraction, but they do so exploiting only distributional representations. Event extraction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has been very successful for SRL. Early attempts relied on word alignment (Van der Plas et al., 2011) or bilingual dictionaries (Kozhevnikov and Titov, 2013). Recent work incorporates universal dependencies (Prazák and Konopík, 2017) or multilingual word embeddings for Polyglot SRL (Mulcaire et al., 2018). Liu et al. (2019) and Mulcaire et al. (2019) exploit multi-lingual contextualized word embedding for SRL and other Polyglot NLP tasks including dependency parsing and name tagging. To the best of our knowledge, our work is the first to construct a cross-lingual structure transfer framework that combines language-universal symbolic representations and distributional representations for relation and event extraction over texts written in a language without any training data. GCN has been successfully applied to several in"
D19-1030,D18-1248,0,0.0916018,"ise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques h"
D19-1030,P14-1055,0,0.115821,"l., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et al., 2010; Qian et al., 2014) or machine translation (Faruqui and Kumar, 2015; Zou et al., 2018). Recent methods (Lin et al., 2017; Wang et al., 2018b) aggregate consistent patterns and complementary information across languages to enhance Relation Extraction, but they do so exploiting only distributional representations. Event extraction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has"
D19-1030,C18-1099,0,0.356373,"ise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques h"
D19-1030,P18-1046,0,0.0122437,"tions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine le"
D19-1030,N15-1104,0,0.0713422,"Missing"
D19-1030,E17-1110,0,0.0234348,"m extracted entity mentions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of sup"
D19-1030,N16-1033,0,0.150323,"cluded in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingu"
D19-1030,N16-1091,0,0.0130887,"Joulin et al., 2018). As shown in Figure 2, even for distinct pairs of entity mentions (colored pink and blue, in both English and Russian), the structures share similar language-universal symbolic features, such as a common labeled dependency path, as well as distributional features, such as multilingual word embeddings. Based on these language-universal representations, we then project all entity mentions, event triggers and their contexts into one multilingual common space. Unlike recent work on multilingual common space construction that makes use of linear mappings (Mikolov et al., 2013; Rothe et al., 2016; Zhang et al., 2016; Lazaridou et al., 2015; Xing et al., 2015; Smith et al., 2017) or canonical correlation analysis (CCA) (Ammar et al., 2016; Faruqui and Dyer, 2014; Lu et al., 2015) to transfer surface features across languages, our major innovation is to convert the text data into structured representations derived from universal dependency parses and enhanced with distributional information to capture individual entities as well 314 as the relations and events involving those entities, so we can share structural representations across multiple languages. Then we construct a novel cross-"
D19-1030,D15-1203,0,0.025466,"mentions. The system extracted entity mentions introduce noise and 318 (a) Target Language: Chinese (b) Target Language: Arabic Figure 3: Relation Extraction: Comparison between supervised monolingual GCN model and cross-lingual transfer learning. (a) Target Language: Chinese (b) Target Language: Arabic Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and crosslingual transfer learning. thus decrease the performance of the model, but the overall results are still promising. 4 et al., 2009; Surdeanu et al., 2012; Min et al., 2013; Angeli et al., 2014; Zeng et al., 2015; Quirk and Poon, 2017; Qin et al., 2018; Wang et al., 2018a) through knowledge bases (KBs), where entities and static relations are plentiful. Distant supervision is less applicable for the task of event extraction because very few dynamic events are included in KBs. These approaches, however, incorporate language-specific characteristics and thus are costly in requiring substantial amount of annotations to adapt to a new language (Chen and Vincent, 2012; Blessing and Schütze, 2012; Li et al., 2012; Danilova et al., 2014; Agerri et al., 2016; Hsi et al., 2016; Feng et al., 2016). Related Work"
D19-1030,C14-1220,0,0.0417218,"Work A large number of supervised machine learning techniques have been used for English event extraction, including traditional techniques based on symbolic features (Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2014; Yang and Mitchell, 2016), and recently with neural networks (Nguyen and Grishman, 2015a; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2018; Liu et al., 2018b; Lu and Nguyen, 2018; Liu et al., 2018a; Zhang et al., 2018a, 2019). English relation extraction in the early days also followed supervised paradigms (Li and Ji, 2014; Zeng et al., 2014; Nguyen and Grishman, 2015b; Miwa and Bansal, 2016; Pawar et al., 2017; Bekoulis et al., 2018; Wang et al., 2018b). Recent efforts have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et a"
D19-1030,K17-3009,0,0.116769,"Missing"
D19-1030,N16-1156,0,0.0604827,"Missing"
D19-1030,D18-1244,0,0.136645,"versal, we first convert each tree node into a vector which is a concatenation of three language-universal representations at wordlevel: multilingual word embedding, POS embedding (Nivre et al., 2016), entity-type embedding, and dependency relation embedding. More details are reported in Section 3.2. Model Overview 2.3 GCN Encoder Structural information is important for relation extraction and event argument role labeling, thus we aim to generate contextualized word representations by leveraging neighbors in dependency trees for each node. Our GCN encoder is based on the monolingual design by Zhang et al. (2018b). The graphical sentence representation obtained from dependency parsing of a sentence with N tokens is converted into an N × N adjacency matrix A, with added self-connections at each node to help capture information about the current node itself, as in Kipf and Welling (2017). Here, Ai,j = 1 denotes the presence of a directed edge from node i to node j in the dependency tree. Initially, each node contains distributional information about the ith word, including word embedding xw i , embeddings for symbolic information including its POS tag xpi , dependency relation xdi and entity type xei ."
D19-1030,C18-1037,0,0.0763441,"have attempted to reduce annotation costs using distant supervision (Mintz Regardless of the recent successes in applying cross-lingual transfer learning to sequence labeling tasks, such as name tagging (e.g., (Mayhew et al., 2017; Lin et al., 2018; Huang et al., 2019)), only limited work has explored cross-lingual relation and event structure transfer. Most previous efforts working with cross-lingual structure trans319 fer rely on bilingual dictionaries (Hsi et al., 2016), parallel data (Chen and Ji, 2009; Kim et al., 2010; Qian et al., 2014) or machine translation (Faruqui and Kumar, 2015; Zou et al., 2018). Recent methods (Lin et al., 2017; Wang et al., 2018b) aggregate consistent patterns and complementary information across languages to enhance Relation Extraction, but they do so exploiting only distributional representations. Event extraction shares with Semantic Role Labeling (SRL) the task of assigning to each event argument its event role label, in the process of completing other tasks to extract the full event structure (assigning event types to predicates and more specific roles to arguments). Cross-lingual transfer has been very successful for SRL. Early attempts relied on word alignme"
hobbs-etal-2008-mtriage,brants-plaehn-2000-interactive,0,\N,Missing
jaja-etal-2012-assessing,2010.amta-papers.16,0,\N,Missing
jaja-etal-2012-assessing,D10-1044,0,\N,Missing
jaja-etal-2012-assessing,C04-1059,0,\N,Missing
jaja-etal-2012-assessing,W01-0521,0,\N,Missing
jaja-etal-2012-assessing,D08-1093,0,\N,Missing
jaja-etal-2012-assessing,W07-0733,0,\N,Missing
jaja-etal-2012-assessing,P11-1157,0,\N,Missing
jaja-etal-2012-assessing,W10-2605,0,\N,Missing
jaja-etal-2012-assessing,I08-2097,0,\N,Missing
jaja-etal-2012-assessing,N10-1004,0,\N,Missing
L18-1017,bunt-etal-2012-iso,1,0.79484,"pplications of these annotations are introduced. Keywords: dialogue structure annotation, human-robot interaction, multiparty dialogue 1. Introduction We present an annotation scheme for meso-level dialogue structure (Traum and Nakatani, 1999), specifically designed for multi-floor dialogue. The scheme includes both a transaction unit for clustering utterances from multiple participants and floors that contribute to realization of an initiating participant’s intent, and relations between individual utterances within the unit. While there are standard annotation schemes for both dialogue acts (Bunt et al., 2012) and discourse relations (Prasad and Bunt, 2015), these schemes do not fully address the issues of dialogue structure. Of particular interest to us, and not previously addressed in other schemes, are cases in which the units and relations span across multiple conversational floors. Dialogues can be characterized by distinct information states (Traum and Larsson, 2003). These include sets of participants, participant roles (e.g. active, ratified participant vs. overhearer), turn-taking or floor-holding, expectation of how many participants will make substantial contributions at a time (Edelsky,"
L18-1017,J86-3001,0,0.827712,"ferent kinds of multi-party, multi-floor contributions. This is addressed by analysis of dialogue annotated with this scheme and the kinds of patterns of interaction that are observed (see section 5). Second, we use data from the corpus annotated with this scheme to serve as training and evaluation data for creating automated multicommunicators (see section 6). 2. Annotation Scheme We annotate two aspects of Dialogue Structure at the mesolevel (bigger than a single speaker-turn, but smaller than a complete dialogue activity) (Traum and Nakatani, 1999). First, we look at intentional structure (Grosz and Sidner, 1986), consisting of units of dialogue utterances that all have a role in explicating and addressing an initiating participant’s intention. Second, we look at the relations between different utterances within this unit, which reveal 104 Expansions Responses Translations processing: relate utterances that are produced by the same participant within the same floor. relate utterances by different participants within the same floor. relate utterances in different floors. acknowledgement: Table 1: Top Level Corpus Relations how the information state of participants in the dialogue is updated as the unit"
L18-1017,W17-2808,1,0.455485,"t apply this annotation scheme to a corpus of humanrobot interaction, taken from a project with a long-term goal to create an autonomous robot intelligence that can collaborate with remotely located human participants on exploration and navigation tasks. In the initial versions, a human “Commander” tasks the robot verbally, and gets feedback via multiple modalities, including text messages, a live 2Dmap built from the robot’s LIDAR scanner, and still photos captured from the robot’s front-facing camera. In order to collect sufficient information about the type of language used by a Commander (Marge et al., 2017), and provide training data to support development of appropriate language processing components, the development of the autonomous human-robot interaction begins with a series of “Wizard of Oz” experiments (Marge et al., 2016; Bonial et al., 2017), where the robot is controlled by two wizards, with an internal communication floor, distinct from the floor used by the Commander to communicate with the robot. The wizards include a Dialogue Manager (DMWizard, or DM) who handles communication to the Commander and “speaks” via text messages (Bonial et al., 2017) and a Robot Navigator (RN-Wizard, or"
L18-1017,passonneau-2006-measuring,0,0.130273,"Missing"
L18-1017,W15-0210,0,0.0252823,"ced. Keywords: dialogue structure annotation, human-robot interaction, multiparty dialogue 1. Introduction We present an annotation scheme for meso-level dialogue structure (Traum and Nakatani, 1999), specifically designed for multi-floor dialogue. The scheme includes both a transaction unit for clustering utterances from multiple participants and floors that contribute to realization of an initiating participant’s intent, and relations between individual utterances within the unit. While there are standard annotation schemes for both dialogue acts (Bunt et al., 2012) and discourse relations (Prasad and Bunt, 2015), these schemes do not fully address the issues of dialogue structure. Of particular interest to us, and not previously addressed in other schemes, are cases in which the units and relations span across multiple conversational floors. Dialogues can be characterized by distinct information states (Traum and Larsson, 2003). These include sets of participants, participant roles (e.g. active, ratified participant vs. overhearer), turn-taking or floor-holding, expectation of how many participants will make substantial contributions at a time (Edelsky, 1981), and other factors. Often distinct dialog"
L18-1017,W99-0313,1,0.538189,"ation of an initiator’s intent, and relations between individual utterances within the unit. We apply this scheme to annotate a corpus of multi-floor human-robot interaction dialogues. We examine the patterns of structure observed in these dialogues and present inter-annotator statistics and relative frequencies of types of relations and transaction units. Finally, some example applications of these annotations are introduced. Keywords: dialogue structure annotation, human-robot interaction, multiparty dialogue 1. Introduction We present an annotation scheme for meso-level dialogue structure (Traum and Nakatani, 1999), specifically designed for multi-floor dialogue. The scheme includes both a transaction unit for clustering utterances from multiple participants and floors that contribute to realization of an initiating participant’s intent, and relations between individual utterances within the unit. While there are standard annotation schemes for both dialogue acts (Bunt et al., 2012) and discourse relations (Prasad and Bunt, 2015), these schemes do not fully address the issues of dialogue structure. Of particular interest to us, and not previously addressed in other schemes, are cases in which the units"
laoudi-etal-2006-task,E06-1032,0,\N,Missing
laoudi-etal-2006-task,levin-etal-2000-lessons,0,\N,Missing
laoudi-etal-2006-task,taylor-white-1998-predicting,0,\N,Missing
laoudi-etal-2006-task,N03-2021,0,\N,Missing
laoudi-etal-2006-task,P02-1040,0,\N,Missing
N16-3015,P15-1056,1,0.400174,"ructured data in different modalities (e.g., texts, images and videos) is posted online for ready viewing. Complex event extraction and recommendation is critical for many information distillation tasks, including tracking current events, providing alerts, and predicting possible changes, as related to topics of ongoing concern. State-of-the-art Information Extraction (IE) technologies focus on extracting events from a single data modality and ignore cross-media fusion. More importantly, users are presented with extracted events in a passive way (e.g., in a temporally ordered event chronicle (Ge et al., 2015)). Such technologies do not leverage user behavior to identify the event 1 The system demo is available at: http://nlp.cs. rpi.edu/multimedia/event/navigation_dark. html properties of interest to them in selecting new scenarios for presentation. In this paper we present a novel event extraction and recommendation system that incorporates advances in extracting events across multiple sources with data in diverse modalities and so yields a more comprehensive understanding of collective events, their importance, and their inter-connections. The novel capabilities of our system include: • Event Ex"
N16-3015,P12-1038,0,0.0307608,"of sentences and apply spectral clustering to find several clustering centers (i.e., representative sentences including the most important phrases) as the summary. The user is also provided two options to show the original documents and the document containing the summary. 4.2 Visual Information Extraction For each event, we retrieve the most representative video/image online using the key-phrases such as date and entities as queries. Videos and images are often more impressive and efficient at conveying information. We first apply a pretrained convolutional neural network (CNN) architecture (Kuznetsova et al., 2012) to extract visual concepts from each video key frame based on the EventNet concept library (Ye et al., 2015). For example, the extracted visual concepts “crowd on street, riot, demonstration or protest, people marching” appear when the user’s mouse is over the video of the primary event (Figure 3). Then we adopt the approach described in (Li et al., 2015) which applies CNN and association rule mining technique to generate visual patterns and extract semantically meaningful relations between visual and textual information to name the patterns. Figure 3: Recommendation Interface. 5 6 Conclusion"
N16-3015,D14-1198,1,0.888744,"Missing"
N19-4019,P14-5010,0,0.00631396,"Missing"
N19-4019,P17-1178,1,0.823224,", Movement, Business, Conflict, Contact, Manufacture, Personnel, Justice, Transaction, Government, Inspection, Existence Entity Table 1: Main types of knowledge elements Multilingual Knowledge Extraction not adopt the alternative approach of translating the source documents into English and then applying English knowledge extraction system due to the low-quality of state-of-the-art machine translation and word alignment for these two languages. The overall architecture of our multilingual knowledge extraction system is illustrated in Figure 1. The system performs entity discovery and linking (Pan et al., 2017; Lin et al., 2018), time expression extraction and normalization (Manning et al., 2014), relation extraction (Shi et al., 2018), event extraction (Zhang et al., 2017, 2019), and event coreference (Zhang et al., 2015). The system supports the extraction of 7 entity types, 23 relations, and 47 event types, as defined in the DARPA AIDA ontology.4 Table 1 shows the main types. For Russian and Ukrainian text input, we did 4 https://www.darpa.mil/program/ active-interpretation-of-disparate-alternatives 111 Once within-document knowledge elements for each language are extracted, the system performs"
N19-4019,I17-2072,0,0.0222216,"Missing"
N19-4019,D18-1125,1,0.8884,"Missing"
N19-4019,I17-1037,1,0.88047,"Missing"
N19-4019,D15-1020,1,0.85196,"the alternative approach of translating the source documents into English and then applying English knowledge extraction system due to the low-quality of state-of-the-art machine translation and word alignment for these two languages. The overall architecture of our multilingual knowledge extraction system is illustrated in Figure 1. The system performs entity discovery and linking (Pan et al., 2017; Lin et al., 2018), time expression extraction and normalization (Manning et al., 2014), relation extraction (Shi et al., 2018), event extraction (Zhang et al., 2017, 2019), and event coreference (Zhang et al., 2015). The system supports the extraction of 7 entity types, 23 relations, and 47 event types, as defined in the DARPA AIDA ontology.4 Table 1 shows the main types. For Russian and Ukrainian text input, we did 4 https://www.darpa.mil/program/ active-interpretation-of-disparate-alternatives 111 Once within-document knowledge elements for each language are extracted, the system performs cross-lingual entity linking to Wikipedia, crossdocument entity clustering for unlinkable mentions, and cross-document event coreference resolution for cross-lingual information fusion. Further details of each compone"
N19-4023,P16-4012,0,0.264776,"Missing"
N19-4023,P18-4016,1,0.876761,"LT 2019: Demonstrations, pages 132–137 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Dialogue Processing Robotic Behaviors Robot(s) Environment Exp 1 (Marge et al., 2016) wizard + typing wizard + joystick 1 physical indoors + real building Exp 2 (Bonial et al., 2017) wizard + button presses wizard + joystick 1 physical indoors + real building Exp 3 completed 2018 wizard + button presses wizard + joystick 1 simulated indoors + sim building Exp 4 ongoing data collection ASR + auto-DM wizard + joystick 1 simulated indoors + sim building ScoutBot (Lukin et al., 2018) ASR + auto-DM finite state machine 1 simulated indoors + sim building MultiBot current ASR + auto-DM auto-assign via TBS 2 simulated outdoors + sim buildings Table 1: Testing scenarios over time. Columns depict progression of testing scenario experimentation and development; rows represent scenario components (DM: Dialogue Management; TBS: tactical behavior specification) research platform. We needed a testing scenario with a coherent, structured narrative involving a ground and an aerial robot demonstrating new behavior sequences as the software components were being developed. To develop Mu"
P16-1025,C96-1079,0,0.793923,"ccording to predefined event types. The extraction quality of new event types is also promising. 1 E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster represents a type. We rely on distributi"
P16-1025,P03-2030,0,0.054262,"er and argument representations are then passed to a joint constraint clustering framework. Finally, we name each cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Temporal Spatial Relations ARG0, ARG1, ARG2, ARG3, ARG4 mod, location, poss, manner, topic, medium, inst"
P16-1025,W13-2322,0,0.0832222,"over vectors in the embedding space. Argument representations are generated as a by-product. Trigger and argument representations are then passed to a joint constraint clustering framework. Finally, we name each cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Tempora"
P16-1025,P13-1088,0,0.0611697,"Missing"
P16-1025,N07-4013,0,0.0723425,"Missing"
P16-1025,P11-1113,0,0.0548102,"r = O(Ccurr , Ccurr ) if Ocurr < Omin T A ∗ Omin = Ocurr , C T = Ccurr , C A = Ccurr – while iterate time ≤ 10 T T A ∗ Ccurr = spectral(T, EgT , ER , KT , Ccurr ) A A T ∗ Ccurr = spectral(A, Eg , KA , Ccurr ) T A ∗ Ocurr = O(Ccurr , Ccurr ) ∗ if Ocurr < Omin T A · Omin =Ocurr , C T = Ccurr , C A = Ccurr Table 4: None-Core Role Mapping. compare the impact of perfect AMR and system generated AMR. To compare with state-of-the-art event extraction on Automatic Content Extraction (ACE2005) data, we follow the same evaluation setting in previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and use 40 newswire documents as our test set. • return Omin , C T , C A ; 3.2 AMR Core Role fire.1 ARG0 fire.1 ARG1 extrude.1 ARG0 extrude.1 ARG1 extrude.1 ARG2 blood.1 ARG0 blood.1 ARG1 Table 5 compares the coverage of event schema discovered by our approach, using AMR as meaning representation, with the predefined ACE and ERE event schemas. Besides the types defined in ACE and ERE, this approach discovers many new event types such as Build and Threaten as displayed in Figure 6. Our approach can also discover new argument roles for a given event type. For example, for Attack events, besides"
P16-1025,P08-1004,0,0.117222,"Missing"
P16-1025,E06-1018,0,0.0144014,"Missing"
P16-1025,N06-2015,0,0.0484569,"sed on Hypothesis 1, we learn sense-based embeddings from a large data set, using the Continuous Skip-gram model (Mikolov et al., 2013). Specifically, we first apply WSD to link each word to its sense in WordNet using a state-of-the-art tool (Zhong and Ng, 2010), and map WordNet sense output to OntoNotes senses. 4 We map each trigger candidate to its OntoNotes sense and learn a distinct embedding for each sense. We use general lexical embeddings for arguments. Candidate Trigger and Argument Identification Given a sentence, we consider all noun and verb concepts that are assigned an OntoNotes (Hovy et al., 2006) sense by WSD as candidate event triggers. Any remaining concepts that match both a verbal and a nominal lexical unit in the FrameNet corpus are considered candidate event triggers as well. This mainly helps to identify more nominal triggers like “pickpocket” and “sin”.2 For each candidate event trigger, we consider as candidate arguments all concepts for which one of a manually-selected set of semantic relations holds between it and the event trigger. For the setting in which AMR serves as our meaning representation, we selected a subset of all AMR relations that specify event arguments, as s"
P16-1025,P08-1030,1,0.847292,"KT ) A A Ccurr = spectral(A, Eg , KA ) T A Ocurr = O(Ccurr , Ccurr ) if Ocurr < Omin T A ∗ Omin = Ocurr , C T = Ccurr , C A = Ccurr – while iterate time ≤ 10 T T A ∗ Ccurr = spectral(T, EgT , ER , KT , Ccurr ) A A T ∗ Ccurr = spectral(A, Eg , KA , Ccurr ) T A ∗ Ocurr = O(Ccurr , Ccurr ) ∗ if Ocurr < Omin T A · Omin =Ocurr , C T = Ccurr , C A = Ccurr Table 4: None-Core Role Mapping. compare the impact of perfect AMR and system generated AMR. To compare with state-of-the-art event extraction on Automatic Content Extraction (ACE2005) data, we follow the same evaluation setting in previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and use 40 newswire documents as our test set. • return Omin , C T , C A ; 3.2 AMR Core Role fire.1 ARG0 fire.1 ARG1 extrude.1 ARG0 extrude.1 ARG1 extrude.1 ARG2 blood.1 ARG0 blood.1 ARG1 Table 5 compares the coverage of event schema discovered by our approach, using AMR as meaning representation, with the predefined ACE and ERE event schemas. Besides the types defined in ACE and ERE, this approach discovers many new event types such as Build and Threaten as displayed in Figure 6. Our approach can also discover new argument roles for a given event"
P16-1025,E09-1013,0,0.0124643,"Missing"
P16-1025,P11-1115,1,0.769669,"E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster represents a type. We rely on distributional similarity for our clustering distance metric. The distributional hypothesis (Harris, 1"
P16-1025,P11-1098,0,0.270072,"Missing"
P16-1025,D13-1185,0,0.0229869,"Missing"
P16-1025,C12-1033,0,0.0174421,"Missing"
P16-1025,P13-1008,1,0.853732,"Missing"
P16-1025,P15-1017,0,0.247756,"Missing"
P16-1025,P10-1081,0,0.0298077,"ral(A, Eg , KA ) T A Ocurr = O(Ccurr , Ccurr ) if Ocurr < Omin T A ∗ Omin = Ocurr , C T = Ccurr , C A = Ccurr – while iterate time ≤ 10 T T A ∗ Ccurr = spectral(T, EgT , ER , KT , Ccurr ) A A T ∗ Ccurr = spectral(A, Eg , KA , Ccurr ) T A ∗ Ocurr = O(Ccurr , Ccurr ) ∗ if Ocurr < Omin T A · Omin =Ocurr , C T = Ccurr , C A = Ccurr Table 4: None-Core Role Mapping. compare the impact of perfect AMR and system generated AMR. To compare with state-of-the-art event extraction on Automatic Content Extraction (ACE2005) data, we follow the same evaluation setting in previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and use 40 newswire documents as our test set. • return Omin , C T , C A ; 3.2 AMR Core Role fire.1 ARG0 fire.1 ARG1 extrude.1 ARG0 extrude.1 ARG1 extrude.1 ARG2 blood.1 ARG0 blood.1 ARG1 Table 5 compares the coverage of event schema discovered by our approach, using AMR as meaning representation, with the predefined ACE and ERE event schemas. Besides the types defined in ACE and ERE, this approach discovers many new event types such as Build and Threaten as displayed in Figure 6. Our approach can also discover new argument roles for a given event type. For example, for At"
P16-1025,J14-1002,0,0.0287898,"n the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Temporal Spatial Relations ARG0, ARG1, ARG2, ARG3, ARG4 mod, location, poss, manner, topic, medium, instrument, duration, prep-X year, duration, decade, weekday, time destination, path, location Table 2: Event-Related AMR Relations. In E1, for example, “killed”, “injured” and “fighting” ar"
P16-1025,C10-2087,0,0.399724,"Missing"
P16-1025,N06-1039,0,0.0802104,"Missing"
P16-1025,de-marneffe-etal-2006-generating,0,0.102939,"Missing"
P16-1025,D11-1014,0,0.0551092,"Reconstruct: (X’ga,Y’l)=Z1W’mod+b’ T Z1=fmod(Wmod,Xga,Yl)=X gaWmodYl+b lose Figure 4: Partial AMR and Event Structure for E2. E(VI , VO ) = 1 ||VI − VO ||2 2 For each pair of words X and Y , the reconstruction error back-propagates from its output layer to input layer through parameters Θr = 0 0 (Wr , br , Wr , br ). Let δO be the residual error of the output layer, and δH be the error of the hidden layer: 0 O lated words for the event trigger with sense “lose1” and construct the event structure for the whole event, as shown in Figure 4. We design a Tensor based Recursive AutoEncoder (TRAE) (Socher et al., 2011) framework to utilize a tensor based composition function for each of a subset of the AMR semantic relations and compose the event structure representation based on multiple functional applications. This subset was manually selected by the authors as the set of relations that link a trigger to concepts that help to determine its type. Similarly, we selected a subset of dependency and FrameNet relations using the same criteria for experiments using those meaning representations. Figure 4 shows an instance of a TRAE applied to an event structure to generate its representation. For each semantic"
P16-1025,P11-1163,0,0.0337219,"Missing"
P16-1025,D09-1013,0,0.086859,"Missing"
P16-1025,D13-1170,0,0.00276702,"Missing"
P16-1025,W03-0419,0,0.104049,"he extraction quality of new event types is also promising. 1 E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster represents a type. We rely on distributional similarity for our clu"
P16-1025,P11-1148,0,0.0424776,"Missing"
P16-1025,P15-2060,0,0.0603427,"Missing"
P16-1025,P15-2141,0,0.0160239,"ach cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Temporal Spatial Relations ARG0, ARG1, ARG2, ARG3, ARG4 mod, location, poss, manner, topic, medium, instrument, duration, prep-X year, duration, decade, weekday, time destination, path, location Table 2: Event-"
P16-1025,P15-1019,0,0.0988925,"Missing"
P16-1025,Q15-1005,0,0.0118071,"ach cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning representation we allow all frame relations to identify arguments. For dependencies, we manually mapped dependency relations to AMR relations and use Table 2. Categories Core roles Non-core roles Temporal Spatial Relations ARG0, ARG1, ARG2, ARG3, ARG4 mod, location, poss, manner, topic, medium, instrument, duration, prep-X year, duration, decade, weekday, time destination, path, location Table 2: Event-"
P16-1025,X93-1013,0,0.717501,"a large amount of data labeled according to predefined event types. The extraction quality of new event types is also promising. 1 E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster repre"
P16-1025,W12-3022,0,0.0652492,"Missing"
P16-1025,J05-1004,0,0.0379358,"ts. For each event trigger, we apply a series of compositional functions to generate that trigger’s event structure representation. Each function is specific to a semantic relation, and operates over vectors in the embedding space. Argument representations are generated as a by-product. Trigger and argument representations are then passed to a joint constraint clustering framework. Finally, we name each cluster of triggers, and name each trigger’s arguments using mappings between the meaning representation and semantic role descriptions in FrameNet, VerbNet (Kipper et al., 2008) and Propbank (Palmer et al., 2005). We compare settings in which semantic relations connecting triggers to context words are derived from three meaning representations: Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Stanford Typed Dependencies (Marie-Catherine et al., 2006), and FrameNet (Baker and Sato, 2003). We derive semantic relations automatically for these three representations using CAMR (Wang et al., 2015a), Stanford’s dependency parser (Manning, 2003), and SEMAFOR (Das et al., 2014), respectively. 2.2 which is used to combine multiple sentences into one AMR graph.3 When FrameNet is the meaning repres"
P16-1025,P12-1059,0,0.056824,"Missing"
P16-1025,S07-1096,0,0.0753062,"Missing"
P16-1025,P10-4014,0,0.0661832,"e 2: Event-Related AMR Relations. In E1, for example, “killed”, “injured” and “fighting” are identified as candidate triggers, and three concept sets are identified as candidate arguments using AMR relations: “{Two Soldiers, very large missile}”, “{one, Kut}” and “{Two Soldiers, Kut}”, as shown in Figure 3. 2.3 Trigger Sense and Argument Representation Based on Hypothesis 1, we learn sense-based embeddings from a large data set, using the Continuous Skip-gram model (Mikolov et al., 2013). Specifically, we first apply WSD to link each word to its sense in WordNet using a state-of-the-art tool (Zhong and Ng, 2010), and map WordNet sense output to OntoNotes senses. 4 We map each trigger candidate to its OntoNotes sense and learn a distinct embedding for each sense. We use general lexical embeddings for arguments. Candidate Trigger and Argument Identification Given a sentence, we consider all noun and verb concepts that are assigned an OntoNotes (Hovy et al., 2006) sense by WSD as candidate event triggers. Any remaining concepts that match both a verbal and a nominal lexical unit in the FrameNet corpus are considered candidate event triggers as well. This mainly helps to identify more nominal triggers li"
P16-1025,W11-1901,0,0.0129852,"y of new event types is also promising. 1 E1. Two Soldiers were killed and one injured in the close-quarters fighting in Kut. E2. Bill Bennet’s glam gambling loss changed my opinion. Introduction E3. Gen. Vincent Brooks announced the capture of Barzan Ibrahim Hasan al-Tikriti, telling reporters he was an adviser to Saddam. Event extraction aims at identifying and typing trigger words and participants (arguments). It remains a challenging and costly task. The first question is what to extract? The TIPSTER (Onyshkevych et al., 1993), MUC (Grishman and Sundheim, 1996), CoNLL (Tjong et al., 2003; Pradhan et al., 2011), ACE 1 and TACKBP (Ji and Grishman, 2011) programs found that it was feasible to manually define an event schema based on the needs of potential users. An ACE event schema example is shown in Figure 1. This process is very expensive because consumers and 1 E4. This was the Italian ship that was captured by Palestinian terrorists back in 1985. E5. Ayman Sabawi Ibrahim was arrested in Tikrit and was sentenced to life in prison. We seek to cluster the event triggers and event arguments so that each cluster represents a type. We rely on distributional similarity for our clustering distance metric"
P16-1025,D11-1001,0,0.0699272,"Missing"
P16-1025,P06-2094,0,0.0223593,"Missing"
P16-1025,Q14-1017,0,\N,Missing
P16-1025,S10-1011,0,\N,Missing
P18-1201,W13-2322,0,0.048347,"for Computational Linguistics (Long Papers), pages 2160–2170 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Event Mention Example: dispatching is the trigger of a Transport-Person event with four arguments: the solid lines show the event annotations for the sentence while the dotted lines show the Abstract Meaning Representation parsing output. patching is the trigger for the event mention of type Transport Person and in E2, conflict is the trigger for the event mention of type Attack. We make use of Abstract Meaning Representations (AMR) (Banarescu et al., 2013) to identify the candidate arguments and construct event mention structures as shown in Figure 2 (top). Figure 2 (bottom) also shows event type structures defined in the Automatic Content Extraction (ACE) guideline.2 We can see that a trigger and its event type name usually have some shared meaning. Furthermore, their structures also tend to be similar: a Transport Person event typically involves a Person as its patient role, while an Attack event involves a Person or Location as an Attacker. This observation matches the theory by Pustejovsky (1991): “the semantics of an event structure can be"
P18-1201,N07-4013,0,0.12327,"Missing"
P18-1201,P08-1004,0,0.122949,"Missing"
P18-1201,P15-2061,1,0.915193,"Missing"
P18-1201,P15-1017,0,0.661906,"Missing"
P18-1201,P16-2011,1,0.921764,"Missing"
P18-1201,C16-1017,0,0.0883192,"Missing"
P18-1201,P11-1163,0,0.389091,"Missing"
P18-1201,P11-1113,0,0.557271,"Missing"
P18-1201,P16-1025,1,0.911858,"Missing"
P18-1201,D09-1013,0,0.0959611,"Missing"
P18-1201,N16-1034,1,0.915997,"Missing"
P18-1201,P08-1030,1,0.902064,"Missing"
P18-1201,P15-2060,0,0.36822,"Missing"
P18-1201,P11-1115,1,0.767935,"Missing"
P18-1201,K17-1034,0,0.0861971,"Missing"
P18-1201,P13-1008,1,0.94998,"Missing"
P18-1201,P10-1081,0,0.668857,"Missing"
P18-1201,C10-2087,0,0.0604086,"Missing"
P18-1201,D16-1038,0,0.0734626,"Missing"
P18-1201,D16-1087,0,0.0557832,"Missing"
P18-1201,D11-1001,1,0.900064,"Missing"
P18-1201,P16-1201,0,0.0472642,"Missing"
P18-1201,P06-2094,0,0.0941745,"Missing"
P18-1201,P12-1088,0,0.128376,"Missing"
P18-1201,N06-1039,0,0.0621108,"Missing"
P18-1201,D13-1170,0,0.00421772,"xtraction as a classification problem, by assigning event triggers to event types from a pre-defined fixed set. These methods rely heavily on manual annotations and features specific to each event type, and thus are not easily adapted to new event types without extra annotation effort. Handling new event types may even entail starting over, without being able to re-use annotations from previous event types. To make event extraction effective as new realworld scenarios emerge, we take a look at this task from the perspective of zero-shot learning, ZSL (Frome et al., 2013; Norouzi et al., 2013; Socher et al., 2013a). ZSL, as a type of transfer learning, makes use of separate, pre-existing classifiers to build a semantic, cross-concept space that maps between their respective classes. The resulting shared semantic space then allows for building a novel “zero-shot” classifier, i,e,, requiring no (zero) additional training examples, to handle unseen cases. We observe that each event mention has a structure consisting of a candidate trigger and arguments, with corresponding predefined name labels for the event type and argument roles. We propose to enrich the semantic representations of each event mention"
P18-1201,W15-0812,0,0.0620811,"Missing"
P18-1201,N15-1040,0,0.0767342,"Missing"
P18-1201,P10-4014,0,0.0766931,"Missing"
P18-4016,W15-4629,1,0.822936,"sponse of “Moving..” vs “Turning...” as seen in #4 and #7 in Figure 2. VHMSG includes several protocols that implement parts of the Virtual Human architecture. We use the protocols for speech recognition, as well as component monitoring and logging. The NPCEditor and other components that use these protocols are available as part of the ICT Virtual Human Toolkit (Hartholt et al., 2013). These protocols have also been used in systems, as reported by Hill et al. (2003) and Traum et al. (2012, 2015). In particular, we used the adaptation of Google’s Automated Speech Recognition (ASR) API used in Traum et al. (2015). The NPCEditor (Leuski and Traum, 2011) was used for Natural Language Understanding (NLU) and dialogue management. The new ros2vhmsg component for bridging the messages was used to send instructions from the NPCEditor to the automated RN. 3.1.1 NPCEditor We implemented NLU using the statistical text classifier included in the NPCEditor. The classifier learns a mapping from inputs to outputs from training data using cross-language retrieval models (Leuski and Traum, 2011). The dialogues collected from our first two experimental phases served as training data, and consisted of 1,500 pairs of Co"
P18-4016,L18-1017,1,0.460343,"versations are linked together. Rather than just responding to input in a single conversation, the DM-Wizard in our first project phases often translates input from one conversational floor to another (e.g., from the Commander to the RNWizard, or visa versa), or responds to input with messages to both the Commander and the RN. These responses need to be consistent (e.g. translating a command to the RN should be accompanied by positive feedback to the Commander, while a clarification to the commander should not include an RN action command). Using the dialogue relation annotations described in Traum et al. (2018), we trained a hybrid classifier, including translations to RN if they existed, and negative feedback to the Commander where they did not. We also created a new dialogue manager policy that would accompany RN commands with appropriate positive feedback to the commander, e.g., response of “Moving..” vs “Turning...” as seen in #4 and #7 in Figure 2. VHMSG includes several protocols that implement parts of the Virtual Human architecture. We use the protocols for speech recognition, as well as component monitoring and logging. The NPCEditor and other components that use these protocols are availab"
voss-etal-2014-finding,J96-1002,0,\N,Missing
voss-etal-2014-finding,habash-etal-2012-conventional,0,\N,Missing
voss-etal-2014-finding,W13-2317,1,\N,Missing
voss-etal-2014-finding,elfardy-diab-2012-simplified,0,\N,Missing
W00-0501,taylor-white-1998-predicting,0,0.138482,"a Department of Defense Ft. Meade, MD cjvanes@ afterlife.ncsc.mil independent of the content of other documents in the processing collection. When Church and Hovy (1993) introduced the notion that &quot;crummy&quot; MT engines could be put to good use on tasks less-demanding than publication-quality translation, MT research efforts did not typically evaluate system performance in the context of specific tasks. (Sparck Jones and Galliers, 1996). In the last few years, however, the Church and Hovy insight has led to innovative experiments, like those reported by Resnik (1997), Pomarede et al. (1998), and Taylor and White (1998), using task-based evaluation methods. Most recently, research on task-based evaluation has been. proposed within TIDES, a recent DARPA initiative whose goals include enabling Englishspeaking individuals to access, correlate, and interpret multilingual sources of information (DARPA, 1999; Harmon, 1999). This paper introduces a method of assessing when an embedded MT system is &quot;good enough&quot; for the filtering of hard-copy foreign language (FL) documents by individuals with no knowledge of that language. We describe preliminary work developing measures on system-internal components that assess: ("
W08-0511,elkateb-etal-2006-building,0,0.066151,"Missing"
W13-2317,N07-2043,1,0.762549,"’s dataset includes no Darija, we collected Darija examples from the following sources to augment their dataset: Moroccan jokes from noktazwina. com, web pages collected using Darija-speciﬁc query terms with a popular search engine, and 37,538 Arabic script commentary entries from hespress.com (a Moroccan news website). Nearly all the joke (N=399) and query term (N=874) data contained Darija. By contrast, the commentary data was mostly MSA. To extract a subset of the commentary entries most likely to contain Darija, we applied an iterative, semisupervised approach similar to that described by Tratz and Sanﬁlippo (2007), in which the joke and query term data were treated as initial seeds and, in each iteration, a small portion of commentary data with the highest Darija scores were added to the training set. After having run this process to its completion, we examined 131 examples at intervals of 45 from the resulting ranked list of commentary. The 62nd example was the ﬁrst of these to have been incorrectly classiﬁed as containing Darija. We thus elected to assume all examples up to the 61st of the 131 contain Darija, for a total of 2,745 examples (61*45=2,745). As an additional check, we examined two more co"
W13-2317,vatanen-etal-2010-language,0,0.0451471,"Missing"
W13-2317,J96-1002,0,0.0233231,"Missing"
W13-2317,W12-2108,0,0.0672677,"minantly Romanized. With more data, we plan to assess the impact of one user’s script and language choice on others. 5 Conclusion and Future Work The DAT OOL now supports semi-automated annotation of tweet conversations for Darija. As we scale the process of building low-resource language corpora, we will document its impact on annotation time when few native speakers are available, a condition also relevant and critical to preserving endangered languages. We have begun extending the classiﬁer to support additional Arabic script languages (e.g., Farsi, Urdu), leveraging resources from others (Bergsma et al., 2012). Many other open questions remain regarding the annotation process, the visualizations, and the human expert. Which classiﬁed examples should the language expert review? When should an annotator adjust the conﬁdence threshold in the DAT OOL? For deeper linguistic analysis and codeswitching prediction, would seeing participants and tweets, turn by turn, in network diagrams such as Figure 4 help experts understand new patterns emerging in tweet conversations? Figure 3: Information network visualization. Red—contains Romanized Darija; green— contains Arabic-script Darija; blue—no Darija. Code-Sw"
W13-2317,J14-1006,0,\N,Missing
W14-0207,eberhard-etal-2010-indiana,0,0.0694061,"Missing"
W14-0207,stoia-etal-2008-scare,0,0.0282883,"es to ResearchCyc’s semantic predicates to assess its lexical coverage for our corpora. 4 recording similar data modalities do exist (Green et al., 2006; Wienke et al., 2012; Maas et al., 2006) but for fundamentally different tasks. Tellex et al. (2011) and Matuszek et al. (2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; MacMahon et al., 2006; Vogel and Jurafsky, 2010) or referring expressions without routes (Sch¨utte et al., 2010; Fang et al., 2013), assuming pre-existing abstractions from sensor data. Image Processing Interval corpus images were labelled by a neural network trained for visual scene classification (Munoz, 2013) of nine material classes: dirt, foliage, grass, road,"
W14-0207,D13-1038,0,0.0196454,"i et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; MacMahon et al., 2006; Vogel and Jurafsky, 2010) or referring expressions without routes (Sch¨utte et al., 2010; Fang et al., 2013), assuming pre-existing abstractions from sensor data. Image Processing Interval corpus images were labelled by a neural network trained for visual scene classification (Munoz, 2013) of nine material classes: dirt, foliage, grass, road, sidewalk, sky, wall, wood, and ground cover (organic debris). Figures 4 and 5 show the images from Figures 1 and 2 with two additional versions: one with colored zones for system-recognized class boundaries and another with colored zones as trasparent overlays on the original. The classes differentiate terrain types that work well with route-finding techniques"
W14-0207,P10-1083,0,0.0297823,"(2012) pair commands with formal plans without dialog and Zender et al. (2008) and Randelli et al. (2013) build multi-level maps but with a situated commander. Eberhard et al. (2010)’s CReST corpus contains a set-up similar to ours minus the robot; a human task-solver wears a forward-facing camera instead. The SCARE corpus (Stoia et al., 2008) records similar modalities but in a virtual environment, where C has full access to R’s video feed. Other projects yielded corpora from virtual environments that include route descriptions without dialog (Marge and Rudnicky, 2011; MacMahon et al., 2006; Vogel and Jurafsky, 2010) or referring expressions without routes (Sch¨utte et al., 2010; Fang et al., 2013), assuming pre-existing abstractions from sensor data. Image Processing Interval corpus images were labelled by a neural network trained for visual scene classification (Munoz, 2013) of nine material classes: dirt, foliage, grass, road, sidewalk, sky, wall, wood, and ground cover (organic debris). Figures 4 and 5 show the images from Figures 1 and 2 with two additional versions: one with colored zones for system-recognized class boundaries and another with colored zones as trasparent overlays on the original. Th"
W14-0207,P10-3011,0,0.0148214,"n the robot’s location. For example, the latest interval in a task’s time sequence that was constructed with the robot being outside a building is distinct from the first interval that covers when the robot moves inside the building.5 Figure 3: Analyses of Scan next open room on left. Our next step is to augment SLURP’s lexicon and retrain a parser for new vocabulary so that we can directly map semantic structures of the prepilot corpora into ResearchCyc8 , an extensive ontology, for cross-reference to other events and objects, already stored and possibly originated as visual input. Following McFate (2010), we will test 6 https://github.com/PennNLP/SLURP. Verbnet associates each frame with a conjunction of boolean semantic predicates that specify how and when event participants interact, for an event variable (not shown). 8 ResearchCyc and CycL are trademarks of Cycorp, Inc. 7 5 This appears likely due to the paced descriptions in R’s utterances. Another pre-pilot is needed to test this hypothesis. 45 Figure 4: Outside View: Image, Zones, Overlay Figure 5: Inside View: Image, Zones, Overlay. Brightness and contrast of video image and overlay increased for print publication. the mapping of match"
W14-0207,green-etal-2006-developing,0,\N,Missing
W14-1008,P06-1023,0,0.0257518,"or several languages, including English, Chinese, Korean, and Hindi. Empty categories are nodes in a parse tree that do not correspond to any written morpheme; these are used to handle several linguistic phenomena, including prodrop. Recent research demonstrates that recovery of empty categories can lead to improved translation quality for some language pairs (Chung and Gildea, 2010; Xiang et al., 2013). For more information on the recovery of empty categories, we refer the interested reader to work by Kukkadapu and Mannem (2013), Cai et al. (2011), Yang and Xue (2010), Gabbard et al. (2006), Schmid (2006), Dienes and Dubey (2003), and Johnson (2002). Future Work Going forward, we plan to experiment with applying our resumptive pronoun identifier to enhance MT performance, likely by deleting all resumptive pronouns during alignment and, again, at translation time. Another natural next step is to train the system using automatically generated parse, partof-speech tag, and clitic segmentation information instead of gold standard annotation to see if this produces a similar drop in performance. We also plan to investigate the use of frame information of Arabic VerbNet (Mousser, 2010) as features,"
W14-1008,P11-2037,0,0.0173265,"relevant to ours is the work on identifying empty categories for several languages, including English, Chinese, Korean, and Hindi. Empty categories are nodes in a parse tree that do not correspond to any written morpheme; these are used to handle several linguistic phenomena, including prodrop. Recent research demonstrates that recovery of empty categories can lead to improved translation quality for some language pairs (Chung and Gildea, 2010; Xiang et al., 2013). For more information on the recovery of empty categories, we refer the interested reader to work by Kukkadapu and Mannem (2013), Cai et al. (2011), Yang and Xue (2010), Gabbard et al. (2006), Schmid (2006), Dienes and Dubey (2003), and Johnson (2002). Future Work Going forward, we plan to experiment with applying our resumptive pronoun identifier to enhance MT performance, likely by deleting all resumptive pronouns during alignment and, again, at translation time. Another natural next step is to train the system using automatically generated parse, partof-speech tag, and clitic segmentation information instead of gold standard annotation to see if this produces a similar drop in performance. We also plan to investigate the use of frame"
W14-1008,D10-1062,0,0.0142577,"on the development and test sets for gold standard clitic separation/POS tagging/parsing and automatic preprocessing. Related Work The computational linguistics research most relevant to ours is the work on identifying empty categories for several languages, including English, Chinese, Korean, and Hindi. Empty categories are nodes in a parse tree that do not correspond to any written morpheme; these are used to handle several linguistic phenomena, including prodrop. Recent research demonstrates that recovery of empty categories can lead to improved translation quality for some language pairs (Chung and Gildea, 2010; Xiang et al., 2013). For more information on the recovery of empty categories, we refer the interested reader to work by Kukkadapu and Mannem (2013), Cai et al. (2011), Yang and Xue (2010), Gabbard et al. (2006), Schmid (2006), Dienes and Dubey (2003), and Johnson (2002). Future Work Going forward, we plan to experiment with applying our resumptive pronoun identifier to enhance MT performance, likely by deleting all resumptive pronouns during alignment and, again, at translation time. Another natural next step is to train the system using automatically generated parse, partof-speech tag, and"
W14-1008,W13-4904,1,0.861769,"fourth challenge is that MSA has no equivalent word for the English word ‘whose’ and, to convey a similar meaning, employs resumptive pronouns as possessive modifiers. Examples illustrating these differences are provided in Table 1. For further background on MSA relative clauses and MSA grammar, we refer readers to books by Ryding (2005) and Badawi et al. (2004). Data In our research, we rely on the conversion of constituent into dependency structures and the training/dev/test splits of the Arabic Treebank (ATB) parts 1, 2, & 3 (Maamouri et al., 2004; Maamouri and Bies, 2004) as presented by Tratz (2013). We extract features from labeled dependency trees (rather than constituent trees) generated by Tratz’s (2013) Arabic NLP system, which separates clitics, labels parts-of-speech, produces dependency parses, and identifies and labels affixes. The original ATB dependency conversion does not mark pronouns for resumptiveness, so we modify the conversion process to obtain this information. The original ATB constituent trees mark this by labeling WHNP nodes and NP nodes with identical indices. If the NP node corresponds to a null subject and the head of the S under the SBAR is a verb, we mark the i"
W14-1008,W02-1001,0,0.0214045,"Missing"
W14-1008,P13-1081,0,0.0471886,"nk data when using gold standard parses and automatic parses, respectively. To the best of our knowledge, this is the first attempt to automatically identify resumptive pronouns in any language. Introduction One of the challenges for modern machine translation (MT) is the need to systematically insert or delete information that is overtly expressed in only one of the languages in order to maintain intelligibility and/or fluency. For example, word alignment between pro-drop and non-prodrop languages can be negatively impacted by the systematic dropping of pronouns in only one of the languages (Xiang et al., 2013). A similar type of linguistic phenomenon of great interest to linguists that has not yet received significant attention in MT research is the mismatch between languages in their usage of resumptive pronouns. Some languages, such as Modern Standard Arabic (MSA), 2 Relevant MSA Linguistics MSA and English relative clauses differ in structure, with one of the most prominent differences being in regard to resumptive pronouns. Resumptive pronouns are required in many MSA relative clauses but are almost never grammatical in English. In MSA, like English, if the external 42 Proceedings of the 3rd Wo"
W14-1008,W03-1005,0,0.0362004,"uages, including English, Chinese, Korean, and Hindi. Empty categories are nodes in a parse tree that do not correspond to any written morpheme; these are used to handle several linguistic phenomena, including prodrop. Recent research demonstrates that recovery of empty categories can lead to improved translation quality for some language pairs (Chung and Gildea, 2010; Xiang et al., 2013). For more information on the recovery of empty categories, we refer the interested reader to work by Kukkadapu and Mannem (2013), Cai et al. (2011), Yang and Xue (2010), Gabbard et al. (2006), Schmid (2006), Dienes and Dubey (2003), and Johnson (2002). Future Work Going forward, we plan to experiment with applying our resumptive pronoun identifier to enhance MT performance, likely by deleting all resumptive pronouns during alignment and, again, at translation time. Another natural next step is to train the system using automatically generated parse, partof-speech tag, and clitic segmentation information instead of gold standard annotation to see if this produces a similar drop in performance. We also plan to investigate the use of frame information of Arabic VerbNet (Mousser, 2010) as features, and we would like to focu"
W14-1008,C10-2158,0,0.0210708,"s the work on identifying empty categories for several languages, including English, Chinese, Korean, and Hindi. Empty categories are nodes in a parse tree that do not correspond to any written morpheme; these are used to handle several linguistic phenomena, including prodrop. Recent research demonstrates that recovery of empty categories can lead to improved translation quality for some language pairs (Chung and Gildea, 2010; Xiang et al., 2013). For more information on the recovery of empty categories, we refer the interested reader to work by Kukkadapu and Mannem (2013), Cai et al. (2011), Yang and Xue (2010), Gabbard et al. (2006), Schmid (2006), Dienes and Dubey (2003), and Johnson (2002). Future Work Going forward, we plan to experiment with applying our resumptive pronoun identifier to enhance MT performance, likely by deleting all resumptive pronouns during alignment and, again, at translation time. Another natural next step is to train the system using automatically generated parse, partof-speech tag, and clitic segmentation information instead of gold standard annotation to see if this produces a similar drop in performance. We also plan to investigate the use of frame information of Arabic"
W14-1008,N06-1024,0,0.0234792,"ying empty categories for several languages, including English, Chinese, Korean, and Hindi. Empty categories are nodes in a parse tree that do not correspond to any written morpheme; these are used to handle several linguistic phenomena, including prodrop. Recent research demonstrates that recovery of empty categories can lead to improved translation quality for some language pairs (Chung and Gildea, 2010; Xiang et al., 2013). For more information on the recovery of empty categories, we refer the interested reader to work by Kukkadapu and Mannem (2013), Cai et al. (2011), Yang and Xue (2010), Gabbard et al. (2006), Schmid (2006), Dienes and Dubey (2003), and Johnson (2002). Future Work Going forward, we plan to experiment with applying our resumptive pronoun identifier to enhance MT performance, likely by deleting all resumptive pronouns during alignment and, again, at translation time. Another natural next step is to train the system using automatically generated parse, partof-speech tag, and clitic segmentation information instead of gold standard annotation to see if this produces a similar drop in performance. We also plan to investigate the use of frame information of Arabic VerbNet (Mousser, 2010"
W14-1008,N10-1115,0,0.0198304,"ic annotation, are presented in Table 4. The system performs well when given input with gold standard clitic segmentation, POS tags, and dependency parses, achieving 91.9 F1 for resumptive pronouns on the test set and 95.4 F1 for the affixes. Performance however degrades substantially when automatic pre-processing of the source is input instead. Some of this drop can be explained by the use of gold standard markup in training—more weight was likely assigned to For space reasons, we omit a review of the training procedure for the structured perceptron and refer the interested reader to work by Goldberg and Elhadad (2010). 6 Occasionally an imperfect verb will have both a written inflectional prefix and a written inflectional suffix. For these cases, the system only considers the prefix as there is no need to make two separate judgments. 45 Dev Gold Auto Gold Auto P 92.5 88.0 92.1 83.6 Pronoun R 92.8 81.0 91.7 72.8 F1 92.6 84.4 91.9 77.8 Inflectional Affix P R F1 96.7 96.4 96.5 86.1 77.3 81.5 95.0 95.9 95.4 86.6 76.0 81.0 8 Conclusion parse and POS tag-related features than would have if automatic pre-processing of the source had been used in training. Having examined the classification system errors on the de"
W14-1008,P02-1018,0,0.0171163,"nese, Korean, and Hindi. Empty categories are nodes in a parse tree that do not correspond to any written morpheme; these are used to handle several linguistic phenomena, including prodrop. Recent research demonstrates that recovery of empty categories can lead to improved translation quality for some language pairs (Chung and Gildea, 2010; Xiang et al., 2013). For more information on the recovery of empty categories, we refer the interested reader to work by Kukkadapu and Mannem (2013), Cai et al. (2011), Yang and Xue (2010), Gabbard et al. (2006), Schmid (2006), Dienes and Dubey (2003), and Johnson (2002). Future Work Going forward, we plan to experiment with applying our resumptive pronoun identifier to enhance MT performance, likely by deleting all resumptive pronouns during alignment and, again, at translation time. Another natural next step is to train the system using automatically generated parse, partof-speech tag, and clitic segmentation information instead of gold standard annotation to see if this produces a similar drop in performance. We also plan to investigate the use of frame information of Arabic VerbNet (Mousser, 2010) as features, and we would like to focus in greater detail"
W14-1008,W13-4911,0,0.0157131,"nal linguistics research most relevant to ours is the work on identifying empty categories for several languages, including English, Chinese, Korean, and Hindi. Empty categories are nodes in a parse tree that do not correspond to any written morpheme; these are used to handle several linguistic phenomena, including prodrop. Recent research demonstrates that recovery of empty categories can lead to improved translation quality for some language pairs (Chung and Gildea, 2010; Xiang et al., 2013). For more information on the recovery of empty categories, we refer the interested reader to work by Kukkadapu and Mannem (2013), Cai et al. (2011), Yang and Xue (2010), Gabbard et al. (2006), Schmid (2006), Dienes and Dubey (2003), and Johnson (2002). Future Work Going forward, we plan to experiment with applying our resumptive pronoun identifier to enhance MT performance, likely by deleting all resumptive pronouns during alignment and, again, at translation time. Another natural next step is to train the system using automatically generated parse, partof-speech tag, and clitic segmentation information instead of gold standard annotation to see if this produces a similar drop in performance. We also plan to investigat"
W14-1008,W04-1602,0,0.0265454,"pronouns for introducing the clause3 . A fourth challenge is that MSA has no equivalent word for the English word ‘whose’ and, to convey a similar meaning, employs resumptive pronouns as possessive modifiers. Examples illustrating these differences are provided in Table 1. For further background on MSA relative clauses and MSA grammar, we refer readers to books by Ryding (2005) and Badawi et al. (2004). Data In our research, we rely on the conversion of constituent into dependency structures and the training/dev/test splits of the Arabic Treebank (ATB) parts 1, 2, & 3 (Maamouri et al., 2004; Maamouri and Bies, 2004) as presented by Tratz (2013). We extract features from labeled dependency trees (rather than constituent trees) generated by Tratz’s (2013) Arabic NLP system, which separates clitics, labels parts-of-speech, produces dependency parses, and identifies and labels affixes. The original ATB dependency conversion does not mark pronouns for resumptiveness, so we modify the conversion process to obtain this information. The original ATB constituent trees mark this by labeling WHNP nodes and NP nodes with identical indices. If the NP node corresponds to a null subject and the head of the S under the"
W14-1008,mousser-2010-large,0,0.0702682,"Missing"
W14-5402,W11-0905,0,0.0203881,"ry content as contextual knowledge, and automatically generating captions describing the image content as relevant for the story. For “text to scene” processing, a robot “understanding” a commander’s language entails going beyond linguistic semantic interpretation down to the the robot controller level, as in, for example, Kress-Gazit et al. (2008). Within computational linguistics, Srihari and Burhans (1994) tackled going from text to images, exploiting the conventions and spatial language in news caption to identify people 10 by their relative positions in accompanying images. More recently Coyne et al. (2011) presented work for text-to-graphics generation, grounding conceptual knowledge in relational semantic encoding of lexical meanings from FrameNet. These one-way, directional approaches provide strong evidence that text and image modalities can each inform the processing of the other, and that, with concurrent audio and video streaming data, the alignment of time-stamped files across the two data modalities should also yield additional benefits in shared structural analyses and disambiguating references.1 3 Approach In previous work, we had teams search a series of buildings, where all informat"
W14-5402,kordjamshidi-etal-2010-spatial,0,0.035998,"Missing"
W14-5402,W14-4302,0,0.0164983,"an both generation and understanding of spatial language. There exists a large literature on spatial language, starting several decades ago (Talmy, 1983; Anderson et al., 1991; Gurney et al., 1996; Bloom et al., 1996; Olivier and Gapp, 1998) inter alia. This work yielded linguistic insights into the underlying structure of spatial expressions, that has led more recently to annotation efforts like SpaceML (Morarescu, 2006) and spatial role labeling (Kordjamshidi et al., 2010). These results, theoretical and computational, have been incorporated into NLP research, such as spoken dialog systems (Meena et al., 2014). For “scene to text” processing, starting from a robot’s perception of the scene or environment, exploiting even known dependencies among objects (spatial relations, relative motion, etc.) is a central problem in computer vision research. In the current state of robotics, the perceived world (a.k.a. semantic perception) derived from data collected by the robot is limited by what is available within its immediate sensor and video reach (Hebert et al., 2012). Within computational linguistic research, (Feng and Lapata, 2013) have tackled going from news images to text, leveraging the news story"
W14-5402,morarescu-2006-principles,0,0.0247344,"The second is “text to scene”: given natural language instructions, how do people move about in new locations, and how does that impact their expectations of robot navigation? These issues span both generation and understanding of spatial language. There exists a large literature on spatial language, starting several decades ago (Talmy, 1983; Anderson et al., 1991; Gurney et al., 1996; Bloom et al., 1996; Olivier and Gapp, 1998) inter alia. This work yielded linguistic insights into the underlying structure of spatial expressions, that has led more recently to annotation efforts like SpaceML (Morarescu, 2006) and spatial role labeling (Kordjamshidi et al., 2010). These results, theoretical and computational, have been incorporated into NLP research, such as spoken dialog systems (Meena et al., 2014). For “scene to text” processing, starting from a robot’s perception of the scene or environment, exploiting even known dependencies among objects (spatial relations, relative motion, etc.) is a central problem in computer vision research. In the current state of robotics, the perceived world (a.k.a. semantic perception) derived from data collected by the robot is limited by what is available within its"
W14-5402,W14-0207,1,\N,Missing
W17-2808,Q13-1016,0,0.0662557,"the methodology of corpus-based robotics (Bugmann et al., 2004), where some natural language, primarily route instructions, is collected. Route instruction interpreters dating back to M ARCO (MacMahon, 2006), and more recently the robotic forklift (Tellex et al., 2011) and Tactical Behavior Specification grammar (Hemachandra et al., 2015; Boularias et al., 2016), rely on these initial route instructions to learn mappings to robot-executable procedures like path planning. Additionally, some use semantic parsers (e.g., (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Krishnamurthy and Kollar, 2013)) or translation (Matuszek et al., 2010) to map natural language to actions. A gap in these works is bi-directional dialogue interaction, specifically cases where initial instructions are not well-formed and need additional clarification, or when participants grow to better grasp the robot’s capabilities, varying instruction strategies over time. Our work collected instructions to a robot, but also included the dialogue and followon responses needed to establish or build common ground. This paper focused on analyzing initial robot-directed instructions, leaving analysis of responses during the"
W17-2808,Q13-1005,0,0.0154904,"to natural language interpretation for robots follow the methodology of corpus-based robotics (Bugmann et al., 2004), where some natural language, primarily route instructions, is collected. Route instruction interpreters dating back to M ARCO (MacMahon, 2006), and more recently the robotic forklift (Tellex et al., 2011) and Tactical Behavior Specification grammar (Hemachandra et al., 2015; Boularias et al., 2016), rely on these initial route instructions to learn mappings to robot-executable procedures like path planning. Additionally, some use semantic parsers (e.g., (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Krishnamurthy and Kollar, 2013)) or translation (Matuszek et al., 2010) to map natural language to actions. A gap in these works is bi-directional dialogue interaction, specifically cases where initial instructions are not well-formed and need additional clarification, or when participants grow to better grasp the robot’s capabilities, varying instruction strategies over time. Our work collected instructions to a robot, but also included the dialogue and followon responses needed to establish or build common ground. This paper focused on analyzing initial robot-directe"
W17-2808,W10-4328,1,0.77268,"s shown in Figure 5. command:send-image. Others require additional parameters to fully specify the complete action. Of particular interest to us is the information that participants chose to include in robot-directed motion requests. We focused on command:drive and command:rotate for variation in how participants communicated. We annotated motion-command parameters for their usage of metric (e.g., move forward 2 feet; turn left 90 degrees) and landmarkbased points of reference (e.g., move to the table; turn to face the doorway) similar to absolute and relative steps in route instructions from Marge and Rudnicky (2010). 3.2.3 Dialogue-Moves To analyze internal IU structure, we annotated Commander-issued lower-level dialogue-moves. This annotation scheme is inspired by a prior approach to military dialogue that identified dialogue-moves in calls for artillery fire (Roque et al., 2006). Examples of a command type request are command:drive or command:rotate, that instruct the robot to perform certain motions. A dialogue-move list is provided in Appendix B. Three annotators independently validated the dialogue-move set on 99 dialogue turns in our human-robot dialogue corpus. Annotators had high agreement (α = 0"
W17-2808,J97-1002,0,0.437754,"ial1 , Ashley Foots1 , Cory Hayes1 , Cassidy Henry1 , Kimberly A. Pollard1 , Ron Artstein2 , Clare R. Voss1 , and David Traum2 1 U.S. 2 USC Army Research Laboratory, Adelphi, MD 20783 Institute for Creative Technologies, Playa Vista, CA 90094 matthew.r.marge.civ@mail.mil Abstract ulate robot intelligence and actions without participant awareness. With ten participants, we collected ten hours of human-robot dialogue. We are currently undertaking corpus curation and plan to make the data freely available in the next year. In this experiment, a human and robot engage in a series of transactions (Carletta et al., 1997) where an instruction is issued, and wizards acting on behalf of the robot either perform a task or prompt for clarification until the requested task is completed or abandoned. We propose a new term, instruction unit (IU), to identify all commands within a transaction issued before the robot generates a response. IUs were analyzed both in structure and variation. Our findings suggest a general, initial preference for including metric information over landmarks in motion commands, but this decreased over time. Results will assist in future work adapting robot responses to varied instruction sty"
W17-2808,passonneau-2006-measuring,0,0.0754703,"e, we annotated Commander-issued lower-level dialogue-moves. This annotation scheme is inspired by a prior approach to military dialogue that identified dialogue-moves in calls for artillery fire (Roque et al., 2006). Examples of a command type request are command:drive or command:rotate, that instruct the robot to perform certain motions. A dialogue-move list is provided in Appendix B. Three annotators independently validated the dialogue-move set on 99 dialogue turns in our human-robot dialogue corpus. Annotators had high agreement (α = 0.92; Krippendorf’s α using the MASI distance measure (Passonneau, 2006)). 3.3 Participants This study recruited ten participants: two female, eight male. Ages ranged from 28 to 58 (mean = 44, s.d. = 10.6). Two participants reported one year or less of robotics research; others reported none.1 1 We collected measures that were included in our statistical analysis but not presented in this paper. The Spatial Orientation Survey, part of the Guilford-Zimmerman Aptitude Survey (Guilford and Zimmerman, 1948), assesses spatial orientation perception. The HRI Trust Survey (Schaefer, 2013) measures subjective trust of the robot, based upon personal belief of the robot’s c"
W17-2808,P08-1073,0,0.0405719,"angement), the environment is sparsely filled with objects and is not in a finished state. These were practical limitations of laboratory resources, but in future work we plan to explore the effects of the environment further by varying it in a fully simulated version of the experiment. 5.2 6.1 By far the WOz method’s most common use has been for handling natural language (Riek, 2012). Many studies use a wizard in automated dialogue system development (e.g., in virtual agent negotiation (Gandhe and Traum, 2007), time-offset storytelling (Artstein et al., 2015), and in-car personal assistants (Rieser and Lemon, 2008)). Some researchers have considered a multiwizard setup for multimodal interfaces. The SimSensei project (DeVault et al., 2014) used a twowizard setup during the development stage; one controlling the virtual agent’s verbal behaviors and another the non-verbal behaviors. Green et al. (2004) investigated using multiple wizards for dialogue processing and navigation capabilities for a robot in a home touring scenario, finding the multi-wizard approach effective when the robot and human were co-present. Dialogue-Move Types We found that most IUs contained command dialogue-moves, but with some exc"
W17-2808,eberhard-etal-2010-indiana,0,0.0441354,"Missing"
W17-2808,stoia-etal-2008-scare,0,0.324368,"Missing"
W17-2808,gargett-etal-2010-give,0,\N,Missing
W18-1408,N03-1013,1,0.732872,"ed on the surface. For example, in the LCS Class of Verbs of inherently directed motion (corresponding to Class 51.1.a in (Levin, 1993)), the verb leave can take a NP complement (as in leave the room) and the verb depart can take a PP complement (as in departed from the room). For either case, the spatial component of meaning is uniformly move to a position outside of the room. Whereas the collocations were derived from thematic roles in the original LVD, the spatial components of meaning were derived from verbprepositions pairs associated with a subset of the “Categorial Variation” database (Habash and Dorr, 2003). Representative members of LCS classes were then paired with prepositions that were propagated to other members of the class. Table 1 summarizes the number of LCS classes associated with the lexical notions introduced above (Blocks, Overlaps, Fills-Oblig, Fills-Opt).4 Not all LCS classes are spatial in nature; thus, the second column provides a tally for the full set of LCS classes, and the third column provides a tally for just the spatial subset. The fourth column presents the number of spatial verbs included in the corresponding spatial classes. Representative spatial examples are provided"
W18-1408,habash-dorr-2002-handling,1,0.79309,"take these structures to capture language-bound meanings, that is semantic forms. In our framework, these do not, despite their name, capture language-independent, conceptual knowledge. 63 Proceedings of the First International Workshop on Spatial Language Understanding (SpLU-2018), pages 63–70 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics to its compositional, lexicon-based formalism and its potential for follow-on work in other language processing applications for which cross-lingual LCS mappings have already been devised (e.g., machine translation (Habash and Dorr, 2002)). We assume second, that for human-robot natural-language mediated communication, a number of constraints at the syntax-semantics interface are crucial for interpreting the wide ranging flexibility of real utterances and the context of the system is central to dialogue management. We leverage previously collected dialogue data with naturally occurring spoken Bot Language (Marge et al., 2017) that provides transcripts and dialog analyses (Traum et al., 2018), but without any form of lexical semantics. We assume third, that we will test and validate our approach by augmenting an implemented dia"
W18-1408,2006.amta-papers.7,1,0.65256,"ork and concluding remarks. 2 Approach This section introduces the notion of LCS and describes an LCS-based approach to systematic derivation of usage patterns for understanding and generation. We extend an LCS resource to include constraints (blocks, overlaps, and fills) and present the upshot of these extensions. 2.1 Lexical Conceptual Structure Lexical Conceptual Structure (LCS) (Jackendoff, 1983, 1990; Dorr, 1993; Dowty, 1979; Guerssel et al., 1985) has been used for a range of different applications, including interlingual machine translation (Habash and Dorr, 2002), lexical acquisition (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). The LCS representation was introduced by Jackendoff as based in the spatial domain and naturally extended to non-spatial domains, as specified by fields.3 For example, the spatial dimension of the LCS representation corresponds to the (Loc)ational field, which underlies the meaning of John traveled from Chicago to Boston in the LCS [John GOLoc [From Chicago] [To Boston]]. This is straightforwardly extended to the (Temp)oral field to represen"
W18-1408,1997.mtsummit-workshop.4,1,0.459919,"Missing"
W18-1408,kordjamshidi-etal-2010-spatial,0,0.0755491,"Missing"
W18-1408,W17-7415,0,0.190644,"th for hearing and for producing utterances for robot-robot communication. However, the position adopted here is one in which generalizations about language structure are assumed and available in natural language generation for both use (“lift up”) and suppression (“elevate”) of spatial prepositions in phrases containing motion and direction verbs, depending on the context. guage resources and standards for capturing spatial information. For example, the ISO 24617 standard provides guidelines for annotating spatial information in English language texts (246177, 2014) that continues to evolve (Pustejovsky and Lee, 2017). This Semantic Annotation Framework (semAF) identifies places, paths, spatial entities, and spatial relations that can be used to associate sequences of processes and events in news articles (Pustejovsky et al., 2011). Spatial prepositions and particles (such as near, off ) and verbs of position and movement (such as lean, swim) in text have corresponding spatial components of meanings, collocations, and classes of spatial verbs in the perspective adopted in this paper. Spatial role labeling using holistic spatial semantics (i.e., analysis at the level of the full utterance) has been used for"
W18-1408,W18-1503,1,0.822678,"cs. We assume third, that we will test and validate our approach by augmenting an implemented dialogue system for understanding and generation of Bot Language. The application of our foundational paradigm to this problem is a future direction outside of the scope of this position paper. The layered lexical representations referred to in the first assumption above form the basis for this discussion. Specifically, we posit that the development of an application such as robot navigation (Bonial et al., 2018; Moolchandani et al., 2018) or generation of narrative explanations (Korpan et al., 2017; Lukin et al., 2018) requires a layered representation scheme to include a set of spatial primitives (the basis for the LCS representation) coupled with a representation of constraints at the syntax-semantics interface. Additional layers include prepositional collocates2 and spatial semantics that are crucial for understanding and production of unconstrained spatial expressions. We describe our extensions to an LCS resource covering 500 semantic classes of verbs, of which 219 fall within a spatial subset. We demonstrate that this resource is designed to systematically account for certain types of spatial expressi"
W18-1408,W17-2808,1,0.717052,"tional, lexicon-based formalism and its potential for follow-on work in other language processing applications for which cross-lingual LCS mappings have already been devised (e.g., machine translation (Habash and Dorr, 2002)). We assume second, that for human-robot natural-language mediated communication, a number of constraints at the syntax-semantics interface are crucial for interpreting the wide ranging flexibility of real utterances and the context of the system is central to dialogue management. We leverage previously collected dialogue data with naturally occurring spoken Bot Language (Marge et al., 2017) that provides transcripts and dialog analyses (Traum et al., 2018), but without any form of lexical semantics. We assume third, that we will test and validate our approach by augmenting an implemented dialogue system for understanding and generation of Bot Language. The application of our foundational paradigm to this problem is a future direction outside of the scope of this position paper. The layered lexical representations referred to in the first assumption above form the basis for this discussion. Specifically, we posit that the development of an application such as robot navigation (Bo"
W18-1408,L18-1017,1,0.706712,"k in other language processing applications for which cross-lingual LCS mappings have already been devised (e.g., machine translation (Habash and Dorr, 2002)). We assume second, that for human-robot natural-language mediated communication, a number of constraints at the syntax-semantics interface are crucial for interpreting the wide ranging flexibility of real utterances and the context of the system is central to dialogue management. We leverage previously collected dialogue data with naturally occurring spoken Bot Language (Marge et al., 2017) that provides transcripts and dialog analyses (Traum et al., 2018), but without any form of lexical semantics. We assume third, that we will test and validate our approach by augmenting an implemented dialogue system for understanding and generation of Bot Language. The application of our foundational paradigm to this problem is a future direction outside of the scope of this position paper. The layered lexical representations referred to in the first assumption above form the basis for this discussion. Specifically, we posit that the development of an application such as robot navigation (Bonial et al., 2018; Moolchandani et al., 2018) or generation of narr"
W18-1408,W00-0207,0,0.556603,"based approach to systematic derivation of usage patterns for understanding and generation. We extend an LCS resource to include constraints (blocks, overlaps, and fills) and present the upshot of these extensions. 2.1 Lexical Conceptual Structure Lexical Conceptual Structure (LCS) (Jackendoff, 1983, 1990; Dorr, 1993; Dowty, 1979; Guerssel et al., 1985) has been used for a range of different applications, including interlingual machine translation (Habash and Dorr, 2002), lexical acquisition (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). The LCS representation was introduced by Jackendoff as based in the spatial domain and naturally extended to non-spatial domains, as specified by fields.3 For example, the spatial dimension of the LCS representation corresponds to the (Loc)ational field, which underlies the meaning of John traveled from Chicago to Boston in the LCS [John GOLoc [From Chicago] [To Boston]]. This is straightforwardly extended to the (Temp)oral field to represent analogous meanings such as The meeting went from 7pm to 9pm in the LCS [Meeting GOTemp [From 7pm] [To 9"
W18-1408,N15-3006,0,0.0135928,"ructures with layers, and those semantic structures contain primitives that are grounded at a conceptual level (not discussed herein). We leverage Lexical Conceptual Structure (LCS) (Jackendoff, 1983; Dorr, 1993), a logical representation with compositional properties, to guide development of semantics for spatial language in language understanding and generation.1 We note that other logical representations may also be adequate for this study, e.g., Abstract Meaning Representation (Banarescu et al., 2014), Prague Dependency Trees (Hajiˇc et al., 2018), and descendants of such representations (Vanderwende et al., 2015). LCS has been selected due Introduction While prior work in spatial language understanding for tasks such as robot navigation focuses on mapping natural language into deep conceptual or non-linguistic representations—for further reasoning or embodied cognition (Perera et al., 2017; Pastra et al., 2011)—we argue that it is possible to systematically derive regular patterns of language usage from existing lexical-semantic resources (Dorr et al., 2001). Furthermore, even with access to such resources, effective solutions to many application areas such as robot navigation and narrative generation"
W18-1503,N16-1098,0,\N,Missing
W18-1921,2015.iwslt-papers.18,0,0.113297,"Missing"
W18-1921,J09-3007,0,0.0246341,"mily), Canada, Mexico (Nahuatl, UtoAztecan family), and Central Chile (Mapudungun, Araucanian), as well as in Australia (Nunggubuyu, Macro-Gunwinyguan family), Northeastern Siberia (Chukchi and Koryak, both from the Chukotko-Kamchatkan family), and India (Sora, Munda family), as shown in the map below (Figure 1). 2 https://www.nytimes.com/2017/10/04/world/africa/special-forces-killed-niger.html Abbreviations follow the Leipzig Glossing Rules; additional glosses are spelled out in full. 4 In fact, the majority of the languages spoken in the world today are endangered and disappearing fast (See Bird, 2009). Estimates are that, of the approximately 7000 languages in the world today, at least one disappears every day (https://www.ethnologue.com). 3 Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018 |Page 284 Figure 1: Polysynthetic Languages5 Although there are many definitions of polysynthesis, there is often confusion on what constitutes the exact criteria and phenomena (Mithun 2017). Even authoritative sources categorize languages in conflicting ways.6 Typically, polysynthetic languages demonstrate holophrasis, i.e. the ability of an entire sentence to be expressed i"
W18-1921,P86-1019,1,0.256019,"both theoretical and practical reasons, as discussed more fully in the next section. On the theoretical side, these languages offer a potentially unique window into human cognition and language capabilities as well as into language acquisition (Mithun 1989; Greenberg 1960; Comrie 1981; Fortescue 1994; Fortescue et al. 2017). On the practical side, they offer significant obstacles to accurate linguistic analysis as well as to computational modeling. 3. Some Computational Challenges of Polysynthetic Languages Polysynthetic languages pose unique challenges for traditional computational systems (Byrd et al. 1986). Even in allegedly cross-linguistic or typological analyses of specific phenomena, e.g. in forming a theory of clitics and cliticization (Klavans 1995), finding the full range of language types on which to test hypotheses proves difficult. Often, the data is simply not available so claims cannot neither refuted nor supported fully. One of the underlying causes of this difficulty is that there are many languages for which a clear lexical division between nouns and verbs has been challenged; these languages are characterized by a large class of roots that are used either nominally or verbally,"
W18-1921,W17-0102,0,0.0301653,"tal recurrent neural networks (Kong et al. 2015, Micher 2017) and byte-pair encoding Sennrich, Haddow, Birch 2015) to several challenging problems for polysynthetic language analysis and processing. For ASR, we have implemented adaptive learning for iterative ASR, incorporating principles from the Kaldi toolkit9 with modifications as required by different workflows and tasks. 5. Corpus Collection - Electronically-available resources Only recently have researchers started collecting well-designed corpora for polysynthetic languages, e.g. for Circassian (Arkhangelskiy & Lander 2016) or Arapaho (Kazeminejad et al. 2017). There is an urgent need for documentation, archiving, creation of corpora and teaching materials that are specific to polysynthetic languages. Documentation and corpus-building challenges arise for many languages, but the complex morphological makeup of polysynthetic languages makes consistent documentation particularly difficult. The more language data that is gathered and accurately analyzed, the deeper cross-linguistic analyses can be conducted which in turn will contribute to a range of fields including linguistic theory, language teaching and lexicography. For example, in examining cros"
W18-1921,W17-0106,0,0.0294692,"research-programs/material and LORELEI, http://www.darpa.mil/program/low-resource-languages-for-emergent-incidents, respectively. Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018 |Page 288 Concomitant with the collection and cataloging of corpora, we are working with colleagues especially from the the NSF-funded EL-STEC Shared Task Evaluation Campaign project13 on a future shared task in order to bring linguists and computational linguists together around the common area: accuracy in data analysis. We aim to formulate a shared task that meets the goals outlined in Levow, et al. (2017), namely, to “align the interests of the speech and language processing communities with those of … language documentation communities….”, guided by their design principles of realism, typological diversity, accessibility of the shared task, accessibility of the resulting software, extensibility and nuanced evaluation. 6. Future Research and Applications Our next steps involve a two-phase approach, one on the ASR input and then one on the MT side (as shown in Figure 2.) On the ASR side, we plan to use Multi-Task Learning (MTL) (Caruana 1997), using corpora from multiple languages. Multitask Le"
W18-1921,W17-0114,1,0.829109,"of each component and discuss further the novel methodological contributions of the research. Figure 2: Overview of Speech-MT Polysynthetic Language Architecture 8 https://www..gov/index.php/research-programs/babel Proceedings of AMTA 2018, vol. 2: MT Users' Track Boston, March 17 - 21, 2018 |Page 287 ARL has demonstrated leading technologies in the field with critical expertise. We are planning on developing systems, capable of performing speech translation. We are applying machine learning techniques using neural network approaches e.g. segmental recurrent neural networks (Kong et al. 2015, Micher 2017) and byte-pair encoding Sennrich, Haddow, Birch 2015) to several challenging problems for polysynthetic language analysis and processing. For ASR, we have implemented adaptive learning for iterative ASR, incorporating principles from the Kaldi toolkit9 with modifications as required by different workflows and tasks. 5. Corpus Collection - Electronically-available resources Only recently have researchers started collecting well-designed corpora for polysynthetic languages, e.g. for Circassian (Arkhangelskiy & Lander 2016) or Arapaho (Kazeminejad et al. 2017). There is an urgent need for documen"
W18-3808,W96-0306,1,0.633997,"to Boston in the LCS [John GOLoc [From Chicago] [To Boston]]. This is straightforwardly extended to the (Temp)oral field to represent analogous meanings such as The meeting went from 7pm to 9pm in the LCS [Meeting GOTemp [From 7pm] [To 9pm]]. The LVD developed in prior work (Dorr et al., 2001) includes a set of LCS templates classified according to an extension of Levin (1993)’s 192 classes to a total of 500 classes covering 9525 verb entries (an additional 5500+ verb entries beyond the original 4000+ verb entries). The first 44 classes were added beyond the original set of semantic classes (Dorr and Jones, 1996). The remaining classes were derived through aspectual distinctions to yield a set of LCS classes that were finer-grained than the original Levin classes (Olsen et al., 1997). Each LCS class consists of a set of verbs and, in several cases, the classes included non-Levin words (those not in Levin (1993)), derived semi-automatically (Dorr, 1997). The original LVD provides a mapping of lexical-semantic structures to their surface realization. This mapping serves as a foundation for the enrichments that yield STYLUS. The new resource benefits from decades of prior study that led to the LVD. Speci"
W18-3808,W18-1404,1,0.327075,"lsemantic structures of predicates and their syntactic argument structure. Subsequent work (Dorr and Voss, 2018) argued that regular patterns of language usage can be systematically derived from lexicalsemantic representations and used in applications such as dialogue management for robot navigation. The latter investigation focused on the spatial dimension, e.g., motion and direction. We adopt the view that this systematicity also holds for verbs in the non-spatial dimension, including those that have been metaphorically related to the spatial dimension by a range of corpus-based techniques (Dorr and Olsen, 2018). We consider one example of a non-spatial application: generation of cyber-related textual notifications. We argue that this application requires knowledge at the syntaxsemantics interface that is analogous to spatial knowledge for robot navigation. A recent survey of narrative generation techniques (Kybartas and Bidarra, 2017) highlights several important components of narrative generation (including a story, plot, space, and discourse for telling the story), leaving open the means for surface-realization of arguments from an underlying lexical-semantic structure. STYLUS is designed to accom"
W18-3808,W18-1408,1,0.583813,"ion and for non-spatial applications such as generation of cyber-related notifications. 1 Introduction* This paper presents a derivative resource, called STYLUS (SysTematicallY Derived Language USage), produced through extraction of a set of argument realizations from lexical-semantic representations for a range of different verb classes (Appendix A). Prior work (Jackendoff, 1996; Levin, 1993; Olsen, 1994; Kipper et al., 2008; Palmer et al., 2017) has suggested a close relation between underlying lexicalsemantic structures of predicates and their syntactic argument structure. Subsequent work (Dorr and Voss, 2018) argued that regular patterns of language usage can be systematically derived from lexicalsemantic representations and used in applications such as dialogue management for robot navigation. The latter investigation focused on the spatial dimension, e.g., motion and direction. We adopt the view that this systematicity also holds for verbs in the non-spatial dimension, including those that have been metaphorically related to the spatial dimension by a range of corpus-based techniques (Dorr and Olsen, 2018). We consider one example of a non-spatial application: generation of cyber-related textual"
W18-3808,habash-dorr-2002-handling,1,0.411777,"objects (Kordjamshidi et al., 2011). The association between thematic roles and their corresponding surface realizations has been investigated previously, including in the LCS formalism (described next), but Kordjamshidi et al’s approach also ties into deeper notions such as region of space and frame of reference. 2.2 Lexical-Conceptual Structure Verb Database (LVD) Lexical Conceptual Structure (LCS) (Jackendoff, 1983; Jackendoff, 1990; Dorr, 1993; Dowty, 1979; Guerssel et al., 1985) has been used for wide-ranging applications, including interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition (Habash et al., 2006), crosslanguage information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). LCS incorporates primitives whose combination captures syntactic generalities, i.e., actions and entities must be systematically related to a syntactic structure: GO, STAY, BE, GO-EXT, ORIENT, and also an ACT primitive developed by Dorr and Olsen (1997). LCS is grounded in the spatial domain and is naturally extended to non-spatial domains, as specified by fields. For example, the spatial dimension o"
W18-3808,N03-1013,1,0.86152,"Components of mean1 Teletype font is used for components of meaning such as UPWARD. Several examples throughout this paper were purposely selected to illustrate the full range of syntactic realizations for the concept of “upwardness.” Other verbs and collocations could easily have been selected (e.g., lower with the collocation down), but a varied selection of lexical distinctions would confound the illustration of more general distinctions at the syntax-semantics interface. 59 ing were derived either from the LCS structure or from verb-prepositions pairs in a “Categorial Variation” database (Habash and Dorr, 2003). For example, the LCS above (from the :LCS slot) includes a sublexical component of meaning, ((* from 3) loc (thing 2) (at loc (thing 2) (thing 4))), that maps optionally to a preposition from in the surface form (as in exit (from) the room). Prepositional collocations (such as from) were derived from the thematic roles (in the :THETA ROLES slot) of the original LVD. These prepositions are specified in parentheses, and are preceded either by a comma (,) for optional collocations or by underscore (_) for obligatory collocations. In the example above, the theme is obligatory (_th), whereas the"
W18-3808,2006.amta-papers.7,1,0.830834,"ociation between thematic roles and their corresponding surface realizations has been investigated previously, including in the LCS formalism (described next), but Kordjamshidi et al’s approach also ties into deeper notions such as region of space and frame of reference. 2.2 Lexical-Conceptual Structure Verb Database (LVD) Lexical Conceptual Structure (LCS) (Jackendoff, 1983; Jackendoff, 1990; Dorr, 1993; Dowty, 1979; Guerssel et al., 1985) has been used for wide-ranging applications, including interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition (Habash et al., 2006), crosslanguage information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). LCS incorporates primitives whose combination captures syntactic generalities, i.e., actions and entities must be systematically related to a syntactic structure: GO, STAY, BE, GO-EXT, ORIENT, and also an ACT primitive developed by Dorr and Olsen (1997). LCS is grounded in the spatial domain and is naturally extended to non-spatial domains, as specified by fields. For example, the spatial dimension of the LCS representation corresponds to the"
W18-3808,W04-2604,0,0.0622208,"e benefits from decades of prior study that led to the LVD. Specifically, Levin’s classes are based on significant corpus analysis and have been validated in numerous within-language studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998) and cross-language studies (Guerssel et al., 1985; Levin, 2015). Thus, STYLUS is expected to have an important downstream impact, in both depth and breadth, for future linguistic investigations and computational applications. 2.3 Syntax-Semantics Interface Prior work (Jackendoff, 1996; Levin, 1993; Dorr and Voss, 1993; Voss and Dorr, 1995; Kipper et al., 2004; Kipper et al., 2008; Palmer et al., 2017) suggests that there is a close relation between underlying lexical-semantic structures of verbs and nominal predicates and their syntactic argument structure. VerbNet (Kipper et al., 2004) reinforces the view in this resource paper, that prepositions and their relation with verb classes serve as significant predictors of semantic content, but does not leverage an inner structure of events for compositional derivation of argument realizations. FrameNet also sits at the syntax-semantics interface (Fillmore, 2002), with linking generalizations based on"
W18-3808,W17-2808,1,0.826349,"same language usage patterns as their robot navigation counterparts. 5 Conclusion and Future Work STYLUS provides the basis for both understanding and generation in the spatial robot navigation domain and for generation in the non-spatial cyber notification domain. A larger scale application and evaluation of the effectiveness of STYLUS for understanding and generation in these two domains is a 4 An asterisk at the start of a sentence indicates an invalid generated form. Verbs in the designated class are italicized. 61 future area of study. A starting point is an ongoing Bot Language project (Marge et al., 2017) that has heretofore focused on dialogue annotation (Traum et al., 2018) and has not yet incorporated lexiconbased knowledge necessary for automatically detecting incomplete, vague, or implicit navigation commands. Another avenue for exploration is the enhancement of cyber notifications through systematic derivation of mappings to surface realizations for other parts of speech. This work will involve access to a “Categorial Variation” database (CatVar) (Habash and Dorr, 2003) to map verbs in the LCS classes to their nominalized and adjectivalized forms. For example, the CatVar entry for infect"
W18-3808,1997.mtsummit-workshop.4,1,0.494672,"nt from 7pm to 9pm in the LCS [Meeting GOTemp [From 7pm] [To 9pm]]. The LVD developed in prior work (Dorr et al., 2001) includes a set of LCS templates classified according to an extension of Levin (1993)’s 192 classes to a total of 500 classes covering 9525 verb entries (an additional 5500+ verb entries beyond the original 4000+ verb entries). The first 44 classes were added beyond the original set of semantic classes (Dorr and Jones, 1996). The remaining classes were derived through aspectual distinctions to yield a set of LCS classes that were finer-grained than the original Levin classes (Olsen et al., 1997). Each LCS class consists of a set of verbs and, in several cases, the classes included non-Levin words (those not in Levin (1993)), derived semi-automatically (Dorr, 1997). The original LVD provides a mapping of lexical-semantic structures to their surface realization. This mapping serves as a foundation for the enrichments that yield STYLUS. The new resource benefits from decades of prior study that led to the LVD. Specifically, Levin’s classes are based on significant corpus analysis and have been validated in numerous within-language studies (Levin and Rappaport Hovav, 1995; Rappaport Hova"
W18-3808,W17-7415,0,0.0278837,"ext section reviews related work, starting with the spatial underpinnings of the original LVD. Following this, we describe our extensions and present examples for two applications: natural language processing for robot navigation and generation of cyber-related notifications. 2 2.1 Background Spatial Language Understanding Spatial language understanding has made great strides in recent years, with the emergence of language resources and standards for capturing spatial information, e.g., (ISO-24617-7, 2014), which provide guidelines for annotating spatial information in English language texts (Pustejovsky and Lee, 2017; * This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 57 Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing, pages 57–64 Santa Fe, New Mexico, USA, August 20, 2018. Pustejovsky and Yocum, 2014). This work differs from the perspective adopted for STYLUS in that it provides annotation guidelines for training systems for spatial information extraction, and so it does not focus on generalized mappings at the syntax-semantics interface. The Semantic Annotation Fra"
W18-3808,pustejovsky-yocum-2014-image,0,0.0284658,"atial language understanding has made great strides in recent years, with the emergence of language resources and standards for capturing spatial information, e.g., (ISO-24617-7, 2014), which provide guidelines for annotating spatial information in English language texts (Pustejovsky and Lee, 2017; * This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 57 Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing, pages 57–64 Santa Fe, New Mexico, USA, August 20, 2018. Pustejovsky and Yocum, 2014). This work differs from the perspective adopted for STYLUS in that it provides annotation guidelines for training systems for spatial information extraction, and so it does not focus on generalized mappings at the syntax-semantics interface. The Semantic Annotation Framework (semAF) identifies places, paths, spatial entities, and spatial relations that can be used to associate sequences of processes and events in news articles (Pustejovsky et al., 2011). Prepositions and particles (near, off) and verbs of position and movement (lean, swim) have corresponding components of meanings and colloca"
W18-3808,W00-0207,0,0.0639704,"eviously, including in the LCS formalism (described next), but Kordjamshidi et al’s approach also ties into deeper notions such as region of space and frame of reference. 2.2 Lexical-Conceptual Structure Verb Database (LVD) Lexical Conceptual Structure (LCS) (Jackendoff, 1983; Jackendoff, 1990; Dorr, 1993; Dowty, 1979; Guerssel et al., 1985) has been used for wide-ranging applications, including interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition (Habash et al., 2006), crosslanguage information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). LCS incorporates primitives whose combination captures syntactic generalities, i.e., actions and entities must be systematically related to a syntactic structure: GO, STAY, BE, GO-EXT, ORIENT, and also an ACT primitive developed by Dorr and Olsen (1997). LCS is grounded in the spatial domain and is naturally extended to non-spatial domains, as specified by fields. For example, the spatial dimension of the LCS representation corresponds to the (Loc)ational field, which underlies the meaning of John traveled from Chicago to Boston in the LCS [Joh"
W18-3808,L18-1017,1,0.831244,"onclusion and Future Work STYLUS provides the basis for both understanding and generation in the spatial robot navigation domain and for generation in the non-spatial cyber notification domain. A larger scale application and evaluation of the effectiveness of STYLUS for understanding and generation in these two domains is a 4 An asterisk at the start of a sentence indicates an invalid generated form. Verbs in the designated class are italicized. 61 future area of study. A starting point is an ongoing Bot Language project (Marge et al., 2017) that has heretofore focused on dialogue annotation (Traum et al., 2018) and has not yet incorporated lexiconbased knowledge necessary for automatically detecting incomplete, vague, or implicit navigation commands. Another avenue for exploration is the enhancement of cyber notifications through systematic derivation of mappings to surface realizations for other parts of speech. This work will involve access to a “Categorial Variation” database (CatVar) (Habash and Dorr, 2003) to map verbs in the LCS classes to their nominalized and adjectivalized forms. For example, the CatVar entry for infect includes the nominalized form infection, which provides additional opti"
W18-4307,D13-1149,1,0.839616,"and VN: events involving motion with a vehicle (VN meta-class 51.4.X), throwing events (VN 17.X), hitting events (VN 18.X), and human group motion events (VN 51.3.2). Intuitively, we felt that each of these events had some clear visual properties associated with the semantics of the type (i.e. fairly clear, distinct image-schemas (Lakoff, 1990)) 1 VerbNet version 3.2 is used: https://verbs.colorado.edu/verb-index Levin’s hypothesis continues to be debated, but efforts to crowdsource empirical evidence of the presence and saliency of the semantics in VN are promising (Hartshorne et al., 2014; Hartshorne et al., 2013). 2 56 One linguist and author of this paper, experienced with VN annotation, annotated each of the 101 UCF activity categories with an indication of whether or not the semantics of one of the four types was present in that activity. This was done by first completing a thorough review of the semantic representations found in the (meta-)classes for a given event type.3 Then, a sample of 10-12 videos from each UCF activity category were observed to get a sense of the nature of actions included in an activity, and the variability of the clips included under a single category.4 For each UCF catego"
W18-4307,P14-2065,1,0.846147,"some overlap between UCF and VN: events involving motion with a vehicle (VN meta-class 51.4.X), throwing events (VN 17.X), hitting events (VN 18.X), and human group motion events (VN 51.3.2). Intuitively, we felt that each of these events had some clear visual properties associated with the semantics of the type (i.e. fairly clear, distinct image-schemas (Lakoff, 1990)) 1 VerbNet version 3.2 is used: https://verbs.colorado.edu/verb-index Levin’s hypothesis continues to be debated, but efforts to crowdsource empirical evidence of the presence and saliency of the semantics in VN are promising (Hartshorne et al., 2014; Hartshorne et al., 2013). 2 56 One linguist and author of this paper, experienced with VN annotation, annotated each of the 101 UCF activity categories with an indication of whether or not the semantics of one of the four types was present in that activity. This was done by first completing a thorough review of the semantic representations found in the (meta-)classes for a given event type.3 Then, a sample of 10-12 videos from each UCF activity category were observed to get a sense of the nature of actions included in an activity, and the variability of the clips included under a single cate"
W18-4910,L18-1402,0,0.0229899,"that go beyond the tradition of relying on manually identifying expressions in terms of their syntactic and semantic idiosyncracies; instead, researchers have analyzed a wide range of actual usage patterns in texts (Fazly et al., 2009), and most recently using word embeddings (Peng and Feldman, 2017) and leveraging existing sources of idioms, such as in Wiktionary (Muzny and Zettlemoyer, 2013). Related research in idiom identification in non-English languages is also being conducted by native speakers of those languages, e.g., Hindi (Priyanka and Sinha, 2014), Italian (Vietri, 2014), Russian (Aharodnik et al., 2018), and Japanese (Hashimoto and Kawahara, 2008). 6.3 Constructicons Though constructicons are intended to be language-specific, they are based on the notion of continuity between the grammar and the lexicon in all languages (Fillmore, 2008). Resources for constructicons that we are aware of (Bäckström et al., 2014; Torrent et al., 2014; Forsberg et al., 2014; Ohara, 2016) take the following approach (similar to our own here): they begin by comparing English constructions to known constructions in another language of choice. Such comparison offers a baseline of constructions that may exist in the"
W18-4910,L16-1207,0,0.0244796,"plausible translations for The Xer the Y-er, these constructions certainly have distinctions in meaning and usage as well. Further analysis is required to determine those nuances and the extent to which The X-er the Y-er might be an adequate English translation of all three of these constructions. Thus, our analysis of MD translations of English constructions has led to three new potential candidates for a MD constructicon. 6 Related Work 6.1 Moroccan Darija There are a variety of other recent efforts to create computational resources for MD, not to mention other Arabic dialects. For example, Al-Shargi et al. (2016) create a morphologically annotated corpus of MD, which they use to train an automatic morphological analyzer, and Darwish et al. (2018) leverage a diacritized MD Bible to build a diacritization system for MD. Another area receiving attention from 10 FrameNet Constructicon: http://www.icsi.berkeley.edu/pubs/ai/framenetconstructicon11.pdf 81 English Source The less we feed him the more defiant he gets ﺷﺤﺎل ﻣﺎ ﻧﻘﺼﻨﺎ ﻟﻴﻪ ﻓﻲ اﻟﻤﺎﻛﻠﺔ وﻫﻮ ﺗﻴﺰﻳﺪ ﻳﺤﻴﺢ MD 1 Transliteration 1 chh’al ma nqass-na li-h fi al-makla w-huwa t-y-zid y-h’ayah’ Gloss 1 as_much_as that decreased-we to-him in the-food and-he (PR"
W18-4910,W13-2322,1,0.681577,"productively filled with a variety of different lexical items. Constructicons (inventories of constructions) have been developed in some languages as a resource to facilitate recognition, interpretation and translation of constructions (Fillmore et al., 2012; Torrent et al., 2014; Bäckström et al., 2014). Here, we provide a preliminary exploration of constructions that should be considered for an MD constructicon. A native speaker of MD and author of this paper examined instances of particular motion and degreerelated constructions drawn from the Abstract Meaning Representation (AMR) corpus (Banarescu et al., 2013; Bonial et al., 2018). We selected AMR instances of constructions because the corpus offers a data-driven (as opposed to seeking out instances of a construction; these are constructions that arose in sentence-by-sentence AMR annotation) variety of annotated examples, which also facilitates a sense of the relative frequency of particular constructions in a larger corpus. After the native speaker of MD examined these English examples, examples of what were thought to be instantiations of the same construction in MD were selected. In addition, translation was attempted of some of the English exa"
W18-4910,P13-1133,0,0.0350957,"detection system for MD, focusing on MD text written in Latin script. In addition to the aforementioned digital resources, there are a few print dictionaries for Moroccan, including a recent verb dictionary (El Haloui and Bowman, 2011) as well as an older dictionary in Latin script edited by Harrell and Sobelman (1966). The Moroccan Darija Wordnet (MDW) project (Mrini and Bond, 2017) is endeavoring to create a WordNet from the Harrell and Sobelman dictionary and link it with Princeton’s WordNet (Fellbaum, 1998). MDW will eventually be released as part of the Open Multilingual Wordnet project (Bond and Foster, 2013). 6.2 Idioms In the last decade, researchers have turned to the task of automatically detecting idioms in English texts, developing statistical measures that go beyond the tradition of relying on manually identifying expressions in terms of their syntactic and semantic idiosyncracies; instead, researchers have analyzed a wide range of actual usage patterns in texts (Fazly et al., 2009), and most recently using word embeddings (Peng and Feldman, 2017) and leveraging existing sources of idioms, such as in Wiktionary (Muzny and Zettlemoyer, 2013). Related research in idiom identification in non-E"
W18-4910,L18-1266,1,0.83749,"h a variety of different lexical items. Constructicons (inventories of constructions) have been developed in some languages as a resource to facilitate recognition, interpretation and translation of constructions (Fillmore et al., 2012; Torrent et al., 2014; Bäckström et al., 2014). Here, we provide a preliminary exploration of constructions that should be considered for an MD constructicon. A native speaker of MD and author of this paper examined instances of particular motion and degreerelated constructions drawn from the Abstract Meaning Representation (AMR) corpus (Banarescu et al., 2013; Bonial et al., 2018). We selected AMR instances of constructions because the corpus offers a data-driven (as opposed to seeking out instances of a construction; these are constructions that arose in sentence-by-sentence AMR annotation) variety of annotated examples, which also facilitates a sense of the relative frequency of particular constructions in a larger corpus. After the native speaker of MD examined these English examples, examples of what were thought to be instantiations of the same construction in MD were selected. In addition, translation was attempted of some of the English examples in order to eval"
W18-4910,bouamor-etal-2014-multidialectal,0,0.0265531,"ion of MD idioms; thus, in Section 4 describing the MD idioms, we provide an analysis of the strategies used to translate idioms as well as a quantitative results illuminating which strategies were applicable to particular idiom types. 2 Moroccan Darija: an Arabic Dialect The Arabic language family encompasses many varieties, including Modern Standard Arabic (MSA) and a large number of regional dialects. Despite being the standard written form of Arabic and appearing in many formal venues such as news broadcasts and parliamentary speeches, MSA is not the native language of any Arabic speaker (Bouamor et al., 2014). Instead, typical everyday conversation is conducted in Arabic dialects (or other languages) rather than MSA. Arabic dialects vary substantially from MSA and each other, with significant phonological, morphological, and syntactic differences (Brustad, 2002), as well as in their vocabularies. For example, MSA has been established as VSO, whereas MD is viewed as SVO (Simons and Fennig, 2018). The dialects have varying degrees of mutual intelligibility, with those of northwestern Africa (e.g., Morocco, Algeria, Tunisia) being particularly challenging to outsiders. Although Arabic dialects lack o"
W18-4910,W18-1408,1,0.743128,"ath He blinked the snow off of her eyelashes. They booed him off the stage. She sneezed the foam off the cappuccino. This construction is quite productive in English, licensing a variety of different verbs that are not typically associated with motion semantics, as shown above. Precisely what constraints exist upon the compatibility of a particular verb within this construction remains under debate. This idiosyncratic semi-productivity is precisely what makes adding this construction to the lexicon—as opposed to adding on motion senses 7 Recently, expanding from within a generative framework, Dorr and Voss (2018) propose multi-layered verb structures for a computational lexicon to support spatial language understanding. 8 Ideally, an MD constructicon would be developed through a careful analysis of the language through the lens of construction grammar, thereby avoiding the inevitable bias that stems from considering correspondents to English constructions. We hope to complete such analysis in the future as we develop the needed expertise in both construction grammar and MD. It would also be fruitful to explore a semi-automatic approach to detecting constructions, like that of Forsberg et al. (2014). 7"
W18-4910,J09-1005,0,0.010752,"ndeavoring to create a WordNet from the Harrell and Sobelman dictionary and link it with Princeton’s WordNet (Fellbaum, 1998). MDW will eventually be released as part of the Open Multilingual Wordnet project (Bond and Foster, 2013). 6.2 Idioms In the last decade, researchers have turned to the task of automatically detecting idioms in English texts, developing statistical measures that go beyond the tradition of relying on manually identifying expressions in terms of their syntactic and semantic idiosyncracies; instead, researchers have analyzed a wide range of actual usage patterns in texts (Fazly et al., 2009), and most recently using word embeddings (Peng and Feldman, 2017) and leveraging existing sources of idioms, such as in Wiktionary (Muzny and Zettlemoyer, 2013). Related research in idiom identification in non-English languages is also being conducted by native speakers of those languages, e.g., Hindi (Priyanka and Sinha, 2014), Italian (Vietri, 2014), Russian (Aharodnik et al., 2018), and Japanese (Hashimoto and Kawahara, 2008). 6.3 Constructicons Though constructicons are intended to be language-specific, they are based on the notion of continuity between the grammar and the lexicon in all"
W18-4910,D08-1104,0,0.0390509,"on manually identifying expressions in terms of their syntactic and semantic idiosyncracies; instead, researchers have analyzed a wide range of actual usage patterns in texts (Fazly et al., 2009), and most recently using word embeddings (Peng and Feldman, 2017) and leveraging existing sources of idioms, such as in Wiktionary (Muzny and Zettlemoyer, 2013). Related research in idiom identification in non-English languages is also being conducted by native speakers of those languages, e.g., Hindi (Priyanka and Sinha, 2014), Italian (Vietri, 2014), Russian (Aharodnik et al., 2018), and Japanese (Hashimoto and Kawahara, 2008). 6.3 Constructicons Though constructicons are intended to be language-specific, they are based on the notion of continuity between the grammar and the lexicon in all languages (Fillmore, 2008). Resources for constructicons that we are aware of (Bäckström et al., 2014; Torrent et al., 2014; Forsberg et al., 2014; Ohara, 2016) take the following approach (similar to our own here): they begin by comparing English constructions to known constructions in another language of choice. Such comparison offers a baseline of constructions that may exist in the language, elucidating equivalent, approximat"
W18-4910,D13-1145,0,0.0173604,"leased as part of the Open Multilingual Wordnet project (Bond and Foster, 2013). 6.2 Idioms In the last decade, researchers have turned to the task of automatically detecting idioms in English texts, developing statistical measures that go beyond the tradition of relying on manually identifying expressions in terms of their syntactic and semantic idiosyncracies; instead, researchers have analyzed a wide range of actual usage patterns in texts (Fazly et al., 2009), and most recently using word embeddings (Peng and Feldman, 2017) and leveraging existing sources of idioms, such as in Wiktionary (Muzny and Zettlemoyer, 2013). Related research in idiom identification in non-English languages is also being conducted by native speakers of those languages, e.g., Hindi (Priyanka and Sinha, 2014), Italian (Vietri, 2014), Russian (Aharodnik et al., 2018), and Japanese (Hashimoto and Kawahara, 2008). 6.3 Constructicons Though constructicons are intended to be language-specific, they are based on the notion of continuity between the grammar and the lexicon in all languages (Fillmore, 2008). Resources for constructicons that we are aware of (Bäckström et al., 2014; Torrent et al., 2014; Forsberg et al., 2014; Ohara, 2016)"
W18-4910,L16-1658,0,0.0243356,"akla w-huwa t-y-zid y-h’ayah’ Gloss 2 as_long_as-we decreased-(PL) to-him in the-food and-he (PRS)-he-increases he-defies ﻛﻠﻤﺎ ﻧﻘﺼﻨﺎ ﻟﻴﻪ ﻓﻲ اﻟﻤﺎﻛﻠﺔ ﻛﻠﻤﺎ ﺗﻴﺰﻳﺪ ﻳﺤﻴﺢ MD 3 Transliteration 3 kul-ma nqass-na li-h fi al-makla kul-ma t-y-zid y-h’ayah’ Gloss 3 all/every-what/that decreased-we to-him in the-food all/every-what/that (PRS)-he-increases he-defies Table 10: Examples of the-Xer, the-Yer MD constructions NLP researchers is automatic dialect/language identification, especially for code-switched data, since MD speakers frequently mix MD with other languages such as MSA, French, and English. Samih and Maier (2016a) create a 223,000 token corpus from Moroccan internet discussion forums and blogs and annotate it for use in code-switching detection experiments (Samih and Maier, 2016b).11 Voss et al. (2014) also build a code-switching detection system for MD, focusing on MD text written in Latin script. In addition to the aforementioned digital resources, there are a few print dictionaries for Moroccan, including a recent verb dictionary (El Haloui and Bowman, 2011) as well as an older dictionary in Latin script edited by Harrell and Sobelman (1966). The Moroccan Darija Wordnet (MDW) project (Mrini and Bo"
W18-4910,O07-5005,0,0.0468633,"ique MD translations provided by MADAR’s Fes and Rabat translators, 342 correspond to items in our lexicon. However, there are several important differences between the two lexicons. For example, the MD entries in MADAR were all generated in translation from other languages whereas ours were originally produced in MD and then translated into English. Because of this, our lexicon is likely to contain a variety of morphological and orthographic variations for the same base word, in contrast to the MADAR lexicon. Also, the MADAR concept keys were derived from the Basic Travel Expressions Corpus (Takezawa et al., 2007) and, thus, the lexicon is most relevant to the travel domain. In contrast, our lexicon spans multiple complementary domains, including politics, sports, and entertainment. 4 More than a Word: Idioms In the process of creating the word-level lexicon described in Section 3, we encountered numerous idioms. Idioms pose an intriguing question in the development of a lexicon, as their status straddles the boundary between being compositional and non-compositional in meaning. A central challenge in identifying and translating idioms is thus pinpointing this variable idiomatic meaning and transferrin"
W18-4910,W14-5817,0,0.0177782,"ng statistical measures that go beyond the tradition of relying on manually identifying expressions in terms of their syntactic and semantic idiosyncracies; instead, researchers have analyzed a wide range of actual usage patterns in texts (Fazly et al., 2009), and most recently using word embeddings (Peng and Feldman, 2017) and leveraging existing sources of idioms, such as in Wiktionary (Muzny and Zettlemoyer, 2013). Related research in idiom identification in non-English languages is also being conducted by native speakers of those languages, e.g., Hindi (Priyanka and Sinha, 2014), Italian (Vietri, 2014), Russian (Aharodnik et al., 2018), and Japanese (Hashimoto and Kawahara, 2008). 6.3 Constructicons Though constructicons are intended to be language-specific, they are based on the notion of continuity between the grammar and the lexicon in all languages (Fillmore, 2008). Resources for constructicons that we are aware of (Bäckström et al., 2014; Torrent et al., 2014; Forsberg et al., 2014; Ohara, 2016) take the following approach (similar to our own here): they begin by comparing English constructions to known constructions in another language of choice. Such comparison offers a baseline of c"
W18-4910,voss-etal-2014-finding,1,0.845378,"qass-na li-h fi al-makla kul-ma t-y-zid y-h’ayah’ Gloss 3 all/every-what/that decreased-we to-him in the-food all/every-what/that (PRS)-he-increases he-defies Table 10: Examples of the-Xer, the-Yer MD constructions NLP researchers is automatic dialect/language identification, especially for code-switched data, since MD speakers frequently mix MD with other languages such as MSA, French, and English. Samih and Maier (2016a) create a 223,000 token corpus from Moroccan internet discussion forums and blogs and annotate it for use in code-switching detection experiments (Samih and Maier, 2016b).11 Voss et al. (2014) also build a code-switching detection system for MD, focusing on MD text written in Latin script. In addition to the aforementioned digital resources, there are a few print dictionaries for Moroccan, including a recent verb dictionary (El Haloui and Bowman, 2011) as well as an older dictionary in Latin script edited by Harrell and Sobelman (1966). The Moroccan Darija Wordnet (MDW) project (Mrini and Bond, 2017) is endeavoring to create a WordNet from the Harrell and Sobelman dictionary and link it with Princeton’s WordNet (Fellbaum, 1998). MDW will eventually be released as part of the Open M"
W18-4910,N12-1006,0,0.0347842,"ia, Tunisia) being particularly challenging to outsiders. Although Arabic dialects lack orthographic standards and historically have not been written, they now appear constantly in online social media. Unfortunately, these dialects are severely under-resourced from a computational perspective, impeding the creation of NLP systems, such as machine translation engines. MT systems trained on large parallel MSA-English corpora perform substantially worse on dialectal text than when such systems are trained on significantly smaller Arabic dialect-English parallel corpora created via crowdsourcing (Zbib et al., 2012), illustrating the need for computational lexical resources specific to the individual dialects. 3 Traditional Lexicon: Words and Their Translations The most straightforward component to include in an MD computational lexicon is a traditional wordlevel lexicon consisting of words and their translations. For our work, we leverage Hespress.com, a popular news portal whose reporting covers events in Morocco and other topics of interest to Moroccans. The articles span numerous domains, including politics, sports, and entertainment. Although the articles are primarily written in MSA, the commentary"
W18-5012,W15-4611,0,0.0604603,"Missing"
W18-5012,stoia-etal-2008-scare,0,0.18304,"contains more than one intent (Dialogue 4). Understanding stylistic differences can support the development of dialogue systems with strategies that tailor system responses to the user’s style, rather than constrain the user’s style to the expected input. The taxonomy is described in more detail in Section 3. We observe and analyze these stylistic differences in a corpus of human-robot direction-giving dialogue from Marge et al. (2017). These styles are not unique to this corpus; they emerge in other human-robot and human-human dialogue, such as TeamTalk (Marge and Rudnicky, 2011) and SCARE (Stoia et al., 2008). The corpus contains 60 dialogues from 20 participants (Section 4). The robot dialogue management in the corpus is controlled by a Wizard-of-Oz experimenter, allowing for the study of users’ style with a fluent and naturalistic partner (i.e., with an approximation of an idealized automated system). In Section 5, we investigate possible consequences and implications of these categorized styles in this corpus. We examine the relationship of style and miscommunication frequency, applying an existing taxonomy for miscommunication in human-agent conversational dialogue (Higashinaka et al., 2015a)"
W18-5012,D15-1268,0,0.319779,"z for dialogue management. This allowed us specifically to isolate the style usage and miscommunication errors of the human partner (because the Wizard makes very few errors on the robot’s end). Studies of human-robot automated systems tend to focus on the miscommunication errors of the dialogue system (i.e., the robot itself), rather than the miscommunication or style of the human partner. In conversational agents, the research focus is also primarily to categorize errors made by the agent, not the human, including errors in ASR, surface realization, or appropriateness of the response (e.g., Higashinaka et al. (2015b); Paek and Horvitz (2000)). The more generic task-oriented and agent-based response-level errors from Higashinaka et al. (2015a) map well to the user miscommunication in the corpus we examine, including excess/lack of information, nonunderstanding, unclear intention, and misunderstanding. Works that focus specifically on miscommunication from the user when interacting with a robot include those categorizing referential ambiguity and impossible-to-execute commands (Marge and Rudnicky, 2015). These categories are common in the data we examine as well. In this analysis, we predict that trust wi"
W18-5012,L16-1504,0,0.0301679,"ed dialogue management strategies and offer concluding remarks on future work (Sections 7 and 8). 2 3 Stylistic Differences We describe two classes of stylistic differences for instruction-giving: differences in the verbosity of an instruction, and in the structure of the instruction. These styles emerge when decomposing a high-level plan or intent (e.g., exploring a physical space) into (potentially, but not necessarily) lowlevel instructions (e.g., how to explore the space, where to move, how to turn). Related Work A number of human-human direction-giving corpora exist, among them, ArtWalk (Liu et al., 2016), CReST (Eberhard et al., 2010), SCARE (Stoia et al., 2008), and SaGA (L¨ucking et al., 2010). The majority of existing analyses on these 111 3.1 Verbosity while looking at a live 2D-map built from the robot’s LIDAR scanner. A low bandwidth environment was simulated by disabling video streaming; instead, photos could be captured on-demand from the robot’s front-facing camera. To allow full natural language use, users were not provided example commands to the robot, though they were provided with a list of the robot’s capabilities which they could reference throughout the trials. Well-formed in"
W18-5012,L18-1017,1,0.612951,"plan of the user. In Dialogue 3, the user issues a single instruction “go through the other door” and waits until the instruction has been completed. Upon receiving completion feedback from the robot (“executing” and “done” responses), the next instruction, “take a picture”, is issued. Compare this with Dialogue 4, where the intents “face your starting position” and “send a picture” are compounded together and issued at the same time. This is classified as an Extended intent structure: instructions that have more than one expressed intent. These structural definitions were first described in Traum et al. (2018) to classify the composition of an instruction. In this work, we use these definitions to classify the style of the user. 4 4.1 Corpus Statistics The corpus contains 3,573 utterances from 20 users, totaling 18,336 words. 1,981 instructions were issued. The least verbose instruction observed is 1 word (“stop”), and the most verbose is 59 words (mean 7.3, SD 5.8). Of the total instructions, 1,383 are of the Minimal style, and 598, Extended. A moderate, positive correlation exists between higher verbosity and the Extended style in this corpus (rs (1969) = .613, p < .001)), supporting an intuition"
W18-5012,W17-2808,1,0.679528,"uman-robot collaboration has been studied with respect to engagement with the robot, and memory of information from the robot (Powers et al., 2007). two Minimal) or Extended if it contains more than one intent (Dialogue 4). Understanding stylistic differences can support the development of dialogue systems with strategies that tailor system responses to the user’s style, rather than constrain the user’s style to the expected input. The taxonomy is described in more detail in Section 3. We observe and analyze these stylistic differences in a corpus of human-robot direction-giving dialogue from Marge et al. (2017). These styles are not unique to this corpus; they emerge in other human-robot and human-human dialogue, such as TeamTalk (Marge and Rudnicky, 2011) and SCARE (Stoia et al., 2008). The corpus contains 60 dialogues from 20 participants (Section 4). The robot dialogue management in the corpus is controlled by a Wizard-of-Oz experimenter, allowing for the study of users’ style with a fluent and naturalistic partner (i.e., with an approximation of an idealized automated system). In Section 5, we investigate possible consequences and implications of these categorized styles in this corpus. We exami"
W18-5012,W15-4604,1,0.853954,", not the human, including errors in ASR, surface realization, or appropriateness of the response (e.g., Higashinaka et al. (2015b); Paek and Horvitz (2000)). The more generic task-oriented and agent-based response-level errors from Higashinaka et al. (2015a) map well to the user miscommunication in the corpus we examine, including excess/lack of information, nonunderstanding, unclear intention, and misunderstanding. Works that focus specifically on miscommunication from the user when interacting with a robot include those categorizing referential ambiguity and impossible-to-execute commands (Marge and Rudnicky, 2015). These categories are common in the data we examine as well. In this analysis, we predict that trust will have an effect on stylistic variations. Factors of trust in co-present and remote human-robot collaboration has been studied with respect to engagement with the robot, and memory of information from the robot (Powers et al., 2007). two Minimal) or Extended if it contains more than one intent (Dialogue 4). Understanding stylistic differences can support the development of dialogue systems with strategies that tailor system responses to the user’s style, rather than constrain the user’s sty"
W19-0124,P13-1023,0,0.0242758,"-scale implementation. A notable exception is combinatory categorical grammar (CCG) (Steedman and Baldridge, 2009); CCG parsers have already been incorporated in some current dialogue systems (Chai et al., 2014). Although promising, CCG parses closely mirror the input language, so systems making use of CCG parses still face the challenge of a great deal of linguistic variability that can be associated with a single intent. Again, in abstracting away from surface variation, AMR may offer more regular, consis243 tent parses in comparison to CCG. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013), which also abstracts away from syntactic idiosyncrasies, and its corresponding parser (Hershcovich et al., 2017) merits future investigation. 6.2 NLU in Dialogue Systems Task-oriented spoken dialogue systems have been an active area of research since the early 1990s. Broadly, the architecture of such systems includes (i) automatic speech recognition (ASR) to recognize an utterance, (ii) an NLU component to identify the user’s intent, and (iii) a dialogue manager to interact with the user and achieve the intended task (Bangalore et al., 2006). The meaning representation within such systems ha"
W19-0124,P98-1013,0,0.559692,"tion type) are indicated in the right columns. (Traum et al., 2018) sets (want-01), or special keywords indicating generic entity types: date-entity, world-region, distance-quantity, etc. In addition to the PropBank lexicon of rolesets, which associate argument numbers (ARG 0–6) with predicate-specific semantic roles (e.g., ARG0=wanter in ex. 1), AMR uses approximately 100 relations of its own (e.g., :time, :age, :quantity, :destination, etc.). The representation captures who is doing what to whom like other semantic role labeling (SRL) schemes (e.g., PropBank (Palmer et al., 2005), FrameNet (Baker et al., 1998; Fillmore et al., 2003), VerbNet (Kipper et al., 2008)), but also represents other aspects of meaning outside of semantic role information, such as fine-grained quantity and unit information and parthood relations. Also distinguishing it from other SRL schemes, a goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic structures; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. AMR has been widely used to support NLU, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016)"
W19-0124,W13-2322,1,0.891842,"nge in human-agent collaboration is that robots (or their virtual counterparts) do not have sufficient linguistic or world knowledge to communicate in a timely and effective manner with their human collaborators (Chai et al., 2017; She and Chai, 2017). We address this challenge in ongoing research directed at analyzing robotdirected communication in collaborative humanagent exploration tasks, with the ultimate goal of enabling robots to adapt to domain-specific language. In this paper, we choose to adopt an intermediate semantic representation and select Abstract Meaning Representation (AMR) (Banarescu et al., 2013) in particular for three reasons: (i) the semantic representation framework abstracts away from surface variation, therefore the robot will only be trained to process and execute the actions corresponding to semantic elements of the representation (ii) there are a variety of fairly robust AMR parsers we can employ for this work, enabling us to forego manual annotation of substantial portions of our data and facilitating efficient automatic parsing in a future end-to-end system; and (iii) the structured representation facilitates the interpretation of novel instructions and grounding instructio"
W19-0124,W18-4912,1,0.837165,"oles within the speech act (:ARG0 and :ARG1 of command-02). Future work will be able to reference these roles and model how participant relations evolve over the course of the discourse (Allwood et al., 2000). (c / command-02 :ARG0-commander :ARG1-impelled agent :ARG2 (g / go-02 :completable + :ARG0-goer :ARG1-extent :ARG3-start point :ARG4-end point :path :direction :time (a / after :op1 (n / now)))) Figure 7: Speech act template for command:move. Arguments and relations in italics are filled in from context and the utterance. Tense and Aspect. We also adopt the annotation scheme proposed by Donatelli et al. (2018) for augmenting AMR with tense and aspect. This scheme identifies the temporal nature of an event relative to the dialogue act in which it is expressed as past, present, or future. The aspectual nature of the event can be specified as atelic (:ongoing -/+), telic and hypothetical (:ongoing -, :completable +), telic and in progress (:ongoing +, :complete -), or telic and complete (:ongoing -, :complete +). A telic and hypothetical event representation can be seen in figure 7. This tense/aspect annotation scheme is specific to AMR and coarse-grained in nature, but through its use of existing AMR"
W19-0124,P14-1134,0,0.0280937,", 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017) 4 Evaluating AMR parsers on Human-Robot Dialogue Data To serve as a conduit for NLU in a dialogue system, the ideal semantic representation would have robust parsers, allowing the representation to be implemented efficiently on a large scale. There have been a variety of parsers developed for AMR; two parsers using very different approaches are explored in the sections to follow. 238 4.1 Parsers To automatically create AMRs for the humanrobot diaogue data, we used two off-the-shelf AMR parsers, JAMR2 (Flanigan et al., 2014) and CAMR3 (Wang et al., 2015). JAMR was one of the first AMR parsers and uses a two-part algorithm to first identify concepts and then to build the maximum spanning connected subgraph of those concepts, adding in the relations. CAMR, in contrast, starts by obtaining the dependency tree (in this case, using the Charniak parser4 and Stanford CoreNLP toolkit (Manning et al., 2014)) and then uses their algorithm to apply a series of transformations to the dependency tree, ultimately transforming it into an AMR graph. One strength of CAMR is that the dependency parser is independent of the AMR cre"
W19-0124,J86-3001,0,0.660852,"not included in the current scheme). This goal follows from the understanding that dialogue acts are composed of two primary components: (i) semantic content, identifying the entities, events, and propositions relevant to the dialogue; and (ii) communicative function, identifying the ways an addressee may use semantic content to update the information state (Bunt et al., 2012). The existing dialogue structure annotation scheme of Traum et al. (2018) distinguishes two primary levels of pragmatic meaning important to dialogue that our research aims to maintain. The first, intentional structure (Grosz and Sidner, 1986), is equivalent to a TU11 : all utterances that explicate and address an initiator’s intent. The second, interactional structure, captures how the information state of participants in the dialogue is updated as the TU is constructed (Traum et al., 2018). These two levels of meaning stand apart from the basic compositional meaning of their associated utterances.We seek to represent these pragmatic levels of meaning and link them to their respective semantic forms. We also seek to represent the temporal, aspectual, and veridical/modal nature of the robot’s actions. Human instructions must be int"
W19-0124,P17-1104,0,0.0195982,"CCG parsers have already been incorporated in some current dialogue systems (Chai et al., 2014). Although promising, CCG parses closely mirror the input language, so systems making use of CCG parses still face the challenge of a great deal of linguistic variability that can be associated with a single intent. Again, in abstracting away from surface variation, AMR may offer more regular, consis243 tent parses in comparison to CCG. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013), which also abstracts away from syntactic idiosyncrasies, and its corresponding parser (Hershcovich et al., 2017) merits future investigation. 6.2 NLU in Dialogue Systems Task-oriented spoken dialogue systems have been an active area of research since the early 1990s. Broadly, the architecture of such systems includes (i) automatic speech recognition (ASR) to recognize an utterance, (ii) an NLU component to identify the user’s intent, and (iii) a dialogue manager to interact with the user and achieve the intended task (Bangalore et al., 2006). The meaning representation within such systems has, in the past, been predefined frames for particular subtasks (e.g., flight inquiry), with slots to be filled (e."
W19-0124,N15-1114,0,0.0173598,"et al., 2005), FrameNet (Baker et al., 1998; Fillmore et al., 2003), VerbNet (Kipper et al., 2008)), but also represents other aspects of meaning outside of semantic role information, such as fine-grained quantity and unit information and parthood relations. Also distinguishing it from other SRL schemes, a goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic structures; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. AMR has been widely used to support NLU, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017) 4 Evaluating AMR parsers on Human-Robot Dialogue Data To serve as a conduit for NLU in a dialogue system, the ideal semantic representation would have robust parsers, allowing the representation to be implemented efficiently on a large scale. There have been a variety of parsers developed for AMR; two parsers using very different approaches are explored in the sections to fol"
W19-0124,P18-4016,1,0.765002,"Missing"
W19-0124,P14-5010,0,0.00549658,"for AMR; two parsers using very different approaches are explored in the sections to follow. 238 4.1 Parsers To automatically create AMRs for the humanrobot diaogue data, we used two off-the-shelf AMR parsers, JAMR2 (Flanigan et al., 2014) and CAMR3 (Wang et al., 2015). JAMR was one of the first AMR parsers and uses a two-part algorithm to first identify concepts and then to build the maximum spanning connected subgraph of those concepts, adding in the relations. CAMR, in contrast, starts by obtaining the dependency tree (in this case, using the Charniak parser4 and Stanford CoreNLP toolkit (Manning et al., 2014)) and then uses their algorithm to apply a series of transformations to the dependency tree, ultimately transforming it into an AMR graph. One strength of CAMR is that the dependency parser is independent of the AMR creation, so a dependency parser that is trained on a larger data set, and therefore more accurate, can be used. Both JAMR and CAMR have algorithms that have learned probabilities from training data in order to execute their algorithms on novel sentences. 4.2 Gold Standard Data Set In order to evaluate parser performance on our data set, we hand-annotated a subset of the participan"
W19-0124,J05-1004,0,0.141851,"son, 1969; Parsons, 1990), AMR introduces variables (or graph nodes) for entities, events, properties, and states. Leaves are labeled with concepts, so that (d / dog) refers to an instance (d) of the concept dog. Relations link entities, so that (w / walk-01 :location (p/ park)) means the walking event (w) has the relation location to the entity, park (p). When an entity plays multiple roles in a sentence (e.g., (d / dog) above), AMR employs re-entrancy in graph notation (nodes with multiple parents) or variable re-use in PENMAN notation. AMR concepts are either English words (boy), PropBank (Palmer et al., 2005) role# 1 2 3 4 5 Left floor Participant move forward 3 feet Right Floor DM Ñ RN DM Ñ Participant ok move forward 3 feet I moved forward 3 feet RN done Annotations TU Ant 1 1 1 1 1 1 3 1 4 Rel ack-wilco trans-r ack-done trans-l Table 1: Example of a Transaction Unit (TU) in the existing corpus dialogue annotation, which contains an instruction initiated by the participant, its translation to a simplified form (DM to RN), and the execution of the instruction and acknowledgement of such by the RN. TU, Ant(ecedent), and Rel(ation type) are indicated in the right columns. (Traum et al., 2018) sets"
W19-0124,N15-1119,0,0.042982,"of meaning outside of semantic role information, such as fine-grained quantity and unit information and parthood relations. Also distinguishing it from other SRL schemes, a goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic structures; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. AMR has been widely used to support NLU, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017) 4 Evaluating AMR parsers on Human-Robot Dialogue Data To serve as a conduit for NLU in a dialogue system, the ideal semantic representation would have robust parsers, allowing the representation to be implemented efficiently on a large scale. There have been a variety of parsers developed for AMR; two parsers using very different approaches are explored in the sections to follow. 238 4.1 Parsers To automatically create AMRs for the humanrobot diaogue data, we used two off-the-shelf AMR parsers, JAMR2 (Flaniga"
W19-0124,W16-6603,0,0.0572122,"ameNet (Baker et al., 1998; Fillmore et al., 2003), VerbNet (Kipper et al., 2008)), but also represents other aspects of meaning outside of semantic role information, such as fine-grained quantity and unit information and parthood relations. Also distinguishing it from other SRL schemes, a goal of AMR is to capture core facets of meaning while abstracting away from idiosyncratic syntactic structures; thus, for example, She adjusted the machine and She made an adjustment to the machine share the same AMR. AMR has been widely used to support NLU, generation, and summarization (Liu et al., 2015; Pourdamghani et al., 2016), machine translation, question answering (Mitra and Baral, 2016), information extraction (Pan et al., 2015), and biomedical text mining (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017) 4 Evaluating AMR parsers on Human-Robot Dialogue Data To serve as a conduit for NLU in a dialogue system, the ideal semantic representation would have robust parsers, allowing the representation to be implemented efficiently on a large scale. There have been a variety of parsers developed for AMR; two parsers using very different approaches are explored in the sections to follow. 238 4.1 Parsers To auto"
W19-0124,W17-2315,0,0.399221,"Missing"
W19-0124,P17-1150,0,0.020758,"how well does it capture the meaning and distinctions needed in our collaborative human-robot dialogue domain? We find that AMR has gaps that align with linguistic information critical for effective human-robot collaboration in search and navigation tasks, and we present task-specific modifications to AMR to address the deficiencies. 1 Introduction A central challenge in human-agent collaboration is that robots (or their virtual counterparts) do not have sufficient linguistic or world knowledge to communicate in a timely and effective manner with their human collaborators (Chai et al., 2017; She and Chai, 2017). We address this challenge in ongoing research directed at analyzing robotdirected communication in collaborative humanagent exploration tasks, with the ultimate goal of enabling robots to adapt to domain-specific language. In this paper, we choose to adopt an intermediate semantic representation and select Abstract Meaning Representation (AMR) (Banarescu et al., 2013) in particular for three reasons: (i) the semantic representation framework abstracts away from surface variation, therefore the robot will only be trained to process and execute the actions corresponding to semantic elements of"
W19-0124,L18-1017,1,0.884243,"re work. 2 Background: Human-Robot Dialogue Corpus We aim to support NLU within the broader context of ongoing research to develop a human-robot dialogue system (Marge et al., 2016a) to be used onboard a remotely located agent collaborating with humans in search and navigation tasks (e.g., disaster relief). In developing this dialogue system, we 236 Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 236-246. New York City, New York, January 3-6, 2019 are making use of portions of the corpus of humanrobot dialogue data collected under this effort (Bonial et al., 2018; Traum et al., 2018).1 This corpus was collected via a phased ‘Wizard-of-Oz’ (WoZ) methodology, in which human experimenters perform the dialogue and navigation capabilities of the robot during experimental trials, unbeknownst to participants interacting with the ‘robot’(Marge et al., 2016b). Specifically, a na¨ıve participant (unaware of the wizards) is tasked with instructing a robot to navigate through a remote, unfamiliar house-like environment, and asked to find and count objects such as shoes and shovels. In reality, the participant is not speaking directly to a robot, but to an unseen Dialogue Manager (DM)"
W19-0124,N15-1040,0,0.0402005,"g (Garg et al., 2016; Rao et al., 2017; Wang et al., 2017) 4 Evaluating AMR parsers on Human-Robot Dialogue Data To serve as a conduit for NLU in a dialogue system, the ideal semantic representation would have robust parsers, allowing the representation to be implemented efficiently on a large scale. There have been a variety of parsers developed for AMR; two parsers using very different approaches are explored in the sections to follow. 238 4.1 Parsers To automatically create AMRs for the humanrobot diaogue data, we used two off-the-shelf AMR parsers, JAMR2 (Flanigan et al., 2014) and CAMR3 (Wang et al., 2015). JAMR was one of the first AMR parsers and uses a two-part algorithm to first identify concepts and then to build the maximum spanning connected subgraph of those concepts, adding in the relations. CAMR, in contrast, starts by obtaining the dependency tree (in this case, using the Charniak parser4 and Stanford CoreNLP toolkit (Manning et al., 2014)) and then uses their algorithm to apply a series of transformations to the dependency tree, ultimately transforming it into an AMR graph. One strength of CAMR is that the dependency parser is independent of the AMR creation, so a dependency parser"
W19-3322,W18-4912,1,0.829047,"e assertions. We found that more fine-grained speech act information is needed for human-robot dialogue. 5 Tense & Aspect AMR currently lacks information that specifies when an action occurs relative to speech time and whether or not this action is completed (if a past event) or able to be completed (if a future event). This information is essential for situated humanrobot dialogue, where successful collaboration depends on bridging the gap between differing perceptions of the shared environment and creating common ground (Chai et al., 2014). Our tense and aspect annotation scheme is based on Donatelli et al. (2018), who propose a four-way division of temporal annotation and three multi-valued categories for aspectual annotation that fits seamlessly into existing AMR annotation practice. We reduced the authors’ proposed temporal categories to three, to capture temporal relations before, during, and after the speech time. In addition to the aspectual categories proposed by Donatelli et al. (2018), we added the category :completable +/- to signal whether or not a hypothetical event has an end-goal that is executable for the robot (described further in Section 5.3). Our annotation categories for tense and a"
W19-3322,P13-1023,0,0.0277445,"systems for NLU. However, for many of these representations, there are no existing automatic parsers, limiting their feasibility for largerscale implementation. An exception is combinatory categorical grammar (CCG) (Steedman and Baldridge, 2011); CCG parsers have been incorporated in some current dialogue systems (Chai et al., 2014). Although promising, CCG parses closely mirror the input language, so systems making use of CCG parses still face the challenge of a great deal of linguistic variability that can be associated with a single intent. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013), which also abstracts away from syntactic idiosyncrasies, and its corresponding parser (Hershcovich et al., 2017) merits future investigation. 7.2 7.3 Speech Act Taxonomies for Dialogue Speech acts have been used as part of the meaning representation of task-oriented dialogue systems since the 1970s (e.g., Bruce, 1975; Cohen and Perrault, 1979; Allen and Perrault, 1980). For a summary of some of the earlier work in this area, see Traum (1999). There have been a number of widely used speech act taxonomies, including an ISO standard (Bunt et al., 2012), however these often have to be particular"
W19-3322,P14-1134,0,0.0835122,"ture, a declarative statement that the instruction is being carried out, and an acknowledgment that it has been carried out are critical for conveying the robot’s current status in a live system. (m / move-01 :ARG0 (i / i) :direction (f / forward) :extent (d / distance-quantity :quant 10 :unit (f2 / foot))) Figure 2: Identical AMR for I will move / I am moving / I moved forward...10 feet. Although the imperative Move forward 10 feet should receive an AMR marker :mode imperative, our evaluation of the existing 1 https://github.com/amrisi/amr-guidelines/blob/master/ amr.md 201 5.1 parsers JAMR (Flanigan et al., 2014) and CAMR (Wang et al., 2015) showed that parser output does not include this marker as it is rare if not entirely missing from the AMR 1.0 or 2.0 training corpora (Section 6).2 As a result, the command to move forward also received the identical above AMR (Figure 2) in parser output. While this suggests that additional training data is needed that includes imperatives, this speaks to a larger issue of AMR: the existing representation is very limited with respect to speech act information. Current AMR includes :mode imperative and represents questions through the presence of amr-unknown standi"
W19-3322,W00-1015,0,0.0512794,"ch acts have been used as part of the meaning representation of task-oriented dialogue systems since the 1970s (e.g., Bruce, 1975; Cohen and Perrault, 1979; Allen and Perrault, 1980). For a summary of some of the earlier work in this area, see Traum (1999). There have been a number of widely used speech act taxonomies, including an ISO standard (Bunt et al., 2012), however these often have to be particularized to the domain of interest to be fully useful. Our approach with speech act types and subtypes representing a kind of semantic frame is perhaps most similar to the dialogue primitives of Hagen and Popowich (2000). Combining these types with fully compositional AMRs will allow flexible expressiveness, inferential power and tractable connection to robot action. 8 Conclusions This paper has proposed refinements for AMR to encode information necessary for situated humanrobot dialogue. Specifically, we elaborate 36 templates specific to situated dialogue that capture i) tense and aspect information; ii) speech acts; and iii) spatial parameters for robot execution. These refinements come after evaluating the coverage of existing AMR for a corpus of human-robot dialogue elicited from tasks related to search-"
W19-3322,W13-2322,1,0.855315,"by the participant. The RN then tele-operates the robot to complete the participant’s instructions. Finally, the RN provides spoken feedback to the DM of completed actions or problems that arose, which are relayed by the DM to the participant. A sample interaction can be seen in Table 1. The corpus contains dialogues from a total of 82 participants across three separate phased data collections. The participants’ speech and the RN’s speech are transcribed and time-aligned with text messages generated by the DM and sent either to the participant or the RN. 2.2 3 Background: AMR The AMR project (Banarescu et al., 2013) has created a manually annotated semantics bank of text drawn from a variety of genres. Each sentence is represented by a rooted directed acyclic graph in which variables (or graph nodes) are introduced for entities, events, properties, and states; leaves are labeled with concepts (e.g., (d / dog)). For ease of creation and manipulation, annotators work with the PENMAN representation of the same information (Penman Natural Language Group, 1989), as in Figure 1. Dialogue Structure Annotations (w / want-01 :ARG0 (d / dog) :ARG1 (p / pet-01 :ARG0 (g / girl) :ARG1 d)) The corpus also includes ann"
W19-3322,P17-1104,0,0.0184341,"r feasibility for largerscale implementation. An exception is combinatory categorical grammar (CCG) (Steedman and Baldridge, 2011); CCG parsers have been incorporated in some current dialogue systems (Chai et al., 2014). Although promising, CCG parses closely mirror the input language, so systems making use of CCG parses still face the challenge of a great deal of linguistic variability that can be associated with a single intent. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013), which also abstracts away from syntactic idiosyncrasies, and its corresponding parser (Hershcovich et al., 2017) merits future investigation. 7.2 7.3 Speech Act Taxonomies for Dialogue Speech acts have been used as part of the meaning representation of task-oriented dialogue systems since the 1970s (e.g., Bruce, 1975; Cohen and Perrault, 1979; Allen and Perrault, 1980). For a summary of some of the earlier work in this area, see Traum (1999). There have been a number of widely used speech act taxonomies, including an ISO standard (Bunt et al., 2012), however these often have to be particularized to the domain of interest to be fully useful. Our approach with speech act types and subtypes representing a"
W19-3322,T75-2014,0,0.588771,"Although promising, CCG parses closely mirror the input language, so systems making use of CCG parses still face the challenge of a great deal of linguistic variability that can be associated with a single intent. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013), which also abstracts away from syntactic idiosyncrasies, and its corresponding parser (Hershcovich et al., 2017) merits future investigation. 7.2 7.3 Speech Act Taxonomies for Dialogue Speech acts have been used as part of the meaning representation of task-oriented dialogue systems since the 1970s (e.g., Bruce, 1975; Cohen and Perrault, 1979; Allen and Perrault, 1980). For a summary of some of the earlier work in this area, see Traum (1999). There have been a number of widely used speech act taxonomies, including an ISO standard (Bunt et al., 2012), however these often have to be particularized to the domain of interest to be fully useful. Our approach with speech act types and subtypes representing a kind of semantic frame is perhaps most similar to the dialogue primitives of Hagen and Popowich (2000). Combining these types with fully compositional AMRs will allow flexible expressiveness, inferential po"
W19-3322,P13-2131,0,0.121004,"ection of communication. 204 Figure 7: Planned pipeline for implementing AMRs into our human-robot dialogue system: natural language instructions are parsed using AMR parsers into existing AMR, which is then converted via graph-to-graph transformation into one of our augmented AMR templates. If all required parameters in the template are complete and the instruction executable, it will be mapped onto one of the robot’s action specifications for execution. Clarifications and feedback from the robot are generated from the AMR templates. quate scores of .82, .82, and .91 using the Smatch metric (Cai and Knight, 2013). According to AMR development group communication, 2014, IAA Smatch scores on AMRs are generally between .7 and .8, depending on the complexity of the data. Having created a gold standard sample of our data, we ran both JAMR4 (Flanigan et al., 2014) and CAMR5 (Wang et al., 2015) on the same sample and obtained the Smatch scores when compared to the gold standard. We selected these two parsers to explore because JAMR was one of the first AMR parsers and uses a two-part algorithm to first identify concepts and then to build the maximum spanning connected subgraph of those concepts, adding in th"
W19-3322,N15-1114,0,0.046006,"ser-output AMRs to our set of in-domain AMRs. Rather than train parsers to parse directly into the augmented AMRs described here, a graph-to-graph transformation allows us to maintain the parser output as a representation of the sentence meanings themselves as input, while the output captures our contextual domain-specific layer and includes speaker intent on top of the sentence meaning. To create training data for graphto-graph transformation algorithms and to evaluate the coverage and quality of the set of in-domain 7 We plan to eventually model our graph-to-graph transformation on work by (Liu et al., 2015) for abstractive summarization with AMR, though in the opposite direction. 206 (Hakkani-T¨ur et al., 2016; Chen et al., 2016), the latter of which allows the system to take advantage of information from the discourse context to achieve improved NLU. Substantial challenges to these systems include working in domains with intents that have a large number of possible values for each slot and accommodation of out-ofvocabulary slot values (i.e. operating in a domain with a great deal of linguistic variability). able to perform slot-filling dialogue (Xu and Rudnicky, 2000) including clarification of"
W19-3322,P18-4016,1,0.891986,"Missing"
W19-3322,P18-1037,0,0.0227543,"Missing"
W19-3322,P14-5010,0,0.0028211,"of the data. Having created a gold standard sample of our data, we ran both JAMR4 (Flanigan et al., 2014) and CAMR5 (Wang et al., 2015) on the same sample and obtained the Smatch scores when compared to the gold standard. We selected these two parsers to explore because JAMR was one of the first AMR parsers and uses a two-part algorithm to first identify concepts and then to build the maximum spanning connected subgraph of those concepts, adding in the relations. CAMR, in contrast, starts by obtaining the dependency tree— in this case, using the Charniak parser6 and Stanford CoreNLP toolkit (Manning et al., 2014)—and then applies a series of transformations to the dependency tree, ultimately transforming it into an AMR graph. As seen in Table 3, CAMR performs better on both precision and recall when trained on AMR 1.0, thus obtaining the higher F-score. However, compared to their self-reported F-scores (0.58 for JAMR and 0.63 for CAMR) on other corpora, both under-perform on the human-robot dialogue data. Given the relatively poor performance of both parsers on the human-robot dialogue data and erParser Data Precision Recall F-score CAMR JAMR JAMR JAMR 1.0 1.0 2.0 2.0+D 0.33 0.27 0.46 0.56 0.51 0.44 0"
W19-3322,N15-1040,0,0.293404,"t the instruction is being carried out, and an acknowledgment that it has been carried out are critical for conveying the robot’s current status in a live system. (m / move-01 :ARG0 (i / i) :direction (f / forward) :extent (d / distance-quantity :quant 10 :unit (f2 / foot))) Figure 2: Identical AMR for I will move / I am moving / I moved forward...10 feet. Although the imperative Move forward 10 feet should receive an AMR marker :mode imperative, our evaluation of the existing 1 https://github.com/amrisi/amr-guidelines/blob/master/ amr.md 201 5.1 parsers JAMR (Flanigan et al., 2014) and CAMR (Wang et al., 2015) showed that parser output does not include this marker as it is rare if not entirely missing from the AMR 1.0 or 2.0 training corpora (Section 6).2 As a result, the command to move forward also received the identical above AMR (Figure 2) in parser output. While this suggests that additional training data is needed that includes imperatives, this speaks to a larger issue of AMR: the existing representation is very limited with respect to speech act information. Current AMR includes :mode imperative and represents questions through the presence of amr-unknown standing in for the concept or pola"
W19-3322,W17-2808,1,0.930987,"(provided by one senior and two recently trained AMR annotators), based on existing guidelines.1 We then examined how effectively these gold, guideline-based AMRs can capture the distinctions of interest for humanrobot dialogue and how accurately two available AMR parsers generate those gold annotations. Common instructions in the corpus include Move forward 10 feet, Take a picture, and Turn right 45 degrees. People also used landmarkbased instructions such as Move to face the yellow cone, and Go to the doorway to your right, although these were less common than the metricbased instructions (Marge et al., 2017). In response to these instructions from the DM to the participant, common feedback would be indications that an instruction will be carried out (I will move forward 10 feet), is in progress (Moving. . . ), or completed (I moved forward 10 feet). Given that current AMR guidelines do not make tense/aspect distinctions, these three types of feedback from the robot are represented identically under the current guidelines (see Figure 2). The distinctions between a promise to carry out an instruction in the future, a declarative statement that the instruction is being carried out, and an acknowledg"
W19-3322,W00-0309,0,0.772256,"raph transformation on work by (Liu et al., 2015) for abstractive summarization with AMR, though in the opposite direction. 206 (Hakkani-T¨ur et al., 2016; Chen et al., 2016), the latter of which allows the system to take advantage of information from the discourse context to achieve improved NLU. Substantial challenges to these systems include working in domains with intents that have a large number of possible values for each slot and accommodation of out-ofvocabulary slot values (i.e. operating in a domain with a great deal of linguistic variability). able to perform slot-filling dialogue (Xu and Rudnicky, 2000) including clarification of missing or vague descriptions and, if all required parameters are present, will use the domain-specific AMR for robot execution. 7 7.1 Related Work Semantic Representation There is a long-standing tradition of research in semantic representation within NLP, AI, as well as theoretical linguistics and philosophy (see Schubert (2015) for an overview). Thus, there are a variety of options that could be used within dialogue systems for NLU. However, for many of these representations, there are no existing automatic parsers, limiting their feasibility for largerscale impl"
W19-3322,C18-1313,0,0.0293395,"Missing"
W19-3322,J05-1004,0,0.105239,"ch acts work such as Austin (1975) and Searle (1969). To capture the range of speech acts present in the corpus, we arrived at an inventory of 36 unique speech acts specific to human-robot dialogue, inspired loosely by the dialogue move annotation of Marge et al. (2017). These 36 speech acts are classified into 5 types. In Figure 4, these are listed with the number of their subtypes in parentheses, along with a list of example subtypes for the type command. A full listing of subtypes and can be found in the Appendix. To integrate speech acts into AMR design, we selected existing AMR/PropBank (Palmer et al., 2005) rolesets corresponding to each speech act (e.g., command-02, assert-02, request-01, etc.) 5.3 Spatial Information A key component of successful human-robot collaboration is whether or not robot-directed commands are executable. In the dialogues represented in the corpus, for a command to be effectively executable by the robot, it must have a clear beginning and endpoint and comprise a basic action. For example, Move forward is not executable, since it lacks a clear endpoint; Move forward two feet, which identifies an endpoint, is executable. Additionally, a command such as Explore this room i"
W19-4610,D17-1098,0,0.0299594,"we demonstrate how the templatic pattern-based word formation process that transforms the root to the original word can be used for further morphological decomposition. Our root extraction method differentiates itself from other methods in three 89 character-level input space as does our own method, they ignore the sequential nature in the target class. Closely related to our model, constrained sequence-to-sequence models have been used for sentence simplification forcing the model to select simple words (Zhang et al., 2017). Similar approaches have been used for constrained image captioning (Anderson et al., 2017). Our model differs in that it constrains not only on specific vocabulary, but on specific sequences. This method, however, may incorrectly remove many letters that are part of the root. Another of these models achieves high accuracy by incorporating sentence-level context and inferred syntactic categories into a parametric Bayesian model (Lee et al., 2011). Our model forgoes these context features as it attempts to identify the root solely on the word itself. Additionally, this method cannot model non-contiguous roots, of which Semitic languages have many. Other unsupervised methods utilize d"
W19-4610,W11-0301,0,0.0318201,"related to our model, constrained sequence-to-sequence models have been used for sentence simplification forcing the model to select simple words (Zhang et al., 2017). Similar approaches have been used for constrained image captioning (Anderson et al., 2017). Our model differs in that it constrains not only on specific vocabulary, but on specific sequences. This method, however, may incorrectly remove many letters that are part of the root. Another of these models achieves high accuracy by incorporating sentence-level context and inferred syntactic categories into a parametric Bayesian model (Lee et al., 2011). Our model forgoes these context features as it attempts to identify the root solely on the word itself. Additionally, this method cannot model non-contiguous roots, of which Semitic languages have many. Other unsupervised methods utilize dictionaries to select the characters from within words (Darwish, 2002; Boudlal et al., 2011; Alhanini and Ab Aziz, 2011). Another line of research leverages the templatic nature for human-constructed rule-based constraints (Elghamry, 2005; Rodrigues and Cavar, 2007; Choueka, 1990). Finally, methods have been proposed that utilize both a root dictionary and"
W19-4610,D15-1166,0,0.0105536,"the non-contiguous nature of Semitic roots, they fail to leverage the sequential structure of the root label space. We show that such methods that forgo the sequential structure in the label space underperform on words with rare roots. Additionally, these methods are only applied to triconsonantal leaving out many biconsonantal and quadriliteral roots. Sequence-to-sequence models have been utilized for learning to map sequences to other sequences and predominantly applied to machine translation (Sutskever et al., 2014), with later variations of these models enhanced with attention mechanisms (Luong et al., 2015). While LSTM variants have been dominant, previous work has shown that GRU-based models perform comparably to LSTM-based models with superior train time (Chung et al., 2014). More recent work has investigated character-level language models in order to handle the many outof-vocabulary (OOV) words in morphologically rich languages (Gerz et al., 2018). Such methods have shown large improvements in language modeling across many morphologically rich languages. While such methods share the same 3 Root Extraction Framework We introduce a framework for extracting the root from templatic words within"
W19-4610,W02-0506,0,0.0228147,"ecific vocabulary, but on specific sequences. This method, however, may incorrectly remove many letters that are part of the root. Another of these models achieves high accuracy by incorporating sentence-level context and inferred syntactic categories into a parametric Bayesian model (Lee et al., 2011). Our model forgoes these context features as it attempts to identify the root solely on the word itself. Additionally, this method cannot model non-contiguous roots, of which Semitic languages have many. Other unsupervised methods utilize dictionaries to select the characters from within words (Darwish, 2002; Boudlal et al., 2011; Alhanini and Ab Aziz, 2011). Another line of research leverages the templatic nature for human-constructed rule-based constraints (Elghamry, 2005; Rodrigues and Cavar, 2007; Choueka, 1990). Finally, methods have been proposed that utilize both a root dictionary and rule-based templatic constraints (Yaseen and Hmeidi, 2014). Supervised methods have been developed for identifying Hebrew roots by combining various multiclass classification models with Hebrewspecific linguistic constraints (Daya et al., 2004). This same technique was extended to extract both Arabic and Hebr"
W19-4610,W04-3246,0,0.0441307,"thods utilize dictionaries to select the characters from within words (Darwish, 2002; Boudlal et al., 2011; Alhanini and Ab Aziz, 2011). Another line of research leverages the templatic nature for human-constructed rule-based constraints (Elghamry, 2005; Rodrigues and Cavar, 2007; Choueka, 1990). Finally, methods have been proposed that utilize both a root dictionary and rule-based templatic constraints (Yaseen and Hmeidi, 2014). Supervised methods have been developed for identifying Hebrew roots by combining various multiclass classification models with Hebrewspecific linguistic constraints (Daya et al., 2004). This same technique was extended to extract both Arabic and Hebrew roots (Daya et al., 2008). While these supervised methods effectively address the non-contiguous nature of Semitic roots, they fail to leverage the sequential structure of the root label space. We show that such methods that forgo the sequential structure in the label space underperform on words with rare roots. Additionally, these methods are only applied to triconsonantal leaving out many biconsonantal and quadriliteral roots. Sequence-to-sequence models have been utilized for learning to map sequences to other sequences an"
W19-4610,Q17-1010,0,0.103511,"Missing"
W19-4610,J08-3005,0,0.0374915,"et al., 2011; Alhanini and Ab Aziz, 2011). Another line of research leverages the templatic nature for human-constructed rule-based constraints (Elghamry, 2005; Rodrigues and Cavar, 2007; Choueka, 1990). Finally, methods have been proposed that utilize both a root dictionary and rule-based templatic constraints (Yaseen and Hmeidi, 2014). Supervised methods have been developed for identifying Hebrew roots by combining various multiclass classification models with Hebrewspecific linguistic constraints (Daya et al., 2004). This same technique was extended to extract both Arabic and Hebrew roots (Daya et al., 2008). While these supervised methods effectively address the non-contiguous nature of Semitic roots, they fail to leverage the sequential structure of the root label space. We show that such methods that forgo the sequential structure in the label space underperform on words with rare roots. Additionally, these methods are only applied to triconsonantal leaving out many biconsonantal and quadriliteral roots. Sequence-to-sequence models have been utilized for learning to map sequences to other sequences and predominantly applied to machine translation (Sutskever et al., 2014), with later variations"
W19-4610,W18-1202,1,0.901188,"Missing"
W19-4610,P17-2072,0,0.100663,"an and Pearson rank correlation coefficients. As seen in Table 5, enriching the embedding vectors with the template-based extracted 5.3 Word Analogy Evaluation Given our comprehensive dataset of Arabic roots and human-curated evaluation set of Arabic word embeddings, we show the effectiveness of enriching Arabic word embeddings with their morphological decompositions via a word analogy task. The goal of said task is to identify the best value for D in analogies of the form “A is to B as C is to D”. After training each embedding model on the Arabic Wikipedia dataset, we use an analogy dataset (Elrazzaz et al., 2017) curated for methodological evaluation of Arabic word embeddings. 94 Embedding Model SkipGram FastText ISRI-RootVec BiGRU-RootVec S2S-RootVec CS2S-RootVec ISRI-TemplaticVec Class-TemplaticVec S2S-TemplaticVec CS2S-TemplaticVec Pearson 0.496 0.459 0.491 0.492 0.508 0.507 0.482 0.474 0.514 0.512 Spearman 0.520 0.468 0.518 0.510 0.516 0.514 0.501 0.491 0.529 0.533 Embedding Model SkipGram FastText ISRI-RootVec BiGRU-RootVec S2S-RootVec CS2S-RootVec ISRI-TemplaticVec Class-TemplaticVec S2S-TemplaticVec CS2S-TemplaticVec Table 5: Word Similarity Table 6: Language Modeling morphemes substantially im"
W19-4610,Q18-1032,0,0.0566188,"Missing"
W19-4610,D14-1181,0,0.00439462,"Missing"
W99-0406,taylor-white-1998-predicting,0,0.0320704,"e language sustainment tools that enables users to guide their own learning during MT-aided tasks, such as filtering, in contrast to single-purpose tutoring systems (e.g., Holland et al., 1995) 3For others addressing multiple uses of linguistic resources, see NLP-IA (1998). 4We use the term divergence as in Dorr (1994). 32 We selected the domain of spatial expressions for evaluation in part because, as example 3 shows, the ambiguity of English spatial prepositions may significantly interfere with the task of accurate message understanding--whether by MT systems or second language learners. As Taylor and White (1998) point out, in a real-world, task-based evaluation of MT systems or language learners, the measure of interest is the correct and incorrect consequences of our users&apos; actions based on their understanding of a foreign language text document. Such measures of effectiveness are difficult to obtain, and researchers, outside of the field, must rely instead on linguistically based measures of performance. Thus, our approach has been to build our test suite relying on extensive pre-existing, linguistically motivated spatial language research (e.g., Bloom et al., 1996, Herskovits, 1986; Jackendoff, 19"
W99-0406,J94-4004,0,\N,Missing
