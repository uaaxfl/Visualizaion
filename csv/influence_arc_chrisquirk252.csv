2003.mtsummit-papers.44,P00-1014,0,0.303289,"eposition information we need to make PP attachment decisions. To avoid the expensive and difficult hand-coding of the fine-grained lexical information that we need for PP attachment resolution, we followed the approach described in the next section. 3 Overview of Our Approach Recently, a variety of approaches to the problem of PP attachment have been described in the literature. These fall into two categories, supervised and unsupervised. Among the unsupervised approaches, which use large, unanalyzed monolingual corpora, are those described in Hindle and Rooth (1993), Ratnaparkhi (1998), and Pantel and Lin (2000). Among the supervised approaches, which learn from disambiguated attachments, are those described in Stetina and Nagao (1997), Collins and Brooks (1995) and Brill and Resnik (1994). Our approach, unlike those above, makes use of bilingual corpora as data. The approach is unsupervised, but it does require a large, parsed, sentence-aligned, bilingual corpus. We exploit the unambiguous nature of PP attachment in Japanese in our approach. Just as our training data differs from those used in the systems mentioned above, so does our goal. Our aim is not to create a new algorithm for reattachment, b"
2003.mtsummit-papers.44,P98-2177,0,0.121881,"not have all the verb-preposition information we need to make PP attachment decisions. To avoid the expensive and difficult hand-coding of the fine-grained lexical information that we need for PP attachment resolution, we followed the approach described in the next section. 3 Overview of Our Approach Recently, a variety of approaches to the problem of PP attachment have been described in the literature. These fall into two categories, supervised and unsupervised. Among the unsupervised approaches, which use large, unanalyzed monolingual corpora, are those described in Hindle and Rooth (1993), Ratnaparkhi (1998), and Pantel and Lin (2000). Among the supervised approaches, which learn from disambiguated attachments, are those described in Stetina and Nagao (1997), Collins and Brooks (1995) and Brill and Resnik (1994). Our approach, unlike those above, makes use of bilingual corpora as data. The approach is unsupervised, but it does require a large, parsed, sentence-aligned, bilingual corpus. We exploit the unambiguous nature of PP attachment in Japanese in our approach. Just as our training data differs from those used in the systems mentioned above, so does our goal. Our aim is not to create a new al"
2003.mtsummit-papers.44,W01-1402,0,0.0275786,"Missing"
2003.mtsummit-papers.44,W97-0109,0,0.0527304,"Missing"
2003.mtsummit-papers.44,J93-1005,0,\N,Missing
2003.mtsummit-papers.44,W01-1406,0,\N,Missing
2003.mtsummit-papers.44,2001.mtsummit-ebmt.4,0,\N,Missing
2003.mtsummit-papers.44,W01-1414,0,\N,Missing
2004.tmi-1.14,J93-2003,0,0.00584889,"A, USA {anthaue, arulm, bobmoore, chrisq, ringger}@microsoft.com Abstract We present a series of models for doing statistical machine translation based on labeled semantic dependency graphs. We describe how these models were employed to augment an existing example-based MT system, and present results showing that doing so led to a significant improvement in translation quality as measured by the BLEU metric. 1. Introduction Much research of late has been devoted to the invention and implementation of statistical machine translation (hereafter: SMT) systems of the kind originally described in (Brown et al., 1993). What these systems have in common is that they try to predict the most likely target language string given an input string in the source language using statistical methods. While traditional SMT systems do not explicitly model structural linguistic properties of the languages they are translating, recent trends in both SMT and speech recognition research suggest that it may be worthwhile to pay more attention to these properties when creating language models. Language models and SMT systems that take syntactic information into account are becoming more numerous. A representative sample inclu"
2004.tmi-1.14,P01-1017,0,0.0353472,"mmon is that they try to predict the most likely target language string given an input string in the source language using statistical methods. While traditional SMT systems do not explicitly model structural linguistic properties of the languages they are translating, recent trends in both SMT and speech recognition research suggest that it may be worthwhile to pay more attention to these properties when creating language models. Language models and SMT systems that take syntactic information into account are becoming more numerous. A representative sample includes the work of Chelba (1997), Charniak (2001), Knight and Yamada (2001), Charniak, Knight and Yamada (2003), and Eisner (2003). The intuition is that syntax-based models ought to be able to capture long-distance dependencies that surface-string models are unable to capture because events that depend on each other are often closer together in the syntax tree than they are in the surface string, in the sense that the distance to a sibling or common parent may be significantly shorter than distance between the same events in the surface string. The system described in this paper is an attempt to take one step further in the same direction:"
2004.tmi-1.14,2003.mtsummit-papers.6,0,0.0674115,"Missing"
2004.tmi-1.14,P98-1035,0,0.0205089,"sentences: 1. John hit the ball. 2. The balls were hit by Lucy. A surface-string-based 3-gram model would count the following n-grams, each n-gram getting count 1: Table 1: Surface string N-gram Counts <Pr&gt; <Pr&gt; John 1 <Pr&gt; John hit 1 John hit the 1 hit the ball 1 the ball <POST&gt; 1 <Pr&gt;<Pr&gt; The 1 <Pr&gt; The balls 1 the balls were 1 balls were hit 1 were hit by 1 hit by Lucy 1 By Lucy <Po&gt; 1 Note that each of these trigrams occurs only once, even though the event (the hitting of a ball) is the same in both cases. If we look at syntax trees, we get a slightly different picture: One could, as in (Chelba and Jelinek, 1998) build up a statistical grammar from a corpus of parse trees such as these. This grammar could then be used in a syntax-tree-based SMT system in order to assign a probability to target language syntax tree candidates. Note, however, that here, too, the hitting of the ball would be split into two separate buckets (one set of rules for the active voice construction and another for the passive voice), and so the system would fail to learn a useful generalization. The dependency graphs our system produces for the two sentences looks like this: The dependency graph target language model would gener"
2004.tmi-1.14,P03-2041,0,0.0296997,"put string in the source language using statistical methods. While traditional SMT systems do not explicitly model structural linguistic properties of the languages they are translating, recent trends in both SMT and speech recognition research suggest that it may be worthwhile to pay more attention to these properties when creating language models. Language models and SMT systems that take syntactic information into account are becoming more numerous. A representative sample includes the work of Chelba (1997), Charniak (2001), Knight and Yamada (2001), Charniak, Knight and Yamada (2003), and Eisner (2003). The intuition is that syntax-based models ought to be able to capture long-distance dependencies that surface-string models are unable to capture because events that depend on each other are often closer together in the syntax tree than they are in the surface string, in the sense that the distance to a sibling or common parent may be significantly shorter than distance between the same events in the surface string. The system described in this paper is an attempt to take one step further in the same direction: if syntactic models are good because they move events that depend on each other c"
2004.tmi-1.14,P03-1054,0,0.00431929,"probability of the target dependency graph τ by the following formula: |τ | PµT (τ ) = ∏ P(ci |ci −1...ci − ( n −1) , µT ) , (4) i where C are all of the nodes in τ, ci-1 denotes the parent of ci, and n is the order of the model. The model is pruned by removing infrequently-occurring n-grams and smoothed using interpolated absolute discounting. Consider the following dependency graph: The probability of the graph according to a trigram DG model would be: P(A |ROOT) * P(R1 |ROOT A) * P (R2 |ROOT A) * P (B |A R1) * P(C |A R2) * P(D |A R2) * P (LEAF |R1 B) * P(LEAF | R2 C) * P(LEAF |R2 D) 1 See Klein and Manning, 2003 for further discussion of vertical Markovization. It would have been nice to include horizontal (i.e. sibling) information in our model as well, but the top-down nature of our decoder (and the performance-motivated incorporation of the Markov assumption therein) made this impractical. Treating semantic relationships (in this example: R1 and R2) as first-class entities simplifies the creation of the model a great deal, since it means we don’t have to train and store two separate models for lemmas and semantic relationships. To see why dependency graph-based models might model certain phenomena"
2004.tmi-1.14,J99-4005,0,0.0744632,"Missing"
2004.tmi-1.14,W01-1411,0,0.0120893,"tion of the decoding algorithm, “total” is the total number of constituents in ms, and Pu is the unnormalized probability.2 Since we are not guaranteed to have a compatible set of mappings covering any given input, we also add default nodes of size 1 for each input lemma and relation not already covered by a single-node mapping from the training data. For relations, the default node is simply a copy of the input relation. For lemmas, we create a new node by translating the input lemma using the most likely single-word translation according to the LLR-based word association model described in (Moore, 2001). Then we compute word translation probabilities over the corpus using IBM Model 1 and assign the resulting probability to the chosen mapping. Since the IBM Model 1 probabilities are obtained by a completely different process than the dependency graph mappings, we have found it advantageous to train up a weight, λL, by which the Model 1 probabilities are multiplied before they are combined with the other channel model probabilities. In addition there is another free parameter, λA, which is the default value assigned to the single-node semantic relationship mappings. Presently, the handling of"
2004.tmi-1.14,P02-1038,0,0.0552518,"source language sentence and T the target language sentence. By Bayes’ rule, T = arg max( P (T |S )) = arg max( P ( S |T ) P(T )). (2) A target language model trained on monolingual target language data is used to compute an estimate of P(T), and channel models of varying complexity are built to compute and estimate P(S|T). In our system, individual candidate mappings and combinations of mappings are scored using a linearly interpolated combination of scores from several heterogeneous information sources. This “kitchen sink” approach to SMT is superficially similar to the system described in (Och and Ney, 2002), except that it works with dependency graph mappings instead of with surface strings. Formally, then, we are looking for the set of transfer mappings Tmax out of all sets of transfer mappings T such that: Μ Tmax = arg max{∑ λµ Scorem (T )} , (3) µ =1 where µ ε Μ are the individual models, each of which assigns a score to a set of transfer mappings. The sources consulted for our system include a probabilistic channel model, target language model, and simple fertility model. Additional information sources whose scores are interpolated with the traditional SMT models include mapping size and num"
2004.tmi-1.14,2001.mtsummit-papers.68,0,0.0153675,"pings Tmax out of all sets of transfer mappings T such that: Μ Tmax = arg max{∑ λµ Scorem (T )} , (3) µ =1 where µ ε Μ are the individual models, each of which assigns a score to a set of transfer mappings. The sources consulted for our system include a probabilistic channel model, target language model, and simple fertility model. Additional information sources whose scores are interpolated with the traditional SMT models include mapping size and number of binary features matched. We train weights for the models using Powell’s algorithm to maximize the BLEU score on the output of the system (Papineni et al., 2001). While the general idea behind these approaches is not new, the innovation in both cases is to apply these techniques at the labeled dependency graph level rather than at the string level. We know of no other system to date that has used statistical techniques to maximize the probability of labeled dependency-graph-to-dependency-graph mappings for machine translation. 3. Models 3.1. Target language model Most SMT systems use surface string n-gram models for their target language model. This has a number of advantages, particularly in a string-to-string translation system. Perhaps most importa"
2004.tmi-1.14,2001.mtsummit-papers.53,1,0.894706,"to bring related events into nearer proximity by explicitly representing semantic dependencies still latent in the syntactic representation. In this paper, we show how our existing example-based MT system using labeled semantic dependency graph translation mappings benefited from the application of SMT techniques while still preserving the benefits provided by rich hierarchical linguistic information. 2. Description of the baseline system and the statistical framework 2.1. The baseline system For an overview of the example-based system to which we’ve applied these statistical techniques, see (Richardson et al, 2001). In a nutshell, the system parses aligned source and target language training sentences using a bottom-up, multi-path chart parsing algorithm, aligns the resulting dependency graphs and creates a set of mappings from source language dependency graph fragments to target language dependency graph fragments (hereafter referred to as DG mappings). The nodes in a given dependency graph represent the lemmas of all concept words in the corresponding sentence. A pair of nodes may be connected by a directed edge labeled by a semantic or deep-syntactic dependency relationship. Associated with each node"
2004.tmi-1.14,P01-1067,0,0.190481,"Missing"
2004.tmi-1.14,P02-1040,0,\N,Missing
2004.tmi-1.14,C98-1035,0,\N,Missing
2005.mtsummit-ebmt.13,N03-1017,0,0.0621711,"Missing"
2005.mtsummit-ebmt.13,J03-1002,0,0.0135815,"Missing"
2005.mtsummit-ebmt.13,P03-1021,0,0.0191425,"Missing"
2005.mtsummit-ebmt.13,N04-1021,0,0.0380107,"Missing"
2005.mtsummit-ebmt.13,P02-1040,0,0.0707835,"Missing"
2005.mtsummit-ebmt.13,C04-1097,0,0.0484298,"Missing"
2005.mtsummit-ebmt.13,2003.mtsummit-papers.53,0,0.0313044,"Missing"
2005.mtsummit-ebmt.13,J00-1004,0,0.0556021,"ation as a process of parallel parsing of the source and target language via a synchronized grammar. To make this process computationally efficient, however, some severe simplifying assumptions are made, such as using a single non-terminal label. This results in the model simply learning a very high level preference regarding how often nodes should switch order without any contextual information. Also these translation models are intrinsically word-based; phrasal combinations are not modeled directly, and results have not been competitive with the top phrasal SMT systems. Along similar lines, Alshawi et al. (2000) treat translation as a process of simultaneous induction of source and target dependency trees using headtransduction; again, no separate parser is used. Yamada and Knight (01) employ a parser in the target language to train probabilities on a set of operations that convert a target language tree to a source language string. This improves fluency slightly (Charniak et al., 03), but fails to significantly impact overall translation quality. This may be because the parser is applied to MT output, which is notoriously unlike native language, and no additional insight is gained via source languag"
2005.mtsummit-ebmt.13,J97-3002,0,0.115569,"Missing"
2005.mtsummit-ebmt.13,2003.mtsummit-papers.6,0,0.0769235,"Missing"
2005.mtsummit-ebmt.13,P01-1067,0,0.192523,"Missing"
2005.mtsummit-ebmt.13,P03-1012,0,0.0264722,"Missing"
2007.mtsummit-papers.43,J93-2003,0,0.0108973,"obabilities. Our choice is completely equivalent, since the product of a set of probabilities is monotonically related to the corresponding sum of log probabilities. • a distortion penalty reflecting the degree of divergence of the order of the target phrases from the order of the source phrases. The probabilities of source phrases given target phrases and target phrases given source phrases are estimated from a word-aligned bilingual corpus. The lexical scores are computed as the log of the unnormalized probability of the Viterbi alignment for a phrase pair under IBM wordtranslation Model 1 (Brown et al., 1993). For each phrase pair extracted from the word-aligned corpus, the values of these four features are stored in a “phrase table”. The target language model is a trigram model smoothed with bigram and unigram language models, estimated from the target language half of the bilingual training corpus. The distortion penalty is computed as required by the Pharaoh decoder, which we explain in Section 4. We train the feature weights for the overall translation model to maximize the B LEU metric using Och’s (2003) minimum-errorrate training procedure. 3. Description of Pharaoh The Pharaoh decoder uses"
2007.mtsummit-papers.43,N03-1017,0,0.0062092,"measured by the B LEU metric (Papineni et al., 2002). The first modification improves the estimated cost function used by Pharaoh to rank partial hypotheses, by incorporating an estimate of the distortion penalty to be incurred in translating the rest of the sentence. The second modification uses early pruning of possible next-phrase translations to cut down the overall size of the search space. These modifications enable decoding speed-ups of an order of magnitude or more, with no reduction in the B LEU score of the resulting translations. 2. A Phrasal SMT Model Phrasal SMT, as described by Koehn et al. (2003) translates a source sentence into a target sentence by decomposing the source sentence into a sequence of source phrases, which can be any contiguous sequences of words (or tokens treated as words) in the source sentence. For each source phrase, a target phrase translation is selected, and the target phrases are arranged in some order to produce the complete translation. A set of possible translation candidates created in this way is scored according to a weighted linear combination of feature values, and the highest scoring translation candidate is selected as the translation of the source s"
2007.mtsummit-papers.43,P06-1065,1,0.854122,"Missing"
2007.mtsummit-papers.43,P03-1021,0,0.00717514,"Missing"
2007.mtsummit-papers.43,P02-1040,0,0.110455,"ems have been much slower than the best RBMT systems. For example, Language Weaver, currently the only commercial provider of SMT systems, claims to translate 5,000 words per minute per CPU,1 , while SYSTRAN, the market leader in commercial RBMT, claims to translate up to 450 words per second (27,000 words per minute) per CPU.2 In this paper, we present two modifications to the algorithm implemented in the widely-used Pharaoh phrasal SMT decoder (Koehn, 2003; Koehn 2004a; Koehn 2004b) that together permit much faster decoding without losing translation quality as measured by the B LEU metric (Papineni et al., 2002). The first modification improves the estimated cost function used by Pharaoh to rank partial hypotheses, by incorporating an estimate of the distortion penalty to be incurred in translating the rest of the sentence. The second modification uses early pruning of possible next-phrase translations to cut down the overall size of the search space. These modifications enable decoding speed-ups of an order of magnitude or more, with no reduction in the B LEU score of the resulting translations. 2. A Phrasal SMT Model Phrasal SMT, as described by Koehn et al. (2003) translates a source sentence into"
2007.mtsummit-papers.43,koen-2004-pharaoh,0,\N,Missing
2007.mtsummit-papers.43,P06-1066,0,\N,Missing
2007.mtsummit-papers.43,J04-4002,0,\N,Missing
2007.mtsummit-papers.43,W06-1905,0,\N,Missing
2007.mtsummit-papers.43,W03-0301,0,\N,Missing
2007.mtsummit-papers.43,J03-1005,0,\N,Missing
2007.mtsummit-papers.50,J93-2003,0,0.00841863,"data. Next, given two news wire corpora, we identify promising sentence pairs using methods very similar to those used in Munteanu and Marcu (2006). Our main innovation comes in identifying the parallel fragments from these comparable sources. In section 2., we describe two new models for extracting parallel fragments, and provide algorithms for effectively using them. We describe the experimental setup and empirical findings in section 3.. In section 4. we discuss our results and we propose some ideas for future exploration. 2. Generative models of fragment alignment In most prior work (e.g. Brown et al. (1993), Vogel et al. (1996)), generative models are used to approximate the translation process. Given a sentence in one language (arbitrarily designed the source, denoted s = sm 1 ), we can find a probability distribution over sentences in the other language (designated target, denoted t = tn1 ). While these models do allow for a certain degree of deviation between sentences, the deviations are assumed to be systematic (e.g. the Spanish word de must often be inserted when generating based on an English string). In noisy comparable sentences, the situation is markedly different: words may be inserte"
2007.mtsummit-papers.50,W04-3208,0,0.188664,"is seldom as carefully edited as newswire data. Therefore we focus on the latter source, but we expect that the techniques developed here should also apply to other sources of comparable data. We loosely use the term “comparable” to describe the document pairs that can be extracted from these newswire sources, though the pairs differ significantly in translational equivalence. Occasionally the articles contain sentence-forsentence translations of one another; there have been several efforts to search for whole-sentence translation pairs within comparable corpora (e.g., Zhao and Vogel (2002), Fung and Cheung (2004b), Fung and Cheung (2004a), Cheung and Fung (2004)). More often it appears that either two reporters have witnessed the same events and written similar accounts or perhaps one reporter has read another reporter’s account and subsequently written a new text with some common information. The latter articles contain few sentence-for-sentence translation pairs. Many researchers have instead tried to gather a bilingual lexicon from these sources, which could then be used by an MT system or even human translators (Fung and Yee (1998), Rapp (1999), Diab and Finch (2000), Koehn and Knight (1999), Gau"
2007.mtsummit-papers.50,C04-1151,0,0.0787245,"is seldom as carefully edited as newswire data. Therefore we focus on the latter source, but we expect that the techniques developed here should also apply to other sources of comparable data. We loosely use the term “comparable” to describe the document pairs that can be extracted from these newswire sources, though the pairs differ significantly in translational equivalence. Occasionally the articles contain sentence-forsentence translations of one another; there have been several efforts to search for whole-sentence translation pairs within comparable corpora (e.g., Zhao and Vogel (2002), Fung and Cheung (2004b), Fung and Cheung (2004a), Cheung and Fung (2004)). More often it appears that either two reporters have witnessed the same events and written similar accounts or perhaps one reporter has read another reporter’s account and subsequently written a new text with some common information. The latter articles contain few sentence-for-sentence translation pairs. Many researchers have instead tried to gather a bilingual lexicon from these sources, which could then be used by an MT system or even human translators (Fung and Yee (1998), Rapp (1999), Diab and Finch (2000), Koehn and Knight (1999), Gau"
2007.mtsummit-papers.50,P98-1069,0,0.288067,"n pairs within comparable corpora (e.g., Zhao and Vogel (2002), Fung and Cheung (2004b), Fung and Cheung (2004a), Cheung and Fung (2004)). More often it appears that either two reporters have witnessed the same events and written similar accounts or perhaps one reporter has read another reporter’s account and subsequently written a new text with some common information. The latter articles contain few sentence-for-sentence translation pairs. Many researchers have instead tried to gather a bilingual lexicon from these sources, which could then be used by an MT system or even human translators (Fung and Yee (1998), Rapp (1999), Diab and Finch (2000), Koehn and Knight (1999), Gaussier et al. (2004), Shao and Ng (2004)). However comparable corpora contain multi-word translation information that is overlooked by these methods. For instance, quoted material from primary sources is often translated literally, as are person names, institution names, and other named entities. We believe that one of the most promising ideas is to identify parallel sub-sentential fragments within comparable corpora, as proposed by Munteanu and Marcu (2006). Starting with a non-parallel corpus consisting of news articles from th"
2007.mtsummit-papers.50,P04-1067,0,0.0242579,"004b), Fung and Cheung (2004a), Cheung and Fung (2004)). More often it appears that either two reporters have witnessed the same events and written similar accounts or perhaps one reporter has read another reporter’s account and subsequently written a new text with some common information. The latter articles contain few sentence-for-sentence translation pairs. Many researchers have instead tried to gather a bilingual lexicon from these sources, which could then be used by an MT system or even human translators (Fung and Yee (1998), Rapp (1999), Diab and Finch (2000), Koehn and Knight (1999), Gaussier et al. (2004), Shao and Ng (2004)). However comparable corpora contain multi-word translation information that is overlooked by these methods. For instance, quoted material from primary sources is often translated literally, as are person names, institution names, and other named entities. We believe that one of the most promising ideas is to identify parallel sub-sentential fragments within comparable corpora, as proposed by Munteanu and Marcu (2006). Starting with a non-parallel corpus consisting of news articles from three sources (the BBC, the Romanian newspapers ‘Evenimentul Zilei’ and ‘Ziua’) they fi"
2007.mtsummit-papers.50,W06-3114,0,0.0135246,",064 91,730 85,232 10,529 8,390 Documents Sentences Words Low recall Document pairs Sentence pairs High recall Document pairs Sentence pairs Spanish 2,223,117 20,177,725 686,902,169 English 3,479,870 49,293,904 1,767,840,671 27,253,262 2,660,283 27,985,397 83,640,447 Table 2: Characteristics of the first pass extraction; size of the raw Gigaword corpora as well as the selected document and sentence pair sets under two parameter settings. Table 1: Characteristics of the parallel seed corpus. 3.1. Data sources As a seed parallel corpus, we use the English-Spanish portion of the Europarl corpus (Koehn and Monz, 2006). Table 1 lists pertinent characteristics of this dataset. We train an HMM alignment model on this parallel data using GIZA++ (Och and Ney, 2003) using 5 iterations of model 1 followed by 5 iterations of the HMM model. This is performed symmetrically in both directions to produce conditional models of Spanish given English and English given Spanish. In addition to the Viterbi alignments, we save the HMM parameters for use in Models A and B. We also build trigram language models (smoothed using modified Kneser-Ney (Goodman, 2001)) on each side to be used both in decoding and in Models A and B."
2007.mtsummit-papers.50,N03-1017,0,0.00281582,"l trend is that although Spanish sentences tend to be longer than English sentences, in several cases the models produce Spanish fragments that are shorter than their English fragments. This may suggest that modeling the length of each fragment could produce more equivalent data pairs. 3.4. MT evaluation Finally we evaluated each of these fragment sets inside a machine translation system. We concatenated the fragment pairs with the original training data, retrained the word alignments on the augmented data again using five iterations of Model 1 and the HMM Model, then extracted phrase tables (Koehn et al., 2003). The language model and phrase tables are then used in a phrasal decoder that faithfully reimplements Pharaoh (Koehn, 2004) to translate the given test sets. Parameter weights are trained for each data set independently using minimum error rate training (Och, 2003) on BLEU (Papineni et al., 2002) using the provided development test set. The test set consists of 2, 000 in-domain sentences drawn from held-out parliamentary data, as well as 1, 064 out-of domain sentences drawn from news commentary web sites. As we see in Table 4, the extracted fragments can positively influence translation quali"
2007.mtsummit-papers.50,koen-2004-pharaoh,0,0.0120671,"agments that are shorter than their English fragments. This may suggest that modeling the length of each fragment could produce more equivalent data pairs. 3.4. MT evaluation Finally we evaluated each of these fragment sets inside a machine translation system. We concatenated the fragment pairs with the original training data, retrained the word alignments on the augmented data again using five iterations of Model 1 and the HMM Model, then extracted phrase tables (Koehn et al., 2003). The language model and phrase tables are then used in a phrasal decoder that faithfully reimplements Pharaoh (Koehn, 2004) to translate the given test sets. Parameter weights are trained for each data set independently using minimum error rate training (Och, 2003) on BLEU (Papineni et al., 2002) using the provided development test set. The test set consists of 2, 000 in-domain sentences drawn from held-out parliamentary data, as well as 1, 064 out-of domain sentences drawn from news commentary web sites. As we see in Table 4, the extracted fragments can positively influence translation quality in the out-ofdomain news commentary data. Adding the MM fragments had a significantly negative impact on in-domain qualit"
2007.mtsummit-papers.50,P06-1011,0,0.836391,"e sources, which could then be used by an MT system or even human translators (Fung and Yee (1998), Rapp (1999), Diab and Finch (2000), Koehn and Knight (1999), Gaussier et al. (2004), Shao and Ng (2004)). However comparable corpora contain multi-word translation information that is overlooked by these methods. For instance, quoted material from primary sources is often translated literally, as are person names, institution names, and other named entities. We believe that one of the most promising ideas is to identify parallel sub-sentential fragments within comparable corpora, as proposed by Munteanu and Marcu (2006). Starting with a non-parallel corpus consisting of news articles from three sources (the BBC, the Romanian newspapers ‘Evenimentul Zilei’ and ‘Ziua’) they first produce a set of similar article pairs using a cross-lingual information retrieval system. Restricting their attention to sentence pairs that contain at least minimal lexical overlap, they search for parallel fragments using an approach inspired by signal processing. Using a set of parameters derived from LLR scores, they annotate each word with a value between −1 and 1 indicating the likelihood that this word has some translational e"
2007.mtsummit-papers.50,J03-1002,0,0.00187076,"2,223,117 20,177,725 686,902,169 English 3,479,870 49,293,904 1,767,840,671 27,253,262 2,660,283 27,985,397 83,640,447 Table 2: Characteristics of the first pass extraction; size of the raw Gigaword corpora as well as the selected document and sentence pair sets under two parameter settings. Table 1: Characteristics of the parallel seed corpus. 3.1. Data sources As a seed parallel corpus, we use the English-Spanish portion of the Europarl corpus (Koehn and Monz, 2006). Table 1 lists pertinent characteristics of this dataset. We train an HMM alignment model on this parallel data using GIZA++ (Och and Ney, 2003) using 5 iterations of model 1 followed by 5 iterations of the HMM model. This is performed symmetrically in both directions to produce conditional models of Spanish given English and English given Spanish. In addition to the Viterbi alignments, we save the HMM parameters for use in Models A and B. We also build trigram language models (smoothed using modified Kneser-Ney (Goodman, 2001)) on each side to be used both in decoding and in Models A and B. The LDC English and Spanish Gigaword corpora (Graff, 2003; Graff, 2006) are a fertile ground for noisy word and phrase alignment. The articles dr"
2007.mtsummit-papers.50,P03-1021,0,0.00213784,"data pairs. 3.4. MT evaluation Finally we evaluated each of these fragment sets inside a machine translation system. We concatenated the fragment pairs with the original training data, retrained the word alignments on the augmented data again using five iterations of Model 1 and the HMM Model, then extracted phrase tables (Koehn et al., 2003). The language model and phrase tables are then used in a phrasal decoder that faithfully reimplements Pharaoh (Koehn, 2004) to translate the given test sets. Parameter weights are trained for each data set independently using minimum error rate training (Och, 2003) on BLEU (Papineni et al., 2002) using the provided development test set. The test set consists of 2, 000 in-domain sentences drawn from held-out parliamentary data, as well as 1, 064 out-of domain sentences drawn from news commentary web sites. As we see in Table 4, the extracted fragments can positively influence translation quality in the out-ofdomain news commentary data. Adding the MM fragments had a significantly negative impact on in-domain quality; we suspect that this is due to the noisy fragments produced by this approach. As the number of fragments in the training data increased, th"
2007.mtsummit-papers.50,P02-1040,0,0.0917429,"evaluation Finally we evaluated each of these fragment sets inside a machine translation system. We concatenated the fragment pairs with the original training data, retrained the word alignments on the augmented data again using five iterations of Model 1 and the HMM Model, then extracted phrase tables (Koehn et al., 2003). The language model and phrase tables are then used in a phrasal decoder that faithfully reimplements Pharaoh (Koehn, 2004) to translate the given test sets. Parameter weights are trained for each data set independently using minimum error rate training (Och, 2003) on BLEU (Papineni et al., 2002) using the provided development test set. The test set consists of 2, 000 in-domain sentences drawn from held-out parliamentary data, as well as 1, 064 out-of domain sentences drawn from news commentary web sites. As we see in Table 4, the extracted fragments can positively influence translation quality in the out-ofdomain news commentary data. Adding the MM fragments had a significantly negative impact on in-domain quality; we suspect that this is due to the noisy fragments produced by this approach. As the number of fragments in the training data increased, the quality of MM suffered even mo"
2007.mtsummit-papers.50,P99-1067,0,0.115958,"able corpora (e.g., Zhao and Vogel (2002), Fung and Cheung (2004b), Fung and Cheung (2004a), Cheung and Fung (2004)). More often it appears that either two reporters have witnessed the same events and written similar accounts or perhaps one reporter has read another reporter’s account and subsequently written a new text with some common information. The latter articles contain few sentence-for-sentence translation pairs. Many researchers have instead tried to gather a bilingual lexicon from these sources, which could then be used by an MT system or even human translators (Fung and Yee (1998), Rapp (1999), Diab and Finch (2000), Koehn and Knight (1999), Gaussier et al. (2004), Shao and Ng (2004)). However comparable corpora contain multi-word translation information that is overlooked by these methods. For instance, quoted material from primary sources is often translated literally, as are person names, institution names, and other named entities. We believe that one of the most promising ideas is to identify parallel sub-sentential fragments within comparable corpora, as proposed by Munteanu and Marcu (2006). Starting with a non-parallel corpus consisting of news articles from three sources ("
2007.mtsummit-papers.50,J03-3002,0,0.0928101,"within a language pair and domain, expand to new domains, or acquire new language pairs, we must find ways to exploit non-parallel data sources. 1.1. Related work There are many ways that we can identify and exploit comparable data. Finding comparable documents is a useful way point in this difficult task: we can significantly reduce the search space of further steps in the pipeline if we limit our attention to information in similar documents. Document pairs can be found from the web by exploiting URL structure, document structure, and lexical similarity amongst other clues (see for instance Resnik and Smith (2003), Zhang et al. (2006), Shi et al. (2006)). Alternatively we can search within large newswire corpora, which can be a rich source of translation information. Crosslingual information retrieval techniques can find promising document pairs from large newswire corpora in different languages, as in (Zhao and Vogel, 2002). Although the web data is likely to be larger and more diverse, it presents obstacles to controlled experimentation (the web is constantly changing) and is seldom as carefully edited as newswire data. Therefore we focus on the latter source, but we expect that the techniques develo"
2007.mtsummit-papers.50,C04-1089,0,0.0106968,"2004a), Cheung and Fung (2004)). More often it appears that either two reporters have witnessed the same events and written similar accounts or perhaps one reporter has read another reporter’s account and subsequently written a new text with some common information. The latter articles contain few sentence-for-sentence translation pairs. Many researchers have instead tried to gather a bilingual lexicon from these sources, which could then be used by an MT system or even human translators (Fung and Yee (1998), Rapp (1999), Diab and Finch (2000), Koehn and Knight (1999), Gaussier et al. (2004), Shao and Ng (2004)). However comparable corpora contain multi-word translation information that is overlooked by these methods. For instance, quoted material from primary sources is often translated literally, as are person names, institution names, and other named entities. We believe that one of the most promising ideas is to identify parallel sub-sentential fragments within comparable corpora, as proposed by Munteanu and Marcu (2006). Starting with a non-parallel corpus consisting of news articles from three sources (the BBC, the Romanian newspapers ‘Evenimentul Zilei’ and ‘Ziua’) they first produce a set of"
2007.mtsummit-papers.50,P06-1062,0,0.0116331,"ew domains, or acquire new language pairs, we must find ways to exploit non-parallel data sources. 1.1. Related work There are many ways that we can identify and exploit comparable data. Finding comparable documents is a useful way point in this difficult task: we can significantly reduce the search space of further steps in the pipeline if we limit our attention to information in similar documents. Document pairs can be found from the web by exploiting URL structure, document structure, and lexical similarity amongst other clues (see for instance Resnik and Smith (2003), Zhang et al. (2006), Shi et al. (2006)). Alternatively we can search within large newswire corpora, which can be a rich source of translation information. Crosslingual information retrieval techniques can find promising document pairs from large newswire corpora in different languages, as in (Zhao and Vogel, 2002). Although the web data is likely to be larger and more diverse, it presents obstacles to controlled experimentation (the web is constantly changing) and is seldom as carefully edited as newswire data. Therefore we focus on the latter source, but we expect that the techniques developed here should also apply to other sour"
2007.mtsummit-papers.50,C96-2141,0,0.0616271,"news wire corpora, we identify promising sentence pairs using methods very similar to those used in Munteanu and Marcu (2006). Our main innovation comes in identifying the parallel fragments from these comparable sources. In section 2., we describe two new models for extracting parallel fragments, and provide algorithms for effectively using them. We describe the experimental setup and empirical findings in section 3.. In section 4. we discuss our results and we propose some ideas for future exploration. 2. Generative models of fragment alignment In most prior work (e.g. Brown et al. (1993), Vogel et al. (1996)), generative models are used to approximate the translation process. Given a sentence in one language (arbitrarily designed the source, denoted s = sm 1 ), we can find a probability distribution over sentences in the other language (designated target, denoted t = tn1 ). While these models do allow for a certain degree of deviation between sentences, the deviations are assumed to be systematic (e.g. the Spanish word de must often be inserted when generating based on an English string). In noisy comparable sentences, the situation is markedly different: words may be inserted or deleted seemingl"
2007.mtsummit-papers.50,I05-1023,0,0.05335,"gradual increase in vocabulary aggregated across iterations might lead to significant differences. Better models could also increase fragment yield. The simplifying assumptions in both models may well be a limitation. For instance, the uniform assumptions over fragment count and length could be replaced with learned models. Furthermore monotone alignments are not sufficient for many language pairs; an extension of the joint model toward ITG (Wu, 1997) could relieve the restriction, though at a cost of greater computational complexity. Despite the hard constraints on reordering imposed by ITG, Wu and Fung (2005) found that an ITG model of translational equivalence was very effective in identifying translation pairs. Even a model with limited reordering is likely to improve over a purely monotone baseline. Noisy translation models also have potential applications beyond fragment extraction. Even so-called parallel corpora often contain loose and noisy translations. Models that allow for sub-sequences of the sentence to not align as well may lead to better alignment quality and better extracted phrase tables. 5. Acknowledgments We thank A. Kumaran and Monojit Choudhury for their comments and Dragos Ste"
2007.mtsummit-papers.50,J97-3002,0,0.0857495,"t new fragments. Such methods could further increase the vocabulary of the MT system. Although only a small addition may occur on the each iteration, the gradual increase in vocabulary aggregated across iterations might lead to significant differences. Better models could also increase fragment yield. The simplifying assumptions in both models may well be a limitation. For instance, the uniform assumptions over fragment count and length could be replaced with learned models. Furthermore monotone alignments are not sufficient for many language pairs; an extension of the joint model toward ITG (Wu, 1997) could relieve the restriction, though at a cost of greater computational complexity. Despite the hard constraints on reordering imposed by ITG, Wu and Fung (2005) found that an ITG model of translational equivalence was very effective in identifying translation pairs. Even a model with limited reordering is likely to improve over a purely monotone baseline. Noisy translation models also have potential applications beyond fragment extraction. Even so-called parallel corpora often contain loose and noisy translations. Models that allow for sub-sequences of the sentence to not align as well may"
2007.mtsummit-papers.50,C98-1066,0,\N,Missing
2008.amta-papers.4,P98-1035,0,0.157867,"Missing"
2008.amta-papers.4,P95-1031,0,0.0921724,"Missing"
2008.amta-papers.4,P05-1063,0,0.0304499,"Missing"
2008.amta-papers.4,N04-1035,0,0.00945573,"Missing"
2008.amta-papers.4,P06-1029,0,0.0611099,"Missing"
2008.amta-papers.4,H05-1049,0,0.0381745,"Missing"
2008.amta-papers.4,P03-1054,0,0.034085,"Missing"
2008.amta-papers.4,P04-1061,0,0.0609874,"Missing"
2008.amta-papers.4,N03-1017,0,0.00846665,"Missing"
2008.amta-papers.4,2004.tmi-1.8,0,0.0192825,"Missing"
2008.amta-papers.4,N04-1021,0,0.0695552,"P approximation to the language model P (x) = z P (x, z). This form of language model, with its syntactic hidden structure, might be able to outperform n-gram methods by capturing long-distance dependencies. Unfortunately, even when tested in domain, scores from generative Treebank parsers fail to consistently rank real sentences above pseudo-negatives, as we show in §5.2. There have been other indications that parse scores do not always behave as useful language models. While investigating the poor performance of parser score as a re-ranking feature for statistical machine translation (SMT), Och et al. (2004) discovered that their Treebank parser had a tendency to rank MT outputs above human reference translations. This is attributed to the fact that MT outputs use fewer unseen words, which causes the parser’s terminal productions to dominate any syntactic distinctions. Attempts to normalize for word choice did not improve the performance of parser probability as a translation feature. Sentence length is another factor that could produce spurious differences in parser probability; however, SMT systems already include a length term in their discriminatively weighted linear models, so this does not"
2008.amta-papers.4,2003.mtsummit-papers.6,0,0.458548,"Missing"
2008.amta-papers.4,D08-1076,0,0.0258286,"Missing"
2008.amta-papers.4,P07-1010,0,0.112831,"ences, as sampled sentences include both syntactic disfluencies and semantic nonsense, such as: We construct a discriminative, syntactic language model (LM) by using a latent support vector machine (SVM) to train an unlexicalized parser to judge sentences. That is, the parser is optimized so that correct sentences receive high-scoring trees, while incorrect sentences do not. Because of this alternative objective, the parser can be trained with only a part-of-speech dictionary and binary-labeled sentences. We follow the paradigm of discriminative language modeling with pseudonegative examples (Okanohara and Tsujii, 2007), and demonstrate significant improvements in distinguishing real sentences from pseudo-negatives. We also investigate the related task of separating machine-translation (MT) outputs from reference translations, again showing large improvements. Finally, we test our LM in MT reranking, and investigate the language-modeling parser in the context of unsupervised parsing. 1 • Basically, we are a fighter jet. • The shortage of topsoil moisture in a personal basis, unlike his book, and extended last summer’s semiconductor’s partner in a big banks. Discriminating between sampled and human sentences"
2008.amta-papers.4,J01-2004,0,0.172013,"Missing"
2008.amta-papers.4,W04-3201,0,0.065068,"Missing"
2008.amta-papers.4,C98-1035,0,\N,Missing
2008.amta-papers.4,W06-3114,0,\N,Missing
2008.amta-papers.4,P01-1017,0,\N,Missing
2010.amta-papers.33,P08-1087,0,0.0943135,"., 2005; Chan et al., 2007). Although their integration efforts have shown promising results, none of these works have focused on the problem of translating from morphologically poor languages into morphologically rich languages. Since such languages generally have complicated rules for generating inflections, modeling a morphologically rich lexicon and disambiguating its senses is particularly challenging. In SMT in general, there has only been a limited amount of work applying morphological processing for translating from English to morphologically rich languages. Of those, Bojar (2007) and Avramidis and Koehn (2008) used morphological features in the factored representation of words implemented in the Moses system. However, this approach is difficult to apply when the syntax-based SMT framework is used, and it tends to focus on simple generative translation models that partition the input into chunks, along with target language models to help model fluency, rather than discriminative models for picking aspects of translation based on varying amounts of context. Other studies closely related to ours are the works on Japanese case-marker generation (Toutanova and Suzuki, 2007) and morphological inflection"
2010.amta-papers.33,P07-1020,0,0.0559774,"h errors introduced by re-ranking while leading to a faster overall pipeline. For the rest of the paper, we first give a review of the literature in Section 2. Section 3 describes our approach in more detail. We describe our experiments with several feature sets and language pairs, both with intrinsic evaluations of lexical selection and in end-to-end BLEU scores in Sections 4 and 5. 2 Related Work Much of the work on discriminative lexicon models has focused on target word selection, under the assumption that sentence-level global information is useful in finding good translation candidates. Bangalore et al. (2007) framed the problem of selecting translated words from a target lexicon as a binary classification task, where each word in the target vocabulary included in or excluded from the translation according to its binary decision. In a similar vein, Mauser et al. (2009) integrated discriminative and trigger-based lexicon models into phrase-based Chinese-English MT system, showing an improvement of the BLEU score. The goal of both of these papers on lexicon modeling was to propose appropriate translated words, looking beyond word- and phrase-based translation pairs by including source context informa"
2010.amta-papers.33,W07-0735,0,0.0760361,"05; Vickrey et al., 2005; Chan et al., 2007). Although their integration efforts have shown promising results, none of these works have focused on the problem of translating from morphologically poor languages into morphologically rich languages. Since such languages generally have complicated rules for generating inflections, modeling a morphologically rich lexicon and disambiguating its senses is particularly challenging. In SMT in general, there has only been a limited amount of work applying morphological processing for translating from English to morphologically rich languages. Of those, Bojar (2007) and Avramidis and Koehn (2008) used morphological features in the factored representation of words implemented in the Moses system. However, this approach is difficult to apply when the syntax-based SMT framework is used, and it tends to focus on simple generative translation models that partition the input into chunks, along with target language models to help model fluency, rather than discriminative models for picking aspects of translation based on varying amounts of context. Other studies closely related to ours are the works on Japanese case-marker generation (Toutanova and Suzuki, 2007"
2010.amta-papers.33,N07-1007,1,0.855715,"es. Of those, Bojar (2007) and Avramidis and Koehn (2008) used morphological features in the factored representation of words implemented in the Moses system. However, this approach is difficult to apply when the syntax-based SMT framework is used, and it tends to focus on simple generative translation models that partition the input into chunks, along with target language models to help model fluency, rather than discriminative models for picking aspects of translation based on varying amounts of context. Other studies closely related to ours are the works on Japanese case-marker generation (Toutanova and Suzuki, 2007) and morphological inflection prediction for Russian and Arabic (Toutanova et al., 2008). They built probabilistic models for morphology generation and applied them to rescore n-best outputs from an SMT engine, augmenting those outputs with additional inflectional variations. In contrast, we focus on direct integration of our discriminative lexicon model with a dependency tree-to-string translation system, in which syntatic features obtained from an English parser are naturally incorporated in the first-pass SMT decoder. 3 Discriminative Lexicon Model This section describes our discriminative"
2010.amta-papers.33,P05-1048,0,0.0278346,"ion pairs by including source context information as bag-of-words features and triggerbased models. The same objective has motivated work on word sense disambiguation (WSD) for machine translation, in which different senses of a word in the source language are defined as its possible translations in the target language (Berger et al., 1996). Again the correct sense, and therefore the correct translation, depends on the specific meaning of the word in context. Recently there has been quite a bit of research on integrating discriminatively trained WSD systems into SMT (Cabezas and Resnik, 2005; Carpuat and Wu, 2005; Vickrey et al., 2005; Chan et al., 2007). Although their integration efforts have shown promising results, none of these works have focused on the problem of translating from morphologically poor languages into morphologically rich languages. Since such languages generally have complicated rules for generating inflections, modeling a morphologically rich lexicon and disambiguating its senses is particularly challenging. In SMT in general, there has only been a limited amount of work applying morphological processing for translating from English to morphologically rich languages. Of those, Bo"
2010.amta-papers.33,J07-2003,0,0.0206916,"anslation pairs and order templates can be combined to form the following transduction rules: (run/Verb (x1: /Noun)) ((x1) 실행하시오) ((x1: /Det) query/Noun) ((x1) 쿼리를) (this) (이) Given rules of this form, finding the best translation is a matter of searching for the best derivation according to a sentence specific grammar. In the absence of a language model or other models that score based on context, we may simply use a standard parsing algorithm such as CKY to find the best derivation. In the presence of context sensitive features, we resort to the approximate search technique of cube pruning (Chiang, 2007). The baseline treelet system scores candidates with a weighted linear combination of features, with weights trained by Minimum Error Rate Training (Och, 2003), hereafter referred to as MERT. The baseline set of feature functions are: log probabilities of the source treelet given the target treelet and vice versa (maximum likelihood estimates); forward and backward lexical weighting; a target language log probability from a Kneser-Ney smoothed language model; word and phrase count feature functions, and order template log probabilities (maximum likelihood estimates). For all features except th"
2010.amta-papers.33,P07-1005,0,0.0389986,"ation as bag-of-words features and triggerbased models. The same objective has motivated work on word sense disambiguation (WSD) for machine translation, in which different senses of a word in the source language are defined as its possible translations in the target language (Berger et al., 1996). Again the correct sense, and therefore the correct translation, depends on the specific meaning of the word in context. Recently there has been quite a bit of research on integrating discriminatively trained WSD systems into SMT (Cabezas and Resnik, 2005; Carpuat and Wu, 2005; Vickrey et al., 2005; Chan et al., 2007). Although their integration efforts have shown promising results, none of these works have focused on the problem of translating from morphologically poor languages into morphologically rich languages. Since such languages generally have complicated rules for generating inflections, modeling a morphologically rich lexicon and disambiguating its senses is particularly challenging. In SMT in general, there has only been a limited amount of work applying morphological processing for translating from English to morphologically rich languages. Of those, Bojar (2007) and Avramidis and Koehn (2008)"
2010.amta-papers.33,erjavec-2004-multext,0,0.0185888,"tWord GramFeat(AlignedWord)+GramFeat(Word-1) +GramFeat(Word+1)+GramFeat(Parent)+GramFeat(Grand parent) & GramFeat(Target) GramFeat(AlignedWord)+GramFeat(Word-1) +GramFeat(Word+1)+GramFeat(Parent)+GramFeat(Grand parent)+AlignedWord & GramFeat(Target) Lemma(AlignedWord) & Lemma(Target) Lemma(Word-1) & Lemma(Target) Lemma(Word+1) & Lemma(Target) Table 1:Features for discriminative lexicon model lemma. The function GramFeat(w) in Table 1 denotes a conjunction of all values of grammatical features as well as the POS tag. For Bulgarian, we created the lexicon from the Multext-East (Version 3) data (Erjavec, 2004), which contains about 41K distinct surface word forms and 23K lemmas. Our Czech lexicon was created using the training and development section of the CzEng corpus data (Bojar and Žabokrtský, 2009), resulting in about 800K distinct word forms and 431K lemmas. For English, we used the lexicon of the dependency Bulgarian Czech Korean Train Dev Test 345K (4.4M) 138K (2.5M) 2.8M (24M) 2K (51K) 2K (39K) 2K (15K) 3K (72K) 3K (60K) 3K (25K) Random/ Oracle Acc@1 5.8 / 96.2 7.8 / 90.5 6.0 / 95.7 Table 2: Parallel data for discriminative lexicon training statistics: # sentence pairs (extracted example s"
2010.amta-papers.33,P07-1104,1,0.852498,"r ranking models is scalability. Training with a large amount of parallel data is the norm in modern SMT systems; therefore, our model needs to accommodate a large set of training examples and features. To do this, we used an online learning approach, namely stochastic gradient descent (SGD) with L1-regularization. In this learning procedure, each example is evaluated sequentially for parameter updates so that the learning algorithm requires a minimal memory footprint. In addition, using an L1-regularizer in the log-linear model has the desirable property of reducing the number of parameters (Gao et al., 2007). We adopt an efficient gradient calculation method for L1regularized log-linear model proposed in (Langford et al., 2009; Tsuruoka et al., 2009). Two hyperparameters, learning rate and regularization prior, are determined using a development set in our experiments. The training time depends on the number of features of the model. On the biggest Korean dataset, using a single CPU, training took about 12 hours per iteration for the local feature set, and about 24 hours per iteration for the local+deptree+morph feature set. About 2 to 4 iterations were sufficient for good performance. The indepe"
2010.amta-papers.33,D09-1022,0,0.0413963,"language pairs, both with intrinsic evaluations of lexical selection and in end-to-end BLEU scores in Sections 4 and 5. 2 Related Work Much of the work on discriminative lexicon models has focused on target word selection, under the assumption that sentence-level global information is useful in finding good translation candidates. Bangalore et al. (2007) framed the problem of selecting translated words from a target lexicon as a binary classification task, where each word in the target vocabulary included in or excluded from the translation according to its binary decision. In a similar vein, Mauser et al. (2009) integrated discriminative and trigger-based lexicon models into phrase-based Chinese-English MT system, showing an improvement of the BLEU score. The goal of both of these papers on lexicon modeling was to propose appropriate translated words, looking beyond word- and phrase-based translation pairs by including source context information as bag-of-words features and triggerbased models. The same objective has motivated work on word sense disambiguation (WSD) for machine translation, in which different senses of a word in the source language are defined as its possible translations in the targ"
2010.amta-papers.33,P03-1021,0,0.00904146,"를) (this) (이) Given rules of this form, finding the best translation is a matter of searching for the best derivation according to a sentence specific grammar. In the absence of a language model or other models that score based on context, we may simply use a standard parsing algorithm such as CKY to find the best derivation. In the presence of context sensitive features, we resort to the approximate search technique of cube pruning (Chiang, 2007). The baseline treelet system scores candidates with a weighted linear combination of features, with weights trained by Minimum Error Rate Training (Och, 2003), hereafter referred to as MERT. The baseline set of feature functions are: log probabilities of the source treelet given the target treelet and vice versa (maximum likelihood estimates); forward and backward lexical weighting; a target language log probability from a Kneser-Ney smoothed language model; word and phrase count feature functions, and order template log probabilities (maximum likelihood estimates). For all features except the language model, we may pre-compute the weighted score of each model. Then, during decoding, we only need to update the score of the language model as larger"
2010.amta-papers.33,P05-1034,1,0.912393,"gth of our model over a plain tree-to-string translation system is that a discriminative model can easily incorporate rich contextual features, such as neighboring words and dependency relations, which are often absent in the generative translation models. 3.1 Treelet Translation System In this work, we build a discriminative lexicon model over a treelet translation system, a depento run this query , 이 쿼리를 실행하려면 [this] [query] [to run] enter 매개 values 변수 [parameter] for its parameters . 값을 입력하십시오 . [value] [enter] Figure 1: Aligned English-Korean sentence pair dency tree-to-string SMT system (Quirk et al., 2005). A treelet is defined as a connected subgraph of a dependency tree, which acts as a unit in the channel model for decoding much like phrases in phrasal SMT models (Koehn et al., 2003). A treelet translation pair is a pair of source and target language treelets, which are extracted from wordaligned sentence pairs. Figure 1 shows an example of an aligned English-Korean sentence pair: a directed arc indicates a dependency relation, which is derived from an English dependency parser and is projected onto the Korean side. For instance, from the phrase “to run this query” in Figure 1, we can extrac"
2010.amta-papers.33,steinberger-etal-2006-jrc,0,0.0191679,"weights that optimize the BLEU score of this development set. 5.3 Data Our full datasets for the three languages are described in Table 7. They consist of training sets (train), dev sets for tuning the weights of the MT component models (MERT dev), and final test sets for evaluating the translation performance (test). As mentioned earlier, the training data for the discriminative lexicon models is a subset of the MT training data: it includes almost all MT training data, excluding 5K sentences for development and testing. For Bulgarian, we used a 300K sentence subset of the JRC-Aquis corpus (Steinberger et al., 2006) for training. The MERT development set and the test sets are from a variety of sources from more general domains and are thus out-of-domain with respect to the training set. The MT system for Bulgarian used a maximum treelet size of 4. For Czech we used data from the EACL 2009 fourth workshop on SMT. Our test set is newsdev2009b, our MERT dev set is 500 sentences from news-dev2009a, and our training set is the union of the news-commentary09 corpus and the news portion of the CzEng corpus data (Bojar and Žabokrtský, 2009) from sections 0 to 7, with duplicates removed. The MT system used a maxi"
2010.amta-papers.33,P09-1054,0,0.0208026,"to accommodate a large set of training examples and features. To do this, we used an online learning approach, namely stochastic gradient descent (SGD) with L1-regularization. In this learning procedure, each example is evaluated sequentially for parameter updates so that the learning algorithm requires a minimal memory footprint. In addition, using an L1-regularizer in the log-linear model has the desirable property of reducing the number of parameters (Gao et al., 2007). We adopt an efficient gradient calculation method for L1regularized log-linear model proposed in (Langford et al., 2009; Tsuruoka et al., 2009). Two hyperparameters, learning rate and regularization prior, are determined using a development set in our experiments. The training time depends on the number of features of the model. On the biggest Korean dataset, using a single CPU, training took about 12 hours per iteration for the local feature set, and about 24 hours per iteration for the local+deptree+morph feature set. About 2 to 4 iterations were sufficient for good performance. The independent models can be straightforwardly parallelized on thousands of CPUs and can thus be much faster. We are currently working on parallelizing th"
2010.amta-papers.33,H05-1097,0,0.0358384,"source context information as bag-of-words features and triggerbased models. The same objective has motivated work on word sense disambiguation (WSD) for machine translation, in which different senses of a word in the source language are defined as its possible translations in the target language (Berger et al., 1996). Again the correct sense, and therefore the correct translation, depends on the specific meaning of the word in context. Recently there has been quite a bit of research on integrating discriminatively trained WSD systems into SMT (Cabezas and Resnik, 2005; Carpuat and Wu, 2005; Vickrey et al., 2005; Chan et al., 2007). Although their integration efforts have shown promising results, none of these works have focused on the problem of translating from morphologically poor languages into morphologically rich languages. Since such languages generally have complicated rules for generating inflections, modeling a morphologically rich lexicon and disambiguating its senses is particularly challenging. In SMT in general, there has only been a limited amount of work applying morphological processing for translating from English to morphologically rich languages. Of those, Bojar (2007) and Avramid"
2010.amta-papers.33,J96-1002,0,\N,Missing
2010.amta-papers.33,P08-1059,1,\N,Missing
2010.amta-papers.33,D08-1076,0,\N,Missing
2011.mtsummit-papers.10,J93-2003,0,0.01827,"etraining or customizing a system. Second, we explore intentional over-fitting of word alignment models. As a special case in machine learning, word alignment for Machine Translation may actually benefit by over-fitting for a specific domain. We discuss this issue and suggest a two-step word alignment scheme to improve quality. 106 Word alignment is a crucial component of stateof-the-art statistical machine translation technology. Most translation models are built upon the word alignment output (Och and Ney, 2004; Chiang, 2007; Quirk and Menezes, 2006; Galley et al., 2004). Generative models (Brown et al., 1993; Vogel et al., 1996; He, 2007; Och and Ney, 2003) are widely used because of their ability to utilize sentence-aligned corpora without manual annotation. However, generative word alignment is a time-consuming process, especially in production environments where new data are constantly being added. In this case, running word alignment repeatedly for millions of sentences to gauge the impact of several thousand new sentences can be a waste of valuable resources. In this work, we explore different ways of performing fast incremental training of word alignment models and incorporating the alignme"
2011.mtsummit-papers.10,J07-2003,0,0.0501201,"small amount of new data, in order to minimize processing time associated with retraining or customizing a system. Second, we explore intentional over-fitting of word alignment models. As a special case in machine learning, word alignment for Machine Translation may actually benefit by over-fitting for a specific domain. We discuss this issue and suggest a two-step word alignment scheme to improve quality. 106 Word alignment is a crucial component of stateof-the-art statistical machine translation technology. Most translation models are built upon the word alignment output (Och and Ney, 2004; Chiang, 2007; Quirk and Menezes, 2006; Galley et al., 2004). Generative models (Brown et al., 1993; Vogel et al., 1996; He, 2007; Och and Ney, 2003) are widely used because of their ability to utilize sentence-aligned corpora without manual annotation. However, generative word alignment is a time-consuming process, especially in production environments where new data are constantly being added. In this case, running word alignment repeatedly for millions of sentences to gauge the impact of several thousand new sentences can be a waste of valuable resources. In this work, we explore different ways of perfo"
2011.mtsummit-papers.10,N04-1035,0,0.0269683,"inimize processing time associated with retraining or customizing a system. Second, we explore intentional over-fitting of word alignment models. As a special case in machine learning, word alignment for Machine Translation may actually benefit by over-fitting for a specific domain. We discuss this issue and suggest a two-step word alignment scheme to improve quality. 106 Word alignment is a crucial component of stateof-the-art statistical machine translation technology. Most translation models are built upon the word alignment output (Och and Ney, 2004; Chiang, 2007; Quirk and Menezes, 2006; Galley et al., 2004). Generative models (Brown et al., 1993; Vogel et al., 1996; He, 2007; Och and Ney, 2003) are widely used because of their ability to utilize sentence-aligned corpora without manual annotation. However, generative word alignment is a time-consuming process, especially in production environments where new data are constantly being added. In this case, running word alignment repeatedly for millions of sentences to gauge the impact of several thousand new sentences can be a waste of valuable resources. In this work, we explore different ways of performing fast incremental training of word alignme"
2011.mtsummit-papers.10,W07-0711,0,0.148015,"d, we explore intentional over-fitting of word alignment models. As a special case in machine learning, word alignment for Machine Translation may actually benefit by over-fitting for a specific domain. We discuss this issue and suggest a two-step word alignment scheme to improve quality. 106 Word alignment is a crucial component of stateof-the-art statistical machine translation technology. Most translation models are built upon the word alignment output (Och and Ney, 2004; Chiang, 2007; Quirk and Menezes, 2006; Galley et al., 2004). Generative models (Brown et al., 1993; Vogel et al., 1996; He, 2007; Och and Ney, 2003) are widely used because of their ability to utilize sentence-aligned corpora without manual annotation. However, generative word alignment is a time-consuming process, especially in production environments where new data are constantly being added. In this case, running word alignment repeatedly for millions of sentences to gauge the impact of several thousand new sentences can be a waste of valuable resources. In this work, we explore different ways of performing fast incremental training of word alignment models and incorporating the alignment results of the new data int"
2011.mtsummit-papers.10,N10-1062,0,0.197899,"is used for word alignment. The model is a generative HMM alignment model with a word-dependent distortion model, where parameters are estimated in a maximum likelihood fashion using the EM algorithm. The implementation is highly optimized, allowing both multithreading and distributed computing. The basic strategy of incremental training is to utilize an existing word alignment model, updating the models on the new data. By running EM only on the smaller amount of new data, we effectively cut down the time needed for training a new system. Step-wise EM for word alignment has been explored in (Levenberg et al., 2010; Liang and Klein, 2009), where sufficient statistics on mini-batches are collected and interpolated with the general baseline. In this work, we do not store these statistics. Instead, we explore the possibility of utilizing the model parameters directly. As we will see later in the second problem we want to address, this allows us to over-fit more radically towards incremental data or specific domains, instead of pursuing a model for the whole corpus. There are two component models inside WDHMM: the lexical translation modelሺ ȁ ሻ, which models the probability of, say, a given French w"
2011.mtsummit-papers.10,N09-1069,0,0.0220975,"nt. The model is a generative HMM alignment model with a word-dependent distortion model, where parameters are estimated in a maximum likelihood fashion using the EM algorithm. The implementation is highly optimized, allowing both multithreading and distributed computing. The basic strategy of incremental training is to utilize an existing word alignment model, updating the models on the new data. By running EM only on the smaller amount of new data, we effectively cut down the time needed for training a new system. Step-wise EM for word alignment has been explored in (Levenberg et al., 2010; Liang and Klein, 2009), where sufficient statistics on mini-batches are collected and interpolated with the general baseline. In this work, we do not store these statistics. Instead, we explore the possibility of utilizing the model parameters directly. As we will see later in the second problem we want to address, this allows us to over-fit more radically towards incremental data or specific domains, instead of pursuing a model for the whole corpus. There are two component models inside WDHMM: the lexical translation modelሺ ȁ ሻ, which models the probability of, say, a given French word  and an English w"
2011.mtsummit-papers.10,J03-1002,0,0.0504553,"ore intentional over-fitting of word alignment models. As a special case in machine learning, word alignment for Machine Translation may actually benefit by over-fitting for a specific domain. We discuss this issue and suggest a two-step word alignment scheme to improve quality. 106 Word alignment is a crucial component of stateof-the-art statistical machine translation technology. Most translation models are built upon the word alignment output (Och and Ney, 2004; Chiang, 2007; Quirk and Menezes, 2006; Galley et al., 2004). Generative models (Brown et al., 1993; Vogel et al., 1996; He, 2007; Och and Ney, 2003) are widely used because of their ability to utilize sentence-aligned corpora without manual annotation. However, generative word alignment is a time-consuming process, especially in production environments where new data are constantly being added. In this case, running word alignment repeatedly for millions of sentences to gauge the impact of several thousand new sentences can be a waste of valuable resources. In this work, we explore different ways of performing fast incremental training of word alignment models and incorporating the alignment results of the new data into the existing trans"
2011.mtsummit-papers.10,J04-4002,0,0.0182969,"al training over a small amount of new data, in order to minimize processing time associated with retraining or customizing a system. Second, we explore intentional over-fitting of word alignment models. As a special case in machine learning, word alignment for Machine Translation may actually benefit by over-fitting for a specific domain. We discuss this issue and suggest a two-step word alignment scheme to improve quality. 106 Word alignment is a crucial component of stateof-the-art statistical machine translation technology. Most translation models are built upon the word alignment output (Och and Ney, 2004; Chiang, 2007; Quirk and Menezes, 2006; Galley et al., 2004). Generative models (Brown et al., 1993; Vogel et al., 1996; He, 2007; Och and Ney, 2003) are widely used because of their ability to utilize sentence-aligned corpora without manual annotation. However, generative word alignment is a time-consuming process, especially in production environments where new data are constantly being added. In this case, running word alignment repeatedly for millions of sentences to gauge the impact of several thousand new sentences can be a waste of valuable resources. In this work, we explore different"
2011.mtsummit-papers.10,C96-2141,0,0.757812,"zing a system. Second, we explore intentional over-fitting of word alignment models. As a special case in machine learning, word alignment for Machine Translation may actually benefit by over-fitting for a specific domain. We discuss this issue and suggest a two-step word alignment scheme to improve quality. 106 Word alignment is a crucial component of stateof-the-art statistical machine translation technology. Most translation models are built upon the word alignment output (Och and Ney, 2004; Chiang, 2007; Quirk and Menezes, 2006; Galley et al., 2004). Generative models (Brown et al., 1993; Vogel et al., 1996; He, 2007; Och and Ney, 2003) are widely used because of their ability to utilize sentence-aligned corpora without manual annotation. However, generative word alignment is a time-consuming process, especially in production environments where new data are constantly being added. In this case, running word alignment repeatedly for millions of sentences to gauge the impact of several thousand new sentences can be a waste of valuable resources. In this work, we explore different ways of performing fast incremental training of word alignment models and incorporating the alignment results of the ne"
2011.mtsummit-papers.48,N09-4002,0,0.0242918,"Missing"
2011.mtsummit-papers.48,P01-1020,0,0.110141,"stems trained on the filtered corpus showed only a small improvement in BLEU (Papineni, 2002) over a random baseline. Other work in MT detection was mostly aimed at finding new methods for automatic MT evaluation. As human translations are considered to be of much higher quality than MT, the task of MT evaluation can be recast as that of determining how “human-like” some MT output is. Several researchers have thus framed MT evaluation as a classification problem, where the quality of a translated sentence is judged to be proportional to the classifier’s confidence that it is human-translated. Corston-Oliver et al. (2001) developed a decision tree classifier designed to determine whether a sentence was human-translated or machinetranslated, without need for reference translations. Their model uses two main groups of features: (1) perplexity measures from a lexicalized language model, and (2) various linguistic features, such as branching properties of parses, and the number of pre- and post-modifiers found in the sentence. They evaluated their system using a corpus of 180,000 English sentences (half human-translated from Spanish, and half machine-translated) and were able to significantly outperform the baseli"
2011.mtsummit-papers.48,2005.eamt-1.15,0,0.0553077,"eloped a decision tree classifier designed to determine whether a sentence was human-translated or machinetranslated, without need for reference translations. Their model uses two main groups of features: (1) perplexity measures from a lexicalized language model, and (2) various linguistic features, such as branching properties of parses, and the number of pre- and post-modifiers found in the sentence. They evaluated their system using a corpus of 180,000 English sentences (half human-translated from Spanish, and half machine-translated) and were able to significantly outperform the baseline. Gamon et al. (2005) developed a system that combined scores from an n-gram language model with those output by an SVM classifier to identify “highly disfluent or ill-formed sentences”. The 423 specific features extracted from the parses differ somewhat from (Corston-Oliver et al., 2001): the system extracts part of speech tag trigrams, context-free grammar productions, and a number of semantic features such as definiteness of noun phrases, semantic relationship between parent and child nodes, and semantic modification relations. Their system achieved a correlation with human judgment on translation quality that"
2011.mtsummit-papers.48,P02-1040,0,0.0838537,"Missing"
2011.mtsummit-papers.48,P05-1034,1,0.736039,"ranslation Quality Our final method of evaluation is to compare SMT systems trained on data filtered with our algorithm to a baseline system. For these experiments, we use our classifier to score and rank a very large set of document pairs (hundreds of thousands per language pair, which contain millions of aligned sentence pairs). For each data point, we then select the N sentences with the highest scores and either add them to a trusted, human-translated training set to train an MT system, or train an MT system on them in isolation. Our models were trained using a treelet translation system (Quirk et al, 2005). Finally, we compute BLEU scores for the resulting systems on a variety of test sets. The MT systems trained on baseline and filtered data sets were evaluated using either one or two test sets. We evaluated BLEU on a newswire test set for each language pair, and for EnglishRomanian and English-Latvian, we also evaluated on a second, domain-balanced test set. These results are reported in Table 6. We have several baselines systems for these experiments. The first is a system trained on just the core data set alone. Second, we have systems trained on the same number of randomly sampled sentence"
2011.mtsummit-papers.48,J03-3002,0,0.0605147,"to classify very large corpora (generally at least several hundred thousand document pairs per language pair), so we must ensure that our solution is efficient and scalable. 3 System Overview The core of our algorithm is a maximum entropy classifier. It assigns scores to candidate document pairs based on its confidence that the translation is adequate and fluent on both sides. The term “side” here refers to the half of the document pair that comes from one of the two languages under consideration. Our system is fed these document pair objects by our Web extractor, which is inspired by STRAND (Resnik and Smith, 2003), but which has significant improvements. Each document pair object consists of the following data: x URL of each side of the web page x Full HTML for each side x A list of aligned sentence pairs x Sentence-broken text for each side x Static Rank for each side 2 Some features used by the document-level classifier are derived from a second, sentence-level maximum entropy classifier, which scores all sentencepairs found in each document pairs by a sentence aligner. The interaction between the two classifiers is depicted in Figure 1. 3.1 Figure 1 - System Flowchart  count of OOV tokens containin"
2011.mtsummit-papers.48,C10-1124,0,0.0460992,"Missing"
2011.mtsummit-papers.48,D11-1033,0,\N,Missing
2011.mtsummit-papers.49,J93-2003,0,0.061902,"Missing"
2011.mtsummit-papers.49,J09-4009,0,0.183874,"lignments that can be generated are perfectly straight diagonals. Given a grammar that can generate inﬁnitely long sentence pairs, the set of these token alignments (one for every possible sentence length) is also inﬁnitely large, which means that we cannot compare the absolute sizes of the sets. Instead we will observe how the number of token alignment grows as a function of their length, and for easy comparison to following grammars, we will express it as a recurrence formula: aF1 = 1, aFn = aFn−1 + 1 Moving on to inversion transduction grammars (ITGs), we know from previous work (Wu, 1997; Huang et al., 2009) that the number of token alignments up to and including length n is equal to the nth large Schr¨oder numbers (Schr¨oder, 1870), which can be expressed as: aI1 = 1, aI2 = 2, aIn = 6n−9 n aIn−1 − n−3 n aIn−2 Finally, we have the arbitrary rank syntax-directed transduction grammars (SDTGs), which are capable of generating any permutation (Lewis and Stearns, 433 1968; Aho and Ullman, 1972). The number of permutations are n!, which we can also formulate as a recurrence formula: aT1 = 1, aTn = naTn−1 It should be clear that these series grow at different paces, and that: aFn &lt; aIn &lt; aTn 4 Weak alig"
2011.mtsummit-papers.49,N03-1017,0,0.0699369,"Missing"
2011.mtsummit-papers.49,W02-1018,0,0.0540128,"Missing"
2011.mtsummit-papers.49,2011.eamt-1.42,1,0.872194,"ments that are allowed under a transduction as its weak alignment capacity. This aspect of expressivity is quantiﬁed for linear transductions using preterminalized linear inversion transduction grammars, and compared to the expressivity of ﬁnite-state transductions, inversion transductions and syntax-directed transductions. 1 Introduction We investigate the formal expressivity properties of linear transductions, the class of transductions generated by linear transduction grammars (Saers, 2011, LTGs), linear inversion transduction grammars (Saers et al., 2010, LITGs) and preterminalized LITGs (Saers and Wu, 2011, PLITGs). While empirical results such as those in previous work are of course an ultimate test of modeling adequacy for machine translation applications, it is 431 Although not always given the attention deserved, the expressivity of different grammar-based methods varies quite a lot. Unlike monolingual grammars, where all context-free grammars, for example, fall into one class of languages, the bilingual case is not as well-behaved. Syntax-directed transduction grammars (Lewis and Stearns, 1968; Aho and Ullman, 1972) form distinct transduction classes for all ranks above 3, while ranks 2 an"
2011.mtsummit-papers.49,N10-1050,1,0.938221,"s generated. We refer to the number of different alignments that are allowed under a transduction as its weak alignment capacity. This aspect of expressivity is quantiﬁed for linear transductions using preterminalized linear inversion transduction grammars, and compared to the expressivity of ﬁnite-state transductions, inversion transductions and syntax-directed transductions. 1 Introduction We investigate the formal expressivity properties of linear transductions, the class of transductions generated by linear transduction grammars (Saers, 2011, LTGs), linear inversion transduction grammars (Saers et al., 2010, LITGs) and preterminalized LITGs (Saers and Wu, 2011, PLITGs). While empirical results such as those in previous work are of course an ultimate test of modeling adequacy for machine translation applications, it is 431 Although not always given the attention deserved, the expressivity of different grammar-based methods varies quite a lot. Unlike monolingual grammars, where all context-free grammars, for example, fall into one class of languages, the bilingual case is not as well-behaved. Syntax-directed transduction grammars (Lewis and Stearns, 1968; Aho and Ullman, 1972) form distinct transd"
2011.mtsummit-papers.49,2010.eamt-1.5,0,0.0218833,"is that a PLITG is unable to generate the inside-out alignments, which is ex3 This is only valid for permutation vectors of length four, since we know that the underspeciﬁed part, being of length three, constitutes a valid permutation vector. pected since linear transductions are a proper subset of inversion transductions. The other two that cannot be generated are [1, 0, 3, 2] (which we call serial inversion) and [2, 3, 0, 1] (which we call constituent swapping). Whereas there are some evidence that the inside-out alignments are irrelevant to natural language translation (Huang et al., 2009; Søgaard, 2010), no such results exist for serial inversion and constituent swapping. On the contrary, we intuitively expect these phenomena to be frequent between natural languages. We consider this to be a serious problem with linear transductions, but empirical studies will have give the ﬁnal say on how much it hurts performance. 5 Conclusions In this paper we have presented an analysis of the weak reordering capacity of linear transductions, and compared it to that of ﬁnite-state transduction grammars, inversion transduction grammars and syntax-directed transduction grammars. We have showed that it is po"
2011.mtsummit-papers.49,C96-2141,0,0.262768,"ence and Engineering One Microsoft Way, Redmond Hong Kong University of Science and Technology Washington, USA {masaers|dekai}@cs.ust.hk chrisq@microsoft.com Abstract equally important to understand the formal theoretical properties of any such new representation. In recent years, there has been a shift away from surface-based translation method such as phrasebased statistical machine translation (phrase-based SMT) (Marcu and Wong, 2002; Koehn et al., 2003) in favor of grammar-based SMT. Although most of the grammar based methods still rely on surfacebased word alignments (Brown et al., 1993; Vogel et al., 1996) and language speciﬁc parsers, the grammar-based models themselves restrict reordering to a much higher degree than the surface-based methods, which typically allow any permutation of any segmentation of the input, and relies on heuristic search methods such as beam search, to restrict the exponential time to a tractable polynomial. We investigate the formal expressivity properties of linear transductions, the class of transductions generated by linear transduction grammars, linear inversion transduction grammars and preterminalized linear inversion transduction grammars. While empirical resul"
2011.mtsummit-papers.49,J97-3002,1,0.895993,"ork are of course an ultimate test of modeling adequacy for machine translation applications, it is 431 Although not always given the attention deserved, the expressivity of different grammar-based methods varies quite a lot. Unlike monolingual grammars, where all context-free grammars, for example, fall into one class of languages, the bilingual case is not as well-behaved. Syntax-directed transduction grammars (Lewis and Stearns, 1968; Aho and Ullman, 1972) form distinct transduction classes for all ranks above 3, while ranks 2 and 3 form a class of their own termed inversion transductions (Wu, 1997), and rank 1 forms the class termed linear transductions (Saers, 2011). In this paper we will take a closer look at the expressive powers of transduction grammars in general, noting that the concept of generative capacity fails to capture all the relevant details. Instead, we will propose a division of the expressivity into a strong and weak transductive capacity and alignCapacity Weak Transductive Strong Weak Alignment Strong Required equality Sentence pairs Biparse trees Token alignments Compositional alignments Table 1: Capacities of transduction grammars. ment capacity. The argument for th"
2020.nli-1.3,Q18-1024,0,0.0223851,"s part of modern life. Such interfaces are used to converse with personal assistants (e.g., Apple Siri, Amazon Alexa, Google Assistant, Microsoft Cortana), to search for and gather information (Google, Bing), and to interact with others on social media. One developing use case is to aid the user during composition by suggesting words, phrases, sentences, and even paragraphs that complete the user’s thoughts (Radford et al., 2019). Personalization of these interfaces is a natural step forward in a world where the vocabulary, grammar, and language can differ hugely user to user (Ishikawa, 2015; Rabinovich et al., 2018). Numerous works have described personalization in NLIs in audio rendering devices (Morse, 2008), digital assistants (Chen et al., 2014), telephone interfaces (Partovi et al., 2005), etc. We explore an approach for personalization of language models 3. We experimentally analyze trade-offs and evaluate our personalization mechanisms on public data, enabling replication by the research community. 2 Related Work Language modeling is a critical component for many NLIs, and personalization is a natural direction to improve these interfaces. Several published works have explored personalization of l"
2020.nli-1.3,P18-2111,0,0.0274268,"in NLIs in audio rendering devices (Morse, 2008), digital assistants (Chen et al., 2014), telephone interfaces (Partovi et al., 2005), etc. We explore an approach for personalization of language models 3. We experimentally analyze trade-offs and evaluate our personalization mechanisms on public data, enabling replication by the research community. 2 Related Work Language modeling is a critical component for many NLIs, and personalization is a natural direction to improve these interfaces. Several published works have explored personalization of language models using historical search queries (Jaech and Ostendorf, 2018), features garnered from social graphs (Wen et al., 2012; Tseng 1 To the best knowledge of the authors, these edge cases are not clearly defined in the literature when combining two LMs trained on two different datasets. * Indicates equal contributions 20 Proceedings of the First Workshop on Natural Language Interfaces, pages 20–26 c July 10, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 3.2 et al., 2015; Lee et al., 2016), and transfer learning techniques (Yoon et al., 2017). Other work has explored using profile information (location, name, etc.) as add"
2021.naacl-industry.1,N18-1162,0,0.0145486,"on of text prediction on production-level online communication tools, to help users compose emails (Chen et al., 2019; Microsoft Text Predictions, 2020), and in addition chat messages. In particular, we focus on examining useful contextual signals to give more accurate predicted text, using time, subject, and prior messages. Various contextualization techniques (e.g., hierarchical RNNs) have been applied to add useful additional signals such as preceding web interaction, linking pages, similar search queries or visitor interests of a page (White et al., 2009); previous sequence of utterances (Park et al., 2018; Zhang et al., 2018; Yoo et al., 2020) or related text snippets (Ke et al., 2018). 3 Time Composition time is a contextual signal which can provide added value for text prediction, enabling suggestions with relevant date-time words, like &quot;weekend&quot;, &quot;tonight&quot;. We encode local date and time, as shown in Figure 1a, and use &lt;BOT&gt; and &lt;EOT&gt; to separate from other signals. Subject Message subjects often contain the purpose or summarized information of a message. In the email scenario, we use subject as context. In the chat scenario, subject is not available, so we use the chat window name as a prox"
2021.naacl-industry.1,2020.emnlp-main.274,0,0.0123723,"el online communication tools, to help users compose emails (Chen et al., 2019; Microsoft Text Predictions, 2020), and in addition chat messages. In particular, we focus on examining useful contextual signals to give more accurate predicted text, using time, subject, and prior messages. Various contextualization techniques (e.g., hierarchical RNNs) have been applied to add useful additional signals such as preceding web interaction, linking pages, similar search queries or visitor interests of a page (White et al., 2009); previous sequence of utterances (Park et al., 2018; Zhang et al., 2018; Yoo et al., 2020) or related text snippets (Ke et al., 2018). 3 Time Composition time is a contextual signal which can provide added value for text prediction, enabling suggestions with relevant date-time words, like &quot;weekend&quot;, &quot;tonight&quot;. We encode local date and time, as shown in Figure 1a, and use &lt;BOT&gt; and &lt;EOT&gt; to separate from other signals. Subject Message subjects often contain the purpose or summarized information of a message. In the email scenario, we use subject as context. In the chat scenario, subject is not available, so we use the chat window name as a proxy for subject (can be auto-generated or"
2021.naacl-industry.1,C18-1317,0,0.0173262,"on on production-level online communication tools, to help users compose emails (Chen et al., 2019; Microsoft Text Predictions, 2020), and in addition chat messages. In particular, we focus on examining useful contextual signals to give more accurate predicted text, using time, subject, and prior messages. Various contextualization techniques (e.g., hierarchical RNNs) have been applied to add useful additional signals such as preceding web interaction, linking pages, similar search queries or visitor interests of a page (White et al., 2009); previous sequence of utterances (Park et al., 2018; Zhang et al., 2018; Yoo et al., 2020) or related text snippets (Ke et al., 2018). 3 Time Composition time is a contextual signal which can provide added value for text prediction, enabling suggestions with relevant date-time words, like &quot;weekend&quot;, &quot;tonight&quot;. We encode local date and time, as shown in Figure 1a, and use &lt;BOT&gt; and &lt;EOT&gt; to separate from other signals. Subject Message subjects often contain the purpose or summarized information of a message. In the email scenario, we use subject as context. In the chat scenario, subject is not available, so we use the chat window name as a proxy for subject (can b"
2021.naacl-main.414,P17-1171,0,0.0244406,"as it involves making nuet al. (2019) control higher level attributes of text, anced changes to text according to natural lansuch as style, tone, or topic. Our task instead guage commands. We also believe this task has 5266 uses natural language commands, which can flexibly express different types of constraints, ranging from low-level lexical ones, to high-level topical ones. In this sense, we can also draw the parallel to dialog response generation (Ghazvininejad et al., 2018; Dinan et al., 2018), task-oriented dialog (Gao et al., 2018), or open domain question answering (Min et al., 2019; Chen et al., 2017), that also involve user responses or queries, although these tasks are not concerned with text generation in the context of document creation. senting them. Related to Wikipedia data, Pryzant et al. (2020) also used Wikipedia revision histories to learn to debias text, whereas we considered general edits. Iso et al. (2020) propose a factbased text editing task, but they do not consider control or other types of edits. Another related task to text editing is text paraphrasing (Gupta et al., 2018), however paraphrasing usually conserves the meaning of a sentence. While the edits we consider inc"
2021.naacl-main.414,P18-1082,0,0.163969,"the office. add years in office Barack Obama was the 44th President of the United States from 2009 to 2017 and the first African-American to hold the office. Figure 1: An illustration of our interactive text generation setting. This is an example generated by our model. The blue panels represent the text being edited, taken from the document shown on the right. The orange panels represent user edit commands. The model grounds edits in query results from a commercial search engine. A long-standing goal of natural language processing research has been to generate long-form text (Lebowitz, 1985; Fan et al., 2018; Rashkin et al., 2020). Recent large generative language models such as GPT-2 (Radford et al., 2019), and GPT3 (Brown et al., 2020), demonstrate an impressive ability to generate fluent text, but their outputs are difficult to control beyond a prompt, and they manifest a tendency to hallucinate facts (Wiseman et al., 2017). Much recent work has thus focused on making such models more controllable (Keskar et al., 2019; Hu et al., 2017; Zhang et al., 2020; Dathathri et al., 2019), and factually grounded (Guu et al., 2020; Liu et al., 2018b). Most such work only considers a one-shot generation s"
2021.naacl-main.414,D18-1028,0,0.0813115,"of our full model are broken down by edit intention labels in Table 6. The columns report the same metrics as Ablations The middle rows of Table 5 show the in our main table of results, with the exception of results for three ablations of our model. The first S-BLEU, which reports the BLEU score between ablation removes everything but the source senthe source sentence and target, and the last coltence s. This is similar to the paraphrase setumn, which reports the number of test edits that ting (Gupta et al., 2018), and the editing setting were classified into each category. With the caveat in Faruqui et al. (2018) and Yin et al. (2018). that intention labels come from an automatic clasWe can see that including the context, grounding, sifier and not human annotation, we can observe and command as additional inputs yields signifithat our model has varying performance across cant improvements over only using the source sendifferent types of edits. The model performs very tence. We can also see from the second ablation well on fluency edits, but worse on content edits. that the commands are a crucial element in the This comes at no surprise given that fluency edmodel’s performance. This is not surprising s"
2021.naacl-main.414,P18-5002,1,0.889002,"Missing"
2021.naacl-main.414,Q18-1031,0,0.0603054,"h pretrained language model weights, yields encouraging results on both automatic and human evaluations. Additionally, our ablation studies showed the crucial role played by the user command and grounding. Breaking down our results by types of edits, we saw that our model not only performs well on easier fluency edits, but also on much harder content edits. Finally, we discussed future research directions for interactive document generation, as well as possible extensions to other domains such as images or code. Acknowledgments Text Editing Several previous works have focused on text editing. Guu et al. (2018) generate The authors would like to thank Thomas Hofsentences by editing prototypes taken from their mann, as well as Sudha Rao, Matt Richardtraining corpus, although they use editing only as a son, Zhang Li, Kosh Narayanan, and Chandra means for language modeling. Wu et al. (2019) exChikkareddy for their helpful suggestions. pand upon Guu et al. (2018)’s setting, but for dialog. More related to our own setting, Faruqui et al. (2018) propose WikiAtomicEdits, a dataset of edReferences its crawled from Wikipedia. However, they consider a much narrower definition of edits than our Giusepppe Attar"
2021.naacl-main.414,P17-1141,0,0.0461026,"Missing"
2021.naacl-main.414,W19-2405,0,0.0163809,"n generating long-form narratives (Jain et al., 2017). While earlier work in Story Generation focused more on plan-based architectures (Lebowitz, 1985), more recent work moved towards end-to-end approaches (Fan et al., 2018) allowing generation to be unconstrained and creative. As narratives are often aimed at particular goals expressed in terms of outlines and plans, much of the literature in Story Generation is framed as a form of controllable generation, using storylines (Peng et al., 2018), events (Martin et al., 2017; Harrison et al., 2017), plot words or word skeletons (Xu et al., 2018; Ippolito et al., 2019), plans (Yao et al., 2019), story ending (Tambwekar et al., 2019), and outlines (Rashkin et al., 2020) as various forms of constraints. Our work takes a significantly different approach, as we treat document or story generation as an iterative process that allows a human to generate a full document from scratch, but also allows constraints to be more dynamic (e.g., add nationality in Table 9 only if the system missed that the first time). 8 Conclusion In this work we argued that text generation should be interactive, and, as a means towards that end, we proposed a general text editing task, wh"
2021.naacl-main.414,2020.acl-main.17,0,0.0328833,"om low-level lexical ones, to high-level topical ones. In this sense, we can also draw the parallel to dialog response generation (Ghazvininejad et al., 2018; Dinan et al., 2018), task-oriented dialog (Gao et al., 2018), or open domain question answering (Min et al., 2019; Chen et al., 2017), that also involve user responses or queries, although these tasks are not concerned with text generation in the context of document creation. senting them. Related to Wikipedia data, Pryzant et al. (2020) also used Wikipedia revision histories to learn to debias text, whereas we considered general edits. Iso et al. (2020) propose a factbased text editing task, but they do not consider control or other types of edits. Another related task to text editing is text paraphrasing (Gupta et al., 2018), however paraphrasing usually conserves the meaning of a sentence. While the edits we consider include meaning-preserving edits, we are mostly interested in edits that affect meaning. Story Generation The task of Document Generation considered in our work bears similarity with work on generating long-form narratives (Jain et al., 2017). While earlier work in Story Generation focused more on plan-based architectures (Leb"
2021.naacl-main.414,J06-4003,0,0.0584681,"Missing"
2021.naacl-main.414,N19-1238,0,0.0634756,"Missing"
C04-1051,P01-1008,0,0.026132,"Missing"
C04-1051,N03-1003,0,0.0936505,"t the idea because of the noisy, comparable nature of their dataset. captured by edit distance techniques, which conflate semantic similarity with formal similarity. We conclude that paraphrase research would benefit by identifying richer data sources and developing appropriate learning techniques. 2 Data/Methodology Our two paraphrase datasets are distilled from a corpus of news articles gathered from thousands of news sources over an extended period. While the idea of exploiting multiple news reports for paraphrase acquisition is not new, previous efforts (for example, Shinyama et al. 2002; Barzilay and Lee 2003) have been restricted to at most two news sources. Our work represents what we believe to be the first attempt to exploit the explosion of news coverage on the Web, where a single event can generate scores or hundreds of different articles within a brief period of time. Some of these articles represent minor rewrites of an original AP or Reuters story, while others represent truly distinct descriptions of the same basic facts. The massive redundancy of information conveyed with widely varying surface strings is a resource begging to be exploited. Figure 1 shows the flow of our data collection"
C04-1051,W03-0301,0,0.0239292,"Missing"
C04-1051,P00-1056,0,0.0174201,"ndencies, and a variety of distributed paraphrases, with alignments spanning widely separated elements. 3.2 tagged corpus of alignments serving as the gold standard. Paraphrase data is of course monolingual, but otherwise the task is very similar to the MT alignment problem, posing the same issues with one-to-many, many-to-many, and one/many-tonull word mappings. Our a priori assumption was that the lower the AER for a corpus, the more likely it would be to yield learnable information about paraphrase alternations. We closely followed the evaluation standards established in Melamed (2001) and Och & Ney (2000, 2003). Following Och & Ney’s methodology, two annotators each created an initial annotation for each dataset, subcategorizing alignments as either SURE (necessary) or POSSIBLE (allowed, but not required). Differences were then highlighted and the annotators were asked to review these cases. Finally we combined the two annotations into a single gold standard in the following manner: if both annotators agreed that an alignment should be SURE, then the alignment was marked as sure in the gold-standard; otherwise the alignment was marked as POSSIBLE. To compute Precision, Recall, and Alignment E"
C04-1051,C96-2141,0,0.113546,"Missing"
C04-1051,J93-2003,0,\N,Missing
C04-1051,J03-1002,0,\N,Missing
C08-1074,W06-3114,0,0.0262178,"h feature weight according to a uniform distribution over a fixed interval, say −1.0 to +1.0. The best point reached, starting from either the previous optimum or one of the random restart points, is selected as the optimum for the current set of hypotheses. This widely-used procedure is described by Koehn et al. (2007, p. 50). 3.1 Preliminary evaluation In our first experiments, we compared a variant of Och’s MERT procedure with and without random restarts as described above. For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference. We built a standard baseline phrasal SMT system, as described by Koehn et al. (2003), for translating from English to French (E-to-F), using the word alignments and French target language model provided by the workshop organizers. We trained a model with the standard eight features: E-to-F and F-to-E phrase translation log 1 Since additional hypotheses have been added, initiating an optimization search from this point on the new set of hypotheses will often lead to a higher local optimum. 5"
C08-1074,N03-1017,0,0.0051363,"ints, is selected as the optimum for the current set of hypotheses. This widely-used procedure is described by Koehn et al. (2007, p. 50). 3.1 Preliminary evaluation In our first experiments, we compared a variant of Och’s MERT procedure with and without random restarts as described above. For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference. We built a standard baseline phrasal SMT system, as described by Koehn et al. (2003), for translating from English to French (E-to-F), using the word alignments and French target language model provided by the workshop organizers. We trained a model with the standard eight features: E-to-F and F-to-E phrase translation log 1 Since additional hypotheses have been added, initiating an optimization search from this point on the new set of hypotheses will often lead to a higher local optimum. 586 probabilities, E-to-F and F-to-E phrase translation lexical scores, French language model log probabilities, phrase pair count, French word count, and distortion score. Feature weight op"
C08-1074,W04-3250,0,0.16091,"Missing"
C08-1074,P03-1021,0,0.165598,"dure is the most commonly used method for training feature weights in statistical machine translation (SMT) models. The use of multiple randomized starting points in MERT is a well-established practice, although there seems to be no published systematic study of its benefits. We compare several ways of performing random restarts with MERT. We find that all of our random restart methods outperform MERT without random restarts, and we develop some refinements of random restarts that are superior to the most common approach with regard to resulting model quality and training time. 1 Introduction Och (2003) introduced minimum error rate training (MERT) for optimizing feature weights in statistical machine translation (SMT) models, and demonstrated that it produced higher translation quality scores than maximizing the conditional likelihood of a maximum entropy model using the same features. Och’s method performs a series of one-dimensional optimizations of the feature weight vector, using an innovative line search that takes advantage of special properties of the mapping from sets of feature weights to the resulting translation quality measurement. Och’s line search is guaranteed to find a globa"
C10-1041,D07-1090,0,0.0200831,"ing provides a flexible modeling framework for incorporating a wide variety of features that would be difficult to model under the noisy channel framework. Second, we explore the use of Web scale LMs for query spelling correction. While traditional LM research focuses on how to make the model “smarter” via how to better estimate the probability of unseen words (Chen and Goodman, 1999); and how to model the grammatical structure of language (e.g., Charniak, 2001), recent studies show that significant improvements can be achieved using “stupid” n-gram models trained on very large corpora (e.g., Brants et al., 2007). We adopt the latter strategy in this study. We present a distributed infrastructure to efficiently train and apply Web scale LMs. In addition, we observe that search queries are composed in a language style different from that of regular text. We thus train multiple LMs using different texts associated with Web corpora and search queries. Third, we propose a phrase-based error model that captures the probability of transforming one 358 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358–366, Beijing, August 2010 multi-term phrase into anothe"
C10-1041,P00-1037,0,0.799464,"are composed in a language style different from that of regular text. We thus train multiple LMs using different texts associated with Web corpora and search queries. Third, we propose a phrase-based error model that captures the probability of transforming one 358 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358–366, Beijing, August 2010 multi-term phrase into another multi-term phrase. Compared to traditional error models that account for transformation probabilities between single characters or substrings (e.g., Kernighan et al., 1990; Brill and Moore, 2000), the phrase-based error model is more effective in that it captures inter-term dependencies crucial for correcting real-word errors, prevalent in search queries. We also present a novel method of extracting large amounts of query-correction pairs from search logs. These pairs, implicitly judged by millions of users, are used for training the error models. Experiments show that each of the extensions leads to significant improvements over its baseline methods that were state-of-the-art until this work, and that the combined method yields a system which outperforms the noisy channel speller by"
C10-1041,P01-1017,0,0.0257571,"aps the feature vector to a real-valued score, indicating the likelihood that this candidate is a desirable correction. We will demonstrate that ranking provides a flexible modeling framework for incorporating a wide variety of features that would be difficult to model under the noisy channel framework. Second, we explore the use of Web scale LMs for query spelling correction. While traditional LM research focuses on how to make the model “smarter” via how to better estimate the probability of unseen words (Chen and Goodman, 1999); and how to model the grammatical structure of language (e.g., Charniak, 2001), recent studies show that significant improvements can be achieved using “stupid” n-gram models trained on very large corpora (e.g., Brants et al., 2007). We adopt the latter strategy in this study. We present a distributed infrastructure to efficiently train and apply Web scale LMs. In addition, we observe that search queries are composed in a language style different from that of regular text. We thus train multiple LMs using different texts associated with Web corpora and search queries. Third, we propose a phrase-based error model that captures the probability of transforming one 358 Proc"
C10-1041,D07-1019,0,0.517248,"ovements are likely given a larger data set. 7 Conclusions and Future Work This paper explores the use of massive Web corpora and search logs for improving a rankerbased search query speller. We show significant improvements over a noisy channel speller using fine-grained features, Web scale LMs, and a phrase-based error model that captures internword dependencies. There are several techniques we are exploring to make further improvements. First, since a query speller is developed for improving the Web search results, it is natural to use features from search results in ranking, as studied in Chen et al. (2007). The challenge is efficiency. Second, in addition to query reformulation sessions, we are exploring other search logs from which we might extract more pairs for error model training. One promising data source is clickthrough data (e.g., Agichtein et al, 2006; Gao et al., 2009). For instance, we might try to learn a transformation from the title or anchor text of a document to the query that led to a click on that document. Finally, the phrase-based error model is inspired by phrase-based SMT systems. We are introducing more SMT techniques such as alignment and translation rule exaction. In a"
C10-1041,D07-1021,1,0.830034,"odels based on different edit distance functions (e.g., Kucich, 1992; Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002). Brill and Moore’s substring-based error model, considered to be state-of-the-art among these models, acts as the baseline against which we compare our models. On the other hand, real-word spelling correction tries to detect incorrect usages of a valid word based on its context, such as &quot;peace&quot; and &quot;piece&quot; in the context &quot;a _ of cake&quot;. N-gram LMs and naïve Bayes classifiers are commonly used models (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). While almost all of the spellers mentioned above are based on a pre-defined dictionary (either a lexicon against which the edit distance is computed, or a set of real-word confusion pairs), recent research on query spelling correction focuses on exploiting noisy Web corpora and query logs to infer knowledge about spellings and word usag in queries (Cucerzan and Brill 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Whitelaw et al., 2009). Like those spellers designed for regular text, most of these query spelling systems are also based on the noisy channel framework. 359 3 A Ranker-Based Spel"
C10-1041,W04-3238,0,0.95372,"improvements over the stateof-the-art baseline methods. 1 Daniel Micol Microsoft Corporation Xiaolong Li Introduction Search queries present a particular challenge for traditional spelling correction methods. New search queries emerge constantly. As a result, many queries contain valid search terms, such as proper nouns and names, which are not well established in the language. Therefore, recent research has focused on the use of Web corpora and search logs, rather than human-compiled lexicons, to infer knowledge about spellings and word usages in search queries (e.g., Whitelaw et al., 2009; Cucerzan and Brill, 2004). The spelling correction problem is typically formulated under the framework of the noisy channel model. Given an input query , we want to find the best spelling correction among all candidates: (1) Applying Bayes&apos; Rule, we have (2) where the error model models the transformation probability from C to Q, and the language model (LM) models the likelihood that C is a correctly spelled query. This paper extends a noisy channel speller designed for regular text to search queries in three ways: using a ranker (Section 3), using Web scale LMs (Section 4), and using phrase-based error models (Sectio"
C10-1041,O01-2002,1,0.69124,"the n-gram LM collection used in this study, and then present a distributed n-gram LM platform based on which these LMs are built and served for the speller. 4.1 Web Scale Language Models Table 1 summarizes the data sets and Web scale n-gram LMs used in this study. The collection is built from high quality English Web documents containing trillions of tokens, served by a popular commercial search engine. The collection con360 ( ) { where is the count of the n-gram in the training corpus and is a normalization factor. is a discount function for smoothing. We use modified absolute discounting (Gao et al., 2001), whose parameters can be efficiently estimated and performance converges to that of more elaborate state-of-the-art techniques like Kneser-Ney smoothing in large data (Nguyen et al. 2007). 4.2 Distributed N-gram LM Platform The platform is developed on a distributed computing system designed for storing and analyzing massive data sets, running on large clusters consisting of hundreds of commodity servers connected via high-bandwidth network. We use the SCOPE (Structured Computations Optimized for Parallel Execution) programming model (Chaiken et al., 2008) to train the Web scale n-gram LMs sh"
C10-1041,C90-2036,0,0.664032,"erve that search queries are composed in a language style different from that of regular text. We thus train multiple LMs using different texts associated with Web corpora and search queries. Third, we propose a phrase-based error model that captures the probability of transforming one 358 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358–366, Beijing, August 2010 multi-term phrase into another multi-term phrase. Compared to traditional error models that account for transformation probabilities between single characters or substrings (e.g., Kernighan et al., 1990; Brill and Moore, 2000), the phrase-based error model is more effective in that it captures inter-term dependencies crucial for correcting real-word errors, prevalent in search queries. We also present a novel method of extracting large amounts of query-correction pairs from search logs. These pairs, implicitly judged by millions of users, are used for training the error models. Experiments show that each of the extensions leads to significant improvements over its baseline methods that were state-of-the-art until this work, and that the combined method yields a system which outperforms the n"
C10-1041,N03-1017,0,0.00776907,"Missing"
C10-1041,P06-1129,0,0.399916,"such as &quot;peace&quot; and &quot;piece&quot; in the context &quot;a _ of cake&quot;. N-gram LMs and naïve Bayes classifiers are commonly used models (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). While almost all of the spellers mentioned above are based on a pre-defined dictionary (either a lexicon against which the edit distance is computed, or a set of real-word confusion pairs), recent research on query spelling correction focuses on exploiting noisy Web corpora and query logs to infer knowledge about spellings and word usag in queries (Cucerzan and Brill 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Whitelaw et al., 2009). Like those spellers designed for regular text, most of these query spelling systems are also based on the noisy channel framework. 359 3 A Ranker-Based Speller The noisy channel model of Equation (2) does not have the flexibility to incorporate a wide variety of features useful for spelling correction, e.g., whether a candidate appears as a Wikipedia document title. We thus generalize the speller to a ranker-based system. Let f be a feature vector of a query and candidate correction pair (Q, C). The ranker maps f to a real value y that indicates how likely C is a desi"
C10-1041,J04-4002,0,0.0349005,"t&form=QBRE&qs=n http://www.bing.com/search? q=harry+potter+theme+park&FORM=SSRE Figure 3. A sample of query reformulation sessions from 3 popular search engines. These sessions show that a user first issues the query &quot;harrypotter sheme part&quot;, and then clicks on the resulting spell suggestion &quot;harry potter theme park&quot;. To find the maximum probability assignment efficiently, we use a dynamic programming approach, similar to the monotone decoding algorithm described in Och (2002). 5.2 Training the Error Model Given a set of (Q, C) pairs as training data, we follow a method commonly used in SMT (Och and Ney, 2004) to extract bi- phrases and estimate their replacement probabilities. A detailed description is discussed in Sun et al. (2010). We now describe how (Q, C) pairs are generated automatically from massive query reformulation sessions of a commercial Web browser. A query reformulation session contains a list of URLs that record user behaviors that relate to the query reformulation functions, provided by a Web search engine. For example, most commercial search engines offer the &quot;did you mean&quot; function, suggesting a possible alternate interpretation or spelling of a user-issued query. Figure 3 shows"
C10-1041,P10-1028,1,0.481669,"e n-gram platform provides a DLL for n-gram batch lookup. In the server, an n-gram LM is stored in the form of multiple lists of key-value pairs, where the key is the hash of an n-gram string and the value is either the n-gram probability or backoff parameter. 5 Phrase-Based Error Models The goal of an error model is to transform a correctly spelled query C into a misspelled query Q. Rather than replacing single words in isolation, the phrase-based error model replaces sequences of words with sequences of words, thus incorporating contextual information. The training procedure closely follows Sun et al. (2010). For instance, we might learn that “theme part” can be replaced by “theme park” with relatively high probability, even though “part” is not a misspelled word. We use this generative story: first the correctly spelled query C is broken into K non-empty word sequences c1, …, ck, then each is replaced with a new non-empty word sequence q1, …, qk, finally these phrases are permuted and concatenated to form the misspelled Q. Here, c and q denote consecutive sequences of words. To formalize this generative process, let S denote the segmentation of C into K phrases c1…cK, and let T denote the K repl"
C10-1041,P02-1019,0,0.610347,"Missing"
C10-1041,D09-1093,0,0.550154,"ns leads to significant improvements over the stateof-the-art baseline methods. 1 Daniel Micol Microsoft Corporation Xiaolong Li Introduction Search queries present a particular challenge for traditional spelling correction methods. New search queries emerge constantly. As a result, many queries contain valid search terms, such as proper nouns and names, which are not well established in the language. Therefore, recent research has focused on the use of Web corpora and search logs, rather than human-compiled lexicons, to infer knowledge about spellings and word usages in search queries (e.g., Whitelaw et al., 2009; Cucerzan and Brill, 2004). The spelling correction problem is typically formulated under the framework of the noisy channel model. Given an input query , we want to find the best spelling correction among all candidates: (1) Applying Bayes&apos; Rule, we have (2) where the error model models the transformation probability from C to Q, and the language model (LM) models the likelihood that C is a correctly spelled query. This paper extends a noisy channel speller designed for regular text to search queries in three ways: using a ranker (Section 3), using Web scale LMs (Section 4), and using phrase"
C10-1041,W06-1626,0,0.030213,"Missing"
C10-1041,H05-1120,0,\N,Missing
D08-1077,P05-1033,0,0.0714864,"lt. Since there are no constraints on where such a rule may apply and the rule does not consume any input words, the decoder must attempt these rules at every point in the search. The reverse operation “de” ՜ “[NULL]” (3) is more feasible to implement, though again, there is great ambiguity – a source word may be deleted at any point during the search, with identical target results. Few systems allow this operation in practice. Estimating the likelihood of this operation and correctly identifying the contexts in which it should occur remain challenging problems. Hierarchical systems, such as (Chiang, 2005) in principle have the capacity to learn insertions and deletions grounded by minimal lexical cues. However, the extracted rules use a single nonterminal. Hence, to avoid explosive ambiguity, they are constrained to contain at least one aligned pair of words. This restriction successfully limits computational complexity at a cost of generalization power. Syntax-based approaches provide fertile context for grounding insertions and deletions. Often we may draw a strong correspondence between function words in one language and syntactic constructions in another. For instance, the syntactic approa"
D08-1077,2005.mtsummit-papers.37,0,0.0389446,"rdering are modeled separately using lexicalized treelets and unlexicalized order templates. We discuss this approach in more detail in Section 4. In Section 5, we describe how we extend this approach to allow for structural insertion and deletion, without the need for content word anchors. 3 Related Work There is surprisingly little prior work in this area. We previously (Menezes & Quirk, 2005) explored the use of deletion operations such as (3) above, but these were not grounded in any syntactic context, and the estimation was somewhat heuristic1. The tuple translation model of Crego et al. (2005), a joint model over source and target translations, also provides a means of deleting words. In training, sentence pairs such as “nombre de archivo” / “file name” are first word aligned, then minimal bilingual tuples are identified, such as “nombre / name”, “de / NULL” and “archivo / file”. The tuples may involve deletion of words by allowing an empty target side, but do not allow insertion tuples with an empty source side. These inserted words are bound to an adjacent neighbor. An n-gram model is trained over the tuple sequences. As a result, deletion probabilities have the desirable propert"
D08-1077,W07-0711,0,0.0715053,"d count, phrase count. • Trigram language model log probability. • Length based distortion model. • Lexicalized reordering model. 7.3 Training We parsed the source (English) side of the corpus using NLPWIN, a broad-coverage rule-based parser able to produce syntactic analyses at varying levels of depth (Heidorn, 2000). For the purposes of these experiments, we used a dependency tree output with part-of-speech tags and unstemmed, case-normalized surface words. For word alignment we used a training regimen of five iterations of Model 1, followed by five iterations of a word-dependent HMM model (He, 2007) in both directions. The forward and backward alignments were combined using a dependency tree-based heuristic combination. The word alignments and English dependency tree were used to project a target tree. From the aligned tree pairs we extracted treelet and order template tables. For the Europarl systems, we use a phrase/treelet size of 7 and train model weights using 2000 sentences of Europarl data. For the “general-domain” systems, we use a phrase/treelet size of 4, and train model weights using 2000 sentences of web data. For any given corpus, all systems used the same treelet or phrase"
D08-1077,N03-1017,0,0.0379839,"Missing"
D08-1077,P02-1040,0,0.0741764,"nslation of the sentence is now: Septiembre es el Mes Nacional de Educación de colesterol Although the choice of prepositions is not the same as the reference, the fluency is much improved and 740 the translation is quite understandable. Figure 6.1, lists the structural insertion templates that are used to produce this translation, and shows how they are unified with treelet translation pairs to produce sentence-specific rewrite rules, which are in turn composed during decoding to produce this translation. 7 Experiments We evaluated the translation quality of the system using the BLEU metric (Papineni et al., 2002). We compared three systems: (a) a standard phrasal system using a decoder based on Pharaoh, (Koehn et al., 2003), (b) A baseline treelet system using unlexicalized order templates and (c) The present work, which adds structural insertion and deletion templates. 7.1 Data We report results for two language pairs, EnglishSpanish and English- Japanese. For EnglishSpanish we use two training sets: (a) the Europarl corpus provided by the NAACL 2006 Statistical Machine Translation workshop (b) a “generaldomain” data set that includes a broad spectrum of data such as governmental data, general web da"
D08-1077,W07-0701,1,\N,Missing
D08-1077,W06-1606,0,\N,Missing
D08-1077,D07-1078,0,\N,Missing
D08-1077,2005.iwslt-1.12,1,\N,Missing
D08-1077,P03-1021,0,\N,Missing
D09-1078,D07-1090,0,0.153673,"Missing"
D09-1078,P96-1041,0,0.150307,"Missing"
D09-1078,W06-3114,0,0.0121708,"simply by dividing Seymore and Rosenfeld’s discounted N-gram count K by the total number of highest-order Ngrams in the training corpus. This is equivalent to smoothing only the highest-order conditional Ngram model in estimating p(w1 ...wn ), estimating all the lower-order probabilities in the chain by the corresponding MLE model. We refer to this joint probability estimate as “partially-smoothed”, and the one suggested by Stolcke as “fully-smoothed”. 5 5.1 Data and Base Smoothing Methods For training, parameter optimzation, and test data we used English text from the WMT-06 Europarl corpus (Koehn and Monz, 2006). We trained on the designated 1,003,349 sentences (27,493,499 words) of English language model training data, and used 2000 sentences each for testing and parameter optimization, from the English half of the English-French dev and devtest data sets. We conducted our experiments on seven language model smoothing methods. Five of these are well-known: (1) interpolated absolute discounting with one discount per N-gram length, estimated according to the formula derived by Ney et al. (1994); (2) Katz backoff with Good-Turing discounts for N-grams occurring 5 times or less; (3) backoff absolute dis"
D09-1078,P09-2088,1,0.839443,"derived by Ney et al. (1994); (2) Katz backoff with Good-Turing discounts for N-grams occurring 5 times or less; (3) backoff absolute discounting with Ney et al. formula discounts; (4) backoff absolute discounting with one discount used for all N-gram lengths, optimized on held-out data; (5) modified interpolated Kneser-Ney smoothing with three discounts per N-gram length, estimated according to the formulas suggested by Chen and Goodman (1998). We also experimented with two variants of a new smoothing method that we have recently developed. Full details of the new method are given elsewhere (Moore and Quirk, 2009), but since it is not well-known, we summarize the method here. Smoothed N-gram probabilities are defined by the formulas shown in Figure 1, for all n such that N ≥ n ≥ 2,3 where N is the greatest N-gram length used in the model. The novelty of this model is that, while it is an interpolated model, the interpolation weights β for the lower-order model Evaluation We carried out three sets of evaluations to test the new techniques described above. First we compared the perplexity of full models and models reduced by significance-based N-gram selection for seven language model smoothing methods."
D09-1078,H92-1021,0,\N,Missing
D11-1004,W08-0304,0,0.659722,"tly; prior work offers no algorithm with a good approximation guarantee. While much of the earlier work in MERT (Chou et al., 1993; Juang et al., 1997) relies on standard convex optimization techniques applied to non-convex problems, the Och algorithm (Och, 2003) represents a significant advance for MERT since it applies a series of special line minimizations that happen to be exhaustive and efficient. Since this algorithm remains inexact in the multidimensional case, much of the recent work on MERT has focused on extending Och’s algorithm to find better search directions and starting points (Cer et al., 2008; Moore and Quirk, 2008), and on experimenting with other derivative-free methods such as the Nelder-Mead simplex algorithm (Nelder and Mead, 1965; Zens et al., 2007; Zhao and Chen, 2009). In this paper, we present LP-MERT, an exact search algorithm for N -best optimization that exploits general assumptions commonly made with MERT, e.g., that the error metric is decomposable by sentence.1 While there is no known optimal algo1 Note that MERT makes two types of approximations. First, the set of all possible outputs is represented only approximately, by N -best lists, lattices, or hypergraphs. Se"
D11-1004,D10-1059,0,0.117181,"Missing"
D11-1004,D08-1024,0,0.709704,"e thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 1 Introduction Minimum error rate training (MERT)—also known as direct loss minimization in machine learning—is a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical, since these metrics"
D11-1004,J07-2003,0,0.0822759,"Quirk et al., 2005): it first applies a dependency parser to the source language data at both training and test time. Multi-word translation mappings constrained to be connected subgraphs of the source tree are extracted from the training data; these provide most lexical translations. Partially lexicalized templates capturing reordering and function word insertion and deletion are also extracted. At runtime, these mappings and templates are used to construct transduction rules to convert the source tree into a target string. The best transduction is sought using approximate search techniques (Chiang, 2007). Each hypothesis is scored by a relatively standard set of features. The mappings contain five features: maximum-likelihood estimates of source given target and vice versa, lexical weighting estimates of source given target and vice versa, and a constant value that, when summed across a whole hypothesis, indicates the number of mappings used. For each template, we include a maximum-likelihood estimate of the target reordering given the source structure. The system may fall back to templates that mimic the source word order; the count of such templates is a feature. Likewise we include a featu"
D11-1004,P08-2010,0,0.204434,"rge tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 1 Introduction Minimum error rate training (MERT)—also known as direct loss minimization in machine learning—is a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical"
D11-1004,W05-1506,0,0.053108,"combinations that are not extreme. For instance, if we find that (h1,1 ,h2,2 ) is interior for sentences s = 1, 2, the divide-and-conquer branch for s = 1 . . . 4 never actually receives this bad combination from its left child, thus avoiding the cost of enumerating combinations that are known to be interior, e.g., (h1,1 ,h2,2 , h3,1 ,h4,1 ). The LP-MERT algorithm for the general case is shown as Algorithm 2. It basically only calls a recursive divide-and-conquer function (G ET N EXT B EST) for sentence range 1 . . . S. The latter function uses binary lazy enumeration in a manner similar to (Huang and Chiang, 2005), and relies on two global variables: I and L. The first of these, I, is used to memoize the results of calls to G ET N EXT B EST; given a range of sentences and a rank n, it stores the nth best combination for that range of sentences. The global variable L stores hypotheses combination matrices, one matrix for each range of sentences (s, t) as shown in {h31, h41} {h32, h41} h21 h11 h12 h13 {h11, h23} 126.0 {h12, h21} 126.1 h22 h24 h23 69.1 69.2 69.2 69.9 h31 h32 69.3 69.4 70.0 h33 L[1,2] 126.5 h41 h42 h23 56.8 57.1 57.9 57.3 57.6 Combinations checked: {h11, h23, h31, h41} {h12, h21, h31, h41}"
D11-1004,P07-2045,0,0.0124069,"owards a promising region), but beam pruning also helps reduce LP optimization time and thus enables us to 44 explore a wider space. Since wbest often improves during search, it is useful to run multiple iterations of LP-MERT until wbest doesn’t change. Two or three iterations suffice in our experience. In our experiments, we use a beam size of 1000. 4 Experimental Setup Our experiments in this paper focus on only the application of machine translation, though we believe that the current approach is agnostic to the particular system used to generate hypotheses. Both phrasebased systems (e.g., Koehn et al. (2007)) and syntaxbased systems (e.g., Li et al. (2009), Quirk et al. (2005)) commonly use MERT to train free parameters. Our experiments use a syntax-directed translation approach (Quirk et al., 2005): it first applies a dependency parser to the source language data at both training and test time. Multi-word translation mappings constrained to be connected subgraphs of the source tree are extracted from the training data; these provide most lexical translations. Partially lexicalized templates capturing reordering and function word insertion and deletion are also extracted. At runtime, these mappin"
D11-1004,P09-1019,0,0.383186,"Missing"
D11-1004,D08-1088,0,0.0475917,"et al., 2008; Kumar et al., 2009; Chatterjee and Cancedda, 2010). This particular refinement is orthogonal to our approach, though. We expect to extend LP-MERT 8 One interesting observation is that the performance of 1DMERT degrades as S grows from 2 to 8 (Fig. 5), which contrasts with the results shown in Tab. 2. This may have to do with the fact that N -best lists with S = 2 have much fewer local maxima than with S = 4, 8, in which case 20 restarts is generally enough. 46 to hypergraphs in future work. Exact search may be challenging due to the computational complexity of the search space (Leusch et al., 2008), but approximate search should be feasible. Other research has explored alternate methods of gradient-free optimization, such as the downhillsimplex algorithm (Nelder and Mead, 1965; Zens et al., 2007; Zhao and Chen, 2009). Although the search space is different than that of Och’s algorithm, it still relies on one-dimensional line searches to reflect, expand, or contract the simplex. Therefore, it suffers the same problems of one-dimensional MERT: feature sets with complex non-linear interactions are difficult to optimize. LP-MERT improves on these methods by searching over a larger subspace"
D11-1004,W09-0424,0,0.0133969,"lps reduce LP optimization time and thus enables us to 44 explore a wider space. Since wbest often improves during search, it is useful to run multiple iterations of LP-MERT until wbest doesn’t change. Two or three iterations suffice in our experience. In our experiments, we use a beam size of 1000. 4 Experimental Setup Our experiments in this paper focus on only the application of machine translation, though we believe that the current approach is agnostic to the particular system used to generate hypotheses. Both phrasebased systems (e.g., Koehn et al. (2007)) and syntaxbased systems (e.g., Li et al. (2009), Quirk et al. (2005)) commonly use MERT to train free parameters. Our experiments use a syntax-directed translation approach (Quirk et al., 2005): it first applies a dependency parser to the source language data at both training and test time. Multi-word translation mappings constrained to be connected subgraphs of the source tree are extracted from the training data; these provide most lexical translations. Partially lexicalized templates capturing reordering and function word insertion and deletion are also extracted. At runtime, these mappings and templates are used to construct transducti"
D11-1004,P06-1096,0,0.269228,"Missing"
D11-1004,C04-1072,0,0.616086,"error count along any line is a piecewise constant function. Furthermore, this function for a single sentence may be computed efficiently by first finding the hypotheses that form the upper envelope of the model score function, then gathering the error count for each hypothesis along the range for which it is optimal. Error counts for the whole corpus are simply the sums of these piecewise constant functions, leading to an 2 A metric such as TER is decomposable by sentence. BLEU is not, but its sufficient statistics are, and the literature offers several sentence-level approximations of BLEU (Lin and Och, 2004; Liang et al., 2006). efficient algorithm for finding the global optimum of the error count along any single direction. Such a hill-climbing algorithm in a non-convex space has no optimality guarantee: without a perfect direction finder, even a globally-exact line search may never encounter the global optimum. Coordinate ascent is often effective, though conjugate direction set finding algorithms, such as Powell’s method (Powell, 1964; Press et al., 2007), or even random directions may produce better results (Cer et al., 2008). Random restarts, based on either uniform sampling or a random wal"
D11-1004,D08-1076,0,0.241701,"Missing"
D11-1004,P05-1012,0,0.0976301,"only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 1 Introduction Minimum error rate training (MERT)—also known as direct loss minimization in machine learning—is a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequ"
D11-1004,E06-1038,0,0.0204582,"r space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 1 Introduction Minimum error rate training (MERT)—also known as direct loss minimization in machine learning—is a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate"
D11-1004,C08-1074,1,0.730402,"fers no algorithm with a good approximation guarantee. While much of the earlier work in MERT (Chou et al., 1993; Juang et al., 1997) relies on standard convex optimization techniques applied to non-convex problems, the Och algorithm (Och, 2003) represents a significant advance for MERT since it applies a series of special line minimizations that happen to be exhaustive and efficient. Since this algorithm remains inexact in the multidimensional case, much of the recent work on MERT has focused on extending Och’s algorithm to find better search directions and starting points (Cer et al., 2008; Moore and Quirk, 2008), and on experimenting with other derivative-free methods such as the Nelder-Mead simplex algorithm (Nelder and Mead, 1965; Zens et al., 2007; Zhao and Chen, 2009). In this paper, we present LP-MERT, an exact search algorithm for N -best optimization that exploits general assumptions commonly made with MERT, e.g., that the error metric is decomposable by sentence.1 While there is no known optimal algo1 Note that MERT makes two types of approximations. First, the set of all possible outputs is represented only approximately, by N -best lists, lattices, or hypergraphs. Second, error functions on"
D11-1004,P02-1038,0,0.921328,"ns such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate well with task-agnostic loss functions such as log-loss. While competitive in practice, MERT faces several challenges, the most significant of which is search. The unsmoothed error count is a highly non-convex objective function and therefore difficult to optimize directly; prior work offers no algorithm with a good approximation gua"
D11-1004,P03-1021,0,0.709167,"that let us scale to reasonably large tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 1 Introduction Minimum error rate training (MERT)—also known as direct loss minimization in machine learning—is a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al."
D11-1004,2001.mtsummit-papers.68,0,0.0219041,"statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate well with task-agnostic loss functions such as log-loss. While competitive in practice, MERT faces several challenges, the most significant of which is search. The unsmoothed error count is a highly non-convex objective function and therefore difficult to optimize directly; prior work offers no algorithm with a good approximation guarantee. While much of the earlier work in MERT (Chou et al., 1993; Juang et al., 1997) relies on"
D11-1004,P05-1034,1,0.759044,"mization time and thus enables us to 44 explore a wider space. Since wbest often improves during search, it is useful to run multiple iterations of LP-MERT until wbest doesn’t change. Two or three iterations suffice in our experience. In our experiments, we use a beam size of 1000. 4 Experimental Setup Our experiments in this paper focus on only the application of machine translation, though we believe that the current approach is agnostic to the particular system used to generate hypotheses. Both phrasebased systems (e.g., Koehn et al. (2007)) and syntaxbased systems (e.g., Li et al. (2009), Quirk et al. (2005)) commonly use MERT to train free parameters. Our experiments use a syntax-directed translation approach (Quirk et al., 2005): it first applies a dependency parser to the source language data at both training and test time. Multi-word translation mappings constrained to be connected subgraphs of the source tree are extracted from the training data; these provide most lexical translations. Partially lexicalized templates capturing reordering and function word insertion and deletion are also extracted. At runtime, these mappings and templates are used to construct transduction rules to convert t"
D11-1004,P06-2101,0,0.138351,"s scale to reasonably large tuning sets (e.g., one thousand sentences), by either searching only promising regions of the parameter space, or by using a variant of LP-MERT that relies on a beam-search approximation. Experimental results show improvements over the standard Och algorithm. 1 Introduction Minimum error rate training (MERT)—also known as direct loss minimization in machine learning—is a crucial component in many complex natural language applications such as speech recognition (Chou et al., 1993; Stolcke et al., 1997; Juang et al., 1997), statistical machine translation (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evalu"
D11-1004,2006.amta-papers.25,0,0.0285717,"on (Och, 2003; Smith and Eisner, 2006; Duh and Kirchhoff, 2008; Chiang et al., 2008), dependency parsing (McDonald et al., 2005), summarization (McDonald, 2006), and phonetic alignment (McAllester et al., 2010). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative 38 Chris Quirk Microsoft Research Redmond, WA 98052, USA chrisq@microsoft.com method (Och and Ney, 2002). In complex text generation tasks like SMT, the ability to optimize BLEU (Papineni et al., 2001), TER (Snover et al., 2006), and other evaluation metrics is critical, since these metrics measure qualities (such as fluency and adequacy) that often do not correlate well with task-agnostic loss functions such as log-loss. While competitive in practice, MERT faces several challenges, the most significant of which is search. The unsmoothed error count is a highly non-convex objective function and therefore difficult to optimize directly; prior work offers no algorithm with a good approximation guarantee. While much of the earlier work in MERT (Chou et al., 1993; Juang et al., 1997) relies on standard convex optimizatio"
D11-1004,D07-1080,0,0.29665,"e can also change the objective function in a number of ways to make it more amenable to optimization, leveraging knowledge from elsewhere in the machine learning community. Instance reweighting as in boosting may lead to better parameter inference (Duh and Kirchhoff, 2008). Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). Smith and Eisner (2006) use a smoothed objective along with deterministic annealing in hopes of finding good directions and climbing past locally optimal points. Other papers use margin methods such as MIRA (Watanabe et al., 2007; Chiang et al., 2008), updated somewhat to match the MT domain, to perform incremental training of potentially large numbers of features. However, in each of these cases the objective function used for training no longer matches the final evaluation metric. 7 Conclusions Our primary contribution is the first known exact search algorithm for direct loss minimization on N best lists in multiple dimensions. Additionally, we present approximations that consistently outperform standard one-dimensional MERT on a competitive machine translation system. While Och’s method of MERT is generally quite s"
D11-1004,D09-1006,0,0.0451561,"lgorithm for optimizing task loss on N -best lists in general dimensions. We also present an approximate version of LP-MERT that offers a natural means of trading speed for accuracy, as we are guaranteed to eventually find the global optimum as we gradually increase beam size. This trade-off may be beneficial in commercial settings and in large-scale evaluations like the NIST evaluation, i.e., when one has a stable system and is willing to let MERT run for days or weeks to get the best possible accuracy. We think this work would also be useful as we turn to more human involvement in training (Zaidan and Callison-Burch, 2009), as MERT in this case is intrinsically slow. 2 Unidimensional MERT Let f S1 = f1 . . . fS denote the S input sentences of our tuning set. For each sentence fs , let Cs = 39 es,1 . . . es,N denote a set of N candidate translations. For simplicity and without loss of generality, we assume that N is constant for each index s. Each input and output sentence pair (fs , es,n ) is weighted by a linear model that combines model parameters w = w1 . . . wD ∈ RD with D feature functions h1 (f , e, ∼) . . . hD (f , e, ∼), where ∼ is the hidden state associated with the derivation from f to e, such as phr"
D11-1004,D07-1055,0,0.365227,"Missing"
D11-1004,N09-2006,0,0.375842,"ization techniques applied to non-convex problems, the Och algorithm (Och, 2003) represents a significant advance for MERT since it applies a series of special line minimizations that happen to be exhaustive and efficient. Since this algorithm remains inexact in the multidimensional case, much of the recent work on MERT has focused on extending Och’s algorithm to find better search directions and starting points (Cer et al., 2008; Moore and Quirk, 2008), and on experimenting with other derivative-free methods such as the Nelder-Mead simplex algorithm (Nelder and Mead, 1965; Zens et al., 2007; Zhao and Chen, 2009). In this paper, we present LP-MERT, an exact search algorithm for N -best optimization that exploits general assumptions commonly made with MERT, e.g., that the error metric is decomposable by sentence.1 While there is no known optimal algo1 Note that MERT makes two types of approximations. First, the set of all possible outputs is represented only approximately, by N -best lists, lattices, or hypergraphs. Second, error functions on such representations are non-convex and previous work only offers approximate techniques to optimize them. Our work avoids the second approximation, while the fir"
D11-1004,P02-1040,0,\N,Missing
D13-1106,W12-2703,0,0.286096,"average across several test sets. 1 Introduction Recently, several feed-forward neural networkbased language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012). In this paper we focus on recurrent neural network architectures, which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012), outperforming multi-layer feed-forward based networks in both perplexity and word error rate in speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). The major attraction of recurrent architectures is their potential to capture long-span dependencies since predictions are based on an unbounded history of previous words. This is in contrast to feed-forward networks as well as conventional n-gram models, both of which are limited to fixed-length contexts. Building on the success of recurrent architectures, we base our joint language and translation model on an extension of the recurrent neural network language model (Mikolov and Zweig, 2012) that introduces a layer of additional inputs (§2). Most previous work on"
D13-1106,J92-4003,0,0.206592,"s from 2010-2011 containing between 2034-3003 sentences. Log-linear weights are estimated on the 2009 data set comprising 2525 sentences. We rescore the lattices produced by the baseline systems with an aggressive but effective context beam of k = 1 that did not harm accuracy in preliminary experiments (§3). Neural Network Language Model. The vocabularies of the language models are comprised of the words in the training set after removing singletons. We obtain word-classes using a version of Brown-Clustering with an additional regularization term to optimize the runtime of the language model (Brown et al., 1992; Zweig and Makarychev, 2013). 1048 Direct connections use maximum entropy features over unigrams, bigrams and trigrams (Mikolov et al., 2011a). We use the standard settings for the model with the default learning rate α = 0.1 that decays exponentially if the validation set entropy does not increase after each epoch. Back propagation through time computes error gradients over the past twenty time steps. Training is stopped after 20 epochs or when the validation entropy does not decrease over two epochs. We experiment with varying training data sizes and randomly draw the data from the same cor"
D13-1106,D11-1103,0,0.0212121,"e representing an unbounded history of both source and target words, rather than a feedforward style network. Feed-forward networks and n-gram models have a finite history which makes predictions independent of anything but a small history of words. Furthermore, we only model the target-side which is different to previous work modeling both sides. We introduced a new algorithm to tackle lattice rescoring with an unbounded model. The automatic speech recognition community has previously addressed this issue by either approximating longspan language models via simpler but more tractable models (Deoras et al., 2011b), or by identifying confusable subsets of the lattice from which n-best lists are constructed and rescored (Deoras et al., 2011a). We extend their work by directly mapping a recurrent neural network model onto the structure of the lattice, rescoring all states instead of focusing only on subsets. 7 Conclusion and Future Work Joint language and translation modeling with recurrent neural networks leads to substantial gains over the 1-best decoder output, raising accuracy by up to 1.5 BLEU and by 1.1 BLEU on average across several test sets. The joint approach also improves over the gains of th"
D13-1106,N03-1017,0,0.0236734,"result in similar recurrent histories, which in turn reduces the effect of aggressive pruning. 4 Language Model Experiments Recurrent neural network language models have previously only been used in n-best rescoring settings and on small-scale tasks with baseline language models trained on only 17.5m words (Mikolov, 2012). We extend this work by experimenting on lattices using strong baselines with ngram models trained on over one billion words and by evaluating on a number of language pairs. 4.1 Experimental Setup Baseline. We experiment with an in-house phrasebased system similar to Moses (Koehn et al., 2003), scoring translations by a set of common features including maximum likelihood estimates of source given target mappings pM LE (e|f ) and vice versa pM LE (f |e), as well as lexical weighting estimates pLW (e|f ) and pLW (f |e), word and phrasepenalties, a linear distortion feature and a lexicalized reordering feature. Log-linear weights are estimated with minimum error rate training (Och, 2003). Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English, German-English and EnglishGerman. Translation models are estimated on 102m words of parallel"
D13-1106,P07-2045,0,0.0219354,"ynamic programming point of view. Fortunately, we do not need to maintain entire translations as context in the states: the recurrent model compactly encodes the entire history of previous words in the hidden layer configuration hi . It is therefore sufficient to add hi as context, instead of the entire translation. The language model can then simply score any new words based on hi from the previous state when a new state is created. A much larger problem is that items, that were previously equivalent from a dynamic programming perspective, may now be different. Standard phrasebased decoders (Koehn et al., 2007) recombine decoder states with the same context into a single state because they are equivalent to the model features; usually recombination retains only the highest scoring candidate.3 However, if the context is large, then the amount of recombination will decrease significantly, leading to less variety in the decoder beam. This was confirmed in preliminary experiments where we simulated context sizes of up to 100 words but found that accuracy dropped by between 0.5-1.0 BLEU. Integrating a long-span language model na¨ıvely requires to keep context equivalent to the entire left prefix of the t"
D13-1106,W04-3250,0,0.111211,"d any significant improvements. Even this representation of sentences is composed of a large number of instances, and so we resorted to feature hashing by computing feature ids as the least significant 20 bits of each feature name. Our best transform achieved a cosine similarity of 0.816 on the training data, 0.757 on the validation data, and 0.749 on news2011. The results (Table 8) show that the transform improves over the recurrent neural network language model on all test sets and by 0.2 BLEU on average. We verified significance over the target-only model using paired bootstrap resampling (Koehn, 2004) over all test sets (7526 sentences) at the p < 0.001 level. Overall, we improve accuracy by up to 1.5 1052 BLEU and by 1.1 BLEU on average across all test sets over the decoder 1-best with our joint language and translation model. 6 Related Work Our approach of combining language and translation modeling is very much in line with recent work on n-gram-based translation models (Crego and Yvon, 2010), and more recently continuous space-based translation models (Le et al., 2012a; Gao et al., 2013). The joint model presented in this paper differs in a number of key aspects: we use a recurrent arc"
D13-1106,N12-1005,0,0.864704,"l builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets. 1 Introduction Recently, several feed-forward neural networkbased language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012). In this paper we focus on recurrent neural network architectures, which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012), outperforming multi-layer feed-forward based networks in both perplexity and word error rate in speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). The major attraction of recurrent architectures is their potential to capture long-span dependencies since predictions are based on an unbounded history of previous words. This is in contrast to feed-forward net"
D13-1106,D08-1076,0,0.0229444,"Missing"
D13-1106,P03-1021,0,0.174336,"models trained on over one billion words and by evaluating on a number of language pairs. 4.1 Experimental Setup Baseline. We experiment with an in-house phrasebased system similar to Moses (Koehn et al., 2003), scoring translations by a set of common features including maximum likelihood estimates of source given target mappings pM LE (e|f ) and vice versa pM LE (f |e), as well as lexical weighting estimates pLW (e|f ) and pLW (f |e), word and phrasepenalties, a linear distortion feature and a lexicalized reordering feature. Log-linear weights are estimated with minimum error rate training (Och, 2003). Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English, German-English and EnglishGerman. Translation models are estimated on 102m words of parallel data for French-English, 91m words for German-English and English-German; between 3.5-5m words are newswire, depending on the language pair, and the remainder are parliamentary proceedings. The baseline systems use two 5-gram modified Kneser-Ney language models; the first is estimated on the target-side of the parallel data, while the second is based on a large newswire corpus released as part o"
D13-1106,W12-2702,0,0.0348831,"known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets. 1 Introduction Recently, several feed-forward neural networkbased language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012). In this paper we focus on recurrent neural network architectures, which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012), outperforming multi-layer feed-forward based networks in both perplexity and word error rate in speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). The major attraction of recurrent architectures is their potential to capture long-span dependencies since predictions are based on an unbounded history of previous words. This is in contrast to feed-forward networks as well as convent"
D13-1106,N13-1090,1,\N,Missing
D13-1106,W11-2135,0,\N,Missing
D13-1109,W10-0701,0,0.0158926,"wever, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing (Callison-Burch and Dredze, 2010) or pivoting through related languages (Schafer and Yarowsky, 2002; Nakov and Ng, 2009). Daum´e III and Jagarlamudi (2011) mine translations for high frequency OOV words in NEWdomain text in order to do domain adaptation. Although that work shows significant MT improvements, it is based primarily on distributional similarity, thus making it difficult to learn translations for low frequency source words with sparse word context counts. Additionally, that work reports results using artificially created monolingual corpora taken from separate source and target halves of a NEWdomain parallel corpu"
D13-1109,P13-1141,1,0.923939,"text, it is one of the largest freely available parallel corpora for any lan6 We could have, analogously, used the target language (English) side of the parallel corpus and measure overlap with the English Wikipedia documents, or even used both. 7 http://www.parl.gc.ca guage pair. In order to simulate more typical data settings, we sample every 32nd line, using the resulting parallel corpus of 253, 387 lines and 5, 051, 016 tokens to train our baseline model. We test our model using three NEW-domain corpora: (1) the EMEA medical corpus (Tiedemann, 2009), (2) a corpus of scientific abstracts (Carpuat et al., 2013a), and (3) a corpus of translated movie subtitles (Tiedemann, 2009). We use development and test sets to tune and evaluate our MT models. We use the NEW-domain parallel training corpora only for language modeling and for identifying NEW -domain-like comparable documents. 4.2 Machine translation We use the Moses MT framework (Koehn et al., 2007) to build a standard statistical phrase-based MT model using our OLD-domain training data. Using Moses, we extract a phrase table with a phrase limit of five words and estimate the standard set of five feature functions (phrase and lexical translation p"
D13-1109,N12-1047,0,0.0302403,"007) to build a standard statistical phrase-based MT model using our OLD-domain training data. Using Moses, we extract a phrase table with a phrase limit of five words and estimate the standard set of five feature functions (phrase and lexical translation probabilities in each direction and a constant phrase penalty feature). We also use a standard lexicalized reordering model and two language models based on the English side of the Hansard data and the given NEW -domain training corpora. Features are combined using a log-linear model optimized for BLEU, using the n-best batch MIRA algorithm (Cherry and Foster, 2012). We call this the “simple baseline.” In Section 5.2 we describe several other baseline approaches. 4.3 Experiments For each domain, we use the marginal matching method described in Section 3 to learn a new, domain-adapted joint distribution, pnew k (s, t), over all French and English words. We use the learned joint to compute conditional probabilities, pnew k (t|s), for each French word s and rank English translations t accordingly. First, we evaluate the learned joint directly using the distribution based on the wordaligned NEW-domain development set as a gold standard. Then, we perform end-"
D13-1109,P11-2071,1,0.710111,"Missing"
D13-1109,H92-1020,0,0.378434,"ora taken from separate source and target halves of a NEWdomain parallel corpus, which may have more lexical overlap with the corresponding test set than we could expect from true monolingual corpora. Our work mines NEW-domain-like document pairs from Wikipedia. In this work, we show that, keeping data resources constant, our model drastically outperforms this previous approach. Razmara et al. (2013) take a fundamentally different approach and construct a graph using source language monolingual text and identify translations for source language OOV words by pivoting through paraphrases. Della Pietra et al. (1992) and Federico (1999) explore models for combining foreground and background distributions for the purpose of language modeling, and their approaches are somewhat similar to ours. However, our focus is on translation. 3 Model Our goal is to recover a probabilistic translation dictionary in a NEW-domain, represented as a joint probability distribution pnew (s, t) over source/target word pairs. At our disposal, we have access to a joint distribution pold (s, t) from the OLD-domain (computed from word alignments), plus comparable document pairs in the NEW-domain. From these comparable documents, w"
D13-1109,D12-1025,0,0.124393,"rns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing (Callison-Burch and Dredze, 2010) or pivoting through related languages (Schafer and Yarowsky, 2002; Nakov and Ng, 2009). Daum´e III and Jagarlamudi (2011) mine translations for high frequency OOV words in NEWdomain te"
D13-1109,P98-1069,0,0.283477,"1078 ing domains in machine translation. That work concludes that errors resulting from unseen (OOV) and new translation sense words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. P"
D13-1109,P08-1088,0,0.365017,"e words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an"
D13-1109,N13-1056,1,0.779102,"n MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourci"
D13-1109,P06-1103,0,0.114295,"esulting from unseen (OOV) and new translation sense words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translati"
D13-1109,W02-0902,0,0.592232,"and new translation sense words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is r"
D13-1109,P07-2045,0,0.00516723,"32nd line, using the resulting parallel corpus of 253, 387 lines and 5, 051, 016 tokens to train our baseline model. We test our model using three NEW-domain corpora: (1) the EMEA medical corpus (Tiedemann, 2009), (2) a corpus of scientific abstracts (Carpuat et al., 2013a), and (3) a corpus of translated movie subtitles (Tiedemann, 2009). We use development and test sets to tune and evaluate our MT models. We use the NEW-domain parallel training corpora only for language modeling and for identifying NEW -domain-like comparable documents. 4.2 Machine translation We use the Moses MT framework (Koehn et al., 2007) to build a standard statistical phrase-based MT model using our OLD-domain training data. Using Moses, we extract a phrase table with a phrase limit of five words and estimate the standard set of five feature functions (phrase and lexical translation probabilities in each direction and a constant phrase penalty feature). We also use a standard lexicalized reordering model and two language models based on the English side of the Hansard data and the given NEW -domain training corpora. Features are combined using a log-linear model optimized for BLEU, using the n-best batch MIRA algorithm (Cher"
D13-1109,D09-1092,0,0.032703,"ity of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual di"
D13-1109,P10-2041,0,0.0677104,"e could have targeted our learning even more by using our NEW-domain MT test sets. Doing so would increase the chances that our source language words of interest appear in the comparable corpus. However, to avoid overfitting any particular test set, we use the French side of the training data. For each Wikipedia document pair, we compute the percent of French phrases up to length four that are observed in the French monolingual NEW -domain corpus and rank document pairs by the geometric mean of the four overlap measures. More sophisticated ways to identify NEW-domainlike Wikipedia pages (e.g. Moore and Lewis (2010)) may yield additional performance gains, but, qualitatively, the ranked Wikipedia pages seemed reasonable to the authors. 4 Experimental setup 4.1 Data We use French-English Hansard parliamentary proceedings7 as our OLD-domain parallel corpus. With over 8 million parallel lines of text, it is one of the largest freely available parallel corpora for any lan6 We could have, analogously, used the target language (English) side of the parallel corpus and measure overlap with the English Wikipedia documents, or even used both. 7 http://www.parl.gc.ca guage pair. In order to simulate more typical d"
D13-1109,D09-1141,0,0.0196136,"system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing (Callison-Burch and Dredze, 2010) or pivoting through related languages (Schafer and Yarowsky, 2002; Nakov and Ng, 2009). Daum´e III and Jagarlamudi (2011) mine translations for high frequency OOV words in NEWdomain text in order to do domain adaptation. Although that work shows significant MT improvements, it is based primarily on distributional similarity, thus making it difficult to learn translations for low frequency source words with sparse word context counts. Additionally, that work reports results using artificially created monolingual corpora taken from separate source and target halves of a NEWdomain parallel corpus, which may have more lexical overlap with the corresponding test set than we could ex"
D13-1109,P12-1017,0,0.140934,"monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing (Callison-Burch and Dredze, 2010) or pivoting through related languages (Schafer and Yarowsky, 2002; Nakov and Ng, 2009). Daum´e III and Jagarlamudi (2011) mine translations for high frequency OOV words in NEWdomain text in order to do domain"
D13-1109,J03-1002,0,0.00435154,"pair marginal distributions. Finally, we describe how we identify comparable document pairs relevant to the NEW-domain. 3.1 Marginal Matching Objective Given word-aligned parallel data in the OLD-domain and source and target comparable corpora in the NEW -domain, we first estimate a joint distribution pold (s, t) over word pairs (s, t) in the OLD-domain, where s and t range over source and target language words, respectively. For the OLD-domain joint distribution, we use a simple maximum likelihood estimate based on non-null automatic word alignments (using grow-diag-final GIZA++ alignments (Och and Ney, 2003)). Next, we find source and target marginal distributions, q(s) and q(t), by relative frequency estimates over the source and target comparable corpora. Our goal is to recover a joint distribution pnew (s, t) for the new domain that matches the marginals, q(s) and q(t), but is minimally different from the original joint distribution, pold (s, t). We cast this as a linear programming problem: pnew = arg min p − pold (1) p 1 X subject to: p(s, t) = 1, p(s, t) ≥ 0 s,t X p(s, t) = q(t), s X p(s, t) = q(s) t In the objective function, the joint probability matrices p and pold are interpreted as lar"
D13-1109,P11-1133,0,0.022372,"formance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource se"
D13-1109,P95-1050,0,0.759395,"ikipedia.org 1078 ing domains in machine translation. That work concludes that errors resulting from unseen (OOV) and new translation sense words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain a"
D13-1109,P99-1067,0,0.15112,"machine translation. That work concludes that errors resulting from unseen (OOV) and new translation sense words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data"
D13-1109,P11-1002,0,0.20353,"ethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing (Callison-Burch and Dredze, 2010) or pivoting through related languages (Schafer and Yarowsky, 2002; Nakov and Ng, 2009). Daum´e III and Jagarlamudi (2011) mine translations for high frequency OO"
D13-1109,P13-1109,0,0.492486,"tributional similarity, thus making it difficult to learn translations for low frequency source words with sparse word context counts. Additionally, that work reports results using artificially created monolingual corpora taken from separate source and target halves of a NEWdomain parallel corpus, which may have more lexical overlap with the corresponding test set than we could expect from true monolingual corpora. Our work mines NEW-domain-like document pairs from Wikipedia. In this work, we show that, keeping data resources constant, our model drastically outperforms this previous approach. Razmara et al. (2013) take a fundamentally different approach and construct a graph using source language monolingual text and identify translations for source language OOV words by pivoting through paraphrases. Della Pietra et al. (1992) and Federico (1999) explore models for combining foreground and background distributions for the purpose of language modeling, and their approaches are somewhat similar to ours. However, our focus is on translation. 3 Model Our goal is to recover a probabilistic translation dictionary in a NEW-domain, represented as a joint probability distribution pnew (s, t) over source/target"
D13-1109,W02-2026,0,0.246367,"nslation. That work concludes that errors resulting from unseen (OOV) and new translation sense words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a"
D13-1109,N10-1063,1,0.926845,"Missing"
D13-1109,C98-1066,0,\N,Missing
D13-1109,Q13-1035,1,\N,Missing
D13-1201,J93-2003,0,0.0480096,"g to compute w0 in the first place is a bit of a disadvantage compared to standard MERT, the need for good initializer is hardly surprising in the context of non-convex optimization. Other non-convex problems in machine learning, such as deep neural networks (DNN) and word alignment models, commonly require such initializers in order to obtain decent performance. In the case of DNN, extensive research is devoted to the problem of finding good initializers.10 In the case of word alignment, it is common practice to initialize search in non-convex optimization problems—such as IBM Model 3 and 4 (Brown et al., 1993)—with solutions of simpler models—such as IBM Model 1. 7 Related work the weights in the context of MERT, (Cer et al., 2008) achieves a related effect. Cer et al.’s goal is to achieve a more regular or smooth objective function, while ours is to obtain a more regular set of parameters. The two approaches may be complementary. More recently, new research has explored direction finding using a smooth surrogate loss function (Flanigan et al., 2013). Although this method is successful in helping MERT find better directions, it also exacerbates the tendency of MERT to overfit.11 As an indirect way"
D13-1201,W08-0304,0,0.721887,"ularized objective function along the line. Finally, we address the issue of searching in a high-dimensional space by using the gradient of expected BLEU (Smith and Eisner, 2006) to find better search directions for our line searches. This direction finder addresses one of the serious concerns raised by Hopkins and May (2011): MERT widely failed to reach the optimum of a synthetic linear objective function. In replicating Hopkins and May’s experiments, we confirm that existing search algorithms for MERT—including coordinate ascent, Powell’s algorithm (Powell, 1964), and random direction sets (Cer et al., 2008)—perform poorly in this experimental condition. However, when using our gradient-based direction finder, MERT has no problem finding the true optimum even in a 1000-dimensional space. Our results suggest that the combination of a regularized objective function and a gradient-informed line search algorithm enables MERT to scale well with a large number of features. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO (Hopkins and May, 2011), a parameter tuning method known to be effective with large feature sets. 2 Unregularized MERT Prior to i"
D13-1201,N12-1047,1,0.910733,"Microsoft Research National Research Council Kristina Toutanova Microsoft Research mgalley@microsoft.com chrisq@microsoft.com kristout@microsoft.com colin.cherry@nrc-cnrc.gc.ca Abstract Secondly, it offers a globally optimal line search. Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization. These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization. In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation. On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method appears at least as good as MERT on small feature se"
D13-1201,N13-1003,1,0.916487,"g to w∗ has a BLEU score of 1, and so that the translation with lowest model score for the sentence gets a BLEU of zero. This normalization has no impact on search, but makes results more interpretable. For our translation experiments, we use multistack phrase-based decoding (Koehn et al., 2007). We report results for two feature sets: non-linear features induced using Gradient Boosting Machines (Toutanova and Ahn, 2013) and sparse lexicalized 7 The objective function remains piecewise constant, and the plateau containing w∗ maps to the optimal value of the function. 1954 reordering features (Cherry, 2013). We exploit these feature sets (GBM and SparseHRM, respectively) in two distinct experimental conditions, which we detail in the two next paragraphs. Both GBM and SparseHRM augment baseline features similar to Moses’: relative frequency and lexicalized phrase translation scores for both translation directions; one or two language model features, depending on the language pair; distortion penalty; word and phrase count; six lexicalized reordering features. For both experimental conditions, phrase tables have maximum phrase length of 7 words on either side. In reference to Table 1, we used the"
D13-1201,D08-1024,0,0.392627,"Missing"
D13-1201,N09-1025,0,0.103202,"Microsoft Research Chris Quirk Colin Cherry Microsoft Research National Research Council Kristina Toutanova Microsoft Research mgalley@microsoft.com chrisq@microsoft.com kristout@microsoft.com colin.cherry@nrc-cnrc.gc.ca Abstract Secondly, it offers a globally optimal line search. Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization. These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization. In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation. On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method app"
D13-1201,J07-2003,0,0.09406,"tion can drive the regularization term down to zero by scaling down w. As special treatments for `2 , we evaluate three linear transforms of the weight vector, where the vector w of the regularization term ||w||22 /2σ 2 is replaced with either: 1. an affine transform: w − w0 2. a vector with only (D − 1) free parameters, e.g., 0 ) (1, w20 , · · · , wD 3. an `1 renormalization: w/||w||1 In (1), regularization is biased towards w0 , a weight vector previously optimized using a competitive yet much smaller feature set, such as core features of a phrase-based (Koehn et al., 2007) or hierarchical (Chiang, 2007) system. The requirement that this feature set be small is to prevent overfitting. Otherwise, any regularization toward an overfit parameter vector w0 would defeat the purpose of introducing a regularization term in the first place.3 In (2), the transformation is motivated by the observation that the D-parameter linear model of Equation 2 only needs (D − 1) degrees of freedom. Fixing one of the components of w to any non-zero constant and allowing the others to vary, the new linear model retains the same modeling power, but the (D − 1) free parameters are no longer scale invariant, i.e., scali"
D13-1201,N13-1025,0,0.320973,"Missing"
D13-1201,D11-1004,1,0.672028,"search towards the greatest increase of expected BLEU score. While our best results are comparable to PRO and not significantly better, we think that this paper provides a deeper understanding of why standard MERT can fail when handling an increasingly larger number of features. Furthermore, this paper complements the analysis by Hopkins and May (2011) of the differences between MERT and optimization with a surrogate loss function. MERT and its extensions have been the target of extensive research (Och, 2003; Macherey et al., 2008; Cer et al., 2008; Moore and Quirk, 2008; Kumar et al., 2009; Galley and Quirk, 2011). More recent work has focused on replacing MERT with a linearly decomposable approximations of the evaluation metric (Smith and Eisner, 2006; Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011; Rosti et al., 2011; Gimpel and Smith, 2012; Cherry and Foster, 2012), which generally involve a surrogate loss function incorporating a regularization term such as the `2 -norm. While we are not aware of any previous work adding a penalty on We thank the anonymous reviewers for their helpful comments and suggestions. 10 For example, (Larochelle et al., 2009) presents"
D13-1201,N12-1023,0,0.787448,"one of the components of w to any non-zero constant and allowing the others to vary, the new linear model retains the same modeling power, but the (D − 1) free parameters are no longer scale invariant, i.e., scaling the (D − 1)-dimensional vector now has an effect on linear model predictions. In (3), the weight vector is normalized as to have an `1 -norm equal to 1. In contrast, the `0 norm is scale insensitive, thus not affected by this problem. 3.1 Exact line search with regularization Optimizing with a regularized error surface requires a change in the line search algorithm presented in 3 (Gimpel and Smith, 2012, footnote 6) briefly mentions the use of such a regularizer with its ramp loss objective function. 1951 Section 2, but the other aspects of MERT remain the same, and we can still use global search algorithms such as coordinate ascent, Powell, and random directions exactly the same way as with unregularized MERT. Line search with a regularization term is still as efficient as in (Och, 2003), and it is still guaranteed to find the optimum of the (now regularized) objective function along the line. Considering again a given point wt and a given direction dt at line search iteration t, finding th"
D13-1201,P12-1031,0,0.190707,"Missing"
D13-1201,D11-1125,0,0.578272,"ris Quirk Colin Cherry Microsoft Research National Research Council Kristina Toutanova Microsoft Research mgalley@microsoft.com chrisq@microsoft.com kristout@microsoft.com colin.cherry@nrc-cnrc.gc.ca Abstract Secondly, it offers a globally optimal line search. Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization. These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization. In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation. On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method appears at least as good a"
D13-1201,P07-2045,0,0.0817011,"constant intervals of the corpus-level error function, and by selecting the one that has the lowest error count (or, correspondingly, highest BLEU score). Assuming the optimum is found in the interval [γk−1 , γk ], we define γopt = (γk−1 + γk )/2 and change the parameters using the update wt+1 = wt + γopt · dt . Finally, this method is turned into a global Ddimensional search using algorithms that repeatedly use the aforementioned exact line search algorithm. Och (2003) first advocated the use of Powell’s method (Powell, 1964; Press et al., 2007). Pharaoh (Koehn, 2004) and subsequently Moses (Koehn et al., 2007) instead use coordinate ascent, and more recent work often uses random search directions (Cer et al., 2008; Macherey et al., 2008). In Section 4, we will present a novel direction finder for maximum-BLEU optimization, which uses the gradient of expected BLEU to find directions where the BLEU score is most likely to increase. 3 Regularization for MERT Because MERT is prone to overfitting when a large number of parameters must be optimized, we study the addition of a regularization term to the objective function. One conventional approach is to regularize the objective function with a penalty ba"
D13-1201,koen-2004-pharaoh,0,0.526758,"omputed by enumerating all piecewise constant intervals of the corpus-level error function, and by selecting the one that has the lowest error count (or, correspondingly, highest BLEU score). Assuming the optimum is found in the interval [γk−1 , γk ], we define γopt = (γk−1 + γk )/2 and change the parameters using the update wt+1 = wt + γopt · dt . Finally, this method is turned into a global Ddimensional search using algorithms that repeatedly use the aforementioned exact line search algorithm. Och (2003) first advocated the use of Powell’s method (Powell, 1964; Press et al., 2007). Pharaoh (Koehn, 2004) and subsequently Moses (Koehn et al., 2007) instead use coordinate ascent, and more recent work often uses random search directions (Cer et al., 2008; Macherey et al., 2008). In Section 4, we will present a novel direction finder for maximum-BLEU optimization, which uses the gradient of expected BLEU to find directions where the BLEU score is most likely to increase. 3 Regularization for MERT Because MERT is prone to overfitting when a large number of parameters must be optimized, we study the addition of a regularization term to the objective function. One conventional approach is to regular"
D13-1201,P09-1019,0,0.704285,"rches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g., BLEU) instead of some surrogate loss. In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search. The idea of adding a regularization term to the MERT objective function can be perplexing at first, because the most common regularizers, such as `1 and `2 , are not directly applicable to MERT. Indeed, these regul"
D13-1201,P06-1096,0,0.129495,"Missing"
D13-1201,C04-1072,0,0.0497267,"our optimizer featuring both regularization and the gradient-based direction finder. All variants of MERT are initialized with a single starting point, which is either uniform weight or w0 . Instead of providing MERT with additional random starting points as in Moses, we use random walks as in (Moore and Quirk, 2008) to attempt to move out of local optima.8 Since PRO and our optimizer have hyperparameters, we use a held-out set (Dev) for adjusting them. For PRO, we adjust three parameters: a regularization penalty for `2 , the parameter α in the add-α smoothed sentence-level version of BLEU (Lin and Och, 2004), and a parameter for scaling the corpus-level length of the references. The latter scaling parameter is discussed in (He and 8 In the case of the gradient-based direction finder, we also use the following strategy whenever optimization converges to a (possibly local) optimum. We run one round of coordinate ascent, and continue with the gradient direction finder as soon as the optimum improves. If the none of the coordinate directions helped, we stop the search. Method MERT MERT MERT PRO `2 MERT (v1: ||w − w0 ||) `2 MERT (v2: D − 1 dimensions) `2 MERT (v3: `1 -renormalized) `0 MERT Starting pt"
D13-1201,D08-1076,0,0.707037,"ed BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g., BLEU) instead of some surrogate loss. In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search. The idea of adding a regularization term to the MERT objective function can be perplexing at first, because the most common regularizers, such as `1 and `2 , are not directly app"
D13-1201,C08-1074,1,0.906037,"Finally, Figure 3 shows our rate of convergence compared to coordinate ascent. Our experimental results with the GBM feature set data are shown in Table 2. Each table is divided into three sections corresponding respectively to MERT (Och, 2003) with Koehn-style coordinate ascent (Koehn, 2004), PRO, and our optimizer featuring both regularization and the gradient-based direction finder. All variants of MERT are initialized with a single starting point, which is either uniform weight or w0 . Instead of providing MERT with additional random starting points as in Moses, we use random walks as in (Moore and Quirk, 2008) to attempt to move out of local optima.8 Since PRO and our optimizer have hyperparameters, we use a held-out set (Dev) for adjusting them. For PRO, we adjust three parameters: a regularization penalty for `2 , the parameter α in the add-α smoothed sentence-level version of BLEU (Lin and Och, 2004), and a parameter for scaling the corpus-level length of the references. The latter scaling parameter is discussed in (He and 8 In the case of the gradient-based direction finder, we also use the following strategy whenever optimization converges to a (possibly local) optimum. We run one round of coo"
D13-1201,C12-1121,0,0.0629013,"on the Tune set. For PRO and regularized MERT, we optimized with different hyperparameters (regularization weight, etc.), and retained for each experimental condition the model that worked best on Dev. The table shows the performance of these retained models. 52.6 52.4 BLEU 52.2 52 51.8 51.6 51.4 51.2 1e-05 expected BLEU gradient coordinate ascent 0.0001 0.001 0.01 0.1 regularization weight 1 10 Figure 4: BLEU score on the Finnish Dev set (GBM) with different values for the 1/2σ 2 regularization weight. To enable comparable results, the other hyperparameter (length) is kept fixed. Deng, 2012; Nakov et al., 2012) and addresses the problem that systems tuned with PRO tend to produce sentences that are too short. On the other hand, regularized MERT only requires one hyperparameter to tune: a regularization penalty for `2 or `0 . However, since PRO optimizes translation length on the Dev dataset and MERT does so using the Tune set, a comparison of the two systems would yield a discrepancy in length that would be undesirable. Therefore, we add another hyperparameter to regularized MERT to tune length in the same manner using the Dev set. Table 2 offers several findings. First, unregularized MERT can achie"
D13-1201,P02-1038,0,0.192281,"integrating them during search. To improve search in large parameter spaces, we also present a new direction finding algorithm that uses the gradient of expected BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g., BLEU) instead of some surrogate loss. In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search. The idea of add"
D13-1201,P03-1021,0,0.145765,"s `2 are inapplicable to MERT due to the scale invariance of its objective function, we turn to two regularizers—`0 and a modification of `2 — and present methods for efficiently integrating them during search. To improve search in large parameter spaces, we also present a new direction finding algorithm that uses the gradient of expected BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g"
D13-1201,2001.mtsummit-papers.68,0,0.129733,"umber of parameters must be optimized, we study the addition of a regularization term to the objective function. One conventional approach is to regularize the objective function with a penalty based on the qP 2 Euclidean norm ||w||2 = i wi , also known as `2 regularization. In the case of MERT, this yields the following objective function:2 ˆ = arg min w w X S s=1 ||w||22 E(rs , ˆ e(fs ; w)) + 2σ 2  (4) 1 This assumes that the sufficient statistics of the metric under consideration are additively decomposable by sentence, which is the case with most popular evaluation metrics such as BLEU (Papineni et al., 2001). 2 The `2 regularizer is often used in conjunction with loglikelihood objectives. The regularization term of Equation 4 could similarly be added to the log of an objective—e.g., log(BLEU) instead of BLEU—but we found that the distinction doesn’t have much of an impact in practice. 1950 1.4 1.2 1 0.8 0.6 0.4 0.2 0 -0.2 -0.4 -0.3 -0.2 -0.1 MERT Max at 0.225 × × 0 0.1 0.2 0.3 0.4 1.4 MERT − `2 1.2 Max at -0.018 × 1 −`2 0.8 0.6 × 0.4 0.2 0 -0.2 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 × 1.4 MERT − `0 1.2 Max at 0 × 1 `0 0.8 0.6 0.4 0.2 0 -0.2 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 γ, the step size in the"
D13-1201,W11-2119,0,0.0513815,"larger number of features. Furthermore, this paper complements the analysis by Hopkins and May (2011) of the differences between MERT and optimization with a surrogate loss function. MERT and its extensions have been the target of extensive research (Och, 2003; Macherey et al., 2008; Cer et al., 2008; Moore and Quirk, 2008; Kumar et al., 2009; Galley and Quirk, 2011). More recent work has focused on replacing MERT with a linearly decomposable approximations of the evaluation metric (Smith and Eisner, 2006; Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011; Rosti et al., 2011; Gimpel and Smith, 2012; Cherry and Foster, 2012), which generally involve a surrogate loss function incorporating a regularization term such as the `2 -norm. While we are not aware of any previous work adding a penalty on We thank the anonymous reviewers for their helpful comments and suggestions. 10 For example, (Larochelle et al., 2009) presents a pre-trained DNN that outperforms a shallow network, but the performance of the DNN becomes much worse relative to the shallow network once pre-training is turned off. 11 Indeed, in their Table 3, a comparison between HILS and HOLS suggests tuning"
D13-1201,P06-2101,0,0.503336,"l Language Processing, pages 1948–1959, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics regularization, where we apply `2 regularization to scale-senstive linear transforms of the original linear model. In addition, we introduce efficient methods of incorporating regularization in Och (2003)’s exact line searches. For all of these regularizers, our methods let us find the true optimum of the regularized objective function along the line. Finally, we address the issue of searching in a high-dimensional space by using the gradient of expected BLEU (Smith and Eisner, 2006) to find better search directions for our line searches. This direction finder addresses one of the serious concerns raised by Hopkins and May (2011): MERT widely failed to reach the optimum of a synthetic linear objective function. In replicating Hopkins and May’s experiments, we confirm that existing search algorithms for MERT—including coordinate ascent, Powell’s algorithm (Powell, 1964), and random direction sets (Cer et al., 2008)—perform poorly in this experimental condition. However, when using our gradient-based direction finder, MERT has no problem finding the true optimum even in a 1"
D13-1201,P13-2072,1,0.846348,"ector hs,m . By this linear construction, w∗ is guaranteed to be a global optimum.7 The pseudo-BLEU score is normalized for each M -best list, so that the translation with highest model score according to w∗ has a BLEU score of 1, and so that the translation with lowest model score for the sentence gets a BLEU of zero. This normalization has no impact on search, but makes results more interpretable. For our translation experiments, we use multistack phrase-based decoding (Koehn et al., 2007). We report results for two feature sets: non-linear features induced using Gradient Boosting Machines (Toutanova and Ahn, 2013) and sparse lexicalized 7 The objective function remains piecewise constant, and the plateau containing w∗ maps to the optimal value of the function. 1954 reordering features (Cherry, 2013). We exploit these feature sets (GBM and SparseHRM, respectively) in two distinct experimental conditions, which we detail in the two next paragraphs. Both GBM and SparseHRM augment baseline features similar to Moses’: relative frequency and lexicalized phrase translation scores for both translation directions; one or two language model features, depending on the language pair; distortion penalty; word and p"
D13-1201,D07-1080,0,0.35769,"tanding of why standard MERT can fail when handling an increasingly larger number of features. Furthermore, this paper complements the analysis by Hopkins and May (2011) of the differences between MERT and optimization with a surrogate loss function. MERT and its extensions have been the target of extensive research (Och, 2003; Macherey et al., 2008; Cer et al., 2008; Moore and Quirk, 2008; Kumar et al., 2009; Galley and Quirk, 2011). More recent work has focused on replacing MERT with a linearly decomposable approximations of the evaluation metric (Smith and Eisner, 2006; Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011; Rosti et al., 2011; Gimpel and Smith, 2012; Cherry and Foster, 2012), which generally involve a surrogate loss function incorporating a regularization term such as the `2 -norm. While we are not aware of any previous work adding a penalty on We thank the anonymous reviewers for their helpful comments and suggestions. 10 For example, (Larochelle et al., 2009) presents a pre-trained DNN that outperforms a shallow network, but the performance of the DNN becomes much worse relative to the shallow network once pre-training is turned off. 11 Indeed, in t"
D13-1201,P02-1040,0,\N,Missing
D15-1029,P07-2045,0,0.0028606,"word using 5-gram target context and 7-gram source context. For evaluation, we present both the model perplexity and the BLEU score when using the model as an additional MT feature. Results are presented on a large scale EnglishGerman speech translation task. The parallel training data consists of 600M words from a variety of sources, including OPUS (Tiedemann, 2012) and a large in-house web crawl. The baseline 4-gram Kneser-Ney smoothed LM is trained on 7B words of German data. The NNLM and NNTMs are trained only on the parallel data. Our MT decoder is a proprietary engine similar to Moses (Koehn et al., 2007). The tuning set consists of 4000 utterances from conversational and newswire data, and the test set consists of 1500 sentences of collected conversational data. Results are show in Table 4. We can see that perplexity improvements are similar to what is We also report results on a 10-gram LM trained on the same data, to explore whether the lateral network can achieve an even higher relative gain when a large input context window is available. Results are shown in Table 3. Although there is a large absolute improvement over the 5-gram LM, the relative improvement between the 1-layer, 3stacked,"
D15-1029,D13-1170,0,0.00347271,"a neural network architecture where the embedding layer feeds into multiple hidden layers that are placed “next to” one another so that each can be pre-computed independently. On a large scale language modeling task, this architecture achieves a 10x speedup at runtime and a significant reduction in perplexity when compared to a standard multilayer network. 1 Arul Menezes Microsoft Research Redmond, WA, USA Introduction Neural network models have become extremely popular in the last several years for a wide variety of NLP tasks, including language modeling (Schwenk, 2007), sentiment analysis (Socher et al., 2013), translation modeling (Devlin et al., 2014), and many others (Collobert et al., 2011). However, a serious drawback of neural network models is their slow speeds in training and test time (runtime) relative to alternative models such as maximum entropy (Berger et al., 1996) or backoff models (Kneser and Ney, 1995). 256 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 256–260, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. functions are explored rather than just the max function. 2 Lateral Network In a stand"
D15-1029,J96-1002,0,0.0948002,"a significant reduction in perplexity when compared to a standard multilayer network. 1 Arul Menezes Microsoft Research Redmond, WA, USA Introduction Neural network models have become extremely popular in the last several years for a wide variety of NLP tasks, including language modeling (Schwenk, 2007), sentiment analysis (Socher et al., 2013), translation modeling (Devlin et al., 2014), and many others (Collobert et al., 2011). However, a serious drawback of neural network models is their slow speeds in training and test time (runtime) relative to alternative models such as maximum entropy (Berger et al., 1996) or backoff models (Kneser and Ney, 1995). 256 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 256–260, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. functions are explored rather than just the max function. 2 Lateral Network In a standard feed-forward embedding-based neural network, the input tokens are mapped into a continuous vector using an embedding table1 , and this embedding vector is fed into the first hidden layer. The output of each hidden layer is then fed into the next hidden layer. We refer t"
D15-1029,tiedemann-2012-parallel,0,0.0175425,"es in a machine translation (MT) system. The first feature is a 5-gram NNLM, which used 1000 dimensions for the stacked network and 500 for the lateral network. The second feature is a neural network joint model (NNJM), which predicts each target word using 5-gram target context and 7-gram source context. For evaluation, we present both the model perplexity and the BLEU score when using the model as an additional MT feature. Results are presented on a large scale EnglishGerman speech translation task. The parallel training data consists of 600M words from a variety of sources, including OPUS (Tiedemann, 2012) and a large in-house web crawl. The baseline 4-gram Kneser-Ney smoothed LM is trained on 7B words of German data. The NNLM and NNTMs are trained only on the parallel data. Our MT decoder is a proprietary engine similar to Moses (Koehn et al., 2007). The tuning set consists of 4000 utterances from conversational and newswire data, and the test set consists of 1500 sentences of collected conversational data. Results are show in Table 4. We can see that perplexity improvements are similar to what is We also report results on a 10-gram LM trained on the same data, to explore whether the lateral n"
D15-1029,D13-1140,0,0.0351616,"o, since decoding a single sentence typically requires tens of thousands or more n-gram lookups. Although we will focus on this particular scenario in this paper, it is important to note that the techniques presented generalize to any feed-forward embedding-based neural network model. One popular technique for improving the runtime speed of NNLMs involves training the network to be “approximately normalized,” so that the softmax normalizer does not have to be computed after training. Two algorithms have been proposed to achieve this: (1) noise-contrastive estimation (NCE) (Mnih and Teh, 2012; Vaswani et al., 2013) and (2) explicit self-normalization (Devlin et al., 2014), which is used in this paper. However, even with self-normalized networks, computing the output of an intermediate hidden layer still requires a costly matrix-vector multiplication. To mitigate this, Devlin et al. (2014) made the observation that for 1-layer NNLMs, the dot product between each embedding+position pair and the first hidden layer can be pre-computed after training is complete, which allows the matrixvector multiplication to be replaced by a handful of vector additions. Using these two techniques in combination improves th"
D15-1029,P14-1129,1,0.92598,"of thousands or more n-gram lookups. Although we will focus on this particular scenario in this paper, it is important to note that the techniques presented generalize to any feed-forward embedding-based neural network model. One popular technique for improving the runtime speed of NNLMs involves training the network to be “approximately normalized,” so that the softmax normalizer does not have to be computed after training. Two algorithms have been proposed to achieve this: (1) noise-contrastive estimation (NCE) (Mnih and Teh, 2012; Vaswani et al., 2013) and (2) explicit self-normalization (Devlin et al., 2014), which is used in this paper. However, even with self-normalized networks, computing the output of an intermediate hidden layer still requires a costly matrix-vector multiplication. To mitigate this, Devlin et al. (2014) made the observation that for 1-layer NNLMs, the dot product between each embedding+position pair and the first hidden layer can be pre-computed after training is complete, which allows the matrixvector multiplication to be replaced by a handful of vector additions. Using these two techniques in combination improves the runtime speed of NNLMs by several orders of magnitude wi"
D15-1029,W11-2123,0,0.0370445,"n in Table 2. These are computed on a single core of a E5-2650 2.6 GHz CPU. Consistent with Devlin et al. (2014), we see that the baseline model achieves only 230 n-gram lookups per second (LPS) at test time, while the pre-computed, self-normalized 1-layer network achieves 600,000 LPS. Adding a second stacked layer slows this down to 24,000 LPS due to the 500 × 500 matrix-vector multiplication that must be performed. However, the lateral configuration achieves 305,000 LPS while obtaining a better perplexity than the stacked network. In comparison, the fastest backoff LM implementation, KenLM (Heafield, 2011), achieves 1-2 million lookups per second. In terms of memory usage, it is difficult to fairly compare backoff LMs and NNLMs because neural networks scale linearly with the vocabulary size, while backoff LMs scale linearly with the number of unique n-grams. In this case, the nonprecomputed neural network model is 25 MB, and the pre-computed 2-lateral network is 136 MB.4 The KenLM models are 1.1 GB for the Probing model and 317 MB for the Trie model. With a vocabulary of 50k, the 2-lateral network would be 425MB. In general, a pre-computed NNLM is comparable to or smaller than an equivalent bac"
D19-5503,P05-1074,0,0.373742,"Missing"
D19-5503,C18-2019,1,0.852844,"4, 2019. 2019 Association for Computational Linguistics Our primary contribution is to evaluate various methods of constructing paraphrase corpora, including monolingual methods with experts and nonexperts as well as automated, semi-automated, and manual translation-based approaches. Each paraphrasing method is evaluated for fluency (“does the resulting paraphrase sound not only grammatical but natural?”) and adequacy (“does the paraphrase accurately convey the original meaning of the source?”) using human direct assessment, inspired by effective techniques in machine translation evaluation (Federmann, 2018). In addition, we measure the degree of change between the original and rewritten sentence using both edit distance and BLEU (Papineni et al., 2002). Somewhat surprisingly, fully automatic neural machine translation actually outperforms manual human translation in terms of adequacy. The semi-automatic method of post-editing neural machine translation output with human editors leads to fluency improvements while retaining diversity and adequacy. Although none of the translationbased approaches outperform monolingual rewrites in terms of adequacy or fluency, they do produce greater diversity. Hu"
D19-5503,P01-1008,0,0.141237,"ovides volumes of casual online conversations; the Enron email corpus represents communication in the professional world.2 Both are noisier than usual NMT training data; traditionally, such noise has been challenging for NMT systems (Michel and Neubig, 2018) and should provide a lowerbound on their performance. It would definitely be valuable, albeit expensive, to rerun our experiments on a cleaner data source. Related Work Translation as a means of generating paraphrases has been explored for decades. Paraphrase corpora can be extracted from multiple translations of the same source material (Barzilay and McKeown, 2001). Sub-sentential paraphrases (mostly 2 However, the Enron emails often contain conversations about casual and personal matters. 18 Tokens per segment Tokens per segment Segments Types Tokens median mean min max 500 2,370 9,835 19 19.67 4 Segments Types 46 14,500 Tokens median mean min max 7,196 285,833 19 19.72 1 68 Table 1: Key characteristics of the source sentences. Table 2: Key characteristics of collected paraphrases. As an initial filtering step, we ran automatic grammar and spell-checking, in order to select sentences that exhibit some disfluency or clear error. Additionally, we asked c"
D19-5503,N13-1092,0,0.200978,"Missing"
D19-5503,W13-2305,0,0.052917,"en both candidates. Note that this may mean two things: Paraphrase diversity Additionally, we measure diversity of all paraphrases (both monolingual and based on translation) by computing the average number of token edits between source and candidate texts. To focus our attention on meaningful changes as opposed to minor function word rewrites, we normalize both source and candidate by lower-casing and excluding any punctuation and stop words using NLTK (Bird et al., 2009). We adopt source-based direct assessment (src-DA) for human evaluation of adequacy and fluency. The original DA approach (Graham et al., 2013, 2014) is reference-based and, thus, needs to be adapted for use in our paraphrase assessment and translation scoring scenarios. In both cases, we can use the source sentence to guide annotators in their assessment. Of course, this makes translation evaluation more difficult, as we require bilingual annotators. Src-DA has previously been used, e.g., in (Cettolo et al., 2017; Bojar et al., 2018). Direct assessment initializes mental context for annotators by asking a priming question. The user interface shows two sentences: 1. candidates are semantically equivalent but similarly fluent or non-"
D19-5503,E14-1047,0,0.0419479,"Missing"
D19-5503,P17-2017,0,0.0327967,"Missing"
D19-5503,2004.iwslt-evaluation.1,0,0.160704,"Missing"
D19-5503,D17-1126,0,0.0189255,"non-linguistic input, then ask humans to describe this input in language. For instance, crowd-sourced descriptions of videos provide a rich source of paraphrase data that is grounded in visual phenomena (Chen and Dolan, 2011). Such visual grounding helps users focus on a clear and specific activity without imparting a bias toward particular lexical realizations. Unfortunately, these paraphrases are limited to phenomena that can be realized visually. Another path is to find multiple news stories describing the same event (Dolan et al., 2004), or multiple commentaries about the same news story (Lan et al., 2017). Although this provides a rich and growing set of paraphrases, the language is again biased, this time toward events commonly reported in the news. An alternative is to provide input in a foreign language. Nearly anything expressible in one human language can be written in another language. When users translate content, some variation in lexical realization occurs. To gather monolingual paraphrases, we can first translate a source sentence into a variety of target languages, then translate back into the source language, using either humans or machines. This provides naturalistic variation in"
D19-5503,P11-1020,0,0.0418854,"ses. Surprisingly, human translators do not reliably outperform neural systems. The resulting data release will not only be a useful test set, but will also allow additional explorations in translation and paraphrase quality assessments and relationships. 1 Figure 1: Generating broad-coverage paraphrases through pivot translation. One path around the obstacle of reference bias is to provide a non-linguistic input, then ask humans to describe this input in language. For instance, crowd-sourced descriptions of videos provide a rich source of paraphrase data that is grounded in visual phenomena (Chen and Dolan, 2011). Such visual grounding helps users focus on a clear and specific activity without imparting a bias toward particular lexical realizations. Unfortunately, these paraphrases are limited to phenomena that can be realized visually. Another path is to find multiple news stories describing the same event (Dolan et al., 2004), or multiple commentaries about the same news story (Lan et al., 2017). Although this provides a rich and growing set of paraphrases, the language is again biased, this time toward events commonly reported in the news. An alternative is to provide input in a foreign language. N"
D19-5503,D18-1512,0,0.0387336,"Missing"
D19-5503,E17-1083,0,0.0312977,"emantic equivalence) and fluency of paraphrases, as well as translation adequacy assessments. Data is publicly available at https://aka.ms/MultilingualWhispers. 2 phrasal replacements) can be gathered from these multiple translations. Alternatively, one can create a large body of phrasal replacements from by pivoting on the phrase-tables used by phrase-based statistical machine translation (Bannard and CallisonBurch, 2005; Ganitkevitch et al., 2013; Pavlick et al., 2015). Recent work has also explored using neural machine translation to generate paraphrases via pivoting (Prakash et al., 2016; Mallinson et al., 2017). One can also use neural MT systems to generate large monolingual paraphrase corpora. Another line of work has translated the Czech side of a Czech-English parallel corpus into English, thus producing 50 million words of English paraphrase data (Wieting and Gimpel, 2018). Not only can the system generate interesting paraphrases, but embeddings trained on the resulting data set prove useful in sentence similarity tasks. When added to a paraphrase system, constraints obtained from a semantic parser can reduce the semantic drift encountered during rewrites (Wang et al., 2018). Adding lexical con"
D19-5503,D18-1050,0,0.135962,"oaches and machine translation systems on gathering paraphrase quality. 3 Methodology To run a comprehensive evaluation of paraphrase techniques, we create many paraphrases of a common data set using multiple methods, then evaluate using human direct assessment as well as automatic diversity measurements. 3.1 Data Input data was sampled from two sources: Reddit provides volumes of casual online conversations; the Enron email corpus represents communication in the professional world.2 Both are noisier than usual NMT training data; traditionally, such noise has been challenging for NMT systems (Michel and Neubig, 2018) and should provide a lowerbound on their performance. It would definitely be valuable, albeit expensive, to rerun our experiments on a cleaner data source. Related Work Translation as a means of generating paraphrases has been explored for decades. Paraphrase corpora can be extracted from multiple translations of the same source material (Barzilay and McKeown, 2001). Sub-sentential paraphrases (mostly 2 However, the Enron emails often contain conversations about casual and personal matters. 18 Tokens per segment Tokens per segment Segments Types Tokens median mean min max 500 2,370 9,835 19 1"
D19-5503,P02-1040,0,0.110105,"rpora, including monolingual methods with experts and nonexperts as well as automated, semi-automated, and manual translation-based approaches. Each paraphrasing method is evaluated for fluency (“does the resulting paraphrase sound not only grammatical but natural?”) and adequacy (“does the paraphrase accurately convey the original meaning of the source?”) using human direct assessment, inspired by effective techniques in machine translation evaluation (Federmann, 2018). In addition, we measure the degree of change between the original and rewritten sentence using both edit distance and BLEU (Papineni et al., 2002). Somewhat surprisingly, fully automatic neural machine translation actually outperforms manual human translation in terms of adequacy. The semi-automatic method of post-editing neural machine translation output with human editors leads to fluency improvements while retaining diversity and adequacy. Although none of the translationbased approaches outperform monolingual rewrites in terms of adequacy or fluency, they do produce greater diversity. Human editors, particularly nonexperts, tend toward small edits rather than substantial rewrites. We conclude that round-tripping with neural machine"
D19-5503,P15-2070,0,0.0605049,"Missing"
D19-5503,C16-1275,0,0.0540999,"sments for adequacy (semantic equivalence) and fluency of paraphrases, as well as translation adequacy assessments. Data is publicly available at https://aka.ms/MultilingualWhispers. 2 phrasal replacements) can be gathered from these multiple translations. Alternatively, one can create a large body of phrasal replacements from by pivoting on the phrase-tables used by phrase-based statistical machine translation (Bannard and CallisonBurch, 2005; Ganitkevitch et al., 2013; Pavlick et al., 2015). Recent work has also explored using neural machine translation to generate paraphrases via pivoting (Prakash et al., 2016; Mallinson et al., 2017). One can also use neural MT systems to generate large monolingual paraphrase corpora. Another line of work has translated the Czech side of a Czech-English parallel corpus into English, thus producing 50 million words of English paraphrase data (Wieting and Gimpel, 2018). Not only can the system generate interesting paraphrases, but embeddings trained on the resulting data set prove useful in sentence similarity tasks. When added to a paraphrase system, constraints obtained from a semantic parser can reduce the semantic drift encountered during rewrites (Wang et al.,"
D19-5503,P18-1042,0,0.247489,"lternative is to provide input in a foreign language. Nearly anything expressible in one human language can be written in another language. When users translate content, some variation in lexical realization occurs. To gather monolingual paraphrases, we can first translate a source sentence into a variety of target languages, then translate back into the source language, using either humans or machines. This provides naturalistic variation in language, centered around a common yet relatively unconstrained starting point. Although several research threads have explored this possibility (e.g., (Wieting and Gimpel, 2018)), we have seen few if any comparative evaluations of the quality of this approach. Introduction Humans naturally paraphrase. These paraphrases are often a byproduct: when we can’t recall the exact words, we can often generate approximately the same meaning with a different surface realization. Recognizing and generating paraphrases are key challenges in many tasks, including translation, information retrieval, question answering, and semantic parsing. Large collections of sentential paraphrase corpora could benefit such systems.1 Yet when we ask humans to generate paraphrases of a given task,"
E17-1110,W09-1402,0,0.0275933,"Missing"
E17-1110,P09-1068,0,0.0137798,"raction along multiple paths leads to more robust extraction, allowing the learner to find structural patterns even when the language varies or the parser makes an error. The cross-sentence scenario presents a new challenge in candidate selection. This motivates our concept of minimal-span candidates in Section 3.2. Excluding non-minimal candidates substantially improves classification accuracy. There is a long line of research on discourse phenomena, including coreference (Haghighi and Klein, 2007; Poon and Domingos, 2008; Rahman and Ng, 2009; Raghunathan et al., 2010), narrative structures (Chambers and Jurafsky, 2009; Cheung et al., 2013), and rhetorical relations (Marcu, 2000). For the most part, this work has not been connected to relation extraction. Our proposed extraction framework makes it easy to integrate such discourse relations. Our experiments evaluated the impact of coreference and discourse parsing, a preliminary step toward in-depth integration with discourse research. We conducted experiments on extracting druggene interactions from biomedical literature, an important task for precision medicine. By bootstrapping from a recently curated knowledge base (KB) with about 162 known interactions,"
E17-1110,N13-1104,1,0.798974,"leads to more robust extraction, allowing the learner to find structural patterns even when the language varies or the parser makes an error. The cross-sentence scenario presents a new challenge in candidate selection. This motivates our concept of minimal-span candidates in Section 3.2. Excluding non-minimal candidates substantially improves classification accuracy. There is a long line of research on discourse phenomena, including coreference (Haghighi and Klein, 2007; Poon and Domingos, 2008; Rahman and Ng, 2009; Raghunathan et al., 2010), narrative structures (Chambers and Jurafsky, 2009; Cheung et al., 2013), and rhetorical relations (Marcu, 2000). For the most part, this work has not been connected to relation extraction. Our proposed extraction framework makes it easy to integrate such discourse relations. Our experiments evaluated the impact of coreference and discourse parsing, a preliminary step toward in-depth integration with discourse research. We conducted experiments on extracting druggene interactions from biomedical literature, an important task for precision medicine. By bootstrapping from a recently curated knowledge base (KB) with about 162 known interactions, our DISCREX system le"
E17-1110,P15-1136,0,0.0315463,"Missing"
E17-1110,P11-1144,0,0.0268645,"interesting future work. Recently, there has been increasing interest in relation extraction for biomedical applications (Kim et al., 2009; N´edellec et al., 2013). However, past methods are generally limited to single sentences, whether using supervised learning (Bj¨orne et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011) or distant supervision (Poon et al., 2015; Parikh et al., 2015). The idea of leveraging graph representations has been explored in many other settings, such as knowledge base completion (Lao et al., 2011; Gardner and Mitchell, 2015), frame-semantic parsing (Das and Smith, 2011), and other NLP tasks (Radev and Mihalcea, 2008; Subramanya et al., 2010). Linear and dependency paths are popular features for relation extraction (Snow et al., 2006; Mintz et al., 2009). However, past extraction focuses on single sentences, and typically considers the shortest path only. In contrast, we allow interleaving edges from dependency and word adjacency, and consider top K paths rather than just the shortest one. This resulted in substantial accuracy gain (Section 4.5). There has been prior work on leveraging coreference in relation extraction, often in the standard supervised setti"
E17-1110,P11-1055,0,0.138892,"ce and technology, where crosssentence extraction is more likely to have a significant impact. The long-tailed characteristics of such domains also make distant supervision a natural choice for scaling up learning. This paper represents a first step toward exploring the confluence of these two directions. Distant supervision has been extended to capture implicit reasoning, via matrix factorization or knowledge base embedding (Riedel et al., 2013; Toutanova et al., 2015; Toutanova et al., 2016). Additionally, various models have been proposed to address the noise in distant supervision labels (Hoffmann et al., 2011; Surdeanu et al., 2012). These directions are orthogonal to cross-sentence extraction, and incorporating them will be interesting future work. Recently, there has been increasing interest in relation extraction for biomedical applications (Kim et al., 2009; N´edellec et al., 2013). However, past methods are generally limited to single sentences, whether using supervised learning (Bj¨orne et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011) or distant supervision (Poon et al., 2015; Parikh et al., 2015). The idea of leveraging graph representations has been explored in many oth"
E17-1110,P14-1002,0,0.0127341,"du/collaborations/past-projects/ace and Klein, 2014), but also in distant supervision (Koch et al., 2014; Augenstein et al., 2016). Notably, while Koch et al. (2014) and Augenstein et al. (2016) still learned to extract from single sentences, they augmented mentions with coreferent expressions to include linked entities that might be in a different sentence. We explored the potential of this approach in our experiments, but found that it had little impact in our domain, as it produced few additional candidates beyond single sentences. Recently, discourse parsing has received renewed interest (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Surdeanu et al., 2015), and discourse information has been shown to improve performance in applications such as question answering (Sharp et al., 2015). In this paper, we generated coreference relations using the state-ofthe-art Stanford coreference systems (Lee et al., 2011; Recasens et al., 2013; Clark and Manning, 2015), and generated rhetorical relations using the winning approach (Wang and Lan, 2015) in the CoNLL-2015 Shared Task on Discourse Parsing. 3 Distant Supervision for Cross-Sentence Relation Extraction In this section, we present DISCREX, short for DIstant"
E17-1110,de-marneffe-etal-2006-generating,0,0.0454393,"Missing"
E17-1110,W09-1401,0,0.0565103,"ring the confluence of these two directions. Distant supervision has been extended to capture implicit reasoning, via matrix factorization or knowledge base embedding (Riedel et al., 2013; Toutanova et al., 2015; Toutanova et al., 2016). Additionally, various models have been proposed to address the noise in distant supervision labels (Hoffmann et al., 2011; Surdeanu et al., 2012). These directions are orthogonal to cross-sentence extraction, and incorporating them will be interesting future work. Recently, there has been increasing interest in relation extraction for biomedical applications (Kim et al., 2009; N´edellec et al., 2013). However, past methods are generally limited to single sentences, whether using supervised learning (Bj¨orne et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011) or distant supervision (Poon et al., 2015; Parikh et al., 2015). The idea of leveraging graph representations has been explored in many other settings, such as knowledge base completion (Lao et al., 2011; Gardner and Mitchell, 2015), frame-semantic parsing (Das and Smith, 2011), and other NLP tasks (Radev and Mihalcea, 2008; Subramanya et al., 2010). Linear and dependency paths are popular fea"
E17-1110,D14-1203,0,0.267669,"supervision has also been applied to completing Wikipedia Infoboxes (Wu and Weld, 2007) or TAC KBP Slot Filling3 , where the goal is to extract attributes for a given entity, which could be considered a special kind of relation triples (attribute, entity, value). These scenarios are very different from general cross-sentence relation extraction. For example, the entity in consideration is often the protagonist in the document (title entity of the article). Moreover, state-of-the-art methods typically consider extracting from single sentences only (Surdeanu et al., 2012; Surdeanu and Ji, 2014; Koch et al., 2014). In general, cross-sentence relation extraction has received little attention, even in the supervised-learning setting. Among the limited amount of prior work, Swampillai & Stevenson (2011) is the most relevant to our approach, as it also considered syntactic features and introduced a dependency link between the root nodes of parse trees containing the given pair of entities. However, the differences are substantial. First and foremost, their approach used standard supervised learning rather than distant supervision. Moreover, we introduced the document-level graph representation, which is mu"
E17-1110,Q14-1037,0,0.0302229,"Missing"
E17-1110,P14-1048,0,0.0175143,"ojects/ace and Klein, 2014), but also in distant supervision (Koch et al., 2014; Augenstein et al., 2016). Notably, while Koch et al. (2014) and Augenstein et al. (2016) still learned to extract from single sentences, they augmented mentions with coreferent expressions to include linked entities that might be in a different sentence. We explored the potential of this approach in our experiments, but found that it had little impact in our domain, as it produced few additional candidates beyond single sentences. Recently, discourse parsing has received renewed interest (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Surdeanu et al., 2015), and discourse information has been shown to improve performance in applications such as question answering (Sharp et al., 2015). In this paper, we generated coreference relations using the state-ofthe-art Stanford coreference systems (Lee et al., 2011; Recasens et al., 2013; Clark and Manning, 2015), and generated rhetorical relations using the winning approach (Wang and Lan, 2015) in the CoNLL-2015 Shared Task on Discourse Parsing. 3 Distant Supervision for Cross-Sentence Relation Extraction In this section, we present DISCREX, short for DIstant Supervision for Cross"
E17-1110,D15-1173,0,0.00629385,"-sentence extraction, and incorporating them will be interesting future work. Recently, there has been increasing interest in relation extraction for biomedical applications (Kim et al., 2009; N´edellec et al., 2013). However, past methods are generally limited to single sentences, whether using supervised learning (Bj¨orne et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011) or distant supervision (Poon et al., 2015; Parikh et al., 2015). The idea of leveraging graph representations has been explored in many other settings, such as knowledge base completion (Lao et al., 2011; Gardner and Mitchell, 2015), frame-semantic parsing (Das and Smith, 2011), and other NLP tasks (Radev and Mihalcea, 2008; Subramanya et al., 2010). Linear and dependency paths are popular features for relation extraction (Snow et al., 2006; Mintz et al., 2009). However, past extraction focuses on single sentences, and typically considers the shortest path only. In contrast, we allow interleaving edges from dependency and word adjacency, and consider top K paths rather than just the shortest one. This resulted in substantial accuracy gain (Section 4.5). There has been prior work on leveraging coreference in relation extr"
E17-1110,D11-1049,0,0.0270035,"Missing"
E17-1110,W11-1902,0,0.0139227,"linked entities that might be in a different sentence. We explored the potential of this approach in our experiments, but found that it had little impact in our domain, as it produced few additional candidates beyond single sentences. Recently, discourse parsing has received renewed interest (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Surdeanu et al., 2015), and discourse information has been shown to improve performance in applications such as question answering (Sharp et al., 2015). In this paper, we generated coreference relations using the state-ofthe-art Stanford coreference systems (Lee et al., 2011; Recasens et al., 2013; Clark and Manning, 2015), and generated rhetorical relations using the winning approach (Wang and Lan, 2015) in the CoNLL-2015 Shared Task on Discourse Parsing. 3 Distant Supervision for Cross-Sentence Relation Extraction In this section, we present DISCREX, short for DIstant Supervision for Cross-sentence Relation EXraction. Similar to conventional approaches, DISCREX learns a classifier to predict the relation between two entities, given text spans where the entities co-occur. Unlike most existing methods, however, DISCREX allows text spans comprising multiple senten"
E17-1110,P07-1107,0,0.0246146,"we augment this graph with new arcs, the number of possible paths between entities grow. We demonstrate that feature extraction along multiple paths leads to more robust extraction, allowing the learner to find structural patterns even when the language varies or the parser makes an error. The cross-sentence scenario presents a new challenge in candidate selection. This motivates our concept of minimal-span candidates in Section 3.2. Excluding non-minimal candidates substantially improves classification accuracy. There is a long line of research on discourse phenomena, including coreference (Haghighi and Klein, 2007; Poon and Domingos, 2008; Rahman and Ng, 2009; Raghunathan et al., 2010), narrative structures (Chambers and Jurafsky, 2009; Cheung et al., 2013), and rhetorical relations (Marcu, 2000). For the most part, this work has not been connected to relation extraction. Our proposed extraction framework makes it easy to integrate such discourse relations. Our experiments evaluated the impact of coreference and discourse parsing, a preliminary step toward in-depth integration with discourse research. We conducted experiments on extracting druggene interactions from biomedical literature, an important"
E17-1110,P14-5010,0,0.00203336,"row specifies a gene, some drugs, the fine-grained relations (e.g., sensitive), the gene status (e.g., mutation), and some supporting article IDs. In this paper, we only consider the coarse drug-gene association and ignore the other fields. 4.2 Unlabeled Text We obtained biomedical literature from PubMed Central7 , which as of early 2015 contained about 960,000 full-text articles. We preprocessed the text using SPLAT (Quirk et al., 2012) to conduct tokenization, part-of-speech tagging, and syntactic parsing, and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP (Manning et al., 2014). We used the entity taggers from Literome (Poon et al., 2014) to identify drug and gene mentions. 4.3 Candidate Selection To avoid unlikely candidates such as entity pairs far apart in the document, we consider entity pairs within K consecutive sentences. K = 1 corresponds to extraction within single sentences. For cross-sentence extraction, we chose K = 3 as it 7 http://www.ncbi.nlm.nih.gov/pmc/ Unique Pairs Instances Matching GDKD K=1 K=3 169,168 1,724,119 58,523 332,969 3,913,338 87,773 Table 1: Statistics for drug-gene interaction candidates in PubMed Central articles: unique pairs, insta"
E17-1110,D13-1029,0,0.0143254,"d other NLP tasks (Radev and Mihalcea, 2008; Subramanya et al., 2010). Linear and dependency paths are popular features for relation extraction (Snow et al., 2006; Mintz et al., 2009). However, past extraction focuses on single sentences, and typically considers the shortest path only. In contrast, we allow interleaving edges from dependency and word adjacency, and consider top K paths rather than just the shortest one. This resulted in substantial accuracy gain (Section 4.5). There has been prior work on leveraging coreference in relation extraction, often in the standard supervised setting (Hajishirzi et al., 2013; Durrett 5 E.g., MUC6, ACE https://www.ldc.upenn. edu/collaborations/past-projects/ace and Klein, 2014), but also in distant supervision (Koch et al., 2014; Augenstein et al., 2016). Notably, while Koch et al. (2014) and Augenstein et al. (2016) still learned to extract from single sentences, they augmented mentions with coreferent expressions to include linked entities that might be in a different sentence. We explored the potential of this approach in our experiments, but found that it had little impact in our domain, as it produced few additional candidates beyond single sentences. Recentl"
E17-1110,P09-1113,0,0.951866,"g individualized diagnosis and treatment of complex genetic diseases such as cancer. The availability of measurement for 20,000 human genes makes it imperative to integrate all knowledge about them, which grows rapidly and is scattered in millions of articles in PubMed2 . Traditional extraction approaches require annotated examples, which makes it difficult to scale to the explosion of extraction demands. Consequently, there has been increasing interest in indirect supervision (Banko et al., 2007; Poon and Domingos, 2009; Toutanova et al., 2015), with distant supervision (Craven et al., 1998; Mintz et al., 2009) emerging as a particularly promising paradigm for augmenting existing knowledge bases from unlabeled text (Poon et al., 2015; Parikh et al., 2015). This progress is exciting, but distantsupervision approaches have so far been limited to single sentences, thus missing out on relations crossing the sentence boundary. Consider the following example:“The p56Lck inhibitor Dasatinib was The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations ex"
E17-1110,W13-2001,0,0.0620623,"Missing"
E17-1110,N15-1077,1,0.936467,"it imperative to integrate all knowledge about them, which grows rapidly and is scattered in millions of articles in PubMed2 . Traditional extraction approaches require annotated examples, which makes it difficult to scale to the explosion of extraction demands. Consequently, there has been increasing interest in indirect supervision (Banko et al., 2007; Poon and Domingos, 2009; Toutanova et al., 2015), with distant supervision (Craven et al., 1998; Mintz et al., 2009) emerging as a particularly promising paradigm for augmenting existing knowledge bases from unlabeled text (Poon et al., 2015; Parikh et al., 2015). This progress is exciting, but distantsupervision approaches have so far been limited to single sentences, thus missing out on relations crossing the sentence boundary. Consider the following example:“The p56Lck inhibitor Dasatinib was The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this pap"
E17-1110,D08-1068,1,0.655087,"h new arcs, the number of possible paths between entities grow. We demonstrate that feature extraction along multiple paths leads to more robust extraction, allowing the learner to find structural patterns even when the language varies or the parser makes an error. The cross-sentence scenario presents a new challenge in candidate selection. This motivates our concept of minimal-span candidates in Section 3.2. Excluding non-minimal candidates substantially improves classification accuracy. There is a long line of research on discourse phenomena, including coreference (Haghighi and Klein, 2007; Poon and Domingos, 2008; Rahman and Ng, 2009; Raghunathan et al., 2010), narrative structures (Chambers and Jurafsky, 2009; Cheung et al., 2013), and rhetorical relations (Marcu, 2000). For the most part, this work has not been connected to relation extraction. Our proposed extraction framework makes it easy to integrate such discourse relations. Our experiments evaluated the impact of coreference and discourse parsing, a preliminary step toward in-depth integration with discourse research. We conducted experiments on extracting druggene interactions from biomedical literature, an important task for precision medici"
E17-1110,D09-1001,1,0.468098,"icine (Bahcall, 2015). The cost of sequencing a person’s genome has fallen below $10001 , enabling individualized diagnosis and treatment of complex genetic diseases such as cancer. The availability of measurement for 20,000 human genes makes it imperative to integrate all knowledge about them, which grows rapidly and is scattered in millions of articles in PubMed2 . Traditional extraction approaches require annotated examples, which makes it difficult to scale to the explosion of extraction demands. Consequently, there has been increasing interest in indirect supervision (Banko et al., 2007; Poon and Domingos, 2009; Toutanova et al., 2015), with distant supervision (Craven et al., 1998; Mintz et al., 2009) emerging as a particularly promising paradigm for augmenting existing knowledge bases from unlabeled text (Poon et al., 2015; Parikh et al., 2015). This progress is exciting, but distantsupervision approaches have so far been limited to single sentences, thus missing out on relations crossing the sentence boundary. Consider the following example:“The p56Lck inhibitor Dasatinib was The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with lim"
E17-1110,N10-1123,1,0.250969,"g (Riedel et al., 2013; Toutanova et al., 2015; Toutanova et al., 2016). Additionally, various models have been proposed to address the noise in distant supervision labels (Hoffmann et al., 2011; Surdeanu et al., 2012). These directions are orthogonal to cross-sentence extraction, and incorporating them will be interesting future work. Recently, there has been increasing interest in relation extraction for biomedical applications (Kim et al., 2009; N´edellec et al., 2013). However, past methods are generally limited to single sentences, whether using supervised learning (Bj¨orne et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011) or distant supervision (Poon et al., 2015; Parikh et al., 2015). The idea of leveraging graph representations has been explored in many other settings, such as knowledge base completion (Lao et al., 2011; Gardner and Mitchell, 2015), frame-semantic parsing (Das and Smith, 2011), and other NLP tasks (Radev and Mihalcea, 2008; Subramanya et al., 2010). Linear and dependency paths are popular features for relation extraction (Snow et al., 2006; Mintz et al., 2009). However, past extraction focuses on single sentences, and typically considers the shortest path only. In"
E17-1110,D10-1048,0,0.0180862,"en entities grow. We demonstrate that feature extraction along multiple paths leads to more robust extraction, allowing the learner to find structural patterns even when the language varies or the parser makes an error. The cross-sentence scenario presents a new challenge in candidate selection. This motivates our concept of minimal-span candidates in Section 3.2. Excluding non-minimal candidates substantially improves classification accuracy. There is a long line of research on discourse phenomena, including coreference (Haghighi and Klein, 2007; Poon and Domingos, 2008; Rahman and Ng, 2009; Raghunathan et al., 2010), narrative structures (Chambers and Jurafsky, 2009; Cheung et al., 2013), and rhetorical relations (Marcu, 2000). For the most part, this work has not been connected to relation extraction. Our proposed extraction framework makes it easy to integrate such discourse relations. Our experiments evaluated the impact of coreference and discourse parsing, a preliminary step toward in-depth integration with discourse research. We conducted experiments on extracting druggene interactions from biomedical literature, an important task for precision medicine. By bootstrapping from a recently curated kno"
E17-1110,D09-1101,0,0.0477744,"possible paths between entities grow. We demonstrate that feature extraction along multiple paths leads to more robust extraction, allowing the learner to find structural patterns even when the language varies or the parser makes an error. The cross-sentence scenario presents a new challenge in candidate selection. This motivates our concept of minimal-span candidates in Section 3.2. Excluding non-minimal candidates substantially improves classification accuracy. There is a long line of research on discourse phenomena, including coreference (Haghighi and Klein, 2007; Poon and Domingos, 2008; Rahman and Ng, 2009; Raghunathan et al., 2010), narrative structures (Chambers and Jurafsky, 2009; Cheung et al., 2013), and rhetorical relations (Marcu, 2000). For the most part, this work has not been connected to relation extraction. Our proposed extraction framework makes it easy to integrate such discourse relations. Our experiments evaluated the impact of coreference and discourse parsing, a preliminary step toward in-depth integration with discourse research. We conducted experiments on extracting druggene interactions from biomedical literature, an important task for precision medicine. By bootstrapping"
E17-1110,N13-1071,0,0.0226704,"Missing"
E17-1110,D11-1001,0,0.011871,"anova et al., 2015; Toutanova et al., 2016). Additionally, various models have been proposed to address the noise in distant supervision labels (Hoffmann et al., 2011; Surdeanu et al., 2012). These directions are orthogonal to cross-sentence extraction, and incorporating them will be interesting future work. Recently, there has been increasing interest in relation extraction for biomedical applications (Kim et al., 2009; N´edellec et al., 2013). However, past methods are generally limited to single sentences, whether using supervised learning (Bj¨orne et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011) or distant supervision (Poon et al., 2015; Parikh et al., 2015). The idea of leveraging graph representations has been explored in many other settings, such as knowledge base completion (Lao et al., 2011; Gardner and Mitchell, 2015), frame-semantic parsing (Das and Smith, 2011), and other NLP tasks (Radev and Mihalcea, 2008; Subramanya et al., 2010). Linear and dependency paths are popular features for relation extraction (Snow et al., 2006; Mintz et al., 2009). However, past extraction focuses on single sentences, and typically considers the shortest path only. In contrast, we allow interlea"
E17-1110,N13-1008,0,0.0607713,"re often exist single sentences expressing the relation (Banko et al., 2007). However, there is much less redundancy in specialized domains such as the frontiers of science and technology, where crosssentence extraction is more likely to have a significant impact. The long-tailed characteristics of such domains also make distant supervision a natural choice for scaling up learning. This paper represents a first step toward exploring the confluence of these two directions. Distant supervision has been extended to capture implicit reasoning, via matrix factorization or knowledge base embedding (Riedel et al., 2013; Toutanova et al., 2015; Toutanova et al., 2016). Additionally, various models have been proposed to address the noise in distant supervision labels (Hoffmann et al., 2011; Surdeanu et al., 2012). These directions are orthogonal to cross-sentence extraction, and incorporating them will be interesting future work. Recently, there has been increasing interest in relation extraction for biomedical applications (Kim et al., 2009; N´edellec et al., 2013). However, past methods are generally limited to single sentences, whether using supervised learning (Bj¨orne et al., 2009; Poon and Vanderwende,"
E17-1110,N15-1025,0,0.0145465,"n et al. (2016) still learned to extract from single sentences, they augmented mentions with coreferent expressions to include linked entities that might be in a different sentence. We explored the potential of this approach in our experiments, but found that it had little impact in our domain, as it produced few additional candidates beyond single sentences. Recently, discourse parsing has received renewed interest (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Surdeanu et al., 2015), and discourse information has been shown to improve performance in applications such as question answering (Sharp et al., 2015). In this paper, we generated coreference relations using the state-ofthe-art Stanford coreference systems (Lee et al., 2011; Recasens et al., 2013; Clark and Manning, 2015), and generated rhetorical relations using the winning approach (Wang and Lan, 2015) in the CoNLL-2015 Shared Task on Discourse Parsing. 3 Distant Supervision for Cross-Sentence Relation Extraction In this section, we present DISCREX, short for DIstant Supervision for Cross-sentence Relation EXraction. Similar to conventional approaches, DISCREX learns a classifier to predict the relation between two entities, given text sp"
E17-1110,N12-3006,1,0.816291,"gene status. 4.1 Knowledge Base We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) for distant supervision. Figure 2 shows a snapshot of the dataset. Each row specifies a gene, some drugs, the fine-grained relations (e.g., sensitive), the gene status (e.g., mutation), and some supporting article IDs. In this paper, we only consider the coarse drug-gene association and ignore the other fields. 4.2 Unlabeled Text We obtained biomedical literature from PubMed Central7 , which as of early 2015 contained about 960,000 full-text articles. We preprocessed the text using SPLAT (Quirk et al., 2012) to conduct tokenization, part-of-speech tagging, and syntactic parsing, and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP (Manning et al., 2014). We used the entity taggers from Literome (Poon et al., 2014) to identify drug and gene mentions. 4.3 Candidate Selection To avoid unlikely candidates such as entity pairs far apart in the document, we consider entity pairs within K consecutive sentences. K = 1 corresponds to extraction within single sentences. For cross-sentence extraction, we chose K = 3 as it 7 http://www.ncbi.nlm.nih.gov/pmc/ Unique Pairs Instan"
E17-1110,D10-1017,0,0.00985467,"in relation extraction for biomedical applications (Kim et al., 2009; N´edellec et al., 2013). However, past methods are generally limited to single sentences, whether using supervised learning (Bj¨orne et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011) or distant supervision (Poon et al., 2015; Parikh et al., 2015). The idea of leveraging graph representations has been explored in many other settings, such as knowledge base completion (Lao et al., 2011; Gardner and Mitchell, 2015), frame-semantic parsing (Das and Smith, 2011), and other NLP tasks (Radev and Mihalcea, 2008; Subramanya et al., 2010). Linear and dependency paths are popular features for relation extraction (Snow et al., 2006; Mintz et al., 2009). However, past extraction focuses on single sentences, and typically considers the shortest path only. In contrast, we allow interleaving edges from dependency and word adjacency, and consider top K paths rather than just the shortest one. This resulted in substantial accuracy gain (Section 4.5). There has been prior work on leveraging coreference in relation extraction, often in the standard supervised setting (Hajishirzi et al., 2013; Durrett 5 E.g., MUC6, ACE https://www.ldc.up"
E17-1110,D12-1042,0,0.450628,"not extract cross-sentence relations. Distant supervision has also been applied to completing Wikipedia Infoboxes (Wu and Weld, 2007) or TAC KBP Slot Filling3 , where the goal is to extract attributes for a given entity, which could be considered a special kind of relation triples (attribute, entity, value). These scenarios are very different from general cross-sentence relation extraction. For example, the entity in consideration is often the protagonist in the document (title entity of the article). Moreover, state-of-the-art methods typically consider extracting from single sentences only (Surdeanu et al., 2012; Surdeanu and Ji, 2014; Koch et al., 2014). In general, cross-sentence relation extraction has received little attention, even in the supervised-learning setting. Among the limited amount of prior work, Swampillai & Stevenson (2011) is the most relevant to our approach, as it also considered syntactic features and introduced a dependency link between the root nodes of parse trees containing the given pair of entities. However, the differences are substantial. First and foremost, their approach used standard supervised learning rather than distant supervision. Moreover, we introduced the docum"
E17-1110,N15-3001,0,0.0118903,"2014), but also in distant supervision (Koch et al., 2014; Augenstein et al., 2016). Notably, while Koch et al. (2014) and Augenstein et al. (2016) still learned to extract from single sentences, they augmented mentions with coreferent expressions to include linked entities that might be in a different sentence. We explored the potential of this approach in our experiments, but found that it had little impact in our domain, as it produced few additional candidates beyond single sentences. Recently, discourse parsing has received renewed interest (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Surdeanu et al., 2015), and discourse information has been shown to improve performance in applications such as question answering (Sharp et al., 2015). In this paper, we generated coreference relations using the state-ofthe-art Stanford coreference systems (Lee et al., 2011; Recasens et al., 2013; Clark and Manning, 2015), and generated rhetorical relations using the winning approach (Wang and Lan, 2015) in the CoNLL-2015 Shared Task on Discourse Parsing. 3 Distant Supervision for Cross-Sentence Relation Extraction In this section, we present DISCREX, short for DIstant Supervision for Cross-sentence Relation EXrac"
E17-1110,R11-1004,0,0.55628,"h could be considered a special kind of relation triples (attribute, entity, value). These scenarios are very different from general cross-sentence relation extraction. For example, the entity in consideration is often the protagonist in the document (title entity of the article). Moreover, state-of-the-art methods typically consider extracting from single sentences only (Surdeanu et al., 2012; Surdeanu and Ji, 2014; Koch et al., 2014). In general, cross-sentence relation extraction has received little attention, even in the supervised-learning setting. Among the limited amount of prior work, Swampillai & Stevenson (2011) is the most relevant to our approach, as it also considered syntactic features and introduced a dependency link between the root nodes of parse trees containing the given pair of entities. However, the differences are substantial. First and foremost, their approach used standard supervised learning rather than distant supervision. Moreover, we introduced the document-level graph representation, which is much more general, capable of incorporating a diverse set of discourse relations and enabling the use of rich syntactic and surface features (Section 3). Finally, Swampillai & Stevenson (2011)"
E17-1110,D15-1174,1,0.272265,"e cost of sequencing a person’s genome has fallen below $10001 , enabling individualized diagnosis and treatment of complex genetic diseases such as cancer. The availability of measurement for 20,000 human genes makes it imperative to integrate all knowledge about them, which grows rapidly and is scattered in millions of articles in PubMed2 . Traditional extraction approaches require annotated examples, which makes it difficult to scale to the explosion of extraction demands. Consequently, there has been increasing interest in indirect supervision (Banko et al., 2007; Poon and Domingos, 2009; Toutanova et al., 2015), with distant supervision (Craven et al., 1998; Mintz et al., 2009) emerging as a particularly promising paradigm for augmenting existing knowledge bases from unlabeled text (Poon et al., 2015; Parikh et al., 2015). This progress is exciting, but distantsupervision approaches have so far been limited to single sentences, thus missing out on relations crossing the sentence boundary. Consider the following example:“The p56Lck inhibitor Dasatinib was The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However"
E17-1110,P16-1136,1,0.33163,"e relation (Banko et al., 2007). However, there is much less redundancy in specialized domains such as the frontiers of science and technology, where crosssentence extraction is more likely to have a significant impact. The long-tailed characteristics of such domains also make distant supervision a natural choice for scaling up learning. This paper represents a first step toward exploring the confluence of these two directions. Distant supervision has been extended to capture implicit reasoning, via matrix factorization or knowledge base embedding (Riedel et al., 2013; Toutanova et al., 2015; Toutanova et al., 2016). Additionally, various models have been proposed to address the noise in distant supervision labels (Hoffmann et al., 2011; Surdeanu et al., 2012). These directions are orthogonal to cross-sentence extraction, and incorporating them will be interesting future work. Recently, there has been increasing interest in relation extraction for biomedical applications (Kim et al., 2009; N´edellec et al., 2013). However, past methods are generally limited to single sentences, whether using supervised learning (Bj¨orne et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011) or distant super"
E17-1110,K15-2002,0,0.131491,"at it had little impact in our domain, as it produced few additional candidates beyond single sentences. Recently, discourse parsing has received renewed interest (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Surdeanu et al., 2015), and discourse information has been shown to improve performance in applications such as question answering (Sharp et al., 2015). In this paper, we generated coreference relations using the state-ofthe-art Stanford coreference systems (Lee et al., 2011; Recasens et al., 2013; Clark and Manning, 2015), and generated rhetorical relations using the winning approach (Wang and Lan, 2015) in the CoNLL-2015 Shared Task on Discourse Parsing. 3 Distant Supervision for Cross-Sentence Relation Extraction In this section, we present DISCREX, short for DIstant Supervision for Cross-sentence Relation EXraction. Similar to conventional approaches, DISCREX learns a classifier to predict the relation between two entities, given text spans where the entities co-occur. Unlike most existing methods, however, DISCREX allows text spans comprising multiple sentences and explores potentially many paths between these entities. 3.1 Distant Supervision Like prior approaches, DISCREX learns from an"
J12-2012,D10-1001,0,0.0272041,"ed on an example in sequence tagging. The broad coverage, multi-viewpoint discussion encourages the reader to make connections between many distinct approaches, and provides solid formalism for reasoning about decoding problems. It is a comprehensive introduction to the most common and effective decoding approaches, with one signiﬁcant exception: the recent advances in dual decomposition and Lagrangian relaxation methods. Timing is likely the culprit. This book was developed mainly from 2006 to 2009, whereas dual decomposition did not attain notoriety in our community until a few years later (Rush et al. 2010). Relaxation approaches, though potentially a passing phase, have successfully broadened the reach of simpler decoding techniques into more complicated domains such as structured event extraction. They would have made a nice addition. Regardless, this second chapter equips the reader with sufﬁcient machinery to solve a number of structured prediction problems. Chapter 3 applies the machinery described in the prior chapter to the problem of supervised structure induction. Probabilistic generative and conditional models are introduced in some detail, followed by a discussion of margin-based meth"
J12-2012,W10-2902,0,0.0222482,"accustomed to more practical descriptions of material. Were I to teach a course based on this book, I would be tempted to present the third chapter before the second. Chapter 4 focuses on semisupervised, unsupervised, and hidden variable learning. With a good mix of theory and practical examples, Expectation-Maximization (EM) is introduced and grounded in several problems, then generalized with log-linear models and approximated with contrastive estimation. Hard EM is mentioned in the context of several examples, though a more detailed description of this potentially important technique (cf. Spitkovsky et al., 2010) would bridge the material of Chapters 2 and 4. The chapter then describes Bayesian approaches to NLP, working from theory into speciﬁc techniques and landing in models. Finally, a brief section is devoted to the related area of hidden variable learning. Chapter 5 begins by describing the partition function, as well as inference techniques for the partition function and decoding methods. I found it strange that this important section was postponed so late in the book; much of the material was forwardreferenced throughout Chapter 4. Regardless, the techniques are described in a unifying, generi"
N06-1002,W05-0823,0,0.417083,"Missing"
N06-1002,J93-2003,0,0.0125862,"ons of Model 4, in both directions. These alignments were combined heuristically as described in our previous work. We then projected the dependency trees and used the aligned dependency tree pairs to extract treelet translation pairs, train the order model, and train MTU models. The target language models were trained using only the target side of the corpus. Finally we trained model weights by maximizing BLEU (Och 2003) and set decoder optimization parameters (n-best list size, timeouts (σ ,τ )∈treelets( A) s∈σ t∈τ We use word probability tables p(t |s) and p(s |t) estimated by IBM Model 1 (Brown et al. 1993). Such models can be built over phrases if used in a phrasal decoder or over treelets if used in a treelet decoder. These models are referred to as set (2). Word-based models A target language model using modified KneserNey smoothing captures fluency; a word count feature offsets the target LM preference for shorter selections; and a treelet/phrase count helps bias toward translations using fewer phrases. These models are referred to as set (3). f targetLM ( S , T , A) = ∏ P(t |parent (t )) |T | ∏ P(ti |tii−−1n ) i =1 f wordcount ( S , T , A) = |T | f phrasecount ( S , T , A) = |treelets( A) |"
N06-1002,P05-1032,0,0.0236016,"esistant to degradation at smaller phrase sizes than the phrase-based system, it nevertheless suffered significantly at very small phrase sizes. Thus it is also subject to practical problems of size, and again these problems are exacerbated since there are potentially an exponential number of treelets. 2. Previous work 2.1. Delayed phrase construction To avoid the major practical problem of phrasal SMT—namely large phrase tables, most of which are not useful to any one sentence—one can instead construct phrase tables on the fly using an indexed form of the training data (Zhang and Vogel 2005; Callison-Burch et al. 2005). However, this does not relieve any of the theoretical problems with phrase-based SMT. 2.2. Syntax-based SMT Two recent systems have attempted to address the contiguity limitation and global re-ordering problem using syntax-based approaches. Hierarchical phrases Recent work in the use of hierarchical phrases (Chiang 2005) improves the ability to capture linguistic generalizations, and also removes the limitation to contiguous phrases. Hierarchical phrases differ from standard phrases in one important way: in addition to lexical items, a phrase pair may contain indexed placeholders, where each"
N06-1002,P05-1033,0,0.661837,"ch as the English password translating as the French mot de passe cannot be easily subdivided. Allowing such translations to be first class entities simplifies translation implementation and improves translation quality. 1. Introduction The last several years have seen phrasal statistical machine translation (SMT) systems outperform word-based approaches by a wide margin (Koehn 2003). Unfortunately the use of phrases in SMT is beset by a number of difficult theoretical and practical problems, which we attempt to characterize below. Recent research into syntaxbased SMT (Quirk and Menezes 2005; Chiang 2005) has produced promising results in addressing some of the problems; research motivated by other statistical models has helped to address others (Banchs et al. 2005). We refine and unify two threads of research in an attempt to address all of these problems simultaneously. Such an approach proves both theoretically more desirable and empirically superior. In brief, Phrasal SMT systems employ phrase pairs automatically extracted from parallel corpora. To translate, a source sentence is first partitioned into a sequence of phrases I = s1…sI. Each source phrase si is then translated into a target"
N06-1002,N03-1017,0,0.0383076,"rases. Neither phrasal SMT nor alignment templates allow discontiguous translation pairs. Global re-ordering Phrases do capture local reordering, but provide no global re-ordering strategy, and the number of possible orderings to be considered is not lessened significantly. Given a sentence of n words, if the average target phrase length is 4 words (which is unusually high), then the reordering space is reduced from n! to only (n/4)!: still impractical for exact search in most sentences. Systems must therefore impose some limits on phrasal reordering, often hard limits based on distance as in Koehn et al. (2003) or some linguistically motivated constraint, such as ITG (Zens and Ney, 2004). Since these phrases are not bound by or even related to syntactic constituents, linguistic generalizations (such as SVO becoming SOV, or prepositions becoming postpositions) are not easily incorporated into the movement models. 1.3. Practical problem with phrases: size In addition to the theoretical problems with phrases, there are also practical issues. While phrasal systems achieve diminishing returns due 1 The Alignment Template approach differs slightly here. Phrasal SMT estimates the probability of a phrase pa"
N06-1002,koen-2004-pharaoh,0,0.224289,"on Another means of extending phrase-based translation is to incorporate source language syntactic information. In Quirk and Menezes (2005) we presented an approach to phrasal SMT based on a parsed dependency tree representation of the source language. We use a source dependency parser and project a target dependency tree using a word-based alignment, after which we extract tree-based phrases (‘treelets’) and train a tree-based ordering model. We showed that using treelets and a tree-based ordering model results in significantly better translations than a leading phrase-based system (Pharaoh, Koehn 2004), keeping all other models identical. Like the hierarchical phrase approach, treelet translation succeeds in improving the global reordering search and allowing discontiguous phrases, but does not solve the partitioning or estimation problems. While we found our treelet system more resistant to degradation at smaller phrase sizes than the phrase-based system, it nevertheless suffered significantly at very small phrase sizes. Thus it is also subject to practical problems of size, and again these problems are exacerbated since there are potentially an exponential number of treelets. 2. Previous"
N06-1002,J03-1002,0,0.00709634,"es; Table 4.1 presents the major characteristics of this data. ∏ f InverseMLE ( S , T , A) = f DirectM1 ( S , T , A) = f InverseM1 ( S , T , A) = c(σ ,τ ) c(*,τ ) (σ ,τ )∈treelets( A) ∏ ∏ ∏ p (t |s ) ∏ ∏ p( s |t ) 4.2. Training (σ ,τ )∈treelets( A) t∈τ s∈σ We parsed the source (English) side of the corpora using NLPWIN, a broad-coverage rulebased parser able to produce syntactic analyses at varying levels of depth (Heidorn 2002). For the purposes of these experiments we used a dependency tree output with part-of-speech tags and unstemmed surface words. Word alignments were produced by GIZA++ (Och and Ney 2003) with a standard training regimen of five iterations of Model 1, five iterations of the HMM Model, and five iterations of Model 4, in both directions. These alignments were combined heuristically as described in our previous work. We then projected the dependency trees and used the aligned dependency tree pairs to extract treelet translation pairs, train the order model, and train MTU models. The target language models were trained using only the target side of the corpus. Finally we trained model weights by maximizing BLEU (Och 2003) and set decoder optimization parameters (n-best list size,"
N06-1002,J04-4002,0,0.198555,"Missing"
N06-1002,P03-1021,0,0.0977507,"ace words. Word alignments were produced by GIZA++ (Och and Ney 2003) with a standard training regimen of five iterations of Model 1, five iterations of the HMM Model, and five iterations of Model 4, in both directions. These alignments were combined heuristically as described in our previous work. We then projected the dependency trees and used the aligned dependency tree pairs to extract treelet translation pairs, train the order model, and train MTU models. The target language models were trained using only the target side of the corpus. Finally we trained model weights by maximizing BLEU (Och 2003) and set decoder optimization parameters (n-best list size, timeouts (σ ,τ )∈treelets( A) s∈σ t∈τ We use word probability tables p(t |s) and p(s |t) estimated by IBM Model 1 (Brown et al. 1993). Such models can be built over phrases if used in a phrasal decoder or over treelets if used in a treelet decoder. These models are referred to as set (2). Word-based models A target language model using modified KneserNey smoothing captures fluency; a word count feature offsets the target LM preference for shorter selections; and a treelet/phrase count helps bias toward translations using fewer phrases"
N06-1002,P02-1040,0,0.105265,"Missing"
N06-1002,P05-1034,1,0.878164,"mmon everyday phrases such as the English password translating as the French mot de passe cannot be easily subdivided. Allowing such translations to be first class entities simplifies translation implementation and improves translation quality. 1. Introduction The last several years have seen phrasal statistical machine translation (SMT) systems outperform word-based approaches by a wide margin (Koehn 2003). Unfortunately the use of phrases in SMT is beset by a number of difficult theoretical and practical problems, which we attempt to characterize below. Recent research into syntaxbased SMT (Quirk and Menezes 2005; Chiang 2005) has produced promising results in addressing some of the problems; research motivated by other statistical models has helped to address others (Banchs et al. 2005). We refine and unify two threads of research in an attempt to address all of these problems simultaneously. Such an approach proves both theoretically more desirable and empirically superior. In brief, Phrasal SMT systems employ phrase pairs automatically extracted from parallel corpora. To translate, a source sentence is first partitioned into a sequence of phrases I = s1…sI. Each source phrase si is then translated"
N06-1002,P03-1019,0,0.0136094,"Missing"
N06-1002,2005.eamt-1.39,0,0.0258086,"treelet system more resistant to degradation at smaller phrase sizes than the phrase-based system, it nevertheless suffered significantly at very small phrase sizes. Thus it is also subject to practical problems of size, and again these problems are exacerbated since there are potentially an exponential number of treelets. 2. Previous work 2.1. Delayed phrase construction To avoid the major practical problem of phrasal SMT—namely large phrase tables, most of which are not useful to any one sentence—one can instead construct phrase tables on the fly using an indexed form of the training data (Zhang and Vogel 2005; Callison-Burch et al. 2005). However, this does not relieve any of the theoretical problems with phrase-based SMT. 2.2. Syntax-based SMT Two recent systems have attempted to address the contiguity limitation and global re-ordering problem using syntax-based approaches. Hierarchical phrases Recent work in the use of hierarchical phrases (Chiang 2005) improves the ability to capture linguistic generalizations, and also removes the limitation to contiguous phrases. Hierarchical phrases differ from standard phrases in one important way: in addition to lexical items, a phrase pair may contain ind"
N06-1002,2003.mtsummit-papers.53,0,\N,Missing
N10-1063,W06-2810,0,0.673272,"Missing"
N10-1063,P06-1009,0,0.00624231,"entence alignment model is able to capture this phenomenon. For both parallel and comparable corpora, global sentence alignments have been used, though the alignments were monotonic (Gale and Church, 1991; Moore, 2002; Zhao and Vogel, 2002). Our model is a first order linear chain Conditional Random Field (CRF) (Lafferty et al., 2001). The set of source and target sentences are observed. For each source sentence, we have a hidden variable indicating the corresponding target sentence to which it is aligned (or null). The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006). 3.3 Features Our features can be grouped into four categories. 3.1 Binary Classifiers and Rankers Much of the previous work involves building a binary classifier for sentence pairs to determine whether or not they are parallel (Munteanu and Marcu, 2005; Tillmann, 2009). The training data usually comes from a standard parallel corpus. There is a substantial class imbalance (O(n) positive examples, and O(n2 ) negative examples), and various heuristics are used to mitigate this problem. Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based"
N10-1063,J93-2003,0,0.0430686,"ine translation system, the size of the parallel corpus used for training is a major factor in its performance. For some language pairs, such as Chinese-English and Arabic-English, large amounts of parallel data are readily available, but for most language pairs this is not the case. The ∗ This research was conducted during the author’s internship at Microsoft Research. Once promising document pairs are identified, the next step is to extract parallel sentences. Usually, some seed parallel data is assumed to be available. This data is used to train a word alignment model, such as IBM Model 1 (Brown et al., 1993) or HMM-based word alignment (Vogel et al., 1996). Statistics from this word alignment model are used to train a classifier which identifies bilingual sentence pairs as parallel or not parallel. This classifier is applied to all sentence pairs in documents which were found to be similar. Typically, some pruning is done to reduce the number of sen403 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 403–411, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tence pairs that need to be classified. While thes"
N10-1063,C04-1151,0,0.608848,"d by users. This is an extremely valuable resource when extracting parallel sentences, as the document alignment is already provided. Table 1 shows how many of these “interwiki” links are present between the English Wikipedia and the 16 largest non-English Wikipedias. Wikipedia’s markup contains other useful indicators for parallel sentence extraction. The many hyperlinks found in articles have previously been used as a valuable source of information. (Adafre and de Rijke, 2006) use matching hyperlinks to identify similar sentences. Two links match if the arti404 Types of Non-Parallel Corpora Fung and Cheung (2004) give a more fine-grained description of the types of non-parallel corpora, which we will briefly summarize. A noisy parallel corpus has documents which contain many parallel sentences in roughly the same order. Comparable corpora contain topic aligned documents which are not translations of each other. The corpora Fung and Cheung (2004) examine are quasi-comparable: they contain bilingual documents which are not necessarily on the same topic. Wikipedia is a special case, since the aligned article pairs may range from being almost completely parallel (e.g., the Spanish and English entries for"
N10-1063,H91-1026,0,0.452331,"llmann (2009). Since our corpus already contains document alignments, we sidestep this problem, and will not discuss further details of this issue. That said, we believe that our methods will be effective in corpora without document alignments when combined with one of the aforementioned algorithms. 3.2 Sequence Models In Wikipedia article pairs, it is common for parallel sentences to occur in clusters. A global sentence alignment model is able to capture this phenomenon. For both parallel and comparable corpora, global sentence alignments have been used, though the alignments were monotonic (Gale and Church, 1991; Moore, 2002; Zhao and Vogel, 2002). Our model is a first order linear chain Conditional Random Field (CRF) (Lafferty et al., 2001). The set of source and target sentences are observed. For each source sentence, we have a hidden variable indicating the corresponding target sentence to which it is aligned (or null). The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006). 3.3 Features Our features can be grouped into four categories. 3.1 Binary Classifiers and Rankers Much of the previous work involves building a binary classifier for sentence pair"
N10-1063,P08-1088,0,0.206055,"rcu, 2005). However, a selftrained sentence pair extraction system is only able to acquire new lexical items that occur in parallel sentences. Within Wikipedia, many linked article pairs do not contain any parallel sentences, yet con406 tain many words and phrases that are good translations of each other. In this paper we explore an alternative approach to lexicon acquisition for use in parallel sentence extraction. We build a lexicon model using an approach similar to ones developed for unsupervised lexicon induction from monolingual or comparable corpora (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). We briefly describe the lexicon model and its use in sentence-extraction. The lexicon model is based on a probabilistic model P (wt |ws , T, S) where wt is a word in the target language, ws is a word in the source language, and T and S are linked articles in the target and source languages, respectively. We train this model similarly to the sentenceextraction ranking model, with the difference that we are aligning word pairs and not sentence pairs. The model is trained from a small set of annotated Wikipedia article pairs, where for some words in the source language we have marked one or mor"
N10-1063,W02-0902,0,0.601618,", as in (Munteanu and Marcu, 2005). However, a selftrained sentence pair extraction system is only able to acquire new lexical items that occur in parallel sentences. Within Wikipedia, many linked article pairs do not contain any parallel sentences, yet con406 tain many words and phrases that are good translations of each other. In this paper we explore an alternative approach to lexicon acquisition for use in parallel sentence extraction. We build a lexicon model using an approach similar to ones developed for unsupervised lexicon induction from monolingual or comparable corpora (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). We briefly describe the lexicon model and its use in sentence-extraction. The lexicon model is based on a probabilistic model P (wt |ws , T, S) where wt is a word in the target language, ws is a word in the source language, and T and S are linked articles in the target and source languages, respectively. We train this model similarly to the sentenceextraction ranking model, with the difference that we are aligning word pairs and not sentence pairs. The model is trained from a small set of annotated Wikipedia article pairs, where for some words in the source language w"
N10-1063,N03-1017,0,0.00903619,"based features can be dramatic as in the case of Bulgarian (the CRF model average precision increased by nearly 9 points). The lower gains on Spanish and German may be due in part to the lack of language-specific training data. These results are very promising and motivate further exploration. We also note that this is perhaps the first successful practical application of an automatically induced word translation lexicon. 4.3 SMT Evaluation We also present results in the context of a full machine translation system to evaluate the potential utility of this data. A standard phrasal SMT system (Koehn et al., 2003) serves as our testbed, using a conventional set of models: phrasal models of source given target and target given source; lexical weighting models in both directions, language model, word count, phrase count, distortion penalty, and a lexicalized reordering model. Given that the extracted Wikipedia data takes the standard form of parallel sentences, it would be easy to exploit this same data in a number of systems. For each language pair we explored two training conditions. The “Medium” data condition used easily downloadable corpora: Europarl for GermanEnglish and Spanish-English, and JRC/Ac"
N10-1063,2005.mtsummit-papers.11,0,0.331918,"s annotated with possible parallel sentences in the target language (the target language was English in all experiments). The pairs were annotated with a quality level: 1 if the sentences contained some parallel fragments, 2 if the sentences were mostly parallel with some missing words, and 3 if the sentences appeared to be direct translations. In all experiments, sentence pairs with quality 2 or 3 were taken as positive examples. The resulting datasets are available at http://research.microsoft.com/enus/people/chrisq/wikidownload.aspx. For our seed parallel data, we used the Europarl corpus (Koehn, 2005) for Spanish and German and the JRC-Aquis corpus for Bulgarian, plus the article titles for parallel Wikipedia documents, and translations available from Wiktionary entries.3 4.2 Intrinsic Evaluation Using 5-fold cross-validation on the 20 document pairs for each language condition, we compared the binary classifier, ranker, and CRF models for parallel sentence extraction. To tune for precision/recall, we used minimum Bayes risk decoding. We define the loss L(τ, µ) of picking target sentence τ when the correct target sentence is µ as 0 if τ = µ, λ if τ = NULL and µ 6= NULL, and 1 otherwise. By"
N10-1063,moore-2002-fast,0,0.383005,"r corpus already contains document alignments, we sidestep this problem, and will not discuss further details of this issue. That said, we believe that our methods will be effective in corpora without document alignments when combined with one of the aforementioned algorithms. 3.2 Sequence Models In Wikipedia article pairs, it is common for parallel sentences to occur in clusters. A global sentence alignment model is able to capture this phenomenon. For both parallel and comparable corpora, global sentence alignments have been used, though the alignments were monotonic (Gale and Church, 1991; Moore, 2002; Zhao and Vogel, 2002). Our model is a first order linear chain Conditional Random Field (CRF) (Lafferty et al., 2001). The set of source and target sentences are observed. For each source sentence, we have a hidden variable indicating the corresponding target sentence to which it is aligned (or null). The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006). 3.3 Features Our features can be grouped into four categories. 3.1 Binary Classifiers and Rankers Much of the previous work involves building a binary classifier for sentence pairs to determin"
N10-1063,J05-4003,0,0.774258,"llel corpus also strongly influences the quality of translations produced. Many parallel corpora are taken from the news domain, or from parliamentary proceedings. Translation quality suffers when a system is not trained on any data from the domain it is tested on. The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training. In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages. We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity. We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model. Results for both accuracy in sentence extraction and downstrea"
N10-1063,P03-1021,0,0.0038481,"in training data is unlikely to provide appropriate phrasal translations. Therefore, we experimented with two broad domain test sets. First, Bing Translator provided a sample of translation requests along with translations in GermanEnglish and Spanish-English, which acted our standard development and test set. Unfortunately no such tagged set was available in Bulgarian-English, so we held out a portion of the large system’s training data to use for development and test. In each language pair, the test set was split into a devel409 opment portion (“Dev A”) used for minimum error rate training (Och, 2003) and a test set (“Test A”) used for final evaluation. Second, we created new test sets in each of the three language pairs by sampling parallel sentences from held out Wikipedia articles. To ensure that this test data was clean, we manually filtered the sentence pairs that were not truly parallel and edited them as necessary to improve adequacy. We called this “Wikitest”. This test set is available at http://research.microsoft.com/enus/people/chrisq/wikidownload.aspx. Characteristics of these test sets are summarized in Table 5. We evaluated the resulting systems using BLEU4 (Papineni et al.,"
N10-1063,P02-1040,0,0.0927712,"ining (Och, 2003) and a test set (“Test A”) used for final evaluation. Second, we created new test sets in each of the three language pairs by sampling parallel sentences from held out Wikipedia articles. To ensure that this test data was clean, we manually filtered the sentence pairs that were not truly parallel and edited them as necessary to improve adequacy. We called this “Wikitest”. This test set is available at http://research.microsoft.com/enus/people/chrisq/wikidownload.aspx. Characteristics of these test sets are summarized in Table 5. We evaluated the resulting systems using BLEU4 (Papineni et al., 2002); the results are presented in Table 6. First we note that the extracted Wikipedia data are very helpful in medium data conditions, significantly improving translation performance in all conditions. Furthermore we found that the extracted Wikipedia sentences substantially improved translation quality on held-out Wikipedia articles. In every case, training on medium data plus Wikipedia extracts led to equal or better translation quality than the large system alone. Furthermore, adding the Wikipedia data to the large data condition still made substantial improvements. 5 Conclusions Our first sub"
N10-1063,P99-1067,0,0.346802,"elf-training, as in (Munteanu and Marcu, 2005). However, a selftrained sentence pair extraction system is only able to acquire new lexical items that occur in parallel sentences. Within Wikipedia, many linked article pairs do not contain any parallel sentences, yet con406 tain many words and phrases that are good translations of each other. In this paper we explore an alternative approach to lexicon acquisition for use in parallel sentence extraction. We build a lexicon model using an approach similar to ones developed for unsupervised lexicon induction from monolingual or comparable corpora (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). We briefly describe the lexicon model and its use in sentence-extraction. The lexicon model is based on a probabilistic model P (wt |ws , T, S) where wt is a word in the target language, ws is a word in the source language, and T and S are linked articles in the target and source languages, respectively. We train this model similarly to the sentenceextraction ranking model, with the difference that we are aligning word pairs and not sentence pairs. The model is trained from a small set of annotated Wikipedia article pairs, where for some words"
N10-1063,J03-3002,0,0.875704,"roduced. Many parallel corpora are taken from the news domain, or from parliamentary proceedings. Translation quality suffers when a system is not trained on any data from the domain it is tested on. The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training. In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages. We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity. We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model. Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented. 1 While parallel c"
N10-1063,N09-2024,0,0.108475,"ese 252K Czech 87K Table 1: Number of aligned bilingual articles in Wikipedia by language (paired with English). write the content themselves. Furthermore, even articles created through translations may later diverge due to independent edits in either language. 3 Models for Parallel Sentence Extraction In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents. The related problem of automatic document alignment in news and web corpora has been explored by a number of researchers, including Resnik and Smith (2003), Munteanu and Marcu (2005), Tillmann and Xu (2009), and Tillmann (2009). Since our corpus already contains document alignments, we sidestep this problem, and will not discuss further details of this issue. That said, we believe that our methods will be effective in corpora without document alignments when combined with one of the aforementioned algorithms. 3.2 Sequence Models In Wikipedia article pairs, it is common for parallel sentences to occur in clusters. A global sentence alignment model is able to capture this phenomenon. For both parallel and comparable corpora, global sentence alignments have been used, though the alignments were mon"
N10-1063,P09-2057,0,0.287582,"Number of aligned bilingual articles in Wikipedia by language (paired with English). write the content themselves. Furthermore, even articles created through translations may later diverge due to independent edits in either language. 3 Models for Parallel Sentence Extraction In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents. The related problem of automatic document alignment in news and web corpora has been explored by a number of researchers, including Resnik and Smith (2003), Munteanu and Marcu (2005), Tillmann and Xu (2009), and Tillmann (2009). Since our corpus already contains document alignments, we sidestep this problem, and will not discuss further details of this issue. That said, we believe that our methods will be effective in corpora without document alignments when combined with one of the aforementioned algorithms. 3.2 Sequence Models In Wikipedia article pairs, it is common for parallel sentences to occur in clusters. A global sentence alignment model is able to capture this phenomenon. For both parallel and comparable corpora, global sentence alignments have been used, though the alignments were monotonic (Gale and Chur"
N10-1063,C96-2141,0,0.887352,"corpus used for training is a major factor in its performance. For some language pairs, such as Chinese-English and Arabic-English, large amounts of parallel data are readily available, but for most language pairs this is not the case. The ∗ This research was conducted during the author’s internship at Microsoft Research. Once promising document pairs are identified, the next step is to extract parallel sentences. Usually, some seed parallel data is assumed to be available. This data is used to train a word alignment model, such as IBM Model 1 (Brown et al., 1993) or HMM-based word alignment (Vogel et al., 1996). Statistics from this word alignment model are used to train a classifier which identifies bilingual sentence pairs as parallel or not parallel. This classifier is applied to all sentence pairs in documents which were found to be similar. Typically, some pruning is done to reduce the number of sen403 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 403–411, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tence pairs that need to be classified. While these methods have been applied to news corpora and w"
N12-3006,D09-1111,1,0.814882,"Missing"
N12-3006,J93-2004,0,0.04636,"the NLP group at Microsoft Research. The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty,"
N12-3006,J05-1004,0,0.0277178,". The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty, and shared task organizers are now"
N12-3006,P06-1055,0,0.0153341,"Missing"
N12-3006,C08-1094,0,0.0225179,"Missing"
N12-3006,J08-2002,1,0.0600596,"abels, like ARG0, ARG1, …, ARG5 for core arguments, and labels like ARGMTMP,ARGM-LOC, etc. for adjunct-like arguments. The meaning of the numbered arguments is verb-specific, with ARG0 typically representing an agent-like role, and ARG1 a patient-like role. This implementation of an SRL system follows the approach described in (Xue and Palmer, 04), and includes two log-linear models for argument identification and classification. A single syntax tree generated by the MSR SPLAT split-merge parser is used as input. Non-overlapping arguments are derived using the dynamic programming algorithm by Toutanova et al. (2008). 3 3.1 Other Language Analysis Functionality Sentence Boundary / Tokenization This analyzer identifies sentence boundaries and breaks the input into tokens. Both are represented as offsets of character ranges. Each token has both a raw form from the string and a normalized form in the PTB specification, e.g., open and close parentheses are replaced by -LRB- and -RRB-, respectively, to remove ambiguity with parentheses indicating syntactic structure. A finite state machine using simple rules and abbreviations detects sentence boundaries with high accuracy, and a set of regular expressions toke"
N12-3006,W04-3212,0,0.0291436,"Missing"
N12-3006,J03-4003,0,\N,Missing
N13-1002,P11-2031,0,0.0145787,"5-gram models being better than the 3-gram. Here the individual 3-gram models are better than the baseline at significance level 0.02 and their combination is better than the baseline at our earlier defined threshold of 0.01. The withinphrase MTU MMs (results shown in the last two rows) improve upon the baseline slightly, but here again the improvements mostly stem from the use of context across phrase boundaries. Our final results on German-English are better than the best result of 27.30 from the shared task (Koehn and Monz, 2006). Thanks to the reviewers for referring us to recent work by (Clark et al., 2011) that pointed out problems with significance tests for machine translation, where the randomness and local optima in the MERT weight tuning method lead to a large variance in development and test set performance across different runs of optimization (using a different random seed or starting point). (Clark et al., 2011) proposed a stratified approximate randomization statistical significance test, which controls for optimizer instability. Using this test, for the English-Bulgarian system, we confirmed that the combination of four 3-gram MMs and the combination of 5-gram MMs is better than the"
N13-1002,P11-1105,0,0.148685,"nguage pairs, looking at distinct orders in isolation and combination. 12 Proceedings of NAACL-HLT 2013, pages 12–21, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Related work Marino et al. (2006) proposed a translation model using a Markov model of bilingual n-grams, demonstrating state-of-the-art performance compared to conventional phrase-based models. Crego and Yvon (2010) further explored factorized n-gram approaches, though both models considered rather large n-grams; this paper focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, where the translation probability of each rule is adjusted to include context information from parent rules (Vaswani et al., 2011). Most prior work tends to replace the existing probabilities rather than"
N13-1002,D09-1117,0,0.259413,"similar to recombination in a standard-phrase based decoder with the difference that it is not always the last two target MTUs that define the context needed by future extensions. The weights λ of different models are trained on a development set using MER training to maximize the BLEU score of the resulting model. Note that this method of model combination was not considered in any of the previous works comparing different decompositions. The system combination method is motivated by prior work in machine translation which combined left-to-right and right-to-left machine translation systems (Finch and Sumita, 2009). Similarly, we perform sentence-level system combination between systems using different MTU Markov models to come up with most likely translations. If we have k systems guessing hypotheses based on MM1 , . . . , MMk respectively, we generate 1000best lists from each system, resulting in a pool of up to 1000k possible distinct translations. Each of the candidate hypotheses from MMi is scored with its Markov model log-probability logP MMi (h). We compute normalized probabilities for each system’s n-best by exponentiating and normalizing: Pi (h) ∝ P MMi (h). If a hypothesis h is not in system i"
N13-1002,W07-0711,0,0.0146267,"lopment set consists of 1,497 sentences, the English side from WMT 2009 news test data, and the Bulgarian side a human translation thereof. The test set comes from the same mixture of sources as the training set. For this system we used a single four-gram target language model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). 19 3-gram models Dev Test 32.58 31.78 33.05 32.78* 33.05 32.96* 32.90 33.00* 32.94 32.98* 33.22 33.07* 32.58 31.78 5-gram models Dev Test 32.58 31.78 33.16 32.88* 33.16 32.81* 32.98 32.98* 33.09 32.96* 33.37 33.00* 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starre"
N13-1002,W06-3114,0,0.190924,"T 2002 and 2003 test sets as the development set and the NIST 2005 test set as our test set. The baseline phrasal system uses a 5-gram language model with modified Kneser-Ney smoothing (Kenser and Ney, 1995), trained on the Xinhua portion of the English Gigaword corpus (238M English words). For German-English we used the dataset from Language Chs-En Deu-En En-Bgr Training 1 Mln 751 K 4 Mln Dev NIST02+03 WMT06dev 1,497 Test NIST05 WMT06test 2,498 Model Baseline L2RT R2LT L2RS R2LS 4 MMs 4 MMs phrs Table 3: Data sets for different language pairs. the WMT 2006 shared task on machine translation (Koehn and Monz, 2006). The parallel training set contains approximately 751K sentences. We also used the English monolingual data of around 1 million sentences for language model training. The development set contains 2000 sentences. The final test set (the in-domain test set for the shared task) also contains 2000 sentences. Two Kneser-Ney language models were used as separate features: a 4gram LM trained on the parallel portion of the data, and a 5-gram LM trained on the monolingual corpus. For English-Bulgarian we used a dataset containing sentences from several data sources: JRCAcquis (Steinberger et al., 2006"
N13-1002,N03-1017,0,0.189237,"“that the source and target side of a tuple of words are synchronized, i.e. that they occur in the same order in their respective languages” (Crego and Yvon, 2010). For language pairs with significant typological divergences, such as Chinese-English, it is quite difficult to extract a synchronized sequence of units; in the limit, the smallest synchronized unit may be the whole sentence. Other approaches explore incorporation into syntax-based MT systems or replacing the phrasal translation system altogether. Introduction The translation procedure of a classical phrasebased translation model (Koehn et al., 2003) first divides the input sentence into a sequence of phrases, translates each phrase, explores reorderings of these translations, and then scores the resulting candidates with a linear combination of models. Conventional models include phrase-based channel models that effectively model each phrase as a large unigram, reordering models, and target language models. Of these models, only the target language model ∗ This research was conducted during the author’s internship at Microsoft Research We investigate the addition of MTUs to a phrasal translation system to improve modeling of context and"
N13-1002,W04-3250,0,0.0168628,"ate prediction. The best decomposition order varies from language to language: right-to-left in source order is best for Chinese-English, right-to-left in target order is best for German-English and left-to-right or rightto-left in target order are best in English-Bulgarian. We computed statistical significance tests, testing the difference between the L2RT model (the standard in prior work) and models achieving higher test set performance. The models that are significantly better at significance α &lt; 0.01 are marked with a star in the table. We used a paired bootstrap test with 10,000 trials (Koehn, 2004). Next we evaluate the methods for combining decomposition orders introduced in Sections 4.1 and 4.2. The results are reported in Table 2. The upper part of the table focuses on combining different 18 Model Baseline-1 TgtProduct TgtSysComb TgtDynamic Baseline-2 AllProduct AllSyscomb Chs-En Dev Test 24.04 25.09 25.27 25.84* 24.49 25.27 24.07 25.10 26.48 27.96 28.68 29.59* 27.02 28.30 Deu-En Dev Test 30.14 30.14 30.47 30.49 30.20 30.15 30.60 30.41 30.14 30.14 31.54 31.36* 30.20 30.17 En-Bgr Dev Test 49.86 46.45 51.04 47.27* 50.46 46.31 49.99 46.52 49.86 46.45 51.50 48.10* 50.90 46.53 Table 2: Le"
N13-1002,J06-4004,0,0.727376,"Missing"
N13-1002,C08-1074,1,0.791289,"me mixture of sources as the training set. For this system we used a single four-gram target language model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). 19 3-gram models Dev Test 32.58 31.78 33.05 32.78* 33.05 32.96* 32.90 33.00* 32.94 32.98* 33.22 33.07* 32.58 31.78 5-gram models Dev Test 32.58 31.78 33.16 32.88* 33.16 32.81* 32.98 32.98* 33.09 32.96* 33.37 33.00* 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starred results on the test set indicate significantly better performance than the baseline. 6.3 MT reranking experiments We first report detailed experiments on ChineseEnglish, and then ve"
N13-1002,P02-1038,0,0.0382882,"approximations to the trigram MTU Markov model scores as future scores, since not all needed context is available for a hypothesis at the time of construction. As additional context becomes available, the exact score can be computed. 2 4.1 Basic decomposition order combinations We first introduce two methods of combining different decomposition orders: product and system combination. The product method arises naturally in the machine translation setting, where probabilities from different models are multiplied together and further weighted to form the log-linear model for machine translation (Och and Ney, 2002). We define a similar scoring function using a set of MTU Markov models MM 1 , ..., MM k for a hypothesis h as follows: Score(h) = λ1 logP MM1 (h) + ... + λk logP MMk (h) Figure 2: Lexical selection. 2 We use this constrained MT setting to evaluate the performance of models using different MTU decomposition orders and models using combinations of decomposition orders. The simplified setting allows 15 We apply hypothesis recombination, which can merge hypotheses that are indistinguishable with respect to future continuations. This is similar to recombination in a standard-phrase based decoder w"
N13-1002,P03-1021,0,0.0140661,"f. The test set comes from the same mixture of sources as the training set. For this system we used a single four-gram target language model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). 19 3-gram models Dev Test 32.58 31.78 33.05 32.78* 33.05 32.96* 32.90 33.00* 32.94 32.98* 33.22 33.07* 32.58 31.78 5-gram models Dev Test 32.58 31.78 33.16 32.88* 33.16 32.81* 32.98 32.98* 33.09 32.96* 33.37 33.00* 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starred results on the test set indicate significantly better performance than the baseline. 6.3 MT reranking experiments We first report detail"
N13-1002,P02-1040,0,0.102213,"uage model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). 19 3-gram models Dev Test 32.58 31.78 33.05 32.78* 33.05 32.96* 32.90 33.00* 32.94 32.98* 33.22 33.07* 32.58 31.78 5-gram models Dev Test 32.58 31.78 33.16 32.88* 33.16 32.81* 32.98 32.98* 33.09 32.96* 33.37 33.00* 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starred results on the test set indicate significantly better performance than the baseline. 6.3 MT reranking experiments We first report detailed experiments on ChineseEnglish, and then verify our main conclusions on the other language pairs. Table 4 looks at the impact of individual"
N13-1002,N06-1002,1,0.92188,"anslation model using a Markov model of bilingual n-grams, demonstrating state-of-the-art performance compared to conventional phrase-based models. Crego and Yvon (2010) further explored factorized n-gram approaches, though both models considered rather large n-grams; this paper focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, where the translation probability of each rule is adjusted to include context information from parent rules (Vaswani et al., 2011). Most prior work tends to replace the existing probabilities rather than augmenting them. We believe that Markov rules provide an additional signal but are not a replacement. Their distributions should be more informative than the so-called “lexical weighting” models, and less sparse than relative frequency estimates, thoug"
N13-1002,N03-1033,1,0.0127916,"text for disambiguation and it is not clear apriori which would perform best. We compare all four decomposition orders (source order left-to-right and right-to-left, and target order left-to-right and rightto-left). Although the independence assumptions of left-to-right and right-to-left are the same, the resulting models may be different due to smoothing. In addition to studying these four basic decomposition orders, we report performance of two cyclic orders: cyclic in source or target sentence order. These models are inspired by the cyclic dependency network model proposed for POS tagging (Toutanova et al., 2003) and also used as a baseline in previous work on dynamic decomposition orders (Tsuruoka and Tsujii, 2005). 1 The probability according to the cyclic orders is defined by conditioning each MTU on both its left and right neighbor MTUs. For example, the probability of the sentence pair in Figure 1 under the source cyclic order, using a 3-gram model is defined as: P(M1|M2) · P(M2|M1, M3) · P(M3|M2, M4) · P(M4|M3, M5) · P(M5|M4). All n-gram Markov models over MTUs are esti1 The correct application of such models requires sampling to find the highest scoring sequence, but we apply the max product ap"
N13-1002,H05-1059,0,0.328021,"unigrams to capture contextual information, n-grams of minimal translation units allow a robust contextual model that is less constrained by segmentation. 3.2 MTU enumeration orders When defining a joint probability distribution over MTUs of an aligned sentence pair, it is necessary to define a decomposition, or generation order for the sentence pair. For a single sequence in language modeling or synchronized sequences in channel modeling, the default enumeration order has been left-to-right. Different decomposition orders have been used in part-of-speech tagging and named entity recognition (Tsuruoka and Tsujii, 2005). Intuitively, information from the left or right could be more useful for particular disambiguation choices. Our research on different decomposition orders was motivated by this work. When applying such ideas to machine translation, there are additional challenges and opportunities. The task exhibits much more ambiguity – the number of possible MTUs is in the millions. An opportunity arises from the reordering phenomenon in machine translation: while in POS tagging the natural decomposition orders to study are only left-to-right and right-to-left, in machine translation we can further disting"
N13-1002,P11-1086,0,0.364188,"er focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, where the translation probability of each rule is adjusted to include context information from parent rules (Vaswani et al., 2011). Most prior work tends to replace the existing probabilities rather than augmenting them. We believe that Markov rules provide an additional signal but are not a replacement. Their distributions should be more informative than the so-called “lexical weighting” models, and less sparse than relative frequency estimates, though potentially not as effective for truly non-compositional units. Therefore, we explore the inclusion of all such information. Also, unlike prior work, we explore combinations of multiple decomposition orders, as well as dynamic decompositions. The most useful context for t"
N13-1002,steinberger-etal-2006-jrc,0,\N,Missing
N15-3006,P14-1134,0,0.588109,"to the LDC data sets, and 2) we will demonstrate AMR parsers for German, French, Spanish and Japanese that make use of the same small set of LF-to-AMR conversion rules. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2014) is a semantic representation for which a large amount of manually-annotated data is being created, with the intent of constructing and evaluating parsers that generate this level of semantic representation for previously unseen text. 1 Available at: http://research.microsoft.com/msrsplat Already one method for training an AMR parser has appeared in (Flanigan et al., 2014), and we anticipate that more attempts to train parsers will follow. In this demonstration, we will present our AMR parser, which converts our existing semantic representation formalism, Logical Form (LF), into the AMR format. We do this with two goals: first, as our existing LF is close in design to AMR, we can now use the manually-annotated AMR datasets to measure the accuracy of our LF system, which may serve to provide a benchmark for parsers trained on the AMR corpus. We gratefully acknowledge the contributions made by Banarescu et al. (2014) towards defining a clear and interpretable sem"
N15-3006,flickinger-etal-2014-towards,0,0.0211558,"nnotated test suite (Suzuki, 2002). We start by curating a sentence corpus that exemplifies the syntactic and semantic phenomena that the LF is designed to cover; one might view this sentence corpus as the LF specification. When, during development, the system outputs the desired representation, that LF is saved as “gold annotation”. In this way, the gold annotations are produced by the LF system itself, automatically, and thus with good system internal consistency. We note that this method of system development is quite different from SemBanking AMR, but is similar to the method described in Flickinger et al. (2014). As part of this demonstration, we share with participants the gold annotations for the curated sentence corpora used during LF development, currently 550 sentences that are vetted to produce correct LF analyses. Note that the example in Figure 2 requires a parser to handle both the passive/active alternation as well as control verbs. We believe that there is value in curated targeted datasets to supplement annotating natural data; e.g., AMR clearly includes control phenomena in its spec (the first example is “the boy wants to go”) but in the data, there are only 3 instances of “persuade” in"
N15-3006,W02-1510,0,0.0502554,"e data. Adding training data from other sources leads to improvements on the discussion forum data, but at the cost of accuracy on newswire. The lack of sophisticated sense disambiguation in LF causes a substantial degradation in performance on newswire. 6 Data Sets for LF development The LF component was developed by authoring rules that access information from a rich lexicon consisting of several online dictionaries as well as information output by a rich grammar formalism. Authoring these LF rules is supported by a suite of tools that allow iterative development of an annotated test suite (Suzuki, 2002). We start by curating a sentence corpus that exemplifies the syntactic and semantic phenomena that the LF is designed to cover; one might view this sentence corpus as the LF specification. When, during development, the system outputs the desired representation, that LF is saved as “gold annotation”. In this way, the gold annotations are produced by the LF system itself, automatically, and thus with good system internal consistency. We note that this method of system development is quite different from SemBanking AMR, but is similar to the method described in Flickinger et al. (2014). As part"
N15-3006,W04-2807,0,\N,Missing
N15-3006,P13-2131,0,\N,Missing
N19-1269,P17-1059,0,0.0299531,"s its context and is grounded in a content-rich external textual source such as a news story. Our experiments on Wikipedia data show significant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in"
N19-1269,P17-1070,0,0.0137682,"ant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.1 Consider for example Fig. 1, which illustrates a situation where an author edits a document (here a Wikip"
N19-1269,D16-1140,0,0.0198381,"titive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.1 Consider for example Fig. 1, which illustrates a situation where an author edits a document (here a Wikipedia article), 1 Historically, NLG"
N19-1269,D16-1128,0,0.0461276,"Missing"
N19-1269,N15-1145,0,0.0246576,"9, 2014, manning signed a one-year contract with the cincinnati bengals. on feb 9, 2013, barrett signed with the memphis grizzlies. some people think elvis, but most of us think he’s dead and gone.” it’s always the goal of the foreign- entry film award executive to be as possible.” Table 5: Example generations from the CIG system, paired with the human generated updates. is a substantial difference. Also our datasets are substantially larger, enabling generative models to be used in this space, where prior update summarization techniques have been primarily extractive (Fisher and Roark, 2008; Li et al., 2015). For any generation task, it is important to address both the content (‘what’ is being said) as well its style (‘how’ it is being said). Recently, a great deal of research has focused on the ‘how’ (Li et al., 2018; Shen et al., 2017), including efforts to collect a parallel dataset that differs in politeness (Rao and Tetreault, 2018), to control author characteristics in the generated sentences (Prabhumoye et al., 2018), to control the perceived personality traits of dialog responses (Zhang et al., 2018). We believe this research thread is complementary to our efforts on generating the ‘what’"
N19-1269,N16-1014,1,0.950328,"ource such as a news story. Our experiments on Wikipedia data show significant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.1 Consider for example Fig. 1,"
N19-1269,P16-1094,1,0.943366,"ource such as a news story. Our experiments on Wikipedia data show significant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.1 Consider for example Fig. 1,"
N19-1269,N18-1169,0,0.0582512,"Missing"
N19-1269,D15-1166,0,0.0540266,"t salient to the topic or focus of the text, then generate a single sentence that represents this information. 3.1 Generative models A natural though difficult means of generating this additional update sentence x is to use a generative model conditioned on the information in the curated text s and the new document d. Recent methods inspired by successful neural machine translation systems have produced impressive results in abstractive summarization (Nallapati et al., 2016). Hence, our first step is to use the sequence-to-sequence encoder-decoder model (Bahdanau et al., 2015) with attention (Luong et al., 2015) for our task. This kind of model assumes that the output sentence can be generated word-by-word. Each output word xti generated is conditioned on all prior words x&lt;t i and an encoded representation of the context z: Y p(ˆ xti |ˆ x&lt;t (1) i , z) of the curated text. Hence, the model may capture some generalizations about the kinds of information and locations in d that are most likely to contribute novel information to s. Context Only Generative (COG) Model: This algorithm is trained to generate the most likely upˆ = arg max p(x|s). This model date sentence x is similar to CAG except that we co"
N19-1269,N16-1086,0,0.0570062,"2018), to control author characteristics in the generated sentences (Prabhumoye et al., 2018), to control the perceived personality traits of dialog responses (Zhang et al., 2018). We believe this research thread is complementary to our efforts on generating the ‘what’. Another form of content transfer bridges across modalities: text generation given schematized or semi-structured information. Recent research has addressed neural natural language generation techniques given a range of structured sources: selecting relevant database records and generating natural language descriptions of them (Mei et al., 2016), selecting and describing slot-value pairs for task-specific dialog response generation (Wen et al., 2015), and even generating Wikipedia biography abstracts given Infobox information (Lebret et al., 2016). Our task, while grounded in external content, is different in that it leverages linguistic grounding as well as prior text context when generating text. This challenging setting enables a huge range of grounded generation tasks: there are vast amounts of unstructured textual data. formation. We demonstrate how multiple models can address this challenging problem on a novel dataset derived"
N19-1269,N16-1098,0,0.0319046,"hor edits a document (here a Wikipedia article), 1 Historically, NLG has focused on generation from structured content such as a database or semantic representation, but this paper is interested in generation from free-form text. Figure 1: Example of content transfer: Given existing curated text (yellow) and a document with additional relevant information (green), the task is to update the curated text (orange) to reflect the most salient updates. and the goal is to generate or suggest a next sentence (shown in orange) to the author. This type of unconstrained, long-form text generation task (Mostafazadeh et al., 2016; Fan et al., 2018) is of course extremely difficult. Free-form generation can easily go astray due to two opposing factors. On one hand, ensuring that the generated output is of relatively good quality often comes at the cost of making it bland and devoid of factual content (Li et al., 2016a). On the other hand, existing techniques can help steer neural models away 2622 Proceedings of NAACL-HLT 2019, pages 2622–2632 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics from blandness in order to produce more contentful outputs (using temperature sampl"
N19-1269,K16-1028,0,0.464293,"to incorporate the additional information from document di . The goal would be to identify new information (in particular, di  si ) that is most salient to the topic or focus of the text, then generate a single sentence that represents this information. 3.1 Generative models A natural though difficult means of generating this additional update sentence x is to use a generative model conditioned on the information in the curated text s and the new document d. Recent methods inspired by successful neural machine translation systems have produced impressive results in abstractive summarization (Nallapati et al., 2016). Hence, our first step is to use the sequence-to-sequence encoder-decoder model (Bahdanau et al., 2015) with attention (Luong et al., 2015) for our task. This kind of model assumes that the output sentence can be generated word-by-word. Each output word xti generated is conditioned on all prior words x&lt;t i and an encoded representation of the context z: Y p(ˆ xti |ˆ x&lt;t (1) i , z) of the curated text. Hence, the model may capture some generalizations about the kinds of information and locations in d that are most likely to contribute novel information to s. Context Only Generative (COG) Model"
N19-1269,W16-4620,0,0.012626,"y. Our experiments on Wikipedia data show significant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.1 Consider for example Fig. 1, which illustrates a situation whe"
N19-1269,P02-1040,0,0.103679,"ntencepiece toolkit6 to use byte-pairencoding (BPE) with a vocabulary size of 32k. We used stochastic gradient descent optimizer and the stopping criterion was perplexity on the validation set. We filtered our dataset to contain instances which have length of the document between 50 and 2000 tokens, length of the curated text between 20 and 500 tokens and the length of the update sentence between 5 and 200 tokens. 5.1 Automated Evaluation Our primary automated evaluation metric for system-generated update sentences is ROUGE-L F1 against reference update sentence,7 though we also include BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011) as additional indicators. ROUGE is a standard family of metrics for summarization tasks; ROUGE-L measures the longest common subsequence between the system and the reference, capturing both lexical selection and word order. Table 2 illustrates that this task is quite difficult for extractive techniques. Furthermore, the results emphasize the importance of having curated text as context when generating the update. In all experimental conditions, models aware of context perform much better than models agnostic of it. In contrast to Liu et al. (2018), gener"
N19-1269,P18-1205,0,0.0223743,"rior update summarization techniques have been primarily extractive (Fisher and Roark, 2008; Li et al., 2015). For any generation task, it is important to address both the content (‘what’ is being said) as well its style (‘how’ it is being said). Recently, a great deal of research has focused on the ‘how’ (Li et al., 2018; Shen et al., 2017), including efforts to collect a parallel dataset that differs in politeness (Rao and Tetreault, 2018), to control author characteristics in the generated sentences (Prabhumoye et al., 2018), to control the perceived personality traits of dialog responses (Zhang et al., 2018). We believe this research thread is complementary to our efforts on generating the ‘what’. Another form of content transfer bridges across modalities: text generation given schematized or semi-structured information. Recent research has addressed neural natural language generation techniques given a range of structured sources: selecting relevant database records and generating natural language descriptions of them (Mei et al., 2016), selecting and describing slot-value pairs for task-specific dialog response generation (Wen et al., 2015), and even generating Wikipedia biography abstracts giv"
N19-1269,P18-1080,1,0.837215,"sets are substantially larger, enabling generative models to be used in this space, where prior update summarization techniques have been primarily extractive (Fisher and Roark, 2008; Li et al., 2015). For any generation task, it is important to address both the content (‘what’ is being said) as well its style (‘how’ it is being said). Recently, a great deal of research has focused on the ‘how’ (Li et al., 2018; Shen et al., 2017), including efforts to collect a parallel dataset that differs in politeness (Rao and Tetreault, 2018), to control author characteristics in the generated sentences (Prabhumoye et al., 2018), to control the perceived personality traits of dialog responses (Zhang et al., 2018). We believe this research thread is complementary to our efforts on generating the ‘what’. Another form of content transfer bridges across modalities: text generation given schematized or semi-structured information. Recent research has addressed neural natural language generation techniques given a range of structured sources: selecting relevant database records and generating natural language descriptions of them (Mei et al., 2016), selecting and describing slot-value pairs for task-specific dialog respons"
N19-1269,N18-1012,0,0.0318856,"tem, paired with the human generated updates. is a substantial difference. Also our datasets are substantially larger, enabling generative models to be used in this space, where prior update summarization techniques have been primarily extractive (Fisher and Roark, 2008; Li et al., 2015). For any generation task, it is important to address both the content (‘what’ is being said) as well its style (‘how’ it is being said). Recently, a great deal of research has focused on the ‘how’ (Li et al., 2018; Shen et al., 2017), including efforts to collect a parallel dataset that differs in politeness (Rao and Tetreault, 2018), to control author characteristics in the generated sentences (Prabhumoye et al., 2018), to control the perceived personality traits of dialog responses (Zhang et al., 2018). We believe this research thread is complementary to our efforts on generating the ‘what’. Another form of content transfer bridges across modalities: text generation given schematized or semi-structured information. Recent research has addressed neural natural language generation techniques given a range of structured sources: selecting relevant database records and generating natural language descriptions of them (Mei e"
N19-1269,D15-1044,0,0.0955856,"nerated update is close to the reference. well as the reference update. As we can see in examples 3 and 4, the CIG model misplaces the date but correctly generates the remaining content. In examples 1 and 2, the CIG model appears to successfully select the correct pronouns for coreference resolution, though it gets confused as to when to use the pronoun or the named entity. Examples 5 and 6 represent failure cases due to missing words. 6 Related Work The proposed content transfer task is clearly related to a long series of papers in summarization, including recent work with neural techniques (Rush et al., 2015; Nallapati et al., 2016). In particular, one recent paper casts the the task of generating an entire Wikipedia article as a multidocument summarization problem (Liu et al., 2018). Their best-performing configuration was a two-stage extractive-abstractive framework; a multi-stage approach helped circumvent the diffiDocument (News Article) anne kirkbride, who portrayed bespectacled, gravelly-voiced deirdre barlow in coronation street for more that four decades, has died. the 60-year-old, whose first appearance in the soap opera was in 1972, died in a manchester hospital after a short illness..."
N19-1269,N16-1005,0,0.0327879,"a content-rich external textual source such as a news story. Our experiments on Wikipedia data show significant improvements against competitive baselines. As another contribution of this paper, we release a benchmark dataset of 640k Wikipedia referenced sentences paired with the source articles to encourage exploration of this new task. 1 Introduction Recent work in neural natural language generation (NLG) has witnessed a growing interest in controlling text for various form-related and linguistic properties, such as style (Ficler and Goldberg, 2017), affect (Ghosh et al., 2017), politeness (Sennrich et al., 2016), persona (Li et al., 2016b) voice (Yamagishi et al., 2016), grammatical correctness (Ji et al., 2017), and length (Kikuchi et al., 2016). This trend offers the promise of empowering existing authoring tools such as Grammarly, Google Smart Compose, and Microsoft Word with the ability to control a much greater variety of textual properties, which are currently mostly limited to grammar, spelling, word choice, and wordiness. What has been relatively less explored in neural NLG research is the ability to control the generation of a current sentence not only in its form, but also its content.1 Con"
N19-1269,D15-1199,0,0.141923,"Missing"
P05-1034,P03-1012,1,0.660767,"Missing"
P05-1034,J00-1004,0,0.0980277,"Missing"
P05-1034,2004.tmi-1.14,1,0.686027,"Missing"
P05-1034,2003.mtsummit-papers.6,0,0.185854,"Missing"
P05-1034,C04-1090,0,0.373079,"Missing"
P05-1034,W01-1406,1,0.192233,"Missing"
P05-1034,J03-1002,0,0.0553796,"Missing"
P05-1034,P03-1021,0,0.333042,"Missing"
P05-1034,N04-1021,0,0.0987084,"Missing"
P05-1034,P02-1040,0,0.11186,"Missing"
P05-1034,C04-1097,0,0.0277307,"Missing"
P05-1034,2003.mtsummit-papers.53,0,0.0108741,"Missing"
P05-1034,P01-1067,0,0.956293,"Missing"
P05-1034,N04-1014,0,\N,Missing
P05-1034,koen-2004-pharaoh,0,\N,Missing
P05-1034,W99-0604,0,\N,Missing
P05-1034,J99-4005,0,\N,Missing
P05-1034,N04-1023,0,\N,Missing
P05-1034,C04-1030,0,\N,Missing
P05-1034,J93-2003,0,\N,Missing
P05-1034,2001.mtsummit-ebmt.4,1,\N,Missing
P05-1034,C96-2141,0,\N,Missing
P05-1034,P03-1011,0,\N,Missing
P05-1034,W03-0301,0,\N,Missing
P05-1034,P03-2041,0,\N,Missing
P05-1034,N03-1017,0,\N,Missing
P05-1034,P02-1038,0,\N,Missing
P05-1034,P97-1037,0,\N,Missing
P05-1034,J97-3002,0,\N,Missing
P05-1034,W02-1039,0,\N,Missing
P05-1034,P98-2230,0,\N,Missing
P05-1034,C98-2225,0,\N,Missing
P05-1034,N04-1033,0,\N,Missing
P05-1034,P00-1056,0,\N,Missing
P05-1034,W01-1408,0,\N,Missing
P05-1034,P01-1030,0,\N,Missing
P05-1034,P03-1019,0,\N,Missing
P05-1034,W04-1513,0,\N,Missing
P08-1012,J93-2003,0,0.0547987,"models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. 1 Introduction Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments. These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al. (1996). As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, then heuristically combining these alignments. Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the basis of the decoding process. While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems. A na"
P08-1012,W07-0403,0,0.541593,"ly speaking, the goal of this section is the same as the previous section, namely, to limit the set of phrase pairs that needs to be considered in the training process. The tic-tac-toe pruning relies on IBM model 1 for scoring a given aligned area. In this part, we use word-based ITG alignments as anchor points in the alignment space to pin down the potential phrases. The scope of iterative phrasal ITG training, therefore, is limited to determining the boundaries of the phrases anchored on the given one-toone word alignments. The heuristic method is based on the NonCompositional Constraint of Cherry and Lin (2007). Cherry and Lin (2007) use GIZA++ intersections which have high precision as anchor points in the 102 bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the p"
P08-1012,P05-1033,0,0.0486337,"recision as anchor points in the 102 bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link. Mathematically, let e(i, j) count the number of word links that are emitted from the substring ei...j , and f (l, m) count the number of word links emitted from the substring fl...m . The non-compositional phrase pairs satisfy e(i, j) = f (l, m) ≤ 1. Figure 3 (a) shows all possible non-compositional phrases given the Viterbi word alignment of the example sentence pair. 6 Summary of the Pipeline We summarize the pipeline of our system, demonstrating the interactions between the three main contributions of this"
P08-1012,P07-1094,0,0.0064648,"r non-zero values. Our second approach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training. First we train a lower level word alignment model, then we place hard constraints on the phrasal alignment space using confident word links from this simpler model. Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model. 3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging. In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior. The traditional estimation method for word alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data. The drawback of maximum likelihood is obvious for phrase-based models. If we do not put any co"
P08-1012,D07-1031,0,0.0815598,"ach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training. First we train a lower level word alignment model, then we place hard constraints on the phrasal alignment space using confident word links from this simpler model. Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model. 3 Variational Bayes for ITG Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging. In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior. The traditional estimation method for word alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data. The drawback of maximum likelihood is obvious for phrase-based models. If we do not put any constraint on the dis"
P08-1012,koen-2004-pharaoh,0,0.0360953,"translation tables using five iterations of Model 1. These values were used to perform tic-tac-toe pruning with τb = 1 × 10−3 and τs = 1 × 10−6 . Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB. The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model. Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB, and extracted the Viterbi alignments. For translation, we used the standard phrasal decoding approach, based on a re-implementation of the Pharaoh system (Koehn, 2004). The output of the word alignment systems (GIZA++ or ITG) were fed to a standard phrase extraction procedure that extracted all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies. Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units. In addition the phrases were annotated with lexical weights using the IBM Model 1 tables. The decoder also used a trigram language model trained on the target sid"
P08-1012,W02-1018,0,0.18838,"ues is unite the word-level and phrase-level models into one learning procedure. Ideally, such a procedure would remedy the deficiencies of word-level alignment models, including the strong restrictions on the form of the alignment, and the strong independence assumption between words. Furthermore it would obviate the need for heuristic combination of word alignments. A unified procedure may also improve the identification of non-compositional phrasal translations, and the attachment decisions for unaligned words. In this direction, Expectation Maximization at the phrase level was proposed by Marcu and Wong (2002), who, however, experienced two major difficulties: computational complexity and controlling overfitting. Computational complexity arises from the exponentially large number of decompositions of a sentence pair into phrase pairs; overfitting is a problem because as EM attempts to maximize the likelihood of its training data, it prefers to directly explain a sentence pair with a single phrase pair. In this paper, we attempt to address these two issues in order to apply EM above the word level. 97 Proceedings of ACL-08: HLT, pages 97–105, c Columbus, Ohio, USA, June 2008. 2008 Association for Co"
P08-1012,E03-1035,1,0.44657,"ee (either as terminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and"
P08-1012,J03-1002,0,0.0141074,"7.2 End-to-end Evaluation Given an unlimited amount of time, we would tune the prior to maximize end-to-end performance, using an objective function such as BLEU. Unfortunately these experiments are very slow. Since we observed monotonic increases in alignment performance with smaller values of αC , we simply fixed the prior at a very small value (10−100 ) for all translation experiments. We do compare VB against EM in terms of final BLEU scores in the translation experiments to ensure that this sparse prior has a significant impact on the output. We also trained a baseline model with GIZA++ (Och and Ney, 2003) following a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4. We computed Chinese-toEnglish and English-to-Chinese word translation tables using five iterations of Model 1. These values were used to perform tic-tac-toe pruning with τb = 1 × 10−3 and τs = 1 × 10−6 . Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB. The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model. Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB"
P08-1012,J04-4002,0,0.0390477,"tions which have high precision as anchor points in the 102 bitext space to constraint ITG phrases. We use ITG Viterbi alignments instead. The benefit is two-fold. First of all, we do not have to run a GIZA++ aligner. Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns. GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG. Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link. Mathematically, let e(i, j) count the number of word links that are emitted from the substring ei...j , and f (l, m) count the number of word links emitted from the substring fl...m . The non-compositional phrase pairs satisfy e(i, j) = f (l, m) ≤ 1. Figure 3 (a) shows all possible non-compositional phrases given the Viterbi word alignment of the example sentence pair. 6 Summary of the Pipeline We summarize the pipeline of our system, demonstrating the interactions between the three main con"
P08-1012,P03-1021,0,0.03586,"d all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies. Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units. In addition the phrases were annotated with lexical weights using the IBM Model 1 tables. The decoder also used a trigram language model trained on the target side of the training data, as well as word count, phrase count, and distortion penalty features. Minimum Error Rate training (Och, 2003) over BLEU was used to optimize the weights for each of these models over the development test data. We used the NIST 2002 evaluation datasets for tuning and evaluation; the 10-reference development set was used for minimum error rate training, and the 4-reference test set was used for evaluation. We trained several phrasal translation systems, varying only the word alignment (or phrasal alignment) method. Table 1 compares the four systems: the GIZA++ baseline, the ITG word-based model, the ITG multiword model using EM training, and the ITG multiword model using VB training. ITG-mwm-VB is our"
P08-1012,C96-2141,0,0.614103,"ble, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches. 1 Introduction Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments. These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al. (1996). As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, then heuristically combining these alignments. Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the basis of the decoding process. While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems. A natural solution to severa"
P08-1012,2005.mtsummit-papers.33,0,0.067956,"minals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and VB . Only those"
P08-1012,J97-3002,0,0.894319,"arning small noncompositional phrases. We address the tendency of EM to overfit by using Bayesian methods, where sparse priors assign greater mass to parameter vectors with fewer non-zero values therefore favoring shorter, more frequent phrases. We test our model by extracting longer phrases from our model’s alignments using traditional phrase extraction, and find that a phrase table based on our system improves MT results over a phrase table extracted from traditional word-level alignments. 2 Phrasal Inversion Transduction Grammar We use a phrasal extension of Inversion Transduction Grammar (Wu, 1997) as the generative framework. Our ITG has two nonterminals: X and C, where X represents compositional phrase pairs that can have recursive structures and C is the preterminal over terminal phrase pairs. There are three rules with X on the left-hand side: X → [X X], X → hX Xi, X → C. The first two rules are the straight rule and inverted rule respectively. They split the left-hand side constituent which represents a phrase pair into two smaller phrase pairs on the right-hand side and order them according to one of the two possible permutations. The rewriting process continues until the third ru"
P08-1012,P05-1059,1,0.873,"nals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005). Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases. The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j]. Our pruning differs from Zhang and Gildea (2005) in two major ways. First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and VB . Only those spans that pass the pruning th"
P09-2088,W06-3114,0,0.0777906,"Missing"
P09-2088,H93-1017,0,0.128565,"the same context: p(wn |w1 . . . wn−1 ) = P w0 C 0 (w1 . . . wn ) = {w0 |C(w0 w1 . . . wn ) > 0} In other words, the count used for a lower-order N-gram is the number of distinct word types that precede it in the training corpus. The fact that the lower-order models are estimated differently from the highest-order model makes the use of Kneser-Ney (KN) smoothing awkward in some situations. For example, coarse-to-fine search using a sequence of lowerorder to higher-order language models has been shown to be an efficient way of constraining highdimensional search spaces for speech recognition (Murveit et al., 1993) and machine translation (Petrov et al., 2008). The lower-order models used in KN smoothing, however, are very poor estimates of the probabilities for N-grams that have been observed in the training corpus, so they are C(w1 . . . wn ) C(w1 . . . wn−1 w0 ) One obvious problem with this method is that it assigns a probability of zero to any N-gram that is 349 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 349–352, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP p(wn |w1 . . . wn−1 ) =            αw1 ...wn−1 Cn (w1 ...wn )−Dn,Cn (w1 ...wn ) P C (w1 ...wn−1 w 0"
P09-2088,D08-1012,0,0.0303439,"C 0 (w1 . . . wn ) = {w0 |C(w0 w1 . . . wn ) > 0} In other words, the count used for a lower-order N-gram is the number of distinct word types that precede it in the training corpus. The fact that the lower-order models are estimated differently from the highest-order model makes the use of Kneser-Ney (KN) smoothing awkward in some situations. For example, coarse-to-fine search using a sequence of lowerorder to higher-order language models has been shown to be an efficient way of constraining highdimensional search spaces for speech recognition (Murveit et al., 1993) and machine translation (Petrov et al., 2008). The lower-order models used in KN smoothing, however, are very poor estimates of the probabilities for N-grams that have been observed in the training corpus, so they are C(w1 . . . wn ) C(w1 . . . wn−1 w0 ) One obvious problem with this method is that it assigns a probability of zero to any N-gram that is 349 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 349–352, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP p(wn |w1 . . . wn−1 ) =            αw1 ...wn−1 Cn (w1 ...wn )−Dn,Cn (w1 ...wn ) P C (w1 ...wn−1 w 0 ) w0 n + βw1 ...wn−1 p(wn |w2 . . . wn−1 ) γw"
P10-1028,P06-1129,0,0.255299,"r a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the da"
P10-1028,J04-4002,0,0.0557688,"using clickthrough data. Most of the speller systems reviewed above are based on the framework of the source channel model. Typically, a language model (source model) is used to capture contextual information, while an error model (channel model) is considered to be context free in that it does not take into account any contextual information in modeling word transformation probabilities. In this study we argue that it is beneficial to capture contextual information in the error model. To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrase-based error model where we assume that query spelling correction is performed at the phrase level. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and proved to be useful for W"
P10-1028,H05-1120,0,0.426726,"n text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to"
P10-1028,D08-1047,0,0.0208292,"word that is not found in a pre-compiled lexicon is considered to be misspelled. Then, a list of lexical words that are similar to the misspelled word are proposed as candidate spelling corrections. Most traditional systems use a manually tuned similarity function (e.g., edit distance function) to rank the candidates, as reviewed by Kukich (1992). During the last two decades, statistical error models learned on training data (i.e., query-correction pairs) have become increasingly popular, and have proven more effective (Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as ""peace"" and ""piece"" in the context ""a _ of cake"". A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular"
P10-1028,P00-1037,0,0.920791,"or the development of a query speller system. In this paper we present a novel method of extracting large amounts of query-correction pairs from the clickthrough data. These pairs, implicitly judged by millions of users, are used to train a set of spelling error models. Among these models, the most effective one is a phrase-based error model that captures the probability of transforming one multi-term phrase into another multi-term phrase. Comparing to traditional error models that account for transformation probabilities between single characters (Kernighan et al., 1990) or sub-word strings (Brill and Moore, 2000), the phrase-based model is more powerful in that it captures some contextual information by retaining inter-term dependencies. We show that this information is crucial to detect the correction of a query term, because unlike in regular written text, any query word can be a valid search term and in many cases the only way for a speller system to make the judgment is to explore its usage according to the contextual information. We conduct a set of experiments on a large data set, consisting of human-labeled The work was done when Xu Sun was visiting Microsoft Research Redmond. 266 Proceedings o"
P10-1028,D07-1019,0,0.747082,"ry, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the data have not been fully explored for query spelling correction. This study tries to learn error models fr"
P10-1028,D07-1021,1,0.837998,"2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as ""peace"" and ""piece"" in the context ""a _ of cake"". A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular written text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correct"
P10-1028,W04-3238,0,0.73309,"r context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular written text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain muc"
P10-1028,D09-1154,1,0.767258,"evel. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and proved to be useful for Web search (Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009) and query reformulation (Wang and Zhai, 2008; Suzuki et al., 2009). We start with this same data with the hope of achieving similar improvements in our task. The data consist of a set of query sessions that were extracted from one year of log files from a commercial Web search engine. A query session contains a query issued by a user and a ranked list of links (i.e., URLs) returned to that same user along with records of which URLs were clicked. Following Suzuki et al. (2009), we extract query-correction pairs as follows. First, we extract pairs of queries Q1 and Q2 such that (1) they are issued by the same user; (2) Q2 was issued within 3 minutes of Q1; and"
P10-1028,P02-1019,0,0.516968,"r spelling correction, any word that is not found in a pre-compiled lexicon is considered to be misspelled. Then, a list of lexical words that are similar to the misspelled word are proposed as candidate spelling corrections. Most traditional systems use a manually tuned similarity function (e.g., edit distance function) to rank the candidates, as reviewed by Kukich (1992). During the last two decades, statistical error models learned on training data (i.e., query-correction pairs) have become increasingly popular, and have proven more effective (Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as ""peace"" and ""piece"" in the context ""a _ of cake"". A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When desi"
P10-1028,D09-1093,0,0.0826042,"fore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the data have not been fully explored for query spelling correction. This study tries to learn error models from clickthrough data. To our knowledge, this is the first such attempt using clickthroug"
P10-1028,C90-2036,0,0.296919,"been little research on exploiting the data for the development of a query speller system. In this paper we present a novel method of extracting large amounts of query-correction pairs from the clickthrough data. These pairs, implicitly judged by millions of users, are used to train a set of spelling error models. Among these models, the most effective one is a phrase-based error model that captures the probability of transforming one multi-term phrase into another multi-term phrase. Comparing to traditional error models that account for transformation probabilities between single characters (Kernighan et al., 1990) or sub-word strings (Brill and Moore, 2000), the phrase-based model is more powerful in that it captures some contextual information by retaining inter-term dependencies. We show that this information is crucial to detect the correction of a query term, because unlike in regular written text, any query word can be a valid search term and in many cases the only way for a speller system to make the judgment is to explore its usage according to the contextual information. We conduct a set of experiments on a large data set, consisting of human-labeled The work was done when Xu Sun was visiting M"
P10-1028,N03-1017,0,0.165419,"first such attempt using clickthrough data. Most of the speller systems reviewed above are based on the framework of the source channel model. Typically, a language model (source model) is used to capture contextual information, while an error model (channel model) is considered to be context free in that it does not take into account any contextual information in modeling word transformation probabilities. In this study we argue that it is beneficial to capture contextual information in the error model. To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrase-based error model where we assume that query spelling correction is performed at the phrase level. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and prove"
P10-2037,W05-1506,0,0.646769,". . sn−1 is called the Viterbi outside score α(e). The goal of a kbest parsing algorithm is to compute the k best (minimum weight) inside derivations of the edge (G, 0, n). We formulate the algorithms in this paper in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003). A prioritized weighted deduction rule has the form Introduction Many situations call for a parser to return a kbest list of parses instead of a single best hypothesis.1 Currently, there are two efficient approaches known in the literature. The k-best algorithm of Jim´enez and Marzal (2000) and Huang and Chiang (2005), referred to hereafter as L AZY, operates by first performing an exhaustive Viterbi inside pass and then lazily extracting k-best lists in top-down manner. The k-best A∗ algorithm of Pauls and Klein (2009), hereafter KA∗ , computes Viterbi inside and outside scores before extracting k-best lists bottom up. Because these additional passes are only partial, KA∗ can be significantly faster than L AZY, especially when a heuristic is used (Pauls and Klein, 2009). In this paper, we propose TKA∗ , a topdown variant of KA∗ that, like L AZY, performs only an inside pass before extracting k-best lists"
P10-2037,W01-1812,1,0.776118,"on items called inside edge items I(A, i, j), which represent the many possible inside derivations of an edge (A, i, j). Inside edge items are constructed according to the IN deduction rule of Table 1. This deduction rule constructs inside edge items in a bottom-up fashion, combining items representing smaller edges I(B, i, k) and I(C, k, j) with a grammar rule r = A → B C to form a larger item I(A, i, j). The weight of a newly constructed item is given by the sum of the weights of the antecedent items and the grammar rule r, and its priority is given by hypergraph search problems as shown in Klein and Manning (2001). 201 IN∗† : IN-D† : OUT-L† : OUT-R† : OUT-D∗ : O(A, i, j) : w1 O(A, i, j) : w1 O(A, i, j) : w1 I(B, i, l) : w1 D(T B , i, l) : w2 I(B, i, l) : w2 I(B, i, l) : w2 I(C, l, j) : w2 D(T C , l, j) : w3 I(C, l, j) : w3 I(C, l, j) : w3 Q(TAG , i, j, F) : w1 I(B, i, l) : w2 I(C, l, j) : w3 w1 +w2 +wr +h(A,i,j) −−−−−−−−−−−−−−→ w +w3 +wr +w1 −−2−−− −−−−−→ w +w3 +wr +w2 −−1−−− −−−−−→ w +w2 +wr +w3 −−1−−− −−−−−→ w1 +wr +w2 +w3 +β(F ) −−−−−−−−−−−−−−−→ I(A, i, j) : w1 + w2 + wr D(T A , i, j) : w2 + w3 + wr O(B, i, l) : w1 + w3 + wr O(C, l, j) : w1 + w2 + wr Q(TBG , i, l, FC ) : w1 + wr Table 1: The deducti"
P10-2037,N03-1016,1,0.920129,"Abstract 2 We propose a top-down algorithm for extracting k-best lists from a parser. Our algorithm, TKA∗ is a variant of the kbest A∗ (KA∗ ) algorithm of Pauls and Klein (2009). In contrast to KA∗ , which performs an inside and outside pass before performing k-best extraction bottom up, TKA∗ performs only the inside pass before extracting k-best lists top down. TKA∗ maintains the same optimality and efficiency guarantees of KA∗ , but is simpler to both specify and implement. 1 Because our algorithm is very similar to KA∗ , which is in turn an extension of the (1-best) A∗ parsing algorithm of Klein and Manning (2003), we first introduce notation and review those two algorithms before presenting our new algorithm. 2.1 Notation Assume we have a PCFG2 G and an input sentence s0 . . . sn−1 of length n. The grammar G has a set of symbols denoted by capital letters, including a distinguished goal (root) symbol G. Without loss of generality, we assume Chomsky normal form: each non-terminal rule r in G has the form r = A → B C with weight wr . Edges are labeled spans e = (A, i, j). Inside derivations of an edge (A, i, j) are trees with root nonterminal A, spanning si . . . sj−1 . The weight (negative log-probabil"
P10-2037,J03-1006,0,0.0252999,"= (A, i, j). Inside derivations of an edge (A, i, j) are trees with root nonterminal A, spanning si . . . sj−1 . The weight (negative log-probability) of the best (minimum) inside derivation for an edge e is called the Viterbi inside score β(e), and the weight of the best derivation of G → s0 . . . si−1 A sj . . . sn−1 is called the Viterbi outside score α(e). The goal of a kbest parsing algorithm is to compute the k best (minimum weight) inside derivations of the edge (G, 0, n). We formulate the algorithms in this paper in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003). A prioritized weighted deduction rule has the form Introduction Many situations call for a parser to return a kbest list of parses instead of a single best hypothesis.1 Currently, there are two efficient approaches known in the literature. The k-best algorithm of Jim´enez and Marzal (2000) and Huang and Chiang (2005), referred to hereafter as L AZY, operates by first performing an exhaustive Viterbi inside pass and then lazily extracting k-best lists in top-down manner. The k-best A∗ algorithm of Pauls and Klein (2009), hereafter KA∗ , computes Viterbi inside and outside scores before extrac"
P10-2037,P09-1108,1,0.935694,"is paper in terms of prioritized weighted deduction rules (Shieber et al., 1995; Nederhof, 2003). A prioritized weighted deduction rule has the form Introduction Many situations call for a parser to return a kbest list of parses instead of a single best hypothesis.1 Currently, there are two efficient approaches known in the literature. The k-best algorithm of Jim´enez and Marzal (2000) and Huang and Chiang (2005), referred to hereafter as L AZY, operates by first performing an exhaustive Viterbi inside pass and then lazily extracting k-best lists in top-down manner. The k-best A∗ algorithm of Pauls and Klein (2009), hereafter KA∗ , computes Viterbi inside and outside scores before extracting k-best lists bottom up. Because these additional passes are only partial, KA∗ can be significantly faster than L AZY, especially when a heuristic is used (Pauls and Klein, 2009). In this paper, we propose TKA∗ , a topdown variant of KA∗ that, like L AZY, performs only an inside pass before extracting k-best lists top-down, but maintains the same optimality and efficiency guarantees as KA∗ . This algorithm can be seen as a generalization of the lattice k-best algorithm of Soong and Huang (1991) to parsing. Because TK"
P10-2037,P06-1055,1,0.677118,"Missing"
P11-1131,2009.eamt-1.23,0,0.575459,"Missing"
P11-1131,W04-3216,0,0.0424911,"Missing"
P11-1131,W06-3105,0,0.110753,"-word links. ments, such English not to French ne ? pas. Pruning the set of allowed phrases preserves the time complexity of the word-to-word HMM alignment model. 1.1 Related Work Our first major influence is that of conditional phrase-based models. An early approach by Deng and Byrne (2005) changed the parameterization of the traditional word-based HMM model, modeling subsequent words from the same state using a bigram model. However, this model changes only the parameterization and not the set of possible alignments. More closely related are the approaches of Daum´e III and Marcu (2004) and DeNero et al. (2006), which allow phrase-to-phrase alignments between the source and target domain. As DeNero warns, though, an unconstrained model may overfit using unusual segmentations. Interestingly, the phrase-based hidden semi-Markov model of Andr´es-Ferrer and Juan (2009) does not seem to encounter these problems. We suspect two main causes: first, the model interpolates with Model 1 (Brown et al., 1994), which may help prevent overfitting, and second, the model is monotonic, which screens out many possible alignments. Monotonicity is generally undesirable, though: almost all parallel sentences exhibit som"
P11-1131,H05-1022,0,0.192963,"Figure 2: The model of E given F can represent the phrasal alignment {e1 , e2 } ∼ {f1 }. However, the model of F given E cannot: the probability mass is distributed between {e1 } ∼ {f1 } and {e2 } ∼ {f1 }. Agreement of the forward and backward HMM alignments tends to place less mass on phrasal links and greater mass on word-to-word links. ments, such English not to French ne ? pas. Pruning the set of allowed phrases preserves the time complexity of the word-to-word HMM alignment model. 1.1 Related Work Our first major influence is that of conditional phrase-based models. An early approach by Deng and Byrne (2005) changed the parameterization of the traditional word-based HMM model, modeling subsequent words from the same state using a bigram model. However, this model changes only the parameterization and not the set of possible alignments. More closely related are the approaches of Daum´e III and Marcu (2004) and DeNero et al. (2006), which allow phrase-to-phrase alignments between the source and target domain. As DeNero warns, though, an unconstrained model may overfit using unusual segmentations. Interestingly, the phrase-based hidden semi-Markov model of Andr´es-Ferrer and Juan (2009) does not see"
P11-1131,P10-1016,0,0.0111129,"h as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find that agreement discou"
P11-1131,J07-3002,0,0.0790977,"Missing"
P11-1131,N10-1140,0,0.0267577,"ligned sentence pairs. models prevents EM from overfitting, even in the absence of harsh penalties. We also allow gappy (noncontiguous) phrases on the state side, which makes agreement more successful but agreement needs approximation of posterior marginals. Using pruned lists of good phrases, we maintain complexity equal to the baseline word-to-word model. There are several steps forward from this point. Limiting the gap length also prevents combinatorial explosion; we hope to explore this in future work. Clearly a translation system that uses discontinuous mappings at runtime (Chiang, 2007; Galley and Manning, 2010) may make better use of discontinuous alignments. This model can also be applied at the morpheme or character level, allowing joint inference of segmentation and alignment. Furthermore the state space could be expanded and enhanced to include more possibilities: states with multiple gaps might be useful for alignment in languages with template morphology, such as Arabic or Hebrew. More exploration in the model space could be useful – a better distortion model might place a stronger distribution on the likely starting and ending points of phrases. Acknowledgments We would like to thank the anon"
P11-1131,N04-1035,0,0.0383112,"arkov Models, while maintaining asymptotically equivalent runtime. 1 English would not like traveling by railroad Figure 1: French-English pair with complex word alignment. Introduction Word alignment is an important part of statistical machine translation (MT) pipelines. Phrase tables containing pairs of source and target language phrases are extracted from word alignments, forming the core of phrase-based statistical machine translation systems (Koehn et al., 2003). Most syntactic machine translation systems extract synchronous context-free grammars (SCFGs) from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006), which in turn are derived from bilingual word alignments and syntactic ∗ Author was a summer intern at Microsoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this proble"
P11-1131,N03-1017,0,0.0294345,"ontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime. 1 English would not like traveling by railroad Figure 1: French-English pair with complex word alignment. Introduction Word alignment is an important part of statistical machine translation (MT) pipelines. Phrase tables containing pairs of source and target language phrases are extracted from word alignments, forming the core of phrase-based statistical machine translation systems (Koehn et al., 2003). Most syntactic machine translation systems extract synchronous context-free grammars (SCFGs) from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006), which in turn are derived from bilingual word alignments and syntactic ∗ Author was a summer intern at Microsoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are n"
P11-1131,W06-2402,0,0.0162878,"also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this"
P11-1131,N06-1014,0,0.704607,"ns (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find that agreement discourages EM from overfitting. Finally, we make the alignment space more symmetric by including gappy (or non-contiguous) phrases. This allows agreement to reinforce non-contiguous align1308 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1308–1317, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Observations→ e1 e2 e3 f1 f2 f3 States→ f1 e1 ? f2 e2 ? f3 e3 HMM(E|F) HMM(F|E) Figure 2: The model of E given F can represent the phrasal alignment {e1 , e2 } ∼ {f1 }. However"
P11-1131,P07-1039,0,0.0160259,"NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find t"
P11-1131,W02-1018,0,0.0409262,"from bilingual word alignments and syntactic ∗ Author was a summer intern at Microsoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous seq"
P11-1131,J03-1002,0,0.202395,"osoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We"
P11-1131,C96-2141,0,0.958377,"-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find that agreement discourages EM from overfitting. Finally, we make the alignment space more symmetric by including gappy (or non-contiguous) phrases. This allows agreement to reinforce non-contiguous align1308 Proceedings of the 49th Annual Meeting of the Association fo"
P11-1131,J97-3002,0,0.620845,"zation and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for direct phrasal alignment (Marcu and Wong, 2002), heuristic word-alignment combinations (Koehn et al., 2003; Och and Ney, 2003), models with pseudoword collocations (Lambert and Banchs, 2006; Ma et al., 2007; Duan et al., 2010), synchronous grammar based approaches (Wu, 1997), etc. Most have a large state-space, using constraints and approximations for efficient inference. We present a new phrasal alignment model based on the hidden Markov framework (Vogel et al., 1996). Our approach is semi-Markov: each state can generate multiple observations, representing wordto-phrase alignments. We also augment the state space to include contiguous sequences. This corresponds to phrase-to-word and phrase-to-phrase alignments. We generalize alignment by agreement (Liang et al., 2006) to this space, and find that agreement discourages EM from overfitting. Finally, we make the a"
P11-1131,W06-3119,0,0.016468,"aintaining asymptotically equivalent runtime. 1 English would not like traveling by railroad Figure 1: French-English pair with complex word alignment. Introduction Word alignment is an important part of statistical machine translation (MT) pipelines. Phrase tables containing pairs of source and target language phrases are extracted from word alignments, forming the core of phrase-based statistical machine translation systems (Koehn et al., 2003). Most syntactic machine translation systems extract synchronous context-free grammars (SCFGs) from aligned syntactic fragments (Galley et al., 2004; Zollmann et al., 2006), which in turn are derived from bilingual word alignments and syntactic ∗ Author was a summer intern at Microsoft Research during this project. parses. Alignment is also used in various other NLP problems such as entailment, paraphrasing, question answering, summarization and spelling correction. A limitation to word-based alignment is undesirable. As seen in the French-English example in Figure 1, many sentence pairs are naturally aligned with multi-word units in both languages (chemin de fer; would ? like, where ? indicates a gap). Much work has addressed this problem: generative models for"
P11-1131,J93-2003,0,\N,Missing
P11-1131,J07-2003,0,\N,Missing
P13-1164,aggarwal-etal-2012-twins,0,0.0170181,"s. 1 U: “I want to add page numbers and a title” S: “Top or Bottom of the page?” U: “Top” S: “Please select page design from the templates” (*System shows drop down menu*) U: *User selects from menu* S: “Enter header or footer content” U: “C.V.” S: “Task completed.” Figure 1: An example dialog interaction between a system (S) and user (U) that can be automatically achieved by learning from instructional web page and query click logs. Introduction Procedural dialog systems aim to assist users with a wide range of goals. For example, they can guide visitors through a museum (Traum et al., 2012; Aggarwal et al., 2012), teach students physics (Steinhauser et al., 2011; Dzikovska et al., 2011), or enable interaction with a health care system (Morbini et al., 2012; Rizzo et al., 2011). However, such systems are challenging to build, currently requiring expensive, expert engineering of significant domain-specific task knowledge and dialog management strategies. In this paper, we present a new approach for learning procedural dialog systems from taskoriented textual resources in combination with light, non-expert supervision. Specifically, we assume access to task knowledge in textual form (e.g., instructional"
P13-1164,Q13-1005,1,0.816066,"s that (A and B) complete successfully, (C) have a redundant question, and (D) fail to recognize the user intent. 7 Related work To the best of our knowledge, this paper presents the first effort to induce full procedural dialog systems from instructional text and query click logs. Grounded Language Learning There has been significant interest in grounded language learning. Perhaps the most closely related work learns to understand instructions and automati1676 cally complete the tasks they describe (Branavan et al., 2009; Vogel and Jurafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of video snippets (Chen and Dolan, 2011). Dialog Generation from Text Similarly to Piwek’s work (2007; 201"
P13-1164,P09-1010,1,0.828959,"t” S: *System Restarts* (D) Figure 6: Four example dialogs from the user study, including cases that (A and B) complete successfully, (C) have a redundant question, and (D) fail to recognize the user intent. 7 Related work To the best of our knowledge, this paper presents the first effort to induce full procedural dialog systems from instructional text and query click logs. Grounded Language Learning There has been significant interest in grounded language learning. Perhaps the most closely related work learns to understand instructions and automati1676 cally complete the tasks they describe (Branavan et al., 2009; Vogel and Jurafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of vi"
P13-1164,P10-1129,1,0.884129,"r study, including cases that (A and B) complete successfully, (C) have a redundant question, and (D) fail to recognize the user intent. 7 Related work To the best of our knowledge, this paper presents the first effort to induce full procedural dialog systems from instructional text and query click logs. Grounded Language Learning There has been significant interest in grounded language learning. Perhaps the most closely related work learns to understand instructions and automati1676 cally complete the tasks they describe (Branavan et al., 2009; Vogel and Jurafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of video snippets (Chen and Dolan, 2011). Dialog Generation from Text Simila"
P13-1164,P11-1028,0,0.0163665,"procedural dialog systems from instructional text and query click logs. Grounded Language Learning There has been significant interest in grounded language learning. Perhaps the most closely related work learns to understand instructions and automati1676 cally complete the tasks they describe (Branavan et al., 2009; Vogel and Jurafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of video snippets (Chen and Dolan, 2011). Dialog Generation from Text Similarly to Piwek’s work (2007; 2010; 2011), we study extracting dialog knowledge from documents (monologues or instructions). However, Piwek’s approach generates static dialogs, for example to generate animations of virtual characters havi"
P13-1164,P12-1035,0,0.0266953,"dialogs, for example to generate animations of virtual characters having a conversation. There is no model of dialog management or user interaction, and the approach does not use any machine learning. In contrast, to the best of our knowledge, we are the first to demonstrate it is possible to learn complete, interactive dialog systems using instructional texts (and nonexpert annotation). Learning from Web Query Logs Web query logs have been extensively studied. For example, they are widely used to represent user intents in spoken language dialogs (T¨ur et al., 2011; Celikyilmaz et al., 2011; Celikyilmaz and Hakkani-Tur, 2012). Web query logs are also used in many other NLP tasks, including entity linking (Pantel et al., 2012) and training product and job intent classifiers (Li et al., 2008). Dialog Modeling and User Simulation Many existing dialog systems learn dialog strategies from user interactions (Young, 2010; Rieser and Lemon, 2008). Moreover, dialog data is often limited and, therefore, user simulation is commonly used (Scheffler and Young, 2002; Schatzmann et al., 2006; Georgila et al., 2005). Our overall approach is also related to many other dialog management approaches, including those that construct di"
P13-1164,P09-1011,0,0.022129,"and automati1676 cally complete the tasks they describe (Branavan et al., 2009; Vogel and Jurafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of video snippets (Chen and Dolan, 2011). Dialog Generation from Text Similarly to Piwek’s work (2007; 2010; 2011), we study extracting dialog knowledge from documents (monologues or instructions). However, Piwek’s approach generates static dialogs, for example to generate animations of virtual characters having a conversation. There is no model of dialog management or user interaction, and the approach does not use any machine learning. In contrast, to the best of our knowledge, we are the first to demonstrate it is possible to learn compl"
P13-1164,W12-1620,0,0.0313178,"shows drop down menu*) U: *User selects from menu* S: “Enter header or footer content” U: “C.V.” S: “Task completed.” Figure 1: An example dialog interaction between a system (S) and user (U) that can be automatically achieved by learning from instructional web page and query click logs. Introduction Procedural dialog systems aim to assist users with a wide range of goals. For example, they can guide visitors through a museum (Traum et al., 2012; Aggarwal et al., 2012), teach students physics (Steinhauser et al., 2011; Dzikovska et al., 2011), or enable interaction with a health care system (Morbini et al., 2012; Rizzo et al., 2011). However, such systems are challenging to build, currently requiring expensive, expert engineering of significant domain-specific task knowledge and dialog management strategies. In this paper, we present a new approach for learning procedural dialog systems from taskoriented textual resources in combination with light, non-expert supervision. Specifically, we assume access to task knowledge in textual form (e.g., instructional web pages) and examples of user intent statements (e.g., search query logs or dialog interactions). Such instructional resources are available in"
P13-1164,P11-1020,0,0.0164534,"rafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of video snippets (Chen and Dolan, 2011). Dialog Generation from Text Similarly to Piwek’s work (2007; 2010; 2011), we study extracting dialog knowledge from documents (monologues or instructions). However, Piwek’s approach generates static dialogs, for example to generate animations of virtual characters having a conversation. There is no model of dialog management or user interaction, and the approach does not use any machine learning. In contrast, to the best of our knowledge, we are the first to demonstrate it is possible to learn complete, interactive dialog systems using instructional texts (and nonexpert annotation). Learning"
P13-1164,P12-1059,0,0.0173017,"g management or user interaction, and the approach does not use any machine learning. In contrast, to the best of our knowledge, we are the first to demonstrate it is possible to learn complete, interactive dialog systems using instructional texts (and nonexpert annotation). Learning from Web Query Logs Web query logs have been extensively studied. For example, they are widely used to represent user intents in spoken language dialogs (T¨ur et al., 2011; Celikyilmaz et al., 2011; Celikyilmaz and Hakkani-Tur, 2012). Web query logs are also used in many other NLP tasks, including entity linking (Pantel et al., 2012) and training product and job intent classifiers (Li et al., 2008). Dialog Modeling and User Simulation Many existing dialog systems learn dialog strategies from user interactions (Young, 2010; Rieser and Lemon, 2008). Moreover, dialog data is often limited and, therefore, user simulation is commonly used (Scheffler and Young, 2002; Schatzmann et al., 2006; Georgila et al., 2005). Our overall approach is also related to many other dialog management approaches, including those that construct dialog graphs from dialog data via clustering (Lee et al., 2009), learn information state updates using"
P13-1164,N10-1048,0,0.0703818,"Missing"
P13-1164,P11-2042,0,0.0420886,"Missing"
P13-1164,P08-1073,0,0.0271142,"systems using instructional texts (and nonexpert annotation). Learning from Web Query Logs Web query logs have been extensively studied. For example, they are widely used to represent user intents in spoken language dialogs (T¨ur et al., 2011; Celikyilmaz et al., 2011; Celikyilmaz and Hakkani-Tur, 2012). Web query logs are also used in many other NLP tasks, including entity linking (Pantel et al., 2012) and training product and job intent classifiers (Li et al., 2008). Dialog Modeling and User Simulation Many existing dialog systems learn dialog strategies from user interactions (Young, 2010; Rieser and Lemon, 2008). Moreover, dialog data is often limited and, therefore, user simulation is commonly used (Scheffler and Young, 2002; Schatzmann et al., 2006; Georgila et al., 2005). Our overall approach is also related to many other dialog management approaches, including those that construct dialog graphs from dialog data via clustering (Lee et al., 2009), learn information state updates using discriminative classification models (Hakkani-Tur et al., 2012; Mairesse et al., 2009), optimize dialog strategy using reinforcement learning (RL) (Scheffler and Young, 2002; Rieser and Lemon, 2008), or combine RL wit"
P13-1164,N07-1034,0,0.0204458,"imited and, therefore, user simulation is commonly used (Scheffler and Young, 2002; Schatzmann et al., 2006; Georgila et al., 2005). Our overall approach is also related to many other dialog management approaches, including those that construct dialog graphs from dialog data via clustering (Lee et al., 2009), learn information state updates using discriminative classification models (Hakkani-Tur et al., 2012; Mairesse et al., 2009), optimize dialog strategy using reinforcement learning (RL) (Scheffler and Young, 2002; Rieser and Lemon, 2008), or combine RL with information state update rules (Heeman, 2007). However, our approach is unique in the use of inducing task and domain knowledge with light supervision to assist the user with many goals. 8 Conclusions and Future Work This paper presented a novel approach for automatically constructing procedural dialog systems with light supervision, given only textual resources such as instructional text and search query click logs. Evaluations demonstrated highly accurate performance, on automatic benchmarks and through a user study. Although we showed it is possible to build complete systems, more work will be required to scale the approach to new dom"
P13-1164,N09-2023,0,0.0152868,"NLP tasks, including entity linking (Pantel et al., 2012) and training product and job intent classifiers (Li et al., 2008). Dialog Modeling and User Simulation Many existing dialog systems learn dialog strategies from user interactions (Young, 2010; Rieser and Lemon, 2008). Moreover, dialog data is often limited and, therefore, user simulation is commonly used (Scheffler and Young, 2002; Schatzmann et al., 2006; Georgila et al., 2005). Our overall approach is also related to many other dialog management approaches, including those that construct dialog graphs from dialog data via clustering (Lee et al., 2009), learn information state updates using discriminative classification models (Hakkani-Tur et al., 2012; Mairesse et al., 2009), optimize dialog strategy using reinforcement learning (RL) (Scheffler and Young, 2002; Rieser and Lemon, 2008), or combine RL with information state update rules (Heeman, 2007). However, our approach is unique in the use of inducing task and domain knowledge with light supervision to assist the user with many goals. 8 Conclusions and Future Work This paper presented a novel approach for automatically constructing procedural dialog systems with light supervision, given"
P13-1164,P10-1083,0,0.0224588,"(D) Figure 6: Four example dialogs from the user study, including cases that (A and B) complete successfully, (C) have a redundant question, and (D) fail to recognize the user intent. 7 Related work To the best of our knowledge, this paper presents the first effort to induce full procedural dialog systems from instructional text and query click logs. Grounded Language Learning There has been significant interest in grounded language learning. Perhaps the most closely related work learns to understand instructions and automati1676 cally complete the tasks they describe (Branavan et al., 2009; Vogel and Jurafsky, 2010; Kushman et al., 2009; Branavan et al., 2010; Artzi and Zettlemoyer, 2013). However, these approaches did not model user interaction. There are also many related approaches for other grounded language problems, including understanding game strategy guides (Branavan et al., 2011), modeling users goals in a Windows domain (Horvitz et al., 1998), learning from conversational interaction (Artzi and Zettlemoyer, 2011), learning to sportscast (Chen and Mooney, 2011), learning from event streams (Liang et al., 2009), and learning paraphrases from crowdsourced captions of video snippets (Chen and Dol"
P13-1164,W12-1816,0,\N,Missing
P13-2002,N10-1083,0,0.0582041,"Missing"
P13-2002,J93-2003,0,0.0905698,"ithm for finding the single best alignment with a fertility HMM. Finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality. 1 Introduction Word-based translation models intended to model the translation process have found new uses identifying word correspondences in sentence pairs. These word alignments are a crucial training component in most machine translation systems. Furthermore, they are useful in other NLP applications, such as entailment identification. The simplest models may use lexical information alone. The seminal Model 1 (Brown et al., 1993) has proved very powerful, performing nearly as well as more complicated models in some phrasal systems (Koehn et al., 2003). With minor improvements to initialization (Moore, 2004) (which may be important (Toutanova and Galley, 2011)), it can be quite competitive. Subsequent IBM models include more detailed information about context. Models 7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7–11, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 HMM alignment estimate the posterior distribution using Markov chain"
P13-2002,W07-0711,0,0.0229852,"earby the French word aligned to the previous English word). Models 3, 4, and 5 all incorporate a notion of “fertility”: the number of French words that align to any English word. Although these latter models covered a broad range of phenomena, estimation techniques and MAP inference were challenging. The authors originally recommended heuristic procedures based on local search for both. Such methods work reasonably well, but can be computationally inefficient and have few guarantees. Thus, many researchers have switched to the HMM model (Vogel et al., 1996) and variants with more parameters (He, 2007). This captures the positional information in the IBM models in a framework that admits exact parameter estimation inference, though the objective function is not concave: local maxima are a concern. Modeling fertility is challenging in the HMM framework as it violates the Markov assumption. Where the HMM jump model considers only the prior state, fertility requires looking across the whole state space. Therefore, the standard forward-backward and Viterbi algorithms do not apply. Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques"
P13-2002,N03-1017,0,0.0214251,"Missing"
P13-2002,P04-1066,0,0.0322677,"oduction Word-based translation models intended to model the translation process have found new uses identifying word correspondences in sentence pairs. These word alignments are a crucial training component in most machine translation systems. Furthermore, they are useful in other NLP applications, such as entailment identification. The simplest models may use lexical information alone. The seminal Model 1 (Brown et al., 1993) has proved very powerful, performing nearly as well as more complicated models in some phrasal systems (Koehn et al., 2003). With minor improvements to initialization (Moore, 2004) (which may be important (Toutanova and Galley, 2011)), it can be quite competitive. Subsequent IBM models include more detailed information about context. Models 7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7–11, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 HMM alignment estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010). In this case, we make some initial estimate of the a vector, potentially randomly. We then repeatedly resample e"
P13-2002,P11-2081,0,0.0157218,"intended to model the translation process have found new uses identifying word correspondences in sentence pairs. These word alignments are a crucial training component in most machine translation systems. Furthermore, they are useful in other NLP applications, such as entailment identification. The simplest models may use lexical information alone. The seminal Model 1 (Brown et al., 1993) has proved very powerful, performing nearly as well as more complicated models in some phrasal systems (Koehn et al., 2003). With minor improvements to initialization (Moore, 2004) (which may be important (Toutanova and Galley, 2011)), it can be quite competitive. Subsequent IBM models include more detailed information about context. Models 7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7–11, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 HMM alignment estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010). In this case, we make some initial estimate of the a vector, potentially randomly. We then repeatedly resample each element of that vector conditioned on all other p"
P13-2002,C96-2141,0,0.298924,"English word tends to align to a French word that is nearby the French word aligned to the previous English word). Models 3, 4, and 5 all incorporate a notion of “fertility”: the number of French words that align to any English word. Although these latter models covered a broad range of phenomena, estimation techniques and MAP inference were challenging. The authors originally recommended heuristic procedures based on local search for both. Such methods work reasonably well, but can be computationally inefficient and have few guarantees. Thus, many researchers have switched to the HMM model (Vogel et al., 1996) and variants with more parameters (He, 2007). This captures the positional information in the IBM models in a framework that admits exact parameter estimation inference, though the objective function is not concave: local maxima are a concern. Modeling fertility is challenging in the HMM framework as it violates the Markov assumption. Where the HMM jump model considers only the prior state, fertility requires looking across the whole state space. Therefore, the standard forward-backward and Viterbi algorithms do not apply. Recent work (Zhao and Gildea, 2010) described an extension to the HMM"
P13-2002,D10-1058,0,0.256683,"chers have switched to the HMM model (Vogel et al., 1996) and variants with more parameters (He, 2007). This captures the positional information in the IBM models in a framework that admits exact parameter estimation inference, though the objective function is not concave: local maxima are a concern. Modeling fertility is challenging in the HMM framework as it violates the Markov assumption. Where the HMM jump model considers only the prior state, fertility requires looking across the whole state space. Therefore, the standard forward-backward and Viterbi algorithms do not apply. Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation. However, they do not have a efficient means of MAP inference, which is necessary in many applications such as machine translation. This paper introduces a method for exact MAP inference with the fertility HMM using dual decomposition. The resulting model leads to substantial improvements in alignment quality. The notion of fertility in word alignment (the number of words emitted by a single state) is useful but difficult to model. Initial attempts at modeling fertility used heuristic sear"
P13-2040,N03-1003,0,0.327677,"across the floor by a boy. That is difficult with lattice representations. Consider the following context free grammar: S → X0 X1 | X2 X3 X0 → a man |a boy Introduction X1 → is sliding X2 on X4 | Humans can construct a broad range of descriptions for almost any object or event. In this paper, we will refer to such objects or events as groundings, in the sense of grounded semantics. Examples of groundings include pictures (Rashtchian et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). One crucial problem is recognizing whether novel utterances are relevant descriptions of those groundings. In the case of machine translation, this is the evaluation problem; for images and videos, this is recognition and retrieval. Generating descriptions of events is also often an interesting task: we might like to find a novel paraphrase for a given sentence, or generate a description of a grounding that meets certain criteria (e.g., brevity, use of a restricted vocabulary). is cleaning X4 with X2 X2 → a cat |the cat X3 → is being pushed across X4 by X0 X4 → the floor This grammar compact"
P13-2040,P11-1020,0,0.120233,"by a man. Ideally we would like to recognize that the following utterance is also a valid description of that event: A cat is being pushed across the floor by a boy. That is difficult with lattice representations. Consider the following context free grammar: S → X0 X1 | X2 X3 X0 → a man |a boy Introduction X1 → is sliding X2 on X4 | Humans can construct a broad range of descriptions for almost any object or event. In this paper, we will refer to such objects or events as groundings, in the sense of grounded semantics. Examples of groundings include pictures (Rashtchian et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). One crucial problem is recognizing whether novel utterances are relevant descriptions of those groundings. In the case of machine translation, this is the evaluation problem; for images and videos, this is recognition and retrieval. Generating descriptions of events is also often an interesting task: we might like to find a novel paraphrase for a given sentence, or generate a description of a grounding that meets certain criteria (e.g., brevity, use of"
P13-2040,N12-1017,0,0.031805,"e is also a valid description of that event: A cat is being pushed across the floor by a boy. That is difficult with lattice representations. Consider the following context free grammar: S → X0 X1 | X2 X3 X0 → a man |a boy Introduction X1 → is sliding X2 on X4 | Humans can construct a broad range of descriptions for almost any object or event. In this paper, we will refer to such objects or events as groundings, in the sense of grounded semantics. Examples of groundings include pictures (Rashtchian et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). One crucial problem is recognizing whether novel utterances are relevant descriptions of those groundings. In the case of machine translation, this is the evaluation problem; for images and videos, this is recognition and retrieval. Generating descriptions of events is also often an interesting task: we might like to find a novel paraphrase for a given sentence, or generate a description of a grounding that meets certain criteria (e.g., brevity, use of a restricted vocabulary). is cleaning X4 with X2 X2 → a cat |the cat X3 →"
P13-2040,P03-1054,0,0.00468085,"not only the seed sentences, but also a broad range of nearby sentences. In the case above with cat, man, and boy, we would be able to generate cases legitimate variants where man was replaced by boy as well as undesired variants where man is replaced by cat or floor. This initial grammar captures a large neighborhood of nearby utterances including many such undesirable ones. Therefore, we refine the grammar. Refinements have been in common use in syntactic parsing for years now. Inspired by the result that manual annotations of Treebank categories can substantially increase parser accuracy (Klein and Manning, 2003), several approaches have been introduced to automatically induce latent symbols on existing trees. We use the splitmerge method commonly used in syntactic parsing (Petrov et al., 2006). In its original setting, 223 (a) Input: grammar using these fractional counts. • the man plays the piano • the guy plays the keyboard P (Xi → Yj Zk ) = (b) Parses: In Petrov et al., these latent refinements are later discarded as the goal is to find the best parse with the original coarse symbols. Here, we retain the latent refinements during parsing, since they distinguish semantically related utterances from"
P13-2040,J00-1003,0,0.0219996,"dditional adjectives, for instance. In this work, we perform a very simple form of smoothing. If the fractional count of a word given a pre-terminal symbol falls below a threshold k, then we consider that instance rare and reserve a fraction of its probability mass for unseen words. This accounts for lexical variation of the grounding, especially in the least consistently used words. Substantial speedups could be attained by using finite state approximations of this grammar: matching complexity drops to cubic to linear in the length of the input. A broad range of approximations are available (Nederhof, 2000). Since the small grammars in our evaluation below seldom exhibit self-embedding (latent state identification (d) Refined grammar: S NP0 NP1 NP DT NN0 NN1 VBZ c(Xi , Yj , Zk ) c(Xi ) NP0 VP DT NN0 DT NN1 VBZ NP1 the man |guy piano |keyboard plays Figure 1: Example of hypergraph induction. First a conventional Treebank parser converts input utterances (a) into parse trees (b). A grammar could be directly read from this small treebank, but it would conflate all phrases of the same type. Instead we induce latent refinements of this small treebank (c). The resulting grammar (d) can match and gener"
P13-2040,N03-1024,0,0.10288,"Missing"
P13-2040,P06-1055,0,0.0138734,"aced by boy as well as undesired variants where man is replaced by cat or floor. This initial grammar captures a large neighborhood of nearby utterances including many such undesirable ones. Therefore, we refine the grammar. Refinements have been in common use in syntactic parsing for years now. Inspired by the result that manual annotations of Treebank categories can substantially increase parser accuracy (Klein and Manning, 2003), several approaches have been introduced to automatically induce latent symbols on existing trees. We use the splitmerge method commonly used in syntactic parsing (Petrov et al., 2006). In its original setting, 223 (a) Input: grammar using these fractional counts. • the man plays the piano • the guy plays the keyboard P (Xi → Yj Zk ) = (b) Parses: In Petrov et al., these latent refinements are later discarded as the goal is to find the best parse with the original coarse symbols. Here, we retain the latent refinements during parsing, since they distinguish semantically related utterances from unrelated utterances. Note in Figure 1 how NN0 and NN1 refer to different objects; were we to ignore that distinction, the parser would recognize semantically different utterances such"
P13-2040,N12-3006,1,0.839055,"s from unrelated or merely related utterances, and this line may not be consistent between judges. Annotating whether an utterance clearly describes a grounding is a much easier task. This paper describes a simple method for constructing hypergraph-shaped Semantic Neighborhoods from sets of expressions describing the same grounding. The method is evaluated in a paraphrase recognition task, inspired by a CAPTCHA task (Von Ahn et al., 2003). 2 2.1 Split-merge induction We begin with a set of utterances that describe a specific grounding. They are parsed with a conventional Penn Treebank parser (Quirk et al., 2012) to produce a type of treebank. Unlike conventional treebanks which are annotated by human experts, the trees here are automatically created and thus are more likely to contain errors. This treebank is the input to the split-merge process. Split: Given an input treebank, we propose refinements of the symbols in hopes of increasing the likelihood of the data. For each original symbol in the grammar such as NP, we consider two latent refinements: NP0 and NP1 . Each binary rule then produces 8 possible variants, since the parent, left child, and right child now have two possible refinements. The"
P13-2040,W10-0721,0,0.0141114,"t is being pushed across the floor by a man. Ideally we would like to recognize that the following utterance is also a valid description of that event: A cat is being pushed across the floor by a boy. That is difficult with lattice representations. Consider the following context free grammar: S → X0 X1 | X2 X3 X0 → a man |a boy Introduction X1 → is sliding X2 on X4 | Humans can construct a broad range of descriptions for almost any object or event. In this paper, we will refer to such objects or events as groundings, in the sense of grounded semantics. Examples of groundings include pictures (Rashtchian et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). One crucial problem is recognizing whether novel utterances are relevant descriptions of those groundings. In the case of machine translation, this is the evaluation problem; for images and videos, this is recognition and retrieval. Generating descriptions of events is also often an interesting task: we might like to find a novel paraphrase for a given sentence, or generate a description of a grounding that meets certain"
P13-2040,W10-0707,0,\N,Missing
P14-1064,W13-2233,0,0.0514209,"ys (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use higher order n-grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn transla"
P14-1064,N09-1014,0,0.020533,"Missing"
P14-1064,N13-1056,0,0.495081,"ys (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use higher order n-grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn transla"
P14-1064,E12-1014,0,0.0149254,"uct a more extensive evaluation of their graph-based BLI techniques, where the emphasis and end-to-end BLEU evaluations concentrated on OOVs, i.e., unigrams, and not on enriching the entire translation model. As with previous BLI work, these approaches only take into account source-side similarity of words; only moderate gains (and in the latter work, on a subset of language pairs evaluated) are obtained. Additionally, because of our structured propagation algorithm, our approach is better at handling multiple translation candidates and does not need to restrict itself to the top translation. Klementiev et al. (2012) propose a method that utilizes a pre-existing phrase table and a small bilingual lexicon, and performs BLI using monolingual corpora. The operational scope of their approach is limited in that they assume a scenario where unknown phrase pairs are provided (thereby sidestepping the issue of translation candidate generation for completely unknown phrases), and what remains is the estimation of phrasal probabilities. In our case, we obtain the phrase pairs from the graph structure (and therefore indirectly from the monolingual data) and a separate generation step, which plays an important role i"
P14-1064,N06-1003,0,0.182311,"corpora. Next, graph propagation identifies translations of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets. 1 The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara e"
P14-1064,W02-0902,0,0.141694,"are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. The idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only. Recent improvements to BLI (Tamura et al., 2012; Irvine and Callison-Burch, 2013b) have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams. Razmara et al. (2013) and Irvine and CallisonBurch (2013a) conduct a more extensive evaluation of their graph-based BLI techniques, where th"
P14-1064,D13-1174,0,0.019094,"e 677 of the sizes of these spaces on the length of instances is to blame. Thus, the target phrase inventory from the parallel corpus may be inadequate for unlabeled instances. We therefore need to enrich the target or label space for unknown phrases. A na¨ıve way to achieve this goal would be to extract all n-grams, from n = 1 to a maximum ngram order, from the monolingual data, but this strategy would lead to a combinatorial explosion in the number of target phrases. Instead, by intelligently expanding the target space using linguistic information such as morphology (Toutanova et al., 2008; Chahuneau et al., 2013), or relying on the baseline system to generate candidates similar to self-training (McClosky et al., 2006), we can tractably propose novel translation candidates (white nodes in Fig. 1’s target graph) whose probabilities are then estimated during propagation. We refer to these additional candidates as “generated” candidates. To generate new translation candidates using the baseline system, we decode each unlabeled source bigram to generate its m-best translations. This set of candidate phrases is filtered to include only n-grams occurring in the target monolingual corpus, and helps to prune p"
P14-1064,N03-1017,0,0.149168,"ach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with their probabilities. With large amounts of data, phrase-based translation systems (Koehn et al., 2003; Chiang, 2007) achieve state-of-the-art results in many typologically diverse language pairs (Bojar et al., 2013). However, the limiting factor in the success of these techniques is parallel data availability. Even in resource-rich languages, learning reliable translations of multiword phrases is a challenge, and an adequate phrasal inventory is crucial We then limit the set of translation options for each unlabeled source phrase (§2.3), and using a structured graph propagation algorithm, where translation information is propagated from labeled to unlabeled phrases proportional to both source"
P14-1064,J07-2003,0,0.0595165,"mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with their probabilities. With large amounts of data, phrase-based translation systems (Koehn et al., 2003; Chiang, 2007) achieve state-of-the-art results in many typologically diverse language pairs (Bojar et al., 2013). However, the limiting factor in the success of these techniques is parallel data availability. Even in resource-rich languages, learning reliable translations of multiword phrases is a challenge, and an adequate phrasal inventory is crucial We then limit the set of translation options for each unlabeled source phrase (§2.3), and using a structured graph propagation algorithm, where translation information is propagated from labeled to unlabeled phrases proportional to both source and target phr"
P14-1064,P12-1032,0,0.243142,"of a certain number of classes. In our problem, the “label” for each node is actually a probability distribution over a set of translation candidates (target phrases). For a given node f , let e refer to a candidate in the label set for node f ; then in graph propagation, the probability of candidate e given source phrase f in iteration t + 1 is: X Pt+1 (e|f ) = Ts (j|f )Pt (e|j) (1) j2N (f ) 2.4.1 Structured Label Propagation The label set we are considering has a similarity structure encoded by the target graph. How can we exploit this structure in graph propagation on the source graph? In Liu et al. (2012), the authors generalize label propagation to structured label propagation (SLP) in an effort to work more elegantly with structured labels. In particular, the definition of target similarity is similar to that of source similarity: where the set N (f ) contains the (labeled and unlabeled) neighbors of node f , and Ts (j|f ) is a term that captures how similar nodes f and j are. This quantity is also known as the propagation probability, and its exact form will depend on the type of graph propagation algorithm used. For our purposes, node f is a source phrasal node, the set N (f ) refers to ot"
P14-1064,D12-1025,0,0.0314991,"veral different angles. Paraphrases extracted by “pivoting” via a third language (Callison-Burch et al., 2006) can be derived solely from monolingual corpora using distributional similarity (Marton et al., 2009). Snover et al. (2008) use cross-lingual information retrieval techniques to find potential sentence-level translation candidates among comparable corpora. In this case, the goal is to try and construct a corpus as close to parallel as possible from comparable corpora, and is a fairly different take on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. The idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translatio"
P14-1064,D09-1040,0,0.0271007,"he level of sentences, and while the latter does extend the framework to sub-spans of sentences, they do not discover new translation pairs or phrasal probabilities for new pairs at all, but instead re-estimate phrasal probabilities using the graph structure and add this score as an additional feature during decoding. The goal of leveraging non-parallel data in machine translation has been explored from several different angles. Paraphrases extracted by “pivoting” via a third language (Callison-Burch et al., 2006) can be derived solely from monolingual corpora using distributional similarity (Marton et al., 2009). Snover et al. (2008) use cross-lingual information retrieval techniques to find potential sentence-level translation candidates among comparable corpora. In this case, the goal is to try and construct a corpus as close to parallel as possible from comparable corpora, and is a fairly different take on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. The idea presented in this paper is similar in"
P14-1064,P98-1069,0,0.0208295,"ke on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. The idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only. Recent improvements to BLI (Tamura et al., 2012; Irvine and Callison-Burch, 2013b) have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams. Razmara et al. (2013) and Irvine and CallisonBurch (2013a) conduct a more extensive evaluation of their graph-based"
P14-1064,N06-1020,0,0.0144871,"from the parallel corpus may be inadequate for unlabeled instances. We therefore need to enrich the target or label space for unknown phrases. A na¨ıve way to achieve this goal would be to extract all n-grams, from n = 1 to a maximum ngram order, from the monolingual data, but this strategy would lead to a combinatorial explosion in the number of target phrases. Instead, by intelligently expanding the target space using linguistic information such as morphology (Toutanova et al., 2008; Chahuneau et al., 2013), or relying on the baseline system to generate candidates similar to self-training (McClosky et al., 2006), we can tractably propose novel translation candidates (white nodes in Fig. 1’s target graph) whose probabilities are then estimated during propagation. We refer to these additional candidates as “generated” candidates. To generate new translation candidates using the baseline system, we decode each unlabeled source bigram to generate its m-best translations. This set of candidate phrases is filtered to include only n-grams occurring in the target monolingual corpus, and helps to prune passed-through OOV words and invalid translations. To generate new translation candidates using morphologica"
P14-1064,D08-1089,0,0.0304745,"propagation step to sum to one over the fixed list of translation candidates, and run the SLP algorithm to convergence.3 2.5 Phrase-based SMT Expansion After graph propagation, each unlabeled phrase is labeled with a categorical distribution over the set of translation candidates defined in §2.3. In order to utilize these newly acquired phrase pairs, we need to compute their relevant features. The phrase pairs have four log-probability features with two likelihood features and two lexical weighting features. In addition, we use a sophisticated lexicalized hierarchical reordering model (HRM) (Galley and Manning, 2008) with five features for each phrase pair. We utilize the graph propagation-estimated forward phrasal probabilities P(e|f ) as the forward likelihood probabilities for the acquired phrases; to obtain the backward phrasal probability for a given phrase pair, we make use of Bayes’ Theorem: P(f |e) = P(e|f )P(f ) P(e) where the marginal probabilities of source and target phrases e and f are obtained from the counts extracted from the monolingual data. The baseline system’s lexical models are used for the forward and backward lexical scores. The HRM probabilities for the new phrase pairs are estima"
P14-1064,P03-1021,0,0.0354415,"learn from noisy parallel data compared to the traditional SMT system. Baseline phrasal systems are used both for comparison and for generating translation candidates for unlabeled phrases as described in §2.1. The baseline is a state-of-the-art phrase-based system; we perform word alignment using a lexicalized hidden Markov model, and then the phrase table is extracted using the grow-diag-final heuristic (Koehn et al., 2003). The 13 baseline features (2 lexical, 2 phrasal, 5 HRM, and 1 language model, word penalty, phrase length feature and distortion penalty feature) were tuned using MERT (Och, 2003), which is also used to tune the 4 feature weights introduced by the secondary phrase table (2 lexical and 2 phrasal, other features being shared between the two tables). For all systems, we use a distortion limit of 4. We use case-insensitive BLEU (Papineni et al., 2002) to evaluate translation quality. Tt (e0 |e)Pt (e0 |j) (5) e0 2H(j) With this formulation, even if e 6= e0 , the similarity Tt (e0 |e) as determined by the target phrase graph will dictate propagation probability. We renormalize the probability distributions after each propagation step to sum to one over the fixed list of tran"
P14-1064,P08-1088,0,0.184622,"tion identifies translations of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets. 1 The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use hi"
P14-1064,P02-1040,0,0.0915957,"system; we perform word alignment using a lexicalized hidden Markov model, and then the phrase table is extracted using the grow-diag-final heuristic (Koehn et al., 2003). The 13 baseline features (2 lexical, 2 phrasal, 5 HRM, and 1 language model, word penalty, phrase length feature and distortion penalty feature) were tuned using MERT (Och, 2003), which is also used to tune the 4 feature weights introduced by the secondary phrase table (2 lexical and 2 phrasal, other features being shared between the two tables). For all systems, we use a distortion limit of 4. We use case-insensitive BLEU (Papineni et al., 2002) to evaluate translation quality. Tt (e0 |e)Pt (e0 |j) (5) e0 2H(j) With this formulation, even if e 6= e0 , the similarity Tt (e0 |e) as determined by the target phrase graph will dictate propagation probability. We renormalize the probability distributions after each propagation step to sum to one over the fixed list of translation candidates, and run the SLP algorithm to convergence.3 2.5 Phrase-based SMT Expansion After graph propagation, each unlabeled phrase is labeled with a categorical distribution over the set of translation candidates defined in §2.3. In order to utilize these newly"
P14-1064,P95-1050,0,0.650341,"monolingual corpora. Next, graph propagation identifies translations of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets. 1 The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Cal"
P14-1064,P11-1002,0,0.127132,"tions of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets. 1 The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use higher order n-grams inste"
P14-1064,P13-1109,0,0.432707,"al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use higher order n-grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with th"
P14-1064,D08-1090,0,0.023427,"and while the latter does extend the framework to sub-spans of sentences, they do not discover new translation pairs or phrasal probabilities for new pairs at all, but instead re-estimate phrasal probabilities using the graph structure and add this score as an additional feature during decoding. The goal of leveraging non-parallel data in machine translation has been explored from several different angles. Paraphrases extracted by “pivoting” via a third language (Callison-Burch et al., 2006) can be derived solely from monolingual corpora using distributional similarity (Marton et al., 2009). Snover et al. (2008) use cross-lingual information retrieval techniques to find potential sentence-level translation candidates among comparable corpora. In this case, the goal is to try and construct a corpus as close to parallel as possible from comparable corpora, and is a fairly different take on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. The idea presented in this paper is similar in spirit to bilingual le"
P14-1064,D12-1003,0,0.0728179,"embed these nodes in their graphs, we utilize the monolingual corpora on both the 1 The q most frequent words in the monolingual corpus were removed as keys from this mapping, as these high entropy features do not provide much information. 2 We also obtained the k-nearest neighbors of the translation candidates generated through these methods by utilizing the target graph, but this had minimal impact. 678 decoder output, or from a morphological generator (e.g., a cat and catlike in Fig. 1). A classic propagation algorithm that has been suitably modified for use in bilingual lexicon induction (Tamura et al., 2012; Razmara et al., 2013) is the label propagation (LP) algorithm of Zhu et al. (2003). In this case, Ts (f, j) is chosen to be: s wf,j Ts (j|f ) = P (2) s j 0 2N (f ) wf,j 0 The morphologically-generated candidates for a given source unlabeled phrase are initially defined as the target word sequences in the monolingual data that have the same stem sequence as one of the baseline’s target translations for a source phrase which has the same stem sequence as the unlabeled source phrase. These candidates are scored using stem-level translation probabilities, morpheme-level lexical weighting probabi"
P14-1064,P08-1059,1,0.812209,"he exponential dependence 677 of the sizes of these spaces on the length of instances is to blame. Thus, the target phrase inventory from the parallel corpus may be inadequate for unlabeled instances. We therefore need to enrich the target or label space for unknown phrases. A na¨ıve way to achieve this goal would be to extract all n-grams, from n = 1 to a maximum ngram order, from the monolingual data, but this strategy would lead to a combinatorial explosion in the number of target phrases. Instead, by intelligently expanding the target space using linguistic information such as morphology (Toutanova et al., 2008; Chahuneau et al., 2013), or relying on the baseline system to generate candidates similar to self-training (McClosky et al., 2006), we can tractably propose novel translation candidates (white nodes in Fig. 1’s target graph) whose probabilities are then estimated during propagation. We refer to these additional candidates as “generated” candidates. To generate new translation candidates using the baseline system, we decode each unlabeled source bigram to generate its m-best translations. This set of candidate phrases is filtered to include only n-grams occurring in the target monolingual cor"
P14-1064,P13-1140,0,0.157296,"g phrase table and a small bilingual lexicon, and performs BLI using monolingual corpora. The operational scope of their approach is limited in that they assume a scenario where unknown phrase pairs are provided (thereby sidestepping the issue of translation candidate generation for completely unknown phrases), and what remains is the estimation of phrasal probabilities. In our case, we obtain the phrase pairs from the graph structure (and therefore indirectly from the monolingual data) and a separate generation step, which plays an important role in good performance of the method. Similarly, Zhang and Zong (2013) present a series of heuristics that are applicable in a fairly narrow setting. The notion of translation consensus, wherein similar sentences on the source side are encour5 Conclusion In this work, we presented an approach that can expand a translation model extracted from a sentence-aligned, bilingual corpus using a large amount of unstructured, monolingual data in both source and target languages, which leads to improvements of 1.4 and 1.2 BLEU points over strong baselines on evaluation sets, and in some scenarios gains in excess of 4 BLEU points. In the future, we plan to estimate the grap"
P14-1064,C98-1066,0,\N,Missing
P14-1064,W13-2201,0,\N,Missing
P15-1085,W99-0604,0,\N,Missing
P15-1085,N10-1083,0,\N,Missing
P15-1085,N07-1022,1,\N,Missing
P15-1085,N06-1056,1,\N,Missing
P15-1085,P07-1121,1,\N,Missing
P15-1085,P13-2009,0,\N,Missing
P15-1085,P07-2045,0,\N,Missing
P15-1085,P06-1115,1,\N,Missing
P15-1085,P05-1033,0,\N,Missing
P15-1085,N03-1017,0,\N,Missing
P15-1085,J97-3002,0,\N,Missing
P15-1085,P06-1121,1,\N,Missing
P15-1085,D13-1160,0,\N,Missing
P15-1085,P09-1010,0,\N,Missing
P15-2073,W05-0909,0,0.0517039,"irable. Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the"
P15-2073,E06-1032,0,0.0476642,"on, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propos"
P15-2073,2003.mtsummit-papers.9,0,0.031569,"run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Ve"
P15-2073,N12-1017,0,0.0244703,"ri,j )  PP i g ∈ n-grams(hi ) maxj wi,j ·#g (hi ) n 1 e(1−ρ/η) if η &gt; ρ otherwise Discriminative B LEU (2) where ρ and η are respectively hypothesis and reference lengths.2 Then corpus-level n-gram precision is defined as:  P P i g ∈ n-grams(hi ) maxj #g (hi , ri,j ) P P pn = i g ∈ n-grams(hi ) #g (hi ) where #g (·) is the number of occurrences of n-gram g in a given  sentence, and #g (u, v) is a shorthand for min #g (u), #g (v) . It has been demonstrated that metrics such as B LEU show increased correlation with human judgment as the number of references increases (Przybocki et al., 2008; Dreyer and Marcu, 2012). Unfortunately, gathering multiple references is difficult in the case of conversations. Data gathered from naturally occurring conversations offer only one response per message. One could search (c, m) pairs that occur multiple times in conversational data with the hope of finding distinct responses, but this solution is not feasible. Indeed, the larger 1 Unless mentioned otherwise, B LEU refers to the original IBM B LEU as first described in (Papineni et al., 2002). 2 In the case of multiple references, B LEU selects the reference whose length is closest to that of the hypothesis. In a nuts"
P15-2073,D14-1020,0,0.0691491,"evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), emp"
P15-2073,N15-1124,0,0.0162952,"w model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ a variant of B LEU"
P15-2073,W07-0734,0,0.0243517,"ool”, “well then! Why were the biscuits needed?”); others are a little more plausible, but irrelevant or possibly topic changing (“ohh I love that song”). Higher-valued positive-weighted mined responses are typically reasonably appropriate and relevant (even though 447 3 For this work, we sought 2 additional annotations of the seed responses for consistency with the mined responses. As a result, scores for some seed responses slipped below our initial threshold of 4. Nonetheless, these responses were retained. test set.4 While much prior work assesses automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by computing correlations on observations consisting of single-sentence system outputs, it has been shown (e.g., Przybocki et al. (2008)) that correlation coefficients significantly increase as observation units become larger. For instance, corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that B LEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwi"
P15-2073,C12-1121,0,0.0718467,"corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that B LEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwise. We evaluate qi by averaging human ratings on the M sentences, and mi by computing metric scores on the same set of sentences.7 We compare three different metrics: B LEU, ∆B LEU, and sentence-level B LEU (sB LEU). The last computes sentence-level B LEU scores (Nakov et al., 2012) and averages them on the M sentences (akin to macro-averaging). Finally, unless otherwise noted, all versions of B LEU use n-gram order up to 2 (B LEU-2), as this achieves better correlation for all metrics on this data. extracted from a completely unrelated conversation), and in some cases can outscore the original response, as can be seen in the third set of examples. 4.2 Human Evaluation of System Outputs Responses generated by the 7 systems used in this study on the 2114-triple test set were hand evaluated by 5 crowdsourced raters each on a 5-point Likert-type scale. From these 7 systems,"
P15-2073,P03-1021,0,0.0436342,"rced raters each on a 5-point Likert-type scale. From these 7 systems, 12 system pairs were evaluated, for a total of about pairwise 126K ratings (12 · 5 · 2114). Here too, raters were asked to evaluate responses in terms of their relevance to both context and message. Outputs from different systems were randomly interleaved for presentation to the raters. We obtained human ratings on the following systems: Phrase-based MT: A phrase-based MT system similar to (Ritter et al., 2011), whose weights have been manually tuned. We also included four variants of that system, which we tuned with MERT (Och, 2003). These variants differ in their number of features, and augment (Ritter et al., 2011) with the following phrase-level features: edit distance between source and target, cosine similarity, Jaccard index and distance, length ratio, and DSSM score (Huang et al., 2013). RNN-based MT: the log-probability according to the RNN model of (Sordoni et al., 2015). Baseline: a random baseline. While ∆B LEU relies on human qualitative judgments, it is important to note that human judgments on multi-references (§ 4.1) and those on system outputs were collected completely independently. We also note that the"
P15-2073,P02-1040,0,0.116946,"f outputs are acceptable or even desirable. Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result"
P15-2073,D11-1054,0,0.592121,"f a corpus with which to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU), a new metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference B LEU. In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized. Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems. Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs. Below, we describe ∆B LEU and investigate its characteristics in comparison to standard B LEU in the context of conversational response generation. We demonstrate that ∆B LEU correlates well with human evaluation scores in this task and thus can 445 Proceedings of the 53rd Annual Meeting of the Association for C"
P15-2073,N15-1020,1,0.452044,"to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU), a new metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference B LEU. In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized. Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems. Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs. Below, we describe ∆B LEU and investigate its characteristics in comparison to standard B LEU in the context of conversational response generation. We demonstrate that ∆B LEU correlates well with human evaluation scores in this task and thus can 445 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistic"
P15-2073,P12-2008,0,0.0788347,"llison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ a variant of B LEU in which n-grams are weighted by tf ·idf. This assumes the availability of a corpus with which to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU"
P16-1069,N16-1181,0,0.00826568,"tic parsing task as a machine translation task, where the source language is natural language commands and the target language is the meaning representation. Several approaches have applied standard machine translation techniques to semantic parsing (Wong and Mooney, 2006; Andreas et al., 2013; Ratnaparkhi, 1999) with successful results. More recently, neural network approaches have been developed for semantic parsing, and especially for querying a database. A neural network is trained to translate the query and the database into some continuous representation then use it to answer the query (Andreas et al., 2016; Yin et al., 2016). Related Work Semantic Parsing Semantic parsing is the task of translating natural language to a meaning representation language that the machine can execute. Various semantic parsing tasks have been proposed before, including querying a database (Zelle and Mooney, 1996), following navigation instructions (Chen, 2012), translating to Abstract Meaning Representation (AMR) (Artzi et al., 2015), as well as the If-Then task we explore. Meaning representation languages vary with the task. In database queries, the meaning representation language is either the native query languag"
P16-1069,D15-1198,0,0.0180597,"arsing, and especially for querying a database. A neural network is trained to translate the query and the database into some continuous representation then use it to answer the query (Andreas et al., 2016; Yin et al., 2016). Related Work Semantic Parsing Semantic parsing is the task of translating natural language to a meaning representation language that the machine can execute. Various semantic parsing tasks have been proposed before, including querying a database (Zelle and Mooney, 1996), following navigation instructions (Chen, 2012), translating to Abstract Meaning Representation (AMR) (Artzi et al., 2015), as well as the If-Then task we explore. Meaning representation languages vary with the task. In database queries, the meaning representation language is either the native query language (e.g. SQL or Prolog), or some alternative that can be deterministically transformed into the native query language. To follow navigation instructions, the meaning representation language is comprised of sequences of valid actions: turn left, turn right, move forward, etc. For parsing If-Then rules, the meaning representation is an abstract syntax tree (AST) in a very simple language. Each root node expands in"
P16-1069,D15-1041,0,0.0137009,"tems. Yet when all techniques are combined in an ensemble, the resulting predictions are better. Furthermore, an ensemble without the synthetic data or without the alternate grammar has lower accuracy: each technique contributes to the final result. • Missed inference. In certain cases the cue was more of a loose inference. Words such as “payment” and “refund” should tend to 733 NN errors LR errors Swapped trigger and action Duplicated Missed word cue Missed multi-word cue Missed inference Related channel 4 3 8 2 8 5 4 4 8 0 0 8 Grand Total 30 24 Error type improved syntactic parsing results (Ballesteros et al., 2015). We believe that noisy data such as the If-Then corpus would benefit from character modelings, since the models could be more robust to spelling errors and variations. Another important future work direction is to model the arguments of the If-Then statements. However, that requires segmenting the arguments into those that are general across all users, and those that are specific to the recipe’s author. Likely this would require further annotation of the data. Table 4: Count of error cases by type for NN and LR models, in their default configurations. This table only counts those instances in"
P16-1069,D13-1160,0,0.102366,"Missing"
P16-1069,C10-1011,0,0.0117574,"ween the language and the code, jointly learning to select the correct productions in the meaning representation grammar. Although the latter approach is more appealing from a modeling standpoint, empirically it doesn’t perform substantially better than the alignment-free model. Furthermore the alignment-free model is much simpler to implement and optimize. Therefore, we build upon the alignment-free approach. 2.4 TRIGGER tant features, such as indicator features for words and phrases, were often very sparse. Furthermore, the best systems often relied on manually-induced feature combinations (Bohnet, 2010). Multi-layer neural networks have several advantages. Words (or, more generally, features) are first embedded into a continuous space where similar features land in nearby locations; this helps lead to lexical generalization. The additional hidden layers can model feature interactions in complex ways, obviating the need for manual feature template induction. Feed-forward neural networks with relatively simple structure have shown great gains in both dependency parsing (Chen and Manning, 2014) and machine translation (Devlin et al., 2014) without the need for complex feature templates and larg"
P16-1069,W14-2405,0,0.00938961,"Missing"
P16-1069,P09-1010,0,0.0264134,"Missing"
P16-1069,D14-1082,0,0.0221475,"s, were often very sparse. Furthermore, the best systems often relied on manually-induced feature combinations (Bohnet, 2010). Multi-layer neural networks have several advantages. Words (or, more generally, features) are first embedded into a continuous space where similar features land in nearby locations; this helps lead to lexical generalization. The additional hidden layers can model feature interactions in complex ways, obviating the need for manual feature template induction. Feed-forward neural networks with relatively simple structure have shown great gains in both dependency parsing (Chen and Manning, 2014) and machine translation (Devlin et al., 2014) without the need for complex feature templates and large models. Our NN models here are inspired by these effective approaches. 3 Approach We next describe the details of how If-Then recipes are constructed given natural language descriptions. As in prior work, we treat semantic parsing as a structure prediction task. First we describe the structure and features of the model, then expand on the details of inference. 3.1 Neural Networks Grammar Along the lines of Quirk et al. (2015), we build a context-free grammar baseline. This grammar generates"
P16-1069,P12-1045,0,0.0143305,"ently, neural network approaches have been developed for semantic parsing, and especially for querying a database. A neural network is trained to translate the query and the database into some continuous representation then use it to answer the query (Andreas et al., 2016; Yin et al., 2016). Related Work Semantic Parsing Semantic parsing is the task of translating natural language to a meaning representation language that the machine can execute. Various semantic parsing tasks have been proposed before, including querying a database (Zelle and Mooney, 1996), following navigation instructions (Chen, 2012), translating to Abstract Meaning Representation (AMR) (Artzi et al., 2015), as well as the If-Then task we explore. Meaning representation languages vary with the task. In database queries, the meaning representation language is either the native query language (e.g. SQL or Prolog), or some alternative that can be deterministically transformed into the native query language. To follow navigation instructions, the meaning representation language is comprised of sequences of valid actions: turn left, turn right, move forward, etc. For parsing If-Then rules, the meaning representation is an abst"
P16-1069,P14-1129,0,0.0284087,"Missing"
P16-1069,P15-1085,1,0.264391,"ing approaches have been proposed, but most fit into the following broad divisions. First, approaches driven by Combinatory Categorical Grammar (CCG) have proven successful at several semantic parsing tasks. This approach is attractive in that it simultaneously provides syntactic and semantic parses of a natural language utterance. Syntactic structure helps constrain and guide semantic interpretation. CCG relies heavily on a lexicon that specifies both the syntactic category and formal se2.2 If-Then dataset We use a semantic parsing dataset collected from http://ifttt.com, first introduced in Quirk et al. (2015). This website publishes a large set of recipes in the form of If-Then rules. Each recipe was authored by a website user to automate simple tasks. For instance, a recipe could send you a message every time you are tagged on a picture on Facebook. From a natural language standpoint, the most interesting part of this data is that alongside each recipe, there is a short natural language description intended to name or advertise the task. This provides a naturalistic albeit often noisy source of parallel data for training semantic parsing systems. Some of these descriptions faithfully represent th"
P16-1069,P16-1004,0,0.0297511,"Missing"
P16-1069,P81-1022,0,0.757442,"resulting lattice represents many possible paraphrases of the input. We select at most 10 diverse paths through this lattice using the method of Gimpel et al. (2013).5 This adds around 470,000 training examples. Inference When, at test time, we are given a new sentence, we would like to infer its most probable derivation tree D. Classifiers trained as in the prior section give probability distributions over productions given the sentence and all prior productions P (ri |r1 , . . . , ri−1 , f (S)). Were the distribution to be context free, we could rely on algorithms similar to Earley parsing (Earley, 1970) to find the max derivation. However, the dependency on prior productions breaks the context free assumption. Therefore, we resort to approximate inference, namely beam search. Each partial hypothesis is grouped into a beam based on the number of productions it contains; we use a beam width of 8, and search for the highest scoring hypothesis. 4 Synthetic data using paraphrases Improving generalization The data set we use for training and testing is primarily English but contains a broad vocabulary as 4 We tried the sequence-to-sequence model with LSTMs (Sutskever et al., 2014) to map word sequ"
P16-1069,N13-1092,0,0.0297604,"Missing"
P16-1069,D13-1111,0,0.0192834,"of word sense disambiguation. This adds around 50,000 additional training examples. Next, we consider augmenting the data using the Paraphrase Database (Ganitkevitch et al., 2013). Each original description is converted into a lattice. The original word at each position is left in place with a constant score. For each word or phrase in the description found PPDB, we add one arc for each paraphrase, parameterized by the PPDB score of that phrase. The resulting lattice represents many possible paraphrases of the input. We select at most 10 diverse paths through this lattice using the method of Gimpel et al. (2013).5 This adds around 470,000 training examples. Inference When, at test time, we are given a new sentence, we would like to infer its most probable derivation tree D. Classifiers trained as in the prior section give probability distributions over productions given the sentence and all prior productions P (ri |r1 , . . . , ri−1 , f (S)). Were the distribution to be context free, we could rely on algorithms similar to Earley parsing (Earley, 1970) to find the max derivation. However, the dependency on prior productions breaks the context free assumption. Therefore, we resort to approximate infere"
P16-1069,W16-0105,0,0.109352,"machine translation task, where the source language is natural language commands and the target language is the meaning representation. Several approaches have applied standard machine translation techniques to semantic parsing (Wong and Mooney, 2006; Andreas et al., 2013; Ratnaparkhi, 1999) with successful results. More recently, neural network approaches have been developed for semantic parsing, and especially for querying a database. A neural network is trained to translate the query and the database into some continuous representation then use it to answer the query (Andreas et al., 2016; Yin et al., 2016). Related Work Semantic Parsing Semantic parsing is the task of translating natural language to a meaning representation language that the machine can execute. Various semantic parsing tasks have been proposed before, including querying a database (Zelle and Mooney, 1996), following navigation instructions (Chen, 2012), translating to Abstract Meaning Representation (AMR) (Artzi et al., 2015), as well as the If-Then task we explore. Meaning representation languages vary with the task. In database queries, the meaning representation language is either the native query language (e.g. SQL or Prol"
P16-1069,N06-1056,0,\N,Missing
P16-1069,P13-2009,0,\N,Missing
P16-1136,D13-1160,0,0.0328171,"ons. We conduct a theoretical analysis of the efficiency gain from the approach. Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion. 1 Wen-tau Yih Microsoft Research Redmond, WA, USA Introduction Intelligent applications benefit from structured knowledge about the entities and relations in their domains. For example, large-scale knowledge bases (KB), such as Freebase (Bollacker et al., 2008) or DBPedia (Auer et al., 2007), have proven to be important resources for supporting open-domain question answering (Berant et al., 2013; Sun et al., 2015; Yih et al., 2015). In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) are crucial for understanding complex diseases such as cancer and for advancing precision medicine. ∗ This research was conducted during the author’s internship at Microsoft Research. While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 200"
P16-1136,D14-1165,1,0.764736,"d at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; Poon et al., 2015) or inferring facts from the relationships among known entities (Lao and Cohen, 2010) are thus important approaches for populating existing knowledge bases. Originally proposed as an alternative statistical relational learning method, the knowledge base embedding approach has gained a significant amount of attention, due to its simple prediction time computation and strong empirical performance (Nickel et al., 2011; Chang et al., 2014). In this framework, entities and relations in a knowledge base are represented in a continuous space, such as vectors and matrices. Whether two entities have a previously unknown relationship can be predicted by simple functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015). In c"
P16-1136,D15-1034,0,0.442388,"known relationship can be predicted by simple functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015). In contrast, using multi-step relation paths (e.g., husband(barack, michelle) ∧ mother(michelle, sasha) to train KB embeddings has been proposed very recently (Guu et al., 2015; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015). While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened w"
P16-1136,D13-1080,0,0.0302143,"oses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propo"
P16-1136,D14-1044,0,0.0461743,"number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic programming method that"
P16-1136,D15-1038,0,0.26163,"Missing"
P16-1136,D12-1093,0,0.0097446,"ormance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact"
P16-1136,D15-1082,0,0.816198,"redicted by simple functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015). In contrast, using multi-step relation paths (e.g., husband(barack, michelle) ∧ mother(michelle, sasha) to train KB embeddings has been proposed very recently (Guu et al., 2015; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015). While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is a"
P16-1136,P09-1113,0,0.0591613,"rant et al., 2013; Sun et al., 2015; Yih et al., 2015). In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) are crucial for understanding complex diseases such as cancer and for advancing precision medicine. ∗ This research was conducted during the author’s internship at Microsoft Research. While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; Poon et al., 2015) or inferring facts from the relationships among known entities (Lao and Cohen, 2010) are thus important approaches for populating existing knowledge bases. Originally proposed as an alternative statistical relational learning method, the knowledge base embedding approach has gained a significant amount of attention, due to its simple prediction time computation and strong empirical performance (Nickel et al., 2011; Chang et al., 2014). In this framework, entities and relations in a knowledge base are represented in a continuous space, such as vectors"
P16-1136,P15-1016,0,0.505284,"functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015). In contrast, using multi-step relation paths (e.g., husband(barack, michelle) ∧ mother(michelle, sasha) to train KB embeddings has been proposed very recently (Guu et al., 2015; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015). While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled tex"
P16-1136,N13-1008,0,0.861813,"cal challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic pr"
P16-1136,D12-1042,0,0.0364585,"un et al., 2015; Yih et al., 2015). In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) are crucial for understanding complex diseases such as cancer and for advancing precision medicine. ∗ This research was conducted during the author’s internship at Microsoft Research. While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; Poon et al., 2015) or inferring facts from the relationships among known entities (Lao and Cohen, 2010) are thus important approaches for populating existing knowledge bases. Originally proposed as an alternative statistical relational learning method, the knowledge base embedding approach has gained a significant amount of attention, due to its simple prediction time computation and strong empirical performance (Nickel et al., 2011; Chang et al., 2014). In this framework, entities and relations in a knowledge base are represented in a continuous space, such as vectors and matrices. Whether"
P16-1136,W15-4007,1,0.757471,"lation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic programming method that enables efficient modeling"
P16-1136,D15-1174,1,0.720524,"uded textual relations that occur at least 5 times between mentions of two genes that have a KB relation. This resulted in 3,827 distinct textual relations and 1,244,186 mentions.6 The number of textual relations is much larger than that of KB relations, and it helped induce much larger connectivity among genes (390,338 pairs of genes are directly connected in text versus 12,100 pairs in KB). Systems A LL - PATHS denotes our compositional learning approach that sums over all paths using 6 Modeling such a large number of textual relations introduces sparsity, which necessitates models such as (Toutanova et al., 2015; Verga et al., 2015) to derive composed representations of text. We leave integration with such methods for future work. 1440 dynamic programming; A LL - PATHS + NODES additionally models nodes in the paths. P RUNED PATHS denotes the traditional approach that learns from sampled paths detailed in §3.1.2; paths with occurrence less than a cutoff are pruned (c = 1 in Table 1 means that all sampled paths are used). The most relevant prior approach is Guu et al. (2015). We ran experiments using both their publicly available code and our re-implementation. We also included the B ILINEAR - DIAG bas"
P16-1136,P15-1128,1,0.122455,"of the efficiency gain from the approach. Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion. 1 Wen-tau Yih Microsoft Research Redmond, WA, USA Introduction Intelligent applications benefit from structured knowledge about the entities and relations in their domains. For example, large-scale knowledge bases (KB), such as Freebase (Bollacker et al., 2008) or DBPedia (Auer et al., 2007), have proven to be important resources for supporting open-domain question answering (Berant et al., 2013; Sun et al., 2015; Yih et al., 2015). In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) are crucial for understanding complex diseases such as cancer and for advancing precision medicine. ∗ This research was conducted during the author’s internship at Microsoft Research. While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; Poon et al."
P16-1136,N16-1103,0,\N,Missing
P18-1069,C04-1046,0,0.0758888,"santo and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To the best of our knowledge, conﬁdence modeling for semantic parsing remains largely unexplored. A common scheme for modeling uncertainty in neural networks is to place distributions over the network’s weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017). But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difﬁcult to work with. Gal and Ghahramani (2016) develop"
P18-1069,P17-2098,0,0.026462,"have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; So"
P18-1069,P17-1089,0,0.0291545,"(Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and ques"
P18-1069,P16-1002,0,0.32272,"dence model signiﬁcantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores. 1 Introduction Semantic parsing aims to map natural language text to a formal meaning representation (e.g., logical forms or SQL queries). The neural sequenceto-sequence architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has been widely adopted in a variety of natural language processing tasks, and semantic parsing is no exception. However, despite achieving promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), neural semantic parsers remain difﬁcult to interpret, acting in most cases as a black box, not providing any information about what made them arrive at a particular decision. In this work, we explore ways to estimate and interpret the ∗ Work carried out during an internship at Microsoft Research. 743 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 743–753 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics We compute these conﬁdence metrics for a given prediction and use th"
P18-1069,P16-1004,1,0.939878,"ults show that our conﬁdence model signiﬁcantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores. 1 Introduction Semantic parsing aims to map natural language text to a formal meaning representation (e.g., logical forms or SQL queries). The neural sequenceto-sequence architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has been widely adopted in a variety of natural language processing tasks, and semantic parsing is no exception. However, despite achieving promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), neural semantic parsers remain difﬁcult to interpret, acting in most cases as a black box, not providing any information about what made them arrive at a particular decision. In this work, we explore ways to estimate and interpret the ∗ Work carried out during an internship at Microsoft Research. 743 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 743–753 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics We compute these conﬁdence metrics for a given"
P18-1069,W17-2631,0,0.0143956,"do indeed produce uncertain outputs, which we would like our framework to identify. A widely-used conﬁdence scoring method is based on posterior probabilities p (y|x) where x is the input and y the model’s prediction. For a linear model, this method makes sense: as more positive evidence is gathered, the score becomes larger. Neural models, in contrast, learn a complicated function that often overﬁts the training data. Posterior probability is effective when making decisions about model output, but is no longer a good indicator of conﬁdence due in part to the nonlinearity of neural networks (Johansen and Socher, 2017). This observation motivates us to develop a conﬁdence modeling framework for sequenceto-sequence models. We categorize the causes of uncertainty into three types, namely model uncertainty, data uncertainty, and input uncertainty and design different metrics to characterize them. In this work we focus on conﬁdence modeling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty, and design various metrics to quantify these factors. These metrics are then used to estimate conﬁdence scores that indicate whether model predictions"
P18-1069,P18-1068,1,0.721265,"Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical"
P18-1069,D16-1116,0,0.0361875,"Missing"
P18-1069,W17-2607,0,0.0279905,"; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To t"
P18-1069,D17-1160,0,0.0356097,"ores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the c"
P18-1069,D11-1140,0,0.172747,"Missing"
P18-1069,P17-1030,0,0.0311205,"an estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To the best of our knowledge, conﬁdence modeling for semantic parsing remains largely unexplored. A common scheme for modeling uncertainty in neural networks is to place distributions over the network’s weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017). But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difﬁcult to work with. Gal and Ghahramani (2016) develop a theoretical framework which shows that the use of dropout in neural networks can be interpreted as a Bayesian approximation of Gaussian Process. We adapt their framework so as to represent uncertainty in the encoder-decoder architectures, and extend it by adding Gaussian noise to weights. 3 Neural Semantic Parsing Model In the following section we describe the neural semantic parsing model (Dong"
P18-1069,P16-1057,0,0.0299597,"Missing"
P18-1069,P13-2121,0,0.0350755,"Missing"
P18-1069,D08-1082,0,0.0305941,"Hochreiter and Schmidhuber 1997) which process tokens sequentially. The probability of generating the whole sequence p (a|q) is factorized as: p (a|q) = |a|  p (at |a&lt;t , q) (1) t=1 where a&lt;t = a1 · · · at−1 . Let et ∈ Rn denote the hidden vector of the encoder at time step t. It is computed via et = fLSTM (et−1 , qt ), where fLSTM refers to the LSTM unit, and qt ∈ Rn is the word embedding Semantic Parsing Various methods have been developed to learn a semantic parser from natural language descriptions paired with meaning representations (Tang and Mooney, 2000; Zettlemoyer and Collins, 2007; Lu et al., 2008; Kwiatkowski 744 ܽଵ … … ܽȁȁ Algorithm 1 Dropout Perturbation iv) ଵ i) Input: q, a: Input and its prediction M: Model parameters 1: for i ← 1, · · · , F do ˆ i ← Apply dropout layers to M  Figure 1 2: M ˆ i) 3: Run forward pass and compute pˆ(a|q; M  ii) ݍଵ ଶ … ȁȁ ݍଶ … ݍȁȁ iii) i) ଵ ଶ … &lt;s&gt; ܽଵ … ȁȁ ܽ  ିଵ ˆ i )}F 4: Compute variance of {ˆ p(a|q; M i=1  Equation (6) Figure 1: We use dropout as approximate Bayesian inference to obtain model uncertainty. The dropout layers are applied to i) token vectors; ii) the encoder’s output vectors; iii) bridge vectors; and iv) de"
P18-1069,D15-1166,0,0.0607755,"o estimate “what we do not know”. To this end, we identify three causes of uncertainty, and design various metrics characterizing each one of them. We then feed these metrics into a regression model in order to predict s (q, a). of qt . Once the tokens of the input sequence are encoded into vectors, e|q |is used to initialize the hidden states of the ﬁrst time step in the decoder. Similarly, the hidden vector of the decoder at time step t is computed by dt = fLSTM (dt−1 , at−1 ), where at−1 ∈ Rn is the word vector of the previously predicted token. Additionally, we use an attention mechanism (Luong et al., 2015a) to utilize relevant encoder-side context. For the current time step t of the decoder, we compute its attention score with the k-th hidden state in the encoder as: rt,k ∝ exp{dt · ek } 4.1 The model’s parameters or structures contain uncertainty, which makes the model less conﬁdent about the values of p (a|q). For example, noise in the training data and the stochastic learning algorithm itself can result in model uncertainty. We describe metrics for capturing uncertainty below: Dropout Perturbation Our ﬁrst metric uses dropout (Srivastava et al., 2014) as approximate Bayesian inference to es"
P18-1069,P17-2007,0,0.0191705,"nce-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004"
P18-1069,P15-1002,0,0.0402218,"o estimate “what we do not know”. To this end, we identify three causes of uncertainty, and design various metrics characterizing each one of them. We then feed these metrics into a regression model in order to predict s (q, a). of qt . Once the tokens of the input sequence are encoded into vectors, e|q |is used to initialize the hidden states of the ﬁrst time step in the decoder. Similarly, the hidden vector of the decoder at time step t is computed by dt = fLSTM (dt−1 , at−1 ), where at−1 ∈ Rn is the word vector of the previously predicted token. Additionally, we use an attention mechanism (Luong et al., 2015a) to utilize relevant encoder-side context. For the current time step t of the decoder, we compute its attention score with the k-th hidden state in the encoder as: rt,k ∝ exp{dt · ek } 4.1 The model’s parameters or structures contain uncertainty, which makes the model less conﬁdent about the values of p (a|q). For example, noise in the training data and the stochastic learning algorithm itself can result in model uncertainty. We describe metrics for capturing uncertainty below: Dropout Perturbation Our ﬁrst metric uses dropout (Srivastava et al., 2014) as approximate Bayesian inference to es"
P18-1069,W00-1317,0,0.0545413,"al networks with long short-term memory units (LSTMs; Hochreiter and Schmidhuber 1997) which process tokens sequentially. The probability of generating the whole sequence p (a|q) is factorized as: p (a|q) = |a|  p (at |a&lt;t , q) (1) t=1 where a&lt;t = a1 · · · at−1 . Let et ∈ Rn denote the hidden vector of the encoder at time step t. It is computed via et = fLSTM (et−1 , qt ), where fLSTM refers to the LSTM unit, and qt ∈ Rn is the word embedding Semantic Parsing Various methods have been developed to learn a semantic parser from natural language descriptions paired with meaning representations (Tang and Mooney, 2000; Zettlemoyer and Collins, 2007; Lu et al., 2008; Kwiatkowski 744 ܽଵ … … ܽȁȁ Algorithm 1 Dropout Perturbation iv) ଵ i) Input: q, a: Input and its prediction M: Model parameters 1: for i ← 1, · · · , F do ˆ i ← Apply dropout layers to M  Figure 1 2: M ˆ i) 3: Run forward pass and compute pˆ(a|q; M  ii) ݍଵ ଶ … ȁȁ ݍଶ … ݍȁȁ iii) i) ଵ ଶ … &lt;s&gt; ܽଵ … ȁȁ ܽ  ିଵ ˆ i )}F 4: Compute variance of {ˆ p(a|q; M i=1  Equation (6) Figure 1: We use dropout as approximate Bayesian inference to obtain model uncertainty. The dropout layers are applied to i) token vectors; ii) the encoder’s"
P18-1069,H05-1096,0,0.10971,"Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To the best of our knowledge, conﬁdence modeling for semantic parsing remains largely unexplored. A common scheme for modeling uncertainty in neural networks is to place distributions over the network’s weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017). But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difﬁcult to work with. Gal and Ghahramani (2016) develop a theoretical framewo"
P18-1069,P16-1127,0,0.0949186,"atively more interpretable compared to those based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Rela"
P18-1069,P15-1085,1,0.857422,"trics for a given prediction and use them as features in a regression model which is trained on held-out data to ﬁt prediction F1 scores. At test time, the regression model’s outputs are used as conﬁdence scores. Our approach does not interfere with the training of the model, and can be thus applied to various architectures, without sacriﬁcing test accuracy. Furthermore, we propose a method based on backpropagation which allows to interpret model behavior by identifying which parts of the input contribute to uncertain predictions. Experimental results on two semantic parsing datasets (I FTTT, Quirk et al. 2015; and D JANGO, Oda et al. 2015) show that our model is superior to a method based on posterior probability. We also demonstrate that thresholding conﬁdence scores achieves a good trade-off between coverage and accuracy. Moreover, the proposed uncertainty backpropagation method yields results which are qualitatively more interpretable compared to those based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and s"
P18-1069,P17-1041,0,0.135118,"based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimat"
P18-1069,P17-1105,0,0.0122733,"retable compared to those based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estima"
P18-1069,D07-1071,0,0.115715,"hort-term memory units (LSTMs; Hochreiter and Schmidhuber 1997) which process tokens sequentially. The probability of generating the whole sequence p (a|q) is factorized as: p (a|q) = |a|  p (at |a&lt;t , q) (1) t=1 where a&lt;t = a1 · · · at−1 . Let et ∈ Rn denote the hidden vector of the encoder at time step t. It is computed via et = fLSTM (et−1 , qt ), where fLSTM refers to the LSTM unit, and qt ∈ Rn is the word embedding Semantic Parsing Various methods have been developed to learn a semantic parser from natural language descriptions paired with meaning representations (Tang and Mooney, 2000; Zettlemoyer and Collins, 2007; Lu et al., 2008; Kwiatkowski 744 ܽଵ … … ܽȁȁ Algorithm 1 Dropout Perturbation iv) ଵ i) Input: q, a: Input and its prediction M: Model parameters 1: for i ← 1, · · · , F do ˆ i ← Apply dropout layers to M  Figure 1 2: M ˆ i) 3: Run forward pass and compute pˆ(a|q; M  ii) ݍଵ ଶ … ȁȁ ݍଶ … ݍȁȁ iii) i) ଵ ଶ … &lt;s&gt; ܽଵ … ȁȁ ܽ  ିଵ ˆ i )}F 4: Compute variance of {ˆ p(a|q; M i=1  Equation (6) Figure 1: We use dropout as approximate Bayesian inference to obtain model uncertainty. The dropout layers are applied to i) token vectors; ii) the encoder’s output vectors; iii) bridge ve"
P18-1069,D17-1298,0,0.0270318,"ground truth; we compute the overlap between tokens identiﬁed as contributing to uncertainty by our method and those found in the gold standard. Overlap is shown for top 2 and 4 tokens. Best results are in bold. 7 Conclusions In this paper we presented a conﬁdence estimation model and an uncertainty interpretation method for neural semantic parsing. Experimental results show that our method achieves better performance than competitive baselines on two datasets. Directions for future work are many and varied. The proposed framework could be applied to a variety of tasks (Bahdanau et al., 2015; Schmaltz et al., 2017) employing sequence-to-sequence architectures. We could also utilize the conﬁdence estimation model within an active learning framework for neural semantic parsing. google calendar−any event starts THEN facebook −create a status message−(status message ({description})) ATT post calendar event to facebook BP post calendar event to facebook feed−new feed item−(feed url( url sports.espn.go.com)) THEN ... ATT espn mlb headline to readability BP espn mlb headline to readability weather−tomorrow’s low drops below−(( temperature(0)) (degrees in(c))) THEN ... ATT warn me when it’s going to be freezing"
P18-1069,P10-1063,0,0.0574285,"7), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To the best of our knowledge, conﬁdence modeling for semantic parsing remains largely unexplored. A common scheme for modeling uncertainty in neural networks is to place distributions over the network’s weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017). But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difﬁcult to work with. Gal and Ghahramani (2016) develop a theoretical framework which shows that the use o"
P18-1069,N15-1162,0,0.0310473,"nterpret model behavior by identifying which parts of the input contribute to uncertain predictions. Experimental results on two semantic parsing datasets (I FTTT, Quirk et al. 2015; and D JANGO, Oda et al. 2015) show that our model is superior to a method based on posterior probability. We also demonstrate that thresholding conﬁdence scores achieves a good trade-off between coverage and accuracy. Moreover, the proposed uncertainty backpropagation method yields results which are qualitatively more interpretable compared to those based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapa"
P18-1069,P13-2009,0,\N,Missing
P19-3021,P19-1441,1,0.838229,"nt. Figure from (Gao et al., 2019b). Figure 1: An example of a basic multi-task configuration. Two encoder-decoder chains share a common decoder, alternately trained on separate datasets. coders. This architecture consists of additional personality embeddings, which are provided to the decoder alongside token embeddings at each timestep of decoding. Grounding generated responses helps condition outputs based on a given personality embedding: for the same query, the system learns to generate responses in different styles, all while preserving the underlying context. tations (Gao et al., 2019b; Liu et al., 2019). By unifying a conversational sequence-to-sequence model and an autoencoder with a shared decoder, multi-task learning can personalize the conversational model (Luan et al., 2017). Multi-task learning has potentially many other powerful applications for inducing biases in conversational systems. I CECAPS allows users to build arrays of models with arbitrary sharing of components, and place them in a multi-task learning environment. Users can construct arbitrary multi-task training schedules, assigning different tasks or balances among tasks per training step. 3 3.2 SpaceFusion (Gao et al., 20"
P19-3021,P18-1157,1,0.844079,"general censor-list and a start-token censor-list. The general censor-list contains a list of tokens to disable during response generation; probabilities associated with these tokens are clamped to zero. The start-token censor-list is similar, but only masks the response’s first token. We also support infrequency filters; users may restrict the decoder from generating responses with rare words. provide informed responses with context about the real world, without needing comprehensive paired conversational data to embody that information. We provide an extension of stochastic answer networks (Liu et al., 2018), a machine reading comprehension system, that acts as a full knowledgegrounded conversation model (Qin et al., 2019), hybridizing machine reading comprehension with a response generation model. At a high level, this model consists of two deep biLSTMs in parallel that encode conversational context and knowledge, respectively. The information from these encoders is then combined using cross-attention, the output of which forms the basis of a memory cell that powers a response generator. 4 4.3 The standard beam-search implementation in TensorFlow works by iteratively generating tokens, generatin"
P19-3021,P18-4021,0,0.146101,"onversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and decoders, which can be chained together to form a single, end-to-end functional model. This chaining paradigm allows users"
P19-3021,I17-1061,1,0.82046,"atasets. coders. This architecture consists of additional personality embeddings, which are provided to the decoder alongside token embeddings at each timestep of decoding. Grounding generated responses helps condition outputs based on a given personality embedding: for the same query, the system learns to generate responses in different styles, all while preserving the underlying context. tations (Gao et al., 2019b; Liu et al., 2019). By unifying a conversational sequence-to-sequence model and an autoencoder with a shared decoder, multi-task learning can personalize the conversational model (Luan et al., 2017). Multi-task learning has potentially many other powerful applications for inducing biases in conversational systems. I CECAPS allows users to build arrays of models with arbitrary sharing of components, and place them in a multi-task learning environment. Users can construct arbitrary multi-task training schedules, assigning different tasks or balances among tasks per training step. 3 3.2 SpaceFusion (Gao et al., 2019b) is a learning paradigm that aligns latent spaces learned by different models trained over different datasets. Of particular interest is its application to neural conversation"
P19-3021,D15-1166,0,0.0461808,"me context to be placed nearby in latent space and aligning semantically related responses along straight lines in latent space. This induces a structure in the latent space such that distance and direction from a predicted response vector roughly correspond to relevance and diversity, respectively, as in Figure 2. Built-in modules and configurations I CECAPS provides several built-in modules and configurations. Most standard NLP architectures are available, including transformers (Vaswani et al., 2017), LSTM-based seq2seq models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015; Luong et al., 2015), n-gram convolutional language models, and deep convolutional networks for baseline image grounding. Where applicable, these are implemented as chains of simpler components as per our design philosophy. We also provide features that target conversational scenarios, from individual chainable components to custom multi-task learning presets. 3.1 SpaceFusion Personality grounding 3.3 Inspired by recent work on modeling personality differences in conversational systems (Li et al., 2016b), I CECAPS provides implementations of personality-grounded seq2seq and transformer deKnowledge grounding A cri"
P19-3021,D17-2014,0,0.0618856,"Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and decoders, which can be chained together to form a single, end-to-end functional model. This chainin"
P19-3021,N18-1202,0,0.0294649,"ons with their agents and directly observe their responses. Response generation is powered by the custom decoder described in Section 4. While the commandline session is useful for quick testing, for conveTraining configurations I CECAPS training configurations follow a basic five-phase pattern. We include example training scripts that users may use as templates. 126 learning models. It places a strong emphasis on sequence modeling baselines. AllenNLP (Gardner et al., 2018) is a PyTorch library developed by AI2 for natural language processing tasks, notable for an open-source release of ELMo (Peters et al., 2018). OpenNMT (Klein et al., 2017) is a popular neural machine translation toolkit originally developed for LuaTorch that now has implementations in PyTorch and TensorFlow. MarianNMT (Junczys-Dowmunt et al., 2018) is another framework for neural machine translation developed between the Adam Mickiewicz University in Pozna and the University of Edinburgh. It is built in C++ and designed for fast training in multi-GPU systems. Texar (Hu et al., 2018) is a text generation toolkit affiliated with Carnegie Mellon University, featuring a similar emphasis on modularity to I CE CAPS . It includes reinforc"
P19-3021,N19-1125,1,0.84795,"sed directly or loaded for fine-tuning or bootstrapping other models; these models power an online demo of our framework. 1 2 Architecture I CECAPS is designed for modularity, flexibility, and ease of use. Modules are built on top of TensorFlow Estimators, making them easy for developers to use and extend flexibly. I CECAPS supports arbitrary architectures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-tu"
P19-3021,P19-1539,1,0.727317,"g response generation; probabilities associated with these tokens are clamped to zero. The start-token censor-list is similar, but only masks the response’s first token. We also support infrequency filters; users may restrict the decoder from generating responses with rare words. provide informed responses with context about the real world, without needing comprehensive paired conversational data to embody that information. We provide an extension of stochastic answer networks (Liu et al., 2018), a machine reading comprehension system, that acts as a full knowledgegrounded conversation model (Qin et al., 2019), hybridizing machine reading comprehension with a response generation model. At a high level, this model consists of two deep biLSTMs in parallel that encode conversational context and knowledge, respectively. The information from these encoders is then combined using cross-attention, the output of which forms the basis of a memory cell that powers a response generator. 4 4.3 The standard beam-search implementation in TensorFlow works by iteratively generating tokens, generating a constant number of hypotheses at the end of the decoding phase. I CECAPS implements a modified beam search decode"
P19-3021,W18-2501,0,0.128523,"rts arbitrary architectures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and"
P19-3021,P16-1162,0,0.198835,"ke building complex dialogue systems intuitive for the end user. 5.1 Text data processing TensorFlow estimators expect to read data from TFRecord binary files for efficient processing. We provide a script TEXT DATA PROCESSING . PY for converting text data into TFRecords, equipped with several useful preprocessing transformations. Our script can sort data within local windows so that batches fed during training have minimal padding inefficiency. These batches can be shuffled amongst each other to mitigate any biases induced by sorting. We provide token preprocessing through byte pair encoding (Sennrich et al., 2016), which builds a token set at a level of abstraction between characters and words. This often allows for faster training and improved generalization. Another feature focused on conversational scenarios is fixed-length context extraction. Conversational data often contains large, potentially unwieldy multi-turn contexts; we can limit our data samples to a desired context length. We also provide an option for annotating datasets with topic grounding information, by analyzing the data for unique tokens to use as topic markers. 5.2 Training the system. The system is now ready to train: a single ca"
P19-3021,W18-2503,0,0.111568,"NLP (Gardner et al., 2018) is a PyTorch library developed by AI2 for natural language processing tasks, notable for an open-source release of ELMo (Peters et al., 2018). OpenNMT (Klein et al., 2017) is a popular neural machine translation toolkit originally developed for LuaTorch that now has implementations in PyTorch and TensorFlow. MarianNMT (Junczys-Dowmunt et al., 2018) is another framework for neural machine translation developed between the Adam Mickiewicz University in Pozna and the University of Edinburgh. It is built in C++ and designed for fast training in multi-GPU systems. Texar (Hu et al., 2018) is a text generation toolkit affiliated with Carnegie Mellon University, featuring a similar emphasis on modularity to I CE CAPS . It includes reinforcement learning capabilities alongside its sequence modelling tools. A few other toolkits have a dialog emphasis. DeepPavlov (Burtsev et al., 2018) is a deep learning library with a focus on task-oriented dialogue. It provides demos and pre-trained models for tasks such as question answering and sentiment classification. Affiliated with DeepPavlov is the ConvAI2 challenge (Dinan et al., 2019), a general dialogue competition featuring a synthetic"
P19-3021,P18-4020,0,0.0368469,"Missing"
P19-3021,W18-1819,0,0.0298992,"exibly. I CECAPS supports arbitrary architectures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements"
P19-3021,P17-4012,0,0.176011,"tures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and decoders, which can b"
P19-3021,N16-1014,1,0.717728,"ani et al., 2017), LSTM-based seq2seq models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015; Luong et al., 2015), n-gram convolutional language models, and deep convolutional networks for baseline image grounding. Where applicable, these are implemented as chains of simpler components as per our design philosophy. We also provide features that target conversational scenarios, from individual chainable components to custom multi-task learning presets. 3.1 SpaceFusion Personality grounding 3.3 Inspired by recent work on modeling personality differences in conversational systems (Li et al., 2016b), I CECAPS provides implementations of personality-grounded seq2seq and transformer deKnowledge grounding A critical task in building intelligent conversational agents is grounding their responses in an external knowledge base. This allows agents to 124 agent to avoid profanities or other offensive language. Likewise, the system should avoid obvious ungrammatical outputs, such as broken abbreviations or nonsensical punctuation marks. I CECAPS supports several filters, including a general censor-list and a start-token censor-list. The general censor-list contains a list of tokens to disable d"
P19-3021,P16-1094,1,0.920476,"ani et al., 2017), LSTM-based seq2seq models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015; Luong et al., 2015), n-gram convolutional language models, and deep convolutional networks for baseline image grounding. Where applicable, these are implemented as chains of simpler components as per our design philosophy. We also provide features that target conversational scenarios, from individual chainable components to custom multi-task learning presets. 3.1 SpaceFusion Personality grounding 3.3 Inspired by recent work on modeling personality differences in conversational systems (Li et al., 2016b), I CECAPS provides implementations of personality-grounded seq2seq and transformer deKnowledge grounding A critical task in building intelligent conversational agents is grounding their responses in an external knowledge base. This allows agents to 124 agent to avoid profanities or other offensive language. Likewise, the system should avoid obvious ungrammatical outputs, such as broken abbreviations or nonsensical punctuation marks. I CECAPS supports several filters, including a general censor-list and a start-token censor-list. The general censor-list contains a list of tokens to disable d"
Q17-1008,P98-1013,0,0.0391779,"2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure. Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer. N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998). Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker et al., 1998) style are also instances of n-ary relation extraction, with extraction of events expressed in a single sentence. McDonald et al. (2005) extract n-ary relations in a biomedical domain, by first factoring the n-ary relation into pair-wise relations between all entity pairs, and then constructing maximal cliques of related entities. Recently, neural models have been applied to semantic role labeling (FitzGerald et al., 2015; Roth 111 and Lapata, 2016). These works learned neural representations by effectively decomposing the n-ary relation into binary relations between the predicate and each arg"
Q17-1008,H05-1091,0,0.217196,"rst review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of"
Q17-1008,P16-1072,0,0.0800421,"g powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure. Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer. N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998). Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker et al., 1998) styl"
Q17-1008,C10-1018,0,0.0365625,"cularly beneficial. 7 Related Work Most work on relation extraction has been applied to binary relations of entities in a single sentence. We first review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity"
Q17-1008,M98-1001,0,0.238877,"-21 was noted in 10. NEXTSENT All patients were treated with gefitinib and showed a partial response. DET AUXPASS NSUBJPASS ROOT AMOD DET DOBJ PREP WITH CONJ AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory netwo"
Q17-1008,de-marneffe-etal-2006-generating,0,0.120922,"Missing"
Q17-1008,D15-1112,0,0.0433277,"Missing"
Q17-1008,P10-1160,0,0.211478,"AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory networks (graph LSTMs). By adopting the graph formulation, our framework subsumes prior approaches based on chain or tree LSTMs, and can incorporate a rich set of linguis"
Q17-1008,P05-1053,0,0.36783,"tic parses, suggesting that encoding high-quality analysis is particularly beneficial. 7 Related Work Most work on relation extraction has been applied to binary relations of entities in a single sentence. We first review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue)"
Q17-1008,W09-1401,0,0.0236307,"to the n-ary setting is challenging, as there are n2 paths. One apparent solution is inspired by Davidsonian semantics: first, identify a single trigger phrase that signifies the whole relation, then reduce the n-ary relation to n binary relations between the trigger and an argument. However, challenges remain. It is often hard to specify a single trigger, as the relation is manifested by several words, often not contiguous. Moreover, it is expensive and time-consuming to annotate training examples, especially if triggers are required, as is evident in prior annotation efforts such as GENIA (Kim et al., 2009). The realistic and widely adopted paradigm is to leverage indirect supervision, such as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), where triggers are not available. Additionally, lexical and syntactic patterns signifying the relation will be sparse. To handle such sparsity, traditional feature-based approaches require extensive engineering and large data. Unfortunately, this challenge becomes much more severe in crosssentence extraction when the text spans multiple sentences. To overcome these challenges, we explore a general relation extraction framework based on gra"
Q17-1008,J13-4004,0,0.0121401,"ected acyclic graphs (bottom); the graph LSTMs is constructed by a forward pass (Left to Right) followed by a backward pass (Right to Left). Note that information goes from dependency child to parent. 3.1 Document Graph To model various dependencies from linguistic analysis at our disposal, we follow Quirk and Poon (2017) and introduce a document graph to capture intra- and inter-sentential dependencies. A document graph consists of nodes that represent words and edges that represent various dependencies such as linear context (adjacent words), syntactic dependencies, and discourse relations (Lee et al., 2013; Xue et al., 2015). Figure 1 shows the document graph for our running example; this instance suggests that tumors with L858E mutation in EGFR gene responds to the drug gefitinib. This document graph acts as the backbone upon which a graph LSTM is constructed. If it con104 3.2 Backpropagation in Graph LSTMs Conventional LSTMs are essentially very deep feedforward neural networks. For example, a left-to-right linear LSTM has one hidden vector for each word. This vector is generated by a neural network (recurrent unit) that takes as input the embedding of the given word and the hidden vector of"
Q17-1008,P14-5010,0,0.00700447,"ody of biomedical papers is exactly the challenge. As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts. In future work, we will explore incorporating more known facts for distant supervision and extracting from more full-text articles. We conducted tokenization, part-of-speech tagging, and syntactic parsing using SPLAT (Quirk et al., 2012), and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP (Manning et al., 2014). We used the entity taggers from Literome (Poon et al., 2014) to identify drug, gene and mutation mentions. We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision. The knowledge bases distinguish fine-grained interaction types, which we do not use in this paper. 5 6 Distant Supervision After identifying drug, gene and mutation mentions in the text, co-occurring triples with known interactions were chosen as positive examples. However, unlike the single-sentence setting in"
Q17-1008,P05-1061,0,0.489844,"tation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10. NEXTSENT All patients were treated with gefitinib and showed a partial response. DET AUXPASS NSUBJPASS ROOT AMOD DET DOBJ PREP WITH CONJ AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework"
Q17-1008,P09-1113,0,0.740799,"phrase that signifies the whole relation, then reduce the n-ary relation to n binary relations between the trigger and an argument. However, challenges remain. It is often hard to specify a single trigger, as the relation is manifested by several words, often not contiguous. Moreover, it is expensive and time-consuming to annotate training examples, especially if triggers are required, as is evident in prior annotation efforts such as GENIA (Kim et al., 2009). The realistic and widely adopted paradigm is to leverage indirect supervision, such as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), where triggers are not available. Additionally, lexical and syntactic patterns signifying the relation will be sparse. To handle such sparsity, traditional feature-based approaches require extensive engineering and large data. Unfortunately, this challenge becomes much more severe in crosssentence extraction when the text spans multiple sentences. To overcome these challenges, we explore a general relation extraction framework based on graph LSTMs. By learning a continuous representation for words and entities, LSTMs can handle sparsity effectively without requiring intense feature engineeri"
Q17-1008,P16-1105,0,0.26527,"sing a series of gates (input, forget and output) to avoid amplifying or suppressing gradients during backpropagation. Consequently, LSTMs are much more effective in capturing long-distance dependencies, and have been applied to a variety of NLP tasks. However, most approaches are based on linear chains and only explicitly model the linear context, which ignores a variety of linguistic analyses, such as syntactic and discourse dependencies. In this section, we propose a general framework that generalizes LSTMs to graphs. While there is some prior work on learning tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), to the best of our knowledge, graph LSTMs have not been applied to any NLP task yet. Figure 2 shows the architecture of this approach. The input layer is the word embedding of input text. Next is the graph LSTM which learns a contextual representation for each word. For the entities in question, their contextual representations are concatenated and become the input to the relation classifiers. For a multi-word entity, we simply used the average of its word representations and leave the exploration of more sophisticated aggregation approaches to future work. The layers are trained jointly wit"
Q17-1008,P14-2012,0,0.0332938,"Related Work Most work on relation extraction has been applied to binary relations of entities in a single sentence. We first review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such repre"
Q17-1008,J05-1004,0,0.184752,"P DEP The deletion mutation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10. NEXTSENT All patients were treated with gefitinib and showed a partial response. DET AUXPASS NSUBJPASS ROOT AMOD DET DOBJ PREP WITH CONJ AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we expl"
Q17-1008,P16-2025,1,0.847562,"dge-type embedding. Edge-Type Embedding To reduce the number of parameters and leverage potential correlation among fine-grained edge types, we learned a lowdimensional embedding of the edge types, and conducted an outer product of the predecessor’s hidden vector and the edge-type embedding to generate a “typed hidden representation”, which is a matrix. The new computation is as follows: 3.5 X Multi-task Learning with Sub-relations Multi-task learning has been shown to be beneficial in training neural networks (Caruana, 1998; Collobert ftj = σ(Wf xt + Uf ×T (hj ⊗ ej ) + bf ) and Weston, 2008; Peng and Dredze, 2016). By X learning contextual entity representations, our frameUo ×T (hj ⊗ ej ) + bo ) ot = σ(Wo xt + j∈P (t) work makes it straightforward to conduct multi-task X Uc ×T (hj ⊗ ej ) + bc ) learning. The only change is to add a separate classic˜t = tanh(Wc xt + j∈P (t) X fier for each related auxiliary relation. All classifiers ftj cj ct = it c˜t + share the same graph LSTMs representation learner j∈P (t) and word embeddings, and can potentially help each ht = ot tanh(ct ) other by pooling their supervision signals. U ’s are now l × l × d tensors (l is the dimension of In the molecular tumor board"
Q17-1008,D14-1162,0,0.0882041,"Missing"
Q17-1008,C08-1088,0,0.0226594,"n the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolu"
Q17-1008,E17-1110,1,0.202831,"icity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory networks (graph LSTMs). By adopting the graph formulation, our framework subsumes prior approaches based on chain or tree LSTMs, and can incorporate a rich set of linguistic analyses to aid relation extraction. Relation classification takes as input the entity representations learned from the entire text, and can be easily extended for arbitrary re"
Q17-1008,N12-3006,1,0.827666,"f papers contain knowledge about drug-gene-mutation interactions. Extracting such knowledge from the vast body of biomedical papers is exactly the challenge. As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts. In future work, we will explore incorporating more known facts for distant supervision and extracting from more full-text articles. We conducted tokenization, part-of-speech tagging, and syntactic parsing using SPLAT (Quirk et al., 2012), and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP (Manning et al., 2014). We used the entity taggers from Literome (Poon et al., 2014) to identify drug, gene and mutation mentions. We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision. The knowledge bases distinguish fine-grained interaction types, which we do not use in this paper. 5 6 Distant Supervision After identifying drug, gene and mutation mentions in the text, co-occurring trip"
Q17-1008,reschke-etal-2014-event,0,0.0164171,"ent, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions. Recently, cross-sentence relation extraction models have been learned with distant supervision, and used integrated contextual evidence of diverse types without reliance on these assumptions (Quirk and Poon, 2017), but that work focused on binary relations only and explicitly engineered sparse indicator features. Relation extraction using distant supervision Distant supervision has been applied to extraction of binary (Mintz et al., 2009; Poon et al., 2015) and n-ary (Reschke et al., 2014; Li et al., 2015) relations, traditionally using hand-engineered features. Neural architectures have recently been applied to distantly supervised extraction of binary relations (Zeng et al., 2015). Our work is the first to propose a neural architecture for n-ary relation extraction, where the representation of a tuple of entities is not decomposable into independent representations of the individual entities or entity pairs, and which integrates diverse information from multi-sentential context. To utilize training data more effectively, we show how multitask learning for component binary su"
Q17-1008,P16-1113,0,0.055469,"Missing"
Q17-1008,P15-1061,0,0.43501,"iLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-directional LSTM (BiLSTM). Following Wang et al. (2016), we used input attention for the CNN and a input window size of 5. Quirk and Poon (2017) only extracted binary relations. We extended it to ternary relations by deriving features for each entity pair (with added annotation to signify the two entity types), and pooling the features 108 from all pairs. For binary relation extraction, prior syntax-aware approaches are directly applicable. So we also compared with a state-of-the-art tree LSTM system (Miwa and Bansal, 2016) and a BiLSTM on the shortest dependency"
Q17-1008,D12-1110,0,0.158707,"automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure. Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer. N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998). Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker"
Q17-1008,R11-1004,0,0.156856,"gument, by embedding the dependency path between each pair, or by combining features of the two using a feed-forward network. Although some re-ranking or joint inference models have been employed, the representations of the individual arguments do not influence each other. In contrast, we propose a neural architecture that jointly represents n entity mentions, taking into account long-distance dependencies and inter-sentential information. Cross-sentence relation extraction Several relation extraction tasks have benefited from crosssentence extraction, including MUC fact and event extraction (Swampillai and Stevenson, 2011), record extraction from web pages (Wick et al., 2006), extraction of facts for biomedical domains (Yoshikawa et al., 2011), and extensions of semantic role labeling to cover implicit inter-sentential arguments (Gerber and Chai, 2010). These prior works have either relied on explicit co-reference annotation, or on the assumption that the whole document refers to a single coherent event, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions. Recently, cross-sentence relation extraction models have been learned with distant super"
Q17-1008,P15-1150,0,0.811543,"hese problems by using a series of gates (input, forget and output) to avoid amplifying or suppressing gradients during backpropagation. Consequently, LSTMs are much more effective in capturing long-distance dependencies, and have been applied to a variety of NLP tasks. However, most approaches are based on linear chains and only explicitly model the linear context, which ignores a variety of linguistic analyses, such as syntactic and discourse dependencies. In this section, we propose a general framework that generalizes LSTMs to graphs. While there is some prior work on learning tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), to the best of our knowledge, graph LSTMs have not been applied to any NLP task yet. Figure 2 shows the architecture of this approach. The input layer is the word embedding of input text. Next is the graph LSTM which learns a contextual representation for each word. For the entities in question, their contextual representations are concatenated and become the input to the relation classifiers. For a multi-word entity, we simply used the average of its word representations and leave the exploration of more sophisticated aggregation approaches to future work. The layers"
Q17-1008,P16-1123,0,0.436558,"Missing"
Q17-1008,W06-1671,0,0.152345,"ombining features of the two using a feed-forward network. Although some re-ranking or joint inference models have been employed, the representations of the individual arguments do not influence each other. In contrast, we propose a neural architecture that jointly represents n entity mentions, taking into account long-distance dependencies and inter-sentential information. Cross-sentence relation extraction Several relation extraction tasks have benefited from crosssentence extraction, including MUC fact and event extraction (Swampillai and Stevenson, 2011), record extraction from web pages (Wick et al., 2006), extraction of facts for biomedical domains (Yoshikawa et al., 2011), and extensions of semantic role labeling to cover implicit inter-sentential arguments (Gerber and Chai, 2010). These prior works have either relied on explicit co-reference annotation, or on the assumption that the whole document refers to a single coherent event, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions. Recently, cross-sentence relation extraction models have been learned with distant supervision, and used integrated contextual evidence of div"
Q17-1008,D15-1062,0,0.540872,"able 1: Average test accuracy in five-fold crossvalidation for drug-gene-mutation ternary interactions. Feature-Based used the best performing model in (Quirk and Poon, 2017) with features derived from shortest paths between all entity pairs. Model Single-Sent. Cross-Sent. Feature-Based 73.9 75.2 CNN BiLSTM BiLSTM-Shortest-Path Tree LSTM Graph LSTM-EMBED Graph LSTM-FULL 73.0 73.9 70.2 75.9 74.3 75.6 74.9 76.0 71.7 75.9 76.5 76.7 Table 2: Average test accuracy in five-fold crossvalidation for drug-mutation binary relations, with an extra baseline using a BiLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-dire"
Q17-1008,D15-1206,0,0.709967,"able 1: Average test accuracy in five-fold crossvalidation for drug-gene-mutation ternary interactions. Feature-Based used the best performing model in (Quirk and Poon, 2017) with features derived from shortest paths between all entity pairs. Model Single-Sent. Cross-Sent. Feature-Based 73.9 75.2 CNN BiLSTM BiLSTM-Shortest-Path Tree LSTM Graph LSTM-EMBED Graph LSTM-FULL 73.0 73.9 70.2 75.9 74.3 75.6 74.9 76.0 71.7 75.9 76.5 76.7 Table 2: Average test accuracy in five-fold crossvalidation for drug-mutation binary relations, with an extra baseline using a BiLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-dire"
Q17-1008,C16-1138,0,0.0397931,"egrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchi"
Q17-1008,K15-2001,0,0.021367,"hs (bottom); the graph LSTMs is constructed by a forward pass (Left to Right) followed by a backward pass (Right to Left). Note that information goes from dependency child to parent. 3.1 Document Graph To model various dependencies from linguistic analysis at our disposal, we follow Quirk and Poon (2017) and introduce a document graph to capture intra- and inter-sentential dependencies. A document graph consists of nodes that represent words and edges that represent various dependencies such as linear context (adjacent words), syntactic dependencies, and discourse relations (Lee et al., 2013; Xue et al., 2015). Figure 1 shows the document graph for our running example; this instance suggests that tumors with L858E mutation in EGFR gene responds to the drug gefitinib. This document graph acts as the backbone upon which a graph LSTM is constructed. If it con104 3.2 Backpropagation in Graph LSTMs Conventional LSTMs are essentially very deep feedforward neural networks. For example, a left-to-right linear LSTM has one hidden vector for each word. This vector is generated by a neural network (recurrent unit) that takes as input the embedding of the given word and the hidden vector of the previous word."
Q17-1008,C14-1220,0,0.860338,"baseline using a BiLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-directional LSTM (BiLSTM). Following Wang et al. (2016), we used input attention for the CNN and a input window size of 5. Quirk and Poon (2017) only extracted binary relations. We extended it to ternary relations by deriving features for each entity pair (with added annotation to signify the two entity types), and pooling the features 108 from all pairs. For binary relation extraction, prior syntax-aware approaches are directly applicable. So we also compared with a state-of-the-art tree LSTM system (Miwa and Bansal, 2016) and a BiLSTM on th"
Q17-1008,D15-1203,0,0.237949,"h distant supervision, and used integrated contextual evidence of diverse types without reliance on these assumptions (Quirk and Poon, 2017), but that work focused on binary relations only and explicitly engineered sparse indicator features. Relation extraction using distant supervision Distant supervision has been applied to extraction of binary (Mintz et al., 2009; Poon et al., 2015) and n-ary (Reschke et al., 2014; Li et al., 2015) relations, traditionally using hand-engineered features. Neural architectures have recently been applied to distantly supervised extraction of binary relations (Zeng et al., 2015). Our work is the first to propose a neural architecture for n-ary relation extraction, where the representation of a tuple of entities is not decomposable into independent representations of the individual entities or entity pairs, and which integrates diverse information from multi-sentential context. To utilize training data more effectively, we show how multitask learning for component binary sub-relations can improve performance. Our learned representation combines information sources within a single sentence in a more integrated and generalizable fashion than prior approaches, and can al"
Q17-1008,Y15-1009,0,0.0475333,"fully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on mo"
Q17-1008,C98-1013,0,\N,Missing
W02-1604,W01-0808,1,0.849298,"t string. The generation module is language-specific and used for both monolingual generation and MT. In the context of MT, generation takes as input the transferred LF and converts it into a basic syntactic tree. A small set of heuristic rules preprocesses the transferred LF to “nativize” some structural differences, such as pro-drop phenomena in Japanese. A series of core generation rules then applies to the LF tree, transforming it into a Japanese sentence string. Generation rules operate on a single tree only, are application-independent and are developed in a monolingual environment (see Aikawa et al. 2001a, 2001b for further details.) Generation of inflectional morphology is also handled in this component. The generation component has no explicit knowledge of the source language. 2 Acquisition of Complex Structural Mappings The generalization provided by LF makes it possible for MSR-MT to handle complex structural relations in cases where English and Japanese are systematically divergent. This is 2 MSR-MT resorts to lexical lookup only when a term is not found in the Mindnet. The handcrafted dictionary is slated for replacement by purely statistically generated data. Training Data Translation"
W02-1604,J90-2002,0,0.153813,"transfer patterns between languages, and generate output strings. Abstraction permits structural neutralizations that facilitate learning of translation examples across languages with radically different surface structure characteristics, and allows MT development to proceed within a largely languageindependent NLP architecture. Comparative evaluation indicates that after training in a domain the English-Japanese system is statistically indistinguishable from a non-customized commercially available MT system in the same domain. Introduction In the wake of the pioneering work of Nagao (1984), Brown et al. (1990) and Sato and Nagao (1990), Machine Translation (MT) research has increasingly focused on the issue of how to acquire translation knowledge from aligned parallel texts. While much of this research effort has focused on acquisition of correspondences between individual lexical items or between unstructured strings of words, closer attention has begun to be paid to the learning of structured phrasal units: Yamamoto and Matsumoto (2000), for example, describe a method for automatically extracting correspondences between dependency relations in Japanese and English. Similarly, Imamura (2001a, 2001"
W02-1604,C00-1057,1,0.870678,"Missing"
W02-1604,W01-1406,1,0.832484,"ociations,” that is, lexical mappings extracted from the training corpora using statistical techniques based on mutual information (Moore 2001). From these possible lexical correspondences, the algorithm uses a small grammar of (language-pair-independent) rules to align LF nodes on lexical and structural principles. The aligned LF pairs are then partitioned into smaller aligned LF segments, with individual node mappings captured in a relationship we call “sublinking.” Finally, the aligned LF segments are filtered on the basis of frequency, and compiled into a database known as a Mindnet. (See Menezes & Richardson 2001 for a detailed description of this process.) The Mindnet is a general-purpose database of semantic information (Richardson et al. 1998) that has been repurposed as the primary repository of translation information for MT applications. The process of building the Mindnet is entirely automated; there is no human vetting of candidate entries. At the end of a typical training session, 1,816,520 transfer In the Mindnet, LF segments from the source language are represented as linked to the corresponding LF segment from the target languages. These can be seen in Figs. 3 and 4, discussed below in Sec"
W02-1604,W01-1411,0,0.0129651,"deploys the general-purpose parsers to analyze the English and Japanese sentence pairs and generate LFs for each sentence. In the next step, an LF alignment algorithm is used to match source language and target language LFs at the sub-sentence level. The LF alignment algorithm first establishes tentative lexical correspondences between nodes in the source and target LFs on the basis of lexical matching over dictionary information and approximately 31,000 “word associations,” that is, lexical mappings extracted from the training corpora using statistical techniques based on mutual information (Moore 2001). From these possible lexical correspondences, the algorithm uses a small grammar of (language-pair-independent) rules to align LF nodes on lexical and structural principles. The aligned LF pairs are then partitioned into smaller aligned LF segments, with individual node mappings captured in a relationship we call “sublinking.” Finally, the aligned LF segments are filtered on the basis of frequency, and compiled into a database known as a Mindnet. (See Menezes & Richardson 2001 for a detailed description of this process.) The Mindnet is a general-purpose database of semantic information (Richa"
W02-1604,2001.mtsummit-papers.50,0,0.0341849,"Missing"
W02-1604,2002.tmi-papers.16,0,0.0221111,"Missing"
W02-1604,W01-1402,1,0.792933,"00). These parsers are robust in that if the analysis grammar fails to find an appropriate parse, it outputs a best-guess “fitted” parse. System development is not confined to English-Japanese: MSR-MT is part of a broader natural language processing project involving three Asian languages (Japanese, Chinese, and Korean) and four European languages (English, French, German, and Spanish). Development of the MSR-MT systems proceeds more or less simultaneously across these languages and in multiple directions, including Japanese-English. The Spanish-English version of MSR-MT has been described in Richardson et al. 2001a, Richardson et al 2001b, and the reader is referred to these papers for more information concerning algorithms employed during phrase alignment. A description of the French-Spanish MT system is found in Pinkham & Smets. 2002. 1.1 Training Data MSR-MT requires that a large corpus of aligned sentences be available as examples for training. For English-Japanese MT, the system currently trains on a corpus of approximately 596,000 pre-aligned sentence pairs. About 274,000 of these are sentence pairs extracted from Microsoft technical documentation that had been professionally translated from Engl"
W02-1604,2001.mtsummit-papers.53,1,0.770798,"00). These parsers are robust in that if the analysis grammar fails to find an appropriate parse, it outputs a best-guess “fitted” parse. System development is not confined to English-Japanese: MSR-MT is part of a broader natural language processing project involving three Asian languages (Japanese, Chinese, and Korean) and four European languages (English, French, German, and Spanish). Development of the MSR-MT systems proceeds more or less simultaneously across these languages and in multiple directions, including Japanese-English. The Spanish-English version of MSR-MT has been described in Richardson et al. 2001a, Richardson et al 2001b, and the reader is referred to these papers for more information concerning algorithms employed during phrase alignment. A description of the French-Spanish MT system is found in Pinkham & Smets. 2002. 1.1 Training Data MSR-MT requires that a large corpus of aligned sentences be available as examples for training. For English-Japanese MT, the system currently trains on a corpus of approximately 596,000 pre-aligned sentence pairs. About 274,000 of these are sentence pairs extracted from Microsoft technical documentation that had been professionally translated from Engl"
W02-1604,P98-2180,0,0.0252536,"2001). From these possible lexical correspondences, the algorithm uses a small grammar of (language-pair-independent) rules to align LF nodes on lexical and structural principles. The aligned LF pairs are then partitioned into smaller aligned LF segments, with individual node mappings captured in a relationship we call “sublinking.” Finally, the aligned LF segments are filtered on the basis of frequency, and compiled into a database known as a Mindnet. (See Menezes & Richardson 2001 for a detailed description of this process.) The Mindnet is a general-purpose database of semantic information (Richardson et al. 1998) that has been repurposed as the primary repository of translation information for MT applications. The process of building the Mindnet is entirely automated; there is no human vetting of candidate entries. At the end of a typical training session, 1,816,520 transfer In the Mindnet, LF segments from the source language are represented as linked to the corresponding LF segment from the target languages. These can be seen in Figs. 3 and 4, discussed below in Section 2. 1.4 Transfer and Generation At translation time, the broad-coverage source language parser processes the English input sentence,"
W02-1604,C90-3044,0,0.0695244,"en languages, and generate output strings. Abstraction permits structural neutralizations that facilitate learning of translation examples across languages with radically different surface structure characteristics, and allows MT development to proceed within a largely languageindependent NLP architecture. Comparative evaluation indicates that after training in a domain the English-Japanese system is statistically indistinguishable from a non-customized commercially available MT system in the same domain. Introduction In the wake of the pioneering work of Nagao (1984), Brown et al. (1990) and Sato and Nagao (1990), Machine Translation (MT) research has increasingly focused on the issue of how to acquire translation knowledge from aligned parallel texts. While much of this research effort has focused on acquisition of correspondences between individual lexical items or between unstructured strings of words, closer attention has begun to be paid to the learning of structured phrasal units: Yamamoto and Matsumoto (2000), for example, describe a method for automatically extracting correspondences between dependency relations in Japanese and English. Similarly, Imamura (2001a, 2001b) seeks to match correspo"
W02-1604,C00-2119,1,0.871246,"Missing"
W02-1604,C00-2135,0,0.0709887,"Missing"
W02-1604,2001.mtsummit-ebmt.4,1,\N,Missing
W02-1604,C98-2175,0,\N,Missing
W04-3219,P01-1008,0,0.257977,"of the resulting corpus. A monotone phrasal decoder generates contextual replacements. Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches. 1 Introduction The ability to categorize distinct word sequences as “meaning the same thing” is vital to applications as diverse as search, summarization, dialog, and question answering. Recent research has treated paraphrase acquisition and generation as a machine learning problem (Barzilay & McKeown, 2001; Lin & Pantel, 2002; Shinyama et al, 2002, Barzilay & Lee, 2003, Pang et al., 2003). We approach this problem as one of statistical machine translation (SMT), within the noisy channel model of Brown et al. (1993). That is, we seek to identify the optimal paraphrase T* of a sentence S by finding: T * = arg max{P(T |S )} T = arg max{P( S |T ) P(T )} T T and S being sentences in the same language. We describe and evaluate an SMT-based paraphrase generation system that utilizes a monotone phrasal decoder to generate meaning-preserving paraphrases across multiple domains. By adopting at the outset"
W04-3219,N03-1003,0,0.650357,"lternate WordNet baseline using a target language model. 8 In combination with the language model described in section 3.4, we used a very simple replacement model: each appropriately inflected member of the most frequent synset was proposed as a possible replacement with uniform probability. This was intended to isolate the contribution of the language model from the replacement model. Given that our alignments, while aggregated into phrases, are fundamentally word-aligned, one question that arises is whether the information we learn is different in character than that learned 8 In contrast, Barzilay and Lee (2003) avoided using a language model for essentially the same reason: their MSA approach did not take advantage of such a resource. Evaluation We have experimented with several methods for extracting a parallel sentence-aligned corpus from news clusters using word alignment error rate, or AER, (Och & Ney 2003) as an evaluation metric. A brief summary of these experiments is provided in Table 1. To evaluate the quality of generation, we followed the lead of Barzilay & Lee (2003). We started with the 59 sentences and corresponding paraphrases from MSA and WordNet (designated as WN below). Since the s"
W04-3219,C04-1051,1,0.664678,"edit distance to identify likely paraphrases has the unfortunate result of excluding interesting sentence pairs that are similar in meaning though different in form. For example: The Cassini spacecraft, which is en route to Saturn, is about to make a close pass of the ringed planet's mysterious moon Phoebe On its way to an extended mission at Saturn, the Cassini probe on Friday makes its closest rendezvous with Saturn's dark moon Phoebe. We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al. 2004). While noisier than the edit distance data, initial results suggest that these can be a rich source of information about larger phrasal substitutions and syntactic reordering. Although we have not attempted to address the issue of paraphrase identification here, we are currently exploring machine learning techniques, based in part on features of document structure and other linguistic features that should allow us to bootstrap initial alignments to develop more data. This will we hope, eventually allow us to address such issues as paraphrase identification for IR. To exploit richer data sets,"
W04-3219,W03-1608,0,0.32129,"cient monolingual parallel data.1 We show that a huge corpus of comparable and alignable sentence pairs can be culled from ready-made topical/temporal clusters of news articles gathered on a daily basis from thousands of sources on the World Wide Web, thereby permitting the system to operate outside the narrow domains typical of existing systems. 2 Related work Until recently, efforts in paraphrase were not strongly focused on generation and relied primarily on narrow data sources. One data source has been multiple translations of classic literary works (Barzilay & McKeown 2001; Ibrahim 2002; Ibrahim et al. 2003). Pang et al. (2003) obtain parallel monolingual texts from a set of 100 multiply-translated news articles. While translation-based approaches to obtaining data do address the problem of how to identify two strings as meaning the same thing, they are limited in scalability owing to the difficulty (and expense) of obtaining large quantities of multiply-translated source documents. Other researchers have sought to identify patterns in large unannotated monolingual corpora. Lin & Pantel (2002) derive inference rules by parsing text fragments and extracting semantically similar paths. Shinyama et"
W04-3219,N03-1017,0,0.0189416,"Missing"
W04-3219,W03-0301,0,0.0106332,"Missing"
W04-3219,P00-1056,0,0.167624,"Missing"
W04-3219,J03-1002,0,0.00962553,"isolate the contribution of the language model from the replacement model. Given that our alignments, while aggregated into phrases, are fundamentally word-aligned, one question that arises is whether the information we learn is different in character than that learned 8 In contrast, Barzilay and Lee (2003) avoided using a language model for essentially the same reason: their MSA approach did not take advantage of such a resource. Evaluation We have experimented with several methods for extracting a parallel sentence-aligned corpus from news clusters using word alignment error rate, or AER, (Och & Ney 2003) as an evaluation metric. A brief summary of these experiments is provided in Table 1. To evaluate the quality of generation, we followed the lead of Barzilay & Lee (2003). We started with the 59 sentences and corresponding paraphrases from MSA and WordNet (designated as WN below). Since the size of this data set made it difficult to obtain statistically significant results, we also included 141 randomly selected sentences from held-out clusters. We then produced paraphrases with each of the following systems and compared them with MSA and WN: • • • WN+LM: WordNet with a trigram LM CL: Statist"
W04-3219,N03-1024,0,0.186586,"an evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches. 1 Introduction The ability to categorize distinct word sequences as “meaning the same thing” is vital to applications as diverse as search, summarization, dialog, and question answering. Recent research has treated paraphrase acquisition and generation as a machine learning problem (Barzilay & McKeown, 2001; Lin & Pantel, 2002; Shinyama et al, 2002, Barzilay & Lee, 2003, Pang et al., 2003). We approach this problem as one of statistical machine translation (SMT), within the noisy channel model of Brown et al. (1993). That is, we seek to identify the optimal paraphrase T* of a sentence S by finding: T * = arg max{P(T |S )} T = arg max{P( S |T ) P(T )} T T and S being sentences in the same language. We describe and evaluate an SMT-based paraphrase generation system that utilizes a monotone phrasal decoder to generate meaning-preserving paraphrases across multiple domains. By adopting at the outset a paradigm geared toward generating sentences, this approach overcomes many problem"
W04-3219,W01-1401,0,0.0210743,"Missing"
W04-3219,P97-1037,0,0.0043073,"hrasal replacement probability with MLE (Vogel et al. 2003). Secondly, it appears to boost translation quality in more sophisticated translation systems by inducing lexical triggering (Och et al. 2004). Collocations and other non-compositional phrases receive a higher probability as a whole than they would as independent single word replacements. One further simplification was made. Given that our domain is restricted to the generation of monolingual paraphrase, interesting output can be produced without tackling the difficult problem of inter-phrase reordering.7 Therefore, along the lines of Tillmann et al. (1997), we rely on only monotone phrasal alignments, although we do allow intra-phrasal reordering. While this means certain common structural alternations (e.g., active/passive) cannot be generated, we are still able to express a broad range of phenomena: pings to be both unwieldy in practice and very often indicative of poor a word alignment. 7 Even in the realm of MT, such an assumption can produce competitive results (Vogel et al. 2003). In addition, we were hesitant to incur the exponential increase in running time associated with those movement models in the tradition of Brown el al (1993), es"
W04-3219,P03-1020,0,0.011373,"t case O(kn), where n is the maximal target length and k is the maximal number of replacements for any word). In addition, fast algorithms exist for computing the n-best lists over a lattice (Soong & Huang 1991). Figure 2. A simplified generation lattice: 44 top ranked edges from a total 4,140 Finally the resultant paraphrases were cleaned up in a post-processing phase to ensure output was not trivially distinguishable from other systems during human evaluation. All generic named entity tokens were re-instantiated with their source values, and case was restored using a model like that used in Vita et al. (2003). from much simpler techniques. To explore this hypothesis, we introduced an additional baseline that used statistical clustering to produce an automated, unsupervised synonym list, again with a trigram language model. We used standard bigram clustering techniques (Goodman 2002) to produce 4,096 clusters of our 65,225 vocabulary items. 3.5 4 Alternate approaches Barzilay & Lee (2003) have released a common dataset that provides a basis for comparing different paraphrase generation systems. It consists of 59 sentences regarding acts of violence in the Middle East. These are accompanied by parap"
W04-3219,C96-2141,0,0.0903917,"ces were identical or differed only in punctuation; Duplicate sentence pairs; Sentence pairs with significantly different lengths (the shorter is less than two-thirds the length of the longer); Sentence pairs where the Levenshtein distance was greater than 12.0.3 A total of 139K non-identical sentence pairs were obtained. Mean Levenshtein distance was 5.17; mean sentence length was 18.6 words. 3.2 Word alignment To this corpus we applied the word alignment algorithms available in Giza++ (Och & Ney, 2000), a freely available implementation of IBM Models 1-5 (Brown, 1993) and the HMM alignment (Vogel et al, 1996), along with various improvements and modifications motivated by experimentation by Och & Ney (2000). In order to capture the many-to-many alignments that identify correspondences between idioms and other phrasal chunks, we align in the forward direction and again in the backward direction, heuristically recombining each unidirectional word alignment into a single bidirectional alignment (Och & Ney 2000). Figure 1 shows an example of a monolingual alignment produced by Giza++. Each line represents a uni-directional link; directionality is indicated by a tick mark on the target side of the link"
W04-3219,2003.mtsummit-papers.53,0,0.0440398,"y running five iterations of Model 1 on the training corpus. This allows for computing the probability of a sequence of source words S given a sequence of target words T as the sum over all possible alignments of the Model 1 probabilities: P (S |T ) = ∑ P (S , A |T ) A = ∏ ∑ P (s |t ) t∈T s∈S (Brown et al. (1993) provides a more detailed derivation of this identity.) Although simple, this approach has proven effective in SMT for several reasons. First and foremost, phrasal scoring by Model 1 avoids the sparsity problems associated with estimating each phrasal replacement probability with MLE (Vogel et al. 2003). Secondly, it appears to boost translation quality in more sophisticated translation systems by inducing lexical triggering (Och et al. 2004). Collocations and other non-compositional phrases receive a higher probability as a whole than they would as independent single word replacements. One further simplification was made. Given that our domain is restricted to the generation of monolingual paraphrase, interesting output can be produced without tackling the difficult problem of inter-phrase reordering.7 Therefore, along the lines of Tillmann et al. (1997), we rely on only monotone phrasal al"
W06-1608,N03-1017,0,0.067073,"following sections we provide background on the data used for training, the dependency parsing framework used to produce treelets, the treelet translation framework and salient characteristics of the target languages. Introduction The current study is a response to a question that proponents of syntactically-informed machine translation frequently encounter: How sensitive is a syntactically-informed machine translation system to the quality of the input syntactic analysis? It has been shown that phrasal machine translation systems are not affected by the quality of the input word alignments (Koehn et al., 2003). This finding has generally been cast in favorable terms: such systems are robust to poor quality word alignment. A less favorable interpretation of these results might be to conclude that phrasal statistical machine translation (SMT) systems do not stand to benefit from improvements in word alignment. In a similar vein, one might ask whether contemporary syntactically-informed machine translation systems would benefit from improvements in parse accuracy. One possibility is that current syntactically-informed SMT systems are deriving only limited value from the syntactic analyses, and would t"
W06-1608,J93-2004,0,0.0288351,"ine learn2.1 Dependency parsing Dependency analysis is an alternative to constituency analysis (Tesni`ere, 1959; Melˇcuk, 1988). In a dependency analysis of syntax, words directly modify other words, with no intervening non-lexical nodes. We use the terms child node and parent node to denote the tokens in a dependency relation. Each child has a single parent, with the lexical root of the sentence dependent on a synthetic ROOT node. We use the parsing approach described in (Corston-Oliver et al., 2006). The parser is trained on dependencies extracted from the English Penn Treebank version 3.0 (Marcus et al., 1993) by using the head-percolation rules of (Yamada and Matsumoto, 2003). Given a sentence x, the goal of the parser is to find the highest-scoring parse yˆ among all possible parses y ∈ Y : yˆ = arg max s(x, y) y∈Y (1) The score of a given parse y is the sum of the 62 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 62–69, c Sydney, July 2006. 2006 Association for Computational Linguistics scores of all its dependency links (i, j) ∈ y: s(x, y) = ∑ d(i, j) = (i, j)∈y ∑ w · f(i, j) pair; together, the target language treelets in those treele"
W06-1608,P00-1056,0,0.026173,"isner, 1996). T ∗ = arg max T ∑ λ f f (S, T, A) (3) f ∈F The models include inverted and direct channel models estimated by relative frequency, lexical weighting channel models following (Vogel et al., 2003), a trigram target language model using modified Kneser-Ney smoothing (Goodman, 2001), an order model following (Quirk et al., 2005), and word count and phrase count functions. The weights for these models are determined using the method described in (Och, 2003). To estimate the models and extract the treelets, we begin from a parallel corpus. First the corpus is word-aligned using GIZA++ (Och and Ney, 2000), then the source sentence are parsed, and finally dependencies are projected onto the target side following the heuristics described in (Quirk et al., 2005). This word aligned parallel dependency tree corpus provides training material for an order model and a target language tree-based language model. We also extract treelet translation pairs from this parallel corpus. To limit the combinatorial explosion of treelets, we only gather treelets that contain at most four words and at most two gaps in the surface string. This limits the number of mappings to be O(n3 ) in the worst case, where n is"
W06-1608,P03-1021,0,0.00643006,"ith wide margin techniques. To find the optimal parse given the weight vector w and feature vector f(i, j) we use the decoder described in (Eisner, 1996). T ∗ = arg max T ∑ λ f f (S, T, A) (3) f ∈F The models include inverted and direct channel models estimated by relative frequency, lexical weighting channel models following (Vogel et al., 2003), a trigram target language model using modified Kneser-Ney smoothing (Goodman, 2001), an order model following (Quirk et al., 2005), and word count and phrase count functions. The weights for these models are determined using the method described in (Och, 2003). To estimate the models and extract the treelets, we begin from a parallel corpus. First the corpus is word-aligned using GIZA++ (Och and Ney, 2000), then the source sentence are parsed, and finally dependencies are projected onto the target side following the heuristics described in (Quirk et al., 2005). This word aligned parallel dependency tree corpus provides training material for an order model and a target language tree-based language model. We also extract treelet translation pairs from this parallel corpus. To limit the combinatorial explosion of treelets, we only gather treelets that"
W06-1608,P02-1040,0,0.105466,"Missing"
W06-1608,P05-1034,1,0.422233,"ined to form a Bayes Point Machine (Herbrich et al., 2001; Harrington et al., 2003), resulting in a linear classifier that is competitive with wide margin techniques. To find the optimal parse given the weight vector w and feature vector f(i, j) we use the decoder described in (Eisner, 1996). T ∗ = arg max T ∑ λ f f (S, T, A) (3) f ∈F The models include inverted and direct channel models estimated by relative frequency, lexical weighting channel models following (Vogel et al., 2003), a trigram target language model using modified Kneser-Ney smoothing (Goodman, 2001), an order model following (Quirk et al., 2005), and word count and phrase count functions. The weights for these models are determined using the method described in (Och, 2003). To estimate the models and extract the treelets, we begin from a parallel corpus. First the corpus is word-aligned using GIZA++ (Och and Ney, 2000), then the source sentence are parsed, and finally dependencies are projected onto the target side following the heuristics described in (Quirk et al., 2005). This word aligned parallel dependency tree corpus provides training material for an order model and a target language tree-based language model. We also extract t"
W06-1608,W02-1001,0,0.0118987,"f-speech (POS), lexeme and stem of the parent and child tokens, the POS of tokens adjacent to the child and parent, and the POS of each token that intervenes between the parent and child. Various combinations of these features are used, for example a new feature is created that combines the POS of the parent, lexeme of the parent, POS of the child and lexeme of the child. Each feature is also conjoined with the direction and distance of the parent, e.g. does the child precede or follow the parent, and how many tokens intervene? To set the weight vector w, we train twenty averaged perceptrons (Collins, 2002) on different shuffles of data drawn from sections 02–21 of the Penn Treebank. The averaged perceptrons are then combined to form a Bayes Point Machine (Herbrich et al., 2001; Harrington et al., 2003), resulting in a linear classifier that is competitive with wide margin techniques. To find the optimal parse given the weight vector w and feature vector f(i, j) we use the decoder described in (Eisner, 1996). T ∗ = arg max T ∑ λ f f (S, T, A) (3) f ∈F The models include inverted and direct channel models estimated by relative frequency, lexical weighting channel models following (Vogel et al., 2"
W06-1608,N03-1033,0,0.0489124,"al machine translation. One method for producing parsers of varying quality might be to train a parser and then to transform its output, e.g. 64 Dependency accuracy. by replacing the parser’s selection of the parent for certain tokens with different nodes. Rather than randomly adding noise to the parses, we decided to vary the quality in ways that more closely mimic the situation that confronts us as we develop machine translation systems. Annotating data for POS requires considerably less human time and expertise than annotating syntactic relations. We therefore used an automatic POS tagger (Toutanova et al., 2003) trained on the complete training section of the Penn Treebank (sections 02–21). Annotating syntactic dependencies is time consuming and requires considerable linguistic expertise.1 We can well imagine annotating syntactic dependencies in order to develop a machine translation system by annotating first a small quantity of data, training a parser, training a system that uses the parses produced by that parser and assessing the quality of the machine translation output. Having assessed the quality of the output, one might annotate additional data and train systems until it appears that the qual"
W06-1608,corston-oliver-gamon-2004-normalizing,1,0.837924,"ypologically in ways that are especially problematic for current approaches to statistical machine translation as we shall now illustrate. We believe that these typological differences make English-to-German machine translation a fertile test bed for syntax-based SMT. German has richer inflectional morphology than English, with obligatory marking of case, number and lexical gender on nominal elements and person, number, tense and mood on verbal elements. This morphological complexity, combined with pervasive, productive noun compounding is problematic for current approaches to word alignment (Corston-Oliver and Gamon, 2004). Equally problematic for machine translation is the issue of word order. The position of verbs is strongly determined by clause type. For example, in main clauses in declarative sentences, finite verbs occur as the second constituent of the sentence, but certain non-finite verb forms occur in final position. In Figure 1, for example, the English “can” aligns with German “k¨onnen” in second position and “set” aligns with German “festlegen” in final position. Aside from verbs, German is usually characterized as a “free word-order” language: major constituents of the sentence may occur in variou"
W06-1608,2003.mtsummit-papers.53,0,0.00920668,"(Collins, 2002) on different shuffles of data drawn from sections 02–21 of the Penn Treebank. The averaged perceptrons are then combined to form a Bayes Point Machine (Herbrich et al., 2001; Harrington et al., 2003), resulting in a linear classifier that is competitive with wide margin techniques. To find the optimal parse given the weight vector w and feature vector f(i, j) we use the decoder described in (Eisner, 1996). T ∗ = arg max T ∑ λ f f (S, T, A) (3) f ∈F The models include inverted and direct channel models estimated by relative frequency, lexical weighting channel models following (Vogel et al., 2003), a trigram target language model using modified Kneser-Ney smoothing (Goodman, 2001), an order model following (Quirk et al., 2005), and word count and phrase count functions. The weights for these models are determined using the method described in (Och, 2003). To estimate the models and extract the treelets, we begin from a parallel corpus. First the corpus is word-aligned using GIZA++ (Och and Ney, 2000), then the source sentence are parsed, and finally dependencies are projected onto the target side following the heuristics described in (Quirk et al., 2005). This word aligned parallel dep"
W06-1608,N06-1021,1,0.788229,"s. Another possibility is that syntactic analysis does indeed contain valuable information that could be exploited by machine learn2.1 Dependency parsing Dependency analysis is an alternative to constituency analysis (Tesni`ere, 1959; Melˇcuk, 1988). In a dependency analysis of syntax, words directly modify other words, with no intervening non-lexical nodes. We use the terms child node and parent node to denote the tokens in a dependency relation. Each child has a single parent, with the lexical root of the sentence dependent on a synthetic ROOT node. We use the parsing approach described in (Corston-Oliver et al., 2006). The parser is trained on dependencies extracted from the English Penn Treebank version 3.0 (Marcus et al., 1993) by using the head-percolation rules of (Yamada and Matsumoto, 2003). Given a sentence x, the goal of the parser is to find the highest-scoring parse yˆ among all possible parses y ∈ Y : yˆ = arg max s(x, y) y∈Y (1) The score of a given parse y is the sum of the 62 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 62–69, c Sydney, July 2006. 2006 Association for Computational Linguistics scores of all its dependency links (i,"
W06-1608,W03-3023,0,0.0339761,"ernative to constituency analysis (Tesni`ere, 1959; Melˇcuk, 1988). In a dependency analysis of syntax, words directly modify other words, with no intervening non-lexical nodes. We use the terms child node and parent node to denote the tokens in a dependency relation. Each child has a single parent, with the lexical root of the sentence dependent on a synthetic ROOT node. We use the parsing approach described in (Corston-Oliver et al., 2006). The parser is trained on dependencies extracted from the English Penn Treebank version 3.0 (Marcus et al., 1993) by using the head-percolation rules of (Yamada and Matsumoto, 2003). Given a sentence x, the goal of the parser is to find the highest-scoring parse yˆ among all possible parses y ∈ Y : yˆ = arg max s(x, y) y∈Y (1) The score of a given parse y is the sum of the 62 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 62–69, c Sydney, July 2006. 2006 Association for Computational Linguistics scores of all its dependency links (i, j) ∈ y: s(x, y) = ∑ d(i, j) = (i, j)∈y ∑ w · f(i, j) pair; together, the target language treelets in those treelet translation pairs will form the target translation. Next the targe"
W06-1608,C96-1058,0,0.0226759,"the direction and distance of the parent, e.g. does the child precede or follow the parent, and how many tokens intervene? To set the weight vector w, we train twenty averaged perceptrons (Collins, 2002) on different shuffles of data drawn from sections 02–21 of the Penn Treebank. The averaged perceptrons are then combined to form a Bayes Point Machine (Herbrich et al., 2001; Harrington et al., 2003), resulting in a linear classifier that is competitive with wide margin techniques. To find the optimal parse given the weight vector w and feature vector f(i, j) we use the decoder described in (Eisner, 1996). T ∗ = arg max T ∑ λ f f (S, T, A) (3) f ∈F The models include inverted and direct channel models estimated by relative frequency, lexical weighting channel models following (Vogel et al., 2003), a trigram target language model using modified Kneser-Ney smoothing (Goodman, 2001), an order model following (Quirk et al., 2005), and word count and phrase count functions. The weights for these models are determined using the method described in (Och, 2003). To estimate the models and extract the treelets, we begin from a parallel corpus. First the corpus is word-aligned using GIZA++ (Och and Ney,"
W06-1608,2004.tmi-1.9,0,0.0983703,"Missing"
W06-1608,C02-1036,1,\N,Missing
W06-3124,P02-1040,0,0.0950288,"ent source and target trees; s and t represent source and target treelets (connected subgraphs of the dependency tree). The expression ∀t∈ T refers to all the lexical items in the target language tree T and |T| refers to the count of lexical items in T. We use subscripts to indicate selected words: Tn represents th the n lexical item in an in-order traversal of T. 2.1. Training We use the broad coverage dependency parser NLPWIN [3] to obtain source language dependency trees, and we use GIZA++ [4] to produce word alignments. The GIZA++ training regimen and parameters are tuned to optimize BLEU [5] scores on held-out data. Using the word alignments, we follow a set of dependency tree projection heuristics [1] to construct target dependency trees, producing a word-aligned parallel dependency tree corpus. Treelet translation pairs are extracted by enumerating all source treelets (to a maximum size) aligned to a target treelet. 2.2. Decoding We use a tree-based decoder, inspired by dynamic programming. It searches for an approximation of 158 Proceedings of the Workshop on Statistical Machine Translation, pages 158–161, c New York City, June 2006. 2006 Association for Computational Linguist"
W06-3124,2004.tmi-1.14,1,0.8501,"r> m4 = <the / el> m5 = <Rio / Rio> m6 = <agenda / programa> m7 = <NULL / de> We can then predict the probability of each MTU in the context of (a) the previous MTUs in source order, (b) the previous MTUs in target order, or (c) the ancestor MTUs in the tree. We consider all of these traversal orders, each acting as a separate feature function in the log linear combination. For source and target traversal order we use a trigram model, and a bigram model for tree order. 2.3.3. Target language models We use both a surface level trigram language model and a dependency-based bigram language model [7], similar to the bilexical dependency modes used in some English Treebank parsers (e.g. [8]). ∣T∣ P surf T = ∏ P trisurf T i∣T i−2 , T i−1  i=1 ∣T∣ P bilex T =∏ P bidep T i∣ parent T i  i=1 Ptrisurf is a Kneser-Ney smoothed trigram language model trained on the target side of the training corpus, and Pbilex is a Kneser-Ney smoothed bigram language model trained on target language dependencies extracted from the aligned parallel dependency tree corpus. 2.3.4. Order model The order model assigns a probability to the position (pos) of each target node relative to its head based on infor"
W06-3124,P97-1003,0,0.0512225,"redict the probability of each MTU in the context of (a) the previous MTUs in source order, (b) the previous MTUs in target order, or (c) the ancestor MTUs in the tree. We consider all of these traversal orders, each acting as a separate feature function in the log linear combination. For source and target traversal order we use a trigram model, and a bigram model for tree order. 2.3.3. Target language models We use both a surface level trigram language model and a dependency-based bigram language model [7], similar to the bilexical dependency modes used in some English Treebank parsers (e.g. [8]). ∣T∣ P surf T = ∏ P trisurf T i∣T i−2 , T i−1  i=1 ∣T∣ P bilex T =∏ P bidep T i∣ parent T i  i=1 Ptrisurf is a Kneser-Ney smoothed trigram language model trained on the target side of the training corpus, and Pbilex is a Kneser-Ney smoothed bigram language model trained on target language dependencies extracted from the aligned parallel dependency tree corpus. 2.3.4. Order model The order model assigns a probability to the position (pos) of each target node relative to its head based on information in both the source and target trees: P order  order T ∣S ,T = ∏ P  pos  t , pa"
W06-3124,P05-1034,1,0.922582,"n the source dependency tree. These models are combined with several other knowledge sources in a log-linear manner. The weights of the individual components in the loglinear model are set by an automatic parametertuning method. We give a brief overview of the components of the system and discuss our experience with the Europarl data translating from English to Spanish. 1. Introduction The dependency treelet translation system developed at MSR is a statistical MT system that takes advantage of linguistic tools, namely a source language dependency parser, as well as a word alignment component. [1] To train a translation system, we require a sentence-aligned parallel corpus. First the source side is parsed to obtain dependency trees. Next the corpus is word-aligned, and the source dependencies are projected onto the target sentences using the word alignments. From the aligned dependency corpus we extract all treelet translation pairs, and train an order model and a bi-lexical dependency model. To translate, we parse the input sentence, and employ a decoder to find a combination and ordering of treelet translation pairs that cover the source tree and are optimal according to a set of mod"
W06-3124,P02-1038,0,0.0323972,"ain dependency trees. Next the corpus is word-aligned, and the source dependencies are projected onto the target sentences using the word alignments. From the aligned dependency corpus we extract all treelet translation pairs, and train an order model and a bi-lexical dependency model. To translate, we parse the input sentence, and employ a decoder to find a combination and ordering of treelet translation pairs that cover the source tree and are optimal according to a set of models. In a now-common generalization of the classic noisy-channel framework, we use a loglinear combination of models [2], as in below: translation S , F ,Λ=argmax T {∑ f ∈F λ f  S ,T  f } Such an approach toward translation scoring has proven very effective in practice, as it allows a translation system to incorporate information from a variety of probabilistic or non-probabilistic sources. The weights Λ = { λf } are selected by discriminatively training against held out data. 2. System Details A brief word on notation: s and t represent source and target lexical nodes; S and T represent source and target trees; s and t represent source and target treelets (connected subgraphs of the dependency tree). The e"
W06-3124,J03-1002,0,0.00362208,"em Details A brief word on notation: s and t represent source and target lexical nodes; S and T represent source and target trees; s and t represent source and target treelets (connected subgraphs of the dependency tree). The expression ∀t∈ T refers to all the lexical items in the target language tree T and |T| refers to the count of lexical items in T. We use subscripts to indicate selected words: Tn represents th the n lexical item in an in-order traversal of T. 2.1. Training We use the broad coverage dependency parser NLPWIN [3] to obtain source language dependency trees, and we use GIZA++ [4] to produce word alignments. The GIZA++ training regimen and parameters are tuned to optimize BLEU [5] scores on held-out data. Using the word alignments, we follow a set of dependency tree projection heuristics [1] to construct target dependency trees, producing a word-aligned parallel dependency tree corpus. Treelet translation pairs are extracted by enumerating all source treelets (to a maximum size) aligned to a target treelet. 2.2. Decoding We use a tree-based decoder, inspired by dynamic programming. It searches for an approximation of 158 Proceedings of the Workshop on Statistical Machi"
W06-3124,N04-1021,0,0.0445594,"Missing"
W06-3124,2004.iwslt-evaluation.13,0,0.0239717,"Missing"
W06-3124,P03-1021,0,0.0398057,"Missing"
W06-3124,N06-1002,1,0.775668,"a number of theoretical problems, such as the ad hoc estimation of phrasal probability, the failure to model the partition probability, and the tenuous connection between the phrases and the underlying word-based alignment model. In string-based SMT systems, these problems are outweighed by the key role played by phrases in capturing “local” order. In the absence of good global ordering models, this has led to an 159 inexorable push towards longer and longer phrases, resulting in serious practical problems of scale, without, in the end, obviating the need for a real global ordering story. In [13] we discuss these issues in greater detail and also present our approach to this problem. Briefly, we take as our basic unit the Minimal Translation Unit (MTU) which we define as a set of source and target word pairs such that there are no word alignment links between distinct MTUs, and no smaller MTUs can be extracted without violating the previous constraint. In other words, these are the minimal non-compositional phrases. We then build models based on n-grams of MTUs in source string, target string and source dependency tree order. These bilingual n-gram models in combination with our globa"
W06-3124,J93-2003,0,\N,Missing
W07-0701,P05-1033,0,0.774793,"approaches with greater powers of generalization. There are multiple facets to this issue, including handling of unknown words, new senses of known words etc. In this work, we will focus on the issue of reordering, i.e. can we learn how to transform the sentence structure of one language into the sentence structure of another, in a way that is not tied to a specific domain or sub-domains, or indeed, sequences of individual words. An early attempt at greater generality in a purely phrasal setting was the alignment template approach (Och & Ney 2004); newer approaches include formally syntactic (Chiang 2005), and linguistically syntactic approaches (Quirk et al. 2005), (Huang et al. 2006). In the next section, we examine these representative approaches to the reordering problem. Abstract Today's statistical machine translation systems generalize poorly to new domains. Even small shifts can cause precipitous drops in translation quality. Phrasal systems rely heavily, for both reordering and contextual translation, on long phrases that simply fail to match outof-domain text. Hierarchical systems attempt to generalize these phrases but their learned rules are subject to severe constraints. Syntactic"
W07-0701,N04-1035,0,0.130994,"Missing"
W07-0701,N04-1014,0,0.0243753,"approach (Och & Ney, 2004) uses word classes rather than lexical items to model phrase translation. Yet this approach loses the advantage of context-sensitive lexical selection: the word translation model depends only on the word classes to subcategorize for translations, which leads to less accurate lexical choice in practice (Zens & Ney, 2004). 2.2 2.3 Constituency tree transduction An alternate approach is to use linguistic information from a parser. Transduction rules between Spanish strings and English trees can be learned from a word-aligned parallel corpus with parse trees on one side (Graehl & Knight, 2004). Such rules can be used to translate from Spanish to English by searching for the best English language tree for a given Spanish language string (Marcu et al., 2006). Alternately English trees produced by a parser can be transduced to Hierarchical translation Hierarchical systems (Chiang, 2005) induce a context-free grammar with one non-terminal 2 rules provide an elegant and powerful model of reordering, they come with a potential cost in context-sensitive translation. NP ADJP NP DT RB JJ NN DT JJ NN a very old book the old man un libro más antiguo 2.4 We previously described (Quirk et al, 2"
W07-0701,N03-1017,0,0.0170775,"Missing"
W07-0701,W06-3114,0,0.0766972,"hoice, each rule must match all of the children at any given tree node. On the other hand, treelets are allowed to match more loosely. The translations of the unmatched children (un and muy in this case) are placed by exploring all possible orderings and scoring them with both order model and language model. Although this effectively decouples lexical selection from ordering, it comes at a huge cost in search space and translation quality may suffer due to search error. However, as mentioned in Section 1, this approach is able to generalize better to out-ofdomain data than phrasal approaches. Koehn and Monz (2006) also include a human evaluation, in which this system ranked noticeably higher than one might have predicted from its BLEU score. 3 As compared to the treelet approach described in Section 2.4, the generalization capability is somewhat reduced. In the treelet system all reorderings are exhaustively evaluated, but the size of the search space necessitates tight pruning, leading to significant search error. By contrast, in the order template approach we consider only reorderings that are captured in some order template. The drastic reduction in search space leads to an overall improvement, not"
W07-0701,J90-2002,0,0.660971,"Missing"
W07-0701,N04-1033,0,0.0142457,"cause the paucity of the grammar's single nonterminal causes this rule to incorrectly imply that the translation of very be placed before hombre. Looking at this as a sparse data issue we might suspect that generalization could solve the problem. The alignment template approach (Och & Ney, 2004) uses word classes rather than lexical items to model phrase translation. Yet this approach loses the advantage of context-sensitive lexical selection: the word translation model depends only on the word classes to subcategorize for translations, which leads to less accurate lexical choice in practice (Zens & Ney, 2004). 2.2 2.3 Constituency tree transduction An alternate approach is to use linguistic information from a parser. Transduction rules between Spanish strings and English trees can be learned from a word-aligned parallel corpus with parse trees on one side (Graehl & Knight, 2004). Such rules can be used to translate from Spanish to English by searching for the best English language tree for a given Spanish language string (Marcu et al., 2006). Alternately English trees produced by a parser can be transduced to Hierarchical translation Hierarchical systems (Chiang, 2005) induce a context-free gramma"
W07-0701,2006.amta-papers.8,0,\N,Missing
W07-0701,W06-3124,1,\N,Missing
W07-0701,P02-1040,0,\N,Missing
W07-0701,J04-4002,0,\N,Missing
W07-0701,W06-1606,0,\N,Missing
W07-0701,P05-1034,1,\N,Missing
W07-0701,J03-1002,0,\N,Missing
W07-0701,J08-3004,0,\N,Missing
W07-0701,P03-1021,0,\N,Missing
W07-0715,W06-3123,0,0.225738,"airs are extracted from word-aligned bilingual sentence pairs, and their translation probabilities estimated heuristically from surface statistics of the extracted phrase pairs. We will refer to this approach as “the standard model”. There have been several attempts to estimate phrase translation probabilities directly, using generative models trained iteratively on a parallel corpus using the Expectation Maximization (EM) algorithm. The first of these models, that of Marcu and Wong (2002), was found by Koehn, et al. (2003), to produce translations not quite as good as their method. Recently, Birch et al. (2006) tried the Marcu and Wong model constrained by a word alignment and also found that Koehn, et al.’s model worked better, with the advantage of the standard model increasing as more features were added to the overall translation model. DeNero et al. (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. DeNero et al. (2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM al"
W07-0715,J93-2003,0,0.0128557,"Missing"
W07-0715,P03-1012,0,0.0266405,"s not assume a segmentation of the training data into non-overlapping phrase pairs. We refer to our model as “iteratively-trained” rather than “generative” because we have not proved any of the mathematical properties usually associated with generative models; e.g., that the training procedure maximizes the likelihood of the training data. We will motivate the model, however, with a generative story as to how phrase alignments are produced, given a pair of source and target sentences. Our model extends to phrase alignment the concept of a sentence pair generating a word alignment developed by Cherry and Lin (2003). Our model is defined in terms of two stochastic processes, selection and alignment, as follows: 1. For each word-aligned sentence pair, we identify all the possible phrase pair instances according to the criteria used by Koehn et al. 2. Each source phrase instance that is included in any of the possible phrase pair instances independently selects one of the target phrase instances that it forms a possible phrase pair instance with. We have seen how to derive phrase translation probabilities from the selection probabilities, but where do the latter come from? We answer this question by adding"
W07-0715,W06-3105,0,0.16634,"ral attempts to estimate phrase translation probabilities directly, using generative models trained iteratively on a parallel corpus using the Expectation Maximization (EM) algorithm. The first of these models, that of Marcu and Wong (2002), was found by Koehn, et al. (2003), to produce translations not quite as good as their method. Recently, Birch et al. (2006) tried the Marcu and Wong model constrained by a word alignment and also found that Koehn, et al.’s model worked better, with the advantage of the standard model increasing as more features were added to the overall translation model. DeNero et al. (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. DeNero et al. (2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM algorithm to maximize the probability of the training data without really improving the quality of the model. We propose an iteratively-trained phrase translation model that does not require different segmentations to compete against one another, and we sho"
W07-0715,N03-1017,0,0.0586483,"alignments as described by Koehn, et al. (2003). We propose a new iteratively-trained phrase translation model that produces translations of quality equal to or better than those produced by Koehn, et al.’s model. Moreover, with the new model, translation quality degrades much more slowly as pruning is tightend to reduce translation time. 1 Introduction Estimates of conditional phrase translation probabilities provide a major source of translation knowledge in phrase-based statistical machine translation (SMT) systems. The most widely used method for estimating these probabilities is that of Koehn, et al. (2003), in which phrase pairs are extracted from word-aligned bilingual sentence pairs, and their translation probabilities estimated heuristically from surface statistics of the extracted phrase pairs. We will refer to this approach as “the standard model”. There have been several attempts to estimate phrase translation probabilities directly, using generative models trained iteratively on a parallel corpus using the Expectation Maximization (EM) algorithm. The first of these models, that of Marcu and Wong (2002), was found by Koehn, et al. (2003), to produce translations not quite as good as their"
W07-0715,N06-1014,0,0.0354758,"is training procedure as iteratively trying to find a set of phrase translation probabilities that satisfies all the constraints of the model, although we have not proved that this training procedure always converges. We also have not proved that the procedure maximizes the likelihood of anything, although we find empirically that each iteration decreases the conditional entropy of the phrase translation model. In any case, the training procedure seems to work well in practice. It is also very similar to the joint training procedure for HMM wordalignment models in both directions described by Liang et al. (2006), which was the original inspiration for our training procedure. 4 Experimental Set-Up and Data We evaluated our phrase translation model compared to the standard model of Koehn et al. in the context of a fairly typical end-to-end phrase-based SMT system. The overall translation model score consists of a weighted sum of the following eight aggregated feature values for each translation hypothesis: • the sum of the log probabilities of each source phrase in the hypothesis given the corresponding target phrase, computed either by our model or the standard model, • the sum of the log probabilitie"
W07-0715,W02-1018,0,0.0977314,"SMT) systems. The most widely used method for estimating these probabilities is that of Koehn, et al. (2003), in which phrase pairs are extracted from word-aligned bilingual sentence pairs, and their translation probabilities estimated heuristically from surface statistics of the extracted phrase pairs. We will refer to this approach as “the standard model”. There have been several attempts to estimate phrase translation probabilities directly, using generative models trained iteratively on a parallel corpus using the Expectation Maximization (EM) algorithm. The first of these models, that of Marcu and Wong (2002), was found by Koehn, et al. (2003), to produce translations not quite as good as their method. Recently, Birch et al. (2006) tried the Marcu and Wong model constrained by a word alignment and also found that Koehn, et al.’s model worked better, with the advantage of the standard model increasing as more features were added to the overall translation model. DeNero et al. (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. DeNero et al. (2006) att"
W07-0715,W03-0301,0,0.016243,"rce/target phrase pairs composing the hypothesis, (Brown et al., 1993). The feature weights for the overall translation models were trained using Och’s (2003) minimum-error-rate training procedure. The weights were optimized separately for our model and for the standard phrase translation model. Our decoder is a reimplementation in Perl of the algorithm used by the Pharaoh decoder as described by Koehn (2003).2 The data we used comes from an English-French bilingual corpus of Canadian Hansards parliamentary proceedings supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). Automatic sentence alignment of this data was provided by Ulrich Germann. We used 500,000 sentences pairs from this corpus for training both the phrase translation models and IBM Model 1 lexical scores. These 500,000 sentence pairs were word-aligned using a state-ofthe-art word-alignment method (Moore et al., 2006). A separate set of 500 sentence pairs was used to train the translation model weights, and two additional held-out sets of 2000 sentence pairs each were used as test data. The two phrase translation models were trained using the same set of possible phrase pairs extracted from the"
W07-0715,P06-1065,1,0.830362,"ion in Perl of the algorithm used by the Pharaoh decoder as described by Koehn (2003).2 The data we used comes from an English-French bilingual corpus of Canadian Hansards parliamentary proceedings supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). Automatic sentence alignment of this data was provided by Ulrich Germann. We used 500,000 sentences pairs from this corpus for training both the phrase translation models and IBM Model 1 lexical scores. These 500,000 sentence pairs were word-aligned using a state-ofthe-art word-alignment method (Moore et al., 2006). A separate set of 500 sentence pairs was used to train the translation model weights, and two additional held-out sets of 2000 sentence pairs each were used as test data. The two phrase translation models were trained using the same set of possible phrase pairs extracted from the word-aligned 500,000 sentence pair corpus, finding all possible phrase pairs permitted by the criteria followed by Koehn et al., up to a phrase length of seven words. This produced approximately 69 million distinct phrase pair types. No pruning of the set of possible phrase pairs was done during or before training t"
W07-0715,W99-0604,0,0.0675212,"0 x0 C(x , y) p(x|y) = P This method is used to estimate the conditional probabilities of both target phrases give source phrases and source phrases given target phrases. In contrast to the standard model, DeNero, et al. (2006) estimate phrase translation probabilities according to the following generative model: 1. Begin with a source sentence a. 2. Stochastically segment a into some number of phrases. 3. For each selected phrase in a, stochastically choose a phrase position in the target sentence b that is being generated. 1 This method of phrase pair extraction was originally described by Och et al. (1999). 113 4. For each selected phrase in a and the corresponding phrase position in b, stochastically choose a target phrase. 5. Read off the target sentence b from the sequence of target phrases. DeNero et al.’s analysis of why their model performs relatively poorly hinges on the fact that the segmentation probabilities used in step 2 are, in fact, not trained, but simply assumed to be uniform. Given complete freedom to select whatever segmentation maximizes the likelihood of any given sentence pair, EM tends to favor segmentations that yield source phrases with as few occurrences as possible, si"
W07-0715,P03-1021,0,0.100988,"Missing"
W07-0715,P02-1040,0,0.0741709,"he standard model measured 4.30 bits per phrase. DeNero et al. obtained corresponding measurements of 1.55 bits per phrase and 3.76 bits per phrase, for their model and the standard model, using a different data set and a slightly different estimation method. 6 Translation Experiments We wanted to look at the trade-off between decoding time and translation quality for our new phrase translation model compared to the standard model. Since this trade-off is also affected by the settings of various pruning parameters, we compared decoding time and translation quality, as measured by B LEU score (Papineni et al, 2002), for the two models on our first test set over a broad range of settings for the decoder pruning parameters. The Pharaoh decoding algorithm, has five pruning parameters that affect decoding time: • Distortion limit • Translation table limit The distortion limit is the maximum distance allowed between two source phrases that produce adjacent target phrases in the decoder output. The distortion limit can be viewed as a model parameter, as well as a pruning paramter, because setting it to an optimum value usually improves translation quality over leaving it unrestricted. We carried out experimen"
W07-0715,2006.amta-papers.2,0,\N,Missing
W11-1825,J93-1003,0,0.0914792,"alues were rounded to the closest integer. We found, however, that adding these features did not improve results. 2.2.2 Feature combination and reduction We experimented with feature reduction and feature combination within the set of features described here. For feature reduction we tried a number of simple approaches that typically work well in text classification. The latter is similar to the task at hand, in that there is a very large but sparse feature set. We tried two feature reduction methods: a simple count cutoff, and selection of the top n features in terms of log likelihood ratio (Dunning, 1993) with the target values. For a count cutoff, we used cutoffs from 3 to 10, but we failed to observe any consistent gains. Only low cutoffs (3 and occasionally 5) would ever produce any small improvements on the development set. Using 159 log likelihood ratio (as determined on the training set), we reduced the total number of features to between 10,000 and 75,000. None of these experiments improved results, however. One potential reason for this negative result may be that there were a lot of features in our set that capture the same phenomenon in different ways, i.e. which correlate highly. By"
W11-1825,de-marneffe-etal-2006-generating,0,0.0209894,"Missing"
W11-1825,P08-2026,0,0.0458061,"ndicate edges with posterior > 0.95; edges with posterior < 0.05 were omitted. Most of the ambiguity is in the attachment of “elicited”. words in the sentence. Since proteins may consist of multiple words, for paths we picked a single representative word for each protein to act as its starting point and ending point. Generally this was the token inside the protein that is closest to the root of the dependency parse. In the case of ties, we picked the rightmost such node. 2.1.1 McClosky-Charniak-Stanford parses The organizers provide parses from a version of the McClosky-Charniak parser, MCCC (McClosky and Charniak, 2008), which is a two-stage parser/reranker trained on the GENIA corpus. In addition, we used an improved set of parsing models that leverage unsupervised data, MCCC-I (McClosky, 2010). In both cases, the Stanford Parser was used to convert constituency trees in the Penn Treebank format into labeled dependency parses: we used the collapsed dependency format. 2.1.2 Dependency posteriors Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al. 2008). McClosky-Charniak parses in two passes: the first pass"
W11-1825,N10-1004,0,0.011112,"ltiple words, for paths we picked a single representative word for each protein to act as its starting point and ending point. Generally this was the token inside the protein that is closest to the root of the dependency parse. In the case of ties, we picked the rightmost such node. 2.1.1 McClosky-Charniak-Stanford parses The organizers provide parses from a version of the McClosky-Charniak parser, MCCC (McClosky and Charniak, 2008), which is a two-stage parser/reranker trained on the GENIA corpus. In addition, we used an improved set of parsing models that leverage unsupervised data, MCCC-I (McClosky, 2010). In both cases, the Stanford Parser was used to convert constituency trees in the Penn Treebank format into labeled dependency parses: we used the collapsed dependency format. 2.1.2 Dependency posteriors Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al. 2008). McClosky-Charniak parses in two passes: the first pass is a generative model that produces a set of n-best candidates, and the 156 second pass is a discriminative reranker that uses a rich set of features including non-local informat"
W11-1825,P08-1023,0,0.00685334,"ion of the McClosky-Charniak parser, MCCC (McClosky and Charniak, 2008), which is a two-stage parser/reranker trained on the GENIA corpus. In addition, we used an improved set of parsing models that leverage unsupervised data, MCCC-I (McClosky, 2010). In both cases, the Stanford Parser was used to convert constituency trees in the Penn Treebank format into labeled dependency parses: we used the collapsed dependency format. 2.1.2 Dependency posteriors Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al. 2008). McClosky-Charniak parses in two passes: the first pass is a generative model that produces a set of n-best candidates, and the 156 second pass is a discriminative reranker that uses a rich set of features including non-local information. We renormalized the outputs from this log-linear discriminative model to get a posterior distribution over the 50-best parses. This set of parses preserved some of the syntactic ambiguity present in the sentence. The Stanford parser deterministically converts phrase-structure trees into labeled dependency graphs (de Marneffe et al., 2006). We converted each"
W11-1825,N10-1123,1,0.766932,"ticipating systems, so it would be interesting to consider whether there are some annotations in the development set that cannot be predicted by any of the participating systems1. If this is the case, then those triggers and edges would present an interesting topic for discussion. This might result either in a modification of the annotation protocols, or an opportunity for all systems to learn more. After a certain amount of feature engineering, we found it difficult to achieve further improvements in F1. Perhaps we need a significant shift in architecture, such as a shift to joint inference (Poon and Vanderwende, 2010). Our system may be limited by the pipeline architecture. 1 Our system output for the 2011development set can be downloaded from http://research.microsoft.com/bionlp/ 162 MWEs (multi-word entities) are a challenge. Better multi-word triggers accuracy may improve system performance. Multi-word proteins often led to incorrect part-of-speech tags and parse trees. Cursory inspection of the Epigenetics task shows that some domain-specific knowledge would have been beneficial. Our system had significant difficulties with the rare inverse event types, e.g. “demethylation” (e.g., there are 319 example"
W11-1825,J08-1002,0,\N,Missing
W11-1825,D08-1022,0,\N,Missing
W11-1825,W09-1402,0,\N,Missing
W11-1825,D09-1098,0,\N,Missing
W11-3507,W02-1032,1,0.724029,"ntegrating the word n-gram model. Incorporating this model directly into search requires us to score partial candidates, a somewhat complicated process since the candidates may only cover prefixes of words. Therefore, we rerank the top outputs from the system including all other features to integrate the word n-gram 4.2 Evaluation Metric We measured our results using character error rate (CER), which is based on the longest common subsequence match in characters between the reference and the best system output. This is a standard metric used in evaluating IME systems (e.g., Mori et al., 1998; Gao et al., 2002a,b). Let NREF be the number of characters in a reference sentence, NSYS be the character length of a system output, and NLCS be the length of the longest common subsequence between them. Then the character-level recall is defined as 3 In practice, we recombine more aggressively following the ideas in Li and Khudanpur (2008). 4 45 http://www.statmt.org/wmt09/ Exp ID E1 E2 E3 E4 E5 E6 E7 Models Channel Model Only (C.M) C.M + 4-gram char LM (4-CLM) C.M + 4-CLM + 3-gram word LM (3-WLM) C.M + 6-gram char LM (6-CLM) C.M + 6-CLM + 3-WLM C.M + 10-gram char LM (10-CLM) C.M + 10-CLM + 3-WLM b=3 0.4974"
W11-3507,W08-0402,0,0.0121972,".2 Evaluation Metric We measured our results using character error rate (CER), which is based on the longest common subsequence match in characters between the reference and the best system output. This is a standard metric used in evaluating IME systems (e.g., Mori et al., 1998; Gao et al., 2002a,b). Let NREF be the number of characters in a reference sentence, NSYS be the character length of a system output, and NLCS be the length of the longest common subsequence between them. Then the character-level recall is defined as 3 In practice, we recombine more aggressively following the ideas in Li and Khudanpur (2008). 4 45 http://www.statmt.org/wmt09/ Exp ID E1 E2 E3 E4 E5 E6 E7 Models Channel Model Only (C.M) C.M + 4-gram char LM (4-CLM) C.M + 4-CLM + 3-gram word LM (3-WLM) C.M + 6-gram char LM (6-CLM) C.M + 6-CLM + 3-WLM C.M + 10-gram char LM (10-CLM) C.M + 10-CLM + 3-WLM b=3 0.4974 0.4643 0.2499 0.3744 0.2389 0.2735 0.2183 WMT2009 b=10 b=30 0.4974 0.4974 0.4643 0.4643 0.2494 0.2494 0.3751 0.3751 0.2384 0.2384 0.2751 0.2751 0.2173 0.2173 b=3 0.4907 0.4052 0.2743 0.3322 0.2410 0.2448 0.2175 MTLog b=10 0.4907 0.4052 0.2740 0.3332 0.2418 0.2454 0.2169 b=30 0.4907 0.4052 0.2740 0.3332 0.2418 0.2456 0.2169 T"
W12-3125,N12-1047,1,0.88858,"Missing"
W12-3125,D08-1024,0,0.0773411,"Missing"
W12-3125,C10-2033,0,0.317379,"lin Cherry National Research Council colin.cherry@nrc-cnrc.gc.ca Robert C. Moore Google bobmoore@google.com Abstract decoding can be constrained by distortion limits or by mimicking the restrictions of inversion transduction grammars (Wu, 1997; Zens et al., 2004). The addition of a deterministic permutation parser can provide valuable hierarchical information to a phrase-based statistical machine translation (PBSMT) system. Permutation parsers have been used to implement hierarchical re-ordering models (Galley and Manning, 2008) and to enforce inversion transduction grammar (ITG) constraints (Feng et al., 2010). We present a number of theoretical results regarding the use of permutation parsers in PBSMT. In particular, we show that an existing ITG constraint (Zens et al., 2004) does not prevent all non-ITG permutations, and we demonstrate that the hierarchical reordering model can produce analyses during decoding that are inconsistent with analyses made during training. Experimentally, we verify the utility of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output. 1 Chris Quirk Microsoft Resear"
W12-3125,D08-1089,0,0.866851,"of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output. 1 Chris Quirk Microsoft Research chrisq@microsoft.com Recently, we have begun to see deterministic permutation parsers incorporated into phrase-based decoders. These efficient parsers analyze the sequence of phrases used to produce the target, and assemble them into a hierarchical translation history that can be used to inform re-ordering decisions. Thus far, they have been used to enable a hierarchical re-ordering model, or HRM (Galley and Manning, 2008), as well as an ITG constraint (Feng et al., 2010). We discuss each of these techniques in turn, and then explore the implications of ITG violations on hierarchical re-ordering. Introduction Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (Galley and Manning, 2010). However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars. Therefore, re-ordering must be modeled and constrained explicitly. M"
W12-3125,N10-1140,0,0.04572,"to produce the target, and assemble them into a hierarchical translation history that can be used to inform re-ordering decisions. Thus far, they have been used to enable a hierarchical re-ordering model, or HRM (Galley and Manning, 2008), as well as an ITG constraint (Feng et al., 2010). We discuss each of these techniques in turn, and then explore the implications of ITG violations on hierarchical re-ordering. Introduction Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (Galley and Manning, 2010). However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars. Therefore, re-ordering must be modeled and constrained explicitly. Movement can be modeled with a distortion penalty or lexicalized re-ordering probabilities (Koehn et al., 2003; Koehn et al., 2007), while We present one experimental and four theoretical contributions. Examining the HRM alone, we present an improved algorithm for extracting HRM statistics, reducing the complexity of Galley and Manning’s solution from O(n4 ) to O(n2 ). Examining ITG c"
W12-3125,J99-4005,0,0.485608,"Missing"
W12-3125,N03-1017,0,0.0695883,"turn, and then explore the implications of ITG violations on hierarchical re-ordering. Introduction Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (Galley and Manning, 2010). However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars. Therefore, re-ordering must be modeled and constrained explicitly. Movement can be modeled with a distortion penalty or lexicalized re-ordering probabilities (Koehn et al., 2003; Koehn et al., 2007), while We present one experimental and four theoretical contributions. Examining the HRM alone, we present an improved algorithm for extracting HRM statistics, reducing the complexity of Galley and Manning’s solution from O(n4 ) to O(n2 ). Examining ITG constraints alone, we demonstrate that the three-stack constraint of Feng et al. can be reduced to one augmented stack, and we show that another phrase-based ITG constraint (Zens et al., 2004) actually allows some ITG violations to pass. Finally, we show that in the presence of ITG violations, the original HRM can fail to"
W12-3125,P07-2045,0,0.0669013,"re the implications of ITG violations on hierarchical re-ordering. Introduction Despite the emergence of a number of syntax-based techniques, phrase-based statistical machine translation remains a competitive and very efficient translation paradigm (Galley and Manning, 2010). However, it lacks the syntactically-informed movement models and constraints that are provided implicitly by working with synchronous grammars. Therefore, re-ordering must be modeled and constrained explicitly. Movement can be modeled with a distortion penalty or lexicalized re-ordering probabilities (Koehn et al., 2003; Koehn et al., 2007), while We present one experimental and four theoretical contributions. Examining the HRM alone, we present an improved algorithm for extracting HRM statistics, reducing the complexity of Galley and Manning’s solution from O(n4 ) to O(n2 ). Examining ITG constraints alone, we demonstrate that the three-stack constraint of Feng et al. can be reduced to one augmented stack, and we show that another phrase-based ITG constraint (Zens et al., 2004) actually allows some ITG violations to pass. Finally, we show that in the presence of ITG violations, the original HRM can fail to produce orientations"
W12-3125,2007.mtsummit-papers.43,1,0.945694,"Missing"
W12-3125,2009.mtsummit-papers.10,0,0.327246,"Missing"
W12-3125,J04-4002,0,0.132136,"training data. Galley and Manning (2008) propose an algorithm that begins by run1 This would require a second, right-to-left decoding pass. Galley and Manning (2008) present an under-specified approximation that is consistent with what we present here. 2 202 2 Prev Cov / Approx Top 4 5 6 7 Figure 2: Illustration of the coverage-vector stack approximation, as applied to right-to-left HRM orientation. èS Source Op S S R S S R R S R çM Phrase èM çS Target Figure 3: Relevant corners in HRM extraction. → indicates left-to-right orientation, and ← right-to-left. ning standard phrase extraction (Och and Ney, 2004) without a phrase-length limit, noting the corners of each phrase found. Next, the left-to-right and rightto-left orientation for each phrase of interest (those within the phrase-length limit) can be determined by checking to see if any corners noted in the previous step are adjacent, as shown in Figure 3. 2.1 Efficient Extraction of HRM statistics The time complexity of phrase extraction is bounded by the number of phrases to be extracted, which is determined by the sparsity of the input word alignment. Without a limit on phrase length, a sentence pair with n words in each language can have a"
W12-3125,P02-1040,0,0.0838935,"Missing"
W12-3125,N04-4026,0,0.4744,"ation, including an approximate HRM that requires no permutation parser, and compare them experimentally. The variants perform similarly to the original in terms of BLEU score, but differently in terms of how they permute the source sentence. 200 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 200–209, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 Hierarchical Re-ordering Hierarchical re-ordering models (HRMs) for phrasebased SMT are an extension of lexicalized reordering models (LRMs), so we begin by briefly reviewing the LRM (Tillmann, 2004; Koehn et al., 2007). The goal of an LRM is to characterize how a phrase-pair tends to be placed with respect to the block that immediately precedes it. Both the LRM and the HRM track orientations traveling through the target from left-to-right as well as right-to-left. For the sake of brevity and clarity, we discuss only the left-to-right direction except when stated otherwise. Re-ordering is typically categorized into three orientations, which are determined by examining two sequential blocks [si−1 , ti−1 ] and [si , ti ]: • Monotone Adjacent (M): ti−1 = si • Swap Adjacent (S): ti = si−1 •"
W12-3125,J97-3002,0,0.703132,"Missing"
W12-3125,P03-1019,0,0.285671,"Missing"
W12-3125,C04-1030,0,0.649146,"y mimicking the restrictions of inversion transduction grammars (Wu, 1997; Zens et al., 2004). The addition of a deterministic permutation parser can provide valuable hierarchical information to a phrase-based statistical machine translation (PBSMT) system. Permutation parsers have been used to implement hierarchical re-ordering models (Galley and Manning, 2008) and to enforce inversion transduction grammar (ITG) constraints (Feng et al., 2010). We present a number of theoretical results regarding the use of permutation parsers in PBSMT. In particular, we show that an existing ITG constraint (Zens et al., 2004) does not prevent all non-ITG permutations, and we demonstrate that the hierarchical reordering model can produce analyses during decoding that are inconsistent with analyses made during training. Experimentally, we verify the utility of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output. 1 Chris Quirk Microsoft Research chrisq@microsoft.com Recently, we have begun to see deterministic permutation parsers incorporated into phrase-based decoders. These efficient parsers analyze the sequ"
W12-3125,W07-0404,0,0.154947,"Missing"
W12-3125,N06-1033,0,0.0624761,"Missing"
W12-3125,D11-1003,0,\N,Missing
W12-3158,P08-1024,0,0.0424276,"Missing"
W12-3158,2009.iwslt-papers.2,0,0.0484274,"Missing"
W12-3158,D10-1053,0,0.0325832,"Missing"
W12-3158,W06-3105,0,0.14959,"Missing"
W12-3158,W06-1607,0,0.0608308,"Missing"
W12-3158,P12-1031,0,0.0591497,"Missing"
W12-3158,D07-1103,0,0.0478539,"Missing"
W12-3158,P06-1096,0,0.122379,"Missing"
W12-3158,W10-2915,0,0.025654,"Missing"
W12-3158,P03-1021,0,0.198453,"Missing"
W12-3158,P03-1041,0,0.0854189,"Missing"
W12-3158,P10-1049,1,0.843105,"Missing"
W12-3158,P07-2045,0,\N,Missing
W12-3158,N03-1017,0,\N,Missing
W12-3158,2008.iwslt-evaluation.10,0,\N,Missing
W13-2809,D07-1090,0,0.076784,"Missing"
W13-2809,W10-1703,0,0.0230751,", the baseline systems were trained on over 22M sentence pairs; for German  English, the baseline systems were trained on over 36M sentence pairs.4 We then created five samples of the baseline data for each language pair, consisting of 100K, 500K, 1M, 2M, and 5M sentence pairs (the same samples were used for both EX and XE for the respective pairs). We then trained both treelet and phrasal systems in both directions (EX and XE) over each sample of data. Language models were trained on all systems over the target-side data. For dev data, we used development data from the 2010 WMT competition (Callison-Burch et al., 2010), and we used MERT (Och, 2003) to tune each system. We tested each system against three different test sets: two were from the WMT competitions of 2009 and 2010, and the other was one locally constructed from 5000 sentences of content translated by users of our production service (http://bing.com/translator), which we subsequently had manually translated into the target languages. The former two test sets are somewhat news focused; the latter is a random sample of miscellaneous translations, and is more generally focused. The results of the experiments are shown in Tables 2 and 3, with the rel"
W13-2809,W12-3156,0,0.0266089,"t are systematically mis-parsed, as well as cases where the parser specification is not ideal for the translation task. Recent work in this area has show substantial improvements (Zhang et al., 2011). 4 4.1 Evaluation Fact or Fiction: BLEU is Biased Against Rule-Based or Linguistically-Informed Systems? It has generally been accepted as common wisdom that BLEU favors statistical MT systems and disfavors those that are linguistically informed or rule-based. Surprisingly, the literature on the topic is rather sparse, with some notable exceptions (Riezler and Maxwell, 2005; Farr´us et al., 2012; Carpuat and Simard, 2012). We too have made this assumption, and had a few years ago coined the term treelet penalty to indicate the degree by 3 These results were not published, but were provided to the authors in a personal conversation with Xiaodong. In a related paper (He et al., 2008), He and colleagues showed significant improvements in BLEU on a system combination system, but no diffs in human eval. Upon analysis, the researchers were able to show that the biggest benefit to BLEU was in short content, but the same preference was not exhibited on the same content by the human evaluators. In other words, the impr"
W13-2809,N13-1140,0,0.0234476,"nd voy a hacer porque mis padres son mi vida Table 6: One English FB sentence with and without normalizations, translated to various languages vestment. Making this linguistic information be included softly as features is a powerful way of surfacing linguistic generalizations to the system while not forcing its hand. Some of the greatest successes in mixing linguistic and statistical methods have been in syntax. There is much ground to cover still. Morphology is integrated weakly into current SMT systems, mostly as broad features (Jeong et al., 2010) though sometimes with more sophistication (Chahuneau et al., 2013). Better integration of morphological features could have great effect, especially in agglutinative languages such as Finnish and Turkish. Deeper models of semantics present a rich challenge to the field. As we proceed into deeper models, picking the correct representation is a significant issue. Humans can generally agree on words, mostly on morphology, and somewhat on syntax. But semantics touches on issues of meaning representation: how should we best represent semantic information? Should we attempt to faithfully represent all the information in the source language, or gather only a simple"
W13-2809,2003.mtsummit-papers.9,0,0.0401492,".89 Diff (T-P) WMT 2009 0.32 0.44 0.47 0.73 0.31 0.39 WMT 2010 0.66 0.65 0.85 0.84 0.57 1.02 XE 100K 500K 1M 2M 5M 36M 27.42 30.98 32.30 33.40 34.86 37.31 15.91 18.25 19.16 19.95 21.14 22.72 16.37 19.16 20.40 21.48 22.55 24.97 26.75 29.80 31.26 32.25 33.91 36.08 15.83 18.11 19.06 19.65 20.67 21.99 16.28 19.09 20.18 21.06 22.13 23.85 0.67 1.18 1.04 1.15 0.95 1.23 0.08 0.14 0.10 0.30 0.47 0.73 0.09 0.07 0.22 0.42 0.42 1.12 Table 3: BLEU Score results for the German Treelet Penalty experiments 60 seek to achieve with our quality measures, and since BLEU is only weakly correlated with human eval (Coughlin, 2003), we ran human evals against both the English-Spanish and EnglishGerman output. Performing human evaluation gives us two additional perspectives on the data: (1) do humans perceive a qualitative difference between treelet and phrasal, as we see with BLEU, and (2), if the difference is perceptible, what is its magnitude relative to BLEU. If the magnitude of the difference is much larger than that of BLEU, and especially does not show convergence in the Spanish cases, then we still have a strong case for the Treelet Penalty. In fact, if human evaluators perceive a difference Spanish cases on the"
W13-2809,W12-3131,0,0.324961,"That is not surprising, given the results we observed in Section 4.2. What is interesting is to see how much stronger treelet Figure 11: Scatterplot showing Treelet vs Phrasal systems across different data sizes, plotting BLEU (Y) against Human Eval scores (X) 5 Clearly, the sample is very small, so the regression line should be taken with a grain of salt. We would need a lot more data to be able to draw any strong conclusions. 61 a list of common heuristics applied to data. Some of these are drawn from our own pre-processing, some are mentioned explicitly in other literature, in particular, (Denkowski et al., 2012). • Remove lines containing escape characters, invalid Unicode, and other non-linguistic noise. • Remove content that where the ratio of certain content passes some threshold, e.g., alphabetic/numeric ratio, script ratio (percentage of characters in wrong form passes some threshold, triggering removal). Figure 12: Treelet-Phrasal BLEU differences by bucket across language pair performs on short content than phrasal: treelet does the best on the shortest content, with quality dropping off anywhere between 10-30 words. One conclusion that can be drawn from these data is that treelet performs bes"
W13-2809,D11-1033,0,0.115891,"Missing"
W13-2809,W06-1608,1,0.851587,"Missing"
W13-2809,P13-1155,0,0.0169879,"ing rule-based approaches in a statistical framework can really give us the best of both worlds, giving us higher precision and higher recall. Finding an appropriate mix is difficult, though. As in the case of parsing, we can see how errors can substantially degrade translation quality, especially if we only consider the single best analysis. By making our analysis components as robust as possible, quantifying our degree of certainty with scoring mechanisms, and preserving ambiguity of the analysis, we can achieve a better return on in8 For a complete description of TextCorrector, please see (Hassan and Menezes, 2013). 64 Language Original English To Italian To German To Spanish Unrepaired i’l do cuz ma parnts r ma lyf i ’ l fare cuz ma parnts r ma lyf i ’ l tun Cuz Ma Parnts R Ma lyf traer hacer cuz ma parnts r ma lyf Repaired I’ll do because my parents are my life lo far perch i miei genitori sono la mia vita Ich werde tun, weil meine Eltern mein Leben sind voy a hacer porque mis padres son mi vida Table 6: One English FB sentence with and without normalizations, translated to various languages vestment. Making this linguistic information be included softly as features is a powerful way of surfacing ling"
W13-2809,D08-1011,0,0.0298675,"nguistically-Informed Systems? It has generally been accepted as common wisdom that BLEU favors statistical MT systems and disfavors those that are linguistically informed or rule-based. Surprisingly, the literature on the topic is rather sparse, with some notable exceptions (Riezler and Maxwell, 2005; Farr´us et al., 2012; Carpuat and Simard, 2012). We too have made this assumption, and had a few years ago coined the term treelet penalty to indicate the degree by 3 These results were not published, but were provided to the authors in a personal conversation with Xiaodong. In a related paper (He et al., 2008), He and colleagues showed significant improvements in BLEU on a system combination system, but no diffs in human eval. Upon analysis, the researchers were able to show that the biggest benefit to BLEU was in short content, but the same preference was not exhibited on the same content by the human evaluators. In other words, the improvements observed in the short content that BLEU favored had little impact on the overall impressions of the human evaluators. 57 distortion rates from English to or from German might favor a parser-based approach. We had four baseline systems that were built over"
W13-2809,2006.amta-panels.3,0,0.198317,"econd, we must rank according to both our linguistic intuitions and the patterns that emerge from data. We use a number of different systems based on the availability of linguistic resources. Socalled phrasal statistic machine translation systems, which model translations using no more than sequences of contiguous words, perform surprisingly well and require nothing but tokenization in both languages. In language pairs for which we have a source language parser, a parse of the input sentence is used to guide reordering and help select relevant non-contiguous units; this is the treelet system (Quirk and Menezes, 2006). Regardless of which system we use, however, target language models score the fluency of the output, and have a huge positive impact on translation quality. We are interested in means of incorporating linguistic intuition deeper into such a system. As in the case of the treelet system, this may define the broad structure of the system. However, there are also more accessible ways of influencing existing systems. For instance, linguists may author features that identify promising or problematic translations. We describe one such attempt in the following system. 3.1 Instead, we can soften this"
W13-2809,W05-0908,0,0.0314565,"s also allows translation to handle phenomena that are systematically mis-parsed, as well as cases where the parser specification is not ideal for the translation task. Recent work in this area has show substantial improvements (Zhang et al., 2011). 4 4.1 Evaluation Fact or Fiction: BLEU is Biased Against Rule-Based or Linguistically-Informed Systems? It has generally been accepted as common wisdom that BLEU favors statistical MT systems and disfavors those that are linguistically informed or rule-based. Surprisingly, the literature on the topic is rather sparse, with some notable exceptions (Riezler and Maxwell, 2005; Farr´us et al., 2012; Carpuat and Simard, 2012). We too have made this assumption, and had a few years ago coined the term treelet penalty to indicate the degree by 3 These results were not published, but were provided to the authors in a personal conversation with Xiaodong. In a related paper (He et al., 2008), He and colleagues showed significant improvements in BLEU on a system combination system, but no diffs in human eval. Upon analysis, the researchers were able to show that the biggest benefit to BLEU was in short content, but the same preference was not exhibited on the same content"
W13-2809,D11-1125,0,0.0205011,"ly promising, and prefer those mappings to others; see Figure 5 for an example. This begs the question: how much should we weight such influence? Our answer is a corpus driven one. Each of these linguistic preferences should be noted, and the weight of these preferences should be tuned with all others to optimize the goodness of translation. Already our statistical system has a number of signals that attempt to gauge translation quality: the translation models attempt to capture fidelity of translation; language models focus on fluency; etc. We use techniques such as MERT (Och, 2003) and PRO (Hopkins and May, 2011) to tune the relative weight of these signals. Why not tune indicators from linguists in the same manner? When our linguists mark a mapping as +Like or +DontLike, we track that throughout the search. Each final translation incorporates a count of Like mappings and a count of DontLike mappings, just as it accumulates a language model score, translation model scores, word penalties, and so on. These weights are tuned to optimize some approximate evaluation metric. In Figure 6, the weight of Like and DontLike is shown for a number of systems, demonstrating how optimization may be used to tune the"
W13-2809,2010.amta-papers.33,1,0.848517,"sono la mia vita Ich werde tun, weil meine Eltern mein Leben sind voy a hacer porque mis padres son mi vida Table 6: One English FB sentence with and without normalizations, translated to various languages vestment. Making this linguistic information be included softly as features is a powerful way of surfacing linguistic generalizations to the system while not forcing its hand. Some of the greatest successes in mixing linguistic and statistical methods have been in syntax. There is much ground to cover still. Morphology is integrated weakly into current SMT systems, mostly as broad features (Jeong et al., 2010) though sometimes with more sophistication (Chahuneau et al., 2013). Better integration of morphological features could have great effect, especially in agglutinative languages such as Finnish and Turkish. Deeper models of semantics present a rich challenge to the field. As we proceed into deeper models, picking the correct representation is a significant issue. Humans can generally agree on words, mostly on morphology, and somewhat on syntax. But semantics touches on issues of meaning representation: how should we best represent semantic information? Should we attempt to faithfully represent"
W13-2809,2012.amta-papers.18,0,0.0390677,"Missing"
W13-2809,1983.tc-1.13,0,0.754905,"er and availability of data: only now is this approach gaining some traction (Knight, 2013)1 At the time, however, this direction did not appear promising, and work turned toward rule-based approaches. Effective translation needs to handle a broad range of phenomena. Word substitution ciphers may address lexical selection, but there are many additional complexities: morphological normalization in the source language, morphological inflection in the target language, word order differences, and sentence structure differences, to name 1 For the original 1949 Translation memorandum by Weaver see (Weaver, 1955). 51 Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 51–66, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics source or the target sentences to make better decisions about reordering and lexical selection. Our machine translation group has been an active participant in many of these latest developments. The first MSR MT system used deep linguistic features, often with great positive effect. Inspired by the successes and failures of this system, we invested heavily in syntax-based SMT. However, our current statistical systems are sti"
W13-2809,W01-1406,0,0.0344333,"learned from parallel data. logical form was a graph rather than a tree – in “John ate and drank”, John is the DSUB (deep subject) of both eat and drink – which led to complications in transferring structure. Many such complications were often handled through rules; these rules grew more complex over time. Corpusbased approaches efficiently learned many noncompositional and domain specific issues. 2.1 Details of the LF-based system Training started with a parallel corpus. First, the source and target language sentences were parsed. Then the logical forms of the source and target were aligned (Menezes and Richardson, 2001). These aligned logical forms were partitioned into minimal non-compositional units, each consisting of some non-empty subset of the source and target language nodes and relations. Much like in example-based or phrasal systems, both minimal and composed versions of these units were then stored as possible translations. A schematic of the this data flow is presented in Figure 2. At runtime, an input sentence was first parsed into a logical form. Units whose source sides matched the logical form were gathered. A heuristic search found a set of fragments that: (a) covered every input node at leas"
W13-2809,P11-1084,0,0.014092,"slation derivation. In unpublished results, we found that this made a substantial improvement in translation quality; the effect was corroborated in other syntax directed translation systems (Mi et al., 2008). Alternatively, allowing a neighborhood of trees similar to some predicted tree can handle ambiguity even when the original parser does not maintain a forest. This also allows translation to handle phenomena that are systematically mis-parsed, as well as cases where the parser specification is not ideal for the translation task. Recent work in this area has show substantial improvements (Zhang et al., 2011). 4 4.1 Evaluation Fact or Fiction: BLEU is Biased Against Rule-Based or Linguistically-Informed Systems? It has generally been accepted as common wisdom that BLEU favors statistical MT systems and disfavors those that are linguistically informed or rule-based. Surprisingly, the literature on the topic is rather sparse, with some notable exceptions (Riezler and Maxwell, 2005; Farr´us et al., 2012; Carpuat and Simard, 2012). We too have made this assumption, and had a few years ago coined the term treelet penalty to indicate the degree by 3 These results were not published, but were provided to"
W13-2809,P08-1023,0,0.0229381,"ght branching tree is always proposed. This baseline is much worse than a simple phrasal system. The final four rows evaluate the impact of a parser trained on increasing amounts of sentences from the English Penn Treebank. Even with a tiny amount of training data, the system gets some benefit from syntactic information, and the returns appear to increase with more training data. parse as part of the translation derivation. In unpublished results, we found that this made a substantial improvement in translation quality; the effect was corroborated in other syntax directed translation systems (Mi et al., 2008). Alternatively, allowing a neighborhood of trees similar to some predicted tree can handle ambiguity even when the original parser does not maintain a forest. This also allows translation to handle phenomena that are systematically mis-parsed, as well as cases where the parser specification is not ideal for the translation task. Recent work in this area has show substantial improvements (Zhang et al., 2011). 4 4.1 Evaluation Fact or Fiction: BLEU is Biased Against Rule-Based or Linguistically-Informed Systems? It has generally been accepted as common wisdom that BLEU favors statistical MT sys"
W13-2809,N13-1090,0,0.0133329,"Missing"
W13-2809,P10-2041,1,0.909615,"Missing"
W13-2809,J04-4002,0,0.0277355,"substantial improvements over rule-based systems that dominated the market at the time. Much of these gains were due to domain- and context-sensitivity of the system. Consider the Spanish verb “activar”. A fair gloss into English is “activate”, but the most appropriate translation in context varies (“signal”, “flag”, etc.). The example-based approach was able to capture those contexts very effectively, leading to automatic domain customization given only translation memories. This was a huge improvement over rulebased systems of the time. During this same era, however, statistical approaches (Och and Ney, 2004) were showing great promise. Therefore, we ran a comparison between the LF-based system and a statistical system data driven but not statistical approach to parser development. 53 (a) Effecitve LF translation. Note how the LF system is able to translate “se lleveban a cabo” even though that particular surface form was not present in the training data. SRC : La tabla muestra adem´as d´onde se llevaban a cabo esas tareas en Windows NT versi´on 4.0. The table also shows where these tasks were performed in Windows NT version 4.0. LF : The table shows where, in addition, those tasks were conducted"
W13-2809,P03-1021,0,0.17267,"that look particularly promising, and prefer those mappings to others; see Figure 5 for an example. This begs the question: how much should we weight such influence? Our answer is a corpus driven one. Each of these linguistic preferences should be noted, and the weight of these preferences should be tuned with all others to optimize the goodness of translation. Already our statistical system has a number of signals that attempt to gauge translation quality: the translation models attempt to capture fidelity of translation; language models focus on fluency; etc. We use techniques such as MERT (Och, 2003) and PRO (Hopkins and May, 2011) to tune the relative weight of these signals. Why not tune indicators from linguists in the same manner? When our linguists mark a mapping as +Like or +DontLike, we track that throughout the search. Each final translation incorporates a count of Like mappings and a count of DontLike mappings, just as it accumulates a language model score, translation model scores, word penalties, and so on. These weights are tuned to optimize some approximate evaluation metric. In Figure 6, the weight of Like and DontLike is shown for a number of systems, demonstrating how opti"
W13-2809,P02-1040,0,0.0953442,"much larger than that of BLEU, and especially does not show convergence in the Spanish cases, then we still have a strong case for the Treelet Penalty. In fact, if human evaluators perceive a difference Spanish cases on the full data systems, the case where we show convergence, then the resulting differences could be described as the penalty value. Unfortunately, our human evaluation data on the Treelet Penalty effect was inconclusive. Our evaluations show a strong correlation between BLEU and human evaluation, something that is attested to in the literature (e.g., , the first paper on BLEU (Papineni et al., 2002), and a deeper exploration in (Coughlin, 2003)). However, the effect we were looking for – that is, a difference between human evaluations across decoders – was not evident. In fact, the human evaluations followed the differences we saw in BLEU between the two decoders very closely. Figure 11 shows data points for each data size for each decoder, plotting BLEU against human evaluation. When we fit a regression line against the data points for each decoder, we see complete overlap.5 In summary, we show a strong effect of treelet systems performing better than phrasal systems trained on the same"
W13-2809,2001.mtsummit-ebmt.4,0,\N,Missing
W13-2809,W09-0401,0,\N,Missing
W13-2809,ringger-etal-2004-using,0,\N,Missing
W15-3504,J93-2003,0,0.0364347,"Instead, we train a global model – similarly to Subotin (2011) or more recently Tamchyna et al. (2014). Features for our model are very different from previous work because they come from a deep representation and therefore should capture semantic relations between the languages, instead of surface or morpho-syntactic correspondences. 3 4 Graph-to-String Translation We develop models for semantic-graph-to-string translation. These models are essentially discriminative translation models, relying on a decomposition structure similar to both maximum entropy language models and IBM Models 1, 2 (Brown et al., 1993), and the HMM translation model (Vogel et al., 1996). In particular, we see translation as a process of selecting target words in order conditioned on source language representation as well as prior target words. Similar to the IBM Models, we see each target word as being generated based on source concepts, though in our case the concepts are semantic graph nodes rather than surface words. That is, we assume the existence of an alignment, though it aligns the target words to Semantic Representation Our representation of sentence semantics is based on Logical Form (Vanderwende, 2015). LFs are l"
W15-3504,D07-1007,0,0.019894,"such as (Galley et al., 2004) operate on trees and have also been applied to semantic parsing problems (Li et al., 2013), Jones et al. (2012) generalized these approaches by inducing synchronous hyperedge replacement grammars (HRG), which operate on graphs. In contrast to (Jones et al., 2012), our work does not have to deal with the complexities of HRG decoding, which runs in O(n3 ) (Jones et al., 2012), as our decoder is simply a phrase-based decoder. Discriminative models have been used in statistical MT many times. Global lexicon model (Mauser et al., 2009) and phrase-sense disambiguation (Carpuat and Wu, 2007) are perhaps the best known methods. Similarly to Carpuat and Wu (2007), we use the classifier to rescore phrasal translations, however we do not train a separate classifier for each source phrase. Instead, we train a global model – similarly to Subotin (2011) or more recently Tamchyna et al. (2014). Features for our model are very different from previous work because they come from a deep representation and therefore should capture semantic relations between the languages, instead of surface or morpho-syntactic correspondences. 3 4 Graph-to-String Translation We develop models for semantic-gr"
W15-3504,N04-1035,1,0.675715,"by the free variable X. The logical form can be converted using a sequence of rules to a representation which conforms to the AMR specification (Vanderwende et al., 2015). We do not use the full conversion pipeline in our work, so our semantic graphs are somewhere between the LF and AMR. Notably, we keep the bits which serve as important features for the discriminative modeling of translation. mary. (Jones et al., 2012) presents an MT approach that can exploit semantic graphs such as AMR, in a continuation of earlier work that abstracted translation away from strings (Yamada and Knight, 2001; Galley et al., 2004). While rule extraction algorithms such as (Galley et al., 2004) operate on trees and have also been applied to semantic parsing problems (Li et al., 2013), Jones et al. (2012) generalized these approaches by inducing synchronous hyperedge replacement grammars (HRG), which operate on graphs. In contrast to (Jones et al., 2012), our work does not have to deal with the complexities of HRG decoding, which runs in O(n3 ) (Jones et al., 2012), as our decoder is simply a phrase-based decoder. Discriminative models have been used in statistical MT many times. Global lexicon model (Mauser et al., 2009"
W15-3504,C12-1083,0,0.191367,"Missing"
W15-3504,N15-1114,0,0.0404792,". Finally text generation was applied. The level of analysis is somewhat arguable – sometimes it was purely syntactic, but in other cases it reached into the semantic domain. One of the earliest architectures was described in 1957 (Yngve, 1957). More contemporary examples of such systems include KANT (Nyberg and Mitamura, 1992), which used a very deep representation close to an interlingua, early versions of SysTran and Microsoft Translator, or more reˇ cently TectoMT (Popel and Zabokrtsk´ y, 2010) for English→Czech translation. AMR itself has recently been used for abstractive summarization (Liu et al., 2015). In this work, sentences in the document to be summarized are parsed to AMRs, then a decoding algorithm is run to produce a summary graph. The surface realization of this graph then constitutes the final sum30 Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation, pages 30–36, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics Figure 1: Logical Form (computed tree) for the sentence: I would like to give you a sandwich taken from the fridge. Additional linguistic information, such as verb subcategorization frames, definiteness, tense e"
W15-3504,D09-1022,0,0.024975,"alley et al., 2004). While rule extraction algorithms such as (Galley et al., 2004) operate on trees and have also been applied to semantic parsing problems (Li et al., 2013), Jones et al. (2012) generalized these approaches by inducing synchronous hyperedge replacement grammars (HRG), which operate on graphs. In contrast to (Jones et al., 2012), our work does not have to deal with the complexities of HRG decoding, which runs in O(n3 ) (Jones et al., 2012), as our decoder is simply a phrase-based decoder. Discriminative models have been used in statistical MT many times. Global lexicon model (Mauser et al., 2009) and phrase-sense disambiguation (Carpuat and Wu, 2007) are perhaps the best known methods. Similarly to Carpuat and Wu (2007), we use the classifier to rescore phrasal translations, however we do not train a separate classifier for each source phrase. Instead, we train a global model – similarly to Subotin (2011) or more recently Tamchyna et al. (2014). Features for our model are very different from previous work because they come from a deep representation and therefore should capture semantic relations between the languages, instead of surface or morpho-syntactic correspondences. 3 4 Graph-"
W15-3504,C92-3168,0,0.396558,"ge-specific phenomena, effectively bringing different languages closer together. A number of machine translation systems starting as early as the 1950s therefore used a form of transfer: the source sentences were parsed, and those parsed representations were translated into target representations. Finally text generation was applied. The level of analysis is somewhat arguable – sometimes it was purely syntactic, but in other cases it reached into the semantic domain. One of the earliest architectures was described in 1957 (Yngve, 1957). More contemporary examples of such systems include KANT (Nyberg and Mitamura, 1992), which used a very deep representation close to an interlingua, early versions of SysTran and Microsoft Translator, or more reˇ cently TectoMT (Popel and Zabokrtsk´ y, 2010) for English→Czech translation. AMR itself has recently been used for abstractive summarization (Liu et al., 2015). In this work, sentences in the document to be summarized are parsed to AMRs, then a decoding algorithm is run to produce a summary graph. The surface realization of this graph then constitutes the final sum30 Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation, pages 30–36, c B"
W15-3504,P00-1056,0,0.266378,"ribes the probability of the edge coming into the node ni , the token emission and finally the outgoing edge. We evaluate this probability for each node ni in the graph and re-align the token according to the random sample from this distribution. α and β are hyper-parameters specifying the concentration parameters of symmetric Dirichlet priors over the transition and emission distributions. Specifying values less than 1 for these hyper-parameters pushes the model toward sparse solutions. They are tuned by a grid search which evaluates model perplexity on a held-out set. Direct GIZA++. GIZA++ (Och and Ney, 2000) is a commonly used toolkit for word alignment which implements the IBM models. In this setting, we linearized the semantic graph nodes using a simple heuristic based on the surface word order and aligned them directly to the target-side sentences. We experimented with different symmetrizations and found that grow-diag-final-and gives the best results. Composed alignments. We divided the alignment problem into two stages: aligning semantic graph nodes to source-side words and aligning the source- and target-side words (i.e., standard MT word alignment). We then simply compose the two alignment"
W15-3504,P03-1021,0,0.0276172,"graph node (nai−1 ), from the parent node. (If there are multiple parents in the graph, we break ties in 33 three test sets provided for the Workshop on Statistical Machine Translation (Bojar et al., 2013) – WMT 2009, 2010 and 2013. This system had a set of 13 commonly used features: four channel model scores (forward and backward MLE and lexical weighting scores), a 5-gram language model, five lexicalized reordering model scores (corresponding to different ordering outcomes), linear distortion penalty, word count, and phrase count. The system was optimized using minimum error rate training (Och, 2003) on WMT 2009. a consistent but heuristic manner, picking the leftmost parent node according to its position in the source sentence) We also gather all the bits of the parent and the parent relation. These features may capture agreement phenomena. We also look at the shortest path in the semantic graph from the previous node to the current one and we extract features which describe it: • path length, • relations (edges) along the path. Dataset WMT 2009 = devset WMT 2010 WMT 2013 We use the lemmas of all nodes in the semantic graph as bag-of-word features, as well as all the surface words in the"
W15-3504,W13-2322,0,0.0617336,"s of words and phrases are learned directly from natural data, as are other syntactic operations such as reordering. However, commonly used methods have a very simple view of the linguistic data. Each word is generally modeled independently, for instance, and the relations between words are generally captured only in fixed phrases or as syntactic relationships. Recently there has been a resurgence of interest in unified semantic representations: deep analyses with heavy normalization of morphology, syntax, and even semantic representations. In particular, Abstract Meaning Representation (AMR, Banarescu et al. (2013)) is a novel representation of (sentential) semantics. Such representations could influence a number of natural language understanding and generation tasks, particularly machine translation. Deeper models can be used for multiple aspects of the translation modeling problem. Building translation models that rely on a deeper representation of the input allows for a more parsimonious translation model: morphologically related words can be handled in a unified manner; semantically related concepts are immediately adjacent 2 Related Work There is a large body of related work on utilizing deep langu"
W15-3504,W06-1608,1,0.75418,"French on 1 million parallel sentence pairs and produced 1000-best lists for 34 fort on feature engineering and decoder integration could lead to more substantial gains. Our approach is gated by the accuracy and consistency of the semantic parser. We have used a broad coverage parser with accuracy competitive to the current state-of-the-art, but even the stateof-the-art is rather low. It would be interesting to explore more robust features spanning multiple analyses, or to combine the outputs of multiple parsers. Even syntax-based machine translation systems are dependent on accurate parsers (Quirk and Corston-Oliver, 2006); deeper analyses are likely to be more dependent on parse quality. In a similar vein, it would be interesting to evaluate the impact of morphological, syntactic, and semantic features separately. A careful feature ablation and exploration would help identify promising areas for future research. We have only scratched the surface of possible integrations. Even this model could be applied to MT systems in multiple ways. For instance, rather than applying from source to target, we might evaluate in a noisy channel sense. That is, we could predict the source language surface forms given the targe"
W15-3504,P11-1024,0,0.0201005,"st to (Jones et al., 2012), our work does not have to deal with the complexities of HRG decoding, which runs in O(n3 ) (Jones et al., 2012), as our decoder is simply a phrase-based decoder. Discriminative models have been used in statistical MT many times. Global lexicon model (Mauser et al., 2009) and phrase-sense disambiguation (Carpuat and Wu, 2007) are perhaps the best known methods. Similarly to Carpuat and Wu (2007), we use the classifier to rescore phrasal translations, however we do not train a separate classifier for each source phrase. Instead, we train a global model – similarly to Subotin (2011) or more recently Tamchyna et al. (2014). Features for our model are very different from previous work because they come from a deep representation and therefore should capture semantic relations between the languages, instead of surface or morpho-syntactic correspondences. 3 4 Graph-to-String Translation We develop models for semantic-graph-to-string translation. These models are essentially discriminative translation models, relying on a decomposition structure similar to both maximum entropy language models and IBM Models 1, 2 (Brown et al., 1993), and the HMM translation model (Vogel et al"
W15-3504,P09-1054,0,0.021084,"sociated with node n. We collect all translations observed in the training data and keep the 30 most frequent ones for each lemma. Our model thus assigns zero probability to unseen translations. Because of the size of our training data, we used online learning. We implemented a parallelized (multi-threaded) version of the standard stochastic gradient descent algorithm (SGD). Our learning rate was fixed – using line search, we found the optimal rate to be 0.05. Our batch size was set to one; different batch sizes made almost no difference in model performance. We used online L1 regularization (Tsuruoka et al., 2009) with weight 1. We implemented feature hashing to further improve performance and set the hash length to 22 bits. We shuffled our data and split it into five parts which were processed independently and their final weights were averaged. 4.3 Feature Set Our semantic representation enables us to use a very rich set of features, including information commonly used by both translation models and language models. We extract a significant amount of information from the graph node nai aligned to the generated word: Model • lemma, For our discriminative model, the alignment is assumed to be given. At"
W15-3504,N15-3006,1,0.822073,"´amˇest´ı 25 Prague, Czech Republic Chris Quirk and Michel Galley Mircosoft Research One Microsoft Way Redmond, WA 98052 tamchyna@ufal.mff.cuni.cz {chrisq,mgalley}@microsoft.com Abstract and available for modeling, etc. Language models using deep representations might help us model which interpretations are more plausible. We present an initial discriminative method for modeling the likelihood of a target language surface string given source language deep semantics. This approach relies on an automatic parser for source language semantics. We use a system that parses into AMR-like structures (Vanderwende et al., 2015), and apply the resulting model as an additional feature in a translation system. We present a feature-rich discriminative model for machine translation which uses an abstract semantic representation on the source side. We include our model as an additional feature in a phrase-based decoder and we show modest gains in BLEU score in an n-best re-ranking experiment. 1 Introduction The goal of machine translation is to take source language utterances and convert them into fluent target language utterances with the same meaning. Most recent approaches learn transformations using statistical techni"
W15-3504,C96-2141,0,0.497832,"otin (2011) or more recently Tamchyna et al. (2014). Features for our model are very different from previous work because they come from a deep representation and therefore should capture semantic relations between the languages, instead of surface or morpho-syntactic correspondences. 3 4 Graph-to-String Translation We develop models for semantic-graph-to-string translation. These models are essentially discriminative translation models, relying on a decomposition structure similar to both maximum entropy language models and IBM Models 1, 2 (Brown et al., 1993), and the HMM translation model (Vogel et al., 1996). In particular, we see translation as a process of selecting target words in order conditioned on source language representation as well as prior target words. Similar to the IBM Models, we see each target word as being generated based on source concepts, though in our case the concepts are semantic graph nodes rather than surface words. That is, we assume the existence of an alignment, though it aligns the target words to Semantic Representation Our representation of sentence semantics is based on Logical Form (Vanderwende, 2015). LFs are labeled directed graphs whose nodes roughly correspon"
W15-3504,P01-1067,0,0.188419,"ubject which is replaced by the free variable X. The logical form can be converted using a sequence of rules to a representation which conforms to the AMR specification (Vanderwende et al., 2015). We do not use the full conversion pipeline in our work, so our semantic graphs are somewhere between the LF and AMR. Notably, we keep the bits which serve as important features for the discriminative modeling of translation. mary. (Jones et al., 2012) presents an MT approach that can exploit semantic graphs such as AMR, in a continuation of earlier work that abstracted translation away from strings (Yamada and Knight, 2001; Galley et al., 2004). While rule extraction algorithms such as (Galley et al., 2004) operate on trees and have also been applied to semantic parsing problems (Li et al., 2013), Jones et al. (2012) generalized these approaches by inducing synchronous hyperedge replacement grammars (HRG), which operate on graphs. In contrast to (Jones et al., 2012), our work does not have to deal with the complexities of HRG decoding, which runs in O(n3 ) (Jones et al., 2012), as our decoder is simply a phrase-based decoder. Discriminative models have been used in statistical MT many times. Global lexicon mode"
W18-6104,P17-1107,0,0.0279685,"Missing"
W18-6104,2007.sigdial-1.40,0,0.0386171,"Missing"
W18-6104,E14-1058,0,0.0365703,"Missing"
W18-6104,D17-1231,0,0.0517762,"Missing"
