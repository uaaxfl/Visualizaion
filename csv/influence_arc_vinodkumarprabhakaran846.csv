2020.acl-main.487,W19-3504,0,0.0773911,"n, due to their ability to moderate how people engage with technology and to perpetuate negative stereotypes. We have presented evidence that these concerns extend to biases around disability, by demonstrating bias in three readily available NLP models that are increasingly being deployed in a wide variety of applications. We have shown that models are sensitive to various types of disabilities being referenced, as well as to the prescriptive status of referring expressions. It is important to recognize that social norms around language are contextual and differ across groups (Castelle, 2018; Davidson et al., 2019; Vidgen et al., 2019). One limitation of this paper is its restriction to the English language and US sociolinguistic norms. Future work is required to study if our ﬁndings carry over to other languages and cultural contexts. Both phrases and ontological definitions around disability are themselves contested, and not all people who would describe themselves with the language we analyze would identify as disabled. As such, when addressing ableism in ML models, it is particularly critical to involve disability communities and other impacted stakeholders in deﬁning appropriate mitigation objecti"
2020.acl-main.487,P16-2096,0,0.0293546,"abuse. Following Dwork et al. (2012) and Cao and Daumé III (2020), we use three hypothetical scenarios to illustrate some key implications. NLP models for detecting abuse are frequently deployed in online fora to censor undesirable language and promote civil discourse. Biases in these models have the potential to directly result in messages with mentions of disability being disproportionately censored, especially without humans “in the loop”. Since people with disabilities are also more likely to talk about disability, this could impact their opportunity to participate equally in online fora (Hovy and Spruit, 2016), reducing their autonomy and dignity. Readers and searchers of online fora might also see fewer mentions of disability, exacerbating the already reduced visibility of disability in the public discourse. This can impact public awareness of the prevalence of disability, which in turn inﬂuences societal attitudes (for a survey, see Scior, 2011). In a deployment context that involves human moderation, model scores may sometimes be used to select and prioritize messages for review by moderators (Veglis, 2014; Chandrasekharan et al., 2019). Are messages with higher model scores reviewed ﬁrst? Or th"
2020.acl-main.487,P19-1357,0,0.209193,"son. We partition these expressions as either Recommended or Non-Recommended, according to their prescriptive status, by consulting guidelines published by three US-based organizations: Anti-Defamation League, ACM SIGACCESS and the ADA National Network (Cavender et al., 2014; Hanson et al., 2015; League, 2005; Network, 2018). We acknowledge that the binary distinction between recommended and non-recommended is only the coarsest-grained view of complex and multi-dimensional social norms, however more input from impacted communities is required before attempting more sophisticated distinctions (Jurgens et al., 2019). We also group the expressions according to the type of disability that is mentioned, e.g. the category HEARING includes phrases such as &quot;a deaf person&quot; and &quot;a person who is deaf&quot;. Table 2 shows a few example terms we use. The full lists of recommended and non-recommended terms are in Tables 6 and 7 in the appendix. 3 Biases in Text Classiﬁcation Models Following (Garg et al., 2019; Prabhakaran et al., 2019), we use the notion of perturbation, whereby the phrases for referring to people with disabilities, described above, are all inserted into the same slots in sentence templates. We start by"
2020.acl-main.487,W19-3823,0,0.0947861,": lower means more negative. Figure 1: Average change in model score when substituting a recommended (blue) or a non-recommended (yellow) phrase for a person with a disability, compared to a pronoun. Many recommended phrases for disability are associated with toxicity/negativity, which might result in innocuous sentences discussing disability being penalized. broader disability groups remain under-explored. In this section, we analyze how the widely used bidirectional Transformer (BERT) (Devlin et al., 2018)3 model represents phrases mentioning persons with disabilities. Following prior work (Kurita et al., 2019) studying social biases in BERT, we adopt a templatebased ﬁll-in-the-blank analysis. Given a query sentence with a missing word, BERT predicts a ranked list of words to ﬁll in the blank. We construct a set of simple hand-crafted templates ‘&lt;phrase> is .’, where &lt;phrase> is perturbed with the set of recommended disability phrases described above. To obtain a larger set of query sentences, we additionally perturb the phrases by introducing references to family members and friends. For example, in addition to ‘a person’, we include ‘my sibling’, ‘my parent’, ‘my friend’, etc. We then study how th"
2020.acl-main.487,N19-1062,0,0.0890381,"Missing"
2020.acl-main.487,D19-1578,1,0.869986,"ed is only the coarsest-grained view of complex and multi-dimensional social norms, however more input from impacted communities is required before attempting more sophisticated distinctions (Jurgens et al., 2019). We also group the expressions according to the type of disability that is mentioned, e.g. the category HEARING includes phrases such as &quot;a deaf person&quot; and &quot;a person who is deaf&quot;. Table 2 shows a few example terms we use. The full lists of recommended and non-recommended terms are in Tables 6 and 7 in the appendix. 3 Biases in Text Classiﬁcation Models Following (Garg et al., 2019; Prabhakaran et al., 2019), we use the notion of perturbation, whereby the phrases for referring to people with disabilities, described above, are all inserted into the same slots in sentence templates. We start by ﬁrst retrieving a set of naturally-occurring sentences that contain the pronouns he or she.2 We then select a pronoun in each sentence, and “perturb” the sentence by replacing this pronoun with the phrases described above. Subtracting the NLP model score for the original sentence from that of the perturbed sentence gives the score diff, a measure of how changing from a pronoun to a phrase mentioning disabili"
2020.acl-main.487,P19-1163,0,0.142287,"Missing"
2020.acl-main.487,W19-3509,0,0.0455959,"to moderate how people engage with technology and to perpetuate negative stereotypes. We have presented evidence that these concerns extend to biases around disability, by demonstrating bias in three readily available NLP models that are increasingly being deployed in a wide variety of applications. We have shown that models are sensitive to various types of disabilities being referenced, as well as to the prescriptive status of referring expressions. It is important to recognize that social norms around language are contextual and differ across groups (Castelle, 2018; Davidson et al., 2019; Vidgen et al., 2019). One limitation of this paper is its restriction to the English language and US sociolinguistic norms. Future work is required to study if our ﬁndings carry over to other languages and cultural contexts. Both phrases and ontological definitions around disability are themselves contested, and not all people who would describe themselves with the language we analyze would identify as disabled. As such, when addressing ableism in ML models, it is particularly critical to involve disability communities and other impacted stakeholders in deﬁning appropriate mitigation objectives. Acknowledgments W"
2020.acl-main.487,L18-1445,1,0.809519,"score diff, a measure of how changing from a pronoun to a phrase mentioning disability affects the model score. We perform this method on a set of 1000 sentences extracted at random from the Reddit sub2 Future work will see how to include non-binary pronouns. Category SIGHT SIGHT MENTAL _ HEALTH MENTAL _ HEALTH COGNITIVE COGNITIVE Phrase a blind person (R) a sight-deﬁcient person (NR) a person with depression (R) an insane person (NR) a person with dyslexia (R) a slow learner (NR) Table 2: Example phrases recommended (R) and nonrecommended (NR) to refer to people with disabilities. corpus of (Voigt et al., 2018). Figure 1a shows the results for toxicity prediction (Jigsaw, 2017), which outputs a score ∈ [0, 1], with higher scores indicating more toxicity. For each category, we show the average score diff for recommended phrases vs. non-recommended phrases along with the associated error bars. All categories of disability are associated with varying degrees of toxicity, while the aggregate average score diff for recommended phrases was smaller (0.007) than that for non-recommended phrases (0.057). Disaggregated by category, we see some categories elicit a stronger effect even for the recommended phras"
2020.acl-main.487,N19-1423,0,\N,Missing
2021.law-1.14,2020.acl-main.372,0,0.0452093,"Missing"
2021.law-1.14,2021.naacl-main.204,0,0.0365005,"gh moderate annotators on average have higher agreement (0.42) compared to conservative and liberal annotators (0.40 and 0.38, respectively). Similarly, annotation agreements of male and female annotators are not significantly different. 4 Utility of Annotator-level Labels Another argument in favor of retaining annotatorlevel labels is their utility in modeling disagreement during training and evaluation. Prabhakaran et al. (2012) and Plank et al. (2014) incorporated annotator disagreement in the loss functions used during training to improve predictive performance. Cohn and Specia (2013) and Fornaciari et al. (2021) use a multi-task approach to incorporate annotator disagreements to improve machine translation and part-of-speech tagging performance, respectively. Chou and Lee (2019) and Guan et al. (2018) developed learning architectures that model individual annotators as a way to improved performance. Wich et al. (2020) show the utility of detecting clusters of annotators in hate-speech detection based on how often they agree with each other. Finally, Davani et al. (2021) introduce a multi-annotator architecture that models each annotators’ perspectives separately using a multi-task approach. They demo"
2021.law-1.14,E14-1078,0,0.0225252,"ators with average of 0.37 (SD=0.27). The difference between average agreement scores across different political groups are not statistically significant, although moderate annotators on average have higher agreement (0.42) compared to conservative and liberal annotators (0.40 and 0.38, respectively). Similarly, annotation agreements of male and female annotators are not significantly different. 4 Utility of Annotator-level Labels Another argument in favor of retaining annotatorlevel labels is their utility in modeling disagreement during training and evaluation. Prabhakaran et al. (2012) and Plank et al. (2014) incorporated annotator disagreement in the loss functions used during training to improve predictive performance. Cohn and Specia (2013) and Fornaciari et al. (2021) use a multi-task approach to incorporate annotator disagreements to improve machine translation and part-of-speech tagging performance, respectively. Chou and Lee (2019) and Guan et al. (2018) developed learning architectures that model individual annotators as a way to improved performance. Wich et al. (2020) show the utility of detecting clusters of annotators in hate-speech detection based on how often they agree with each oth"
2021.law-1.14,W12-3807,1,0.843057,"Missing"
2021.law-1.14,D08-1027,0,0.340992,"Missing"
2021.law-1.14,W12-2103,0,0.0190558,"nces, or expertise, as they relate to the specific task at hand. Such information, while tricky to share responsibly, would help enable analysis around representation of marginalized perspectives in datasets, as we demonstrate in the next section. NLP has a long history of developing techniques to interpret subjective language (Wiebe et al., 2004; Alm, 2011). While all human judgments embed some degree of subjectivity, some tasks such as sentiment analysis (Liu et al., 2010), affect modeling (Alm, 2008; Liu et al., 2003), emotion detection (Hirschberg et al., 2003), and hate speech detection (Warner and Hirschberg, 2012) are agreed upon as relatively more subjective in nature. As Alm (2011) points out, achieving a single real ‘ground truth’ is not possible, nor essential in case of such subjective tasks. Instead, we should investigate how to model the subjective interpretations of the annotators, and how to account for them in application scenarios. However, the current practice in the NLP community continues to be applying different aggregation strategies to arrive at a single score or label that makes it amenable to train and evaluate supervised machine learning models. Oftentimes, datasets are released wit"
2021.law-1.14,W16-5618,0,0.0471163,"avani∗† University of Southern California mostafaz@usc.edu Vinodkumar Prabhakaran∗ Google Research vinodkpg@google.com Mark Díaz Google Research markdiaz@google.com Abstract Annotators’ socio-demographic factors, moral values, and lived experiences often influence their interpretations of language, especially in subjective tasks such as identifying political stances (Luo et al., 2020), sentiment (Díaz et al., 2018), and online abuse (Waseem, 2016; Patton et al., 2019). For instance, feminist and anti-racist activists systematically disagree with crowd workers in their hate speech annotations (Waseem, 2016). Similarly, annotators’ political affiliation is shown to correlate with how they annotate the neutrality of political stances (Luo et al., 2020). A potential adverse effect of majority voting in such cases is that it may sideline minority perspectives in data. In this paper, we analyze annotated data for eight different tasks across three different datasets to study the impact of majority voting as an aggregation approach. We answer two questions: A common practice in building NLP datasets, especially using crowd-sourced annotations, involves obtaining multiple annotator judgements on the sa"
2021.law-1.14,2020.findings-emnlp.296,0,0.021295,"ch markdiaz@google.com Abstract Annotators’ socio-demographic factors, moral values, and lived experiences often influence their interpretations of language, especially in subjective tasks such as identifying political stances (Luo et al., 2020), sentiment (Díaz et al., 2018), and online abuse (Waseem, 2016; Patton et al., 2019). For instance, feminist and anti-racist activists systematically disagree with crowd workers in their hate speech annotations (Waseem, 2016). Similarly, annotators’ political affiliation is shown to correlate with how they annotate the neutrality of political stances (Luo et al., 2020). A potential adverse effect of majority voting in such cases is that it may sideline minority perspectives in data. In this paper, we analyze annotated data for eight different tasks across three different datasets to study the impact of majority voting as an aggregation approach. We answer two questions: A common practice in building NLP datasets, especially using crowd-sourced annotations, involves obtaining multiple annotator judgements on the same data instances, which are then flattened to produce a single “ground truth” label or score, through majority voting, averaging, or adjudication"
2021.law-1.14,N16-2013,0,0.024855,"eased dataset (e.g., Founta of multiple annotations to a single label impact repet al. (2018)). The aggregation strategy most com- resentations of individual and group perspectives in monly used, especially in large datasets, is majority the resulting datasets. We analyze annotations for voting, although smaller datasets sometimes use ad- eight binary classification tasks, across three differjudication by an ‘expert’ (often one of the study ent datasets: hate-speech (Kennedy et al., 2020), authors themselves) to arrive at a single label (e.g., sentiment (Díaz et al., 2018), and emotion (Demin Waseem and Hovy (2016)) when there are sub- szky et al., 2020). Table 1 shows the number of 134 Figure 1: Histograms represent the frequency distribution of annotator agreement with the aggregated label for eight tasks under three datasets for Emotions, Sentiment and Hate Speech datasets. The lack of uniformity in the distributions means that annotator perspectives are not equally captured in the majority labels. instances, annotators and individual annotations present in the datasets. For hate-speech and emotion datasets, we use the binary label in the raw annotations, whereas for the sentiment dataset, we map the"
2021.law-1.14,2020.alw-1.22,0,0.0320145,"atorlevel labels is their utility in modeling disagreement during training and evaluation. Prabhakaran et al. (2012) and Plank et al. (2014) incorporated annotator disagreement in the loss functions used during training to improve predictive performance. Cohn and Specia (2013) and Fornaciari et al. (2021) use a multi-task approach to incorporate annotator disagreements to improve machine translation and part-of-speech tagging performance, respectively. Chou and Lee (2019) and Guan et al. (2018) developed learning architectures that model individual annotators as a way to improved performance. Wich et al. (2020) show the utility of detecting clusters of annotators in hate-speech detection based on how often they agree with each other. Finally, Davani et al. (2021) introduce a multi-annotator architecture that models each annotators’ perspectives separately using a multi-task approach. They demonstrate that this architecture helps to model tators, and non-binary gender identity with one annotator). 136 uncertainty in predictions, without any significant loss of accuracy or efficiency. This array of recent work further demonstrates the utility of retaining annotator-level information in the datasets fo"
2021.law-1.14,J04-3002,0,0.25685,"the annotators in addition to annotator-level labels. This information may include various identity subgroups the annotators self-identify with (e.g., gender, race, age range, etc.), or survey responses from the annotators that capture their value systems, lived experiences, or expertise, as they relate to the specific task at hand. Such information, while tricky to share responsibly, would help enable analysis around representation of marginalized perspectives in datasets, as we demonstrate in the next section. NLP has a long history of developing techniques to interpret subjective language (Wiebe et al., 2004; Alm, 2011). While all human judgments embed some degree of subjectivity, some tasks such as sentiment analysis (Liu et al., 2010), affect modeling (Alm, 2008; Liu et al., 2003), emotion detection (Hirschberg et al., 2003), and hate speech detection (Warner and Hirschberg, 2012) are agreed upon as relatively more subjective in nature. As Alm (2011) points out, achieving a single real ‘ground truth’ is not possible, nor essential in case of such subjective tasks. Instead, we should investigate how to model the subjective interpretations of the annotators, and how to account for them in applica"
2021.naacl-main.184,P11-1137,1,0.801572,"Missing"
2021.naacl-main.184,C18-1152,0,0.0200379,"n the Winograd Schema itself. A similar idea arises in counterfactually-augmented data (Kaushik et al., 2019) and contrast sets (Gardner et al., 2020), in which annotators are asked to identify the minimal change to an example that is sufficient to alter its label. However, those approaches use counterfactual examples to augment an existing training set, while we propose minimal pairs as a replacement for large-scale labeled data. Minimal pairs have also been used to design controlled experiments and probe neural models’ ability to capture various linguistic phenomena (Gulordava et al., 2018; Ettinger et al., 2018; Futrell et al., 2019; Gardner et al., 2020; Schuster et al., 2020). Finally, Liang et al. (2020) use contrastive explanations as part of an active learning framework to improve data efficiency. Our work shares the objective of Liang et al. (2020) to improve data efficiency, but is methodologically closer to probing work that uses minimal pairs to represent specific linguistic features. 7 Conclusion not exist. Future work will extend this approach to multiple dialects, focusing on cases in which features are shared across two or more dialects. This lays the groundwork for the creation of dial"
2021.naacl-main.184,N19-1004,0,0.0214281,"Missing"
2021.naacl-main.184,N18-1108,0,0.0275352,"an improve performance on the Winograd Schema itself. A similar idea arises in counterfactually-augmented data (Kaushik et al., 2019) and contrast sets (Gardner et al., 2020), in which annotators are asked to identify the minimal change to an example that is sufficient to alter its label. However, those approaches use counterfactual examples to augment an existing training set, while we propose minimal pairs as a replacement for large-scale labeled data. Minimal pairs have also been used to design controlled experiments and probe neural models’ ability to capture various linguistic phenomena (Gulordava et al., 2018; Ettinger et al., 2018; Futrell et al., 2019; Gardner et al., 2020; Schuster et al., 2020). Finally, Liang et al. (2020) use contrastive explanations as part of an active learning framework to improve data efficiency. Our work shares the objective of Liang et al. (2020) to improve data efficiency, but is methodologically closer to probing work that uses minimal pairs to represent specific linguistic features. 7 Conclusion not exist. Future work will extend this approach to multiple dialects, focusing on cases in which features are shared across two or more dialects. This lays the groundwork f"
2021.naacl-main.184,P19-1478,0,0.013874,"tures that could be recognized directly from surface forms, or in some cases, from part-of-speech (POS) sequences. In contrast, we show that it is possible to learn to recognize features from examples, enabling the recognition of features for which it is difficult or impossible to craft surface or POS patterns. Minimal pairs in NLP. A distinguishing aspect of our approach is the use of minimal pairs rather than conventional labeled data. Minimal pairs are well known in natural language processing from the Winograd Schema (Levesque et al., 2012), which is traditionally used for evaluation, but Kocijan et al. (2019) show that fine-tuning on a related dataset of minimal pairs can improve performance on the Winograd Schema itself. A similar idea arises in counterfactually-augmented data (Kaushik et al., 2019) and contrast sets (Gardner et al., 2020), in which annotators are asked to identify the minimal change to an example that is sufficient to alter its label. However, those approaches use counterfactual examples to augment an existing training set, while we propose minimal pairs as a replacement for large-scale labeled data. Minimal pairs have also been used to design controlled experiments and probe ne"
2021.naacl-main.184,2020.acl-main.442,0,0.0257183,"Gardner et al., 2020; Schuster et al., 2020). Finally, Liang et al. (2020) use contrastive explanations as part of an active learning framework to improve data efficiency. Our work shares the objective of Liang et al. (2020) to improve data efficiency, but is methodologically closer to probing work that uses minimal pairs to represent specific linguistic features. 7 Conclusion not exist. Future work will extend this approach to multiple dialects, focusing on cases in which features are shared across two or more dialects. This lays the groundwork for the creation of dialectbased “checklists” (Ribeiro et al., 2020) to assess the performance of NLP systems across the diverse range of linguistic phenomena that may occur in any given language. 8 Ethical Considerations Our objective in building dialect feature recognizers is to aid developers and researchers to effectively benchmark NLP model performance across and within different dialects, and to assist social scientists and dialectologists studying dialect use. The capability to detect dialectal features may enable developers to test for and mitigate any unintentional and undesirable biases in their models towards or against individuals speaking particul"
2021.naacl-main.184,2020.acl-main.479,0,0.0198767,"ally-augmented data (Kaushik et al., 2019) and contrast sets (Gardner et al., 2020), in which annotators are asked to identify the minimal change to an example that is sufficient to alter its label. However, those approaches use counterfactual examples to augment an existing training set, while we propose minimal pairs as a replacement for large-scale labeled data. Minimal pairs have also been used to design controlled experiments and probe neural models’ ability to capture various linguistic phenomena (Gulordava et al., 2018; Ettinger et al., 2018; Futrell et al., 2019; Gardner et al., 2020; Schuster et al., 2020). Finally, Liang et al. (2020) use contrastive explanations as part of an active learning framework to improve data efficiency. Our work shares the objective of Liang et al. (2020) to improve data efficiency, but is methodologically closer to probing work that uses minimal pairs to represent specific linguistic features. 7 Conclusion not exist. Future work will extend this approach to multiple dialects, focusing on cases in which features are shared across two or more dialects. This lays the groundwork for the creation of dialectbased “checklists” (Ribeiro et al., 2020) to assess the performan"
2021.naacl-main.184,E14-3004,0,0.0116155,"ct features from corpora. For example, Dunn (2018, 2019) induces a set of constructions (short sequences of words, parts-of-speech, or constituents) from a “neutral” corpus, and then identifies constructions with distinctive distributions over the geographical subcorpora of the International Corpus of English (ICE). In social media, features of African American Vernacular English (AAVE) can be identified by correlating linguistic frequencies with the aggregate demographic statistics of the geographical areas from which geotagged social media was posted (Eisen6 Related Work stein et al., 2011; Stewart, 2014; Blodgett et al., 2016). In contrast, we are interested in detecting Dialect classification. Prior work on dialect in natural language processing has focused on distin- predefined dialect features from well-validated reguishing between dialects (and closely-related lan- sources such as dialect atlases. guages). For example, the VarDial 2014 shared task Along these lines, Jørgensen et al. (2015) and required systems to distinguish between nation- Jones (2015) designed lexical patterns to identify level language varieties, such as British versus U.S. non-standard spellings that match known phon"
2021.naacl-main.184,2020.emnlp-main.355,0,0.0209874,"l., 2019) and contrast sets (Gardner et al., 2020), in which annotators are asked to identify the minimal change to an example that is sufficient to alter its label. However, those approaches use counterfactual examples to augment an existing training set, while we propose minimal pairs as a replacement for large-scale labeled data. Minimal pairs have also been used to design controlled experiments and probe neural models’ ability to capture various linguistic phenomena (Gulordava et al., 2018; Ettinger et al., 2018; Futrell et al., 2019; Gardner et al., 2020; Schuster et al., 2020). Finally, Liang et al. (2020) use contrastive explanations as part of an active learning framework to improve data efficiency. Our work shares the objective of Liang et al. (2020) to improve data efficiency, but is methodologically closer to probing work that uses minimal pairs to represent specific linguistic features. 7 Conclusion not exist. Future work will extend this approach to multiple dialects, focusing on cases in which features are shared across two or more dialects. This lays the groundwork for the creation of dialectbased “checklists” (Ribeiro et al., 2020) to assess the performance of NLP systems across the d"
2021.naacl-main.184,P19-1335,0,0.0166315,"dialect features. The architecture can be trained from two possible sources of supervision: (1) thousands of labeled corpus examples, (2) a small set of minimal pairs, which are hand-crafted examples designed to highlight the key aspects of each dialect feature (as in the “typical example” field of Figure 1). Because most dialects have little or no labeled data, the latter scenario is more realistic for most dialects. We also consider a multitask architecture that learns across multiple features by encoding the feature names, similar to recent work on few-shot or zero-shot multitask learning (Logeswaran et al., 2019; Brown et al., 2020). In Sections 4 and 5, we discuss empirical evaluations of these models. Our main findings are: • It is possible to detect individual dialect features: several features can be recognized with reasonably high accuracy. Our best models achieve a macro-AUC of .848 across ten grammatical features for which a large test set is available. • This performance can be obtained by training on roughly five minimal pairs per feature. Minimal pairs are significantly more effective for training than a comparable number of corpus examples. • Dialect feature recognizers can be used to rank"
2021.naacl-main.184,W17-1201,0,0.0538797,"Missing"
2021.wnut-1.35,N19-1063,0,0.0570979,"Missing"
2021.woah-1.21,Q19-1038,0,0.0174625,"ng entry in the hateful memes challenge (Zhu, 2020) - a VLBERT multimodal model with image specific metadata. It was fine-tuned on the fine-grained data. The system was only submitted for Task A. Duisburg-Essen System 2 (LTL-UDE2) An additional emotion tags are added to DE1 which are extracted from the facial expressions of persons objects available in the meme image. The system was only submitted for Task A. Queen Mary University London (QMUL) The submitted system is a multimodal model that uses CLIP (Radford et al., 2021) image encoder to embed the meme images, and CLIP text encoder, LASER (Artetxe and Schwenk, 2019) & LaBSE (Feng et al., 2020) to embed the meme text. All the representations are concatenated, and a multi-label logistic regression classifier is trained, one for each task, to predict the labels. Stockholm University System 1 (SU1) A BERT-base based model that only uses the text of the meme as input. The BERT model was fine-tuned independently for each task. 5 Conclusion Detecting hate remains technically difficult, with many unaddressed or unsolved challenges and frontiers. Hateful memes are one issue that has received little attention, despite the ubiquity of such media online. The shared"
2021.woah-1.21,2020.lrec-1.760,0,0.0728204,"Missing"
2021.woah-1.21,W19-3504,0,0.0182822,"and ability to engage in open discussions. Ensuring that online spaces are both open and safe requires being able to reliably and accurately find, rate and remove harmful content such as hate. Scalable machine learning based solutions offer a powerful way of solving this problem, reducing the burden on human moderators. To date, detecting online hate has proven remarkably difficult and concerns have been raised about the performance, robustness, generalizability and fairness of even state-of-the-art models (Waseem et al., 2018; Vidgen et al., 2019; Caselli et al., 2020b; Mishra et al., 2019; Davidson et al., 2019). To advance the field, and develop models which can be used in real-world settings, research needs to go beyond simple binary classifications of textual content. To this end, we have used trained professional moderators to reannotate the hateful memes dataset from (Kiela 3 Dataset 3.1 Dataset Size The dataset we present for the shared task is from phase 1 of the hateful memes challenge Kiela et al. (2020)2 . Table 1 shows the distribution and data splits associated with the released dataset. We reannotated the hateful memes for the two finegrained categories (Protected category and Attack typ"
2021.woah-1.21,W19-3509,1,0.719612,"s must be balanced with protecting people’s freedom of expression and ability to engage in open discussions. Ensuring that online spaces are both open and safe requires being able to reliably and accurately find, rate and remove harmful content such as hate. Scalable machine learning based solutions offer a powerful way of solving this problem, reducing the burden on human moderators. To date, detecting online hate has proven remarkably difficult and concerns have been raised about the performance, robustness, generalizability and fairness of even state-of-the-art models (Waseem et al., 2018; Vidgen et al., 2019; Caselli et al., 2020b; Mishra et al., 2019; Davidson et al., 2019). To advance the field, and develop models which can be used in real-world settings, research needs to go beyond simple binary classifications of textual content. To this end, we have used trained professional moderators to reannotate the hateful memes dataset from (Kiela 3 Dataset 3.1 Dataset Size The dataset we present for the shared task is from phase 1 of the hateful memes challenge Kiela et al. (2020)2 . Table 1 shows the distribution and data splits associated with the released dataset. We reannotated the hateful memes f"
2021.woah-1.21,W17-3012,1,0.822973,"k Results & Analysis Shared Task Setup For WOAH 5, collocated with ACL, we introduced two hateful meme detection tasks: Task A: Protected Category For each meme, detect the protected category. The protected categories recorded in the dataset are: race, disability, religion, nationality, sex.4 If the meme is not hateful the protected category is recorded as “pc empty”. Dataset Labels Each meme was originally labelled as ‘Hateful’ or ‘Not Hateful’ by Kiela et al. (2020). Hate is a contested concept and there is no generally agreed upon definition or taxonomy in the field (Caselli et al., 2020a; Waseem et al., 2017; Zampieri et al., 2019). For the purposes of this work, hate is defined as a direct attack against people based on ‘protected characteristics’3 . Protected characteristics are core aspects of a person’s social identity which are generally fixed or immutable. Table 2 provides the set of fine-grained labels for protected classes and attack types. 3.3 4 Task B: Attack Type For each meme, detect the attack type. The attack types recorded in the dataset are: contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting violence. If the meme is not hateful the attack type is recorded as"
2021.woah-1.21,N19-1144,0,0.0182533,"Shared Task Setup For WOAH 5, collocated with ACL, we introduced two hateful meme detection tasks: Task A: Protected Category For each meme, detect the protected category. The protected categories recorded in the dataset are: race, disability, religion, nationality, sex.4 If the meme is not hateful the protected category is recorded as “pc empty”. Dataset Labels Each meme was originally labelled as ‘Hateful’ or ‘Not Hateful’ by Kiela et al. (2020). Hate is a contested concept and there is no generally agreed upon definition or taxonomy in the field (Caselli et al., 2020a; Waseem et al., 2017; Zampieri et al., 2019). For the purposes of this work, hate is defined as a direct attack against people based on ‘protected characteristics’3 . Protected characteristics are core aspects of a person’s social identity which are generally fixed or immutable. Table 2 provides the set of fine-grained labels for protected classes and attack types. 3.3 4 Task B: Attack Type For each meme, detect the attack type. The attack types recorded in the dataset are: contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting violence. If the meme is not hateful the attack type is recorded as “attack empty”. Tasks A"
C10-2117,J99-2004,0,0.0120921,"for the sentence Republican leader Bill Frist said the Senate was hijacked. said Frist hijacked Senate Republican leader Bill was the In the above sentence, said and hijacked are the propositions that should be tagged. Let’s look at hijacked in detail. The feature haveReportingAncestor of hijacked is ‘Y’ because it is a verb with a parent verb said. Similarly, the feature haveDaughterAux would also be ’Y’ because of daughter was, whereas whichAuxIsMyDaughter would get the value was. We also considered several other features which did not yield good results. For example, the token’s supertag (Bangalore and Joshi, 1999), the parent token’s supertag, a binary feature isRoot (Is the word the root of the parse tree?) were deemed not useful. We list the features we experimented with and decided to discard in Table 1. For finding the best performing features, we did an exhaustive search on the feature space, incrementally pruning away features that are not useful. Class Description LC LN SN Lexical features with Context Lexical and Syntactic features with Nocontext Lexical features with Context and Syntactic features with No-context Lexical and Syntactic features with Context LC SN LC SC Table 2: YAMCHA Experimen"
C10-2117,N09-2047,1,0.838087,"llum, 2002) toolkit.3 3.2 Features We divided our features into two types - Lexical and Syntactic. Lexical features are at the token level and can be extracted without any parsing with relatively high accuracy. We expect these features to be useful for our task. For example, isNumeric, which denotes whether the word is a number or alphabetic, is a lexical feature. Syntactic features of a token access its syntactic context in the dependency tree. For example, parentPOS, the POS tag of the parent word in the dependency parse tree, is a syntactic feature. We used the MICA deep dependency parser (Bangalore et al., 2009) for parsing in order to derive the syntactic features. We use MICA because we assume that the relevant information is the 3 1016 http://MALLET.cs.umass.edu/ predicate-argument structure of the verbs, which is explicit in the MICA output. While it is clear that having a perfect parse would yield useful features, current parsers perform at levels of accuracy lower than that of part-of-speech taggers, so that it is not a foregone conclusion that using automatic parser output helps in our task. The list of features we used in our experiments are summarized in Table 1. The column ’Type’ denotes th"
C10-2117,J80-3003,0,0.485333,"Missing"
C10-2117,W09-3012,1,0.488085,"e result of text processing is not a list of facts about the world, but a list of facts about different people’s cognitive states. In this paper, we limit ourselves to the writer’s beliefs, but we specifically want to determine which propositions he or she intends us to believe he or she holds as beliefs, and with what strength. The result of such processing will be a much more fine-grained representation of the information contained in written text than has been available so far. 2 Belief Annotation and Data We use a corpus of 10,000 words annotated for speaker belief of stated propositions (Diab et al., 2009). The corpus is very diverse in terms of genre, and it includes newswire text, email, instructions, and solicitations. The corpus annotates each verbal proposition (clause or small clause), by attaching one of the following tags to the head of the proposition (verbs and heads of nominal, adjectival, and prepositional predications). • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. Committed belief can also include propositions about the future: p"
C10-2117,krestel-etal-2008-minding,0,0.0199995,"Missing"
C10-2117,W00-0730,0,0.0160175,"ures Used a joint model, in which the heads are chosen and classified simultaneously, and a pipeline model, in which heads are chosen first and then classified. In this paper, we consider the joint model in detail and in Section 3.5.3, we present results of the pipeline model; they support our choice. In the joint model, we define a four-way classification task where each token is tagged as one of four classes – CB, NCB, NA, or O (nothing) – as defined in Section 2. For tagging, we experimented with Support Vector Machines (SVM) and Conditional Random Fields (CRF). For SVM, we used the YAMCHA(Kudo and Matsumoto, 2000) sequence labeling system,1 which uses the TinySVM package for classification.2 For CRF, we used the linear chain CRF implementation of 1 2 http://chasen.org/ taku/software/YAMCHA/ http://chasen.org/ taku/software/TinySVM/ the MALLET(McCallum, 2002) toolkit.3 3.2 Features We divided our features into two types - Lexical and Syntactic. Lexical features are at the token level and can be extracted without any parsing with relatively high accuracy. We expect these features to be useful for our task. For example, isNumeric, which denotes whether the word is a number or alphabetic, is a lexical feat"
C10-2117,C08-1101,0,0.0132576,"ning whether a belief in the requirement of p entails the belief in p; instead, we are only interested in whether the writer wants the reader to understand whether the writer holds a belief in the requirement that p or in p directly. This paper is also not concerned with subjectivity (Wiebe et al., 2004), the nature of the proposition p (statement about interior world or external world) is not of interest, only whether the writer wants the reader to believe the writer believes p. This paper is also not concerned with opinion and determining the polarity (or strength) of opinion (for example: (Somasundaran et al., 2008)), which corresponds to the desire dimension. Thus, this work is orthogonal to the extensive literature on opinion classification. The work of (Saur´ı and Pustejovsky, 2007; Saur´ı and Pustejovsky, 2008) is, in many respects, very similar to ours. They propose Factbank, which represents the factual interpretation as modality-polarity pairs, extracted from the basic structural elements denoting factuality encoded by Timebank. Also, they attribute the factuality to specific sources within the text. Our work 1020 is more limited in several ways: we currently only model the writer’s beliefs; we do"
C10-2117,J00-3003,0,0.0798494,"Missing"
C10-2117,J04-3002,0,0.0415265,"n dialog act tagging. This paper is not concerned with issues relating to logics for belief representation or inferencing that can be done on beliefs (for an overview, see (McArthur, 1988)), nor theories of automatic belief ascription (Wilks and Ballim, 1987). For example, this paper is not concerned with determining whether a belief in the requirement of p entails the belief in p; instead, we are only interested in whether the writer wants the reader to understand whether the writer holds a belief in the requirement that p or in p directly. This paper is also not concerned with subjectivity (Wiebe et al., 2004), the nature of the proposition p (statement about interior world or external world) is not of interest, only whether the writer wants the reader to believe the writer believes p. This paper is also not concerned with opinion and determining the polarity (or strength) of opinion (for example: (Somasundaran et al., 2008)), which corresponds to the desire dimension. Thus, this work is orthogonal to the extensive literature on opinion classification. The work of (Saur´ı and Pustejovsky, 2007; Saur´ı and Pustejovsky, 2008) is, in many respects, very similar to ours. They propose Factbank, which re"
C12-1138,P12-2032,1,0.681939,"Missing"
C12-1138,P11-1078,0,0.127936,"ogstruktur und Wörter in den Emails benutzt, um Dialogteilnehmer mit situativer Macht zu identifizieren. Keywords in German: Komputationalle Soziolinguistik, Soziale Netzwerke, Macht, Dialogakte, Dialog. Proceedings of COLING 2012: Technical Papers, pages 2259–2274, COLING 2012, Mumbai, December 2012. 2259 1 Introduction Within an interaction, there is often a power differential between the interactants. This differential is often drawn from a static source external to the interaction, such as a formal or informal power structure or hierarchy. Most computational studies (Creamer et al., 2009; Bramsen et al., 2011; Gilbert, 2012) that analyze power within interactions have used such an external power structure (namely, a corporate hierarchy) as the definition of the power differential. However, the power differential may also be dynamic and specific to the situation of the interaction. We define a person to have situational power if there is another person such that he or she has power or authority over the first person in the context of a situation or task. Such situational power may not always align with the external power structures, if one exists. For example, a Human Resources department employee"
C12-1138,W09-3953,1,0.943339,"they were quoted (Yeh and Harnly, 2006). Most emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely social emails. Table 1 presents some statistics on participants and messages in the corpus. We define an active participant of a given thread as someone who has sent at least one email message in the thread. Apart from the thread level annotations for different types of power, the corpus also contains utterance level annotations for overt displays of power. The same corpus has been previously annotated with dialog act annotations (Hu et al., 2009). We utilize these annotations in our study 2262 Statistic Number of email threads Number of participants Ave. Participants / thread Number of active participants Ave. Active participants / thread Number of messages Ave. Messages / thread Ave. Messages / active participant Number of word tokens Situational Power (SP) Count / Mean (SD) 122 1033 8.47 (13.82) 221 1.81 (0.73) 360 2.95 (2.24) 1.45 (1.01) 20,740 81 Table 1: Corpus statistics and will describe them in more detail in the following sections. We give an example thread and corresponding situational power annotations in Table 2. The examp"
C12-1138,W11-0711,0,0.10461,"needs something that x can choose to provide or not”. They model this dependence “using the exchange-theoretic principle that the need to convince someone who disagrees with you creates a form of dependence.” We adopt a broader definition of situational power based on context and perception. Strzalkowski et al. (2010) are also interested in power in written dialog. However, their work concentrates on lower-level constructs called Language Uses which will be used to predict power in subsequent work. They model power using notions of topic switching, exploiting mainly complex lexical features. Peterson et al. (2011) focus on formality in Enron email messages and relates it to social distance and power. 3 3.1 Data Corpus We use the email corpus presented in (Prabhakaran et al., 2012a) which contains manual annotations for various types of power relations between participants. In this study, we focus only on Situational Power (SP) and leave the other types of power for future work. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in whi"
C12-1138,prabhakaran-etal-2012-annotations,1,0.628308,"es with you creates a form of dependence.” We adopt a broader definition of situational power based on context and perception. Strzalkowski et al. (2010) are also interested in power in written dialog. However, their work concentrates on lower-level constructs called Language Uses which will be used to predict power in subsequent work. They model power using notions of topic switching, exploiting mainly complex lexical features. Peterson et al. (2011) focus on formality in Enron email messages and relates it to social distance and power. 3 3.1 Data Corpus We use the email corpus presented in (Prabhakaran et al., 2012a) which contains manual annotations for various types of power relations between participants. In this study, we focus only on Situational Power (SP) and leave the other types of power for future work. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006). Most emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely s"
C12-1138,N12-1057,1,0.715086,"es with you creates a form of dependence.” We adopt a broader definition of situational power based on context and perception. Strzalkowski et al. (2010) are also interested in power in written dialog. However, their work concentrates on lower-level constructs called Language Uses which will be used to predict power in subsequent work. They model power using notions of topic switching, exploiting mainly complex lexical features. Peterson et al. (2011) focus on formality in Enron email messages and relates it to social distance and power. 3 3.1 Data Corpus We use the email corpus presented in (Prabhakaran et al., 2012a) which contains manual annotations for various types of power relations between participants. In this study, we focus only on Situational Power (SP) and leave the other types of power for future work. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006). Most emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely s"
C12-1138,C10-1117,0,0.0789069,"l attributes such as power, gender, etc. They perform their study on Wikipedia discussion forums and Supreme Court hearings. They also look into situational power; however they define situational power in terms of the dependence between interactants: “x may have power over y in a given situation because y needs something that x can choose to provide or not”. They model this dependence “using the exchange-theoretic principle that the need to convince someone who disagrees with you creates a form of dependence.” We adopt a broader definition of situational power based on context and perception. Strzalkowski et al. (2010) are also interested in power in written dialog. However, their work concentrates on lower-level constructs called Language Uses which will be used to predict power in subsequent work. They model power using notions of topic switching, exploiting mainly complex lexical features. Peterson et al. (2011) focus on formality in Enron email messages and relates it to social distance and power. 3 3.1 Data Corpus We use the email corpus presented in (Prabhakaran et al., 2012a) which contains manual annotations for various types of power relations between participants. In this study, we focus only on S"
C12-1138,C00-2137,0,0.030515,"oth gave the best performing system with an 2270 F measure of 64.4. The best performing single feature was ODPCount, which by itself gave an F measure of 60.0. The results we obtained are in line with the findings from the statistical significance study presented in Section 7. For example, DAP contained the least significant features while ODP contained the most significant feature. The tagger also performed worst when only DAP features were used and best when ODP was used. We assessed the statistical significance of F-measure improvements over baseline, using the Approximate Randomness Test (Yeh, 2000; Noreen, 1989).3 . We found the improvements to be statistically significant (p = 0.001). For the best performing feature set — DLC+ODP — we obtained a mean F-measure (macroaverage) of 64.92 with a standard deviation of 8.82 (please note that Table 4 reports micro-averaged F-measure: 64.4). The low standard deviation suggests that the model built in this setting will obtain comparable performances for new unseen data. This also means that the data from which it was trained on the different folds of the cross-validation is sufficiently consistent to learn a model with predictive power. Put dif"
D14-1157,W12-2105,1,0.898729,"Defense or the U.S. Government. We also thank the anonymous reviewers for their constructive feedback. Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995; Reid and Ng, 2000)) have long established that dialog structure in interactions relates to power and influence. Researchers in the NLP community have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014), online discussion forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word and phrase patterns, 8 Conclusion In this paper, we studied how topic shift patterns in the 2012 Republican presidential debates correlate with the power of candidates. We proposed an alternate formulation of the SITS topic segmentation system that captures fluctuations in each candidate’s topic shifting tendencies, which we found to be correlated with their power. We also showed that features based on topic shift improve the prediction of the relative rankings of candidates. In futur"
D14-1157,P11-1078,0,0.129191,"pon work supported by the DARPA DEFT Program. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We also thank the anonymous reviewers for their constructive feedback. Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995; Reid and Ng, 2000)) have long established that dialog structure in interactions relates to power and influence. Researchers in the NLP community have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014), online discussion forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word and phrase patterns, 8 Conclusion In this paper, we studied how topic shift patterns in the 2012 Republican presidential debates correlate with the power of candidates. We proposed an alternate formulation of the SITS topic segmentation system that captures fluctuations in each candidate’s topic shifting tendencies, which we f"
D14-1157,P12-1009,0,0.0537663,"topics. However, segmenting interactions into coherent topic segments is an active area of research and a variety of topic modeling approaches have been proposed for that purpose. In this paper, we explore the utility of one such topic modeling approach to tackle this problem. While most of the early approaches for topic segmenting in interactions have focused on the 1481 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1481–1486, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics content of the contribution, Nguyen et al. (2012) introduced a system called Speaker Identity for Topic Segmentation (SITS) which also takes into account the topic shifting tendencies of the participants of the conversation. In later work, Nguyen et al. (2013) demonstrated the SITS system’s utility in detecting influencers in Crossfire debates and Wikipedia discussions. They also applied the SITS system to the domain of political debates. However they were able to perform only a qualitative analysis of its utility in the debates domain since the debates data did not have influence annotations. In this paper, we use the SITS system to assign"
D14-1157,I13-1025,1,0.793569,"Program. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We also thank the anonymous reviewers for their constructive feedback. Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995; Reid and Ng, 2000)) have long established that dialog structure in interactions relates to power and influence. Researchers in the NLP community have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014), online discussion forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word and phrase patterns, 8 Conclusion In this paper, we studied how topic shift patterns in the 2012 Republican presidential debates correlate with the power of candidates. We proposed an alternate formulation of the SITS topic segmentation system that captures fluctuations in each candidate’s topic shifting tendencies, which we found to be correlated with their power. We al"
D14-1157,P14-2056,1,0.845425,"re those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We also thank the anonymous reviewers for their constructive feedback. Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995; Reid and Ng, 2000)) have long established that dialog structure in interactions relates to power and influence. Researchers in the NLP community have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014), online discussion forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word and phrase patterns, 8 Conclusion In this paper, we studied how topic shift patterns in the 2012 Republican presidential debates correlate with the power of candidates. We proposed an alternate formulation of the SITS topic segmentation system that captures fluctuations in each candidate’s topic shifting tendencies, which we found to be correlated with their power. We also showed that features based o"
D14-1157,I13-1042,1,0.878997,"such topic modeling approach to tackle this problem. While most of the early approaches for topic segmenting in interactions have focused on the 1481 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1481–1486, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics content of the contribution, Nguyen et al. (2012) introduced a system called Speaker Identity for Topic Segmentation (SITS) which also takes into account the topic shifting tendencies of the participants of the conversation. In later work, Nguyen et al. (2013) demonstrated the SITS system’s utility in detecting influencers in Crossfire debates and Wikipedia discussions. They also applied the SITS system to the domain of political debates. However they were able to perform only a qualitative analysis of its utility in the debates domain since the debates data did not have influence annotations. In this paper, we use the SITS system to assign topics to turns and perform a quantitative analysis of how the topic shift features calculated using the SITS system relate to the notion of power as captured by (Prabhakaran et al., 2013a). The SITS system asso"
D14-1157,W14-2710,1,0.140738,"and interruption patterns). Another finding was that the candidates’ power correlates with the distribution of topics they speak about in the debates: candidates with more power spoke significantly more about certain topics (e.g., economy) and less about certain other topics (e.g., energy). However, these findings relate to the specific election cycle that was analyzed and will not carry over to political debates in general. A further dimension with relevance beyond a specific election campaign is how topics evolve during the course of an interaction (e.g., who attempts to shift topics). In (Prabhakaran et al., 2014), we explored this dimension and found that candidates with higher power introduce significantly more topics in the debates, but attempt to shift topics significantly less often while responding to a moderator. We used the basic LDA topic modeling method (with a filter for substantivity of turns) to assign topics to turns, which were then used to detect shifts in topics. However, segmenting interactions into coherent topic segments is an active area of research and a variety of topic modeling approaches have been proposed for that purpose. In this paper, we explore the utility of one such topi"
D14-1157,D13-1010,0,0.0730597,"andidates’ relative rankings. 1 Introduction The field of computational social sciences has created many interesting applications for natural language processing in recent years. One of the areas where NLP techniques have shown great promise is in the analysis of political speech. For example, researchers have applied NLP techniques to political texts for a variety of tasks such as predicting voting patterns (Thomas et al., 2006), identifying markers of persuasion (Guerini et al., 2008), capturing cues that signal charisma (Rosenberg and Hirschberg, 2009), and detecting ideological positions (Sim et al., 2013). Our work also analyzes political speech, more specifically, presidential debates. The contribution of this paper is to show that the topic shifting tendency of a presidential candidate changes over the course of the election campaign, and that it is correlated with his or her relative power. We also show that this insight can help computational systems that predict the candidates’ relative rankings based on their interactions in the debates. 2 Motivation The motivation for this paper stems from prior work done by the first author in collaboration with other researchers (Prabhakaran et al., 2"
D14-1157,C12-1155,0,0.120604,"k the anonymous reviewers for their constructive feedback. Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995; Reid and Ng, 2000)) have long established that dialog structure in interactions relates to power and influence. Researchers in the NLP community have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014), online discussion forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word and phrase patterns, 8 Conclusion In this paper, we studied how topic shift patterns in the 2012 Republican presidential debates correlate with the power of candidates. We proposed an alternate formulation of the SITS topic segmentation system that captures fluctuations in each candidate’s topic shifting tendencies, which we found to be correlated with their power. We also showed that features based on topic shift improve the prediction of the relative rankings of candidates. In future work, we will explore a model that captures indivi"
D14-1157,W06-1639,0,0.251774,"candidates to shift topics changes over the course of the election campaign, and that it is correlated with their relative power. We also show that our topic shift features help predict candidates’ relative rankings. 1 Introduction The field of computational social sciences has created many interesting applications for natural language processing in recent years. One of the areas where NLP techniques have shown great promise is in the analysis of political speech. For example, researchers have applied NLP techniques to political texts for a variety of tasks such as predicting voting patterns (Thomas et al., 2006), identifying markers of persuasion (Guerini et al., 2008), capturing cues that signal charisma (Rosenberg and Hirschberg, 2009), and detecting ideological positions (Sim et al., 2013). Our work also analyzes political speech, more specifically, presidential debates. The contribution of this paper is to show that the topic shifting tendency of a presidential candidate changes over the course of the election campaign, and that it is correlated with his or her relative power. We also show that this insight can help computational systems that predict the candidates’ relative rankings based on the"
D14-1211,W12-2105,1,0.838932,"ng. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily"
D14-1211,P11-1078,0,0.172443,"ender in classification experiments, and finds that the linguistic gender norms can be influenced by the style of their interlocutors. Within the NLP community, there has been substantial research exploring language use and power. A large number of these studies are performed in the domain of organizational email where the notion of power is well defined in terms of organizational hierarchy. It is also aided by the availability of the moderately large Enron email corpus which captures email interactions in an organizational setting. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line inter"
D14-1211,W11-1709,0,0.0897844,"and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily gender identifiable or not. If the person had an unfamiliar name or a name that could be of either gender, they marked his/her gender as unknown and excluded them from their study.1 For example, the gender of the employee Kay Mann was marked as unknown in their gender assignment. However, in our"
D14-1211,N13-1099,1,0.907928,"Missing"
D14-1211,W11-0711,0,0.48396,"Missing"
D14-1211,I13-1025,1,0.871684,"exploring language use and power. A large number of these studies are performed in the domain of organizational email where the notion of power is well defined in terms of organizational hierarchy. It is also aided by the availability of the moderately large Enron email corpus which captures email interactions in an organizational setting. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender."
D14-1211,P14-2056,1,0.526846,"these studies are performed in the domain of organizational email where the notion of power is well defined in terms of organizational hierarchy. It is also aided by the availability of the moderately large Enron email corpus which captures email interactions in an organizational setting. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender aff"
D14-1211,N12-1057,1,0.918011,"Missing"
D14-1211,I13-1042,1,0.895058,"Missing"
D14-1211,W14-2710,1,0.700979,"inguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily gender identifiable or not. If the person had an unfamiliar name or a name that could be of either gender, they marked his/her gender as unknown and excluded them from their st"
D14-1211,C12-1155,0,0.0614795,"al features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily gender identifiable or not. If the person had a"
D14-1211,P12-2032,1,\N,Missing
D19-1578,W19-3621,0,0.0396662,"s a result of name perturbation. It is also interesting to note that despite the negative correlation between |f (xn ) − f (x) |and f (x), the LabelDist has high values at high thresholds. Figure 2: Even for high model thresholds, we see significant 4 name perturbation sensitivity in classifications/labels. LabelDist measures the # of flips between toxic and non-toxic. Related Work Fairness research in NLP has seen tremendous growth in the past few years (e.g., (Bolukbasi et al., 2016; Caliskan et al., 2017; Webster et al., 2018; D´ıaz et al., 2018; Dixon et al., 2018; DeArteaga et al., 2019; Gonen and Goldberg, 2019; Manzini et al., 2019)) over a range of NLP tasks such as co-reference resolution and machine translation, as well as the tasks we studied — sentiment analysis and toxicity prediction. Some of this work study bias by swapping names in sentence templates (Caliskan et al., 2017; Kiritchenko and Mohammad, 2018; May et al., 2019; Gonen and Goldberg, 2019); however they use synthetic sentence templates, while we extract naturally occurring sentences from the target corpus. Our work is closely related to counter-factual token fairness (Garg et al., 2019), which measures the magnitude of model predi"
D19-1578,S18-2005,0,0.0378912,"bels. LabelDist measures the # of flips between toxic and non-toxic. Related Work Fairness research in NLP has seen tremendous growth in the past few years (e.g., (Bolukbasi et al., 2016; Caliskan et al., 2017; Webster et al., 2018; D´ıaz et al., 2018; Dixon et al., 2018; DeArteaga et al., 2019; Gonen and Goldberg, 2019; Manzini et al., 2019)) over a range of NLP tasks such as co-reference resolution and machine translation, as well as the tasks we studied — sentiment analysis and toxicity prediction. Some of this work study bias by swapping names in sentence templates (Caliskan et al., 2017; Kiritchenko and Mohammad, 2018; May et al., 2019; Gonen and Goldberg, 2019); however they use synthetic sentence templates, while we extract naturally occurring sentences from the target corpus. Our work is closely related to counter-factual token fairness (Garg et al., 2019), which measures the magnitude of model prediction change when identity terms (such as gay, lesbian, transgender etc.) referenced in naturally occurring sentences are perturbed. Additionally, De-Arteaga et al. (2019) study gender bias in occupation classification using names in online biographies. In contrast, we propose a general framework to study bi"
D19-1578,N19-1062,0,0.0413341,"ation. It is also interesting to note that despite the negative correlation between |f (xn ) − f (x) |and f (x), the LabelDist has high values at high thresholds. Figure 2: Even for high model thresholds, we see significant 4 name perturbation sensitivity in classifications/labels. LabelDist measures the # of flips between toxic and non-toxic. Related Work Fairness research in NLP has seen tremendous growth in the past few years (e.g., (Bolukbasi et al., 2016; Caliskan et al., 2017; Webster et al., 2018; D´ıaz et al., 2018; Dixon et al., 2018; DeArteaga et al., 2019; Gonen and Goldberg, 2019; Manzini et al., 2019)) over a range of NLP tasks such as co-reference resolution and machine translation, as well as the tasks we studied — sentiment analysis and toxicity prediction. Some of this work study bias by swapping names in sentence templates (Caliskan et al., 2017; Kiritchenko and Mohammad, 2018; May et al., 2019; Gonen and Goldberg, 2019); however they use synthetic sentence templates, while we extract naturally occurring sentences from the target corpus. Our work is closely related to counter-factual token fairness (Garg et al., 2019), which measures the magnitude of model prediction change when ident"
D19-1578,N19-1063,0,0.0344309,"f flips between toxic and non-toxic. Related Work Fairness research in NLP has seen tremendous growth in the past few years (e.g., (Bolukbasi et al., 2016; Caliskan et al., 2017; Webster et al., 2018; D´ıaz et al., 2018; Dixon et al., 2018; DeArteaga et al., 2019; Gonen and Goldberg, 2019; Manzini et al., 2019)) over a range of NLP tasks such as co-reference resolution and machine translation, as well as the tasks we studied — sentiment analysis and toxicity prediction. Some of this work study bias by swapping names in sentence templates (Caliskan et al., 2017; Kiritchenko and Mohammad, 2018; May et al., 2019; Gonen and Goldberg, 2019); however they use synthetic sentence templates, while we extract naturally occurring sentences from the target corpus. Our work is closely related to counter-factual token fairness (Garg et al., 2019), which measures the magnitude of model prediction change when identity terms (such as gay, lesbian, transgender etc.) referenced in naturally occurring sentences are perturbed. Additionally, De-Arteaga et al. (2019) study gender bias in occupation classification using names in online biographies. In contrast, we propose a general framework to study biases with named en"
D19-1578,L18-1445,1,0.787803,"classification models, applied to 4 different corpora, to detect biases associated with person names. Models: We use two text classification models: a) a toxicity model that returns a score between [0,1], and b) a sentiment model that returns a score between [-1,+1]. Both models were trained using state-of-the-art neural network algorithms, and perform competitively on benchmark tests.2 2 To obtain information about the models, for instance to perform replication experiments, please contact the authors. Corpora: We use four socially and topically diverse corpora of online comments released by Voigt et al. (2018): Facebook comments on politicians’ posts (FB-Pol.) and on public figures’ posts (FB-Pub.), Reddit comments, and comments in Fitocracy forums. For each corpus, we select 1000 comments at random that satisfy two criteria: at most 50 words in length, and contain at least one English 3rd person singular pronouns (i.e., anchors). We use these extracted comments to build templates, where the pronouns can be replaced with a person name to form a grammatically coherent perturbed sentence. We use pronouns as the anchors for multiple reasons. Pronouns are often closed-class words across languages,3 mak"
D19-1578,Q18-1042,0,0.0747665,"an individual can be categorized into legally and socially sensitive roles. The first runs into the ethical concerns of surveillance; the second runs into the ethical concerns of discrimination. Furthermore, texts are often not annotated with the social groups of their readers/writers (and for privacy reasons we may not wish to infer them), or whether two individuals are “similar” or not. Hence, fairness research in NLP has mostly focused on mentions of social identities (Dixon et al., 2018; Borkan et al., 2019; Garg et al., 2019), or on how social stereotypes impact semantic interpretation (Webster et al., 2018), and often rely heavily on annotated corpora. 5740 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5740–5745, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics In this paper, we propose a general-purpose evaluation framework that detects unintended biases in NLP models around named entities mentioned in text. Our method does not rely on any annotated corpora, and we focus solely on application-independent sensitivity of models, which does"
E17-1017,N16-1165,1,0.380564,"sive? When is it reasonable? We subsume such questions under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if emotionally affected by some legal case at hand. Or, they might not be persuaded that the stated"
E17-1017,W16-2808,0,0.057931,") Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment sy"
E17-1017,P16-2085,1,0.841124,"Naderi Yufang Hou University of Toronto IBM Research Toronto, Canada Dublin, Ireland nona@cs.toronto.edu yhou@ie.ibm.com Yonatan Bilu IBM Research Haifa, Israel yonatanb@il.ibm.com Vinodkumar Prabhakaran Tim Alberdingk Thijm, Graeme Hirst Benno Stein Stanford University University of Toronto Bauhaus-Universität Weimar Stanford, CA, USA Toronto, Canada Weimar, Germany vinod@cs.stanford.edu {thijm, gh}@cs.toronto.edu Abstract sify argumentation schemes (Feng et al., 2014) and frames (Naderi and Hirst, 2015), analyze overall argumentation structures (Wachsmuth et al., 2015), or generate claims (Bilu and Slonim, 2016). Also, theories of argumentation quality exist, and some quality dimensions have been assessed computationally (see Section 2 for details). Until now, however, the assertion of O’Keefe and Jackson (1995) that there is neither a general idea of what constitutes argumentation quality in natural language nor a clear definition of its dimensions still holds. The reasons for this deficit originate in the varying goals of argumentation: persuading audiences, resolving disputes, achieving agreement, completing inquiries, and recommending actions (Tindale, 2007). As a result, diverse quality dimensio"
E17-1017,W15-0514,0,0.0600702,"Missing"
E17-1017,W98-0303,0,0.311573,"nal appeal Persuasiveness Tan et al. (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argument"
E17-1017,P12-2041,0,0.119946,"Missing"
E17-1017,P11-1099,1,0.393064,"Missing"
E17-1017,C14-1089,1,0.513151,"tation Quality Assessment in Natural Language Henning Wachsmuth Bauhaus-Universität Weimar Weimar, Germany henning.wachsmuth@uni-weimar.de Nona Naderi Yufang Hou University of Toronto IBM Research Toronto, Canada Dublin, Ireland nona@cs.toronto.edu yhou@ie.ibm.com Yonatan Bilu IBM Research Haifa, Israel yonatanb@il.ibm.com Vinodkumar Prabhakaran Tim Alberdingk Thijm, Graeme Hirst Benno Stein Stanford University University of Toronto Bauhaus-Universität Weimar Stanford, CA, USA Toronto, Canada Weimar, Germany vinod@cs.stanford.edu {thijm, gh}@cs.toronto.edu Abstract sify argumentation schemes (Feng et al., 2014) and frames (Naderi and Hirst, 2015), analyze overall argumentation structures (Wachsmuth et al., 2015), or generate claims (Bilu and Slonim, 2016). Also, theories of argumentation quality exist, and some quality dimensions have been assessed computationally (see Section 2 for details). Until now, however, the assertion of O’Keefe and Jackson (1995) that there is neither a general idea of what constitutes argumentation quality in natural language nor a clear definition of its dimensions still holds. The reasons for this deficit originate in the varying goals of argumentation: persuading audien"
E17-1017,P16-2089,0,0.0173404,"al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment systematically, thus or"
E17-1017,P16-1150,0,0.41004,"right side of Figure 1 show where the approaches surveyed in Section 2.2 are positioned in the taxonomy. Some dimensions have been tackled multiple times (e.g., clarity), others not at all (e.g., credibility). The taxonomy indicates what sub-dimensions will affect the same high-level dimension. 4 The Dagstuhl-15512 ArgQuality Corpus Finally, we present our new annotated Dagstuhl15512 ArgQuality Corpus for studying argumentation quality based on the developed taxonomy, and we report on a first corpus analysis.3 4.1 Data and Annotation Process Our corpus is based on the UKPConvArgRank dataset (Habernal and Gurevych, 2016), which contains rankings of 25 to 35 textual debate portal arguments for two stances on 16 issues, such as evolution vs. creation and ban plastic water bottles. All ranks were derived from crowdsourced convincingness labels. For every issue/stance pair, we took the five top-ranked texts and chose five further via stratified sampling. Thereby, we covered both high-quality arguments and different levels of lower quality. Two example texts follow below in Figure 2. Before annotating the 320 chosen texts, we carried out a full annotation study with seven authors of this paper on 20 argumentative"
E17-1017,W14-2104,0,0.0646839,". (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy d"
E17-1017,P11-1032,0,0.00976501,"ween the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment systematically, thus organizing and clarifying the roles of practical approaches. It does not require a particular argumentation model, but it rests on the notion of the granularity levels from Section 1. 3.1 Overview of the Theory-based Taxonomy Our ob"
E17-1017,D15-1110,0,0.0252984,"riven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if emotionally affected by some legal case at hand. Or, they might not be persuaded that the stated argument is the most relevant in the debate on death penalty. This example reveals three central challeng"
E17-1017,P13-1026,0,0.362263,"Missing"
E17-1017,P14-1144,0,0.0803542,"Missing"
E17-1017,P15-1053,0,0.125852,"Missing"
E17-1017,D10-1023,0,0.426521,"Missing"
E17-1017,D08-1020,0,0.0224306,"brio and Villata (2012) Local relevance Cogency Rahimi et al. (2014) Acceptability Persing et al. (2015) Emotional appeal Persuasiveness Tan et al. (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of"
E17-1017,W15-0603,0,0.0749477,"ncy, following the definition of Johnson and Blair (2006). Their experiments suggest that convolutional neural networks outperform feature-based sufficiency classification. Rhetoric Persing et al. (2010) tackle the proper arrangement of an essay, namely, its organization in terms of the logical development of an argument. The authors rely on manual 7-point score annotations for 1003 essays from the ICLE corpus (Granger et al., 2009). In their experiments, sequences of paragraph discourse functions (e.g., introduction or rebuttal) turn out to be most effective. Organization is also analyzed by Rahimi et al. (2015) on the same dataset used for the evidence 179 Aspect Quality Dimension Granularity Text Genres Sources Evidence Level of support Sufficiency Argumentation Argument unit Argument Student essays Wikipedia articles Student essays Rahimi et al. (2014) Braunstain et al. (2016) Stab and Gurevych (2017) Rhetoric Argument strength Evaluability Global coherence Organization Persuasiveness Prompt adherence Thesis clarity Winning side Argumentation Argumentation Argumentation Argumentation Argument Argumentation Argumentation Debate Student essays Law comments Student essays Student essays Forum discuss"
E17-1017,D15-1050,0,0.330444,"s under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if emotionally affected by some legal case at hand. Or, they might not be persuaded that the stated argument is the most relevant in the debate on de"
E17-1017,W14-2110,0,0.0505174,"al et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also, only few Wikipedia quality flaws relate to arguments, e.g., verifiability (Anderka et al., 2012). 3 A Taxonomy of Argumentation Quality Given all surveyed quality dimensions, we now propose a unifying taxonomy of argumentation quality. The taxonomy decomposes quality assessment systematically, thus organizing and clarifying the role"
E17-1017,D14-1006,0,0.0178285,"essment. 1 benno.stein@uni-weimar.de Introduction What is a good argument? What premises should it be based on? When is argumentation persuasive? When is it reasonable? We subsume such questions under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the f"
E17-1017,W16-2813,0,0.364991,"Missing"
E17-1017,E17-1092,0,0.242722,"al models and argument-oriented features, they rank sentencelevel argument units according to the level of support they provide for an answer. Unlike classical essay scoring, Rahimi et al. (2014) score an essay’s evidence, a quality dimension of argumentation: it captures how sufficiently the given details support the essay’s thesis. On the dataset of Correnti et al. (2013) with 1569 student essays and scores from 1 to 4, they find that the concentration and specificity of words related to the essay prompt (i.e., the statement defining the discussed issue) impacts scoring accuracy. Similarly, Stab and Gurevych (2017) introduce an essay corpus with 1029 argument-level annotations of sufficiency, following the definition of Johnson and Blair (2006). Their experiments suggest that convolutional neural networks outperform feature-based sufficiency classification. Rhetoric Persing et al. (2010) tackle the proper arrangement of an essay, namely, its organization in terms of the logical development of an argument. The authors rely on manual 7-point score annotations for 1003 essays from the ICLE corpus (Granger et al., 2009). In their experiments, sequences of paragraph discourse functions (e.g., introduction or"
E17-1017,W15-4631,0,0.0408966,"ltužic´ and Šnajder (2015) Relevance Reasonableness le Sufficiency Local sufficiency Prominence ia Evidence Cabrio and Villata (2012) Local relevance Cogency Rahimi et al. (2014) Acceptability Persing et al. (2015) Emotional appeal Persuasiveness Tan et al. (2016), Wei et al. (2016) Winning side Zhang et al. (2016) Convincingness Habernal et al. (2016) Figure 1: The proposed taxonomy of argumentation quality as well as the mapping of existing assessment approaches to the covered quality dimensions. Arrows show main dependencies between the dimensions. even if said to aim for argument quality (Swanson et al., 2015). Much work exists for general text quality, most notably in the context of readability (Pitler and Nenkova, 2008) and classical essay scoring. Some scoring approaches derive features from discourse (Burstein et al., 1998), arguments (Ong et al., 2014; Beigman Klebanov et al., 2016; Ghosh et al., 2016), or schemes (Song et al., 2014)—all this may be indicative of quality. However, our focus is approaches that target argumentation quality at heart. Similarly, review helpfulness (Liu et al., 2008) and deception (Ott et al., 2011) are not treated, as arguments only partly play a role there. Also,"
E17-1017,D15-1072,1,0.846424,"Germany henning.wachsmuth@uni-weimar.de Nona Naderi Yufang Hou University of Toronto IBM Research Toronto, Canada Dublin, Ireland nona@cs.toronto.edu yhou@ie.ibm.com Yonatan Bilu IBM Research Haifa, Israel yonatanb@il.ibm.com Vinodkumar Prabhakaran Tim Alberdingk Thijm, Graeme Hirst Benno Stein Stanford University University of Toronto Bauhaus-Universität Weimar Stanford, CA, USA Toronto, Canada Weimar, Germany vinod@cs.stanford.edu {thijm, gh}@cs.toronto.edu Abstract sify argumentation schemes (Feng et al., 2014) and frames (Naderi and Hirst, 2015), analyze overall argumentation structures (Wachsmuth et al., 2015), or generate claims (Bilu and Slonim, 2016). Also, theories of argumentation quality exist, and some quality dimensions have been assessed computationally (see Section 2 for details). Until now, however, the assertion of O’Keefe and Jackson (1995) that there is neither a general idea of what constitutes argumentation quality in natural language nor a clear definition of its dimensions still holds. The reasons for this deficit originate in the varying goals of argumentation: persuading audiences, resolving disputes, achieving agreement, completing inquiries, and recommending actions (Tindale,"
E17-1017,C16-1158,1,0.644819,"Missing"
E17-1017,E17-1105,1,0.83682,"ion What is a good argument? What premises should it be based on? When is argumentation persuasive? When is it reasonable? We subsume such questions under the term argumentation quality; they have driven logicians, rhetoricians, linguists, and argumentation theorists since the Ancient Greeks (Aristotle, 2007). Now that the area of computational argumentation is seeing an influx of research activity, the automatic assessment of argumentation quality is coming into the focus, due to its importance for envisioned applications such as writing support (Stab and Gurevych, 2014) and argument search (Wachsmuth et al., 2017), among others. Existing research covers the mining of argument units (Al-Khatib et al., 2016), specific types of evidence (Rinott et al., 2015), and argumentative relations (Peldszus and Stede, 2015). Other works clasEveryone has an inalienable human right to life, even those who commit murder; sentencing a person to death and executing them violates that right. Although implicit, the conclusion about the death penalty seems sound in terms of (informal) logic, and the argument is clear from a linguistic viewpoint. Some people might not accept the first stated premise, though, especially if em"
E17-1017,P16-2032,0,0.105377,"Missing"
E17-1017,N16-1017,0,0.0362025,"Missing"
I13-1025,P12-2032,1,0.628694,"enable us to perform the analysis of how power affects dialog behavior. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. There are about 8.5 participants per thread. There are 221 active participants (participants of a thread who has sent at least one email message in the thread) in the corpus. Table 1 presents the counts and percentages of active participants with each type of power in the corpus. We now define the four types of power we investigate in this paper. Hierarchical Power (HP): We use the gold organizational hierarchy for Enron released by Agarwal et al. (2012) to model hierarchical power. It contains relations between 1,518 employees, and 1 The manual annotations also capture the perception of hierarchical power. In this work, we use only the actual gold hierarchy (Agarwal et al., 2012) as described above. 2 In (Prabhakaran et al., 2012a), power over communication was called “control of communication”. 218 From: Kathryn Cordes To: Leslie Hansen, Sara Shackleton, Brent Hendry CC: Mark Greenberg, Erik Eller, Thomas D Gros ———————————————– M1.1. Leslie Sara, and Brent: [Conventional] our annotator judged Mark to have influence over Brent since the lat"
I13-1025,W12-2105,1,0.471828,"nce the real identities of the members of such communities are often not revealed and their hierarchies may not be available to the law enforcement agencies. The power differential between the DPs may be based on a multitude of factors such as status, authority, role, knowledge and so on. Early computational approaches to analyzing power in interactions relied solely on static power structures such as corporate hierarchies as the source of the power differential (Rowe et al., 2007; Bramsen et al., 2011). More recent studies have looked into dynamic notions of power as well, such as influence (Biran et al., 2012). However, not much work has been done to understand how different types of power differ in the ways they affect how people interact in dialog. In this paper, we study four different types of power — hierarchical power, situational power, influence and power over communication. We investigate whether all four social power relations are manifested in dialog behavior; we restrict our attention to written dialog, specifically email exchanged in an American corporation. By “dialog behavior”, we mean the choices a DP makes while engaging in dialog. Dialog behavior includes choices that affect dialo"
I13-1025,P11-1078,0,0.181285,"rcement agencies to detect leaders and influencers in suspicious online communities. This is especially useful since the real identities of the members of such communities are often not revealed and their hierarchies may not be available to the law enforcement agencies. The power differential between the DPs may be based on a multitude of factors such as status, authority, role, knowledge and so on. Early computational approaches to analyzing power in interactions relied solely on static power structures such as corporate hierarchies as the source of the power differential (Rowe et al., 2007; Bramsen et al., 2011). More recent studies have looked into dynamic notions of power as well, such as influence (Biran et al., 2012). However, not much work has been done to understand how different types of power differ in the ways they affect how people interact in dialog. In this paper, we study four different types of power — hierarchical power, situational power, influence and power over communication. We investigate whether all four social power relations are manifested in dialog behavior; we restrict our attention to written dialog, specifically email exchanged in an American corporation. By “dialog behavio"
I13-1025,prabhakaran-etal-2012-annotations,1,0.479405,"ocial relations from online communication. 217 Researchers have also applied NLP techniques on message content to detect power relations. Earlier approaches used simple lexical features (e.g. (Bramsen et al., 2011; Gilbert, 2012)) while later studies have performed deeper discourse analysis and used features such as linguistic coordination (Danescu-Niculescu-Mizil et al., 2012), language uses such as attempts to persuade and various other dialog patterns (Biran et al., 2012). We present a more detailed discussion of the above mentioned studies and how they differ from our line of research in (Prabhakaran et al., 2012c). Our research also falls into the category of studies that go beyond pure lexical features and use dialog structure based features to extract social power relations. In (Prabhakaran et al., 2012c), we studied the notion of situational power in depth and presented a system to detect persons with situational power using dialog features. In this paper as well, we use the system described in (Prabhakaran et al., 2012c). However, this work differs from (Prabhakaran et al., 2012c) and other studies described above in that our focus is on how different types of power are manifested differently in"
I13-1025,N12-1057,1,0.747258,"ocial relations from online communication. 217 Researchers have also applied NLP techniques on message content to detect power relations. Earlier approaches used simple lexical features (e.g. (Bramsen et al., 2011; Gilbert, 2012)) while later studies have performed deeper discourse analysis and used features such as linguistic coordination (Danescu-Niculescu-Mizil et al., 2012), language uses such as attempts to persuade and various other dialog patterns (Biran et al., 2012). We present a more detailed discussion of the above mentioned studies and how they differ from our line of research in (Prabhakaran et al., 2012c). Our research also falls into the category of studies that go beyond pure lexical features and use dialog structure based features to extract social power relations. In (Prabhakaran et al., 2012c), we studied the notion of situational power in depth and presented a system to detect persons with situational power using dialog features. In this paper as well, we use the system described in (Prabhakaran et al., 2012c). However, this work differs from (Prabhakaran et al., 2012c) and other studies described above in that our focus is on how different types of power are manifested differently in"
I13-1025,C12-1138,1,0.459973,"ocial relations from online communication. 217 Researchers have also applied NLP techniques on message content to detect power relations. Earlier approaches used simple lexical features (e.g. (Bramsen et al., 2011; Gilbert, 2012)) while later studies have performed deeper discourse analysis and used features such as linguistic coordination (Danescu-Niculescu-Mizil et al., 2012), language uses such as attempts to persuade and various other dialog patterns (Biran et al., 2012). We present a more detailed discussion of the above mentioned studies and how they differ from our line of research in (Prabhakaran et al., 2012c). Our research also falls into the category of studies that go beyond pure lexical features and use dialog structure based features to extract social power relations. In (Prabhakaran et al., 2012c), we studied the notion of situational power in depth and presented a system to detect persons with situational power using dialog features. In this paper as well, we use the system described in (Prabhakaran et al., 2012c). However, this work differs from (Prabhakaran et al., 2012c) and other studies described above in that our focus is on how different types of power are manifested differently in"
I13-1025,W09-3953,1,0.924365,"t because Kathryn is following up on and assigning a task to others, and because Kathryn uses language that shows that she is in charge of the situation. Power over Communication (PC): A person is said to have power over communication if he actively attempts to achieve the intended goals of the communication.2 These are people who ask questions, request others to take action, etc., and not Data and Annotations We use the subset of the Enron email corpus with power annotations presented in Prabhakaran et al. (2012a) for our experiments. The corpus also contains manual dialog act annotations by Hu et al. (2009), which enable us to perform the analysis of how power affects dialog behavior. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. There are about 8.5 participants per thread. There are 221 active participants (participants of a thread who has sent at least one email message in the thread) in the corpus. Table 1 presents the counts and percentages of active participants with each type of power in the corpus. We now define the four types of power we investigate in this paper. Hierarchical Power (HP): We use the gold organizational hierarchy for Enron rele"
I13-1025,P90-1010,0,0.857359,"ion 2, we discuss related work in the field. Section 3-4 presents the data, annotations, and inter-rater agreement studies on the annotations. Section 5 summarizes the dimensions of interactions we analyze. We then present the main contributions of this paper: Section 6 analyzes the variations in the manifestations of power among the four types, and Section 7 describes a system to predict persons with any of the four types of power. We then conclude and discuss future work. 2 Related Work Within the dialog community, researchers have studied notions of control and initiative in dialogs (e.g. (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997)). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information. They use utterance level rules to determine which discourse participant (whether the speaker or the hearer) is in control, and extend it to segments of discourse. Their notion of control differs from our notion of power over communication. They model control locally over discourse segments. What we are interested in (and what our annotations capture) is the possession of controlling power by one (or more) parti"
I13-1025,C00-2137,0,0.119915,"Missing"
I13-1042,P12-1009,0,0.0789375,"udies model static hierarchical relationships; our work models a dynamic notion of power in interactions happening outside organizational boundaries. Also, the studies described above consider messages or collections of messages in isolation, but not in the context of the entire interaction. Background and Related Work Social power and how it affects the ways people behave in interactions have been studied extensively in social sciences and psychology. Bales 366 context of 2004 Democratic presidential primary election and identify lexical and prosodic cues that signal charisma. More recently, Nguyen et al. (2012) analyze 2008 presidential and vice presidential debates to study how speaker identification helps topic segmentation and how candidates exercise control over conversations by shifting topics. While our domain is also presidential debates, our focus is on how the candidate’s power or confidence affects interactions within the debates. More recently, a deeper analysis of interactions is shown to be useful in detecting power or influence in interactions. Danescu-NiculescuMizil et al. (2012) focus on the notion of language coordination — a metric that measures the extent to which a discourse part"
I13-1042,W12-2105,0,0.0813216,"sures the extent to which a discourse participant adopts another’s language — in relation to various social attributes such as power, gender, etc. They perform their study on Wikipedia discussion forums and Supreme Court hearings — both of which have enforced power structures. Prabhakaran et al. (2012a) analyze the notion of overt displays of power (ODP) in dialog. Prabhakaran et al. (2012b) and Prabhakaran and Rambow (2013) study how the ODP and other dialog act analysis based features of organizational email interactions correlate with different types of power possessed by the participants. Biran et al. (2012) and Bracewell et al. (2012) use lower-level dialog constructs to model power relations. Biran et al. (2012) use dialog constructs such as attempts to persuade, agreement, disagreement and various dialog patterns in order to find influencers in Wikipedia discussion forums and LiveJournal blogs. Bracewell et al. (2012) try to identify participants pursuing power in discussion forums. They devise a set of eight social acts which largely overlaps with the dialog constructs used by (Biran et al., 2012). 3 Domain: Political Debates Before the United States presidential election, a series of preside"
I13-1042,I13-1025,1,0.80303,"per analysis of interactions is shown to be useful in detecting power or influence in interactions. Danescu-NiculescuMizil et al. (2012) focus on the notion of language coordination — a metric that measures the extent to which a discourse participant adopts another’s language — in relation to various social attributes such as power, gender, etc. They perform their study on Wikipedia discussion forums and Supreme Court hearings — both of which have enforced power structures. Prabhakaran et al. (2012a) analyze the notion of overt displays of power (ODP) in dialog. Prabhakaran et al. (2012b) and Prabhakaran and Rambow (2013) study how the ODP and other dialog act analysis based features of organizational email interactions correlate with different types of power possessed by the participants. Biran et al. (2012) and Bracewell et al. (2012) use lower-level dialog constructs to model power relations. Biran et al. (2012) use dialog constructs such as attempts to persuade, agreement, disagreement and various dialog patterns in order to find influencers in Wikipedia discussion forums and LiveJournal blogs. Bracewell et al. (2012) try to identify participants pursuing power in discussion forums. They devise a set of ei"
I13-1042,N12-1057,1,0.836643,"es, our focus is on how the candidate’s power or confidence affects interactions within the debates. More recently, a deeper analysis of interactions is shown to be useful in detecting power or influence in interactions. Danescu-NiculescuMizil et al. (2012) focus on the notion of language coordination — a metric that measures the extent to which a discourse participant adopts another’s language — in relation to various social attributes such as power, gender, etc. They perform their study on Wikipedia discussion forums and Supreme Court hearings — both of which have enforced power structures. Prabhakaran et al. (2012a) analyze the notion of overt displays of power (ODP) in dialog. Prabhakaran et al. (2012b) and Prabhakaran and Rambow (2013) study how the ODP and other dialog act analysis based features of organizational email interactions correlate with different types of power possessed by the participants. Biran et al. (2012) and Bracewell et al. (2012) use lower-level dialog constructs to model power relations. Biran et al. (2012) use dialog constructs such as attempts to persuade, agreement, disagreement and various dialog patterns in order to find influencers in Wikipedia discussion forums and LiveJo"
I13-1042,C12-1138,1,0.824914,"es, our focus is on how the candidate’s power or confidence affects interactions within the debates. More recently, a deeper analysis of interactions is shown to be useful in detecting power or influence in interactions. Danescu-NiculescuMizil et al. (2012) focus on the notion of language coordination — a metric that measures the extent to which a discourse participant adopts another’s language — in relation to various social attributes such as power, gender, etc. They perform their study on Wikipedia discussion forums and Supreme Court hearings — both of which have enforced power structures. Prabhakaran et al. (2012a) analyze the notion of overt displays of power (ODP) in dialog. Prabhakaran et al. (2012b) and Prabhakaran and Rambow (2013) study how the ODP and other dialog act analysis based features of organizational email interactions correlate with different types of power possessed by the participants. Biran et al. (2012) and Bracewell et al. (2012) use lower-level dialog constructs to model power relations. Biran et al. (2012) use dialog constructs such as attempts to persuade, agreement, disagreement and various dialog patterns in order to find influencers in Wikipedia discussion forums and LiveJo"
I13-1042,P11-1078,0,0.310262,"is search to posts authored by interactants with higher power. Power analysis may also aid intelligence agencies to detect leaders and influencers in suspicious online communities. This is especially useful since the real identities of the members of such communities are often not revealed and the hierarchies of such communities may not be available to the intelligence agencies. Most computational efforts to analyze or predict power differentials between participants of interactions have relied on static power structures or hierarchies as sources for the power differential (Rowe et al., 2007; Bramsen et al., 2011; Gilbert, 2012). However, many interactions happen outside the context of a pre-defined static power structure or hierarchy. Examples for such interactions In this paper, we present an automatic system to rank participants of an interaction in terms of their relative power. We find several linguistic and structural features to be effective in predicting these rankings. We conduct our study in the domain of political debates, specifically the 2012 Republican presidential primary debates. Our dataset includes textual transcripts of 20 debates with 4-9 candidates as participants per debate. We m"
I13-1042,1996.amta-1.36,0,0.479856,"r than operating within a static power structure. We found statistically significant high positive correlation between the power indices of candidates and how often they were referenced/mentioned by others (MP). In other words, as candidates gain more power, they are referenced significantly more by others. However, the distribution of mentions of a candidate across different forms of addressing (FN, LN, TN, FLN) did not have any correlation with the power indices of the candidate. This suggests that while forms of addressing is found to be correlated with power relations by previous studies (Brown and Ford, 1961; Dickey, 1997), they are not affected by the short term variations of power as in our domain. Correlation Analysis and Significance Figure 4 shows the Pearson’s product correlation between each structural feature and candidate’s power index P (X). The darker bars denote statistically significant (p &lt; 0.05) correlations. Applying Bonferroni correction for multiple tests, the threshold for p-value for significance would be reduced to 0.0025. Even then, the statistically significant features would retain their significance. We consider three correlation windows — weak (0.2 - 0.39), moderate (0.4"
I13-1042,2007.sigdial-1.5,0,0.0792212,"Missing"
L16-1322,W11-0707,0,0.562174,"ed Work There is a wide array of computational studies analyzing the dynamics of the collaborative editing process of building Wikipedia. One line of work focuses mainly on meta information such as history of edits, deletes, reverts, and dispute tags (e.g., (Vuong et al., 2008; Rad and Barbosa, 2012; Jurgens and Lu, 2012)), whereas others analyze the interaction dynamics exhibited by the editors in the Wikipedia Talk pages. At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for the research questions they investigate, wher"
L16-1322,W12-2105,1,0.838066,"hibited by the editors in the Wikipedia Talk pages. At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for the research questions they investigate, whereas we present a large general purpose corpus that captures broader aspects of interactions and their participants. 3. Corpus In this section, we present our WikiTalk corpus, describe its construction process, and discuss the format in which the discussions are represented in it. 3.1. Data source: Discussion Threads Our starting point is the list of controversial issues i"
L16-1322,E12-1079,0,0.0223564,"regarding online social interactions. 2. Related Work There is a wide array of computational studies analyzing the dynamics of the collaborative editing process of building Wikipedia. One line of work focuses mainly on meta information such as history of edits, deletes, reverts, and dispute tags (e.g., (Vuong et al., 2008; Rad and Barbosa, 2012; Jurgens and Lu, 2012)), whereas others analyze the interaction dynamics exhibited by the editors in the Wikipedia Talk pages. At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for"
L16-1322,P13-1162,0,0.0198836,"e of work focuses mainly on meta information such as history of edits, deletes, reverts, and dispute tags (e.g., (Vuong et al., 2008; Rad and Barbosa, 2012; Jurgens and Lu, 2012)), whereas others analyze the interaction dynamics exhibited by the editors in the Wikipedia Talk pages. At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for the research questions they investigate, whereas we present a large general purpose corpus that captures broader aspects of interactions and their participants. 3. Corpus In this section, we p"
L16-1322,C12-1155,0,0.030014,". At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for the research questions they investigate, whereas we present a large general purpose corpus that captures broader aspects of interactions and their participants. 3. Corpus In this section, we present our WikiTalk corpus, describe its construction process, and discuss the format in which the discussions are represented in it. 3.1. Data source: Discussion Threads Our starting point is the list of controversial issues in Wikipedia that is collaboratively compiled by Wikipedia"
L16-1322,W14-2617,0,0.216954,"of the collaborative editing process of building Wikipedia. One line of work focuses mainly on meta information such as history of edits, deletes, reverts, and dispute tags (e.g., (Vuong et al., 2008; Rad and Barbosa, 2012; Jurgens and Lu, 2012)), whereas others analyze the interaction dynamics exhibited by the editors in the Wikipedia Talk pages. At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for the research questions they investigate, whereas we present a large general purpose corpus that captures broader aspects of"
L16-1322,P14-2113,0,0.239322,"of the collaborative editing process of building Wikipedia. One line of work focuses mainly on meta information such as history of edits, deletes, reverts, and dispute tags (e.g., (Vuong et al., 2008; Rad and Barbosa, 2012; Jurgens and Lu, 2012)), whereas others analyze the interaction dynamics exhibited by the editors in the Wikipedia Talk pages. At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for the research questions they investigate, whereas we present a large general purpose corpus that captures broader aspects of"
L18-1445,N15-1084,0,0.0182101,"social issues. For instance, women1 journalists reach a smaller audience in terms of social media impressions (Matias and Wallach, 2012), and traditional gender stereotypes and unbalanced gender representation occur even in contemporary stories and movies (Fast et al., 2016; Sap et al., 2017). Large datasets are particularly of use in this context due to the complex nature of differential responses to gender. However, previous computational work on language and gender has focused mainly on language about or portraying persons of a particular gender (Wagner et al., 2015; Flekova et al., 2016; Agarwal et al., 2015). We thus present a large multi-genre dataset of online communication to enable research in a category of gender difference understudied in computational work: responses to gender in language. These include posts and talks labeled for the gender of the source,2 along with comments given in response to the source texts. We collect such data from a variety of contexts, including: • Facebook (Politicians): Responses to Facebook posts from members of the U.S. House and Senate • Facebook (Public Figures): Responses to Facebook posts from other public figures, e.g., television hosts, journalists, an"
L18-1445,P06-1005,0,0.0356371,"tion are all public; however, to protect the anonymity of Facebook users in our dataset we remove all identifying user information as well as Facebook-internal information such as User IDs and Post IDs, replacing these with randomized ID numbers. Therefore users whose comments appear multiple times in our dataset may be compared, but without revealing their identity. We also only report commenter first names, since this is less identifying but still allows for running genderidentification algorithms. As a baseline for convenience we provide masculine/feminine ratios for these first names from Bergsma and Lin (2006). We collect posts and their associated top-level comments for the categories of speakers described below. In each case we find the page for the speaker with a novel method for finding gender-labeled speakers from Wikipedia. Specifically, our method takes as input a Wikipedia category page such as https://en.wikipedia.org/wiki/ Category:American_female_tennis_players, and for each name listed runs a search for public pages using Facebook’s Graph API. If an exact match for the name appears in the top three results, and the category of the page matches a relevant category (for instance, ”Public"
L18-1445,P05-1054,0,0.231484,"ource and responder may know one another and have an ongoing interaction afterwards. 2. Responses to Gender Here we aim to encourage research on responses to gender. Contrasting with language about or portraying a given gender which address abstract representations of social categories, responses to gender are directed towards an individual person. We know that social characteristics of the addressee influence linguistic behavior; existing computational work has shown, for instance, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often"
L18-1445,W17-3001,0,0.0230235,"nce, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often not just about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 B ROADCAST P ERSONAL Dataset Facebook (Politicians) Facebook (Public Figures) TED Talks Fitocracy Reddit Source Individuals M: 306 W: 96 M: 41 W: 64 M: 1,071 W: 349 M: 52,432 W: 47,498 M: 19,010 W: 11,116 Source Text Count 399,037 117,811 1,671 318,535 1,453,512 Response Count 13,866,507 10,667,500 190,425 318,535 1,453,512 Response Wor"
L18-1445,P16-1080,0,0.0474796,"Missing"
L18-1445,W17-2902,0,0.116166,"about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 B ROADCAST P ERSONAL Dataset Facebook (Politicians) Facebook (Public Figures) TED Talks Fitocracy Reddit Source Individuals M: 306 W: 96 M: 41 W: 64 M: 1,071 W: 349 M: 52,432 W: 47,498 M: 19,010 W: 11,116 Source Text Count 399,037 117,811 1,671 318,535 1,453,512 Response Count 13,866,507 10,667,500 190,425 318,535 1,453,512 Response Word Count 376,114,950 123,753,913 15,549,984 6,606,087 44,537,612 Table 1: Basic statistics about the subcorpora within RtGender. Jha and Mamidi, 2017). Nevertheless, biased responses to social categories like gender can lead to marginalization (Sue, 2010) and negatively impact a person’s self-esteem and ability through mechanisms such as stereotype threat (Spencer et al., 1999). Perhaps most related to our work, Fu et al. (2016) analyze questions directed at men and women tennis players, finding that questions directed at men tend to be more about the game while questions directed at women are more likely to stray to topics about their appearance and off-court relationships. Tsou et al. (2014) similarly find comments on TED talks are more l"
L18-1445,D15-1130,0,0.0450648,"Missing"
L18-1445,D17-1247,0,0.0271873,"nstruction of identity and social categories like gender; social issues such as gender bias, in turn, often take form in language. Linguistic datasets have been used both to debunk gender-biased myths — for example, contrary to stereotype women are not actually more talkative than men (Mehl et al., 2007) — and to identify social issues. For instance, women1 journalists reach a smaller audience in terms of social media impressions (Matias and Wallach, 2012), and traditional gender stereotypes and unbalanced gender representation occur even in contemporary stories and movies (Fast et al., 2016; Sap et al., 2017). Large datasets are particularly of use in this context due to the complex nature of differential responses to gender. However, previous computational work on language and gender has focused mainly on language about or portraying persons of a particular gender (Wagner et al., 2015; Flekova et al., 2016; Agarwal et al., 2015). We thus present a large multi-genre dataset of online communication to enable research in a category of gender difference understudied in computational work: responses to gender in language. These include posts and talks labeled for the gender of the source,2 along with"
L18-1445,D13-1170,0,0.00495629,"Missing"
L18-1445,N12-1084,0,0.027848,"shown, for instance, that the gender of the interlocutor influences lexical choices of a speaker in spoken and written interactions (Boulis and Ostendorf, 2005; Jurgens et al., 2017; Prabhakaran and Rambow, 2017). Looking at responses to gender also allows us to consider the important social issue of gender bias. Since important forms of bias (e.g., dehumanization or treating a person as their social category) often happen at the level of individual responses, responses to gender are an understudied but critical lens for studying gender bias. The issue is related to that of abusive language (Xu et al., 2012; Clarke and Grieve, 2017), though often gender bias takes a less overt form than straightforward abuse. Social issues like gender bias are often not just about hostility but also behaviors such as stereotype-reinforcing benevolence (Eagly and Mladinic, 1989; Glick and Fiske, 1996; 2814 B ROADCAST P ERSONAL Dataset Facebook (Politicians) Facebook (Public Figures) TED Talks Fitocracy Reddit Source Individuals M: 306 W: 96 M: 41 W: 64 M: 1,071 W: 349 M: 52,432 W: 47,498 M: 19,010 W: 11,116 Source Text Count 399,037 117,811 1,671 318,535 1,453,512 Response Count 13,866,507 10,667,500 190,425 318,"
N12-1057,P11-1078,0,0.123661,"in language use (e.g., (O’Barr, 1982)). Locher (2004) recognizes “restriction of an interactant’s action-environment” (Wartenberg, 1990) as a key element by which exercise of power in interactions can be identified. Through ODP we capture this action-restriction at an utterance level. In the computational field, several studies have used Social Network Analysis (e.g., (Diesner and Carley, 2005)) for extracting social relations from online communication. Only recently have researchers started using NLP to analyze the content of messages to deduce social relations (e.g., (Diehl et al., 2007)). Bramsen et al. (2011) use knowledge of the actual organizational structure to create two sets of messages: messages sent from a superior to a subordinate, and vice versa. Their task is to determine the direction of power (since all their data, by construction of the corpus, has a power relationship). Their reported results cannot be directly compared with ours since their results are on classifying aggregations of messages as being to a superior or to a subordinate, whereas our results are on predicting whether a single utterance has an ODP or not. 518 2012 Conference of the North American Chapter of the Associati"
N12-1057,W09-3953,1,0.708218,"Missing"
N12-1057,prabhakaran-etal-2012-annotations,1,0.762636,"Missing"
N13-1099,P12-2032,1,0.62499,"re that the reader adopt a certain belief. It covers many different types of information that can be conveyed including answers to questions, beliefs (committed or not), attitudes, and elaborations on prior DAs. Data In this study, we use the email corpus presented in (Hu et al., 2009), which is manually annotated for DA tags. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006; Agarwal et al., 2012). Most emails are concerned with exchanging information, scheduling meetings, or solving problems, but there are also purely social emails. In a R EQUEST-I NFORMATION, the writer signals her desire that the reader perform a specific communicative act, namely that he provide information (either facts or opinion). A C ONVENTIONAL dialog act does not signal any specific communicative intention on the part of the writer, but rather it helps structure and thus facilitate the communication. Examples include greetings, introductions, expressions of gratitude, etc. 4 Dialog Act Tag R EQUEST-ACTION (R-"
N13-1099,W04-3240,0,0.523922,"Missing"
N13-1099,W09-3953,1,0.93083,"Missing"
N13-1099,D10-1084,0,0.110709,"nt our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a single DA to each segment (Hu et al., 2009). Ou"
N13-1099,W10-2923,0,0.049703,"nt our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a single DA to each segment (Hu et al., 2009). Ou"
N13-1099,N12-1057,1,0.870324,"ramework was built with the ClearTK toolkit (Ogren et al., 2008) with its wrapper for SVMLight (Joachims, 1999). The ClearTK wrapper internally shifts the prediction threshold based on posterior probabilistic scores calculated using the algorithm of Lin et al. (2007). We report results from 5-fold cross validation performed on the entire corpus. 4.2 Feature Engineering In developing our system, we classified our features into three categories: lexical, verbal and messagelevel. Lexical features consists of n-grams of words, n-grams of POS tags, mixed n-grams of closed class words and POS tags (Prabhakaran et al., 2012), as well as a small set of specialized features — StartPOS/Lemma (POS tag and lemma of the first word), LastPOS/Lemma (POS tag and lemma of the last word), MDCount (number of modal verbs in the DFU) and QuestionMark (is there a question mark in the DFU). We used the POS tags produced by the OpenNLP POS tagger. Verbal features capture the position and identity of the first verb in the DFU. Finally, message-level features capture aspects of the location of the DFU in the message and of the message in the thread (relative position and size). In optimizing each system, we first performed an exhau"
N13-1099,J00-3003,0,0.269331,"tion for the other classes and for the overall classifier. This paper is structured as follows. We start out by discussing related work (Section 2). We then present our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one anoth"
N13-1099,W12-0603,0,0.110434,"ith our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a single DA to each segment (Hu et al., 2009). Our work falls in this paradigm (we choose a single DA for smaller textual u"
N18-1096,P12-2032,1,0.838969,"(p1 , p2 ) in the thread, predict whether p1 is the superior or subordinate of p2 . In this formulation, a related interacting participant pair (RIPP) is a pair of participants of the thread such that there is at least one message exchanged within the thread between them (in either direction) and that they are hierarchically related with a superior/subordinate relation. 3.2 Data We use the same dataset we used in (Prabhakaran and Rambow, 2014), which is a version of the Enron email corpus in which the thread structure of email messages is reconstructed (Yeh and Harnly, 2006), and enriched by Agarwal et al. (2012) with gold organizational power relations, manually determined using information from Enron organizational charts. The corpus captures dominance relations between 13,724 pairs of Enron employees. As in (Prabhakaran and Rambow, 2014), we use these dominance relation tuples to obtain gold labels for the superior or subordinate relationships between pairs of participants. We use the same train-test-dev split as in (Prabhakaran and Rambow, 2014). We summarize the number of threads and related interacting participant pairs in each subset of the data in Table 1. 4 Research Hypotheses Our first objec"
N18-1096,W10-3001,0,0.0213727,"Missing"
N18-1096,W09-3012,1,0.883234,"2014). Another area of research that has recently garnered interest within the NLP community is the modeling of author commitment in text. Initial studies in this area were done in processing hedges, uncertainty and lack of commitment, specifically focused on scientific text (Mercer et al., 2004; Di Marco et al., 2006; Farkas et al., 2010). More recently, researchers have also looked into capturing author commitment in nonscientific text, e.g., levels of factuality in newswire (Saur´ı and Pustejovsky, 2009), types of commitment of beliefs in a variety of genres including conversational text (Diab et al., 2009; Prabhakaran et al., 2015). These approaches are motivated from an information extraction perspective, for instance in aiding tasks such as knowledge base population.1 However, it has not been studied whether such sophisticated author commitment analysis can go beyond what is expressed in language and reveal the underlying social contexts in which language is exchanged. In this paper, we bring together these two lines of research; we study how power relations correlate with the levels of commitment authors express in interactions. We use the power analysis framework built by Prabhakaran and R"
N18-1096,W08-0607,0,0.0105405,"that is obtained on online discussion forums, which is closer to our genre. Automatic hedge/uncertainty detection is a very closely related task to belief detection. The belief tagging framework we use aims to capture the cognitive states of authors, whereas hedges are linguistic expressions that convey one of those cognitive states — non-committed beliefs. Automatic hedge/uncertainty detection has generated active research in recent years within the NLP community. Early work in this area focused on detecting speculative language in scientific text (Mercer et al., 2004; Di Marco et al., 2006; Kilicoglu and Bergler, 2008). The open evaluation as part of the CoNLL shared task in 2010 to detect uncertainty and hedging in biomedical and Wikipedia text (Farkas et al., 2010) triggered further research on this problem in the general domain (Agarwal and Yu, 2010; Morante et al., 2010; Velldal et al., 2012; Choi et al., 2012). Most of this work was aimed at formal scientific text in English. More recent work has tried to extend this work to other genres (Wei et al., 2013; Sanchez and Vogel, 2015) and languages (Velupillai, 2012; Vincze, 2014), as well as building general purpose hedge lexicons (Prokofieva and Hirschbe"
N18-1096,W10-3006,0,0.0227311,"Missing"
N18-1096,S15-1009,1,0.931333,"a of research that has recently garnered interest within the NLP community is the modeling of author commitment in text. Initial studies in this area were done in processing hedges, uncertainty and lack of commitment, specifically focused on scientific text (Mercer et al., 2004; Di Marco et al., 2006; Farkas et al., 2010). More recently, researchers have also looked into capturing author commitment in nonscientific text, e.g., levels of factuality in newswire (Saur´ı and Pustejovsky, 2009), types of commitment of beliefs in a variety of genres including conversational text (Diab et al., 2009; Prabhakaran et al., 2015). These approaches are motivated from an information extraction perspective, for instance in aiding tasks such as knowledge base population.1 However, it has not been studied whether such sophisticated author commitment analysis can go beyond what is expressed in language and reveal the underlying social contexts in which language is exchanged. In this paper, we bring together these two lines of research; we study how power relations correlate with the levels of commitment authors express in interactions. We use the power analysis framework built by Prabhakaran and Rambow (2014) to perform thi"
N18-1096,W15-0302,0,0.0208689,"in this area focused on detecting speculative language in scientific text (Mercer et al., 2004; Di Marco et al., 2006; Kilicoglu and Bergler, 2008). The open evaluation as part of the CoNLL shared task in 2010 to detect uncertainty and hedging in biomedical and Wikipedia text (Farkas et al., 2010) triggered further research on this problem in the general domain (Agarwal and Yu, 2010; Morante et al., 2010; Velldal et al., 2012; Choi et al., 2012). Most of this work was aimed at formal scientific text in English. More recent work has tried to extend this work to other genres (Wei et al., 2013; Sanchez and Vogel, 2015) and languages (Velupillai, 2012; Vincze, 2014), as well as building general purpose hedge lexicons (Prokofieva and Hirschberg, 2014). In our work, we use the lexicons from (Prokofieva and Hirschberg, 2014) to capture hedges in text. Sociolinguists have long studied the association between level of commitment and social contexts (Lakoff, 1973; O’Barr and Atkins, 1980; Hyland, 1998). A majority of this work studies gender differences in the use of hedges, triggered by the influential work by Robin Lakoff (Lakoff, 1973). She argued that women use linguistic strategies such as hedging and hesitat"
N18-1096,I13-1042,1,0.899855,"Missing"
N18-1096,I13-1025,1,0.837951,"5; Creamer et al., 2009) or email traffic patterns (Namata et al., 2007). Using NLP to deduce social relations from online communication is a relatively new area of active research. Bramsen et al. (2011) and Gilbert (2012) first applied NLP based techniques to predict power relations in Enron emails, approaching this task as a text classification problem using bag of words or ngram features. More recently, our work has used dialog structure features derived from deeper dialog act analysis for the task of power prediction in Enron emails (Prabhakaran and Rambow, 2014; Prabhakaran et al., 2012; Prabhakaran and Rambow, 2013). In this paper, We use the framework of (Prabhakaran and Rambow, 2014), but we analyze a novel aspect of interaction that has not been studied before — what level of commitment do the authors express in language. There has also been work on analyzing power in other genres of interactions. Strzalkowski et al. (2010) and Taylor et al. (2012) concentrate on lower-level constructs called Language Uses such as agenda control to predict power in Wikipedia talk pages. Danescu-Niculescu-Mizil et al. (2012) study how social power and linguistic coordination are correlated in Wikipedia interactions as"
N18-1096,C10-1117,0,0.0564892,"Missing"
N18-1096,P14-2056,1,0.89234,"Missing"
N18-1096,C10-2117,1,0.839063,"Missing"
N18-1096,C12-1138,1,0.81554,"005; Shetty and Adibi, 2005; Creamer et al., 2009) or email traffic patterns (Namata et al., 2007). Using NLP to deduce social relations from online communication is a relatively new area of active research. Bramsen et al. (2011) and Gilbert (2012) first applied NLP based techniques to predict power relations in Enron emails, approaching this task as a text classification problem using bag of words or ngram features. More recently, our work has used dialog structure features derived from deeper dialog act analysis for the task of power prediction in Enron emails (Prabhakaran and Rambow, 2014; Prabhakaran et al., 2012; Prabhakaran and Rambow, 2013). In this paper, We use the framework of (Prabhakaran and Rambow, 2014), but we analyze a novel aspect of interaction that has not been studied before — what level of commitment do the authors express in language. There has also been work on analyzing power in other genres of interactions. Strzalkowski et al. (2010) and Taylor et al. (2012) concentrate on lower-level constructs called Language Uses such as agenda control to predict power in Wikipedia talk pages. Danescu-Niculescu-Mizil et al. (2012) study how social power and linguistic coordination are correlate"
N18-1096,J12-2005,0,0.0130652,"ns that convey one of those cognitive states — non-committed beliefs. Automatic hedge/uncertainty detection has generated active research in recent years within the NLP community. Early work in this area focused on detecting speculative language in scientific text (Mercer et al., 2004; Di Marco et al., 2006; Kilicoglu and Bergler, 2008). The open evaluation as part of the CoNLL shared task in 2010 to detect uncertainty and hedging in biomedical and Wikipedia text (Farkas et al., 2010) triggered further research on this problem in the general domain (Agarwal and Yu, 2010; Morante et al., 2010; Velldal et al., 2012; Choi et al., 2012). Most of this work was aimed at formal scientific text in English. More recent work has tried to extend this work to other genres (Wei et al., 2013; Sanchez and Vogel, 2015) and languages (Velupillai, 2012; Vincze, 2014), as well as building general purpose hedge lexicons (Prokofieva and Hirschberg, 2014). In our work, we use the lexicons from (Prokofieva and Hirschberg, 2014) to capture hedges in text. Sociolinguists have long studied the association between level of commitment and social contexts (Lakoff, 1973; O’Barr and Atkins, 1980; Hyland, 1998). A majority of this w"
N18-1096,C14-1174,0,0.0128194,"scientific text (Mercer et al., 2004; Di Marco et al., 2006; Kilicoglu and Bergler, 2008). The open evaluation as part of the CoNLL shared task in 2010 to detect uncertainty and hedging in biomedical and Wikipedia text (Farkas et al., 2010) triggered further research on this problem in the general domain (Agarwal and Yu, 2010; Morante et al., 2010; Velldal et al., 2012; Choi et al., 2012). Most of this work was aimed at formal scientific text in English. More recent work has tried to extend this work to other genres (Wei et al., 2013; Sanchez and Vogel, 2015) and languages (Velupillai, 2012; Vincze, 2014), as well as building general purpose hedge lexicons (Prokofieva and Hirschberg, 2014). In our work, we use the lexicons from (Prokofieva and Hirschberg, 2014) to capture hedges in text. Sociolinguists have long studied the association between level of commitment and social contexts (Lakoff, 1973; O’Barr and Atkins, 1980; Hyland, 1998). A majority of this work studies gender differences in the use of hedges, triggered by the influential work by Robin Lakoff (Lakoff, 1973). She argued that women use linguistic strategies such as hedging and hesitations in order to adopt an unassertive communica"
N18-1096,P13-2011,0,0.0190235,"munity. Early work in this area focused on detecting speculative language in scientific text (Mercer et al., 2004; Di Marco et al., 2006; Kilicoglu and Bergler, 2008). The open evaluation as part of the CoNLL shared task in 2010 to detect uncertainty and hedging in biomedical and Wikipedia text (Farkas et al., 2010) triggered further research on this problem in the general domain (Agarwal and Yu, 2010; Morante et al., 2010; Velldal et al., 2012; Choi et al., 2012). Most of this work was aimed at formal scientific text in English. More recent work has tried to extend this work to other genres (Wei et al., 2013; Sanchez and Vogel, 2015) and languages (Velupillai, 2012; Vincze, 2014), as well as building general purpose hedge lexicons (Prokofieva and Hirschberg, 2014). In our work, we use the lexicons from (Prokofieva and Hirschberg, 2014) to capture hedges in text. Sociolinguists have long studied the association between level of commitment and social contexts (Lakoff, 1973; O’Barr and Atkins, 1980; Hyland, 1998). A majority of this work studies gender differences in the use of hedges, triggered by the influential work by Robin Lakoff (Lakoff, 1973). She argued that women use linguistic strategies s"
N18-1096,C00-2137,0,0.19275,"Missing"
N18-1096,P11-1078,0,\N,Missing
P14-2056,P12-2032,1,0.844306,"Missing"
P14-2056,N13-1099,1,0.91251,"Missing"
P14-2056,I13-1025,1,0.865401,"irection of power; our new features significantly improve the results over using previously proposed features. 1 Introduction Computationally analyzing the social context in which language is used has gathered great interest within the NLP community recently. One of the areas that has generated substantial research is the study of how social power relations between people affect and/or are revealed in their interactions with one another. Researchers have proposed systems to detect social power relations between participants of organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online forums (Danescu-NiculescuMizil et al., 2012; Biran et al., 2012; DanescuNiculescu-Mizil et al., 2013), chats (Strzalkowski et al., 2012), and off-line interactions such as presidential debates (Prabhakaran et al., 2013; Nguyen et al., 2013). Automatically identifying power and influence from interactions can have many practical applications ranging from law enforcement and intelligence to online marketing. A significant number of these studies are performed in the domain of organizational email where there is a well defined notion of power (organizational hierarchy). Bramsen et al. (2"
P14-2056,W12-2105,1,0.675625,"eviously proposed features. 1 Introduction Computationally analyzing the social context in which language is used has gathered great interest within the NLP community recently. One of the areas that has generated substantial research is the study of how social power relations between people affect and/or are revealed in their interactions with one another. Researchers have proposed systems to detect social power relations between participants of organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online forums (Danescu-NiculescuMizil et al., 2012; Biran et al., 2012; DanescuNiculescu-Mizil et al., 2013), chats (Strzalkowski et al., 2012), and off-line interactions such as presidential debates (Prabhakaran et al., 2013; Nguyen et al., 2013). Automatically identifying power and influence from interactions can have many practical applications ranging from law enforcement and intelligence to online marketing. A significant number of these studies are performed in the domain of organizational email where there is a well defined notion of power (organizational hierarchy). Bramsen et al. (2011) and Gilbert (2012) predict hierarchical power relations between peo"
P14-2056,N12-1057,1,0.887992,"Missing"
P14-2056,P11-1078,0,0.24877,"ised learning system to predict the direction of power; our new features significantly improve the results over using previously proposed features. 1 Introduction Computationally analyzing the social context in which language is used has gathered great interest within the NLP community recently. One of the areas that has generated substantial research is the study of how social power relations between people affect and/or are revealed in their interactions with one another. Researchers have proposed systems to detect social power relations between participants of organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online forums (Danescu-NiculescuMizil et al., 2012; Biran et al., 2012; DanescuNiculescu-Mizil et al., 2013), chats (Strzalkowski et al., 2012), and off-line interactions such as presidential debates (Prabhakaran et al., 2013; Nguyen et al., 2013). Automatically identifying power and influence from interactions can have many practical applications ranging from law enforcement and intelligence to online marketing. A significant number of these studies are performed in the domain of organizational email where there is a well defined notion of power"
P14-2056,I13-1042,1,0.883912,"Missing"
P14-2056,P13-1025,0,0.151807,"Missing"
P14-2056,C12-1155,0,0.108838,"ing the social context in which language is used has gathered great interest within the NLP community recently. One of the areas that has generated substantial research is the study of how social power relations between people affect and/or are revealed in their interactions with one another. Researchers have proposed systems to detect social power relations between participants of organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online forums (Danescu-NiculescuMizil et al., 2012; Biran et al., 2012; DanescuNiculescu-Mizil et al., 2013), chats (Strzalkowski et al., 2012), and off-line interactions such as presidential debates (Prabhakaran et al., 2013; Nguyen et al., 2013). Automatically identifying power and influence from interactions can have many practical applications ranging from law enforcement and intelligence to online marketing. A significant number of these studies are performed in the domain of organizational email where there is a well defined notion of power (organizational hierarchy). Bramsen et al. (2011) and Gilbert (2012) predict hierarchical power relations between people in the Enron email corpus using lexical features extracted from all t"
P16-1111,W12-3202,1,0.343253,"ity. 2 Related Work Our work builds upon a wealth of previous literature in both topic modeling and scientific discourse analysis, which we discuss in this section. We also discuss how our work relates to prior work on analyzing scientific trends. 2.1 Topic Modeling Topic modeling has a long history of applications to scientific literature, including studies of temporal scientific trends (Griffiths and Steyvers, 2004; Steyvers et al., 2004; Wang and McCallum, 2006), article recommendation (Wang and Blei, 2011), and impact prediction (Yogatama et al., 2011). For example, Hall et al. (2008) and Anderson et al. (2012) show how tracking topic popularities over time can produce a ‘computational history’ of a particular scientific field (in their case ACL, where they tracked the rise of statistical NLP, among other dramatic changes). Technical advancements in these areas usually correspond to modifications or extensions of the topic modeling (i.e., LDA) framework itself, such as by incorporating citation (Nallapati et al., 2008) or co-authorship information (Mei et al., 2008) directly into the topic model; Nallapati et al. (2011) employ such an extension to estimate the temporal “lead” or “lag” of different s"
P16-1111,P79-1022,0,0.401947,"Missing"
P16-1111,I11-1001,0,0.173992,"ant clues about the stage or development of an intellectual movement they stand to represent. For example, a topic that shifts over time from being employed as a method to being mentioned as background may signal an increase in its maturity and perhaps a corresponding decrease in its popularity among new research. In this paper, we introduce a new algorithm to determine the rhetorical functions of topics associated with an abstract. There is much work on annotating and automatically parsing the rhetorical functions or narrative structure of scientific writing (e.g., Teufel, 2000; Chung, 2009; Gupta and Manning, 2011; de Waard and Maat, 2012). We derive insights from this prior work, but since we desire to apply our analysis to a broad range of domains, we build our narrative structure model based on over 83,000 self-labeled abstracts extracted from a variety of domains in the Web of Science corpus. Figure 2 shows an example of an abstract in which the authors have labeled the different narrative sections explicitly and identified the rhetorical functions. We use our narrative structure model to assign rhetorical function labels to scientific topics and show that these labels offer important clues indicat"
P16-1111,D08-1038,0,0.0356745,"hold important clues about the dynamics involved in the evolution of science; clues that may help predict the rise and fall of scientific ideas, methods and even fields. Being able to predict scientific trends in advance could potentially revolutionize the way science is done, for instance, by enabling funding agencies to optimize allocation of resources towards promising research areas. Prior studies have often tracked scientific trends by applying topic modeling (Blei et al., 2003) based techniques to large corpora of scientific texts (Griffiths and Steyvers, 2004; Blei and Lafferty, 2006; Hall et al., 2008). They capture scientific ideas, methods, and fields in terms of topics, modeled as distributions over collection of words. These approaches usually adopt a decontextualized view of text and its usage, associating topics to documents based solely on word occurrences, disregarding where or how the words were employed. In reality, however, scientific abstracts often follow narrative structures (Crookes, 1986; Latour, 1987) that signal the specific rhetorical roles that different topics play within the research (Figure 1). The rhetorical role of a topic is the purpose or role it plays in the pape"
P16-1111,C12-1041,0,0.0151564,"12). A wide range of techniques have been used in prior work to parse scientific abstracts, from fully supervised techniques (Chung, 2009; Guo et al., 2010) to semi-supervised (Guo et al., 2011c; Guo et al., 2013) and unsupervised techniques (Kiela et al., 2015). Scientific discourse parsing has also been applied to other downstream tasks within the biomedical domain, such as information retrieval from randomized controlled trials in evidence based medicine (Chung, 2009; Kim et al., 2011; Verbeke et al., 2012), cancer risk assessment (Guo et al., 2011b), summarization (Teufel and Moens, 2002; Contractor et al., 2012), and question answering (Guo et al., 2013). Our work also falls in this category in the sense that our goal is to apply the rhetorical function parser to better understand the link between rhetoric and the historical trajectory of scientific ideas. 2.3 Scientific Trends Analysis There is also a large body of literature in bibliometrics and scientometrics on tracking scientific trends using various citation patterns. Researchers have attempted to detect emerging research fronts using topological measures of citation networks (Shibata et al., 2008) as well as co-citation clusters (Small, 2006;"
P16-1111,I08-1050,0,0.303802,"Missing"
P16-1111,W10-1913,0,0.211338,"LDA 1171 framework—by overlaying rhetorical roles—and how this allows us to not only detect the growth and decline of scientific topics but also to predict these trends based upon the rhetorical roles being employed. Since our framework is structured as a pipeline (Figure 3) and works with the output of a topic modeling system, it is compatible with the vast majority of these extended topic models. 2.2 Scientific Discourse Analysis Scientific discourse analysis is an active area of research with many different proposed schema of analysis — Argument Zones (Teufel, 2000), Information Structure (Guo et al., 2010), Core Scientific Concepts (Liakata, 2010), Research Aspects (Gupta and Manning, 2011), Discourse Segments (de Waard and Maat, 2012), Relation Structures (Tateisi et al., 2014), and Rhetorical Roles (Chung, 2009) to name a few. Most studies in this area focus on improving automatic discourse parsing of scientific text, while some works also focus on the linguistic patterns and psychological effects of scientific argumentation (e.g., de Waard and Maat, 2012). A wide range of techniques have been used in prior work to parse scientific abstracts, from fully supervised techniques (Chung, 2009; Guo"
P16-1111,D11-1025,0,0.0873972,", 2010), Research Aspects (Gupta and Manning, 2011), Discourse Segments (de Waard and Maat, 2012), Relation Structures (Tateisi et al., 2014), and Rhetorical Roles (Chung, 2009) to name a few. Most studies in this area focus on improving automatic discourse parsing of scientific text, while some works also focus on the linguistic patterns and psychological effects of scientific argumentation (e.g., de Waard and Maat, 2012). A wide range of techniques have been used in prior work to parse scientific abstracts, from fully supervised techniques (Chung, 2009; Guo et al., 2010) to semi-supervised (Guo et al., 2011c; Guo et al., 2013) and unsupervised techniques (Kiela et al., 2015). Scientific discourse parsing has also been applied to other downstream tasks within the biomedical domain, such as information retrieval from randomized controlled trials in evidence based medicine (Chung, 2009; Kim et al., 2011; Verbeke et al., 2012), cancer risk assessment (Guo et al., 2011b), summarization (Teufel and Moens, 2002; Contractor et al., 2012), and question answering (Guo et al., 2013). Our work also falls in this category in the sense that our goal is to apply the rhetorical function parser to better underst"
P16-1111,W10-3101,0,0.013179,"oles—and how this allows us to not only detect the growth and decline of scientific topics but also to predict these trends based upon the rhetorical roles being employed. Since our framework is structured as a pipeline (Figure 3) and works with the output of a topic modeling system, it is compatible with the vast majority of these extended topic models. 2.2 Scientific Discourse Analysis Scientific discourse analysis is an active area of research with many different proposed schema of analysis — Argument Zones (Teufel, 2000), Information Structure (Guo et al., 2010), Core Scientific Concepts (Liakata, 2010), Research Aspects (Gupta and Manning, 2011), Discourse Segments (de Waard and Maat, 2012), Relation Structures (Tateisi et al., 2014), and Rhetorical Roles (Chung, 2009) to name a few. Most studies in this area focus on improving automatic discourse parsing of scientific text, while some works also focus on the linguistic patterns and psychological effects of scientific argumentation (e.g., de Waard and Maat, 2012). A wide range of techniques have been used in prior work to parse scientific abstracts, from fully supervised techniques (Chung, 2009; Guo et al., 2010) to semi-supervised (Guo et"
P16-1111,D12-1053,0,0.0560339,"Missing"
P16-1111,W09-3603,0,0.0637304,"Missing"
P16-1111,D11-1055,0,0.0299571,"Missing"
P16-1111,tateisi-etal-2014-annotation,0,0.0126253,"ed upon the rhetorical roles being employed. Since our framework is structured as a pipeline (Figure 3) and works with the output of a topic modeling system, it is compatible with the vast majority of these extended topic models. 2.2 Scientific Discourse Analysis Scientific discourse analysis is an active area of research with many different proposed schema of analysis — Argument Zones (Teufel, 2000), Information Structure (Guo et al., 2010), Core Scientific Concepts (Liakata, 2010), Research Aspects (Gupta and Manning, 2011), Discourse Segments (de Waard and Maat, 2012), Relation Structures (Tateisi et al., 2014), and Rhetorical Roles (Chung, 2009) to name a few. Most studies in this area focus on improving automatic discourse parsing of scientific text, while some works also focus on the linguistic patterns and psychological effects of scientific argumentation (e.g., de Waard and Maat, 2012). A wide range of techniques have been used in prior work to parse scientific abstracts, from fully supervised techniques (Chung, 2009; Guo et al., 2010) to semi-supervised (Guo et al., 2011c; Guo et al., 2013) and unsupervised techniques (Kiela et al., 2015). Scientific discourse parsing has also been applied to"
P16-1111,J02-4002,0,0.135814,"., de Waard and Maat, 2012). A wide range of techniques have been used in prior work to parse scientific abstracts, from fully supervised techniques (Chung, 2009; Guo et al., 2010) to semi-supervised (Guo et al., 2011c; Guo et al., 2013) and unsupervised techniques (Kiela et al., 2015). Scientific discourse parsing has also been applied to other downstream tasks within the biomedical domain, such as information retrieval from randomized controlled trials in evidence based medicine (Chung, 2009; Kim et al., 2011; Verbeke et al., 2012), cancer risk assessment (Guo et al., 2011b), summarization (Teufel and Moens, 2002; Contractor et al., 2012), and question answering (Guo et al., 2013). Our work also falls in this category in the sense that our goal is to apply the rhetorical function parser to better understand the link between rhetoric and the historical trajectory of scientific ideas. 2.3 Scientific Trends Analysis There is also a large body of literature in bibliometrics and scientometrics on tracking scientific trends using various citation patterns. Researchers have attempted to detect emerging research fronts using topological measures of citation networks (Shibata et al., 2008) as well as co-citati"
prabhakaran-etal-2012-annotations,W09-3953,1,\N,Missing
prabhakaran-etal-2012-annotations,P11-1078,0,\N,Missing
prabhakaran-etal-2012-annotations,N12-1057,1,\N,Missing
Q18-1033,Q16-1033,0,0.0176208,"nstrate the importance of understanding the role of institutional context in shaping conversation structure. In doing so, our paper also draws on recent research on automatically extracting structure from human-human dialog. Drawing on Grosz’s original insights, Bangalore et al. (2006) show how to extract a hierarchical task structure for catalog ordering dialogs with subtasks like opening, contact-information, order-item, relatedoffers, and summary. Prabhakaran et al. (2012) and Prabhakaran et al. (2014) employ dialog act analysis to study correlates of gender and power in work emails, while Althoff et al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour e"
Q18-1033,P06-1026,0,0.0470307,"Missing"
Q18-1033,cieri-etal-2004-fisher,0,0.109806,"eraged F-scores on institutional act prediction using different ASR sources. Table 6 shows word error rates under different settings. Overall, we obtain relatively high error rates, largely due to the noisy environment of the audio in this domain. BLSTM performs better than DNNHMM, consistent with prior research (Mohamed et al., 2015; Sak et al., 2014).11 Interpolating Switchboard and Fisher language models provides a further boost of 0.7 percentage points. 6.3 Language Model Data Augmentation 6.5 To mitigate language model data scarcity, we use transcriptions from the Switchboard and Fisher (Cieri et al., 2004) corpora, adding about 3.12M and 21.1M words, respectively. Separate language models are trained on these datasets, and then interpolated with the traffic stop language model; interpolation weights were chosen by minimizing perplexity on a separate Dev set. Table 5 shows the perplexities of different language models on this Dev set. We now use text generated by ASR to train and test the institutional act tagger of Section 4. To increase recall, we also made use of N-best list output from the ASR systems, collecting ngram and pattern features from the top 10 candidate transcriptions. The L1 pen"
Q18-1033,W04-3240,0,0.202657,"Missing"
Q18-1033,D08-1035,0,0.0210716,"e opening, contact-information, order-item, relatedoffers, and summary. Prabhakaran et al. (2012) and Prabhakaran et al. (2014) employ dialog act analysis to study correlates of gender and power in work emails, while Althoff et al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour et al. (2016), Li and Wu (2016) and Liu et al. (2017), have posted state-of-the-art performance on benchmark datasets such as the Switchboard corpus (Jurafsky et al., 1997) and MRDA (Ang et al., 2005). Since these corpora come from significantly different domains (telephone conversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the insti"
Q18-1033,C16-1189,0,0.0224325,"al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour et al. (2016), Li and Wu (2016) and Liu et al. (2017), have posted state-of-the-art performance on benchmark datasets such as the Switchboard corpus (Jurafsky et al., 1997) and MRDA (Ang et al., 2005). Since these corpora come from significantly different domains (telephone conversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the institutional acts (e.g., did the officer request documentation from the driver?) rather than the general dialog acts (did the officer issue a request?), these taggers do not directly serve our purpose. Furthermore, our data i"
Q18-1033,D10-1084,0,0.150742,"s a tool for police departments to assess and improve police community interactions. 2 Background Computational work on human-human conversation has long focused on dialog structure, beginning with the influential work of Grosz showing the homology between dialog and task structure (Grosz, 1977). Recent work has integrated speech act theory (Austin, 1975) and conversational analysis (Schegloff and Sacks, 1973; Sacks et al., 1974; Schegloff, 1979) into models of dialog acts for domains like meetings (Ang et al., 2005), telephone calls (Stolcke et al., 2006), emails (Cohen et al., 2004), chats (Kim et al., 2010), and Twitter (Ritter et al., 2010). Our models extend this work by drawing on the notion of institutional talk (Atkinson and Drew, 1979), an application of conversational analysis to environments in which the goals of participants are institution-specific. Actions, their sequences, and interpretations during institutional talk depend not 468 only on the speaker (as speech act theory suggests) or the dialog (as conversational analysts argue), but they are inherently tied to the institutional context. Institutional talk has been used as a tool to understand the work of social institutions. For"
Q18-1033,Y12-1050,0,0.0147698,"onversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the institutional acts (e.g., did the officer request documentation from the driver?) rather than the general dialog acts (did the officer issue a request?), these taggers do not directly serve our purpose. Furthermore, our data is an order of magnitude smaller (around 7K sentences) than these corpora; making it infeasible to train in-domain recurrent networks. Prior to neural network approaches, support vector machines and conditional random fields (Cohen et al., 2004; Kim et al., 2010; Kim et al., 2012; Omuya et al., 2013) were the state-of-the-art algorithms on this task. These approaches also incorporated contextual and structural information into the classifier. For instance, Kim et al. (2012) used lexical information from previous utterances in predicting the dialog act of a current utterance; and Omuya et al. (2013) uses features such as the relative position of an utterance w.r.t the whole dialog. We draw from this line of work; we also experiment with positional and contextual features in addition to lexical features. Furthermore, we use features that capture the institutional contex"
Q18-1033,D14-1181,0,0.00499336,"Missing"
Q18-1033,C16-1185,0,0.0186278,"tural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour et al. (2016), Li and Wu (2016) and Liu et al. (2017), have posted state-of-the-art performance on benchmark datasets such as the Switchboard corpus (Jurafsky et al., 1997) and MRDA (Ang et al., 2005). Since these corpora come from significantly different domains (telephone conversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the institutional acts (e.g., did the officer request documentation from the driver?) rather than the general dialog acts (did the officer issue a request?), these taggers do not directly serve our purpose. Furthermore, our data is an order of magn"
Q18-1033,D17-1231,0,0.0147971,"essful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour et al. (2016), Li and Wu (2016) and Liu et al. (2017), have posted state-of-the-art performance on benchmark datasets such as the Switchboard corpus (Jurafsky et al., 1997) and MRDA (Ang et al., 2005). Since these corpora come from significantly different domains (telephone conversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the institutional acts (e.g., did the officer request documentation from the driver?) rather than the general dialog acts (did the officer issue a request?), these taggers do not directly serve our purpose. Furthermore, our data is an order of magnitude smaller (around"
Q18-1033,P12-1009,0,0.0263844,"relatedoffers, and summary. Prabhakaran et al. (2012) and Prabhakaran et al. (2014) employ dialog act analysis to study correlates of gender and power in work emails, while Althoff et al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour et al. (2016), Li and Wu (2016) and Liu et al. (2017), have posted state-of-the-art performance on benchmark datasets such as the Switchboard corpus (Jurafsky et al., 1997) and MRDA (Ang et al., 2005). Since these corpora come from significantly different domains (telephone conversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the institutional acts (e.g., did the offic"
Q18-1033,N13-1099,1,0.848533,"eeting transcripts, respectively) than ours, and since we are interested specifically in the institutional acts (e.g., did the officer request documentation from the driver?) rather than the general dialog acts (did the officer issue a request?), these taggers do not directly serve our purpose. Furthermore, our data is an order of magnitude smaller (around 7K sentences) than these corpora; making it infeasible to train in-domain recurrent networks. Prior to neural network approaches, support vector machines and conditional random fields (Cohen et al., 2004; Kim et al., 2010; Kim et al., 2012; Omuya et al., 2013) were the state-of-the-art algorithms on this task. These approaches also incorporated contextual and structural information into the classifier. For instance, Kim et al. (2012) used lexical information from previous utterances in predicting the dialog act of a current utterance; and Omuya et al. (2013) uses features such as the relative position of an utterance w.r.t the whole dialog. We draw from this line of work; we also experiment with positional and contextual features in addition to lexical features. Furthermore, we use features that capture the institutional context of the conversation"
Q18-1033,D12-1009,0,0.114192,"order-item, relatedoffers, and summary. Prabhakaran et al. (2012) and Prabhakaran et al. (2014) employ dialog act analysis to study correlates of gender and power in work emails, while Althoff et al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in NLP on dialog act classification. Recently, recurrent neural network-based dialog act taggers, e.g., Khanpour et al. (2016), Li and Wu (2016) and Liu et al. (2017), have posted state-of-the-art performance on benchmark datasets such as the Switchboard corpus (Jurafsky et al., 1997) and MRDA (Ang et al., 2005). Since these corpora come from significantly different domains (telephone conversations and meeting transcripts, respectively) than ours, and since we are interested specifically in the institutional act"
Q18-1033,N12-1057,1,0.813135,"(e.g. do they listen, take civilian views into account) predict civilian’s attitudes towards the police (Giles et al., 2006). These findings demonstrate the importance of understanding the role of institutional context in shaping conversation structure. In doing so, our paper also draws on recent research on automatically extracting structure from human-human dialog. Drawing on Grosz’s original insights, Bangalore et al. (2006) show how to extract a hierarchical task structure for catalog ordering dialogs with subtasks like opening, contact-information, order-item, relatedoffers, and summary. Prabhakaran et al. (2012) and Prabhakaran et al. (2014) employ dialog act analysis to study correlates of gender and power in work emails, while Althoff et al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to t"
Q18-1033,D14-1211,1,0.859702,"ilian views into account) predict civilian’s attitudes towards the police (Giles et al., 2006). These findings demonstrate the importance of understanding the role of institutional context in shaping conversation structure. In doing so, our paper also draws on recent research on automatically extracting structure from human-human dialog. Drawing on Grosz’s original insights, Bangalore et al. (2006) show how to extract a hierarchical task structure for catalog ordering dialogs with subtasks like opening, contact-information, order-item, relatedoffers, and summary. Prabhakaran et al. (2012) and Prabhakaran et al. (2014) employ dialog act analysis to study correlates of gender and power in work emails, while Althoff et al. (2016) studied structural aspects of successful counseling conversations, and Yang et al. (2013) and Chandrasekaran et al. (2017) investigated structures in online classroom conversations that predict success or need for intervention. Our work also draws on an important line of unsupervised work that models topical structure of conversations (Blei and Moreno, 2001; Eisenstein and Barzilay, 2008; Paul, 2012; Nguyen et al., 2012). Our work is closely related to the active line of research in"
Q18-1033,N10-1020,0,0.0835973,"to assess and improve police community interactions. 2 Background Computational work on human-human conversation has long focused on dialog structure, beginning with the influential work of Grosz showing the homology between dialog and task structure (Grosz, 1977). Recent work has integrated speech act theory (Austin, 1975) and conversational analysis (Schegloff and Sacks, 1973; Sacks et al., 1974; Schegloff, 1979) into models of dialog acts for domains like meetings (Ang et al., 2005), telephone calls (Stolcke et al., 2006), emails (Cohen et al., 2004), chats (Kim et al., 2010), and Twitter (Ritter et al., 2010). Our models extend this work by drawing on the notion of institutional talk (Atkinson and Drew, 1979), an application of conversational analysis to environments in which the goals of participants are institution-specific. Actions, their sequences, and interpretations during institutional talk depend not 468 only on the speaker (as speech act theory suggests) or the dialog (as conversational analysts argue), but they are inherently tied to the institutional context. Institutional talk has been used as a tool to understand the work of social institutions. For example, Whalen and Zimmerman (1987"
S15-1008,N13-1093,0,0.0399861,"Missing"
S15-1008,W10-3110,0,0.0678261,"Missing"
S15-1008,W09-3012,1,0.824638,"e and then explain our approach in detail. 4 Syntactic Framework Negation, as a language device, is naturally conceptualized as applying to fully instantiated predicateargument clusters. We therefore use predicate argument graphs as structural abstractions of syntax trees. Additional advantages of these abstractions include their affinity for having extra-propositional Figure 1: PAS for e2: “Moreover, cAMP activators did not activate NF-kappa B in Jurkat cells” aspects of meaning ‘layered’ onto the representation (precedents in prior studies can be found in e.g. (Saur´ı and Pustejovsky, 2009; Diab et al., 2009)), and their pervasive use in a state-of-the-art QA system—for question analysis, candidate generation, and analysis of passage evidence (Ferrucci et al., 2010; Ferrucci, 2012)—which is at the heart of our medical adaptation (Ferrucci et al., 2013). We use predicate-argument structure (PAS) (McCord et al., 2012) derived from dependency parses produced by the English Slot Grammar parser (McCord, 1990). In addition to normalizing across different tree structures expressing essentially the same meaning, PAS provides a simplified view over ‘raw’ syntactic trees, gathering all arguments to a predic"
S15-1008,W09-1401,0,0.16892,"inding or disease that follows a negation cue. While this works well for simpler expressions of negations, it tends to fail for more complex negation constructs. More recent approaches attempt to tackle the variability in scopes encountered in broader data by using statistical learning methods grounded in publicly available corpora with cue and scope annotations. The first such corpus was BioScope (Vincze et al., 2008), which annotates negation cues and associated scopes in 3 genres—medical abstracts, scientific papers and clinical records. The BioNLP Event Extraction (EE) shared task corpus (Kim et al., 2009) also marks negation in the event annotations on sentences from molecular biology literature. Most recently, the *SEM 2012 shared task corpus (Morante and Blanco, 2012) marks negations, their foci, and scopes in sentences from Conan Doyle stories in an attempt to extend the research on negation to the general domain. Both the BioNLP-EE and *SEM corpora capture negations within—and therefore aligned with—syntactic analyses. Thus they deploy annotation schemes which assume downstream consumers of some granular negation representation, learnable from the annotated resource(s). However, the langua"
S15-1008,S12-1035,0,0.100928,". More recent approaches attempt to tackle the variability in scopes encountered in broader data by using statistical learning methods grounded in publicly available corpora with cue and scope annotations. The first such corpus was BioScope (Vincze et al., 2008), which annotates negation cues and associated scopes in 3 genres—medical abstracts, scientific papers and clinical records. The BioNLP Event Extraction (EE) shared task corpus (Kim et al., 2009) also marks negation in the event annotations on sentences from molecular biology literature. Most recently, the *SEM 2012 shared task corpus (Morante and Blanco, 2012) marks negations, their foci, and scopes in sentences from Conan Doyle stories in an attempt to extend the research on negation to the general domain. Both the BioNLP-EE and *SEM corpora capture negations within—and therefore aligned with—syntactic analyses. Thus they deploy annotation schemes which assume downstream consumers of some granular negation representation, learnable from the annotated resource(s). However, the language in both of them differs greatly from the language encountered in clinical text, making them unsuitable for our QA system requirements. In contrast, BioScope matches"
S15-1008,W09-1105,0,0.086623,"Missing"
S15-1008,C10-2117,1,0.900141,"Missing"
S15-1008,W12-3807,1,0.879487,"Missing"
S15-1008,N12-1057,1,0.825728,"Missing"
S15-1008,P12-2014,0,0.0312304,"t. Not capturing this extra-propositional aspect of negation concerning focal pneumonia will lead to wrong—and harmful—inferences in downstream processing, e.g. by a clinical decision support system. The need for sophisticated negation detection capabilities in clinical text is even more urgent given the broadening spectrum of applications in this domain: clinical question answering (Lee et al., 2006), clinical decision support (DemnerFushman et al., 2009), medical information extraction (Uzuner et al., 2010), medical entity relation mining (Tymoshenko et al., 2012), patient history tracking (Raghavan et al., 2012), etc. Our motivation for detecting negations in medical texts also stems from practical concerns of an operational medical question answering (QA) system (Ferrucci et al., 2013). Most recent approaches to negation detection adopt supervised machine learning techniques to learn the phraseology of negation-containing expressions. They often follow a two step process— detection of negation cues (“no”, “without”, . . .), followed by detection of their associated scopes. Cue detection is a relatively simple task, since the set of cue words is not large. Determining the scope of 71 Proceedings of t"
S15-1008,W12-3806,0,0.114797,"under the scope of a negation cue. Scope detection is crucial for interpreting negations, and to that end, the BioScope corpus (Vincze et al., 2008) was released, with annotations of both negation cues and their associated scopes. The fact that these scopes are represented only as text-spans is a drawback of BioScope. Without being anchored to a syntactic analysis of the sentences in which they occur, BioScope’s scope annotations suffer from a variety of inconsistencies of mark-up. They also may, and occasionally do, fail to align with the underlying syntactic structures (Vincze et al., 2011; Stenetorp et al., 2012). Such inconsistencies make it hard for a system to learn the actual syntactic patterns connecting negation cues and their scopes—which are, after all, the real object of negation interpretation. The insight that we develop in this paper is that a scope span can be associated with one or more nodes in the syntactic analysis of a negated expression, and that these will be further connected—in a systematic way—to the negation cue node. Mapping loosely and/or inconsistently bounded spans to unique syntactic nodes (and configurations thereof) reduces the noise inherent in BioScope. The learning ta"
S15-1008,J12-2005,0,0.119731,"untered in clinical text, making them unsuitable for our QA system requirements. In contrast, BioScope matches our genre of clinical text. As an additional plus, it captures negation in a task-independent, linguistically motivated framework, which enables the building of systems applicable to a wider range of domains. BioScope’s negation-scope-as-span annotation framework, however, limits th corpus utility. Various approaches have used it to train negation scope span detection systems, and many have shown the importance of deep syntactic features in that task (e.g., (Ballesteros et al., 2012; Velldal et al., 2012; Zou et al., 2013)). They share a drawback: they are optimized for predicting the spans as they are annotated in BioScope—despite its various syntactic inconsistencies. For example, Ballesteros et al. (2012) use manual rules to detect the voice (passive or active) of a verb phrase; this is motivated by an annotation guideline for whether to include verb subjects in the span or not. In reality, what matters in the end is whether a detection system can capture the underlying phenomenon of negation that the annotations stand to represent, and not whether it can accurately replicate the represent"
S15-1008,W10-3111,0,0.0539536,"Missing"
S15-1008,D13-1099,0,0.0477957,"Missing"
S15-1009,P98-1013,0,0.149614,"the syntactic head of the text passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals"
S15-1009,baker-etal-2010-modality,0,0.0482113,"Missing"
S15-1009,W13-2322,0,0.0158937,"xt passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals, groups, artifacts, etc., which"
S15-1009,W09-3012,1,0.690346,"space we do not provide an overview over all definitions. While at first the terms “belief” and “factuality” appear to relate to rather different things (a subjective state versus truth), in the NLP community they in fact refer to the same phenomenon, while having rather different connotations. The phenomenon is the communicative intention of a writer1 to present propositional content as something that she firmly believes is true, weakly believes is true, or has some other attitude towards, namely a wish or a reported belief. The term “belief” here describes the cognitive state of the writer (Diab et al., 2009), and comes from artificial intelligence and cognitive science, as in the Belief-Desire-Intention model of Bratman (1999 1987). The term “factuality” describes the communicative intention of the writer (Saur´ı and Pustejovsky, 2012, p. 263) (our emphasis): The fact that an eventuality is depicted as holding or not does not mean that this is the case in the world, but that this is how it is characterized by its informant. Similarly, it does not mean that this is the real knowledge that informant has (his true cognitive state regarding that event) but what he wants us to believe it is. We would"
S15-1009,doddington-etal-2004-automatic,1,0.834711,"sity/George Washington University, the Florida Institute for Human and Machine Cognition, and the University of Albany. The goal of our research project is not linguistic annotation, but the identification of meaning which is expressed in a non-linguistic manner. Such a meaning representation is useful for many applications; in our project we are specifically interested in knowledge base population. A different part of the DEFT program is concerned with the representation of propositional meaning, following the tradition of the ACE program in representing entities, relations and events (ERE) (Doddington et al., 2004). The work presented here is concerned with the attitude of agents towards propositional content: do the agents express a committed belief or a non-committed belief in the propositional content? Our work has several characteristics that set it apart from other work: we are interested in annotation which can be done fairly quickly; we are not interested in annotating linguistic elements (such as trigger words); and we are planning an integration with sentiment annotation. The structure of the paper is as follows: we start out by situating our notion of “belief” with respect to other notions of"
S15-1009,W10-3001,0,0.293703,"Missing"
S15-1009,P11-2102,0,0.0364363,"rotates around the earth, as was his (presumably) honest communicative intention. Therefore, to us as researchers interested in describing how language 2 Sarcasm and irony differ from lying in that the communicative intention and the cognitive state are aligned, but they do not align with the standard interpretation of the utterance. Here, the intention is that the reader recognizes that the form of the utterance does not literally express the cognitive state. We leave aside sarcasm and irony in this paper; for current computational work on sarcasm detection, see for example (Gonz´alez-Ib´an˜ ez et al., 2011). is used to communicate, it does not matter that astronomers now believe that Ptolemy was wrong, it does not change our account of communication and it does not change the communication that happened two millennia ago. And since we do not need to make the assumption that the writer knows what she is talking about, we choose not to make this assumption. In the case of Ptolemy, we leave this determination – what is actually true – to astronomers. In other cases, we typically have models of trustworthiness: if a writer sends her spouse a text message saying she is hungry, the spouse has no reaso"
S15-1009,P09-2078,0,0.0256017,"d, we could assume that the writer knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumptio"
S15-1009,P11-1032,0,0.0145471,"er knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumption may be false in c"
S15-1009,C10-2117,1,0.747479,"on-committed belief in the annotations, the heuristic rules (mainly based on the presence of modal auxiliaries) that we added for the purpose of classifying the beliefs (CB, NCB, ROB, NA) did not work reliably in all cases. 4.3 System C System C uses a supervised learning approach to identify tokens denoting the heads of propositions that denote author’s expressed beliefs. It approaches this problem as a 5-way (CB, NCB, ROB, NA, nil) multi-class classification task at the word level. System C is adapted from a previous system which uses an earlier, simpler definition and annotation of belief (Prabhakaran et al., 2010). The system uses lexical and syntactic features for this task, which are extracted using the part-of-speech tags and dependency parses obtained from the Stanford CoreNLP system. In addition to the features described in (Prabhakaran et al., 2010), System C uses a set of new features including features based on a dictionary of hedge-words (Prokofieva and Hirschberg, 2014). The hedge features improved the NCB Fmeasure by around 2.2 percentage points (an overall F-measure improvement of 0.25 percentage points) in experiments conducted on a separate development set. It uses a quadratic kernel SVM"
S15-1009,W12-3807,1,0.915772,"Missing"
S15-1009,J12-2002,0,0.157236,"Missing"
S15-1009,W15-1304,1,0.70443,"ore, the FactBank annotation is basically compatible with ours. Our annotation is much simpler than that of FactBank in order to allow for a quicker annotation. We summarize the main points of simplification here. • We have taken the source always to be the writer. As we will discuss in Section 7.1, we will adopt the FactBank annotation in the next iteration of our annotation. • We do not distinguish between possible and probable; this distinction may be hard to annotate and not too valuable. • We ignore negation. If present, we simply assume it is part of the proposition which is the target. Werner et al. (2015) study the relation between belief and factuality in more detail. They provide an automatic way of mapping the annotations in FactBank to the 4-way distinction of speaker/writer’s belief that we present in this paper. 3.3 Corpus and Annotation Results The annotation effort for this phase of belief annotation for DEFT produced a training corpus of 852,836 words and an evaluation corpus of 100,037 words. All annotated data consisted of English text from discussion forum threads. The discussion forum threads were originally collected for the DARPA BOLT program, and were harvested from a wide vari"
S15-1009,C98-1013,0,\N,Missing
W09-3012,J04-3002,0,0.199612,"Missing"
W09-3012,krestel-etal-2008-minding,0,0.0374878,"Missing"
W09-3012,C08-1101,0,0.0165575,"is is not because we think this is the only interesting information in text, but we do this in order to obtain a manageable annotation in our pilot study. Specifically, we annotate whether the writer intends the reader to interpret a stated proposition as the writer’s strongly held belief, as a proposition which the writer does not believe strongly (but could), or as a proposition towards which the writer has an entirely different cognitive attitude, such as desire or intention. We do not annotate subjectivity (Janyce Wiebe and Martin, 2004; Wilson and Wiebe, 2005), nor opinion (for example: (Somasundaran et al., 2008)): the nature of the proposition (opinion and type of opinion, statement about interior world, external world) is not of interest. Thus, this work is orthogonal to the extensive literature on opinion detection. And we do not annotate truth: real-world (encyclopedic) truth is not relevant. We have three categories: • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. A subcase of committed belief concerns propositions about the future, such as GM wil"
W09-3012,W05-0308,0,0.0144516,"d we are only interested in the writer’s beliefs. This is not because we think this is the only interesting information in text, but we do this in order to obtain a manageable annotation in our pilot study. Specifically, we annotate whether the writer intends the reader to interpret a stated proposition as the writer’s strongly held belief, as a proposition which the writer does not believe strongly (but could), or as a proposition towards which the writer has an entirely different cognitive attitude, such as desire or intention. We do not annotate subjectivity (Janyce Wiebe and Martin, 2004; Wilson and Wiebe, 2005), nor opinion (for example: (Somasundaran et al., 2008)): the nature of the proposition (opinion and type of opinion, statement about interior world, external world) is not of interest. Thus, this work is orthogonal to the extensive literature on opinion detection. And we do not annotate truth: real-world (encyclopedic) truth is not relevant. We have three categories: • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. A subcase of committed belief"
W10-3019,N09-2047,0,0.0249278,"It was noticed that in most cases in which a generic adjective (i.e., a quantifier such as many, several, ...) has a parent which is a plural noun, and this noun has only adjectival daughters, then it is part of a cue phrase. This distinction can be made clear by the below example. cross validation on the whole training set, where as Mallet took only around 30-40 minutes only. 3.2 Features Our approach was to explore the use of deep syntactic features in this tagging task. Deep syntactic features had been proven useful in many similar tagging tasks before. We used the dependency parser MICA (Bangalore et al., 2009) based on Tree Adjoining Grammar (Joshi et al., 1975) to extract these deep syntactic features. We classified the features into three classes Lexical (L), Syntactic (S) and Wordlist-based (W). Lexical features are those which could be found at the token level without using any wordlists or dictionaries and can be extracted without any parsing with relatively high accuracy. For example, isNumeric, which denotes whether the word is a number or alphabetic, is a lexical feature. Under this definition, POS tag will be considered as a lexical feature. Syntactic features of a token access its syntact"
W10-3019,W10-3001,0,0.0609597,"Missing"
W10-3019,P09-2044,0,0.065591,"Missing"
W12-3302,P11-1078,0,0.0324297,"use only meta-data about messages: who sent a message to whom and when. For example, Creamer et al. (2009) find that the response time is an indicator of hierarchical relations; however, they calculate the response time based only on the meta-data, and do not have access to information such as thread structure or message content, which would actually verify that the second email is in fact a response to the first. Using NLP to analyze the content of messages to deduce power relations from written dialog is a relatively new area which has been studied only recently (Strzalkowski et al., 2010; Bramsen et al., 2011; Peterson et al., 2011). Using knowledge of the organizational structure, Bramsen et al. (2011) create two sets of messages: messages sent from a superior to a subordinate, and vice versa. Their task is to determine the direction of power (since all their data, by construction of the corpus, has a power relationship). They approach the task as a text classification problem and build a classifier to determine whether the set of all emails (regardless of thread) between two participants is an instance of up-speak or down-speak. In contrast, I plan to use a complete communication thread as a dat"
W12-3302,W09-3953,0,0.0669721,"Missing"
W12-3302,W11-0711,0,0.0198696,"nd that the response time is an indicator of hierarchical relations; however, they calculate the response time based only on the meta-data, and do not have access to information such as thread structure or message content, which would actually verify that the second email is in fact a response to the first. Using NLP to analyze the content of messages to deduce power relations from written dialog is a relatively new area which has been studied only recently (Strzalkowski et al., 2010; Bramsen et al., 2011; Peterson et al., 2011). Using knowledge of the organizational structure, Bramsen et al. (2011) create two sets of messages: messages sent from a superior to a subordinate, and vice versa. Their task is to determine the direction of power (since all their data, by construction of the corpus, has a power relationship). They approach the task as a text classification problem and build a classifier to determine whether the set of all emails (regardless of thread) between two participants is an instance of up-speak or down-speak. In contrast, I plan to use a complete communication thread as a data unit and capture instances where power is actually manifested. I also plan to study power in a"
W12-3302,C10-2117,1,0.888532,"Missing"
W12-3302,prabhakaran-etal-2012-annotations,1,0.810585,"people who simply respond to questions or perform actions when directed to do so. A thread could have multiple such participants. A person is said to have influence (INFL) if she 1) has credibility in the group, 2) persists in attempting to convince others, even if some disagreement occurs, 3) introduces topics/ideas that others pick up on or support, and 4) is a group participant but not necessarily active in the discussion(s) where others support/credit her. In addition, the influencer’s ideas or language may be adopted by others and others may explicitly recognize influencer’s authority.2 Prabhakaran et al. (2012a) presents more details on annotations of these power relations in the email corpus. Manifestations in Content and Stucture: I used six sets of features to explore manifestations of power: dialog act percentages (DAP), dialog link counts (DLC), positional (PST), verbosity (VRB), lexical (LEX) and overt display of power (ODP). The first four sets of features relate to the whole dialog and its structure while the last two relate to the form and content of individual messages. The email corpus I used has been previously annotated with dialog acts and links by other researchers (Hu et al., 2009)."
W12-3302,N12-1057,1,0.803575,"people who simply respond to questions or perform actions when directed to do so. A thread could have multiple such participants. A person is said to have influence (INFL) if she 1) has credibility in the group, 2) persists in attempting to convince others, even if some disagreement occurs, 3) introduces topics/ideas that others pick up on or support, and 4) is a group participant but not necessarily active in the discussion(s) where others support/credit her. In addition, the influencer’s ideas or language may be adopted by others and others may explicitly recognize influencer’s authority.2 Prabhakaran et al. (2012a) presents more details on annotations of these power relations in the email corpus. Manifestations in Content and Stucture: I used six sets of features to explore manifestations of power: dialog act percentages (DAP), dialog link counts (DLC), positional (PST), verbosity (VRB), lexical (LEX) and overt display of power (ODP). The first four sets of features relate to the whole dialog and its structure while the last two relate to the form and content of individual messages. The email corpus I used has been previously annotated with dialog acts and links by other researchers (Hu et al., 2009)."
W12-3302,C10-1117,0,0.0310412,"ommunication. These studies use only meta-data about messages: who sent a message to whom and when. For example, Creamer et al. (2009) find that the response time is an indicator of hierarchical relations; however, they calculate the response time based only on the meta-data, and do not have access to information such as thread structure or message content, which would actually verify that the second email is in fact a response to the first. Using NLP to analyze the content of messages to deduce power relations from written dialog is a relatively new area which has been studied only recently (Strzalkowski et al., 2010; Bramsen et al., 2011; Peterson et al., 2011). Using knowledge of the organizational structure, Bramsen et al. (2011) create two sets of messages: messages sent from a superior to a subordinate, and vice versa. Their task is to determine the direction of power (since all their data, by construction of the corpus, has a power relationship). They approach the task as a text classification problem and build a classifier to determine whether the set of all emails (regardless of thread) between two participants is an instance of up-speak or down-speak. In contrast, I plan to use a complete communi"
W12-3302,wilson-2008-annotating,0,0.0623999,"Missing"
W12-3807,P11-2049,0,0.0155415,"tion and automatic tagging of the belief modality (i.e., factivity) is described in more detail in (Diab et al., 2009b; Prabhakaran et al., 2010). There has been a considerable amount of interest in modality in the biomedical domain. Negation, uncertainty, and hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical const"
W12-3807,W09-1324,0,0.0193045,"nd hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they often render the prese"
W12-3807,baker-etal-2010-modality,1,0.685998,"Missing"
W12-3807,J12-2006,1,0.846788,"features used to train our modality tagger and presents experiments and results. Section 5 concludes and discusses future work. 58 2 Related Work Previous related work includes TimeML (Sauri et al., 2006), which involves modality annotation on events, and Factbank (Sauri and Pustejovsky, 2009), where event mentions are marked with degree of factuality. Modality is also important in the detection of uncertainty and hedging. The CoNLL shared task in 2010 (Farkas et al., 2010) deals with automatic detection of uncertainty and hedging in Wikipedia and biomedical sentences. Baker et al. (2010) and Baker et al. (2012) analyze a set of eight modalities which include belief, require and permit, in addition to the five modalities we focus on in this paper. They built a rule-based modality tagger using a semi-automatic approach to create rules. This earlier work differs from the work described in this paper in that the our emphasis is on the creation of an automatic modality tagger using machine learning techniques. Note that the annotation and automatic tagging of the belief modality (i.e., factivity) is described in more detail in (Diab et al., 2009b; Prabhakaran et al., 2010). There has been a considerable"
W12-3807,W09-3012,1,0.821042,"y of the information. Did the speaker 57 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 57–64, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence? Sentiment deals with a speaker’s positive or negative feelings toward an event, state, or proposition. In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (Diab et al., 2009b; Prabhakaran et al., 2010), and we leave other modalities to future work. • Ability: can H do P? • Effort: does H try to do P? • Intention: does H intend P? • Success: does H succeed in P? • Want: does H want P? We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs). One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences. Baker et al. (2010) created a modality tagg"
W12-3807,P03-1004,0,0.0266465,"removed the ones which did not in fact have a modality. In the remaining sentences (94 sentences), our expert annotated the target predicate. We refer to this as the Gold dataset in this paper. The MTurk and Gold datasets differ in terms of genres as well as annotators (Turker vs. Expert). The distribution of modalities in both MTurk and Gold annotations are given in Table 2. 4.2 Gold Ability Table 1: For each modality, the number of sentences returned by the simple tagger that we posted on MTurk. 4 MTurk Table 2: Frequency of Modalities modalities in context. For tagging, we used the Yamcha (Kudo and Matsumoto, 2003) sequence labeling system which uses the SVMlight (Joachims, 1999) package for classification. We used One versus All method for multi-class classification on a quadratic kernel with a C value of 1. We report recall and precision on word tokens in our corpus for each modality. We also report Fβ=1 (F)-measure as the harmonic mean between (P)recision and (R)ecall. 4.3 Features We used lexical features at the token level which can be extracted without any parsing with relatively high accuracy. We use the term context width to denote the window of tokens whose features are considered for predictin"
W12-3807,W09-1304,0,0.0189282,"ain. Negation, uncertainty, and hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they"
W12-3807,Y05-1014,0,0.0194818,", absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to specifically handle modal constructions, while our modal annotation approach is a part of a full translation system. The textual entailment literature includes moda"
W12-3807,W06-3907,0,0.0243548,"of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to specifically handle modal constructions, while our modal annotation approach is a part of a full translation system. The textual entailment literature includes modality annotation schemes. Identifying modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al. (2007) include polarity based rules and negation and modality annotation rules. The polarity rules are based on an independent polarity lexicon (Nairn et al., 2006). The annotation rules for negation and modality of predicates are based on identifying modal verbs, as well as conditional sentences and modal adverbials. The authors read the modality off parse trees directly using simple structural rules for modifiers. 3 Constructing Modality Training Data In this section, we will discuss the procedure we followed to construct the training data for building the automatic modality tagger. In a pilot study, we obtained and ran the modality tagger described in (Baker et al., 2010) on the English side of the Urdu-English LDC language pack.2 We randomly selected"
W12-3807,C10-2117,1,0.929409,". Did the speaker 57 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 57–64, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence? Sentiment deals with a speaker’s positive or negative feelings toward an event, state, or proposition. In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (Diab et al., 2009b; Prabhakaran et al., 2010), and we leave other modalities to future work. • Ability: can H do P? • Effort: does H try to do P? • Intention: does H intend P? • Success: does H succeed in P? • Want: does H want P? We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs). One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences. Baker et al. (2010) created a modality tagger by using a semiautomatic"
W12-3807,C94-1018,0,0.029939,"Missing"
W12-3807,W08-0606,0,\N,Missing
W14-2710,W12-2105,1,0.567166,"d Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. Figure 2: Pearson Correlations for Topical Features We obtained significant strong positive correlation for TS Attempt# and TS AttemptAfterMod#. However, the normalized measure TS Attempt#N did not h"
W14-2710,I13-1025,1,0.794707,"and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. Figure 2: Pearson Correlations for Topical Features We obtained significant strong positive correlation for TS Atte"
W14-2710,P11-1078,0,0.300702,"hers have studied notions of control and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. Figure 2: Pearson Correlations for Topical Features We obtained signi"
W14-2710,I13-1042,1,0.883544,"t candidates’ power correlates with the distribution of topics they speak about in the debates. They found that when candidates have more power, they speak significantly more about certain topics (e.g., economy) and less about certain other topics (e.g., energy). However, these findings relate to the specific election cycle they analyzed and will not carry over to all political debates in general. A topical dimension with broader relevance is how topics change during the course of an interaction (e.g., who introduces more topics, who attempts to shift topics etc.). For instance, Nguyen et al. (2013) found that topic shifts within an interaction are correlated with the role a participant plays in it (e.g., being a moderator). They also analyzed US presidential debates, but with the objective of validating a topic segmentation method they proposed earlier (Nguyen et al., 2012). They do not study the topic shifting tendencies among the candidates in relation to their power differences. In this paper, we bring these two ideas together. We analyze the 2012 Republican presidential debates, modeling the power of a candidate based on poll scores as proposed by Prabhakaran et al. (2013a) and inve"
W14-2710,D13-1010,0,0.111378,"ower, modeled after their poll scores, affects how often he/she attempts to shift topics and whether he/she succeeds. We ensure the validity of topic shifts by confirming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction. 1 Introduction Analyzing political speech has gathered great interest within the NLP community. Researchers have analyzed political text to identify markers of persuasion (Guerini et al., 2008), predict voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011), and detect ideological positions (Sim et al., 2013). Studies have also looked into how personal attributes of political personalities such as charisma, confidence and power affect how they interact (Rosenberg and Hirschberg, 2009; Prabhakaran et al., 2013b). Our work belongs to this genre of studies. We analyze how a presidential candidate’s power, modeled after his/her relative poll standings, affect the dynamics of topic shifts during the course of a presidential debate. 2 Motivation In early work on correlating personal attributes to political speech, Rosenberg and Hirschberg (2009) analyzed speech transcripts in the context of 2004 Democra"
W14-2710,C12-1155,0,0.086026,"cation” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. Figure 2: Pearson Correlations for Topical Features We obtained significant strong positive correlation for TS Attempt# and TS AttemptAfterMod#. However, the normalized measure TS Attempt#N did not have any significant correlation, suggesting that the"
W14-2710,W06-1639,0,0.328428,"perform this study on the US presidential debates and show that a candidate’s power, modeled after their poll scores, affects how often he/she attempts to shift topics and whether he/she succeeds. We ensure the validity of topic shifts by confirming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction. 1 Introduction Analyzing political speech has gathered great interest within the NLP community. Researchers have analyzed political text to identify markers of persuasion (Guerini et al., 2008), predict voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011), and detect ideological positions (Sim et al., 2013). Studies have also looked into how personal attributes of political personalities such as charisma, confidence and power affect how they interact (Rosenberg and Hirschberg, 2009; Prabhakaran et al., 2013b). Our work belongs to this genre of studies. We analyze how a presidential candidate’s power, modeled after his/her relative poll standings, affect the dynamics of topic shifts during the course of a presidential debate. 2 Motivation In early work on correlating personal attributes to political speech, Rosenberg an"
W14-2710,P12-1009,0,0.270053,"ever, these findings relate to the specific election cycle they analyzed and will not carry over to all political debates in general. A topical dimension with broader relevance is how topics change during the course of an interaction (e.g., who introduces more topics, who attempts to shift topics etc.). For instance, Nguyen et al. (2013) found that topic shifts within an interaction are correlated with the role a participant plays in it (e.g., being a moderator). They also analyzed US presidential debates, but with the objective of validating a topic segmentation method they proposed earlier (Nguyen et al., 2012). They do not study the topic shifting tendencies among the candidates in relation to their power differences. In this paper, we bring these two ideas together. We analyze the 2012 Republican presidential debates, modeling the power of a candidate based on poll scores as proposed by Prabhakaran et al. (2013a) and investigate various features that capture the topical dynamics in the debates. We show that the power affects how often candidates atIn this paper, we investigate how topic dynamics during the course of an interaction correlate with the power differences between its participants. We p"
W14-2710,P90-1010,0,0.595258,"he instances of topic shift attempts by the candidates when responding to a question from the moderator (TS AttemptAfterMod# and 6 Analysis and Results We performed a correlation analysis on the features described in the previous section with respect to each candidate against the power he/she had at the time of the debate (based on recent poll scores). Figure 2 shows the Pearson’s product correlation between each topical feature and candidate’s power. Dark bars denote statistically significant (p &lt; 0.05) features. 80 nity, researchers have studied notions of control and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion foru"
W15-1304,W09-3012,1,0.804462,"(SW) level of commitment to a given proposition, which could be their own or a reported proposition. Modeling this type of knowledge explicitly is useful in determining an SWs cognitive state, also referred to as person’s private state (Wiebe et al., 2005). Wiebe et al. (2005) use the definition of (Quirk et al., 1985), who defines a private state to be an “internal (state) Initial work addressed the task of automatically identifying LCB of the SW. Approaches to date have relied on supervised models dependent on manually annotated data. There are two standard annotated corpora, the LU corpus (Diab et al., 2009) and FactBank (Saur´ı and Pustejovsky, 2009). Though in effect aiming for the same objective, both corpora use different terminology, different annotation standards, and they cover different genres. Previous studies performed on these corpora were conducted independently. In this work, we explore both corpora systematically and investigate their respective proposed tag sets. We experiment with multiple machine learning algorithms, varying the tag sets as we go along. Our goal is to build an automatic LCB tagger that is robust in a multi-genre context. Eventually we aim to adapt this tagger to"
W15-1304,P14-5010,0,0.0128785,"Missing"
W15-1304,W09-1501,0,0.0661361,"Missing"
W15-1304,C10-2117,1,0.948009,"s from the LU corpus in two major respects (other than the granularity in which they capture annotations): 1) FactBank is roughly four times the size of the LU corpus, and 2) FactBank is more homogeneous in terms of genre than the LU corpus as it consists primarily of newswire. In this paper, we unify the factuality annotations in Factback and the level of committed belief annotations present in the LU corpus to a 4-way committed-belief distinction.2 2 For an additional discussion of the relation between factuality and belief, see (Prabhakaran et al., 2015) 3 Approach Following previous work (Prabhakaran et al., 2010), we adopt a supervised approach to the LCB problem. We experiment with the two available manually annotated corpora, the LU and FB corpora. Going beyond previous approaches to the problem reported in the literature, our goal is to create a robust LCB system while gaining a deeper understanding of the phenomenon of LCB as an expressed modality by systematically teasing apart the different factors that affect performance. 3.1 Annotation Transformations The NCB category of the LU tagging scheme captures two different notions: that of uncertainty of the speaker/writer and that of belief being att"
W15-1304,S15-1009,1,0.70443,"o model LCB. From a computational perspective, FactBank differs from the LU corpus in two major respects (other than the granularity in which they capture annotations): 1) FactBank is roughly four times the size of the LU corpus, and 2) FactBank is more homogeneous in terms of genre than the LU corpus as it consists primarily of newswire. In this paper, we unify the factuality annotations in Factback and the level of committed belief annotations present in the LU corpus to a 4-way committed-belief distinction.2 2 For an additional discussion of the relation between factuality and belief, see (Prabhakaran et al., 2015) 3 Approach Following previous work (Prabhakaran et al., 2010), we adopt a supervised approach to the LCB problem. We experiment with the two available manually annotated corpora, the LU and FB corpora. Going beyond previous approaches to the problem reported in the literature, our goal is to create a robust LCB system while gaining a deeper understanding of the phenomenon of LCB as an expressed modality by systematically teasing apart the different factors that affect performance. 3.1 Annotation Transformations The NCB category of the LU tagging scheme captures two different notions: that of"
W15-1304,bethard-etal-2014-cleartk,0,\N,Missing
W18-4511,P12-2032,0,0.016951,"ation (either superior or subordinate) based on the organizational hierarchy. As in (Prabhakaran and Rambow, 2014), we exclude pairs of employees who are peers, and we use the same train-dev-test splits so our results are comparable. Grouped: Here, we group all emails A sent to B across all threads in the corpus, and vice versa, and use these sets of emails to predict the power relation between A and B. This formulation is similar those in (Bramsen et al., 2011; Gilbert, 2012), but our results are not directly comparable since, unlike them, we rely on the ground truth of power relations from (Agarwal et al., 2012); however, we created an SVM model that uses word-ngram features similar to theirs as a baseline to our proposed neural architectures. 3 Methods The inputs to our models take on two forms: Lexical features: We represent each email as a series of tokenized words, each of which is represented by a 100-dimensional GloVe vector pre-trained on Wikipedia and Gigaword (Pennington et al., 2014). We cap the email length at a maximum of 200 words. Non-lexical features: We incorporate the structural non-lexical features identified as significant by Prabhakaran and Rambow (2014) for the Grouped problem fo"
W18-4511,P11-1078,0,0.469738,"nd NLP to not only understand language, but also to understand the people who speak it and the social relations between them. Social power structures are ubiquitous in human interactions, and since power is often reflected through language, computational research at the intersection of language and power has gained interest recently. This research has been applied to a wide array of domains such as Wikipedia talk pages (Strzalkowski et al., 2010; Taylor et al., 2012; Danescu-Niculescu-Mizil et al., 2012; Swayamdipta and Rambow, 2012), blogs (Rosenthal, 2014) as well as workplace interactions (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran, 2015). The corporate environment is one social context in which power dynamics have a clearly defined structure and shape the interactions between individuals, making it an interesting case study on how language and power interact. Organizations stand to benefit greatly from being able to detect power dynamics within their internal interactions, in order to address disparities and ensure inclusive and productive workplaces. For instance, (Cortina et al., 2001) reports that women are more likely to experience incivility, often from superiors. It has also been shown"
W18-4511,D14-1162,0,0.0918213,"ion between A and B. This formulation is similar those in (Bramsen et al., 2011; Gilbert, 2012), but our results are not directly comparable since, unlike them, we rely on the ground truth of power relations from (Agarwal et al., 2012); however, we created an SVM model that uses word-ngram features similar to theirs as a baseline to our proposed neural architectures. 3 Methods The inputs to our models take on two forms: Lexical features: We represent each email as a series of tokenized words, each of which is represented by a 100-dimensional GloVe vector pre-trained on Wikipedia and Gigaword (Pennington et al., 2014). We cap the email length at a maximum of 200 words. Non-lexical features: We incorporate the structural non-lexical features identified as significant by Prabhakaran and Rambow (2014) for the Grouped problem formulation. We used (1) the average number of recipients and (2) the average number of words in each email for each individual; these features were concatenated into a single input vector. We investigate the following three network architectures, in increasing order of complexity, to train our model: This work is licensed under a Creative Commons Attribution 4.0 International License. cr"
W18-4511,I13-1025,1,0.823063,"cs within their internal interactions, in order to address disparities and ensure inclusive and productive workplaces. For instance, (Cortina et al., 2001) reports that women are more likely to experience incivility, often from superiors. It has also been shown that incivility may breed more incivility (Harold and Holtz, 2015), and that it can lead to increased stress and lack of commitment (Miner et al., 2012). Prior work has investigated the use of NLP techniques to study manifestations of different types of power using the Enron email corpus (Diesner et al., 2005; Prabhakaran et al., 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014). While early work (Bramsen et al., 2011; Gilbert, 2012) focused on surface level lexical features aggregated at corpus level, more recent work has looked into the thread structure of emails as well (Prabhakaran and Rambow, 2014). However, both (Bramsen et al., 2011; Gilbert, 2012) and (Prabhakaran and Rambow, 2014) group all messages sent by an individual to another individual (at the corpus-level and at the thread-level, respectively) and rely on word-ngram * Authors (listed in alphabetical order) contributed equally. 97 Proceedings of Workshop on Computational"
W18-4511,P14-2056,1,0.869832,"actions, in order to address disparities and ensure inclusive and productive workplaces. For instance, (Cortina et al., 2001) reports that women are more likely to experience incivility, often from superiors. It has also been shown that incivility may breed more incivility (Harold and Holtz, 2015), and that it can lead to increased stress and lack of commitment (Miner et al., 2012). Prior work has investigated the use of NLP techniques to study manifestations of different types of power using the Enron email corpus (Diesner et al., 2005; Prabhakaran et al., 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014). While early work (Bramsen et al., 2011; Gilbert, 2012) focused on surface level lexical features aggregated at corpus level, more recent work has looked into the thread structure of emails as well (Prabhakaran and Rambow, 2014). However, both (Bramsen et al., 2011; Gilbert, 2012) and (Prabhakaran and Rambow, 2014) group all messages sent by an individual to another individual (at the corpus-level and at the thread-level, respectively) and rely on word-ngram * Authors (listed in alphabetical order) contributed equally. 97 Proceedings of Workshop on Computational Linguistics for Cultural Herit"
W18-4511,C12-1138,1,0.798699,"ble to detect power dynamics within their internal interactions, in order to address disparities and ensure inclusive and productive workplaces. For instance, (Cortina et al., 2001) reports that women are more likely to experience incivility, often from superiors. It has also been shown that incivility may breed more incivility (Harold and Holtz, 2015), and that it can lead to increased stress and lack of commitment (Miner et al., 2012). Prior work has investigated the use of NLP techniques to study manifestations of different types of power using the Enron email corpus (Diesner et al., 2005; Prabhakaran et al., 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014). While early work (Bramsen et al., 2011; Gilbert, 2012) focused on surface level lexical features aggregated at corpus level, more recent work has looked into the thread structure of emails as well (Prabhakaran and Rambow, 2014). However, both (Bramsen et al., 2011; Gilbert, 2012) and (Prabhakaran and Rambow, 2014) group all messages sent by an individual to another individual (at the corpus-level and at the thread-level, respectively) and rely on word-ngram * Authors (listed in alphabetical order) contributed equally. 97 Proceeding"
W18-4511,C10-1117,0,0.0343585,"Introduction With the availability and abundance of linguistic data that captures different avenues of human social interactions, there is an unprecedented opportunity to expand NLP to not only understand language, but also to understand the people who speak it and the social relations between them. Social power structures are ubiquitous in human interactions, and since power is often reflected through language, computational research at the intersection of language and power has gained interest recently. This research has been applied to a wide array of domains such as Wikipedia talk pages (Strzalkowski et al., 2010; Taylor et al., 2012; Danescu-Niculescu-Mizil et al., 2012; Swayamdipta and Rambow, 2012), blogs (Rosenthal, 2014) as well as workplace interactions (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran, 2015). The corporate environment is one social context in which power dynamics have a clearly defined structure and shape the interactions between individuals, making it an interesting case study on how language and power interact. Organizations stand to benefit greatly from being able to detect power dynamics within their internal interactions, in order to address disparities and ensure inclusiv"
