2021.eacl-main.273,Summarising Historical Text in Modern Languages,2021,-1,-1,4,0,3825,xutan peng,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"We introduce the task of historical text summarisation, where documents in historical forms of a language are summarised in the corresponding modern language. This is a fundamentally important routine to historians and digital humanities researchers but has never been automated. We compile a high-quality gold-standard text summarisation dataset, which consists of historical German and Chinese news from hundreds of years ago summarised in modern German or Chinese. Based on cross-lingual transfer learning techniques, we propose a summarisation model that can be trained even with no cross-lingual (historical to modern) parallel data, and further benchmark it against state-of-the-art algorithms. We report automatic and human evaluations that distinguish the historic to modern language summarisation task from standard cross-lingual summarisation (i.e., modern to modern language), highlight the distinctness and value of our dataset, and demonstrate that our transfer learning approach outperforms standard cross-lingual benchmarks on this task."
W18-6548,Generating Summaries of Sets of Consumer Products: Learning from Experiments,2018,0,0,4,0,27686,kittipitch kuptavanich,Proceedings of the 11th International Conference on Natural Language Generation,0,"We explored the task of creating a textual summary describing a large set of objects characterised by a small number of features using an e-commerce dataset. When a set of consumer products is large and varied, it can be difficult for a consumer to understand how the products in the set differ; consequently, it can be challenging to choose the most suitable product from the set. To assist consumers, we generated high-level summaries of product sets. Two generation algorithms are presented, discussed, and evaluated with human users. Our evaluation results suggest a positive contribution to consumers{'} understanding of the domain."
W16-6601,Summarising News Stories for Children,2016,21,3,2,0,33308,iain macdonald,Proceedings of the 9th International Natural Language Generation conference,0,"This paper proposes a system to automatically summarise news articles in a manner suitable for children by deriving and combining statistical ratings for how important, positively oriented and easy to read each sentence is. Our results demonstrate that this approach succeeds in generating summaries that are suitable for children, and that there is further scope for combining this extractive approach with abstractive methods used in text implification."
W16-2807,Scrutable Feature Sets for Stance Classification,2016,13,2,2,0,17125,angrosh mandya,Proceedings of the Third Workshop on Argument Mining ({A}rg{M}ining2016),0,"This paper describes and evaluates a novel feature set for stance classification of argumentative texts; i.e. deciding whether a post by a user is for or against the issue being debated. We model the debate both as attitude bearing features, including a set of automatically acquired xe2x80x98topic termsxe2x80x99 associated with a Distributional Lexical Model (DLM) that captures the writerxe2x80x99s attitude towards the topic term, and as dependency features that represent the points being made in the debate. The stance of the text towards the issue being debated is then learnt in a supervised framework as a function of these features. The main advantage of our feature set is that it is scrutable: The reasons for a classification can be explained to a human user in natural language. We also report that our method outperforms previous approaches to stance classification as well as a range of baselines based on sentiment analysis and topic-sentiment analysis."
W16-2816,Summarising the points made in online political debates,2016,17,5,2,0,33843,charlie egan,Proceedings of the Third Workshop on Argument Mining ({A}rg{M}ining2016),0,"Online communities host growing numbers of discussions amongst large groups of participants on all manner of topics. This user-generated content contains millions of statements of opinions and ideas. We propose an abstractive approach to summarize such argumentative discussions, making key content accessible through xe2x80x98pointxe2x80x99 extraction, where a point is a verb and its syntactic arguments. Our approach uses both dependency parse information and verb case frames to identify and extract valid points, and generates an abstractive summary that discusses the key points being made in the debate. We performed a human evaluation of our approach using a corpus of online political debates and report significant improvements over a high-performing extractive summarizer."
W15-4726,Creating Textual Driver Feedback from Telemetric Data,2015,28,3,3,0,2852,daniel braun,Proceedings of the 15th {E}uropean Workshop on Natural Language Generation ({ENLG}),0,"Usage based car insurances, which use sensors to track driver behaviour, are enjoying growing popularity. Although the data collected by these insurances could provide detailed feedback about the driving style, this information is usually kept away from the driver and is used only to calculate insurance premiums. In this paper, we explored the possibility of providing drivers with textual feedback based on telemetric data in order to improve individual driving, but also general road safety. We report that textual feedback generated through NLG was preferred to non-textual summaries currently popular in the field and specifically was better at giving users a concrete idea of how to adapt their driving."
W14-4404,Text simplification using synchronous dependency grammars: Generalising automatically harvested rules,2014,20,8,2,0,38410,mandya angrosh,Proceedings of the 8th International Natural Language Generation Conference ({INLG}),0,"We present an approach to text simplification based on synchronous dependency grammars. Our main contributions in this work are (a) a study of how automatically derived lexical simplification rules can be generalised to enable their application in new contexts without introducing errors, and (b) an evaluation of our hybrid system that combines a large set of automatically acquired rules with a small set of hand-crafted rules for common syntactic simplification. Our evaluation shows significant improvements over the state of the art, with scores comparable to human simplifications."
E14-1076,Hybrid text simplification using synchronous dependency grammars with hand-written and automatically harvested rules,2014,25,35,1,1,10906,advaith siddharthan,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present an approach to text simplification based on synchronous dependency grammars. The higher level of abstraction afforded by dependency representations allows for a linguistically sound treatment of complex constructs requiring reordering and morphological change, such as conversion of passive voice to active. We present a synchronous grammar formalism in which it is easy to write rules by hand and also acquire them automatically from dependency parses of aligned English and Simple English sentences. The grammar formalism is optimised for monolingual translation in that it reuses ordering information from the source sentence where appropriate. We demonstrate the superiority of our approach over a leading contemporary system based on quasi-synchronous tree substitution grammars, both in terms of expressivity and performance."
C14-1188,Lexico-syntactic text simplification and compression with typed dependencies,2014,26,11,3,0,38410,mandya angrosh,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We describe two systems for text simplification using typed dependency structures, one that performs lexical and syntactic simplification, and another that performs sentence compression optimised to satisfy global text constraints such as lexical density, the ratio of difficult words, and text length. We report a substantial evaluation that demonstrates the superiority of our systems, individually and in combination, over the state of the art, and also report a comprehension based evaluation of contemporary automatic text simplification systems with target non-native readers."
P13-4029,{T}ag2{B}log: Narrative Generation from Satellite Tag Data,2013,14,10,2,0,41376,kapila ponnamperuma,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"The aim of the Tag2Blog system is to bring satellite tagged wild animals xe2x80x9cto lifexe2x80x9d through narratives that place their movements in an ecological context. Our motivation is to use such automatically generated texts to enhance public engagement with a specific species reintroduction programme, although the protocols developed here can be applied to any animal or other movement study that involves signal data from tags. We are working with one of the largest nature conservation charities in Europe in this regard, focusing on a single species, the red kite. We describe a system that interprets a sequence of locational fixes obtained from a satellite tagged individual, and constructs a story around its use of the landscape."
W12-2203,Offline Sentence Processing Measures for testing Readability with Users,2012,21,7,1,1,10906,advaith siddharthan,Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations,0,"While there has been much work on computational models to predict readability based on the lexical, syntactic and discourse properties of a text, there are also interesting open questions about how computer generated text should be evaluated with target populations. In this paper, we compare two offline methods for evaluating sentence quality, magnitude estimation of acceptability judgements and sentence recall. These methods differ in the extent to which they can differentiate between surface level fluency and deeper comprehension issues. We find, most importantly, that the two correlate. Magnitude estimation can be run on the web without supervision, and the results can be analysed automatically. The sentence recall methodology is more resource intensive, but allows us to tease apart the fluency and comprehension issues that arise."
W12-1520,Blogging birds: Generating narratives about reintroduced species to promote public engagement,2012,11,6,1,1,10906,advaith siddharthan,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"This paper proposes the use of NLG to enhance public engagement during the course of species reintroductions. We examine whether ecological insights can be effectively communicated through blogs about satellite-tagged individuals, and whether such blogs can help create a positive perception of the species in readers' minds, a requirement for successful reintroduction. We then discuss the implications for NLG systems that generate blogs from satellite-tag data."
C12-1020,Natural Language Generation for Nature Conservation: Automating Feedback to Help Volunteers Identify Bumblebee Species,2012,16,10,2,0,43734,steven blake,Proceedings of {COLING} 2012,0,"This paper explores the use of Natural Language Generation (NLG) for facilitating the provision of feedback to citizen scientists in the context of a nature conservation programme, BEEWATCH. BEEWATCH aims to capture the distribution of bumblebees, an ecologically and economically important species group in decline, across the UK and beyond. The NLG module described here uses comparisons of visual features of bumblebee species as well as contextual information to improve the citizen scientistsxe2x80x99 identification skills and to keep them motivated. We report studies that show a positive effect of NLG feedback on accuracy of bumblebee identification and on volunteer retention, along with a positive appraisal of the generated feedback."
W11-2802,Text Simplification using Typed Dependencies: A Comparision of the Robustness of Different Generation Strategies,2011,28,41,1,1,10906,advaith siddharthan,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"We present a framework for text simplification based on applying transformation rules to a typed dependency representation produced by the Stanford parser. We test two approaches to regeneration from typed dependencies: (a) gen-light, where the transformed dependency graphs are linearised using the word order and morphology of the original sentence, with any changes coded into the transformation rules, and (b) gen-heavy, where the Stanford dependencies are reduced to a DSyntS representation and sentences are generating formally using the RealPro surface realiser. The main contribution of this paper is to compare the robustness of these approaches in the presence of parsing errors, using both a single parse and an n-best parse setting in an overgenerate and rank approach. We find that the gen-light approach is robust to parser error, particularly in the n-best parse setting. On the other hand, parsing errors cause the realiser in the gen-heavy approach to order words and phrases in ways that are disliked by our evaluators."
W11-2824,Investigation into Human Preference between Common and Unambiguous Lexical Substitutions,2011,10,2,2,0,44154,andrew walker,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"We present a study that investigates that factors that determine what makes a good lexical substitution. We begin by observing that there is a correlation between the corpus frequency of words and the number of WordNet senses they have, and hypothesise that readers might prefer common, but more ambiguous words over less ambiguous but also less common ones. We identify four properties of a word that determine whether it is a suitable substitution in a given context, and ask volunteers to rank their preferences between two common but ambiguous lexical substitutions, and two uncommon but also unambiguous ones. Preliminary results suggest a slight preference towards the unambiguous."
J11-4007,Information Status Distinctions and Referring Expressions: An Empirical Study of References to People in News Summaries,2011,68,30,1,1,10906,advaith siddharthan,Computational Linguistics,0,"Although there has been much theoretical work on using various information status distinctions to explain the form of references in written text, there have been few studies that attempt to automatically learn these distinctions for generating references in the context of computer-regenerated text. In this article, we present a model for generating references to people in news summaries that incorporates insights from both theory and a corpus analysis of human written summaries. In particular, our model captures how two properties of a person referred to in the summary-familiarity to the reader and global salience in the news story-affect the content and form of the initial reference to that person in a summary. We demonstrate that these two distinctions can be learned from a typical input for multi-document summarization and that they can be used to make regeneration decisions that improve the quality of extractive summaries."
W10-4213,Complex Lexico-syntactic Reformulation of Sentences Using Typed Dependency Representations,2010,21,22,1,1,10906,advaith siddharthan,Proceedings of the 6th International Natural Language Generation Conference,0,"We present a framework for reformulating sentences by applying transfer rules on a typed dependency representation. We specify a list of operations that the framework needs to support and argue that typed dependency structures are currently the most suitable formalism for complex lexico-syntactic paraphrasing. We demonstrate our approach by reformulating sentences expressing the discourse relation of causation using four lexico-syntactic discourse markers -- cause as a verb and as a noun, because as a conjunction and because of as a preposition."
N10-2001,{C}amtology: Intelligent Information Access for Science,2010,6,0,5,0,21810,ted briscoe,Proceedings of the {NAACL} {HLT} 2010 Demonstration Session,0,"We describe a novel semantic search engine for scientific literature. The Camtology system allows for sentence-level searches of PDF files and combines text and image searches, thus facilitating the retrieval of information present in tables and figures. It allows the user to generate complex queries for search terms that are related through particular grammatical/semantic relations in an intuitive manner. The system uses Grid processing to parallelise the analysis of large numbers of papers."
N10-1144,Reformulating Discourse Connectives for Non-Expert Readers,2010,21,12,1,1,10906,advaith siddharthan,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,In this paper we report a behavioural experiment documenting that different lexico-syntactic formulations of the discourse relation of causation are deemed more or less acceptable by different categories of readers. We further report promising results for automatically selecting the formulation that is most appropriate for a given category of reader using supervised learning. This investigation is embedded within a longer term research agenda aimed at summarising scientific writing for lay readers using appropriate paraphrasing.
liakata-etal-2010-corpora,Corpora for the Conceptualisation and Zoning of Scientific Papers,2010,17,76,3,0,3114,maria liakata,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present two complementary annotation schemes for sentence based annotation of full scientific papers, CoreSC and AZ-II, applied to primary research articles in chemistry. AZ-II is the extension of AZ for chemistry papers. AZ has been shown to have been reliably annotated by independent human coders and useful for various information access tasks. Like AZ, AZ-II follows the rhetorical structure of a scientific paper and the knowledge claims made by the authors. The CoreSC scheme takes a different view of scientific papers, treating them as the humanly readable representations of scientific investigations. It seeks to retrieve the structure of the investigation from the paper as generic high-level Core Scientific Concepts (CoreSC). CoreSCs have been annotated by 16 chemistry experts over a total of 265 full papers in physical chemistry and biochemistry. We describe the differences and similarities between the two schemes in detail and present the two corpora produced using each scheme. There are 36 shared papers in the corpora, which allows us to quantitatively compare aspects of the annotation schemes. We show the correlation between the two schemes, their strengths and weeknesses and discuss the benefits of combining a rhetorical based analysis of the papers with a content-based one."
D09-1155,Towards Domain-Independent Argumentative Zoning: Evidence from Chemistry and Computational Linguistics,2009,20,98,2,0,3813,simone teufel,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Argumentative Zoning (AZ) is an analysis of the argumentative and rhetorical structure of a scientific paper. It has been shown to be reliably used by independent human coders, and has proven useful for various information access tasks. Annotation experiments have however so far been restricted to one discipline, computational linguistics (CL). Here, we present a more informative AZ scheme with 15 categories in place of the original 7, and show that it can be applied to the life sciences as well as to CL. We use a domain expert to encode basic knowledge about the subject (such as terminology and domain specific rules for individual categories) as part of the annotation guidelines. Our results show that non-expert human coders can then use these guidelines to reliably annotate this scheme in two domains, chemistry and computational linguistics."
P08-4002,Generating Research Websites Using Summarisation Techniques,2008,6,0,1,1,10906,advaith siddharthan,Proceedings of the {ACL}-08: {HLT} Demo Session,0,"We describe an application that generates web pages for research institutions by summarising terms extracted from individual researchers' publication titles. Our online demo covers all researchers and research groups in the Computer Laboratory, University of Cambridge. We also present a novel visualisation interface for browsing collaborations."
rupp-etal-2008-language,Language Resources and Chemical Informatics,2008,10,1,5,0,48159,cj rupp,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Chemistry research papers are a primary source of information about chemistry, as in any scientific field. The presentation of the data is, predominantly, unstructured information, and so not immediately susceptible to processes developed within chemical informatics for carrying out chemistry research by information processing techniques. At one level, extracting the relevant information from research papers is a text mining task, requiring both extensive language resources and specialised knowledge of the subject domain. However, the papers also encode information about the way the research is conducted and the structure of the field itself. Applying language technology to research papers in chemistry can facilitate eScience on several different levels. The SciBorg project sets out to provide an extensive, analysed corpus of published chemistry research. This relies on the cooperation of several journal publishers to provide papers in an appropriate form. The work is carried out as a collaboration involving the Computer Laboratory, Chemistry Department and eScience Centre at Cambridge University, and is funded under the UK eScience programme."
N07-1040,"Whose Idea Was This, and Why Does it Matter? Attributing Scientific Work to Citations",2007,13,41,1,1,10906,advaith siddharthan,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"Scientific papers revolve around citations, and for many discourse level tasks one needs to know whose work is being talked about at any point in the discourse. In this paper, we introduce the scientific attribution task, which links different linguistic expressions to citations. We discuss the suitability of different evaluation metrics and evaluate our classification approach to deciding attribution both intrinsically and in an extrinsic evaluation where information about scientific attribution is shown to improve performance on Argumentative Zoning, a rhetorical classification task."
2007.mtsummit-ucnlg.18,"Evaluating an open-domain {GRE} algorithm on closed domains system {ID}s: {CAM}-{B}, {CAM}-{T}, {CAM}-{BU} and {CAM}-{TU}",2007,-1,-1,1,1,10906,advaith siddharthan,Proceedings of the Workshop on Using corpora for natural language generation,0,None
W06-1613,Automatic classification of citation function,2006,29,234,2,0,3813,simone teufel,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"Citation function is defined as the author's reason for citing a given paper (e.g. acknowledgement of the use of the cited method). The automatic recognition of the rhetorical function of citations in scientific text has many applications, from improvement of impact factor calculations to text summarisation and more informative citation indexers. We show that our annotation scheme for citation function is reliable, and present a supervised machine learning framework to automatically classify citation function, using both shallow and linguistically-inspired features. We find, amongst other things, a strong relationship between citation function and sentiment classification."
W06-1312,An annotation scheme for citation function,2006,-1,-1,2,0,3813,simone teufel,Proceedings of the 7th {SIG}dial Workshop on Discourse and Dialogue,0,None
rambow-etal-2006-parallel,Parallel Syntactic Annotation of Multiple Languages,2006,12,10,12,0,1354,owen rambow,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes an effort to investigate the incrementally deepening development of an interlingua notation, validated by human annotation of texts in English plus six languages. We begin with deep syntactic annotation, and in this paper present a series of annotation manuals for six different languages at the deep-syntactic level of representation. Many syntactic differences between languages are removed in the proposed syntactic annotation, making them useful resources for multilingual NLP projects with semantic components."
H05-1005,Improving Multilingual Summarization: Using Redundancy in the Input to Correct {MT} errors,2005,14,7,1,1,10906,advaith siddharthan,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries. We consider the case of multi-document summarization, where the input documents are in Arabic, and the output summary is in English. Typically, information that makes it to a summary appears in many different lexical-syntactic forms in the input documents. Further, the use of multiple machine translation systems provides yet more redundancy, yielding different ways to realize that information in English. We demonstrate how errors in the machine translations of the input Arabic documents can be corrected by identifying and generating from such redundancy, focusing on noun phrases."
H05-1031,Automatically Learning Cognitive Status for Multi-Document Summarization of Newswire,2005,23,14,2,0,8333,ani nenkova,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Machine summaries can be improved by using knowledge about the cognitive status of news article referents. In this paper, we present an approach to automatically acquiring distinctions in cognitive status using machine learning over the forms of referring expressions appearing in the input. We focus on modeling references to people, both because news often revolve around people and because existing natural language tools for named entity identification are reliable. We examine two specific distinctions---whether a person in the news can be assumed to be known to a target audience (hearer-old vs hearer-new) and whether a person is a major character in the news story. We report on machine learning experiments that show that these distinctions can be learned with high accuracy, and validate our approach using human subjects."
W04-2709,Interlingual Annotation of Multilingual Text Corpora,2004,14,20,11,0,50484,stephen helmreich,Proceedings of the Workshop Frontiers in Corpus Annotation at {HLT}-{NAACL} 2004,0,"This paper describes a multi-site project to annotate six sizable bilingual parallel corpora for interlingual content. After presenting the background and objectives of the effort, we will go on to describe the data set that is being annotated, the interlingua representation language used, an interface environment that supports the annotation task and the annotation process itself. We will then present a preliminary version of our evaluation methodology and conclude with a summary of the current status of the project along with a number of issues which have arisen."
P04-1052,Generating Referring Expressions in Open Domains,2004,11,43,1,1,10906,advaith siddharthan,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"We present an algorithm for generating referring expressions in open domains. Existing algorithms work at the semantic level and assume the availability of a classification for attributes, which is only feasible for restricted domains. Our alternative works at the realisation level, relies on Word-Net synonym and antonym sets, and gives equivalent results on the examples cited in the literature and improved results for examples that prior approaches cannot handle. We believe that ours is also the first algorithm that allows for the incremental incorporation of relations. We present a novel corpus-evaluation using referring expressions from the Penn Wall Street Journal Treebank."
C04-1129,Syntactic Simplification for Improving Content Selection in Multi-Document Summarization,2004,18,66,1,1,10906,advaith siddharthan,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper, we explore the use of automatic syntactic simplification for improving content selection in multi-document summarization. In particular, we show how simplifying parentheticals by removing relative clauses and appositives results in improved sentence clustering, by forcing clustering based on central rather than background information. We argue that the inclusion of parenthetical information in a summary is a reference-generation task rather than a content-selection one, and implement a baseline reference rewriting module. We perform our evaluations on the test sets from the 2003 and 2004 Document Understanding Conference and report that simplifying parentheticals results in significant improvement on the automated evaluation metric Rouge."
reeder-etal-2004-interlingual,Interlingual annotation for {MT} development,2004,16,7,11,0,46657,florence reeder,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"MT systems that use only superficial representations, including the current generation of statistical MT systems, have been successful and useful. However, they will experience a plateau in quality, much like other {``}silver bullet{''} approaches to MT. We pursue work on the development of interlingual representations for use in symbolic or hybrid MT systems. In this paper, we describe the creation of an interlingua and the development of a corpus of semantically annotated text, to be validated in six languages and evaluated in several ways. We have established a distributed, well-functioning research methodology, designed a preliminary interlingua notation, created annotation manuals and tools, developed a test collection in six languages with associated English translations, annotated some 150 translations, and designed and applied various annotation metrics. We describe the data sets being annotated and the interlingual (IL) representation language which uses two ontologies and a systematic theta-role list. We present the annotation tools built and outline the annotation process. Following this, we describe our evaluation methodology and conclude with a summary of issues that have arisen."
W03-2602,Resolving Pronouns Robustly: Plumbing the Depths of Shallowness,2003,-1,-1,1,1,10906,advaith siddharthan,Proceedings of the 2003 {EACL} Workshop on The Computational Treatment of Anaphora,0,None
W03-2314,Preserving Discourse Structure when Simplifying Text,2003,0,18,1,1,10906,advaith siddharthan,Proceedings of the 9th {E}uropean Workshop on Natural Language Generation ({ENLG}-2003) at {EACL} 2003,0,None
