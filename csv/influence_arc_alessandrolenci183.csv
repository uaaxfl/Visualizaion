2001.mtsummit-papers.13,bel-etal-2000-simple,1,0.791392,"towards de facto standards has already allowed the field of Language Resources to establish broad consensus on key issues for some well-established areas — and will allow similar consensus to be achieved for other important areas through the ISLE project — providing thus a key opportunity for further consolidation and a basis for technological advance. EAGLES previous results have already become de facto standards. To mention several key examples: the LE PAROLE/SIMPLE resources (morphological/syntactic/semantic lexicons and corpora for 12 EU languages, Ruimy et al., 1998, Lenci et al., 2000, Bel et al., 2000) rely on EAGLES results (Sanfilippo, A. et al., 1996 and 1999), and are now being enlarged at the national level through many National Projects; the ELRA Validation Manuals for Lexicons (Underwood and Navarretta, 1997) and Corpora (Burnard et al., 1997) are based on EAGLES guidelines; morphosyntactic tagging of corpora in a very large number of EU, international and national projects – and for more than 20 languages — is conformant to EAGLES recommendations (Leech and Wilson, 1996). The ISLE objective is more ambitious both in geographic scope , involving European, American and now Asian group"
2020.aacl-main.26,J03-2004,0,0.54055,"enon of logical metonymy can be explained in terms of the thematic fit, that is, the degree of compatibility between the verb and one of its arguments (the direct object, in this case). On the one hand, a low thematic fit between an event-selecting verb and an entity-denoting argument triggers the recovery of a covert event, while on the other hand, the recovered event is often the best fitting one, given the information available in the sentence. Research in NLP on logical metonymy initially focused on the problem of covert event retrieval, which was tackled by means of probabilistic models (Lapata and Lascarides, 2003; Shutova, 2009), or by using Distributional Semantic Models (DSMs) that identify the candidate covert event with the one that has the highest thematic fit with the arguments in the sentence (Zarcone et al., 2012). Following the psycholinguistic works by McElree et al. (2001) and Traxler et al. (2002), which reported increased reading times and longer fixations in eye-tracking for the metonymic sentences, Zarcone et al. (2013) proposed a distributional model of the thematic fit between verb and object, and showed that it accurately reproduces the differences between the experimental conditions"
2020.aacl-main.26,Q17-1010,0,0.0114471,"s computed as the similarity score of its corresponding lexical vector ~e with the prototype vector. As we did the probabilistic model, we discard the metonymic verb from this computation. 3 The verb E representing the preferred interpretation of the metonymy is the verb e maximizing the following equation: E = argmaxe P (e)P (o|e)P (s|e) We test two variations of this model, TF-add and TF-prod, which differ for the filler selection update function. Statistics were extracted from Wikipedia 2018, and the vectors were the publiclyavailable Wikipedia embeddings 4 trained with the FastText model (Bojanowski et al., 2017). The verb-filler association score is the Local Mutual Information (Evert, 2008). Similarly, the scores for the subject fillers are defined as: We computed the statistics from a 2018 dump of the English Wikipedia, parsed with the Stanford CoreNLP toolkit (Manning et al., 2014). Dataset MC TR L&L CE Coverage 19/30 (pairs) 21/36 (pairs) 151/174 (items) 195/285 (items) Table 2: Coverage for the probabilistic model. sbj LM I(s, e) = f (e ←−− s)log2 3.3.2 Logical Metonymy as Thematic Fit Distributional models of logical metonymy assume that the event recovery task can be seen as a thematic fit tas"
2020.aacl-main.26,W11-0607,1,0.906684,"Missing"
2020.aacl-main.26,S17-1021,1,0.940651,"nts and their typical participants (see McRae and Matsuki (2009) for an overview) and claims that words act like cues to access event knowledge, incrementally modulating sentence comprehension. The results obtained in a probe recognition experiment by Zarcone et al. (2014), in line with this explanation, suggest that speakers interpret logical metonymies by inferring the most likely event the sentences could refer to, given the contextual cues. Previous research in NLP on logical metonymy has often been influenced by such theoretical explanation (Zarcone and Pad´o, 2011; Zarcone et al., 2012; Chersoni et al., 2017). In our contribution, we propose a general comparison of different classes of computational models for logical metonymy. To begin with, we tested two approaches that have been previously introduced in the literature on the topic: probabilistic and distributional models (Zarcone et al., 2012). We also examined the Structured Distributional Model (SDM) by Chersoni et al. (2019), which represents sentence meaning with a combination of formal structures and distributional embeddings to dynamically integrate knowledge about events and their typical participants, as they are activated by lexical it"
2020.aacl-main.26,2021.ccl-1.108,0,0.0624012,"Missing"
2020.aacl-main.26,P14-5010,0,0.00429667,"Missing"
2020.aacl-main.26,N19-1423,0,0.363105,"ransformer language models into a contrastive study on 1 Notice however that the evidence is not uncontroversial: Delogu et al. (2017) report that coercion costs largely reflect word surprisal, without any specific effect of type shift in the early processing measures. 224 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 224–234 c December 4 - 7, 2020. 2020 Association for Computational Linguistics logical metonymy. Transformers (Vaswani et al., 2017; Devlin et al., 2019) are the dominant class of NLP systems in the last few years, since they are able to generate “dynamic” representations for a target word depending on the sentence context. As the interpretation of logical metonymy is highly sensitive to context, we deem that the contextual representations built by Transformers might be able to integrate the covert event that is missing in the surface form of the sentence. All models are evaluated on their capability of assigning the correct interpretation to a metonymic sentence, that is, recovering the verb that refers to the correct interpretation. This tas"
2020.aacl-main.26,N18-1202,0,0.00848065,"ly-elaborated compositional model. The authors recently introduced a more up-to-date and refined version of their sentence comprehension model (Chersoni et al., 2019), but it has not been tested on the logical metonymy task so far. 2.2 Transformer Models in NLP The traditional approach in Distributional Semantics has been the building of a single, stable vector representation for each word type in the corpus (Turney and Pantel, 2010; Lenci, 2018). Lately, a new generation of embeddings has emerged, in which each occurrence of a word in a specific sentence context gets a unique representation (Peters et al., 2018). The most recent systems typically rely on an LSTM or a Transformer architecture for getting word representations: they are trained on large amounts of textual data and the word vectors are learned as a function of the internal states of the encoder, such that a word in different sentence contexts determines different activation states and is represented by a different vector. Thus, embeddings generated by these new models are said to be contextualized, as opposed to the static vectors generated by the earlier frameworks, and they aim at modeling the specific sense assumed by the word in cont"
2020.aacl-main.26,Q15-1032,0,0.0700545,"Missing"
2020.aacl-main.26,D17-1068,1,0.651403,"Missing"
2020.aacl-main.26,W16-2518,0,0.285284,"Missing"
2020.aacl-main.26,P09-3001,0,0.0413085,"be explained in terms of the thematic fit, that is, the degree of compatibility between the verb and one of its arguments (the direct object, in this case). On the one hand, a low thematic fit between an event-selecting verb and an entity-denoting argument triggers the recovery of a covert event, while on the other hand, the recovered event is often the best fitting one, given the information available in the sentence. Research in NLP on logical metonymy initially focused on the problem of covert event retrieval, which was tackled by means of probabilistic models (Lapata and Lascarides, 2003; Shutova, 2009), or by using Distributional Semantic Models (DSMs) that identify the candidate covert event with the one that has the highest thematic fit with the arguments in the sentence (Zarcone et al., 2012). Following the psycholinguistic works by McElree et al. (2001) and Traxler et al. (2002), which reported increased reading times and longer fixations in eye-tracking for the metonymic sentences, Zarcone et al. (2013) proposed a distributional model of the thematic fit between verb and object, and showed that it accurately reproduces the differences between the experimental conditions in the data fro"
2020.aacl-main.26,W13-0216,1,0.861246,"Missing"
2020.aacl-main.26,W12-1707,0,0.0210505,"Missing"
2020.coling-main.602,N19-1423,0,0.217491,"nings by integrating information activated by the textual and extralinguistic context. Vector representations (aka word embeddings) produced by Distributional Semantic Models (DSMs) are particularly suitable for modeling contextual semantic effects, due to their “gradedness&quot; and their dependence on the linguistic contexts (Lenci, 2018; Boleda, 2020). Traditional DSMs represent the content of lexical types through a single vector that “summarizes” their whole distributional history. Things have recently changed with the introduction of deep neural architectures for language modeling like BERT (Devlin et al., 2019), whose word representations have helped achieving state-of-the-art results in a wide variety of supervised NLP tasks. These embeddings are intrinsically contextualized, in the sense that the model computes a different vector for each token occurrence of the same word, depending on the sentence in which the token appears. In this work, we test whether BERT contextualized embeddings can be used to model the meaning shifts associated with metonymic uses of words. Given its pervasiveness in everyday communication, we suggest that the extent to which metonymy is captured by BERT is an important te"
2020.coling-main.602,D08-1094,0,0.360538,"r a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6831 Proceedings of the 28th International Conference on Computational Linguistics, pages 6831–6837 Barcelona, Spain (Online), December 8-13, 2020 is instead captured by integrating a rich array of distributional knowledge about events and their typical participants (McRae and Matsuki, 2009). 2 Related Work Over the years, various methods to obtain contextualized representations of word meaning have been developed in different fields. Research in distributional semantics (Erk and Padó, 2008; Thater et al., 2011) has taken non-contextual representations of words as starting point from which contextualized vectors capable of modeling various types of meaning alternations are derived (Erk and Padó, 2008; Zarcone et al., 2012). However, these models have never been used to predict metonymic semantic shifts. Lately, Transformer language models (e.g., BERT, GPT2, etc.) have stormed AI and NLP with a new generation of word embeddings that are expected to capture lexical meaning variation in context (Radford et al., 2019; Devlin et al., 2019). In particular, the representations produced"
2020.coling-main.602,W11-0607,1,0.791308,"n bottle). A key feature of the model is that the activated words are filtered according to their PMI association strength with the metonymic word wm . In our example, words for foods (e.g., fruit, meat, etc.) and drinks (e.g., wine, beer, etc.) are activated by taste, but the former are discarded because they have low PMI values with bottle. This process, which is an original innovation 1 We take only the expectations activated by the verb in a direct syntactic relation to the metonymic word, even though proposals to integrate expectations from all the sentence arguments have been developed (Lenci, 2011; Chersoni et al., 2019) 6832 with respect to the Chersoni et al. model, simulates the interaction between lexical information and active expectations as described by Elman (2014), and at the same time reproduces the associative processes involved in metonymy interpretation by updating the salience of expectations based on their relation with the metonymic word. Finally, we calculate the centroid of the activated expectation vectors and the embedding of wm to obtain its contextualized representation. Let W be the k words with the highest PMI with the verb w and Wm the n words in W with the hig"
2020.coling-main.602,P14-2050,0,0.0145489,"masked (The guest tasted the [MASK] and The man raised the [MASK] respectively). We then compare the probabilities of the metonymic (e.g., wine) and the literal paraphrase (e.g., container). We expect the former to be higher than the latter in the metonymic sentence (Metonymic Matching subtask), and the opposite to be true in the literal sentence (Literal Matching subtask). We use BERTBASE (number of layers=12, hidden size=768, number of self-attention heads=12) in all experiments. To implement SDM, we produce 300-dimensional dependency-based embeddings using Skip-gram with negative sampling (Levy and Goldberg, 2014) trained on a parsed corpus of about 3.9 billion tokens, which is a concatenation of ukWaC and a 2018 dump of Wikipedia. 6834 Experiment 1 Experiment 2 Metonymic Matching Literal Matching BERT SDM BERT SDM BERT SDM Type of Metonymy (#Items) Emb LM Emb LM Emb LM CONTAINER - FOR - CONTENT (89) 0.37 0.53 0.78 0.56 0.76 0.71 0.57 0.45 0.66 PRODUCER - FOR - PRODUCT (110) 0.59 0.80 0.95 0.63 0.49 0.81 0.71 0.86 0.59 PRODUCT- FOR - PRODUCER (47) 0.47 0.23 0.96 0.70 0.53 0.62 0.62 0.91 0.75 LOCATION - FOR - LOCATED (94) 0.39 0.52 0.82 0.66 0.52 0.75 0.78 0.89 0.80 CAUSER - FOR - RESULT (92) 0.17 0.71"
2020.coling-main.602,Q19-1027,0,0.024608,"arcone et al., 2012). However, these models have never been used to predict metonymic semantic shifts. Lately, Transformer language models (e.g., BERT, GPT2, etc.) have stormed AI and NLP with a new generation of word embeddings that are expected to capture lexical meaning variation in context (Radford et al., 2019; Devlin et al., 2019). In particular, the representations produced by BERT (Devlin et al., 2019) have been used to create high performing models for many language understanding tasks, although their status as a linguistically sound model of meaning is debated (Mickus et al., 2020). Shwartz and Dagan (2019) test BERT on several cases of figurative language, but to the best of our knowledge BERT ability to identify metonymy has never been addressed yet. 3 Models In BERT, the embedding of a word is modified with contextual information through the self-attention mechanism of Transformers (Vaswani et al., 2017). As is well known, BERT is trained on two tasks: predicting randomly masked tokens (Masked Language Model) and determining whether a sentence follows another sentence in a dataset (Next Sentence Prediction). Since our intent is to assess the model ability to understand metonymic meanings, we"
2020.coling-main.602,I11-1127,0,0.0344891,"Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6831 Proceedings of the 28th International Conference on Computational Linguistics, pages 6831–6837 Barcelona, Spain (Online), December 8-13, 2020 is instead captured by integrating a rich array of distributional knowledge about events and their typical participants (McRae and Matsuki, 2009). 2 Related Work Over the years, various methods to obtain contextualized representations of word meaning have been developed in different fields. Research in distributional semantics (Erk and Padó, 2008; Thater et al., 2011) has taken non-contextual representations of words as starting point from which contextualized vectors capable of modeling various types of meaning alternations are derived (Erk and Padó, 2008; Zarcone et al., 2012). However, these models have never been used to predict metonymic semantic shifts. Lately, Transformer language models (e.g., BERT, GPT2, etc.) have stormed AI and NLP with a new generation of word embeddings that are expected to capture lexical meaning variation in context (Radford et al., 2019; Devlin et al., 2019). In particular, the representations produced by BERT (Devlin et al"
2020.coling-main.602,W12-1707,0,0.0116066,"Spain (Online), December 8-13, 2020 is instead captured by integrating a rich array of distributional knowledge about events and their typical participants (McRae and Matsuki, 2009). 2 Related Work Over the years, various methods to obtain contextualized representations of word meaning have been developed in different fields. Research in distributional semantics (Erk and Padó, 2008; Thater et al., 2011) has taken non-contextual representations of words as starting point from which contextualized vectors capable of modeling various types of meaning alternations are derived (Erk and Padó, 2008; Zarcone et al., 2012). However, these models have never been used to predict metonymic semantic shifts. Lately, Transformer language models (e.g., BERT, GPT2, etc.) have stormed AI and NLP with a new generation of word embeddings that are expected to capture lexical meaning variation in context (Radford et al., 2019; Devlin et al., 2019). In particular, the representations produced by BERT (Devlin et al., 2019) have been used to create high performing models for many language understanding tasks, although their status as a linguistically sound model of meaning is debated (Mickus et al., 2020). Shwartz and Dagan (2"
2020.lrec-1.114,W13-2308,1,0.803707,"nce of historical variants of words as well as peculiar syntactic structures. For these reasons, contemporary tools for linguistic analysis are generally not suitable for processing historical texts and need to be specialized with respect to the peculiarities of the historical variety of language to be processed. The annotation methodology we have employed for the annotation of the VGG corpus was articulated in the following steps: 1. automatic annotation of representative sample texts using UDPipe (Straka et al., 2016) trained on the Italian Universal Dependency Treebank (IUDT), version 2.0 (Bosco et al., 2013); 913 Text genre Diary (Gadda, Martini, Sonnino) Discourse (D’Annunzio, Morgari, Salandra, Salvemini, Treves, Turati; dichiarazioni del Partito Socialista) Essay (Croce, Gemelli, Gentile) Letters (Fontana, Monteleone, Monti, Procacci, Raviele) Memoir (Cadorna, Jahier, Monelli, Prezzolini, Soffici) Report (Comitati Segreti della Camera dei Deputati) Total Tok. + Lemm. + Meta–ling. 93,287 Morpho–synt. 49,868 Dep. Syntax 3,050 52,734 7,792 2,627 17,876 95,248 157,812 83,122 500,079 9,524 5,310 22,938 7,573 103,005 3,487 – 2,445 3,050 14,659 Table 2: For each genre, number of tokens manually revis"
2020.lrec-1.114,P05-1045,0,0.00891135,"ry (De Mauro, 2000): following Giuliani et al. (2005) who demonstrate the stability and conservativeness of the Italian lexicon throughout the centuries, we took the Basic Italian Dictionary as a reference, starting from the assumption that the most important areas of experience continue to be mostly indicated by a stable nucleus of words in use for at least seven centuries. 3.2. Named Entities Annotation The Named Entity annotation of VGG-Silver was carried out with an adaption of the CoLingLab Named Entity Recognizer (NER) described in Passaro and Lenci (2014) and based on the Stanford NER (Finkel et al., 2005). Our NE label set was composed of three tags: PER for people’s proper names (e.g., Luigi Cadorna), LOC for locations (e.g., passo di Monte Croce), and ORG for organizations, including military formations (e.g., 2◦ Reggimento Bersaglieri) (see Figure 1). To annotate VGG-Silver we concatenated two distinct NER models, both trained on the same dataset. In order to adapt the NER to the Italian variety represented in the VGG corpus, the training 914 data consisted of the ICAB corpus (Magnini et al., 2006), in which the Geo-Political Entities (GPE) were converted into the LOC tag, and the Italian w"
2020.lrec-1.114,magnini-etal-2006-cab,0,0.0845611,"d Entity Recognizer (NER) described in Passaro and Lenci (2014) and based on the Stanford NER (Finkel et al., 2005). Our NE label set was composed of three tags: PER for people’s proper names (e.g., Luigi Cadorna), LOC for locations (e.g., passo di Monte Croce), and ORG for organizations, including military formations (e.g., 2◦ Reggimento Bersaglieri) (see Figure 1). To annotate VGG-Silver we concatenated two distinct NER models, both trained on the same dataset. In order to adapt the NER to the Italian variety represented in the VGG corpus, the training 914 data consisted of the ICAB corpus (Magnini et al., 2006), in which the Geo-Political Entities (GPE) were converted into the LOC tag, and the Italian war bulletins of World War I (Boschetti et al., 2014), in which the military formations originally tagged as MIL were converted into ORG. Moreover, we added three texts from VGG (a sample from Mussolini’s diary, a parliamentary report and a speech by the politician Claudio Treves), whose NEs were manually annotated. Only the second model was also trained on a gazetteer including person names collected from the online Italian Treccani Encyclopedia,4 WWI military organization names gathered from Wikipedi"
2020.lrec-1.114,onelli-etal-2006-diacoris,0,0.0245846,"reover, they are not economically attractive, due to their limited use to develop downstream applications with a large-scale economic impact. Still, historical corpora represent an invaluable asset in the era of Digital Humanities, given the growing interest in applying quantitative and computational methods to diachronic linguistics and historical text analysis (Tahmasebi et al., 2019). Italian makes not exception to this trend, as diachronic corpora are still few, among which it is worth pointing out the Corpus OVI dell’Italiano antico and the Corpus TLIO (by OVI– CNR), the DiaCORIS corpus (Onelli et al., 2006) and the MIDIA corpus (Gaeta et al., 2013). One major shortcoming of such resources is the extremely large timespan they cover in comparison to their limited size. The project Voci della Grande Guerra (“Voices of the Great War”) (in short, VGG)1 aimed at filling this gap by creating the largest digital corpus to date of Italian texts at the time of World War I (WWI). The corpus includes a selection of texts representative of different textual genres and registers, including popular Italian. VGG texts have been 1 http://www.vocidellagrandeguerra.it/ automatically annotated with state-of-the-art"
2020.lrec-1.114,L16-1680,0,0.0216851,"cated venture (Piotrowski, 2012), due to e.g. the absence of standardized spelling, the occurrence of historical variants of words as well as peculiar syntactic structures. For these reasons, contemporary tools for linguistic analysis are generally not suitable for processing historical texts and need to be specialized with respect to the peculiarities of the historical variety of language to be processed. The annotation methodology we have employed for the annotation of the VGG corpus was articulated in the following steps: 1. automatic annotation of representative sample texts using UDPipe (Straka et al., 2016) trained on the Italian Universal Dependency Treebank (IUDT), version 2.0 (Bosco et al., 2013); 913 Text genre Diary (Gadda, Martini, Sonnino) Discourse (D’Annunzio, Morgari, Salandra, Salvemini, Treves, Turati; dichiarazioni del Partito Socialista) Essay (Croce, Gemelli, Gentile) Letters (Fontana, Monteleone, Monti, Procacci, Raviele) Memoir (Cadorna, Jahier, Monelli, Prezzolini, Soffici) Report (Comitati Segreti della Camera dei Deputati) Total Tok. + Lemm. + Meta–ling. 93,287 Morpho–synt. 49,868 Dep. Syntax 3,050 52,734 7,792 2,627 17,876 95,248 157,812 83,122 500,079 9,524 5,310 22,938 7,5"
2020.lrec-1.700,J10-4006,1,0.940389,"e-filler pairs. McRae and Pad´o include, respectively, 1,444 and 414 scores for agents and patients (e.g., doctor-advise and hit-ball), whereas Ferretti includes judgements for 274 instruments and 248 locations (e.g., cut-mower and teach-classroom). The scores range from 1 (atypical) to 7 (very typical) and their distribution per role can be observed in Figures 1, 2 and 3. Experimental Settings 4 5 6 7 McRae dataset 3 3. Ferretti dataset Score tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are"
2020.lrec-1.700,P14-1023,0,0.32911,"e Learning Methods 1. Introduction In recent years, vectors derived from neural network training have quickly replaced the old, count-based Distributional Semantic Models (DSMs) as a de facto standard for word representation in NLP.1 Tools such as Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) have provided the research community with an efficient and scalable method for training vector representations, generally referred to as word embeddings. Moreover, the embeddings have been reported to have an advantage over the old count models also in terms of performance in several NLP tasks (Baroni et al., 2014).2 In this scenario, thematic fit estimation represents an exception. Concretely, the task consists in estimating a typicality score for a filler noun given a verb role (e.g., a system has to predict how plausible a cake is as a patient of the verb to eat). It is generally evaluated by assessing the correlation between collections of human judgements and DSM outputs, and it represents an important benchmark for the capacity of the models of capturing compositional meaning (Lenci, 2018). In a systematic comparison between count-based models and neural embeddings, (Baroni et al., 2014) showed th"
2020.lrec-1.700,Q17-1010,0,0.0525211,"Lenci, 2010) turned out to be particularly influential. Given a verb role, this study made use of a corresponding syntactic relation (e.g., the subject for the agent) to extract its typical fillers. The vectors of the typical fillers were then summed to create distributional representations of the prototypical fillers, and the thematic fit of a noun for a role was finally assessed as the cosine similarity between its filler vector and the role prototype. While word embeddings were taking distributional semantics by storm (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), it is surprising that, after the early studies, such vector representations have not been tested anymore on thematic fit. Moreover, the few works were carried out only with the original Word2Vec embeddings, trained on window-based contexts, and were limited to the Continuous-Bag-of-Words (CBOW) tested on two datasets, in which only the agent and the patient roles are represented. To the best of our knowledge, Skip-Gram vectors have never been tested on thematic fit estimation. Finally, the embeddings trained on syntactic dependencies by (Levy and Goldberg, 2014) proved to be efficient in mod"
2020.lrec-1.700,N19-1423,0,0.0438305,"Missing"
2020.lrec-1.700,N15-1003,0,0.655069,"ectively, 1,444 and 414 scores for agents and patients (e.g., doctor-advise and hit-ball), whereas Ferretti includes judgements for 274 instruments and 248 locations (e.g., cut-mower and teach-classroom). The scores range from 1 (atypical) to 7 (very typical) and their distribution per role can be observed in Figures 1, 2 and 3. Experimental Settings 4 5 6 7 McRae dataset 3 3. Ferretti dataset Score tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved"
2020.lrec-1.700,D17-1138,0,0.0157339,"tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved elements is different: discrete semantic types in the former case, gradient compatibility between arguments and thematic roles in the latter one (Lebani and Lenci, 2018). In the literature using DSMs for modeling thematic fit, the method by (Baroni and Lenci, 2010) turned out to be particularly influential. Given a verb role, this study made use of a corresponding syntactic relation (e.g., the subject for the agent) to extract its typical fillers. The vectors of the typic"
2020.lrec-1.700,K19-1050,0,0.0297538,"Missing"
2020.lrec-1.700,S18-2002,0,0.318261,"tti ones, even without syntactic fillers. Our scores were obtained simply by training the model with standard parameters and with no refined context selection. Thus, we conclude that word embeddings are not always a bad fit for the thematic fit task. Given the growing interest for the psychological plausibility of word embeddings and for their performance on cognitively-motivated benchmarks (Søgaard, 2016; Mandera et al., 2017; Bakarov, 2018; Schwartz and Mitchell, 9 It should be noticed that state-of-the-art neural systems for this task are trained on semantic role labels (Tilk et al., 2016; Hong et al., 2018), and thus they avoid -at least in theory- the problem of dealing with the ambiguity of the prepositions. 5711 2019; Hollenstein et al., 2019), future experiments might add thematic fit estimation to the set of tasks in which they could be tested, by carefully taking into account the impact of factors such as the size of training data and the linguistic information available to the models. Another possible direction of work could aim at adapting the recently-introduced contextualized embeddings (Radford et al., 2018; Peters et al., 2018; Devlin et al., 2019) to the task. 6. Acknowledgements We"
2020.lrec-1.700,E17-2063,0,0.240928,"tors have never been tested on thematic fit estimation. Finally, the embeddings trained on syntactic dependencies by (Levy and Goldberg, 2014) proved to be efficient in modeling the functional similarity between words that tend to have the same function or structural role in a sentence, and thus they seem good candidates to perform well in the task. 4 Here, we present a complete evaluation of the abovementioned models on datasets including agents, patients, instruments and locations. Moreover, given the recent claims that incorporating syntax in DSMs does not lead to significant improvements (Lapesa and Evert, 2017), we pay specific attention to a question not addressed yet in the literature: how essential is syntactic information for building good-quality semantic role prototypes? 1 2 Datasets. We tested our models on three standard datasets derived from (McRae et al., 1998), (Ferretti et al., 2001) and (Pad´o, 2007), containing plausibility judgments for AGENTS specific world knowledge in sentence comprehension, as in the psycholinguistic literature of reference. 4 See also (Turney, 2012) for the distinction between functional similarity and domain similarity. PATIENTS Role Figure 3: Scores distributio"
2020.lrec-1.700,P14-2050,0,0.305943,"t models based on count vectors. Despite the progress made in the recent literature and the introduction of several new architectures and improvements, the studies following the first evaluation attempts have only focused on count models. In the present contribution, we propose a more systematic comparison between embeddings and count-based models on thematic fit estimation. Compared to earlier evaluations, which only tested CBOW vectors on agents and patients datasets, we evaluate both the Word2Vec architectures on a wider variety of roles, as well as the dependency-based word embeddings by (Levy and Goldberg, 2014). Additionally, since the best thematic fit models make use of syntactic information to build ’prototypical’ representations of the verb roles, we test the importance of such information for the model performance. 2. Related Work According to a long tradition of psycholinguistic studies, human semantic memory stores a generalized knowledge about events and their participants (McRae et al., 1998; McRae et al., 2005; Hare et al., 2009). The typicality of the combinations of verbs and arguments has important consequences for sentence processing, as typical combinations require less effort from hu"
2020.lrec-1.700,D17-1257,0,0.0228285,"7 0.248 0.203 0.261 Table 4: Spearman correlations for the Instruments and Locations dataset for all models with all filler sets. This is not surprising: DM is a carefully crafted syntactic DSM, and the addition of lexical syntactic patterns has been hypothesized to have a positive impact in this task (Sayeed et al., 2015). However, the less-refined DEPS model perform similarly to the SG embeddings, which in turn perform always better than the CBOW ones. The performance of LG-DEPS is also close to the SG one: syntactic dependencies, as suggested by some recent contributions in the literature (Li et al., 2017; Lapesa and Evert, 2017), do not improve model performance. As for the fillers, syntax seems instead to play an important role: if we compare models with BOWF fillers with those making use of the ”syntactic” sets, we observe large and significant drops for every model on both datasets, to the point that many correlations on McRae become non-significant. DM fillers are clearly better than the DEPS one, suggesting that syntactic information is more useful to select typical contexts. The results for the other roles (Table 4) instead show a clear advantage of the BOW embeddings over count-based m"
2020.lrec-1.700,D14-1162,0,0.087969,"the method by (Baroni and Lenci, 2010) turned out to be particularly influential. Given a verb role, this study made use of a corresponding syntactic relation (e.g., the subject for the agent) to extract its typical fillers. The vectors of the typical fillers were then summed to create distributional representations of the prototypical fillers, and the thematic fit of a noun for a role was finally assessed as the cosine similarity between its filler vector and the role prototype. While word embeddings were taking distributional semantics by storm (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), it is surprising that, after the early studies, such vector representations have not been tested anymore on thematic fit. Moreover, the few works were carried out only with the original Word2Vec embeddings, trained on window-based contexts, and were limited to the Continuous-Bag-of-Words (CBOW) tested on two datasets, in which only the agent and the patient roles are represented. To the best of our knowledge, Skip-Gram vectors have never been tested on thematic fit estimation. Finally, the embeddings trained on syntactic dependencies by (Levy and Goldberg, 2014) pro"
2020.lrec-1.700,N18-1202,0,0.107127,"Missing"
2020.lrec-1.700,D16-1099,1,0.843203,"cant. DM fillers are clearly better than the DEPS one, suggesting that syntactic information is more useful to select typical contexts. The results for the other roles (Table 4) instead show a clear advantage of the BOW embeddings over count-based models. SG embeddings are again the best, followed by the CBOW ones, and LG-DEPS vectors, unexpectedly, are again lagging behind their BOW counterparts. Disappointing results for dependency-based embeddings have also been reported by (Gamallo, 2017), in comparison to syntactic count-based vectors. A possible cause, as suggested by (Asr et al., 2016; Sahlgren and Lenci, 2016), could be found in the fact that embedding models are suboptimal when trained on smaller data sizes, and this could be especially true with sparse dependency contexts. It is also interesting to observe that, with instruments and locations, dropping syntactically-selected fillers does not always cause huge correlation drops for the embedding models, and in some cases it even leads to improvements (cf. the SG and CBOW scores for Locations). Probably, using prepositions to select the fillers turns out to be a rough approximation, and the prototypes are so noisy that there are no gains with respe"
2020.lrec-1.700,D17-1068,1,0.923154,"scores for agents and patients (e.g., doctor-advise and hit-ball), whereas Ferretti includes judgements for 274 instruments and 248 locations (e.g., cut-mower and teach-classroom). The scores range from 1 (atypical) to 7 (very typical) and their distribution per role can be observed in Figures 1, 2 and 3. Experimental Settings 4 5 6 7 McRae dataset 3 3. Ferretti dataset Score tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved elements is different:"
2020.lrec-1.700,W16-2518,0,0.480117,"ce count between a verb v and a filler f and the expected count under independence (the formula is the same for the syntactic method, but adding the third element of the syntactic relation r). For each DSM and filler type, the vectors of the top scoring k fillers for each role were summed to build the prototypes. After testing with k = 10, 20, 30, 40, 50, we observed that the number of fillers did not significantly affect the performance, coherently with the findings of (Greenberg et al., 2015). The reported results have been obtained with k = 20, as in the works by (Baroni et al., 2014) and (Sayeed et al., 2016). The 5 DSMs have been evaluated with all 3 filler types, making 15 different models. Finally, we measured the cosine similarity between roles prototypes and fillers in the datasets, and we computed the Spearman correlation between scores and human judgements. Role Prototypes As in (Baroni and Lenci, 2010), we extract typical fillers for each verb role and sum them to cre5 Ov,f Ev,f LM I(v, f ) = log 4. Results and Discussion The scores for agents and patients datasets and those for instruments and locations follow quite different patterns. In Table 3, DM turns out to be by far the best model"
2020.lrec-1.700,N19-1005,0,0.0434075,"Missing"
2020.lrec-1.700,W16-2521,0,0.012326,"the task, performing similarly to a standard dependency-based model on the Pad´o and McRae datasets (it only lags behind the carefullycrafted DM), and outperforming all count-based competitors on the Ferretti ones, even without syntactic fillers. Our scores were obtained simply by training the model with standard parameters and with no refined context selection. Thus, we conclude that word embeddings are not always a bad fit for the thematic fit task. Given the growing interest for the psychological plausibility of word embeddings and for their performance on cognitively-motivated benchmarks (Søgaard, 2016; Mandera et al., 2017; Bakarov, 2018; Schwartz and Mitchell, 9 It should be noticed that state-of-the-art neural systems for this task are trained on semantic role labels (Tilk et al., 2016; Hong et al., 2018), and thus they avoid -at least in theory- the problem of dealing with the ambiguity of the prepositions. 5711 2019; Hollenstein et al., 2019), future experiments might add thematic fit estimation to the set of tasks in which they could be tested, by carefully taking into account the impact of factors such as the size of training data and the linguistic information available to the model"
2020.lrec-1.700,D16-1017,0,0.374475,"titors on the Ferretti ones, even without syntactic fillers. Our scores were obtained simply by training the model with standard parameters and with no refined context selection. Thus, we conclude that word embeddings are not always a bad fit for the thematic fit task. Given the growing interest for the psychological plausibility of word embeddings and for their performance on cognitively-motivated benchmarks (Søgaard, 2016; Mandera et al., 2017; Bakarov, 2018; Schwartz and Mitchell, 9 It should be noticed that state-of-the-art neural systems for this task are trained on semantic role labels (Tilk et al., 2016; Hong et al., 2018), and thus they avoid -at least in theory- the problem of dealing with the ambiguity of the prepositions. 5711 2019; Hollenstein et al., 2019), future experiments might add thematic fit estimation to the set of tasks in which they could be tested, by carefully taking into account the impact of factors such as the size of training data and the linguistic information available to the models. Another possible direction of work could aim at adapting the recently-introduced contextualized embeddings (Radford et al., 2018; Peters et al., 2018; Devlin et al., 2019) to the task. 6."
2020.lrec-1.700,J13-3006,0,0.0300509,"re tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved elements is different: discrete semantic types in the former case, gradient compatibility between arguments and thematic roles in the latter one (Lebani and Lenci, 2018). In the literature using DSMs for modeling thematic fit, the method by (Baroni and Lenci, 2010) turned out to be particularly influential. Given a verb role, this study made use of a corresponding syntactic relation (e.g., the su"
2020.lrec-1.700,P19-1071,0,0.421821,"teach-classroom). The scores range from 1 (atypical) to 7 (very typical) and their distribution per role can be observed in Figures 1, 2 and 3. Experimental Settings 4 5 6 7 McRae dataset 3 3. Ferretti dataset Score tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved elements is different: discrete semantic types in the former case, gradient compatibility between arguments and thematic roles in the latter one (Lebani and Lenci, 2018). In the lite"
2020.lrec-1.718,Q17-1002,0,0.144482,"features of the image and offer the advantage of remaining unchanged despite any alteration in light, position or point of view. This way, the images representing the verb arguments used for the textual DSMs M1 and M2 were extracted. However, not all types of nouns are available in ImageNet. In fact, for some of them it was not possible to obtain a visual representation because they belong to the class of abstract nouns (such as theory and experience). Anyway, the problematic nature of finding visual representations from abstract nouns is attested in other researches (Hill and Korhonen, 2014; Anderson et al., 2017). Therefore, this result was quite expected. Instead, for the concrete arguments, it was possible to extract the corresponding images, identifying the descriptors and extracting the Bag of Visual Words. Then, the Visual Words that represented the same concept were aggregated and we calculated the centroid vector for each group of images. This way, we reduced the values of a Visual Word to a single value per image/concept. At this point, as for the textual models, we organized the visual representation of verb’s subjects and objects into a cooccurrence matrix. The visual matrix MV tables the ve"
2020.lrec-1.718,J10-4006,1,0.688649,"ual verb vectors are syntax-based distributional representations that encode co-occurrences with its argument nouns, in particular subjects and objects. In a similar way, visual verb vectors are built from the images that describe the subjects and objects of the verb. Starting from this assumption we i) selected the visual and the textual resources and ii) extracted the verb vectors from these resources. 3.1. The textual vectors To build the textual vectors, we extracted the verbs from SimLex-999 (Hill et al., 2015)1 and their arguments from the annotated tensor of Distributional Memory (DM) (Baroni and Lenci, 2010)2 . SimLex-999 is a resource which describes the similarity between pairs of words and is designed for the evaluation of DSMs. It includes 999 word pairs divided by POS (nouns, verbs and adjectives) and for two categories (concreteness and abstractness). Compared to other resources such as WordSim-353 (Finkelstein et al., 2002), SimLex-999 quantifies the similarity between pairs of words rather than their degree of association. This means that pairs related but not actually similar tend to have a lower score, compared to the one recorded in other datasets. For example, in SimLex999 the pair co"
2020.lrec-1.718,W11-2503,0,0.0271781,", if we would like to define an example of obvious information which is not encoded in a verb vector, we could imagine there is an higher probability to find sentences like ”John is punching the punching bag” instead of ”John is punching using his hands”. Thus, this information cannot be derived from traditional DSMs, while it can be represented with MDSMs. In this paper, we propose a comparison of the performance obtained using DSMs based on textual information with the one obtained using visual DSMs that encode information extracted from images with the Bag of Visual Words (BoVW) technique (Bruni et al., 2011). This way we want to demonstrate that a visual distributional model is able to effectively capture the semantic similarity between concepts, performing in some cases even better than linguistic models. Our analysis focuses in particular on verbs since they play a core role within the sentence. In fact, verbs more than nouns and adjectives are able to convey relevant information about events and actions described in sentences, and impose syntactic and semantics constraints on their arguments. Moreover, verbs have received little attention in MDSMs, which have mostly focused on nouns. The prese"
2020.lrec-1.718,W15-0107,0,0.0671609,"Missing"
2020.lrec-1.718,N10-1011,0,0.0118801,"ions and ideas for further works. 2. Multimodal Distributional Semantics Several works have been developed in years in multimodal Distributional Semantics. Here, we briefly mention the work of Bruni et al. (2014), one of the earliest on this subject. In this work, the textual and visual information are combined, producing a multimodal semantic model. Although their research is not the first attempt to combine textual and visual information, since Feng and Lapata already developed in 2010 a multimodal distributional semantic model using an approach which unified visual and textual information (Feng and Lapata, 2010), from the research of Bruni et al. emerges an interesting behavior of 5865 MDSMs. In fact, the authors underline that while the models based on images are more oriented towards capturing the similarities between concrete nouns, focusing on properties such as colour or shape, textual models are more oriented towards recognizing abstract objects and their properties. According to Lazaridou et al. (2015) this limit of MDSMs may be due to the type of images which are extracted for abstract concepts. In their research, which is based on a multimodal version of Skip-gram (Mikolov et al., 2013b), th"
2020.lrec-1.718,D16-1235,0,0.0609131,"Missing"
2020.lrec-1.718,D14-1032,0,0.404589,", while for concrete words it is more common to find a picture that exactly represents that word, for abstract concepts this is more rare. As a consequence, the multimodal vectors of abstract word convey more complex information, since the visual data for an abstract concept can be extremely diversified one from another. The difficulty to capture abstract properties and concepts using visual models is highlighted also in the research of F˘ag˘ar˘asan et al. (2015), who work on a method for the automatic prediction of the (visual) features of objects elicited by the subjects, and in the work by Hill and Korhonen (2014). In this latter research, authors focus their study on the development of a multimodal model to learn concrete as well as abstract nouns. These works reveal that visual models are able to extract different types of information with respect to textual DSMs. K¨oper and Schulte im Walde (2017) analyze the performance of a neural network model in predicting the compositionality of nominal and verbal multiword expressions, when visual information are included. From their results it emerges that, when combined with textual vectors that describe nouns, the visual features are able to predict better"
2020.lrec-1.718,J15-4004,0,0.552011,"capture the semantic similarity between verbs. More specifically, in the present experiment textual verb vectors are syntax-based distributional representations that encode co-occurrences with its argument nouns, in particular subjects and objects. In a similar way, visual verb vectors are built from the images that describe the subjects and objects of the verb. Starting from this assumption we i) selected the visual and the textual resources and ii) extracted the verb vectors from these resources. 3.1. The textual vectors To build the textual vectors, we extracted the verbs from SimLex-999 (Hill et al., 2015)1 and their arguments from the annotated tensor of Distributional Memory (DM) (Baroni and Lenci, 2010)2 . SimLex-999 is a resource which describes the similarity between pairs of words and is designed for the evaluation of DSMs. It includes 999 word pairs divided by POS (nouns, verbs and adjectives) and for two categories (concreteness and abstractness). Compared to other resources such as WordSim-353 (Finkelstein et al., 2002), SimLex-999 quantifies the similarity between pairs of words rather than their degree of association. This means that pairs related but not actually similar tend to hav"
2020.lrec-1.718,P16-4010,0,0.0146298,"ture used by WordNet (Miller, 2016). ImageNet is a rich dataset with diversified, high resolution images and a high level of image labels accuracy. We used an image corpus instead of a video corpus since our aim was to visually describe only the nominal occurrences of the verbs selected, while a video corpus may be more likely used to describe verbs and actions. To collect the images for our research, we selected the nominal occurrences from M1 and, from those 2,000 nouns, we identified 706 types. Then, we extracted the visual representations of the types and computed their BoVW using MMFeat (Kiela, 2016). MMFeat4 is a toolkit designed to simplify the extraction and the analysis of visual and audio resources for NLP tasks. With this toolkit, we downloaded randomly 5 images which visually described our target nouns, and computed their BoVW using the SIFT descriptor (Lowe, 1999). The SIFT descriptors are automatically calculated based on the most important features of the image and offer the advantage of remaining unchanged despite any alteration in light, position or point of view. This way, the images representing the verb arguments used for the textual DSMs M1 and M2 were extracted. However,"
2020.lrec-1.718,W17-1728,0,0.0339567,"Missing"
2020.lrec-1.718,N15-1016,0,0.0980328,"t words with similar meaning tend to be used within the same linguistic contexts. This hypothesis is implemented by Distributional Semantic Models (DSMs), which represent each lexical element through a distributional vector (Lenci, 2018). The similarity of two words is then calculated based on the position of their vectors within the distribution space. Purely linguistic DSMs may be extended to Multimodal DSMs (MDSMs), which combine the information coded within the linguistic vector with different type of data, the most common of which are visual information extracted from datasets of images (Lazaridou et al., 2015). These models tend to perform better than the ones based only on linguistic data, which show several limitations. In fact, even if linguistic models are able to capture complex linguistic properties, they do not register attributes that for a human being are absolutely intuitive. For example, it is more probable to extract from a text the information that a lemon is sour than that it is yellow, because it is unlikely that a writer would describe such obvious information as lemons are yellow (Baroni and Lenci, 2008; Andrews et al., 2009; Riordan and Jones, 2011). Similarly, if we would like to"
2020.lrec-1.718,W07-2314,0,0.0271641,"s are considered (for a total of 100 verbs). Abstract vs concrete As we have already explained, the use of abstract words does affect the overall performance of visual distributional models, regardless of the POS considered (Hill and Korhonen, 2014; F˘ag˘ar˘asan et al., 2015; Anderson et al., 2017). Therefore, we decided to make a further comparison between SimLex and the three models created, taking into consideration only concrete verbs. We evaluated the concreteness of a verb based on the study conducted by Power with respect to the categories of abstract verbs within the English language (Power, 2007); more specifically, Power illustrates two classes of abstract verbs and their characteristics. We then used Power’s guidelines to define if a verb could be considered concrete or not. Should a verb falls in both categories (e.g. have an idea vs have a car) , we included it in the list of concrete verbs. In comparison with the results obtained considering both abstract and concrete verbs (Figure 2), this second analysis shows that visual models can record a significant improvement when only concrete items are considered (Figure 3). On the other hand, the textual models M1 and M2 do not obtain"
2020.lrec-1.718,W17-6938,0,0.0195173,"l in predicting the compositionality of nominal and verbal multiword expressions, when visual information are included. From their results it emerges that, when combined with textual vectors that describe nouns, the visual features are able to predict better the concrete targets. Otherwise, when the textual verb vectors are combined with the features extracted from the images that visually describe the verb, the opposite case occurs. This demonstrates that the performance of MDSMs differs when applied to the study of nouns rather than to verbs. Another interesting work is the one conducted by Shekhar et al. (2017a), which highlights an important limit of MDSMs. Although these models are able to recognize with a good level of accuracy the objects (represented linguistically by nouns) present in an image, they often have some difficulties in representing attributes (described by adjectives), actions (verbs), mode (adverbs) and spatial relations (prepositions). Adopting the FOIL methodology (Shekhar et al., 2017b), which consists in replacing a word in a generated caption with an incorrect element (the foil), Shekhar et al. demonstrate that MDSMs are often not able to completely identify all the elements"
2020.lrec-1.718,P17-1024,0,0.0269869,"l in predicting the compositionality of nominal and verbal multiword expressions, when visual information are included. From their results it emerges that, when combined with textual vectors that describe nouns, the visual features are able to predict better the concrete targets. Otherwise, when the textual verb vectors are combined with the features extracted from the images that visually describe the verb, the opposite case occurs. This demonstrates that the performance of MDSMs differs when applied to the study of nouns rather than to verbs. Another interesting work is the one conducted by Shekhar et al. (2017a), which highlights an important limit of MDSMs. Although these models are able to recognize with a good level of accuracy the objects (represented linguistically by nouns) present in an image, they often have some difficulties in representing attributes (described by adjectives), actions (verbs), mode (adverbs) and spatial relations (prepositions). Adopting the FOIL methodology (Shekhar et al., 2017b), which consists in replacing a word in a generated caption with an incorrect element (the foil), Shekhar et al. demonstrate that MDSMs are often not able to completely identify all the elements"
2020.lt4gov-1.2,D13-1160,0,0.0515419,"r to guarantee the effec2. Related Work Existing QASs have been categorized in different ways, e.g. depending on the addressed question type (e.g., confirmation questions, factoid questions, list questions), on the features of consulted data bases (e.g., full relational databases, RDF databases), on the adopted approaches and techniques (Ojokoh, 2018). According to Dwivedi and Singh (2013) and Pundge et al. (2016) QASs can be distinguished into three different categories on the basis of the adopted approach: linguistic approach (Green et al., 1961; Clark et al., 1999; Fader and Etzioni, 2013; Berant et al., 2013), statistical approach (Moschitti, 2003; Ferrucci, 2010; Chen et al., 2017; Devlin et al., 2019) and pattern matching approach (Ravichandran and Hovy, 2002; Pas¸ca, 2003). QASs based on a linguistic approach exploit Natural Language Processing (NLP) and language resources such as knowledge-based or corpora. The knowledge architecture of these systems relies on production rules, logic, frames, templates, ontologies, and semantic networks (Dwivedi and Singh, 2013). On the one hand, the linguistic approach is 7 3. very effective in specific domains. On the other hand, it shows limitations in port"
2020.lt4gov-1.2,W02-1033,0,0.30949,"0). The FRAQUE Methodology In this section we present an overview of the user-centered design process employed to create FRAQUE. Moreover, we report on its components through the three main stages described in Dwivedi and Singh (2013), namely document analysis, question analysis and answer analysis. Furthermore, as reported by Jurafsky and Martin (2019), there are two different major paradigms of QASs: information-retrieval based and knowledge-based. In the former case, systems leverage on a vast quantity of textual information, which is retrieved and returned thanks to text analysis methods (Brill et al., 2002; Pas¸ca, 2003; Lin, 2007; Fader and Etzioni, 2013; Chen et al., 2017; Devlin et al., 2019). In the latter case, semantic data are already structured into knowledge bases (Green et al., 1961; Clark et al., 1999; Ravichandran and Hovy, 2002; Fader and Etzioni, 2013; Berant et al., 2013). Finally, hybrid systems, like IBM Watson DeepQA (Ferrucci, 2010), rely both on text datasets and structured knowledge bases to answer questions. Figure 1: The diagram shows the FRAQUE analysis pipeline, which shares some modules with the Text Frame Detector (TFD) system (Miliani et al., 2019). Components in the"
2020.lt4gov-1.2,P17-1171,0,0.0633013,"n different ways, e.g. depending on the addressed question type (e.g., confirmation questions, factoid questions, list questions), on the features of consulted data bases (e.g., full relational databases, RDF databases), on the adopted approaches and techniques (Ojokoh, 2018). According to Dwivedi and Singh (2013) and Pundge et al. (2016) QASs can be distinguished into three different categories on the basis of the adopted approach: linguistic approach (Green et al., 1961; Clark et al., 1999; Fader and Etzioni, 2013; Berant et al., 2013), statistical approach (Moschitti, 2003; Ferrucci, 2010; Chen et al., 2017; Devlin et al., 2019) and pattern matching approach (Ravichandran and Hovy, 2002; Pas¸ca, 2003). QASs based on a linguistic approach exploit Natural Language Processing (NLP) and language resources such as knowledge-based or corpora. The knowledge architecture of these systems relies on production rules, logic, frames, templates, ontologies, and semantic networks (Dwivedi and Singh, 2013). On the one hand, the linguistic approach is 7 3. very effective in specific domains. On the other hand, it shows limitations in portability through different domains, since building an appropriate knowledge"
2020.lt4gov-1.2,dellorletta-etal-2014-t2k,0,0.0605367,"Missing"
2020.lt4gov-1.2,N19-1423,0,0.0357077,".g. depending on the addressed question type (e.g., confirmation questions, factoid questions, list questions), on the features of consulted data bases (e.g., full relational databases, RDF databases), on the adopted approaches and techniques (Ojokoh, 2018). According to Dwivedi and Singh (2013) and Pundge et al. (2016) QASs can be distinguished into three different categories on the basis of the adopted approach: linguistic approach (Green et al., 1961; Clark et al., 1999; Fader and Etzioni, 2013; Berant et al., 2013), statistical approach (Moschitti, 2003; Ferrucci, 2010; Chen et al., 2017; Devlin et al., 2019) and pattern matching approach (Ravichandran and Hovy, 2002; Pas¸ca, 2003). QASs based on a linguistic approach exploit Natural Language Processing (NLP) and language resources such as knowledge-based or corpora. The knowledge architecture of these systems relies on production rules, logic, frames, templates, ontologies, and semantic networks (Dwivedi and Singh, 2013). On the one hand, the linguistic approach is 7 3. very effective in specific domains. On the other hand, it shows limitations in portability through different domains, since building an appropriate knowledge base has usually heav"
2020.lt4gov-1.2,P13-1158,0,0.0485379,"Missing"
2020.lt4gov-1.2,L18-1045,0,0.0126585,"te is described on the text. tained in the thesaurus. Semantic neighbors are computed within a distributional space trained with word2vec (Mikolov et al., 2013) on La Repubblica corpus (Baroni and Mazzoleni, 2004) and PaWaC (Passaro and Lenci, 2017) for administrative domain-specific knowledge. FRAQUE searches for the terms extracted from the question among the distributional space targets. Target words are lemmatized and combined for complex terms. Following the compositional property of word embeddings, each complex term vector consists of an element-wise sum of its word embedding elements (Hazem and Daille, 2018). Semantic neighbors are then detected among the terms with the highest cosine similarity measure. Among these neighbors, FRAQUE searches for triggers. To solve the potential ambiguity resulting from the attribute extraction process and to facilitate a connection between questions and attributes, we implemented a focus detection module. The question focus is expressed by interrogative adverbs, like “how”, and by equivalent linguistic expressions composed by more than one token, such as “in which way”. Each focus is associated with an attribute trigger. For instance “how” is linked to the trigg"
2020.lt4gov-1.2,P14-5010,0,0.00248667,"omposed of both general purpose and domain specific components. First of all, TFD exploits two different linguistic pipelines: T2K2 (Dell’Orletta et al., 2014) and CoreNLP-it (Bondielli et al., 2018). The former has been adapted for administrative acts analysis, the latter for the annotation of questions and texts like social media posts, since it includes statistical models for tokenization, sentence splitting, Part-of-Speech (PoS) tagging, and parsing. For event detection, our QAS exploits a model embedded in the broader system where it has been integrated. To extract NEs, the Stanford NER (Manning et al., 2014) is employed. In particular, it exploits the INFORMed PA (Passaro et al., 2017) model to extract entities related to the administrative domain. Furthermore, it employs EXTra (Passaro and Lenci, 2016) for in-domain complex terms extraction. Table 1 shows the performances of the components used for the morphosyntactic and semantic analysis of texts. As anticipated, T2K2 has been employed to analyze administrative acts, but to our knowledge its performances have not been assessed on the PA domain yet. We report The [Municipality Tax]tax [disbursement]payment must be made through [wire transfer]pa"
2020.lt4gov-1.2,P02-1006,0,0.785395,"confirmation questions, factoid questions, list questions), on the features of consulted data bases (e.g., full relational databases, RDF databases), on the adopted approaches and techniques (Ojokoh, 2018). According to Dwivedi and Singh (2013) and Pundge et al. (2016) QASs can be distinguished into three different categories on the basis of the adopted approach: linguistic approach (Green et al., 1961; Clark et al., 1999; Fader and Etzioni, 2013; Berant et al., 2013), statistical approach (Moschitti, 2003; Ferrucci, 2010; Chen et al., 2017; Devlin et al., 2019) and pattern matching approach (Ravichandran and Hovy, 2002; Pas¸ca, 2003). QASs based on a linguistic approach exploit Natural Language Processing (NLP) and language resources such as knowledge-based or corpora. The knowledge architecture of these systems relies on production rules, logic, frames, templates, ontologies, and semantic networks (Dwivedi and Singh, 2013). On the one hand, the linguistic approach is 7 3. very effective in specific domains. On the other hand, it shows limitations in portability through different domains, since building an appropriate knowledge base has usually heavy time costs. On the contrary, statistical approaches are e"
2020.starsem-1.14,D08-1007,0,0.0949664,"Missing"
2020.starsem-1.14,C00-1028,0,0.286479,"lds higher SPS scores for verb-relation pairs admitting only a restricted range of arguments. Resnik also defines the selectional association (SA) of a verb-relation-class triple as the ratio of the SPS for that class and the overall SPS of the verb-relation pair (see Eq. 2), and the SA of a verb-relation-argument triple as the highest verbrelation-class SA among those computed for each WordNet class the argument belongs to: SAv,r,c = p(c|v, r) log p(c|v,r) p(c|r) SP Sv,r (2) Resnik’s work inspired more taxonomy-based models of SA over the years (Grishman and Sterling, 1992; Abe and Li, 1996; Ciaramita and Johnson, 2000; Clark and Weir, 2001; Alishahi and Stevenson, 2007; Pad´o et al., 2009), but no further refinements of the SPS itself. Distributional Semantic Models (DSMs) (Lenci, 2018) instead tackle the main drawback of taxonomy-based models, i.e. the need for a manually-built lexicon, by requiring no other resource than the corpus they are trained on. They rely on different strategies to compute SA, such as clustering (Pereira et al., 1993), Support Vector Machines (Bergsma et al., 2008), and hybrid approaches (Schulte im Walde et al., 2008). Erk (2007) and Erk et al. (2010) provide a cognitively plausi"
2020.starsem-1.14,P14-2050,0,0.0124587,"ann-Whitney U tests comparing recoverable- and non-recoverable-argument verbs (significance levels). Whenever transitive and Instrumentverb results are different, the former are on the left and the latter on the right of the same cell Word embeddings As for the vector representation of arguments, we a variety of 300-dimensional embeddings trained on a concatenation of ukWaC and a 2018-dump of English Wikipedia. The embeddings we tested include both SVD reduced count-based DSMs and neural embeddings created via word2vec3 (Mikolov et al., 2013), testing both SGNS and CBOW models, and word2vecf (Levy and Goldberg, 2014). We used both window-based and syntax-based contexts, and we tested different window sizes (2 or 10) for word2vec and SVD models, for a total of 12 models (Table 1). SVD w2v w2vf SVD w2v w2vf SVD w2v w2vf SVD w2v w2vf SVD w2v w2vf 5.2 PISA In Tables 2 to 4, we collapsed the results from the 12 distributional models into three types (see Table 1), since there are no within-group notable differences. For each group, we report the worst score. Significance levels are given as follows: *** p ≤ 0.001, ** p ≤ 0.01, * p ≤ 0.05, ns p > 0.05. As shown in Table 2, PISA can reliably separate the two gro"
2020.starsem-1.14,N01-1013,0,0.155171,"b-relation pairs admitting only a restricted range of arguments. Resnik also defines the selectional association (SA) of a verb-relation-class triple as the ratio of the SPS for that class and the overall SPS of the verb-relation pair (see Eq. 2), and the SA of a verb-relation-argument triple as the highest verbrelation-class SA among those computed for each WordNet class the argument belongs to: SAv,r,c = p(c|v, r) log p(c|v,r) p(c|r) SP Sv,r (2) Resnik’s work inspired more taxonomy-based models of SA over the years (Grishman and Sterling, 1992; Abe and Li, 1996; Ciaramita and Johnson, 2000; Clark and Weir, 2001; Alishahi and Stevenson, 2007; Pad´o et al., 2009), but no further refinements of the SPS itself. Distributional Semantic Models (DSMs) (Lenci, 2018) instead tackle the main drawback of taxonomy-based models, i.e. the need for a manually-built lexicon, by requiring no other resource than the corpus they are trained on. They rely on different strategies to compute SA, such as clustering (Pereira et al., 1993), Support Vector Machines (Bergsma et al., 2008), and hybrid approaches (Schulte im Walde et al., 2008). Erk (2007) and Erk et al. (2010) provide a cognitively plausible distributional mod"
2020.starsem-1.14,P07-1028,0,0.0302814,"Sterling, 1992; Abe and Li, 1996; Ciaramita and Johnson, 2000; Clark and Weir, 2001; Alishahi and Stevenson, 2007; Pad´o et al., 2009), but no further refinements of the SPS itself. Distributional Semantic Models (DSMs) (Lenci, 2018) instead tackle the main drawback of taxonomy-based models, i.e. the need for a manually-built lexicon, by requiring no other resource than the corpus they are trained on. They rely on different strategies to compute SA, such as clustering (Pereira et al., 1993), Support Vector Machines (Bergsma et al., 2008), and hybrid approaches (Schulte im Walde et al., 2008). Erk (2007) and Erk et al. (2010) provide a cognitively plausible distributional model that proves particularly relevant for our purposes. Given a verb-relation pair, it computes the plausibility of a potential argument of the pair (i.e., the SA of the triple) via the weighted similarity between that argument and the exemplar arguments stored in the model as vectors, as shown in Eq. 3: SAv,r (a0 ) = X wtv,r (a) sim(a0 , a) (3) a∈args(v,r) 3 PISA: a novel measure of Preference In Selection of Arguments We introduce PISA, our own distributional measure of Preference In Selection of Arguments, which we use"
2020.starsem-1.14,C92-2099,0,0.649829,"ses p(c|v, r) log p(c|v, r) p(c|r) (1) This yields higher SPS scores for verb-relation pairs admitting only a restricted range of arguments. Resnik also defines the selectional association (SA) of a verb-relation-class triple as the ratio of the SPS for that class and the overall SPS of the verb-relation pair (see Eq. 2), and the SA of a verb-relation-argument triple as the highest verbrelation-class SA among those computed for each WordNet class the argument belongs to: SAv,r,c = p(c|v, r) log p(c|v,r) p(c|r) SP Sv,r (2) Resnik’s work inspired more taxonomy-based models of SA over the years (Grishman and Sterling, 1992; Abe and Li, 1996; Ciaramita and Johnson, 2000; Clark and Weir, 2001; Alishahi and Stevenson, 2007; Pad´o et al., 2009), but no further refinements of the SPS itself. Distributional Semantic Models (DSMs) (Lenci, 2018) instead tackle the main drawback of taxonomy-based models, i.e. the need for a manually-built lexicon, by requiring no other resource than the corpus they are trained on. They rely on different strategies to compute SA, such as clustering (Pereira et al., 1993), Support Vector Machines (Bergsma et al., 2008), and hybrid approaches (Schulte im Walde et al., 2008). Erk (2007) and"
2020.starsem-1.14,P93-1024,0,0.554957,"(c|v, r) log p(c|v,r) p(c|r) SP Sv,r (2) Resnik’s work inspired more taxonomy-based models of SA over the years (Grishman and Sterling, 1992; Abe and Li, 1996; Ciaramita and Johnson, 2000; Clark and Weir, 2001; Alishahi and Stevenson, 2007; Pad´o et al., 2009), but no further refinements of the SPS itself. Distributional Semantic Models (DSMs) (Lenci, 2018) instead tackle the main drawback of taxonomy-based models, i.e. the need for a manually-built lexicon, by requiring no other resource than the corpus they are trained on. They rely on different strategies to compute SA, such as clustering (Pereira et al., 1993), Support Vector Machines (Bergsma et al., 2008), and hybrid approaches (Schulte im Walde et al., 2008). Erk (2007) and Erk et al. (2010) provide a cognitively plausible distributional model that proves particularly relevant for our purposes. Given a verb-relation pair, it computes the plausibility of a potential argument of the pair (i.e., the SA of the triple) via the weighted similarity between that argument and the exemplar arguments stored in the model as vectors, as shown in Eq. 3: SAv,r (a0 ) = X wtv,r (a) sim(a0 , a) (3) a∈args(v,r) 3 PISA: a novel measure of Preference In Selection of"
2020.starsem-1.14,P08-1057,0,0.0305451,"years (Grishman and Sterling, 1992; Abe and Li, 1996; Ciaramita and Johnson, 2000; Clark and Weir, 2001; Alishahi and Stevenson, 2007; Pad´o et al., 2009), but no further refinements of the SPS itself. Distributional Semantic Models (DSMs) (Lenci, 2018) instead tackle the main drawback of taxonomy-based models, i.e. the need for a manually-built lexicon, by requiring no other resource than the corpus they are trained on. They rely on different strategies to compute SA, such as clustering (Pereira et al., 1993), Support Vector Machines (Bergsma et al., 2008), and hybrid approaches (Schulte im Walde et al., 2008). Erk (2007) and Erk et al. (2010) provide a cognitively plausible distributional model that proves particularly relevant for our purposes. Given a verb-relation pair, it computes the plausibility of a potential argument of the pair (i.e., the SA of the triple) via the weighted similarity between that argument and the exemplar arguments stored in the model as vectors, as shown in Eq. 3: SAv,r (a0 ) = X wtv,r (a) sim(a0 , a) (3) a∈args(v,r) 3 PISA: a novel measure of Preference In Selection of Arguments We introduce PISA, our own distributional measure of Preference In Selection of Arguments, w"
2021.blackboxnlp-1.13,2020.acl-main.431,0,0.0274844,"Missing"
2021.blackboxnlp-1.13,P19-1356,0,0.101463,"two sets of landa source literal domain to a new target one, we ap- marks for each metaphorical sentence of our dataset 195 P LL(W ) = log P (wt |W 	 ) (1) metaphorical landmarks literal landmarks message confident express A smile is an ambassador. official embassy envoy movement wave sway The flowers nodded in the wind assent head greeting Sentence Table 2: Landmarks for two sentences from the dataset (context window=2, dimension=300). Since we are interested in exploring the inner behavior of the model, we compared the representations for each BERT layer. Recent studies (Liu et al., 2019; Jawahar et al., 2019) have shown that upper-intermediate layers produce general purpose representations which are less dependent on the language modeling objective and approximate linguistic knowledge better. We surmise that this ability could also reflect on the interpretation of metaphors. (two examples of sentences from our datasets with the corresponding sets of landmarks can be seen in Table 2). The metaphorical landmarks are elements of the target conceptual domain to which the 4 Results metaphorical words in a sentence point. For example, the words want and grieve in the sentence The Experiment 1 We calcula"
2021.blackboxnlp-1.13,P18-1113,0,0.0603942,"Missing"
2021.blackboxnlp-1.13,L18-1243,0,0.0267934,"the others (p-value < 0.001). The participants rated conventional metaphors as less meaningful (M= 4.86) than literal expressions (M=5.45), but more plausible than creative metaphors. Creative metaphors were judged (M= 3.69) as more meaningful than nonsense expressions (M= 2.7). Crucially, this test shows that, on the one hand, subjects perceive that metaphorical sentences somehow “deviate” from literal ones, but on the other hand, they recognize that metaphorical sentences, even the most creative ones, differ from purely nonsense structures. We found that existing datasets of metaphors (see Parde and Nielsen (2018) for a review) are not particularly well-suited to test how the model deals with expressions with different structures and different degrees of metaphoricity and plausibility, and more specifically with their interpretation. In fact, to the best of our knowledge none of the existing datasets presents, for different types of structures, annotation regarding both the conventionalization of a metaphor and its interpretation. Moreover, nonsense sentences have never been included in datasets used for computational modeling of such phenomenon. Metaphoricity test To assess the metaphoricity We theref"
2021.blackboxnlp-1.13,D14-1162,0,0.0860916,"ccuracy in producing embed- the embeddings of the literal landmarks (e.g. layer dings that are more similar to the metaphorical 10 (base) M= 0.82691, p-value < 0.03 ; layer 20 landmarks than to the literal ones. To estimate the (large) M = 0.56087, p-value < 0.01) in the upper added value of BERT contextualized embeddings intermediate layers (base: 9,10,11; large: 11-22, pwith respect to traditional static embeddings, we value < 0.05). The accuracy of the models (which compared BERT performance with a baseline com- is shown in figure 4.2) varies along the layers with puted with GloVe vectors (Pennington et al., 2014) the same tendency. 196 BERT base BERT large Human scores 6 −2 −4 −6 5 semantic plausibility pseudo log likelihood pseudo log likelihood −2 −4 −6 4 3 2 −8 −8 lit met_conv met_crea no_sense lit met_conv types of sentences met_crea no_sense lit met_conv types of sentences met_crea no_sense types of sentences Figure 4.1: Boxplots of BERT pseudo-log-likelihood scores and the human ratings of semantic plausibility. For each distribution, the boxplot indicates the median, the quartiles, the maximum and the minimum of the distribution and the outliers when they are present. MetSentences Metwords 0.5"
2021.blackboxnlp-1.13,2020.figlang-1.3,0,0.155328,"f-the-art. While the most recent models proposed for this task differ with respect to the information they exploit, most of them use Transformers Neural Language Models like BERT in order to obtain an initial representation of the processed sequence. This strategy is now very common and has led to general improvements in performance: Four out of the six best systems that participated in the 2020 shared task on the VUA corpus (Steen et al., 2010) used it, and a system based only on BERT and an additional layer outperformed many other systems which were based on explicit linguistic information (Leong et al., 2020). These results strongly suggest that models like BERT already possess some knowledge that is relevant to the detection of metaphors. However, to the best of our knowledge, there is no study directly investigating what these models know about metaphors and if this knowledge shares some aspects with that of humans. The application of this last question to other aspects of language has characterized the field of study known as BERTology (Rogers et al., 2020). Researchers in this field intrinsically evaluate BERT and its variants on challenge sets (Belinkov and Glass, 2019), annotated datasets of"
2021.blackboxnlp-1.13,W18-0912,0,0.0274011,"ng whether a sequence or a single word is an instance of a metaphor or not. On the other hand, Metaphor interpretation concerns the description of the meaning of metaphorical expressions, and is typically cast as a paraphrasing Another possibility is to directly investigate the (Shutova, 2010) or a classification (Bizzoni and model’s embeddings which are then used for tasks Lappin, 2017) task. Even if some recent work like metaphor identification. Since these represenhas approached the interpretation task (Su et al., tations have been shown to be transferable to a high 2016; Mao et al., 2018; Rosen, 2018), much of number of linguistic tasks (Liu et al., 2019), they the literature in the last years has been devoted to must encode some sort of general knowledge about metaphor identification (Leong et al., 2018; Gao linguistic expressions. Previous work (Bommasani et al., 2018; Dankers et al., 2019, 2020; Leong et al., 2020; Chronis and Erk, 2020) hypothesizes 193 that they encode knowledge about meaning, and therefore BERT and its variants can be seen as Distributional Semantics Models (DSMs), (Lenci, 2018). Consequently, they applied the same methods used in distributional semantics to BERT, fo"
2021.blackboxnlp-1.13,W18-0907,0,0.0139855,"lly cast as a paraphrasing Another possibility is to directly investigate the (Shutova, 2010) or a classification (Bizzoni and model’s embeddings which are then used for tasks Lappin, 2017) task. Even if some recent work like metaphor identification. Since these represenhas approached the interpretation task (Su et al., tations have been shown to be transferable to a high 2016; Mao et al., 2018; Rosen, 2018), much of number of linguistic tasks (Liu et al., 2019), they the literature in the last years has been devoted to must encode some sort of general knowledge about metaphor identification (Leong et al., 2018; Gao linguistic expressions. Previous work (Bommasani et al., 2018; Dankers et al., 2019, 2020; Leong et al., 2020; Chronis and Erk, 2020) hypothesizes 193 that they encode knowledge about meaning, and therefore BERT and its variants can be seen as Distributional Semantics Models (DSMs), (Lenci, 2018). Consequently, they applied the same methods used in distributional semantics to BERT, for example comparing the model’s representations via cosine similarity to see whether they reproduce human judgments of similarity. However, BERT as a DSM has a crucial property which previous DSMs lack: The"
2021.blackboxnlp-1.13,2020.acl-main.240,0,0.0210957,"d using autoencoding models like BERT. In fact, these models are inherently bidirectional, that is, they are trained to predict a word given all the other words in the left and the right context. Therefore, they cannot be used for estimating probabilities of sequences via the chain rule, since this requires to compute the probability of any word given the previous words in a sequence. The PLL of a sentence W is obtained by masking one token w at a time, calculating the token’s probability given all the other context words, and summing the log-probabilities for all the tokens as in Equation 1. Salazar et al. (2020) showed that the PLL score, even if strictly speaking it is not a probability, outperforms scores from autoregressive models in a variety of tasks related to the acceptability of sentences. This is probably due to the fact that the PLL eliminates the discrepancy between the probabilities of the first elements of a sequence and those of the last elements. |W | X plied the landmark method, which was proposed by Kintsch (2000) to test a model producing distributional representations of predicate-argument metaphors (e.g., My lawyer is a shark). His aim was to test to which extent these vectors enc"
2021.blackboxnlp-1.13,2020.figlang-1.34,0,0.0593387,"Missing"
2021.blackboxnlp-1.13,N19-1112,0,0.355535,"nce of a metaphor or not. On the other hand, Metaphor interpretation concerns the description of the meaning of metaphorical expressions, and is typically cast as a paraphrasing Another possibility is to directly investigate the (Shutova, 2010) or a classification (Bizzoni and model’s embeddings which are then used for tasks Lappin, 2017) task. Even if some recent work like metaphor identification. Since these represenhas approached the interpretation task (Su et al., tations have been shown to be transferable to a high 2016; Mao et al., 2018; Rosen, 2018), much of number of linguistic tasks (Liu et al., 2019), they the literature in the last years has been devoted to must encode some sort of general knowledge about metaphor identification (Leong et al., 2018; Gao linguistic expressions. Previous work (Bommasani et al., 2018; Dankers et al., 2019, 2020; Leong et al., 2020; Chronis and Erk, 2020) hypothesizes 193 that they encode knowledge about meaning, and therefore BERT and its variants can be seen as Distributional Semantics Models (DSMs), (Lenci, 2018). Consequently, they applied the same methods used in distributional semantics to BERT, for example comparing the model’s representations via cos"
2021.blackboxnlp-1.13,W19-2304,0,0.029072,"e = 1024) cased versions of BERT. We used the pretrained model that is provided by the HuggingFace library Transformers (Wolf et al., 2020). Experiment 1: Sentence plausibility scores To get an estimate of how much BERT considers an expression as plausible, we computed a probability score for each sentence in our dataset. Then we examined whether the scores vary with the sentence types (metaphorical conventional, metaphorical creative, literal, nonsense), and whether they mirror human plausibility judgments. As a measure of sentence plausibility, we used the pseudo-log-likelihood score (PLL) (Wang and Cho, 2019). The probability of a sentence cannot be computed using autoencoding models like BERT. In fact, these models are inherently bidirectional, that is, they are trained to predict a word given all the other words in the left and the right context. Therefore, they cannot be used for estimating probabilities of sequences via the chain rule, since this requires to compute the probability of any word given the previous words in a sequence. The PLL of a sentence W is obtained by masking one token w at a time, calculating the token’s probability given all the other context words, and summing the log-pr"
2021.cmcl-1.12,N19-1423,0,0.0300258,"measures were collected in an available on-line dataset. Similarly to ZuCo, GECO and Provo data are recorded during naturalistic reading on everyday life materials. For every word in GECO and Provo, we extracted its mean total reading time, mean first fixation duration, and mean number of fixations, by averaging over the subjects. Pre-trained contextualized embeddings include the 512-dimensional vectors produced by the three layers of the ELMo bidirectional LSTM architecture (Peters et al., 2018), the 1, 024-dimensional vectors in the 24 layers of BERT-Large Transformers (BERT-Large, Cased) (Devlin et al., 2019), the 1, 600-dimensional vectors of GPT2-xl (Radford et al.), and the 200-dimensional vectors produced by the Neural Complexity model (van Schijndel and Linzen, 2018). 3.3 Method To predict eye tracking data we tested different regression models and several features combinations. Feature Selection. To select the features to be used, for each word embedding model and language model we carried out a preliminary investigation computing Spearman’s correlation between eye tracking features, and respectively surprisal and cosine similarity: The features with the highest correlation with biometrical"
2021.cmcl-1.12,Q17-1010,0,0.0235471,"In order to prevent that the vectors representing words within the context were computed using the target word itself, we passed to BERT a list of sub-sentences, each of which were 3.2 Word Embeddings composed of context words only. So given the senTable 1 shows the embeddings types used in our tence The dog chases the cat: experiments, consisting of 6 non-contextualized S[0] = [&quot;The&quot;] DSMs and 4 contextualized DSMs. The for- S[1] = [&quot;The dog&quot;] mer include predict models (SGNS and FastText) S[2] = [&quot;The dog chases&quot;] (Mikolov et al., 2013; Levy and Goldberg, 2014; S[3] = [&quot;The dog chases the&quot;] Bojanowski et al., 2017) and count models (SVD S[4] = [&quot;The dog chases the cat&quot;] and GloVe) (Bullinaria and Levy, 2012; Penning- Starting from the second sub-sentence, the cosine ton et al., 2014). Four DSMs are window-based similarity is computed between the last word vecand two are syntax-based (synt). Embeddings have tor and the sum of words vectors belonging to the 300 dimensions and were trained on the same cor- previous sub-sentence (list element). Therefore, pus of about 3.9 billion tokens, which is a concate- to compute the cosine similarity between cat and nation of ukWaC and a 2018 dump of Wikipedia. the pr"
2021.cmcl-1.12,N01-1021,0,0.91872,"t al., 2008) between the distributional vectors, representing the context and the target word, produced by different Distributional Seman- 3 Experimental Setting tic Models (DSM) (Lenci, 2018). We compared 10 3.1 Datasets state-of-the-art word embedding models, and two different approaches to compute the context vector. The shared task materials come from ZuCo (HolWe model the predictability of a word within the lenstein et al., 2018), that includes EEG and eyecontext with the word-by-word surprisal computed tracking data, collected on 12 English speakers with 3 of the above mentioned models (Hale, 2001; reading natural texts. The data collection has been 102 Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 102–107 Online Event, June 10, 2021. ©2021 Association for Computational Linguistics done in three different settings: two normal reading tasks and one task-specific reading session. The original dataset comprises 1, 107 sentences, and for the shared task 800 sentences (15, 736 words) have been used for the training data, while the test set included about 200 sentences (3, 554 words). Since the shared task focuses on eye-tracking features, only this l"
2021.cmcl-1.12,K19-1050,0,0.0159697,"acking features from the ZuCo dataset for the shared task of the Cognitive Modeling and Computational Linguistics (CMCL2021) workshop. Using both cosine similarity and surprisal within a regression model, we significantly improved the baseline Mean Absolute Error computed among five eye-tracking features. 1 Introduction Levy, 2008). Finally, cosine similarity and surprisal are combined in different regression models to predict eye tracking data. 2 Related Works Different word embedding models (GloVe, Word2Vec, WordNet2Vec, FastText, ELMo, BERT) have been evaluated in the framework proposed by Hollenstein et al. (2019). The evaluation is based on the model capability to reflect semantic representations in the human mind, using cognitive data in different datasets for eye-tracking, EEG, and fMRI. Word embedding models are used to train neural networks on a regression task. The results of their analyses show that BERT, ELMo, and FastText have the best prediction performances. Regression models with different combinations of cosine similarity and surprisal, to predict (and further study the cognitive dynamics beneath) eye movements have been created by Frank (2017), who claims that, since word embeddings are b"
2021.cmcl-1.12,P14-2050,0,0.0201294,"e input to this model needed a special pre-processing. In order to prevent that the vectors representing words within the context were computed using the target word itself, we passed to BERT a list of sub-sentences, each of which were 3.2 Word Embeddings composed of context words only. So given the senTable 1 shows the embeddings types used in our tence The dog chases the cat: experiments, consisting of 6 non-contextualized S[0] = [&quot;The&quot;] DSMs and 4 contextualized DSMs. The for- S[1] = [&quot;The dog&quot;] mer include predict models (SGNS and FastText) S[2] = [&quot;The dog chases&quot;] (Mikolov et al., 2013; Levy and Goldberg, 2014; S[3] = [&quot;The dog chases the&quot;] Bojanowski et al., 2017) and count models (SVD S[4] = [&quot;The dog chases the cat&quot;] and GloVe) (Bullinaria and Levy, 2012; Penning- Starting from the second sub-sentence, the cosine ton et al., 2014). Four DSMs are window-based similarity is computed between the last word vecand two are syntax-based (synt). Embeddings have tor and the sum of words vectors belonging to the 300 dimensions and were trained on the same cor- previous sub-sentence (list element). Therefore, pus of about 3.9 billion tokens, which is a concate- to compute the cosine similarity between cat"
2021.cmcl-1.12,P10-1021,0,0.229084,"(Hollenstein et al., 2018). Creating systems to efficiently predict biometrical data may be useful to make prediction about linguistic materials for which we have few or none experimental data, and to make new hypothesis about the internal dynamics of cognitive processes. The approach we propose relies mainly on two factors that have been proved to influence language comprehension: i.) the semantic coherence of a word with the previous ones (Ehrlich and Rayner, 1981) and ii.) its predictability from previous context (Kliegl et al., 2004). We model the first factor with the cosine similarity (Mitchell et al., 2010; Pynte et al., 2008) between the distributional vectors, representing the context and the target word, produced by different Distributional Seman- 3 Experimental Setting tic Models (DSM) (Lenci, 2018). We compared 10 3.1 Datasets state-of-the-art word embedding models, and two different approaches to compute the context vector. The shared task materials come from ZuCo (HolWe model the predictability of a word within the lenstein et al., 2018), that includes EEG and eyecontext with the word-by-word surprisal computed tracking data, collected on 12 English speakers with 3 of the above mentioned"
2021.cmcl-1.12,D14-1162,0,0.0981488,"Missing"
2021.cmcl-1.12,N18-1202,0,0.0262351,"total of 2, 689 tokens, and a vocabulary of 1, 197 words. These texts were read by 85 subjects and their eye-tracking measures were collected in an available on-line dataset. Similarly to ZuCo, GECO and Provo data are recorded during naturalistic reading on everyday life materials. For every word in GECO and Provo, we extracted its mean total reading time, mean first fixation duration, and mean number of fixations, by averaging over the subjects. Pre-trained contextualized embeddings include the 512-dimensional vectors produced by the three layers of the ELMo bidirectional LSTM architecture (Peters et al., 2018), the 1, 024-dimensional vectors in the 24 layers of BERT-Large Transformers (BERT-Large, Cased) (Devlin et al., 2019), the 1, 600-dimensional vectors of GPT2-xl (Radford et al.), and the 200-dimensional vectors produced by the Neural Complexity model (van Schijndel and Linzen, 2018). 3.3 Method To predict eye tracking data we tested different regression models and several features combinations. Feature Selection. To select the features to be used, for each word embedding model and language model we carried out a preliminary investigation computing Spearman’s correlation between eye tracking f"
2021.cmcl-1.12,D18-1499,0,0.0419823,"Missing"
2021.starsem-1.1,J10-4006,1,0.542773,"mitations of TLM S as models of GEK. Our results are relevant for researchers interested in assessing the linguistic abilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which can be either a typical or atypical filler of a given role in the event described by"
2021.starsem-1.1,W15-1106,0,0.0158776,"rchers interested in assessing the linguistic abilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which can be either a typical or atypical filler of a given role in the event described by the tuple (cf. Table 1). The dataset includes tuples of different lengt"
2021.starsem-1.1,N15-1003,0,0.0147145,"rchers interested in assessing the linguistic abilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which can be either a typical or atypical filler of a given role in the event described by the tuple (cf. Table 1). The dataset includes tuples of different lengt"
2021.starsem-1.1,W16-4102,1,0.847282,"port) and the other sentence expressing a plausible but less typical one (The mechanic is checking the report), and the task is to assign a higher thematic fit/typicality score to the former. Notice that the two sentences differ only for one argument, and that the “atypical” one might, however, be a common filler with respect to the verb target role (e.g., report is a typical patient for check, it is just less plausible in combination with mechanic as an agent). Several models have tried to tackle the “dynamic” version of the thematic fit task, either based on classical distributional spaces (Chersoni et al., 2016, 2019) or on more sophisticated neural network architectures (Tilk et al., 2016; Hong et al., 2018). On the evaluation side, those works made use of the experimental materials of the study by Lenci (2011), which are, however, limited to agentverb-patient triples. The recently-introduced DTFit dataset (Vassallo et al., 2018) is, in comparison, larger in size and provides more variety of fillers and roles (including instruments, locations and time). Other studies introduced larger datasets, but focused on more specific notions of event plausibility (e.g. the plausibility depending on the physic"
2021.starsem-1.1,S18-2002,0,0.0120126,"report), and the task is to assign a higher thematic fit/typicality score to the former. Notice that the two sentences differ only for one argument, and that the “atypical” one might, however, be a common filler with respect to the verb target role (e.g., report is a typical patient for check, it is just less plausible in combination with mechanic as an agent). Several models have tried to tackle the “dynamic” version of the thematic fit task, either based on classical distributional spaces (Chersoni et al., 2016, 2019) or on more sophisticated neural network architectures (Tilk et al., 2016; Hong et al., 2018). On the evaluation side, those works made use of the experimental materials of the study by Lenci (2011), which are, however, limited to agentverb-patient triples. The recently-introduced DTFit dataset (Vassallo et al., 2018) is, in comparison, larger in size and provides more variety of fillers and roles (including instruments, locations and time). Other studies introduced larger datasets, but focused on more specific notions of event plausibility (e.g. the plausibility depending on the physical properties of the participants) (Wang et al., 2018; Porada et al., 2019; Ko et al., 2019). Contri"
2021.starsem-1.1,S17-1021,1,0.855068,"optimal generalization abilities. 1 Introduction People can discriminate between typical (e.g., A cop arrested a thief ) and atypical events (e.g., A thief arrested a cop) and exploit this ability in online sentence processing to anticipate the upcoming linguistic input. Brains have been claimed to be “prediction machines” (Clark, 2013) and psycholinguistic research has shown that a crucial ingredient 1 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 1–11 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics from corpora (Chersoni et al., 2017, 2021). Language Models are trained to make predictions given a context, and thus, they can also be viewed as models of GEK. This approach is promising if one considers the success of recent Transformer-based Language Models (henceforth TLM S), which are trained on huge corpora and contain a massive number of parameters. Even if these models receive extensive training and have been shown to capture linguistic properties (Jawahar et al., 2019; Goldberg, 2019), it is not obvious whether they acquire the aspects of GEK that have been modeled explicitly in previous approaches. To the best of our"
2021.starsem-1.1,2020.lrec-1.700,1,0.716625,"well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which can be either a typical or atypical filler of a given role in the event described by the tuple (cf. Table 1). The dataset includes tuples of different lengths, and the typicality of a given argument depends on its interaction"
2021.starsem-1.1,N19-1349,0,0.0169179,"2016; Hong et al., 2018). On the evaluation side, those works made use of the experimental materials of the study by Lenci (2011), which are, however, limited to agentverb-patient triples. The recently-introduced DTFit dataset (Vassallo et al., 2018) is, in comparison, larger in size and provides more variety of fillers and roles (including instruments, locations and time). Other studies introduced larger datasets, but focused on more specific notions of event plausibility (e.g. the plausibility depending on the physical properties of the participants) (Wang et al., 2018; Porada et al., 2019; Ko et al., 2019). Contributions: 1. we propose a methodology to adapt TLM S to the dynamic estimation of thematic fit, using a dataset that contains several types of argument combinations differing for their typicality; 2. we present a comprehensive evaluation of various TLM S on this task, performed by comparing them to a strong distributional baseline; 3. we conduct further analysis aimed at identifying the potential limitations of TLM S as models of GEK. Our results are relevant for researchers interested in assessing the linguistic abilities of TLM S, as well as those working on applications involving TLM"
2021.starsem-1.1,W11-0607,1,0.831728,"2011) have shown that humans are able to combine and dynamically update their expectations during sentence processing: for example, their expectations given the sequence The barber cut the differ from the ones given The lumberjack cut the , since the integration of knowledge “cued” by the agent argument with the verb will lead to the activation of different event scenarios. In Distributional Semantics, sophisticated models of the GEK have been proposed to make predictions on upcoming arguments by integrating the cues coming from the verb and the previously-realized arguments in the sentence (Lenci, 2011; Chersoni et al., 2019). Since such knowledge is acquired from both first-hand and linguistic experience (McRae and Matsuki, 2009), an important assumption of this literature is that, at least for its ”linguistic subset”, the GEK can be modeled with distributional information extracted Given the recent success of Transformers Language Models (TLMs), we decided to test them on a benchmark for the dynamic estimation of thematic fit. The evaluation of these models was performed in comparison with SDM, a framework specifically designed to integrate events in sentence meaning representations, and"
2021.starsem-1.1,2021.ccl-1.108,0,0.0279667,"Missing"
2021.starsem-1.1,N19-1423,0,0.016492,"the models that we used, we decided to disregard its tuple and the respective typical/atypical counterpart. For this reason, the final results only take in consideration a subset of the original datasets, which varies from model to model. Additionally, we computed a baseline for each Transformer model, where the model is prevented from attending to the other tokens in the sequence when making predictions. 3.2.2 Transformer-based Language Models We experimented with four TLM S to test how different architectures, training objectives, and sizes of the training corpus affect performance.3 BERT (Devlin et al., 2019) consists of a series of stacked Transformer encoders. It was trained using both a masked language modeling objective (i.e., 2 https://fasttext.cc/docs/en/ english-vectors.html 3 For all experiments involving TLM S, we use pre-trained models available in the HuggingFace’s Python library Transformers (Wolf et al., 2019). 4 Coverage SDM BERT-base(line) BERT-large ROBERTA-large GPT-2 medium AgentDTFit 105/134 0.58 0.46 (0.1) 0.53 0.64 PatientDTFit 323/402 0.62 0.59 (0.06) 0.64 0.64 0.63 InstrumentDTFit 31/100 0.58 0.52 (0.08) 0.53 0.5 0.5 TimeDTFit 89/100 0.58 0.63 (0.06) 0.64 0.66 0.66 LocationD"
2021.starsem-1.1,P07-1028,0,0.0615047,"a strong distributional baseline; 3. we conduct further analysis aimed at identifying the potential limitations of TLM S as models of GEK. Our results are relevant for researchers interested in assessing the linguistic abilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that diffe"
2021.starsem-1.1,N18-2049,0,0.347046,"ral network architectures (Tilk et al., 2016; Hong et al., 2018). On the evaluation side, those works made use of the experimental materials of the study by Lenci (2011), which are, however, limited to agentverb-patient triples. The recently-introduced DTFit dataset (Vassallo et al., 2018) is, in comparison, larger in size and provides more variety of fillers and roles (including instruments, locations and time). Other studies introduced larger datasets, but focused on more specific notions of event plausibility (e.g. the plausibility depending on the physical properties of the participants) (Wang et al., 2018; Porada et al., 2019; Ko et al., 2019). Contributions: 1. we propose a methodology to adapt TLM S to the dynamic estimation of thematic fit, using a dataset that contains several types of argument combinations differing for their typicality; 2. we present a comprehensive evaluation of various TLM S on this task, performed by comparing them to a strong distributional baseline; 3. we conduct further analysis aimed at identifying the potential limitations of TLM S as models of GEK. Our results are relevant for researchers interested in assessing the linguistic abilities of TLM S, as well as thos"
2021.starsem-1.1,2020.coling-main.109,0,0.0529502,"Missing"
2021.starsem-1.1,P19-1071,0,0.0142322,"dberg, 2019), it is not obvious whether they acquire the aspects of GEK that have been modeled explicitly in previous approaches. To the best of our knowledge, Transformers have never been tested on dynamic thematic fit modeling, nor their performance has been compared with traditional distributional models. Our current work is addressing this issue. 2020). Performance in the thematic fit task is typically measured with the correlation between the output scores of the model and human-elicited typicality judgments for verb-argument pairs (McRae et al., 1998; Ferretti et al., 2001; Pad´o, 2007; Zhang et al., 2019; Marton and Sayeed, 2021). In the simplest and most common version of this task, the typicality of verb argument-pairs is evaluated in isolation. Thematic fit is instead a dynamic concept: The expectations for an argument in a given verb role do not depend just on the verb, but also on the compositional combination with the other arguments in the sentence (Bicknell et al., 2010). To check the ability of computational models to account for the compositional update of argument expectations, Lenci (2011) framed the problem as a binary classification task: A system is presented a sentence pair, w"
2021.starsem-1.1,D19-6015,0,0.0193332,"ctures (Tilk et al., 2016; Hong et al., 2018). On the evaluation side, those works made use of the experimental materials of the study by Lenci (2011), which are, however, limited to agentverb-patient triples. The recently-introduced DTFit dataset (Vassallo et al., 2018) is, in comparison, larger in size and provides more variety of fillers and roles (including instruments, locations and time). Other studies introduced larger datasets, but focused on more specific notions of event plausibility (e.g. the plausibility depending on the physical properties of the participants) (Wang et al., 2018; Porada et al., 2019; Ko et al., 2019). Contributions: 1. we propose a methodology to adapt TLM S to the dynamic estimation of thematic fit, using a dataset that contains several types of argument combinations differing for their typicality; 2. we present a comprehensive evaluation of various TLM S on this task, performed by comparing them to a strong distributional baseline; 3. we conduct further analysis aimed at identifying the potential limitations of TLM S as models of GEK. Our results are relevant for researchers interested in assessing the linguistic abilities of TLM S, as well as those working on applicat"
2021.starsem-1.1,2020.aacl-main.26,1,0.745247,"ected by these variations by design, since its predictions are based on semantic roles derived from the syntactic analysis of the sentence, which is explicitly provided to the model. 6 designed for this task, DTFit. Results show that TLM S scores positively correlate with human judgments. However, they do not significantly outperform the distributional prototype-based model (SDM) that we selected for comparison. This confirms the ability of SDM to dynamically update the semantic representation of a sentence, which was recently shown for the challenging task of logical metonymy interpretation (Rambelli et al., 2020). However, we decided to go beyond the simple evaluation against human judgments. We carried out several additional small-scale experiments with the specific aim to understand which factors could affect the predictions of TLM S. The results suggest that models are often too dependent on what they observe during training and lack some key aspects of human event knowledge. In particular, we observed that, in some cases, they are unable to compose all elements of the input to make predictions, and they tend to rely more on salient local associations between words. However, further analysis is nee"
2021.starsem-1.1,D17-1068,1,0.776185,"ilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which can be either a typical or atypical filler of a given role in the event described by the tuple (cf. Table 1). The dataset includes tuples of different lengths, and the typicality of a given argument de"
2021.starsem-1.1,W16-2518,0,0.014262,"ing the linguistic abilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which can be either a typical or atypical filler of a given role in the event described by the tuple (cf. Table 1). The dataset includes tuples of different lengths, and the typicality o"
2021.starsem-1.1,D16-1017,0,0.0195321,"ic is checking the report), and the task is to assign a higher thematic fit/typicality score to the former. Notice that the two sentences differ only for one argument, and that the “atypical” one might, however, be a common filler with respect to the verb target role (e.g., report is a typical patient for check, it is just less plausible in combination with mechanic as an agent). Several models have tried to tackle the “dynamic” version of the thematic fit task, either based on classical distributional spaces (Chersoni et al., 2016, 2019) or on more sophisticated neural network architectures (Tilk et al., 2016; Hong et al., 2018). On the evaluation side, those works made use of the experimental materials of the study by Lenci (2011), which are, however, limited to agentverb-patient triples. The recently-introduced DTFit dataset (Vassallo et al., 2018) is, in comparison, larger in size and provides more variety of fillers and roles (including instruments, locations and time). Other studies introduced larger datasets, but focused on more specific notions of event plausibility (e.g. the plausibility depending on the physical properties of the participants) (Wang et al., 2018; Porada et al., 2019; Ko e"
2021.starsem-1.1,E09-1094,0,0.0557104,"stributional baseline; 3. we conduct further analysis aimed at identifying the potential limitations of TLM S as models of GEK. Our results are relevant for researchers interested in assessing the linguistic abilities of TLM S, as well as those working on applications involving TLM S, such as text generation. 2 Related Work In its classical form, the thematic fit estimation task consists in comparing a candidate argument or filler (e.g., wine) with the typical fillers of a given verb role (e.g., agent, patient, etc.), either in the form of exemplars previously attested in a corpus (Erk, 2007; Vandekerckhove et al., 2009; Erk et al., 2010) or in the form of a vector-based prototype (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015a,b; Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020). Additionally, recent studies explored the use of masked language modeling with BERT for scoring the candidate arguments (Metheniti et al., 3 3.1 Experimental Settings Dataset The DTFit (Vassallo et al., 2018) dataset has been specifically designed for the evaluation of dynamic 2 thematic fit. 1 The dataset contains pairs of tuples that differ only for one element, which"
atkins-etal-2002-resources,2001.mtsummit-papers.39,1,\N,Missing
atkins-etal-2002-resources,bel-etal-2000-simple,1,\N,Missing
atkins-etal-2002-resources,calzolari-etal-2002-towards,1,\N,Missing
attardi-etal-2010-resource,picca-etal-2008-supersense,0,\N,Missing
attardi-etal-2010-resource,C04-1053,0,\N,Missing
attardi-etal-2010-resource,W02-1001,0,\N,Missing
attardi-etal-2010-resource,W03-1022,0,\N,Missing
attardi-etal-2010-resource,W06-1670,0,\N,Missing
attardi-etal-2010-resource,W03-0423,0,\N,Missing
attardi-etal-2010-resource,roventini-etal-2000-italwordnet,0,\N,Missing
bartolini-etal-2002-lexicon,W00-1903,0,\N,Missing
bartolini-etal-2002-lexicon,lenci-etal-2000-opposites,1,\N,Missing
bartolini-etal-2004-hybrid,W02-1501,1,\N,Missing
bartolini-etal-2004-semantic,bartolini-etal-2002-lexicon,1,\N,Missing
bartolini-etal-2004-semantic,C02-2011,0,\N,Missing
bartolini-etal-2006-creation,C04-1021,0,\N,Missing
bertagna-etal-2004-content,P98-1013,0,\N,Missing
bertagna-etal-2004-content,C98-1013,0,\N,Missing
bosco-etal-2010-comparing,W06-2920,0,\N,Missing
bosco-etal-2010-comparing,W08-1004,0,\N,Missing
bosco-etal-2010-comparing,P07-1122,1,\N,Missing
bosco-etal-2010-comparing,P09-1008,0,\N,Missing
bosco-etal-2010-comparing,D07-1096,1,\N,Missing
calzolari-etal-2002-towards,J87-3006,0,\N,Missing
calzolari-etal-2002-towards,bel-etal-2000-simple,1,\N,Missing
calzolari-etal-2004-enabler,binnenpoorte-etal-2002-field,0,\N,Missing
D16-1099,P14-1023,0,0.15199,"uction Distributional Semantic Models (DSMs) have become a staple in natural language processing. The various parameters of DSMs — e.g. size of context windows, weighting schemes, dimensionality reduction techniques, and similarity measures — have been thoroughly studied (Weeds et al., 2004; Sahlgren, 2006; Riordan and Jones, 2011; Bullinaria and Levy, 2012; Levy et al., 2015), and are now well understood. The impact of various processing models — matrix-based models, neural networks, and hashing methods — have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., ¨ 2015; Schnabel et al., 2015; Osterlund et al., 2015; Sahlgren et al., 2016). The consensus interpretation of such experiments seems to be that the choice of processing model is less important than the parameterization of the models, since the various processing models all result in more or less equivalent DSMs (provided that the parameterization is comparable). Alessandro Lenci University of Pisa via Santa Maria 36 56126 Pisa Italy alessandro.lenci@unipi.it One of the least researched aspects of DSMs is the effect on the various models of data size and frequency range of the t"
D16-1099,J15-4004,0,0.517589,"SGNS. These dimensionalities have been reported to perform well for the respective models (Landauer and Dumais, 1997; Sahlgren ¨ et al., 2008; Mikolov et al., 2013; Osterlund et al., 2015). All DSMs use the same parameters as far as possible with a narrow context window of ±2 words, which has been shown to produce good results in semantic tasks (Sahlgren, 2006; Bullinaria and Levy, 2012). We use five standard benchmark tests in these experiments; two multiple-choice vocabulary tests (the TOEFL synonyms and the ESL synonyms), and three similarity/relatedness rating benchmarks (SimLex-999 (SL) (Hill et al., 2015), MEN (Bruni et al., 2014), and Stanford Rare Words (RW) (Luong et al., 2013)). The vocabulary tests measure the synonym relation, while the similarity rating tests measure a broader notion of semantic similarity (SL and RW) or relatedness (MEN).4 The results for the vocabulary tests are given in accuracy (i.e., percentage of correct answers), while the results for the similarity tests are given in Spearman rank correlation. 4 Comparison by data size Table 1 summarizes the results over the different test settings. The most notable aspect of these results 3 Such drastic reduction has a negative"
D16-1099,W14-1618,0,0.0237821,"hod to build distributional vectors. Since our main goal here is 975 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 975–980, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics to gain an understanding of the effect of data size and frequency range on the various models, we focus primarily on the differences in processing models, hence the following typology of DSMs. Explicit matrix models We here include what could be referred to as explicit models, in which each vector dimension corresponds to a specific context (Levy and Goldberg, 2014). The baseline model is a simple co-occurrence matrix F (in the following referred to as CO for CoOccurrence). We also include the model that results from applying Positive Pointwise Mutual Information (PPMI) to the co-occurrence matrix. PPMI is defined as simply discarding any negative values of the PMI, computed as: PMI(a, b) = log fab × T fa fb (1) where fab is the co-occurrence count of word a and word b, fa and fb are the individual frequencies of the words, and T is the number of tokens in the data.1 Factorized matrix models This type of model applies an additional factorization of the w"
D16-1099,Q15-1016,0,0.36231,"items of various frequency. Our results show that neural network-based models underperform when the data is small, and that the most reliable model over data of varying sizes and frequency ranges is the inverted factorized model. 1 Introduction Distributional Semantic Models (DSMs) have become a staple in natural language processing. The various parameters of DSMs — e.g. size of context windows, weighting schemes, dimensionality reduction techniques, and similarity measures — have been thoroughly studied (Weeds et al., 2004; Sahlgren, 2006; Riordan and Jones, 2011; Bullinaria and Levy, 2012; Levy et al., 2015), and are now well understood. The impact of various processing models — matrix-based models, neural networks, and hashing methods — have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., ¨ 2015; Schnabel et al., 2015; Osterlund et al., 2015; Sahlgren et al., 2016). The consensus interpretation of such experiments seems to be that the choice of processing model is less important than the parameterization of the models, since the various processing models all result in more or less equivalent DSMs (provided that the parameteriza"
D16-1099,W13-3512,0,0.146958,"Missing"
D16-1099,D15-1024,1,0.88355,"Missing"
D16-1099,D15-1036,0,0.0594156,"s) have become a staple in natural language processing. The various parameters of DSMs — e.g. size of context windows, weighting schemes, dimensionality reduction techniques, and similarity measures — have been thoroughly studied (Weeds et al., 2004; Sahlgren, 2006; Riordan and Jones, 2011; Bullinaria and Levy, 2012; Levy et al., 2015), and are now well understood. The impact of various processing models — matrix-based models, neural networks, and hashing methods — have also enjoyed considerable attention lately, with at times conflicting conclusions (Baroni et al., 2014; Levy et al., ¨ 2015; Schnabel et al., 2015; Osterlund et al., 2015; Sahlgren et al., 2016). The consensus interpretation of such experiments seems to be that the choice of processing model is less important than the parameterization of the models, since the various processing models all result in more or less equivalent DSMs (provided that the parameterization is comparable). Alessandro Lenci University of Pisa via Santa Maria 36 56126 Pisa Italy alessandro.lenci@unipi.it One of the least researched aspects of DSMs is the effect on the various models of data size and frequency range of the target items. The only previous work in this"
D16-1099,C04-1146,0,0.198782,"Missing"
D16-1099,L16-1053,1,\N,Missing
D16-1205,N09-1003,0,0.141123,"Missing"
D16-1205,J10-4006,1,0.880419,"sts that verbs activate expectations on their typical argument nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010). Experimental subjects seem to exploit a rich event knowledge to activate or inhibit dynamically the representations of the potential arguments. This phenomenon, generally referred to as thematic fit (McRae et al., 1998), supports the idea of a mental lexicon arranged as a web of mutual expectations. Some past works in computational linguistics (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Greenberg et al., 2015) modeled thematic fit estimations by means of dependencybased or of thematic roles-based DSMs. However, these semantic spaces are built similarly to traditional DSMs as they split verb arguments into separate vector dimensions. By using syntactic-semantic links, they encode the relation between an event and each of its participants, but they do not encode directly the relation between participants co-occurring in the same event. Another trend of studies in the NLP community aimed at the introduction of richer contextual features i"
D16-1205,J90-1003,0,0.591713,"ical association measure. 4 4.1 Evaluation Corpus and DSMs We trained our DSMs on the RCV1 corpus, which contains approximately 150 million words (Lewis et al., 2004). The corpus was tagged with the tagger described in Dell’Orletta (2009) and dependencyparsed with DeSR (Attardi et al., 2009). RCV1 was chosen for two reasons: i) to show that our joint context-based representation can deal with data 1969 sparseness even with a training corpus of limited size; ii) to allow a comparison with the results reported by Melamud et al. (2014). All DSMs adopt Positive Pointwise Mutual Information (PPMI; Church and Hanks (1990)) as a context weighting scheme and vary according to three main parameters: i) type of contexts; ii) number of dimensions; iii) application of Singular Value Decomposition (SVD; see Landauer et al. (1998)). For what concerns the first parameter, we developed three types of DSMs: a) traditional bag-ofwords DSMs, where the features are content words co-occurring with the target in a window of width 2; b) dependency-based DSMs, where the features are words in a direct dependency relation with the target; c) joint context-based DSMs, using the joint features described in the previous section. The"
D16-1205,N15-1003,0,0.0186207,"nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010). Experimental subjects seem to exploit a rich event knowledge to activate or inhibit dynamically the representations of the potential arguments. This phenomenon, generally referred to as thematic fit (McRae et al., 1998), supports the idea of a mental lexicon arranged as a web of mutual expectations. Some past works in computational linguistics (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Greenberg et al., 2015) modeled thematic fit estimations by means of dependencybased or of thematic roles-based DSMs. However, these semantic spaces are built similarly to traditional DSMs as they split verb arguments into separate vector dimensions. By using syntactic-semantic links, they encode the relation between an event and each of its participants, but they do not encode directly the relation between participants co-occurring in the same event. Another trend of studies in the NLP community aimed at the introduction of richer contextual features in DSMs, mostly based on word windows. The first example was the"
D16-1205,J15-4004,0,0.051174,"l. (2014) have proposed the Probabilistic Distributional Similarity (PDS), based on the intuition that two words, w1 and w2 , are similar if they are likely to occur in each other’s contexts. PDS assigns a high similarity score when both p(w1 |contexts of w2 ) and p(w2 | contexts of w1 ) are high. We tried to test variations of this measure with our representation, but we were not able to achieve satisfying results. Therefore, we report here only the scores with the cosine. 4.3 Datasets The DSMs are evaluated on two test sets: VerbSim (Yang and Powers, 2006) and the verb subset of SimLex-999 (Hill et al., 2015). The former includes 130 verb pairs, while the latter includes 222 verb pairs. Both datasets are annotated with similarity judgements, so we measured the Spearman correlation between them and the scores assigned by the model. The VerbSim dataset allows for comparison with Melamud et al. (2014), since they also evaluated their model on this test set, achieving a Spearman correlation score of 0.616 and outperforming all the baseline methods. The verb subset of SimLex-999, at the best of our knowledge, has never been used as a benchmark dataset for verb similarity. The SimLex dataset is known fo"
D16-1205,W11-0607,1,0.892111,"expectations on their typical argument nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010). Experimental subjects seem to exploit a rich event knowledge to activate or inhibit dynamically the representations of the potential arguments. This phenomenon, generally referred to as thematic fit (McRae et al., 1998), supports the idea of a mental lexicon arranged as a web of mutual expectations. Some past works in computational linguistics (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Greenberg et al., 2015) modeled thematic fit estimations by means of dependencybased or of thematic roles-based DSMs. However, these semantic spaces are built similarly to traditional DSMs as they split verb arguments into separate vector dimensions. By using syntactic-semantic links, they encode the relation between an event and each of its participants, but they do not encode directly the relation between participants co-occurring in the same event. Another trend of studies in the NLP community aimed at the introduction of richer contextual features in DSMs, mostl"
D16-1205,W14-1619,0,0.128219,"measure such proximity, even though other measures have been proposed (Weeds et al., 2004; Santus et al., 2016). Most of DSMs adopt a bag-of-words approach, that is they turn a text span (i.e., a word window or a parsed sentence) into a set of words and they register separately the co-occurrence of each word with a given target. The problem with this approach is that valuable information concerning word interrelations in a context gets lost, because words co-occurring with a target are treated as independent features. This is why works like Ruiz-Casado et al. (2005), Agirre et al. (2009) and Melamud et al. (2014) proposed to introduce richer contexts in distributional spaces, by using entire word windows as features. These richer contexts proved to be helpful to semantically represent verbs, which are characterized by highly context-sensitive meanings, and complex argument structures. In fact, two verbs may share independent words as features despite being very dissimilar from the semantic point of view. For instance kill and heal share the same object nouns in The doctor healed the patient and the The poison killed the patient, but are highly different if we consider their joint dependencies as a sin"
D16-1205,Y16-2021,1,0.794205,"pus. 1 Introduction Distributional Semantic Models (DSMs) rely on the Distributional Hypothesis (Harris, 1954; Sahlgren, 2008), stating that words occurring in similar contexts have similar meanings. On such theoretical grounds, word co-occurrences extracted from corpora are used to build semantic representations in the form of vectors, which have become very popular in the NLP community. Proximity between word vectors is taken as an index of meaning similarity, and vector cosine is generally adopted to measure such proximity, even though other measures have been proposed (Weeds et al., 2004; Santus et al., 2016). Most of DSMs adopt a bag-of-words approach, that is they turn a text span (i.e., a word window or a parsed sentence) into a set of words and they register separately the co-occurrence of each word with a given target. The problem with this approach is that valuable information concerning word interrelations in a context gets lost, because words co-occurring with a target are treated as independent features. This is why works like Ruiz-Casado et al. (2005), Agirre et al. (2009) and Melamud et al. (2014) proposed to introduce richer contexts in distributional spaces, by using entire word windo"
D16-1205,C04-1146,0,0.156904,"Missing"
D16-1205,N15-1050,0,\N,Missing
D17-1068,W16-2506,0,0.0164198,"ler vectors for that verb-specific role. Differently from Baroni and Lenci, the core and novel aspect of our proposal, described in the following subsections, is that we do not simply measure the correlation between all the features of candidate and prototype vectors (as vector cosine would do on unsorted vectors), but rather we rank and filter the features, computing the weighted overlap with a rank-based similarity measure inspired by AP Syn, a recent proposal by Santus 5 For an overview on the limitations of vector cosine, see: Li and Han (2013); Dinu et al. (2015); Schnabel et al. (2015); Faruqui et al. (2016); Santus et al. (2016a). 6 An early proposal going in this direction is the predication theory by Kintsch (2001), which exploited Latent Semantic Analysis to select only the vector features that are appropriate for predicate-argument composition. 7 See also the so-called ’strong version’ of the Distributional Hypothesis (Miller and Charles, 1991; Lenci, 2008). 650 et al. (2016a,b,c) which has shown interesting results in synonymy detection and similarity estimation. As we will show in the next sections, the new metric assigns high scores to candidate fillers sharing many salient contexts with"
D17-1068,W15-1106,0,0.649512,"redictor variable allowing to disambiguate between possible structural analyses.1 More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (Vandekerckhove et al., 2009). Starting from the work of Erk et al. (2010), several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularly sensitive to linguistic detail and interpretability of the vector space”. The present wo"
D17-1068,P14-1023,0,0.20658,"several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularly sensitive to linguistic detail and interpretability of the vector space”. The present work sets itself among the unsupervised approaches to thematic fit estimation. By relying on explicit and interpretable count-based vector representations, we propose a simple, cognitively-inspired, and efficient thematic fit model using information extracted from dependency-pa"
D17-1068,N15-1003,0,0.495651,"redictor variable allowing to disambiguate between possible structural analyses.1 More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (Vandekerckhove et al., 2009). Starting from the work of Erk et al. (2010), several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularly sensitive to linguistic detail and interpretability of the vector space”. The present wo"
D17-1068,J10-4006,1,0.0718801,"tensively used in sentence comprehension studies on constraint-based models, mainly as a predictor variable allowing to disambiguate between possible structural analyses.1 More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (Vandekerckhove et al., 2009). Starting from the work of Erk et al. (2010), several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularl"
D17-1068,W11-0607,1,0.856257,"ce comprehension studies on constraint-based models, mainly as a predictor variable allowing to disambiguate between possible structural analyses.1 More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (Vandekerckhove et al., 2009). Starting from the work of Erk et al. (2010), several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularly sensitive t"
D17-1068,P07-1028,0,0.0834578,"ning a performance comparable to Lenci (2011). Related Work Erk et al. (2010) were, at the best of our knowledge, the first authors to measure the correlation between human-elicited thematic fit ratings and the scores assigned by a syntax-based Distributional Semantic Model (DSM). More specifically, their gold standard consisted of the human judgments collected by McRae et al. (1998) and Pad´o (2007). The plausibility of each verb-filler pair was computed as the similarity between new candidate nouns and previously attested exemplars for each specific verb-role pairing (as already proposed in Erk (2007)). Baroni and Lenci (2010) evaluated their Distributional Memory (henceforth DM)4 framework on the same datasets, adopting an approach to the task that has become dominant in the literature: for each verb role, they built a prototype vector by averaging the dependency-based vectors of its most typical fillers. The higher the similarity of a noun with a role prototype, the higher its plausibility as a filler for that role. Lenci (2011) has later extended the model to account for the dynamic update of the expectations on an argument, depending on how another role is filled. By using the same DM"
D17-1068,J10-4007,0,0.269326,"Missing"
D17-1068,W16-2518,0,0.70671,"have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance. This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularly sensitive to linguistic detail and interpretability of the vector space”. The present work sets itself among the unsupervised approaches to thematic fit estimation. By relying on explicit and interpretable count-based vector representations, we propose a simple, cognitively-inspired, and efficient thematic fit model using information extracted from dependency-parsed corpora. The key features of our proposal are a) prototypical representations of verb-specific thematic roles, based on feature weighting and filtering of second order cont"
D17-1068,D15-1036,0,0.0134542,"of the most typical filler vectors for that verb-specific role. Differently from Baroni and Lenci, the core and novel aspect of our proposal, described in the following subsections, is that we do not simply measure the correlation between all the features of candidate and prototype vectors (as vector cosine would do on unsorted vectors), but rather we rank and filter the features, computing the weighted overlap with a rank-based similarity measure inspired by AP Syn, a recent proposal by Santus 5 For an overview on the limitations of vector cosine, see: Li and Han (2013); Dinu et al. (2015); Schnabel et al. (2015); Faruqui et al. (2016); Santus et al. (2016a). 6 An early proposal going in this direction is the predication theory by Kintsch (2001), which exploited Latent Semantic Analysis to select only the vector features that are appropriate for predicate-argument composition. 7 See also the so-called ’strong version’ of the Distributional Hypothesis (Miller and Charles, 1991; Lenci, 2008). 650 et al. (2016a,b,c) which has shown interesting results in synonymy detection and similarity estimation. As we will show in the next sections, the new metric assigns high scores to candidate fillers sharing many"
D17-1068,D16-1017,0,0.762572,"s they computed the scores also for the instruments and for the locations of the Ferretti datasets (Ferretti et al., 2001). Greenberg et al. (2015a,b) further developed the TypeDM and the role-based models, investigating the effects of verb polysemy on human thematic fit judgments and introducing a hierarchical agglomerative clustering algorithm into the prototype creation process. Their goal was to cluster together typical fillers into multiple prototypes, corresponding to different verb senses, and their results showed constant improvements of the performance of the DM-based model. Finally, Tilk et al. (2016) presented two neural network architectures for generating probability distributions over selectional preferences for each thematic role. Their models took advantage of supervised training on two role-labeled corpora to optimize the distributional representation for thematic fit modeling, and managed to obtain significant improvements over the other systems on almost all the evaluation datasets. They also evaluated their model on the task of composing and updating verb argument expectations, obtaining a performance comparable to Lenci (2011). Related Work Erk et al. (2010) were, at the best of"
D17-1068,E09-1094,0,0.123359,"rlap Enrico Santus1 , Emmanuele Chersoni2 , Alessandro Lenci3 and Philippe Blache2 enrico santus@sutd.edu.sg emmanuelechersoni@gmail.com alessandro.lenci@unipi.it philippe.blache@univ-amu.fr 1 Singapore University of Technology and Design 2 Aix-Marseille University 3 University of Pisa Abstract has been extensively used in sentence comprehension studies on constraint-based models, mainly as a predictor variable allowing to disambiguate between possible structural analyses.1 More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (Vandekerckhove et al., 2009). Starting from the work of Erk et al. (2010), several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b). Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit"
D17-1068,Y16-2021,1,0.913379,"rb-specific role. Differently from Baroni and Lenci, the core and novel aspect of our proposal, described in the following subsections, is that we do not simply measure the correlation between all the features of candidate and prototype vectors (as vector cosine would do on unsorted vectors), but rather we rank and filter the features, computing the weighted overlap with a rank-based similarity measure inspired by AP Syn, a recent proposal by Santus 5 For an overview on the limitations of vector cosine, see: Li and Han (2013); Dinu et al. (2015); Schnabel et al. (2015); Faruqui et al. (2016); Santus et al. (2016a). 6 An early proposal going in this direction is the predication theory by Kintsch (2001), which exploited Latent Semantic Analysis to select only the vector features that are appropriate for predicate-argument composition. 7 See also the so-called ’strong version’ of the Distributional Hypothesis (Miller and Charles, 1991; Lenci, 2008). 650 et al. (2016a,b,c) which has shown interesting results in synonymy detection and similarity estimation. As we will show in the next sections, the new metric assigns high scores to candidate fillers sharing many salient contexts with the verb-specific rol"
D17-1068,L16-1723,1,0.934831,"rb-specific role. Differently from Baroni and Lenci, the core and novel aspect of our proposal, described in the following subsections, is that we do not simply measure the correlation between all the features of candidate and prototype vectors (as vector cosine would do on unsorted vectors), but rather we rank and filter the features, computing the weighted overlap with a rank-based similarity measure inspired by AP Syn, a recent proposal by Santus 5 For an overview on the limitations of vector cosine, see: Li and Han (2013); Dinu et al. (2015); Schnabel et al. (2015); Faruqui et al. (2016); Santus et al. (2016a). 6 An early proposal going in this direction is the predication theory by Kintsch (2001), which exploited Latent Semantic Analysis to select only the vector features that are appropriate for predicate-argument composition. 7 See also the so-called ’strong version’ of the Distributional Hypothesis (Miller and Charles, 1991; Lenci, 2008). 650 et al. (2016a,b,c) which has shown interesting results in synonymy detection and similarity estimation. As we will show in the next sections, the new metric assigns high scores to candidate fillers sharing many salient contexts with the verb-specific rol"
D17-1068,J90-1003,0,\N,Missing
D17-1068,Q15-1016,0,\N,Missing
declerck-etal-2004-towards,W03-1905,1,\N,Missing
declerck-etal-2004-towards,buitelaar-etal-2004-towards,1,\N,Missing
declerck-etal-2004-towards,broeder-etal-2004-large,1,\N,Missing
declerck-etal-2004-towards,capstick-etal-2002-collate,1,\N,Missing
dellorletta-etal-2006-searching,bel-etal-2000-simple,1,\N,Missing
E14-4008,J10-4006,1,0.410132,"c Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). 38 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38–42, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we introduce SLQS, a new entropy-based distributional measure that aims to identify hypernyms by providing a distributional characterization of their semantic generality. We assess it with two tasks: (i.) the identification of the broader term in hyponym-hypernym pairs (directionality task); (ii.) the discrimination of hypernymy among other semantic relations (detectio"
E14-4008,J07-2002,0,0.0337174,"Distributional Semantic Models (DSMs) have gained much attention in computational linguistics as unsupervised methods to build lexical semantic representations from corpus-derived co-occurrences encoded as distributional vectors (Sahlgren, 2006; Turney and Pantel, 2010). DSMs rely on the Distributional Hypothesis (Harris, 1954) and model lexical semantic similarity as a function of distributional similarity, which is most commonly measured with the vector cosine (Turney and Pantel, 2010). DSMs have achieved impressive results in tasks such as synonym detection, semantic categorization, etc. (Padó and Lapata, 2007; Baroni and Lenci, 2010). 38 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38–42, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics In this paper, we introduce SLQS, a new entropy-based distributional measure that aims to identify hypernyms by providing a distributional characterization of their semantic generality. We assess it with two tasks: (i.) the identification of the broader term in hyponym-hypernym pairs (directionality task); (ii.) the discrimination of hypernymy among other sem"
E14-4008,W09-0215,0,0.851838,"Missing"
E14-4008,P05-1014,0,0.914436,"Missing"
E14-4008,W03-1011,0,0.848579,"we propose SLQS, which measures the semantic generality of a word by the entropy of its statistically most prominent contexts. Related work The problem of identifying asymmetric relations like hypernymy has so far been addressed in distributional semantics only in a limited way (Kotlerman et al., 2010) or treated through semisupervised approaches, such as pattern-based approaches (Hearst, 1992). The few works that have attempted a completely unsupervised approach to the identification of hypernymy in corpora have mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: , = SLQS: A new entropy-based measure For every term wi we identify the N most associated contexts c (where N is a parameter empirically set to 50)1. The association strength has been calculated with Local Mutual Information (LMI; Evert, 2005). For each"
E14-4008,C04-1146,0,0.70813,"h measures the semantic generality of a word by the entropy of its statistically most prominent contexts. Related work The problem of identifying asymmetric relations like hypernymy has so far been addressed in distributional semantics only in a limited way (Kotlerman et al., 2010) or treated through semisupervised approaches, such as pattern-based approaches (Hearst, 1992). The few works that have attempted a completely unsupervised approach to the identification of hypernymy in corpora have mostly relied on some versions of the Distributional Inclusion Hypothesis (DIH; Weeds and Weir, 2003; Weeds et al., 2004), according to which the contexts of a narrow term are also shared by the broad term. One of the first proposed measures formalizing the DIH is WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), which quantifies the weights of the features f of a narrow term u that are included into the set of features of a broad term v: , = SLQS: A new entropy-based measure For every term wi we identify the N most associated contexts c (where N is a parameter empirically set to 50)1. The association strength has been calculated with Local Mutual Information (LMI; Evert, 2005). For each selected context c,"
E14-4008,S12-1012,1,0.739596,"Missing"
E14-4008,C92-2082,0,\N,Missing
E14-4008,W11-2501,1,\N,Missing
E14-4008,W09-0200,0,\N,Missing
J10-4006,W04-3221,0,0.0241765,"ss 1996; Schutze 1997; Bullinaria and Levy 2007; Pado´ and Lapata 2007). These collocates are seen as proxies for various attributes of the concepts that the words denote. Words that share many collocates denote concepts that share many attributes. Both dog and puppy may occur near owner, leash, and bark, because these words denote properties that are shared by dogs and puppies. The attributional similarity between dog and puppy, as approximated by their contextual similarity, will be very high. DSMs succeed in tasks like synonym detection (Landauer and Dumais 1997) or concept categorization (Almuhareb and Poesio 2004) because such tasks require a measure of attributional similarity that favors concepts that share many properties, such as synonyms and co-hyponyms. However, many other tasks require detecting different kinds of semantic similarity. Turney (2006b) deﬁnes relational similarity as the property shared by pairs of words (e.g, dog–animal and car–vehicle) linked by similar semantic relations (e.g., hypernymy), despite the fact that the words in one pair might not be attributionally similar to those in the other pair (e.g., dog is not attributionally similar to car, nor is animal to vehicle). Turney"
J10-4006,J09-2001,0,0.00850195,"Missing"
J10-4006,W09-0201,1,0.731884,"Missing"
J10-4006,P06-1127,0,0.0061999,"Missing"
J10-4006,J90-1003,0,0.292435,"tive subspace with the example pairs n, qr  as unique non-zero dimensions, and a negative subspace with nonzero dimensions corresponding to all w1 , w2  pairs such that w1 is one of the training nominal concepts, and w2 is not a quale qr in the example pairs. We then measure the length of each link in both subspaces. For example, we measure the length of the obj link in a subspace characterized by n, qtelic  example pairs, and the length of obj in a subspace characterized by n, w2  pairs that are probably not Telic examples. We compute the pointwise mutual information (PMI) statistic (Church and Hanks 1990) on these lengths to ﬁnd the links that are most typical of the positive subspace corresponding to each qualia role. PMI, with respect to other association measures, ﬁnds more speciﬁc links, which is good for our purposes. However, it is also notoriously 709 Computational Linguistics Volume 36, Number 4 ¨ prone to over-estimating the importance of rare items (Manning and Schutze 1999, Chapter 5). Thus, before selecting the top 20 links ranked by PMI, we ﬁlter out those links that do not have at least 10 non-zero dimensions in the positive subspace. Many parameters here should be tuned more sys"
J10-4006,P07-1112,0,0.0657599,"istributional approaches. For instance, the notion of property plays a key role in cognitive science and linguistics, which both typically represent concepts as clusters of properties (Jackendoff 1990; Murphy 2002). In this case, the task is not to ﬁnd out that dog is similar to puppy or cat, but that it has a tail, it is used for hunting, and so on. Almuhareb (2006), Baroni and Lenci (2008), and Baroni et al. (2010) use the words co-occurring with a noun to approximate its most prototypical properties and correlate distributionally derived data with the properties produced by human subjects. Cimiano and Wenderoth (2007) instead focus on that subset of noun properties known in lexical semantics as qualia roles (Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutive parts of a concept or its function (this is in turn analogous to the problem of relation extraction). The distributional semantics methodology also extends to more complex aspects of word meaning, addressing issues such as verb selectional preferences (Erk 2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs"
J10-4006,W02-0908,0,0.10217,"mes 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture 674 Baroni and Lenci Distributional Memory a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004; Rapp 2004). Corpus-based semantic models have also attracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008). Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Innovative applications of corpus-based semantics are also being explored in linguistics,"
J10-4006,P08-1027,0,0.0416155,"Missing"
J10-4006,P08-1079,0,0.0243438,"Missing"
J10-4006,J93-1003,0,0.0930495,"Missing"
J10-4006,P07-1028,0,0.0923357,"o approximate its most prototypical properties and correlate distributionally derived data with the properties produced by human subjects. Cimiano and Wenderoth (2007) instead focus on that subset of noun properties known in lexical semantics as qualia roles (Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutive parts of a concept or its function (this is in turn analogous to the problem of relation extraction). The distributional semantics methodology also extends to more complex aspects of word meaning, addressing issues such as verb selectional preferences (Erk 2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture 674 Baroni and Lenci Distributional Memory a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, esp"
J10-4006,J06-1005,0,0.00554731,"Missing"
J10-4006,S07-1003,0,0.00910646,"Missing"
J10-4006,C92-2082,0,0.506399,"t them in the corpus. Pairs of words that are connected by similar patterns probably hold similar relations, that is, they are relationally similar. For example, we can hypothesize that dog–tail is more similar to car–wheel than to dog–animal, because the patterns connecting dog and tail (of, have, etc.) are more like those of car–wheel than like those of dog–animal (is a, such as, etc.). Turney uses the relational space to implement tasks such as solving analogies and harvesting instances of relations. Although they are not explicitly expressed in these terms, relation extraction algorithms (Hearst 1992, 1998; Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006) also rely on relational similarity, and focus on learning one relation type at a time (e.g., ﬁnding parts). Although semantic similarity, either attributional or relational, has the lion’s share in DSMs, similarity is not the only aspect of meaning that is addressed by distributional approaches. For instance, the notion of property plays a key role in cognitive science and linguistics, which both typically represent concepts as clusters of properties (Jackendoff 1990; Murphy 2002). In this case, the task is not to ﬁnd"
J10-4006,W09-0205,1,0.793259,"rney’s model. For instance, language is full of productive semantic phenomena, such as the selectional preferences of verbs with respect to unseen arguments (eating topinambur vs. eating sympathy). Predicting the plausibility of unseen pairs cannot, by deﬁnition, be tackled by the current version of PairClass, which will have to be expanded to deal with such cases, perhaps adopting ideas similar to those we present (that are, in turn, inspired by Turney’s own work on attributional and relational similarity). A ﬁrst step in this direction, within a framework ˇ similar to Turney’s, was taken by Herdagdelen and Baroni (2009). Turney (2007) explicitly formalizes the set of corpus-extracted word–link–word triples as a tensor, and was our primary source of inspiration in formalizing DM in these terms. The focus of Turney’s article, however, is on dimensionality reduction techniques applied to tensors, and the application to corpora is only brieﬂy discussed. Moreover, Turney only derives the W1×LW2 space from the tensor, and does not discuss the possibility of using the tensor-based formalization to unify different views of semantic data, which is instead our main point. The higher-order tensor dimensionality reducti"
J10-4006,W09-3207,1,0.583088,"Missing"
J10-4006,heylen-etal-2008-modelling,0,0.00902465,"opical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004; Rapp 2004). Corpus-based semantic models have also attracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008). Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Innovative applications of corpus-based semantics are also being explored in linguistics, for instance in the study of semantic change (Sagi, Kaufmann, and Clark 2009), lexical variation (Peirsman and Speelman 2009), and for the analysis of multiword expressions (Alishahi and Stevenson 2008). The wealth and variety of semantic issue"
J10-4006,P98-2127,0,0.334913,"son, and James 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture 674 Baroni and Lenci Distributional Memory a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004; Rapp 2004). Corpus-based semantic models have also attracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008). Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Innovative applications of corpus-based semantics are also being e"
J10-4006,J01-3003,0,0.025609,"l properties and correlate distributionally derived data with the properties produced by human subjects. Cimiano and Wenderoth (2007) instead focus on that subset of noun properties known in lexical semantics as qualia roles (Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutive parts of a concept or its function (this is in turn analogous to the problem of relation extraction). The distributional semantics methodology also extends to more complex aspects of word meaning, addressing issues such as verb selectional preferences (Erk 2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture 674 Baroni and Lenci Distributional Memory a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Gre"
J10-4006,D08-1095,0,0.00529955,"Missing"
J10-4006,P08-1028,0,0.65488,"onality and similar issues in DSMs lie beyond the scope of this paper. However, there is nothing in DM that prevents it from interacting with any of the research directions we have mentioned here. Indeed, we believe that the generalized nature of DM represents a precondition for distributional semantics to be able to satisfactorily address these more advanced challenges. A multi-purpose, distributional semantic resource like DM can allow researchers to focus on the next steps of semantic modeling. These include compositionality, but also modulating word meaning in context (Erk and Pado´ 2008; Mitchell and Lapata 2008) and ﬁnding ways to embed the distributional memory in complex NLP systems (e.g., for question answering or textual entailment) or even embodied agents and robots. DM-style triples predicating a relation between two entities are common currency in many semantic representation models (e.g., semantic networks) and knowledgeexchange formalisms such as RDF. This might also pave the way to the integration of corpus-based information with other knowledge sources. It is hard to see how such integration could be pursued within generalized systems, such as PairClass (Turney 2008), that require keeping"
J10-4006,E09-1071,0,0.0662303,"on unfolds the tensor into a matrix with the n-th index indexing the rows of the matrix and a column for each pair of elements from the other two tensor indices. For example, the mode-1 matricization of the tensor in Table 2 results in a matrix with the entries vertically arranged as they are in the table, but replacing the second and third indices with a single index ranging from 1 to 6 (cf. matrix A of Table 3). More explicitly, in mode-n matricization we map each tensor entry (i1 , i2 , ..., iN ) to matrix entry (in , j), where j is computed as in Equation (1), adapted from Kolda and Bader (2009). N  j=1+ k =1 k=n ((ik − 1) k −1 Dm ) (1) m= 1 m=n For example, if we apply mode-1 matricization to the tensor of dimensionality 3 × 2 × 3 in Table 2, we obtain the matrix A3×6 in Table 3 (ignore the labels for now). The tensor entry x3,1,1 is mapped to the matrix cell a3,1 ; x3,2,3 is mapped to a3,6 ; and x1,2,2 is mapped to a1,4 . Observe that each column of the matrix is a mode-1 ﬁber of the tensor: The ﬁrst column is the x∗11 ﬁber; the second column is the x∗21 ﬁber, and so on. Matricization has various mathematically interesting properties and practical applications in computations i"
J10-4006,J07-2002,0,0.461166,"Missing"
J10-4006,D07-1042,0,0.00556845,"Missing"
J10-4006,P06-1015,0,0.0533181,"by similar patterns probably hold similar relations, that is, they are relationally similar. For example, we can hypothesize that dog–tail is more similar to car–wheel than to dog–animal, because the patterns connecting dog and tail (of, have, etc.) are more like those of car–wheel than like those of dog–animal (is a, such as, etc.). Turney uses the relational space to implement tasks such as solving analogies and harvesting instances of relations. Although they are not explicitly expressed in these terms, relation extraction algorithms (Hearst 1992, 1998; Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006) also rely on relational similarity, and focus on learning one relation type at a time (e.g., ﬁnding parts). Although semantic similarity, either attributional or relational, has the lion’s share in DSMs, similarity is not the only aspect of meaning that is addressed by distributional approaches. For instance, the notion of property plays a key role in cognitive science and linguistics, which both typically represent concepts as clusters of properties (Jackendoff 1990; Murphy 2002). In this case, the task is not to ﬁnd out that dog is similar to puppy or cat, but that it has a tail, it is used"
J10-4006,W09-0202,0,0.0117113,"on of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008). Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Innovative applications of corpus-based semantics are also being explored in linguistics, for instance in the study of semantic change (Sagi, Kaufmann, and Clark 2009), lexical variation (Peirsman and Speelman 2009), and for the analysis of multiword expressions (Alishahi and Stevenson 2008). The wealth and variety of semantic issues that DSMs are able to tackle conﬁrms the importance of looking at distributional data to explore meaning, as well as the maturity of this research ﬁeld. However, if we looked from a distance at the whole ﬁeld of DSMs we would see that, besides the general assumption shared by all models that information about the context of a word is an important key in grasping its meaning, the elements of difference overcome the commonalities. For instance, DSMs geared towards attributiona"
J10-4006,2003.mtsummit-papers.42,0,0.0856492,"rovided by Pado´ and Lapata (2007) is built around the notion of a matrix M|B|×|T |, with B the set of basis elements representing the contexts used to compare the distributional similarity of the target elements T. This binary structure is inherently suitable for approaches that represent distributional data in terms of unstructured co-occurrence relations between an element and a context. The latter can be either documents (Landauer and Dumais 1997; Grifﬁths, Steyvers, and Tenenbaum 2007) or lexical collocates within a certain distance from the ¨ target (Lund and Burgess 1996; Schutze 1997; Rapp 2003; Bullinaria and Levy 2007). We will refer to such models as unstructured DSMs, because they do not use the linguistic structure of texts to compute co-occurrences, and only record whether the target occurs 676 Baroni and Lenci Distributional Memory in or close to the context element, without considering the type of this relation. For instance, an unstructured DSM might derive from a sentence like The teacher eats a red apple that eat is a feature shared by apple and red, just because they appear in the same context window, without considering the fact that there is no real linguistic relation"
J10-4006,rapp-2004-freely,0,0.0104959,", and so forth. Finally, some DSMs capture 674 Baroni and Lenci Distributional Memory a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004; Rapp 2004). Corpus-based semantic models have also attracted the attention of lexical semanticists as a way to provide the notion of synonymy with a more robust empirical foundation (Geeraerts 2010; Heylen et al. 2008). Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010). Innovative applications of corpus-based semantics are also being explored in linguistics, for instance in the study of semanti"
J10-4006,W09-0203,0,0.0984847,"Missing"
J10-4006,W09-0214,0,0.0677036,"Missing"
J10-4006,J06-2001,0,0.00767065,"Missing"
J10-4006,N03-1032,0,0.0154253,"Missing"
J10-4006,P06-1040,0,0.0818364,"li, University of Pisa, Via Santa Maria 36, 56126 Pisa (PI), Italy. E-mail: alessandro.lenci@ling.unipi.it. Submission received: 11 January 2010; revised submission received: 15 April 2010; accepted for publication: 1 June 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 4 as a function of the degree of overlap among their linguistic contexts. Conversely, the format of distributional representations greatly varies depending on the speciﬁc aspects of meaning they are designed to model. The most straightforward phenomenon tackled by DSMs is what Turney (2006b) calls attributional similarity, which encompasses standard taxonomic semantic relations such as synonymy, co-hyponymy, and hypernymy. Words like dog and puppy, for example, are attributionally similar in the sense that their meanings share a large number of attributes: They are animals, they bark, and so on. Attributional similarity is typically addressed by DSMs based on word collocates (Grefenstette 1994; Lund and ¨ Burgess 1996; Schutze 1997; Bullinaria and Levy 2007; Pado´ and Lapata 2007). These collocates are seen as proxies for various attributes of the concepts that the words denote"
J10-4006,J06-3003,0,0.626941,"of the target word pairs that are provided with the test set. The second data set (NS) comes from Nastase and Szpakowicz (2003). It pertains to the classiﬁcation of 600 modiﬁer–noun pairs and it is of interest because it proposes a very ﬁne-grained categorization into 30 semantic classes, such as Cause (cloud–storm), Purpose (album–picture), Location-At (pain–chest), Location-From (visitor–country), Frequency (superstition–occasional), Time-At (snack–midnight), and so on. The modiﬁers can be nouns, adjectives, or adverbs. Because the data set is not split into training and test data we follow Turney (2006b) and perform leave-one-out cross-validation. The data set also comes with a coarser ﬁve-way classiﬁcation. Our unreported results on it are comparable, in terms of relative performance, to the ones for the 30-way classiﬁcation. ´ The last data set (OC) contains 1,443 noun–noun compounds classiﬁed by O S´eaghdha and Copestake (2009) into 6 relations: Be (celebrity–winner), Have (door–latch), In (air–disaster), Actor (university–scholarship), Instrument ( freight–train), and About 701 Computational Linguistics Volume 36, Number 4 ´ S´eaghdha and Copestake (2009) and references there. We use th"
J10-4006,C08-1114,0,0.0445781,"of our tensor, with better tuple extraction and weighting techniques on one side, and better matrix manipulation and similarity measurement on the other. As long as the former operations result in data that can be arranged into a weighted tuple structure, and the latter procedures act on vectors, such innovations ﬁt into the DM framework and can be used to improve performance on tasks deﬁned on any space derivable from the DM tensor. Whereas the model proposed by Pado´ and Lapata (2007) is designed only to address tasks involving the measurement of the attributional similarity between words, Turney (2008) shares with DM the goal of unifying attributional and relational similarity under the same distributional model. He observes that tasks that are traditionally solved with an attributional similarity approach can be recast as relational similarity tasks. Instead of determining whether two words are, for example, synonymous by looking at the features they share, we can learn what the typical patterns are that connect synonym pairs when they co-occur (also known as, sometimes called, etc.), and make a decision about a potential synonym pair based on their occurrence in similar contexts. Given a"
J10-4006,W09-0211,0,0.0394864,"Missing"
J10-4006,veale-hao-2008-acquiring,0,0.0244473,"Missing"
J10-4006,C02-1114,0,0.00854843,"Missing"
J10-4006,zarcone-lenci-2008-computational,1,0.375361,"s produced by human subjects. Cimiano and Wenderoth (2007) instead focus on that subset of noun properties known in lexical semantics as qualia roles (Pustejovsky 1995), and use lexical patterns to identify, for example, the constitutive parts of a concept or its function (this is in turn analogous to the problem of relation extraction). The distributional semantics methodology also extends to more complex aspects of word meaning, addressing issues such as verb selectional preferences (Erk 2007), argument alternations (Merlo and Stevenson 2001; Joanis, Stevenson, and James 2008), event types (Zarcone and Lenci 2008), and so forth. Finally, some DSMs capture 674 Baroni and Lenci Distributional Memory a sort of “topical” relatedness between words: They might ﬁnd, for example, a relation between dog and ﬁdelity. Topical relatedness, addressed by DSMs based on document distributions such as LSA (Landauer and Dumais 1997) and Topic Models (Grifﬁths, Steyvers, and Tenenbaum 2007), is not further discussed in this article. DSMs have found wide applications in computational lexicography, especially for automatic thesaurus construction (Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff et al. 2004;"
J10-4006,J09-3004,0,0.309913,"hat closely resembles our W1 W2×L space, whereas we tackle this task under the more standard W1×LW2 view. It is an open question whether there are principled ways to select the optimal space conﬁguration for a given semantic task. In this article, we limit ourselves to proving that each space derived through tensor matricization is semantically interesting in the sense that it provides the proper ground to address some semantic task. Feature selection/reweighting and dimensionality reduction have been shown to improve DSM performance. For instance, the feature bootstrapping method proposed by Zhitomirsky-Geffet and Dagan (2009) boosts the precision of a DSM in lexical entailment recognition. Even if these methods can be applied to DM as well, we did not use them in our experiments. The results presented subsequently should be regarded as a “baseline” performance that could be enhanced in future work by exploring various task-speciﬁc parameters (we will come back in the conclusion to the role of parameter tuning in DM). This is consistent with our current aim of focusing on the generality and adaptivity of DM, rather than on task-speciﬁc optimization. As a ﬁrst, important step in this latter direction, however, we co"
J10-4006,D08-1094,0,\N,Missing
J10-4006,W09-0208,0,\N,Missing
J10-4006,J13-3006,0,\N,Missing
J10-4006,C98-2122,0,\N,Missing
L16-1148,N09-2066,0,0.0452941,"Missing"
L16-1148,baroni-etal-2004-introducing,0,0.468739,", PLANT, POSSESSION , PROCESS , QUAN TITY, RELATION , SHAPE , STATE , SUBSTANCE , TIME. In this way, the selectional preferences that can be inferred from the lexical sets in sentence (4) can be characterized as: 6. [PERSON]subj lire [COMMUNICATION - ARTIFACT]obj If lexical sets describe the behavior of verbs as observed in a corpus, selectional preferences make an important generalization over the semantic properties of arguments. 3.3. A Resource on Italian Argument Structure The database described by Lenci et al. (2012) has been built by applying the LexIt framework to the ‘La Repubblica’ (Baroni et al., 2004) corpus (ca. 331 millions tokens) and to a dump of the Italian section of Wikipedia (ca. 152 millions of tokens). The resulting dataset in the former setting encodes 3,873 verbs, 12,766 nouns and 5,559 adjectives, while the latter setting resulted in the characterization of 2,831 verbs and 11,056 nouns. The LexIt extracted methodology has been evaluated by comparing the SCF frames available in three gold standard dictionaries for 100 test verbs against those automatically extracted from the ‘La Repubblica’ corpus, filtered by exploiting either a MLE-based threshold or a LMI-based threshold (se"
L16-1148,P13-1133,0,0.0160537,"ng its relative simplicity, this strategy turned to be significantly effective to limit the influence of both annotation errors (e.g., wrong syntactic parses) and marginally relevant dependency patterns, often due to a idiosyncratic sequences of adjuncts in a sentence. 4.3. • categorizing the fillers into WordNet supersenses by following the general methodology described by Resnik (1996). To extract the candidate synsets and general classes we resorted to the Wordnet Libre du Franc¸ais lexicon (Sagot and Fiˇser, 2008) available in the Open Multilingual WordNet repository (Bond and Paik, 2012; Bond and Foster, 2013); • aggregating the single co-occurrence for each information of interest (i.e., slots, SCFs, fillers, semantic classes), thus collecting, for each predicate of interest, its joint frequency with: 1. each SCF; 2. each slot (in isolation or in the context of each SCF) ; 3. each filler for given a slot (in isolation or in the context of each SCF) ; 4. each semantic class (in isolation or in the context of each SCF). • calculating the strength of association to be loaded in the distributional profiles. Various weighting measures can be selected, among which relative frequency and common associati"
L16-1148,J10-4007,0,0.379513,"Missing"
L16-1148,korhonen-etal-2006-large,0,0.0424249,"ense disambiguation, machine translation, knowledge extraction (Schulte im Walde, 2009; Korhonen, 2009), so that the automatic acquisition of argument structure information is a long-standing topic in computational linguistics. By embracing the theoretical assumption that the semantics of a predicate and the morpho-syntactic realization of its arguments are intimately related (Levin, 1993; Bresnan, 1996; Roland and Jurafsky, 2002; Levin and Rappaport Hovav, 2005), the last thirty years have witnessed the development of automatic methods for the identification of verb subcategorization frames (Korhonen et al., 2006; Messiant et al., 2008; Schulte im Walde, 2009), selectional preferences (Resnik, 1996; Light and Greiff, 2002; Erk et al., 2010) and diathesis alternation (McCarthy, 2001). The literature reports few examples of automatically built, wide coverage, lexica encoding this information, a.k.a. combinatorial lexica, among which notable mentions include VALEX for English verbs (Korhonen et al., 2006), LexSchem from French verbs (Messiant et al., 2008) and LexIt for Italian verbs, nouns and adjectives (Lenci et al., 2012). These resources represent a reference point for the work presented in these pa"
L16-1148,Y09-1003,0,0.231356,"and-built resources were counterbalanced by the higher cost-effectiveness and flexibility of the automatic methods, features that come in handy especially for languages, domains of topics for which hand-built resources are not available or are just too limited in their scope. Among the several kinds of information that can be included in a lexicon, the description of linguistic entities at the syntax-semantic interface proved to be useful for many traditional Natural Language Processing tasks such as word-sense disambiguation, machine translation, knowledge extraction (Schulte im Walde, 2009; Korhonen, 2009), so that the automatic acquisition of argument structure information is a long-standing topic in computational linguistics. By embracing the theoretical assumption that the semantics of a predicate and the morpho-syntactic realization of its arguments are intimately related (Levin, 1993; Bresnan, 1996; Roland and Jurafsky, 2002; Levin and Rappaport Hovav, 2005), the last thirty years have witnessed the development of automatic methods for the identification of verb subcategorization frames (Korhonen et al., 2006; Messiant et al., 2008; Schulte im Walde, 2009), selectional preferences (Resnik,"
L16-1148,R09-1038,0,0.0615085,"Missing"
L16-1148,lenci-etal-2012-lexit,1,0.848274,"of automatic methods for the identification of verb subcategorization frames (Korhonen et al., 2006; Messiant et al., 2008; Schulte im Walde, 2009), selectional preferences (Resnik, 1996; Light and Greiff, 2002; Erk et al., 2010) and diathesis alternation (McCarthy, 2001). The literature reports few examples of automatically built, wide coverage, lexica encoding this information, a.k.a. combinatorial lexica, among which notable mentions include VALEX for English verbs (Korhonen et al., 2006), LexSchem from French verbs (Messiant et al., 2008) and LexIt for Italian verbs, nouns and adjectives (Lenci et al., 2012). These resources represent a reference point for the work presented in these pages, where we investigated the possibility to automatically extract distributional information about French predicates by adapting an existing framework, LexIt. This led to the realization of LexFr, an automatically built French lexicon describing the syntactic and semantic properties of the argument structures of 2,493 verbs, 7,939 nouns and 2,628 adjectives. As in the original framework, the behavior of a group of target predicates is characterized by a series of statistical information (a.k.a. distributional pro"
L16-1148,messiant-etal-2008-lexschem,0,0.84581,"chine translation, knowledge extraction (Schulte im Walde, 2009; Korhonen, 2009), so that the automatic acquisition of argument structure information is a long-standing topic in computational linguistics. By embracing the theoretical assumption that the semantics of a predicate and the morpho-syntactic realization of its arguments are intimately related (Levin, 1993; Bresnan, 1996; Roland and Jurafsky, 2002; Levin and Rappaport Hovav, 2005), the last thirty years have witnessed the development of automatic methods for the identification of verb subcategorization frames (Korhonen et al., 2006; Messiant et al., 2008; Schulte im Walde, 2009), selectional preferences (Resnik, 1996; Light and Greiff, 2002; Erk et al., 2010) and diathesis alternation (McCarthy, 2001). The literature reports few examples of automatically built, wide coverage, lexica encoding this information, a.k.a. combinatorial lexica, among which notable mentions include VALEX for English verbs (Korhonen et al., 2006), LexSchem from French verbs (Messiant et al., 2008) and LexIt for Italian verbs, nouns and adjectives (Lenci et al., 2012). These resources represent a reference point for the work presented in these pages, where we investiga"
L16-1148,P08-3010,0,0.0267644,"exical elements, among which 6,825 verbs, 37,530 nouns and 10,483 adjectives. Its lexical framework, Alexina (Architecture pour les LEXiques INformatiques et leur Acquisition), has been successfully exploited to create Lefff -like resource in other languages such as Italian and Dutch. manual effort, namely in the need for manual correction of the output of the automatic module. LexSchem has been the first automatically built lexical resource characterizing the subcategorization behavior of a large set of French verbs (Messiant et al., 2008). This information has been extracted by using ASSCI (Messiant, 2008), a subcategorization frames acquisition system whose main task is to extract all the patterns for each target verb and exploit a MLE-based strategy (see section 5) to identify the more plausible set of subcategorization frames. By applying ASSCI to a newspaper corpus composed by 10 years of the French newspaper ‘Le Monde’, 336 subcategorization frames have been isolated and used to describe the combinatorial behavior of 3,297 French verbs. The goodness of the LexSchem framework has been tested by comparing the entries for 20 test verbs against a gold standard dictionary, thus showing 0.79 pre"
L16-1148,P07-1115,0,0.362513,"Extractor The goal of the first module is to analyze a dependencyparsed corpus in order to identify the occurrences of each predicate and to extract, for each occurrence with: the list of dependencies headed by the target predicate; the lexical elements filling each syntactic position. This process is carried out by an algorithm developed to filter and interpret the linguistic annotation available in the input. As a consequence, the design of this algorithm is strictly dependent on the properties of the linguistic annotation available in the corpus. Furthermore, we agree with those scholars (Preiss et al., 2007, inter alia) suggesting that the calibration of this module on the behavior of the specific parser has the effect of reducing the parser-specific bias in the input data. In the original LexIt framework, data were extracted from the linguistic annotation realized by the Part-Of-Speech tagger described in Dell’Orletta (2009), together with the dependency parser DeSR (Attardi and Dell’Orletta, 2009). For this first release of LexFr, we tailored our extraction algorithm on the annotation provided by Talismane, an opensource suite of NLP tools proving a transition based statistical dependency pars"
L16-1148,sagot-2010-lefff,0,0.0344787,"inguishing between 14 semantic classes. • Dicovalence (van den Eynde and Mertens, 2010) is a valency lexicon containing information for more than 3,700 verbs. It is based on the pronominal approach (van den Eynde and Blanche-Benveniste, 1978), a research method that treats pronouns as semantic primitives due to the purely linguistic nature and finite inventory of this lexical class. Accordingly, in this resource valence slots are characterized by the set of accepted pronouns, which subsume the possible lexicalizations of that slot. • The Lexique des Formes Fl´echies du Franc¸ais, a.k.a Lefff (Sagot, 2010), is a semi-automatically built morphological and syntactic lexicon. The several manual validation, correction and extension steps needed to build this resource led some authors to describe it as “an automatically acquired morphological lexicon [...] which has been manually supplemented with partial syntactic information” (Messiant et al., 2008). Version 3.0.1 of this resource describes more than 110k lexical elements, among which 6,825 verbs, 37,530 nouns and 10,483 adjectives. Its lexical framework, Alexina (Architecture pour les LEXiques INformatiques et leur Acquisition), has been successf"
L16-1148,J06-2001,0,0.115271,"Missing"
L16-1148,J90-1003,0,\N,Missing
L16-1347,J10-4006,1,0.766294,"xicon for Italian. As a follow-up study, in this paper we compare different context selection approaches to improve emotive VSMs by exploiting the syntagmatic and paradigmatic properties of the target words. Distributional semantics is grounded on Harris’s distributional hypothesis (Harris, 1954), which states that semantically similar words tend to appear in similar contexts. From a computational point of view, each word is represented by a weighted feature vector, where features correspond to other words that co-occur with the target word in the surrounding context (Turney and Pantel, 2010; Baroni and Lenci, 2010). In order to build ItEM, we exploited the distributional hypothesis, which we have generalized to emotions: A word hwi is strongly associated with an emotion hei if it co-occurs in similar contexts of other words strongly associated with hei. In order to implement this hypothesis, we represented each emotion as a centroid vector built starting from a set of seed words strongly associated to the target emotion and we measured the paradigmatic similarity between the word and the emotion. Besides co-occurring in similar contexts, words with the same (or similar) emotive connotation also tend to"
L16-1347,baroni-etal-2004-introducing,0,0.0575127,"lected the top 10 distinctive nouns, adjectives and verbs for each hemotion, PoSi pair, in order to further expand the set of the seeds used to build the distributional space. In (Passaro et al., 2015) we showed that the process of stepwise seed expansion used to calculate the emotive centroids may be repeated several times, in order to optimize the system and improve its performance. 3. Table 1: Sample of seed lemmas Distributional expansion: We exploited distributional semantic methods to expand the seeds collected in the first phase and populate ItEM. We extracted from La Repubblica corpus(Baroni et al., 2004) and itWaC (Baroni et al., 2009), the list of the 30,000 most frequent nouns, verbs and adjectives, which were used as targets and contexts in a co-occurrence matrix collected using a five-word window centered on the target lemma. For each hemotion, PoSi pair, we built a centroid vector from the vectors of the seeds belonging to that emotion and PoS, obtaining in total 24 centroids. We re-weighted the co-occurrence matrix using the Pointwise Mutual Information (Church and Hanks, 1990), and in particular the Positive PMI (PPMI), in which negative scores are changed to zero (Niwa and Nitta, 1994"
L16-1347,Y10-1092,0,0.0159622,"ut government decisions. Emotion lexica, in which lemmas are associated to the emotions they evoke, are knowledge sources that can help the development of detection algorithms and prediction systems. In recent years great attention has been given to sentiment polarity recognition, but a new trend is leading to the development of novel methods to automatically classify the emotions expressed in an opinionated piece of text (Turney and Littman, 2003; Pang and Lee, 2008; Liu, 2012; Liu, 2015) as well as to the building of annotated lexical resources like SentiWordNet (Esuli and Sebastiani, 2006; Das and Bandyopadhyay, 2010), WordNet Affect (Strapparava and Valitutti, 2004) or EmoLex (Mohammad and Turney, 2013). One major bottleneck of research on emotion detection is the lack of emotive resources. This problem is even more pressing for Italian. In Passaro et al. (2015) we proposed a language-independent distributional semantic method to semi-automatically build an emotive lexicon starting from a small number of seed terms and by exploiting crowdsourcing methods. The output of this methodology is ItEM (Italian EMotive lexicon), a high-coverage emotion lexicon for Italian. As a follow-up study, in this paper we co"
L16-1347,esuli-sebastiani-2006-sentiwordnet,0,0.0141866,"ions and points of views about government decisions. Emotion lexica, in which lemmas are associated to the emotions they evoke, are knowledge sources that can help the development of detection algorithms and prediction systems. In recent years great attention has been given to sentiment polarity recognition, but a new trend is leading to the development of novel methods to automatically classify the emotions expressed in an opinionated piece of text (Turney and Littman, 2003; Pang and Lee, 2008; Liu, 2012; Liu, 2015) as well as to the building of annotated lexical resources like SentiWordNet (Esuli and Sebastiani, 2006; Das and Bandyopadhyay, 2010), WordNet Affect (Strapparava and Valitutti, 2004) or EmoLex (Mohammad and Turney, 2013). One major bottleneck of research on emotion detection is the lack of emotive resources. This problem is even more pressing for Italian. In Passaro et al. (2015) we proposed a language-independent distributional semantic method to semi-automatically build an emotive lexicon starting from a small number of seed terms and by exploiting crowdsourcing methods. The output of this methodology is ItEM (Italian EMotive lexicon), a high-coverage emotion lexicon for Italian. As a follow"
L16-1347,C94-1049,0,0.229825,"Baroni et al., 2004) and itWaC (Baroni et al., 2009), the list of the 30,000 most frequent nouns, verbs and adjectives, which were used as targets and contexts in a co-occurrence matrix collected using a five-word window centered on the target lemma. For each hemotion, PoSi pair, we built a centroid vector from the vectors of the seeds belonging to that emotion and PoS, obtaining in total 24 centroids. We re-weighted the co-occurrence matrix using the Pointwise Mutual Information (Church and Hanks, 1990), and in particular the Positive PMI (PPMI), in which negative scores are changed to zero (Niwa and Nitta, 1994). To optimize the vector space, we followed the approach in (Polajnar and Clark, 2014) and we selected the top 240 contexts for each target word. As a last step, we applied singular value decomposition (SVD), reducing the matrix to 300 dimensions. The VSM allowed us to calculate our emotive scores by measuring the cosine similarity between the target lemmas and the centroid vectors: depending on the PoS of the target lemma, we measured the cosine similarity between the lemma and the eight emotive centroids corresponding to the target PoS. Context selection Another important leverage for optimi"
L16-1347,E14-1025,0,\N,Missing
L16-1347,strapparava-valitutti-2004-wordnet,0,\N,Missing
L16-1347,J09-3004,0,\N,Missing
L16-1419,P98-1046,0,0.226398,"et of “semantically related verbs sharing a range of linguistic properties, such as the possible realizations of their arguments and the particular interpretation associated with each possible argument realization.” (Levin, 2013:1). Understanding how verbs can be grouped and what is precisely the status of the resultant verb classes has been a major goal of many scholars operating in various and different research fields and from different approaches: lexical semantics, computational linguistics, cognitive sciences etc.(see among others Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001). A very large literature on verb classification has appeared since the seminal works by Fillmore, because semantically coherent verb classes are recognized as a useful device for capturing many generalizations over a vast range of properties, both within a given language and cross-linguistically, and can therefore be used as a valuable means of inquiry. English has been indubitably the more studied and analyzed language. Several classifications are currently available for English verbs (e.g.: Pinker, 1989; Jackendoff, 1990; Faber, 1999). The largest and most widely"
L16-1419,P04-2007,0,0.058338,"Missing"
L16-1419,W14-0131,0,0.0345449,"Missing"
L16-1419,P98-1099,0,0.0561374,"Missing"
L16-1419,kipper-etal-2006-extending,0,0.0848875,"Missing"
L16-1419,W04-2606,0,0.0894744,"Missing"
L16-1419,W99-0632,0,0.277695,"Missing"
L16-1419,lebani-etal-2014-bootstrapping,1,0.848625,"specific of a single verb (cf. for instance the notions of template and root in Rappaport-Hovav& Levin, 2003; Levin & Rappaport-Hovav, 2005). We assume here that both such components determine the constructions a verb participates in. Having described the general theoretical principles that have guided our work, we will now turn to analyze the methodology and the online resources that were used to build Italian verb classes. 3.1. Methodology and Resources Italian semantic classes were bootstrapped from a representative sample of Italian verbs, the 1000 most frequent Italian verbs, as found in Lebani et al. (2014): these are the verbal lemmas which are marked as highly frequent in the monolingual Italian dictionary Sabatini e Coletti (henceforth: S&C) (Sabatini & Coletti, 2012) - the only Italian dictionary that indicates the verb valency in the lexical entry - which were then matched with the corresponding verbs in the La Repubblica corpus (Baroni et al., 2004). The first step towards the building of a new class was always a corresponding Levin’s class (e.g. KILL verbs), from which we tried to individuate a comparable Italian class. We then extrapolated from the S&C sample the candidate members (e.g."
L16-1419,bel-etal-2000-simple,1,0.586394,"x-semantics interface, following the model of Levin (1993) and VerbNet, has gained prominence in the scientific community during the last two decades. Different versions of VerbNet-style lexicons have been developed cross-linguistically (see for example Pradet et al., 2014), but besides Merlo et al. (2002), no significant attempt in this direction has been made for Italian yet. 1 1 Other resources have been translated, adapted or created for Italian: the different semantic classifications of Italian WordNet(Pianta et al., 2002) and ItalWordNet(Roventini et al. 2000), or Simple’s verb classes (Lenci et al., 2000), which is partly inspired to the Generative Lexicon developed by Pustejovsky (1995).Jezek (2003) provided an independent syntactic-semantic classification for Italian verbs, limited to transitive and (unaccusative / unergative)intransitives. 2633 This work aims to fill this gap by developing a reliable, comprehensive and coherent method for the creation of Italian verb classes based on both syntactic and semantic ground. The present classification is rooted in the constructionist framework and modeled on the system of Levin (1993) and VerbNet. The paper is articulated as follows: in section t"
L16-1419,lenci-etal-2012-lexit,1,0.862969,"ided us with general information about valency and allowed patterns for each 3 The selectional preferences of the arguments are defined with respect to the ontology of semantic types presented by Lebani and Lenci, (2013). The taxonomy of arguments is minimal but linguistically plausible. 4 “to assassinate” 2635 verb; however, S&C only gives very coarse-grained dictionary-like information, that is does not provide any true insight on the verb distributional properties. This is why we combined its use with the exploration of the corpus-based lexical resource on Italian argument structure LexIt (Lenci et al., 2012). LexIt was particularly useful since it organizes the allowed syntactic frames by their frequency, thus distinguishing between typical and more rare and marked uses. The third resource that was consulted is the Italian section (Cennamo& Fabrizio, 2013) of the typological database ValPal (Hartmann et al. 2013).The ValPal database is part of the Leipzig Valency Classes Project, which aims to follow up cross‐linguistically the works by Levin (1993) for English and by Apresjan (1967) for Russian; the authors analyze the lexical realization of 80 verb meanings in 35 different languages. Selected m"
L16-1419,J01-3003,0,0.0883176,"related verbs sharing a range of linguistic properties, such as the possible realizations of their arguments and the particular interpretation associated with each possible argument realization.” (Levin, 2013:1). Understanding how verbs can be grouped and what is precisely the status of the resultant verb classes has been a major goal of many scholars operating in various and different research fields and from different approaches: lexical semantics, computational linguistics, cognitive sciences etc.(see among others Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001). A very large literature on verb classification has appeared since the seminal works by Fillmore, because semantically coherent verb classes are recognized as a useful device for capturing many generalizations over a vast range of properties, both within a given language and cross-linguistically, and can therefore be used as a valuable means of inquiry. English has been indubitably the more studied and analyzed language. Several classifications are currently available for English verbs (e.g.: Pinker, 1989; Jackendoff, 1990; Faber, 1999). The largest and most widely renown is however Levin (19"
L16-1419,P02-1027,0,0.0744946,"studies has sprung from Levin’s work; in particular, the broad-coverage online lexicon VerbNet (Kipper et al., 2000; Kipper-Schuler, 2005), which is almost completely based on Levin (1993) and on its subsequent implementations (see below). Research on cross‐linguistic verb classifications grounded in the syntax-semantics interface, following the model of Levin (1993) and VerbNet, has gained prominence in the scientific community during the last two decades. Different versions of VerbNet-style lexicons have been developed cross-linguistically (see for example Pradet et al., 2014), but besides Merlo et al. (2002), no significant attempt in this direction has been made for Italian yet. 1 1 Other resources have been translated, adapted or created for Italian: the different semantic classifications of Italian WordNet(Pianta et al., 2002) and ItalWordNet(Roventini et al. 2000), or Simple’s verb classes (Lenci et al., 2000), which is partly inspired to the Generative Lexicon developed by Pustejovsky (1995).Jezek (2003) provided an independent syntactic-semantic classification for Italian verbs, limited to transitive and (unaccusative / unergative)intransitives. 2633 This work aims to fill this gap by devel"
L16-1419,pradet-etal-2014-adapting,0,0.0294164,"Missing"
L16-1419,roventini-etal-2000-italwordnet,0,0.0472977,"guistic verb classifications grounded in the syntax-semantics interface, following the model of Levin (1993) and VerbNet, has gained prominence in the scientific community during the last two decades. Different versions of VerbNet-style lexicons have been developed cross-linguistically (see for example Pradet et al., 2014), but besides Merlo et al. (2002), no significant attempt in this direction has been made for Italian yet. 1 1 Other resources have been translated, adapted or created for Italian: the different semantic classifications of Italian WordNet(Pianta et al., 2002) and ItalWordNet(Roventini et al. 2000), or Simple’s verb classes (Lenci et al., 2000), which is partly inspired to the Generative Lexicon developed by Pustejovsky (1995).Jezek (2003) provided an independent syntactic-semantic classification for Italian verbs, limited to transitive and (unaccusative / unergative)intransitives. 2633 This work aims to fill this gap by developing a reliable, comprehensive and coherent method for the creation of Italian verb classes based on both syntactic and semantic ground. The present classification is rooted in the constructionist framework and modeled on the system of Levin (1993) and VerbNet. Th"
L16-1419,C96-2204,0,0.347742,"Missing"
L16-1419,J06-2001,0,0.103377,"Missing"
L16-1419,W09-3205,0,0.0874011,"Missing"
L16-1722,W11-2501,1,0.38469,"Missing"
L16-1722,E12-1004,0,0.22534,"Missing"
L16-1722,W09-0215,0,0.0557322,"Missing"
L16-1722,C92-2082,0,0.209128,"Missing"
L16-1722,P13-2078,0,0.00781986,"Missing"
L16-1722,N15-1098,0,0.476188,"Missing"
L16-1722,E14-1054,0,0.0390832,"Missing"
L16-1722,C14-1097,0,0.715404,"Missing"
L16-1722,E14-4008,1,0.754471,"close hypernym, which are therefore attributionally similar (Weeds et al., 2014). The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The fe"
L16-1722,Y14-1018,1,0.121991,"ntiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution (Santus et al., 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011). The ablation test has show"
L16-1722,W15-4208,1,0.746309,"s et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution (Santus et al., 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011). The ablation test has shown that four out of thirteen feat"
L16-1722,C08-1107,0,0.029533,"Missing"
L16-1722,W03-1011,0,0.807271,"ting hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly"
L16-1722,C14-1212,0,0.148115,"ymy in fact represents a key organization principle of semantic memory (Murphy, 2002), the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005). Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar (Weeds et al., 2014). The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have cl"
L16-1722,Y15-1021,1,0.87055,"Missing"
L16-1723,W11-2501,1,0.863897,"Missing"
L16-1723,J90-1003,0,0.489356,"ty measures play a fundamental role in tasks such as Information Retrieval (IR), Text Classification (TC), Text Summarization (TS), Question Answering (QA), Sentiment Analysis (SA), and so on (Terra and Clarke, 2003; Tungthamthiti et al., 2015). They can be either knowledge-based or corpus-based (Gomaa and Fahmy, 2013). The former rely on lexicons or semantic networks, such as WordNet (Fellbaum, 1998), measuring the distance between the nodes in the network. The latter, instead, compute the similarity between words relying on statistical information about their distributions in large corpora (Church and Hanks, 1990). Knowledge based approaches generally exploit hand-crafted resources. While being hand-crafted ensures high quality, it also entails arbitrariness and high development and update costs. This is the main reason why these resources are known for their limited coverage (Santus et al., 2015b). Such limitation has often prompted researchers to pursue hybrid approaches (Turney, 2001). A key assumption of corpus-based approaches is that similarity between words can be measured by looking at words co-occurrences. In particular, following the Distributional Hypothesis (Harris, 1954; Firth, 1957), thes"
L16-1723,C92-2082,0,0.177797,"approaches generally exploit the Distributional Hypothesis, according to which words that occur in similar contexts also have similar meanings (Harris, 1954). Although these approaches extract statistics from large corpora, they vary in the way they define what has to be considered context (i.e. lexical context, syntactic context, documents, etc.), how the association with such context is measured (e.g. frequency of co-occurrence, association measures like Pointwise Mutual Information, etc.), and how the association with the contexts is used to identify the similarity (Terra and Clarke, 2003; Hearst, 1992; Santus et al., 2014a; Santus et al., 2014b; Santus et al., 2016a). A common way to represent word meaning in NLP is by using vectors to encode the association between the target words and their contexts. The resulting vector space is generally referred as Vector Space Model (VSM) or, more specifically, as Distributional Semantic Model (DSM). In such vector space, word similarity can be calculated by using the Vector Cosine, which measures the angle between the vectors (Turney and Pantel, 2010). Other measures – such as Manhattan Distance, Dice’s Coefficient, Euclidean Distance, Jaccard Simil"
L16-1723,Q15-1016,0,0.605479,"ctor Cosine is generally considered to be the optimal choice (Bullinaria and Levy 2007). Another common way to represent word meaning is using word embeddings, which are vector-space word representations that are implicitly learned by the input-layer weights of neural networks. These models have shown a strong ability to capture synonymy and analogies (such as in the famous “King - Man + Woman = Queen” example, where Mikolov et al. (2013) subtract the vector of “Man” from the one of “King”, and then add the vector of “Woman”, obtaining a very similar vector to the one of “Queen”), even though Levy et al. (2015) have claimed that traditional count-based DSMs can achieve the same results if their hyperparameters are properly optimized. A well-known problem with the distributional approaches is that they rely on a very loose definition of similarity. In fact, vectors have as nearest neighbours not only synonyms, but also hypernyms, co-hyponyms, antonyms, as well as a wide range of other semantically related items (Santus et al., 2015). For this reason, several datasets have been proposed by the NLP community to test distributional similarity measures. Among the most common ones, there are the English a"
L16-1723,J07-2002,0,0.0420041,"Missing"
L16-1723,2003.mtsummit-papers.42,0,0.263036,"Missing"
L16-1723,W15-4208,1,0.935386,"Missing"
L16-1723,E14-4008,1,0.92777,"Missing"
L16-1723,N03-1032,0,0.173981,"Missing"
L16-1723,Y15-1021,1,0.830439,"Missing"
L16-1723,C08-1114,0,0.186322,"Missing"
lebani-etal-2014-bootstrapping,baroni-etal-2004-introducing,0,\N,Missing
lebani-etal-2014-bootstrapping,C10-1119,0,\N,Missing
lebani-etal-2014-bootstrapping,A00-2034,0,\N,Missing
lebani-etal-2014-bootstrapping,Y09-1003,0,\N,Missing
lebani-etal-2014-bootstrapping,P98-1046,0,\N,Missing
lebani-etal-2014-bootstrapping,C98-1046,0,\N,Missing
lebani-etal-2014-bootstrapping,P98-1013,0,\N,Missing
lebani-etal-2014-bootstrapping,C98-1013,0,\N,Missing
lebani-etal-2014-bootstrapping,J10-4006,1,\N,Missing
lebani-etal-2014-bootstrapping,lenci-etal-2012-lexit,1,\N,Missing
lebani-etal-2014-bootstrapping,P13-2129,0,\N,Missing
lebani-etal-2014-bootstrapping,W04-2605,0,\N,Missing
lebani-etal-2014-bootstrapping,roventini-etal-2000-italwordnet,0,\N,Missing
lenci-etal-2000-opposites,1995.iwpt-1.8,0,\N,Missing
lenci-etal-2000-opposites,W96-0209,0,\N,Missing
lenci-etal-2000-opposites,H91-1005,0,\N,Missing
lenci-etal-2000-opposites,W99-0407,1,\N,Missing
lenci-etal-2000-opposites,H94-1020,0,\N,Missing
lenci-etal-2002-multilingual,W96-0411,1,\N,Missing
lenci-etal-2002-multilingual,W99-0407,1,\N,Missing
lenci-etal-2002-multilingual,bel-etal-2000-simple,1,\N,Missing
lenci-etal-2008-unsupervised,J98-2002,0,\N,Missing
lenci-etal-2008-unsupervised,H91-1067,0,\N,Missing
lenci-etal-2008-unsupervised,J06-2001,0,\N,Missing
lenci-etal-2008-unsupervised,A97-1052,0,\N,Missing
lenci-etal-2008-unsupervised,J01-2001,0,\N,Missing
lenci-etal-2008-unsupervised,P98-2247,0,\N,Missing
lenci-etal-2008-unsupervised,C98-2242,0,\N,Missing
lenci-etal-2008-unsupervised,J01-3003,0,\N,Missing
lenci-etal-2008-unsupervised,zeman-sarkar-2000-learning,0,\N,Missing
lenci-etal-2008-unsupervised,L08-1000,0,\N,Missing
lenci-etal-2008-unsupervised,korhonen-etal-2006-large,0,\N,Missing
lenci-etal-2008-unsupervised,W97-0123,0,\N,Missing
lenci-etal-2010-building,baroni-etal-2004-introducing,0,\N,Missing
lenci-etal-2010-building,J06-2001,0,\N,Missing
lenci-etal-2010-building,N04-4008,0,\N,Missing
lenci-etal-2010-building,W07-0606,0,\N,Missing
lenci-etal-2010-building,ohara-2008-lexicon,0,\N,Missing
lenci-etal-2012-enriching,burchardt-etal-2006-salsa,0,\N,Missing
lenci-etal-2012-enriching,N09-2066,0,\N,Missing
lenci-etal-2012-enriching,N04-4008,0,\N,Missing
lenci-etal-2012-enriching,P06-2057,0,\N,Missing
lenci-etal-2012-enriching,J08-4004,0,\N,Missing
lenci-etal-2012-enriching,J05-1004,0,\N,Missing
lenci-etal-2012-enriching,ohara-2008-lexicon,0,\N,Missing
lenci-etal-2012-lexit,baroni-etal-2004-introducing,0,\N,Missing
lenci-etal-2012-lexit,messiant-etal-2008-lexschem,0,\N,Missing
lenci-etal-2012-lexit,N09-2066,0,\N,Missing
lenci-etal-2012-lexit,J06-2001,0,\N,Missing
lenci-etal-2012-lexit,J10-4007,0,\N,Missing
lenci-etal-2012-lexit,P98-2184,0,\N,Missing
lenci-etal-2012-lexit,C98-2179,0,\N,Missing
lenci-etal-2012-lexit,J01-3003,0,\N,Missing
lenci-etal-2012-lexit,P07-1115,0,\N,Missing
lenci-etal-2012-lexit,korhonen-etal-2006-large,0,\N,Missing
poesio-etal-2010-babyexp,J07-2002,0,\N,Missing
poesio-etal-2010-babyexp,P98-2127,0,\N,Missing
poesio-etal-2010-babyexp,C98-2122,0,\N,Missing
romeo-etal-2014-choosing,W99-0503,0,\N,Missing
romeo-etal-2014-choosing,bel-etal-2012-automatic,1,\N,Missing
romeo-etal-2014-choosing,N07-2002,1,\N,Missing
romeo-etal-2014-choosing,J12-3005,0,\N,Missing
romeo-etal-2014-choosing,P13-4006,0,\N,Missing
romeo-etal-2014-choosing,J10-4006,1,\N,Missing
romeo-etal-2014-choosing,P03-1059,0,\N,Missing
romeo-etal-2014-choosing,J01-3003,0,\N,Missing
S12-1012,J10-4006,1,0.261433,"ld also be found in contexts in which animals other than lions occur. 3 Experiments The main purpose of the experiments reported below is to investigate the ability of the directional similarity measures presented in section 2 to identify the hypernyms of a given target noun, and to discriminate hypernyms from terms related by symmetric P w (f ) semantic relations, such as coordinate terms. u f 2F F W eedsP rec(u, v) = P u v (1) We have represented lexical items with distribuf 2Fu wu (f ) tional feature vectors extracted from the TypeDM cosWeeds (M2) - this measure corresponds to the tensor (Baroni and Lenci, 2010). TypeDM is a pargeometrical average of WeedsPrec and the symmet- ticular instantiation of the Distributional Memory ric similarity between u and v, measured by their (DM) framework. In DM, distributional facts are vectors’ cosine: represented as a weighted tuple structure T , a set of weighted word-link-word tuples hhw1 , l, w2 i, i, q such that w1 and w2 are content words (e.g. nouns, cosW eeds(u, v) = M 1(u, v) ⇤ cos(u, v) (2) verbs, etc.), l is a syntagmatic co-occurrence links This is actually a variation of the balPrec measure between words in a text (e.g. syntactic dependenin Kotlerman"
S12-1012,W11-2501,1,0.595335,"ains 25,336 direct and inverse links formed by (partially lexicalized) syntactic dependencies and patproposed by Clarke (2009): terns. The weight is the Local Mutual InformaP tion (LMI) (Evert, 2005) computed on link type fref 2Fu Fv min(wu (f ), wv (f )) quency (negative LMI values are raised to 0). P ClarkeDE(u, v) = f 2Fu wu (f ) (3) 3.1 Test set invCL (M4) - this a new measure that we introduce and test here for the first time. It takes into account We have evaluated the directional similarity meanot only the inclusion of u in v, but also the non- sures on a subset of the BLESS data set (Baroni and Lenci, 2011), consisting of tuples expressing a reinclusion of v in u, both measured with ClarkeDE: lation between a target concept (henceforth referred to as concept) and a relatum concept (henceforth req ferred to as relatum). BLESS includes 200 distinct invCL(u, v) = M 3(u, v) ⇤ (1 M 3(v, u)) (4) English concrete nouns as target concepts, equally The intuition behind invCL is that, if v is a seman- divided between living and non-living entities, and tically broader term of u, then the features of u are grouped into 17 broader classes (e.g., BIRD, FRUIT, included in the features of v, but crucially the"
S12-1012,W09-0215,0,0.388774,"butional features of u is included in the feature vector of v as well. Since hypernymy is an asymmetric relation and hypernyms are semantically broader terms than their hyponyms, then we Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other and the meaning of the original word"
S12-1012,W09-3711,0,0.0222838,"yponyms, then we Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other and the meaning of the original word can be inferred from the new one. Its relevance for TE notwithstanding, this notion of lexical entailment is more general and looser than hypernymy. In fact, it encom"
S12-1012,W09-1109,0,0.0183371,"yponyms, then we Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other and the meaning of the original word can be inferred from the new one. Its relevance for TE notwithstanding, this notion of lexical entailment is more general and looser than hypernymy. In fact, it encom"
S12-1012,C92-2082,0,0.793923,"l (or asymmetric) similarity measures (Kotlerman et al., 2010). These measures all rely on some variation of the Distributional Inclusion Hypothesis, according to which if u is a semantically narrower term than v, then a significant number of salient distributional features of u is included in the feature vector of v as well. Since hypernymy is an asymmetric relation and hypernyms are semantically broader terms than their hyponyms, then we Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE)"
S12-1012,P98-2127,0,0.0868486,"cture T , a set of weighted word-link-word tuples hhw1 , l, w2 i, i, q such that w1 and w2 are content words (e.g. nouns, cosW eeds(u, v) = M 1(u, v) ⇤ cos(u, v) (2) verbs, etc.), l is a syntagmatic co-occurrence links This is actually a variation of the balPrec measure between words in a text (e.g. syntactic dependenin Kotlerman et al. (2010), the difference being that cies, etc.), and is a weight estimating the statiscosine is used as a symmetric similarity measure tical salience of that tuple. The TypeDM word set contains 30,693 lemmas (20,410 nouns, 5,026 verbs instead of the LIN measure (Lin, 1998). and 5,257 adjectives). The TypeDM link set conClarkeDE (M3) - a close variation of M1, tains 25,336 direct and inverse links formed by (partially lexicalized) syntactic dependencies and patproposed by Clarke (2009): terns. The weight is the Local Mutual InformaP tion (LMI) (Evert, 2005) computed on link type fref 2Fu Fv min(wu (f ), wv (f )) quency (negative LMI values are raised to 0). P ClarkeDE(u, v) = f 2Fu wu (f ) (3) 3.1 Test set invCL (M4) - this a new measure that we introduce and test here for the first time. It takes into account We have evaluated the directional similarity meanot"
S12-1012,P06-1015,0,0.0287256,"ic) similarity measures (Kotlerman et al., 2010). These measures all rely on some variation of the Distributional Inclusion Hypothesis, according to which if u is a semantically narrower term than v, then a significant number of salient distributional features of u is included in the feature vector of v as well. Since hypernymy is an asymmetric relation and hypernyms are semantically broader terms than their hyponyms, then we Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and"
S12-1012,C08-1107,0,0.0182836,"Missing"
S12-1012,W03-1011,0,0.909174,"hen a significant number of salient distributional features of u is included in the feature vector of v as well. Since hypernymy is an asymmetric relation and hypernyms are semantically broader terms than their hyponyms, then we Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other an"
S12-1012,C04-1146,0,0.954503,"Missing"
S12-1012,P05-1014,0,0.773144,"d Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other and the meaning of the original word can be inferred from the new one. Its relevance for TE notwithstanding, this notion of lexical entailment is more general and looser than hypernymy. In fact, it encompasses several standard semantic relations such as synonymy, hypernymy, metonymy, some cases of meronymy, etc. Differently from Kotlerman et al. (2010), here we focus on applying directional, asymmetric similarity measure"
S12-1012,J09-3004,0,0.0388268,"co-hyponym (coordinate) of the concept: halligator, coord, lizardi; HYPER: the relatum is a noun that is a hypernym of the concept: halligator, hyper, animali; MERO: the relatum is a noun referring to a part/component/organ/member of the concept, or something that the concept contains or is made of: halligator, mero, mouthi; RANDOM - N : the relatum is a random noun holding no semantic relation with the target concept: halligator, random n, messagei. Kotlerman et al. (2010) evaluate a set of directional similarity measure on a data set of valid and invalid (substitutable) lexical entailments (Zhitomirsky-Geffet and Dagan, 2009). However, as we said above, lexical entailment is defined as an asymmetric relation that covers various types of classic semantic relations, besides hypernymy . The choice of BLESS is instead motivated by the fact that here we focus on the ability of directional similarity measure to identify hypernyms. 3.2 Evaluation and results For each word x in the test set, we represented x in terms of a set Fx of distributional features hl, w2 i, such that in the TypeDM tensor there is a tuple hhw1 , l, w2 i, i, w1 = x. The feature weight wx (f ) is equal to the weight of the original DM tuple. Then, we"
S12-1012,C98-2122,0,\N,Missing
S17-1021,J10-4006,1,0.83231,"et al. (2001) for the reading times at the type-shifted noun (both conditions engendered significantly longer reading times than the preferred condition). k most salient subjects of V (e.g., the cosine between the vector of author and the centroid vector of the most salient objects of write); finally, θO,S is the cosine between the vector of O and the centroid vector built out of the k most salient direct objects occurring in events whose subject is S (e.g., the cosine between the vector of book and the prototype vector of the most salient objects of events whose subject is author). Following Baroni and Lenci (2010), we used LMI scores to identify the most salient fillers of each target-specific syntactic slot and we fixed k = 20. • σe is the salience score of the triple s, and it corresponds to the sum of the activation scores of i.) the full event represented by the triple and of ii.) the sub-events corresponding to all the partial combinations of the verb and its arguments. Each activation score is the conditional probability of the event given a lexical item in the test tuple. p-values LOW TYP MET X σ ei LOW TYP 0.31 Table 2: Results of the pairwise post-hoc comparisons for the three conditions on th"
S17-1021,W16-4102,1,0.734343,"ctures, with contributions from the context; iii.) Control is responsible for relating language to joint action and social interaction. Similarly to your book (= eating). The event retrieval cannot be explained in terms of qualia structures, as it is unlikely that the lexical entry for book includes something related to eating-events. 2 It should be pointed out that, unlike relevance theory which conceives world knowledge and linguistic knowledge as separate modules, GEK includes both linguistic and extralinguistic information. 3 A previous version of this model has already been introduced in Chersoni et al. (2016a), the main difference being the way the complexity score component based on Memory was computed (see section 5 and 6 of the 2016 paper). Moreover, the model was applied to a different task (i.e., the computation of context-sensitive argument typicality). 169 enriched forms of compositionality (Jackendoff, 1997), both being instances of the same general model of sentence processing. MUC, we argue that the comprehension of a sentence is an incremental process driven by the goal of constructing a coherent semantic representation of the event the speaker intends to communicate. Our model rests o"
S17-1021,D16-1205,1,0.887106,"Missing"
S17-1021,P09-3001,0,0.470032,"ent combination, it is read faster and it is more difficult to inhibit in a probe recognition task. The authors explained their results in the light of the words-ascues paradigm (Elman, 2009, 2014), which claims that the words in the mental lexicon are cues to event knowledge modulating language comprehension in an incremental fashion. Research in computational semantics has focused on two different aspects of the phenomenon: the first one is the retrieval of the covert event, which has been approached by means of either probabilistic methods (Lapata and Lascarides, 2003; Lapata et al., 2003; Shutova, 2009) or of distributional similarity-based thematic fit estimations (Zarcone et al., 2012), whereas the second aspect concerns modeling the experimental data about processing costs. Zarcone et al. (2013) showed that a distributional model of verb-object thematic fit can reproduce the reading times differences in the experimental conditions found by McElree et al. (2001) and Traxler et al. (2002). Their merits notwithstanding, a limit of the former studies is that they did not try to build a single model to account for both aspects involved in logical metonymy. The goal of this paper is twofold. Fi"
S17-1021,W13-0216,1,0.836998,"Missing"
S17-1021,J03-2004,0,0.65244,"he covert event is typical for that specific argument combination, it is read faster and it is more difficult to inhibit in a probe recognition task. The authors explained their results in the light of the words-ascues paradigm (Elman, 2009, 2014), which claims that the words in the mental lexicon are cues to event knowledge modulating language comprehension in an incremental fashion. Research in computational semantics has focused on two different aspects of the phenomenon: the first one is the retrieval of the covert event, which has been approached by means of either probabilistic methods (Lapata and Lascarides, 2003; Lapata et al., 2003; Shutova, 2009) or of distributional similarity-based thematic fit estimations (Zarcone et al., 2012), whereas the second aspect concerns modeling the experimental data about processing costs. Zarcone et al. (2013) showed that a distributional model of verb-object thematic fit can reproduce the reading times differences in the experimental conditions found by McElree et al. (2001) and Traxler et al. (2002). Their merits notwithstanding, a limit of the former studies is that they did not try to build a single model to account for both aspects involved in logical metonymy."
S17-1021,W12-1707,0,0.305975,"Missing"
S18-1117,S18-1161,0,0.0549544,"Missing"
S18-1117,S18-1155,0,0.0525256,"Missing"
S18-1117,P14-1023,0,0.496686,"er pairs like seed/seeds and hypernym/hyponym pairs like doctor/surgeon, since by definition there is no feature that a hypernym has but its hyponym does not have. Each of the three task organizers was given a third of the resulting 1851 candidate noun pairs to annotate, generating discriminative and nondiscriminative attributes for each pair. The suggested triples were then manually filtered by the other two authors. candidate concept was paired with another candidate concept from the list of its 100 closest neighbours in a PPMI-based distributional vector space (using the best settings from Baroni et al. (2014)). The motivation for this step is that finding nontrivial semantic differences only makes sense in the context of related words; detecting the difference between two unrelated concepts, such as a narwhal and a tractor, is rather trivial and would not constitute a very interesting task. For each word pair, if there was an attribute in McRae feature norms that the first word has but the second doesn’t, the word pair – attribute triple was added to the list of candidate positive examples. For simplicity, multi-word attributes were processed so that only the last word is taken into account (e.g."
S18-1117,W16-2502,0,0.0332768,": while it is relatively easy to train a model to recognize that apple and banana are somewhat similar, it is less straightforward to learn that, contrary to an apple, a typical banana is not red. This task is therefore more challenging than, and complementary to, the traditional similarity task, and we expect it to contribute to the progress in computational modeling of meaning. While semantic similarity and relatedness measures have been used extensively to evaluate semantic representations, they may not be sufficient as a method for evaluating lexical semantic models (Faruqui et al., 2016; Batchkarov et al., 2016). Firstly, it has been noted that the relevant notions of similarity and relatedness can vary depending on the linguistic context, on the downstream application, etc. The difference task resolves this concern by effectively providing a context. In our example, comparison with bananas determines the relevance of the redness attribute for apples, which, out of context, might not necessarily be a salient attribute of apples. Existing similarity and relatedness datasets have also been criticized for low inter-annotator agreement. The semantic difference detection task alleviates this issue, too. B"
S18-1117,S18-1153,0,0.0528487,"Missing"
S18-1117,S18-1158,0,0.0526891,"Missing"
S18-1117,W16-2506,0,0.0270434,"tle aspects of meaning: while it is relatively easy to train a model to recognize that apple and banana are somewhat similar, it is less straightforward to learn that, contrary to an apple, a typical banana is not red. This task is therefore more challenging than, and complementary to, the traditional similarity task, and we expect it to contribute to the progress in computational modeling of meaning. While semantic similarity and relatedness measures have been used extensively to evaluate semantic representations, they may not be sufficient as a method for evaluating lexical semantic models (Faruqui et al., 2016; Batchkarov et al., 2016). Firstly, it has been noted that the relevant notions of similarity and relatedness can vary depending on the linguistic context, on the downstream application, etc. The difference task resolves this concern by effectively providing a context. In our example, comparison with bananas determines the relevance of the redness attribute for apples, which, out of context, might not necessarily be a salient attribute of apples. Existing similarity and relatedness datasets have also been criticized for low inter-annotator agreement. The semantic difference detection task all"
S18-1117,S18-1156,0,0.0564961,"Missing"
S18-1117,S18-1117,1,0.0512449,"Missing"
S18-1117,S18-1163,0,0.172317,"Missing"
S18-1117,S18-1164,0,0.104387,"Missing"
S18-1117,J15-4004,0,0.0618657,"tures, triples with manually suggested attributes, and random triples. All of these examples have been verified by the three authors and were then randomly split into a validation partition and a test partition, making sure that no feature occurs in both. In the second phase, we extended the dataset by adding new concepts and attributes. Our intention was to make the dataset more diverse and more representative of the noun lexicon by including words and features that are not part of the McRae feature norms (e.g., human nouns such as doctor or student). To select new nouns, we used SimLex-999 (Hill et al., 2015), one of the largest and most popular datasets for semantic similarity. We extracted from SimLex all the nouns with a concreteness rating above the median, and identified 204 candidate items that were not included in the McRae Norms. Each selected noun was paired with candidate concepts from the list of its 20 closest neighbours in the distributional vector space. We then positive negative total McRae 897 634 1531 manual 1477 1656 3133 random 37 361 398 Table 3: Composition of the manually validated part of the dataset 735 To enable development of systems that require more training data, we al"
S18-1117,S18-1171,0,0.0597795,"Missing"
S18-1117,S18-1168,0,0.0424865,"Missing"
S18-1117,S18-1154,0,0.133751,"Missing"
S18-1117,W16-2509,1,0.721658,"examples of semantic difference, but in our opinion such an approach would only make the task more challenging. 733 word1 airplane bagpipe dolphin gorilla oak octopus pajamas skirt subway word2 helicopter accordion seal crocodile pine lobster necklace jacket train 1. Semi-automatically created triples (section 2.2.1) attribute wings pipes fins bananas leaves tentacles silk pleats dirty 2. Manually created triples (section 2.2.2) 3. Automatically created triples (section 2.2.3) As an initial source of data, we used the feature norms collected by McRae et al. (2005) and created a pilot dataset (Krebs and Paperno, 2016). This set was then reverified and manually extended to improve the quality and the variety of the data. Finally, a large number of triples were automatically generated for training purposes. Table 1: Sample data: Word pairs and their distinguishing features (positive examples) number is potentially huge, the examples were selected randomly so that the number of negative examples matches the number of positive examples. Presence of both positive and negative examples makes it possible to train a binary classifier that, for a given triple, predicts whether the attribute is a difference between"
S18-1117,S18-1162,0,0.0653878,"Missing"
S18-1117,S18-1118,0,0.0505375,"Missing"
S18-1117,S18-1166,0,0.107597,"Missing"
S18-1117,W14-1618,0,0.081597,"the attribute is neither an attribute of word1 nor word2 (both concepts lack the attribute). For that last type of attributes, since their Expected impact The semantic difference task can enable further progress in the field of word representation learning. Indeed, state of art models have reached ceiling performance in the tasks of semantic similarity and relatedness (in part because the ceiling, as determined by the agreement of human subjects, is relatively low). Another commonly employed task, analogy, has its own issues (Linzen, 2016) and effectivey boils down to similarity optimization (Levy and Goldberg, 2014). A new general evaluation task for lexical semantics is long due, 1 This is a somewhat arbitrary choice. One could experiment with a symmetric notion of a discriminative attribute, whereby both apple,banana,red and banana,apple,red are considered examples of semantic difference, but in our opinion such an approach would only make the task more challenging. 733 word1 airplane bagpipe dolphin gorilla oak octopus pajamas skirt subway word2 helicopter accordion seal crocodile pine lobster necklace jacket train 1. Semi-automatically created triples (section 2.2.1) attribute wings pipes fins banana"
S18-1117,S18-1157,0,0.0438152,"Missing"
S18-1117,W16-2503,0,0.13487,"Missing"
S18-1117,S18-1170,0,0.108327,"Missing"
S18-1117,S18-1169,0,0.0427499,"Missing"
S18-1117,S18-1165,0,0.0652242,"Missing"
sprugnoli-lenci-2014-crowdsourcing,C10-1006,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,W10-0717,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,D08-1027,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,W03-0502,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,E06-2001,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,W01-1313,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,N10-1024,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,W10-0701,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,P11-2023,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,P13-2127,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,C12-2121,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,R11-2002,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,S13-2011,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,negri-etal-2012-chinese,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,magnini-etal-2006-cab,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,W11-0418,1,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,W01-1309,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,W10-0713,0,\N,Missing
sprugnoli-lenci-2014-crowdsourcing,S13-2010,0,\N,Missing
W01-1507,P98-1013,0,0.0218495,"hat existing representational frameworks mostly differ in the way pieces of linguistic information are mutually implied, rather than in the intrinsic nature of this information. To give a concrete example, almost all theoretical frameworks claim that lexical items have a complex semantic organization, but some of them try to describe it through a multidimensional internal structure (cf. the qualia structure in the Generative Lexicon, Pustejovsky 1995), others by specifying a network of semantic relations (cf. WordNet, Miller et al. 1990), and others in terms of argumental frames (cf FrameNet, Baker et al. 1998; Lexical Conceptual Structures, Jackendoff 1992; etc.). A way out of this theoretical variation is to augment the expressive power of the lexical representation language both horizontally, i.e. by distributing the linguistic information over mutually independent ""coding layers"", and vertically, by further specifying the information conveyed by each such layer. This solution will contribute to solve the issues raised by theoretical variation by defining a common level onto which different types of resources will be mapped without loss of information. This appears to be a necessary condition to"
W01-1507,bel-etal-2000-simple,1,0.82886,"towards de facto standards has already allowed the field of Language Resources to establish broad consensus on key issues for some well-established areas — and will allow similar consensus to be achieved for other important areas through the ISLE project — providing thus a key opportunity for further consolidation and a basis for technological advance. EAGLES previous results have already become de facto standards. To mention several key examples: the LE PAROLE/SIMPLE resources (morphological/syntactic/semantic lexicons and corpora for 12 EU languages, Ruimy et al., 1998, Lenci et al., 1999, Bel et al., 2000) rely on EAGLES results (Sanfilippo, A. et al., 1996 and 1999), and are now being enlarged at the national level through many National Projects; the ELRA Validation Manuals for Lexicons (Underwood and Navarretta, 1997) and Corpora (Burnard et al., 1997) are based on EAGLES guidelines; morpho-syntactic tagging of corpora in a very large number of EU, international and national projects – and for more than 20 languages — is conformant to EAGLES recommendations (Leech and Wilson, 1996). The first priority of the CLWG in the first phase of the ISLE project was to do a comprehensive survey of exist"
W01-1507,C98-1013,0,\N,Missing
W01-1507,J98-3008,0,\N,Missing
W02-1204,calzolari-etal-2002-towards,1,0.877662,"Missing"
W02-1204,2001.mtsummit-papers.13,1,0.829473,"ion, lexicographic gloss, argument structure, selectional restrictions/preferences on the arguments, event type, links of the arguments to the syntactic subcategorization frames as represented in the PAROLE lexicons, ‘qualia’ structure, following the Generative Lexicon (Pustejovsky, 1995), semantic relations, etc.. SIMPLE and PAROLE lexicons are layered resources, with links between the morphological and syntactic layers expressed in PAROLE and the semantic information present in SIMPLE. In its general design, also MILE is envisaged as a highly modular and layered architecture as described in Calzolari et al. (2001). Modularity concerns the “horizontal” MILE organization, in which independent and yet linked modules target different dimensions of lexical entries. On the other hand, at the “vertical” level, a layered organization is necessary to allow for different degrees of granularity of lexical descriptions, so that both “shallow” and “deep” representations of lexical items can be captured. This feature is particularly crucial in order to stay open to the different styles and approaches to the lexicon adopted by existing multilingual systems. At the top level, MILE includes two main modules, mono-MILE,"
W02-1204,villegas-bel-2002-dtd,0,0.0116006,"LE Lexicographic Station is a development platform used to automatically generate a prototype tool starting from the MILE DTD. The aim of this prototype tool is to i) exemplify the MILE entry ii) make extensive use of already existing monolingual resources, and iii) eventually test the guidelines in a real scenario. This situation led us to define a lexicographic station development platform that guarantees the portability of the final prototype to the final specifications as well as to existing monolingual resources which will serve as the basic data for MILE (for a detailed description, cf. Villegas and Bel, 2002). Both at monolingual and multilingual level (but wit h particular emphasis on the latter), ISLE intends to start up the incremental definition of a more Object-Oriented layer for lexical description and to foster the vision of open and distributed lexicons, with elements possibly residing in different sites of the web. 3 Enlargement to Asian Languages An enlargement of the group to involve also Asian languages is going on and representatives of Chinese, Japanese, Korean, and Thai languages have contributed to ISLE work and participated in some ISLE workshops. The cooperation between Asia and"
W02-1204,bel-etal-2000-simple,1,\N,Missing
W02-1204,atkins-etal-2002-resources,1,\N,Missing
W02-1501,bartolini-etal-2002-lexicon,1,0.82688,"with the ISST relations in TC we make allowance for one level of subsumption. 4 Analysis of Results The parsing outputs of BP and LAP were compared and projected against ISST annotation to assess the contribution of lexical information to parse success. In this paper, we focus on the evaluation of how and to which extent lexicosyntactic information contributes to identification of the proper attachment of prepositional complements. For an assessment of the role and impact of lexical information in the analysis of dependency pairs headed by specific words, the interested reader is referred to Bartolini et al. (2002). 4.1 ent parsing configurations. Precision is defined as the ratio of correctly identified dependency relations over all relations found by the parser (prec = correctly identified relations / total number of identified relations); recall refers to the ratio of correctly identified dependency relations over all relations in ISST (recall = correctly identified relations / ISST relations). Finally, the overall performance of the parsing systems is described in terms of the f score, computed as follows: 2 prec recall / prec + recall. Quantitative Evaluation Table 1 summarises the results obtained"
W02-1501,briscoe-carroll-2002-robust,0,0.0176487,"ne third of verb frames contain positions realized by a PP, and this percentage raises up to the near totality noun-headed frames. 2 Robust Parsing of Italian The general architecture of the Italian parsing system used for testing adheres to the following principles: 1) modular approach to parsing, 2) underspecified output (whenever required), 3) cautious use of lexical information, generally resorted to in order to refine and/or further specify analyses already produced on the basis of grammatical information. These principles underlie other typical robust parsing architectures (Chanod 2001, Briscoe and Carroll 2002). The system consists of i.) CHUNK-IT (Federici et al. 1998a), a battery of finite state automata for non-recursive text segmentation (chunking), and ii.) IDEAL (Lenci et al. 2001), a dependency-based analyser of the full range of intra-sentential functional relations (e.g. subject, object, modifier, complement, etc.). CHUNK-IT requires a minimum of lexical knowledge: lemma, part of speech and morpho-syntactic features. IDEAL includes in turn two main components: (i.) a Core Dependency Grammar of Italian; (ii.) a syntactic lexicon of ~26,400 subcategorization frames for nouns, verbs and adject"
W02-1501,W98-1114,0,0.0125423,"y expert lexicographers, and their natural purpose is to provide general purpose, domain-independent syntactic information, covering the most frequent entries and frames. On the other hand, parsing systems often complement general lexicons with corpusdriven, automatically harvested syntactic information (Federici et al. 1998b, Briscoe 2001, Korhonen 2002). Automatic acquisition of subcategorization frames allows systems to access highly context dependent constructions, to fill in possible lexical gaps and eventually rely on frequency information to tune the relative impact of specific frames (Carroll et al. 1998). Lexicon coverage is usually regarded as the main parameter affecting use of lexical information for parsing. However, the real comparative impact of the type (rather than the mere quantity) of lexical information has been seldom discussed. Our results show that the contribution of various lexical information types to parse success is not uniform. The experiment focuses on a particular subset of the information available in syntactic lexicons - the representation of PP complements in lexical frames - tested on the task of PP-attachment. The reason for this choice is that this piece of informa"
W02-1501,C94-1042,0,0.0132148,"nstraints on the argument realization (e.g. the preposition heading a PP complement), and iv.) the argument functional role. Other types of syntactic information that are also found in syntactic lexicons are: argument optionality, verb control, auxiliary selection, order constraints, etc. On the other hand, collocation-based lexical information is only rarely provided by computational lexicons, a gap often lamented in robust parsing system development. A number of syntactic computational lexicons are nowadays available to the NLP community. Important examples are LDOCE (Procter 1987), ComLex (Grishman et al. 1994), PAROLE (Ruimy et al. 1998). These lexicons are basically hand-crafted by expert lexicographers, and their natural purpose is to provide general purpose, domain-independent syntactic information, covering the most frequent entries and frames. On the other hand, parsing systems often complement general lexicons with corpusdriven, automatically harvested syntactic information (Federici et al. 1998b, Briscoe 2001, Korhonen 2002). Automatic acquisition of subcategorization frames allows systems to access highly context dependent constructions, to fill in possible lexical gaps and eventually rely"
W02-1501,C88-2092,0,0.0185339,") {roberto.bartolini, alessandro.lenci, simonetta.montemagni, vito.pirrelli}@ilc.cnr.it Abstract In the paper we report a qualitative evaluation of the performance of a dependency analyser of Italian that runs in both a nonlexicalised and a lexicalised mode. Results shed light on the contribution of types of lexical information to parsing. Introduction It is widely assumed that rich computational lexicons form a fundamental component of reliable parsing architectures and that lexical information can only have beneficial effects on parsing. Since the beginning of work on broadcoverage parsing (Jensen 1988a, 1988b), the key issue has been how to make effective use of lexical information. In this paper we put these assumptions to the test by addressing the following questions: to what extent should a lexicon be trusted for parsing? What is the neat contribution of lexical information to overall parse success? We present here the results of a preliminary evaluation of the interplay between lexical and grammatical information in parsing Italian using a robust parsing system based on an incremental approach to shallow syntactic analysis. The system can run in both a non-lexicalised and a lexicalise"
W02-1501,lenci-etal-2000-opposites,1,0.819064,"imum of lexical knowledge: lemma, part of speech and morpho-syntactic features. IDEAL includes in turn two main components: (i.) a Core Dependency Grammar of Italian; (ii.) a syntactic lexicon of ~26,400 subcategorization frames for nouns, verbs and adjectives derived from the Italian PAROLE syntactic lexicon (Ruimy et al. 1998). The IDEAL Core Grammar is formed by ~100 rules (implemented as finite state automata) covering major syntactic phenomena,1 and organized into structurally-based rules and lexically-based rules. IDEAL adopts a slightly simplified version of the FAME annotation scheme (Lenci et al. 2000), where functional relations are headbased and hierarchically organised to make provision for underspecified representations of highly ambiguous functional analyses. This feature allows IDEAL to tackle cases where lexical information is incomplete, or where functional relations cannot be disambiguated conclusively (e.g. in the case of the argument vs. adjunct distinction). A “confidence score” is associated with some of the identified dependency relations to determine a plausibility ranking among different possible analyses. In IDEAL, lexico-syntactic information intervenes only after possibly"
W02-1501,W00-1903,0,0.0336731,"Missing"
W03-1905,atkins-etal-2002-resources,1,0.888583,"Missing"
W05-0509,bartolini-etal-2004-hybrid,1,0.869022,"Missing"
W05-0509,J96-1002,0,0.00498103,"ound way to build a probabilistic model for SOI, which combines different linguistic cues. Given a linguistic context c and an outcome a∈A that depends on c, in the ME framework the conditional probability distribution p(a|c) is estimated on the basis of the assumption that no a priori constraints must be met other than those related to a set of features f j (a,c) of c, whose distribution is derived from the training data. It can be proven that the probability distribution p satisfying the above assumption is the one with the highest entropy, is unique and has the following expone ntial form (Berger et al. 1996): (1) p (a |c) = 1 k f ( a ,c ) αjj ∏ Z (c) j =1 where Z(c) is a normalization factor, f j (a,c) are the values of k features of the pair (a,c) and correspond to the linguistic cues of c that are relevant to predict the outcome a. Features are extracted from the training data and define the constraints that the probabilistic model p must satisfy. The parameters of the distribution a 1 , …, a k correspond to weights associated with the features, and determine the relevance of each feature in the overall model. In the experiments reported below feature weights have been estimated with the Genera"
W05-0509,bel-etal-2000-simple,1,0.836713,"Missing"
W06-0604,W05-0509,1,0.838535,"Missing"
W06-0604,bel-etal-2000-simple,1,0.758385,"Missing"
W09-0201,S07-1003,0,0.0353938,"Missing"
W09-0201,P98-2127,0,0.262846,"eling noun-to-noun and noun-to-verb connections, we selected the 20,000 most frequent nouns and 5,000 most frequent verbs as target concepts (minus stop lists of very frequent items). We selected as target links the top 30 most frequent direct verbnoun dependency paths (e.g., kill+obj+victim), the top 30 preposition-mediated noun-to-noun or While our unified framework is, as far as we know, novel, the specific ways in which we tackle the different tasks are standard. Concept similarity is often measured by vectors of co-occurrence with context words that are typed with dependency information (Lin, 1998; Curran and Moens, 2002). Our approach to selectional preference is nearly identical to the one of Pad´o et al. (2007). We solve SAT analogies with a simplified version of the method of Turney (2006). Detecting whether a pair expresses a target relation by looking at shared connector patterns with model pairs is a common strategy in relation extraction (Pantel and Pennacchiotti, 2008). Finally, our method to detect verb slot similarity is analogous to the “slot overlap” of Joanis et al. (2008) and others. Since we aim at a unified approach, the lack of originality of our task-specific methods"
W09-0201,J07-2002,0,0.144773,"Missing"
W09-0201,D07-1042,0,0.0635675,"Missing"
W09-0201,J06-3003,0,0.436413,"cs, inspired by cognitive science, in which different semantic tasks are tackled using the same underlying repository of distributional information, collected once and for all from the source corpus. Task-specific semantic spaces are then built on demand from the repository. A straightforward implementation of our proposal achieves state-of-the-art performance on a number of unrelated tasks. 1 Introduction Corpus-derived distributional semantic spaces have proved valuable in tackling a variety of tasks, ranging from concept categorization to relation extraction to many others (Sahlgren, 2006; Turney, 2006; Pad´o and Lapata, 2007). The typical approach in the field has been a “local” one, in which each semantic task (or set of closely related tasks) is treated as a separate problem, that requires its own corpus-derived model and algorithms. Its successes notwithstanding, the “one task – one model” approach has also some drawbacks. From a cognitive angle, corpus-based models hold promise as simulations of how humans acquire and use conceptual and linguistic information from their environment (Landauer and Dumais, 1997). However, the common view in cognitive (neuro)science is that humans resort t"
W09-0201,W02-0908,0,0.123723,"to-noun and noun-to-verb connections, we selected the 20,000 most frequent nouns and 5,000 most frequent verbs as target concepts (minus stop lists of very frequent items). We selected as target links the top 30 most frequent direct verbnoun dependency paths (e.g., kill+obj+victim), the top 30 preposition-mediated noun-to-noun or While our unified framework is, as far as we know, novel, the specific ways in which we tackle the different tasks are standard. Concept similarity is often measured by vectors of co-occurrence with context words that are typed with dependency information (Lin, 1998; Curran and Moens, 2002). Our approach to selectional preference is nearly identical to the one of Pad´o et al. (2007). We solve SAT analogies with a simplified version of the method of Turney (2006). Detecting whether a pair expresses a target relation by looking at shared connector patterns with model pairs is a common strategy in relation extraction (Pantel and Pennacchiotti, 2008). Finally, our method to detect verb slot similarity is analogous to the “slot overlap” of Joanis et al. (2008) and others. Since we aim at a unified approach, the lack of originality of our task-specific methods should be regarded as a"
W09-0201,C08-1114,0,0.182485,"eir environment (Landauer and Dumais, 1997). However, the common view in cognitive (neuro)science is that humans resort to a multipurpose semantic memory, i.e., a database of interconnected concepts and properties (Rogers and McClelland, 2004), adapting the information stored there to the task at hand. From an engineering perspective, going back to the corpus to train a different model for each application is inefficient and it runs the risk of overfitting the model to a specific task, while losing sight of its adaptivity – a highly desirable feature for any intelligent system. 2 Related work Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. Turney recasts a number of semantic challenges in terms of relational or analogical similarity. Thus, if an algorithm is able to tackle the latter, it can Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 1–8, c Athens, Greece, 31 March 2009. 2009 Association for Computational Linguistics 1 3 also be used to address the former. Turney tests his system in a variety of tasks, obtaining good results across the board. His approach amounts to picking a task (analog"
W09-0201,D08-1094,0,\N,Missing
W09-0201,C98-2122,0,\N,Missing
W11-0607,J10-4006,1,0.490013,"he same verb is brakes. As a consequence, thematic fit judgments are also sensitive to the way other roles of the same verb are filled. Bicknell et al. (2010) show that this fact has clear consequences for sentence processing, and argue that subjects dynamically compute and update verb argument expectations and thematic fit during on-line sentence comprehension, by integrating various types of knowledge about events and their arguments. The aim of this paper is to present a computational model of the dynamic composition and update of verb argument expectations using Distributional Memory (DM)(Baroni and Lenci, 2010), a state-of-the-art Distributional Semantic Model (DSM). DSMs (aka vector space models) represent word meaning with vectors encoding corpusbased co-occurence statistics, under the assumption of the so-called Distributional Hypothesis (Miller and Charles, 1991; Sahlgren, 2006): Words occurring in similar contexts are also semantically similar (Landauer and Dumais, 1997; Pad´o and Lapata, 2007; Turney and Pantel, 2010). Thematic fit judgments have already been successfully modeled with DSMs (Erk et al., 2010), but to the best of our knowledge the problem of how thematic fit is dynamically updat"
W11-0607,P07-1028,0,0.103687,"sentence comprehension, by integrating various types of knowledge. In fact, if the verb expectations about an argument role depend on the nouns filling its other arguments, the hypothesis that they are compositionally updated is highly plausible, since, “it is difficult to envision how the potentially unbounded number of contexts that might be relevant could be anticipated and stored in the lexicon” (Elman 2009: 21). Thematic fit judgments have been successfully modeled in distributional semantics. Erk et al. (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). The thematic fit of a noun n as an argument of a verb v is measured with the similarity in a vector space between n and a set of noun exemplars occurring in the same argument role of v. A related approach is adopted by Baroni and Lenci (2010), the main difference being that the thematic fit of n is measured by comparing its vector with a “prototype” vector obtaining by averaging over the vectors of the most typical arguments of v. In both cases, the distributional measure of thematic fit is shown to be highly correlated with human plausibility judgements. Their success notwithstanding, these"
W11-0607,D08-1094,0,0.0488888,"Missing"
W11-0607,J10-4007,0,0.189046,"Missing"
W11-0607,P98-2127,0,0.0503245,"the type of syntagmatic link between these words, i.e. direct object. The score σ is some function of the co-occurrence frequency of the tuple in a corpus and is used to characterize its statistical salience. Distributional Memory belongs to the family of so-called structured DSMs, which take into account the crucial role played by syntactic structures in shaping the distributional properties of words. To qualify as context of a target item, a word must be linked to it by some (interesting) lexico-syntactic relation, which is also typically used to distinguish the type of this co-occurrence (Lin, 1998; Pad´o and Lapata, 2007). Differently from other structured DSMs, the tuple structure T is formally represented as a 3-way geometrical object, namely a third order labeled tensor. A tensor is a multi-way array (Turney, 2007; Kolda and Bader, 2009), i.e. a generalization of vectors (first order tensors) and matrices (second order tensors). Different semantic spaces are then generated “on demand” through tensor matricization, projecting the tuple tensor onto 2-way matrices, whose rows and columns represent semantic spaces to deal with different semantic tasks. For instance, the space W1 ×LW2 is"
W11-0607,J07-2002,0,0.39461,"Missing"
W11-0607,J06-3003,0,0.00869081,"ond order tensors). Different semantic spaces are then generated “on demand” through tensor matricization, projecting the tuple tensor onto 2-way matrices, whose rows and columns represent semantic spaces to deal with different semantic tasks. For instance, the space W1 ×LW2 is formed by vectors for words and the dimensions represent the attributes of these words in terms of lexico-syntactic relations with lexical collocates, such as �obj, read�, or �use, pen�. Consistently, this space is most suitable to address tasks involving the measurement of the “attributional similarity” between words (Turney, 2006), such as synonym detection or modeling selectional preferences. Instead, the space W1 W2×L contains vectors associated with word pairs, whose dimensions are links between these pairs. This space is thus suitable to address tasks involving the measurement of so-called “relational similarity” (Turney, 2006), such as analogy detection or relation classification (cf. Baroni and Lenci 2010 for more details about the Distributional Memory spaces and tasks). Crucially, these spaces are now alternative “views” of the same underlying distributional memory formalized in the tensor. Many semantic tasks"
W11-0607,C98-2122,0,\N,Missing
W11-2501,N09-1003,0,0.0619297,"Missing"
W11-2501,J10-4006,1,0.539292,"t will contain the highest random value. Thus, this RelationCardinality baseline will favor relations that tend to have large relata set across concepts, controlling for effects due to different cardinalities across semantic relations (again, see Table 1 above). DSMs. We choose a few ways to construct DSMs for illustrative purposes only. All the models contain vector representations for the same words, namely, approximately, the top 20K most frequent nouns, 5K most frequent adjectives and 5K most frequent verbs in the combined corpora. All the models use Local Mutual Information (Evert, 2005; Baroni and Lenci, 2010) to weight raw co-occurrence counts (this association measure is obtained by multiplying the raw count by Pointwise Mutual Information, and it is a close approximation to the Log-Likelihood Ratio). Three DSMs are based on counting co-occurrences with collocates within a window of fixed width, in the tradition of HAL (Lund and Burgess, 1996) and many later models. The ContentWindow2 model records sentence-internal co-occurrence with the nearest 2 content words to the left and right of each target concept (the same 30K target nouns, verbs and adjectives are also employed as context content words"
W11-2501,J06-1003,0,0.0168484,"SMs trained on very large corpora. Think of the presence of extremely rare nouns like casuarina in AP, of proper nouns in WordSim (it is not clear to us that DSMs are adequate semantic models for referring expressions – at the very least they should not be mixed up lightly with common nouns), or multi-word expressions in other data sets. 3 How we intend to BLESS distributional semantic evaluation DSMs measure the distributional similarity between words, under the assumption that proximity in distributional space models semantic relatedness, includ3 ing, as a special case, semantic similarity (Budanitsky and Hirst, 2006). However, semantically related words in turn differ for the type of relation holding between them: e.g., dog is strongly related to both animal and tail, but with different types of relations. Therefore, evaluating the intrinsic ability of DSMs to represent the semantic space of a word entails both (i) determining to what extent words close in semantic space are actually semantically related, and (ii) analyzing, among related words, which type of semantic relation they tend to instantiate. Two models can be equally very good in identifying semantically related words, while greatly differing f"
W11-2501,J06-3003,0,0.0188729,"a hypernym, and we have no reason, in general semantic terms, to penalize one or the other. To maximize coverage, we also make sure that, for each concept and relation, a reasonable number of relata are frequently attested in our reference corpora (see statistics below), we only include single-word relata and, where appropriate, we include multiple forms for the same relatum (both sock and socks as coordinates of scarf – as discussed in Section 4.1, we avoided similar ambiguous items as target concepts). Currently, distributional models for attributional similarity and relational similarity (Turney, 2006) are tested on different data sets, e.g., TOEFL and SAT respectively (briefly, attributional similarity 4 pertains to similarity between a pair of concepts in terms of shared properties, whereas relational similarity measures the similarity of the relations instantiated by couples of concept pairs). Conversely, BLESS is not biased towards any particular type of semantic similarity and thus allows both families of models to be evaluated on the same data set. Given a concept, we can analyze the types of relata that are selected by a model as more attributionally similar to the target. Alternativ"
W13-0216,J10-4006,1,0.761722,". Current distributional models are typically built by collecting contexts of word occurrences in large corpora, where “context” can be defined in many possible ways. Pairwise word similarity is then computed by comparing the similarity between the vectors which record the word co-occurrences in the data. Distributional models have been successful in modelling a range of cognitive tasks, including lexical development (Li, Farkas, and MacWhinney 2004), category-related deficits (Vigliocco, Vinson, Lewis, and Garrett 2004), and thematic fit (Erk, Padó, and Padó 2010). Distributional Memory (DM, Baroni and Lenci (2010)) is a general framework for building distributional semantic models from syntactically analysed corpora. It constructs a three-dimensional tensor of 1 Similarly, thematic-fit based accounts of selectional preferences encompass binary distinctions (e.g., eat requires a [+edible] object), while still including more fine-grained differences (e.g., crook is a more fitting object for arrest than cop). weighted word-relation-word tuples each tuple is mapped onto a score by a function σ : hw1 r w2 i → N, where w2 is a target word, r as a relation and w1 an argument or adjuct of the target word. For"
W13-0216,J10-4007,1,0.822284,"hooti has a higher weight than hteacher subj shooti. The set of relations can be defined in different ways, which gives rise to different flavors of DM. We use TypeDM, which uses generic syntactic relations as well as lexicalized relations (see Baroni and Lenci (2010) for details). For our experiments, we project the DM tensor onto a W1 × RW2 matrix, and we represent each target word W1 in terms of a vector with dimensions corresponding to pairs of context words and their relations (R × W2 ). On this matrix, we compute a verb’s expectations for its most typical object with a method similar to Erk et al. (2010) and Lenci (2011): For each verb v, we determine the 20 highest-scoring nouns in object relation and compute the centroid co (v) of their context vectors. The thematic fit of a new noun n for v’s object position is then defined as the cosine of the angle between n’s own context vector and co (v). Since all the vectors’ components are positive, the thematic fit values range between 0 and 1. 2.2 Datasets As stated above, we model three datasets from psycholinguistic experiments. The datasets fall into two categories: sentence triplets, and sentence quadruplets. Please refer to the corresponding"
W13-0216,J03-2004,0,0.689417,"tic fit has emerged as a pivotal concept to explain effects on expectations about upcoming input in language comprehension (McRae, Spivey-Knowlton, and Tanenhaus 1998; Ferretti, McRae, and Hatherell 2001; Matsuki, Chow, Hare, Elman, Scheepers, and McRae 2011). Concerning logical metonymy, there is considerable behavioral as well as modeling evidence that thematic fit plays an important role in metonymy interpretation, that is, the retrieval of covert events for metonymical constructions. Behavioral studies (Zarcone and Padó 2011; Zarcone, Padó, and Lenci 2012) as well as computational models (Lapata and Lascarides 2003; Zarcone, Utt, and Padó 2012) found that the retrieved event will be the event most compatible with our knowledge about typical events and their participants (as captured, e.g. by generalized event knowledge, (McRae and Matsuki 2009)), that is the interpretation with the highest thematic fit with the context. This is in contrast to traditional accounts of logical metonymy (Pustejovsky 1995) which ascribe covert event retrieval to complex lexical entries associating entities with events corresponding to their typical function or creation mode (qualia: book → read / write). The advantage of the"
W13-0216,W11-0607,1,0.886907,"ght than hteacher subj shooti. The set of relations can be defined in different ways, which gives rise to different flavors of DM. We use TypeDM, which uses generic syntactic relations as well as lexicalized relations (see Baroni and Lenci (2010) for details). For our experiments, we project the DM tensor onto a W1 × RW2 matrix, and we represent each target word W1 in terms of a vector with dimensions corresponding to pairs of context words and their relations (R × W2 ). On this matrix, we compute a verb’s expectations for its most typical object with a method similar to Erk et al. (2010) and Lenci (2011): For each verb v, we determine the 20 highest-scoring nouns in object relation and compute the centroid co (v) of their context vectors. The thematic fit of a new noun n for v’s object position is then defined as the cosine of the angle between n’s own context vector and co (v). Since all the vectors’ components are positive, the thematic fit values range between 0 and 1. 2.2 Datasets As stated above, we model three datasets from psycholinguistic experiments. The datasets fall into two categories: sentence triplets, and sentence quadruplets. Please refer to the corresponding psycholinguistic"
W13-0216,W12-1707,1,0.792069,"Missing"
W13-0604,J10-4006,1,0.9091,"s, using explicit or implicit generalizations of the fillers. These rely either primarily on a lexical hierarchy (Resnik, 1996), distributional information (Rooth et al., 1999; Erk et al., 2010) or both (Schulte im Walde et al., 2008). While such computationally-intensive approaches have proven effective in modeling selectional preferences in general, we are interested in learning about only one aspect of a verb’s argument, namely how ‘event-like’ it is. We use the WordNet (Fellbaum, 2010)4 lexical hierarchy to discover whether a noun has an event sense. We also use Distributional Memory (DM, Baroni and Lenci (2010)) as a source of distributional information that allows us to determine how strongly a noun is associated with a given verb as an object filler. DM is a general distributional semantic resource which allows the generation of vectorbased semantic models (Turney and Pantel, 2010) from the distribution of words in context. In general, distributional semantic models are two-dimensional, relating a word with other words in its context giving 3 4 We will subsequently simplify the terminology and speak of a “verb’s eventhood.” We use version 3 of WordNet, available at: http://wordnet.princeton.edu/wo"
W13-0604,J10-4007,1,0.905095,"Missing"
W13-0604,J03-2004,0,0.0344585,"ting nouns (The boy [started/saw]V [the puzzle/fight]N P ) and report significantly higher processing costs for the “coercion combination” (metonymic verb plus entity-denoting object: The boy started the puzzle). While there has been much debate in theoretical linguistics on individual verbs that may or may not give rise to logical metonymy (for example, on enjoy, see Pustejovsky (1995); Fodor and Lepore (1998); Lascarides and Copestake (1998)), work in psycholinguistics (McElree et al., 2001; Traxler et al., 2002; Pylkk¨anen and McElree, 2006) and computational modeling (Lapata et al., 2003; Lapata and Lascarides, 2003) seem to have agreed on a small set of “metonymic verbs” which is used when looking for empirical correlates of logical metonymy. However, this set of metonymic verbs is semantically rather heterogeneous, as it is selected based on intuition only. It includes not only aspectual verbs2 (begin, complete, continue, end, finish, start) but also psychological verbs (enjoy, hate, like, love, regret, savor, try), as well as others that elude straightforward categorization (attempt, endure, manage, master, prefer). 1 In this paper we follow the accepted broad linguistic-philosophical distinction betwe"
W13-0604,W11-0607,1,0.800359,"to determine to what extent ‘the typical object’ of a verb is event-like. First we take the k most strongly associated object fillers from DM, objk (v) for the verb v and then define the eventhood to be the percentage of these fillers that have an event sense. In other words, the eventhood k for a verb v is defined as: |EV ∩ objk (v)| . (2) k Selecting the top k scored fillers as prototypical arguments has proven a reliable method to characterize the expectations for the argument slot which allows, e.g., the modeling of selectional preferences (cf. Baroni and Lenci (2010); Erk et al. (2010); Lenci (2011)). For the present analysis, we fix k at 100 (i.e.  := 100 ), we thereby also eliminate the issue of using words from DM which are not covered in WordNet. The following section investigates the range of eventhood scores across the verbs in DM. k (v) = 2.3 Evaluation on Verbs in DM 400 200 0 Frequency 600 Figure 1 shows the distribution of eventhood across verbs in DM. Verbs with  ≈ 0, i.e. verbs with low eventhood, include unfrock, detain, marry, and behead, while verbs with high eventhood, i.e. those which rank the highest with respect to  (i.e.  ≈ 1), include expedite, undergo, halt, a"
W13-0604,J07-2002,1,0.75038,"Missing"
W13-0604,P99-1014,0,0.0345597,"a measure of “eventhood” of a verb’s object slot3 and to use it to distinguish between verb classes. Our hypotheses are that (a) aspectual verbs have a higher eventhood score than entity-selecting verbs and (b) aspectual verbs have a higher eventhood-score than non-aspectual metonymic verbs. 2.1 Selection of typical objects from corpus data There has been much work on modeling the various fillers of verbs, i.e. their selectional preferences, using explicit or implicit generalizations of the fillers. These rely either primarily on a lexical hierarchy (Resnik, 1996), distributional information (Rooth et al., 1999; Erk et al., 2010) or both (Schulte im Walde et al., 2008). While such computationally-intensive approaches have proven effective in modeling selectional preferences in general, we are interested in learning about only one aspect of a verb’s argument, namely how ‘event-like’ it is. We use the WordNet (Fellbaum, 2010)4 lexical hierarchy to discover whether a noun has an event sense. We also use Distributional Memory (DM, Baroni and Lenci (2010)) as a source of distributional information that allows us to determine how strongly a noun is associated with a given verb as an object filler. DM is a"
W13-0604,P08-1057,0,0.0139368,"se it to distinguish between verb classes. Our hypotheses are that (a) aspectual verbs have a higher eventhood score than entity-selecting verbs and (b) aspectual verbs have a higher eventhood-score than non-aspectual metonymic verbs. 2.1 Selection of typical objects from corpus data There has been much work on modeling the various fillers of verbs, i.e. their selectional preferences, using explicit or implicit generalizations of the fillers. These rely either primarily on a lexical hierarchy (Resnik, 1996), distributional information (Rooth et al., 1999; Erk et al., 2010) or both (Schulte im Walde et al., 2008). While such computationally-intensive approaches have proven effective in modeling selectional preferences in general, we are interested in learning about only one aspect of a verb’s argument, namely how ‘event-like’ it is. We use the WordNet (Fellbaum, 2010)4 lexical hierarchy to discover whether a noun has an event sense. We also use Distributional Memory (DM, Baroni and Lenci (2010)) as a source of distributional information that allows us to determine how strongly a noun is associated with a given verb as an object filler. DM is a general distributional semantic resource which allows the"
W14-0406,lenci-etal-2012-lexit,1,0.800962,"ion of a dictionary”.17 The project goal is to study the combinatory properties of Italian words by developing advanced computational linguistics methods for extracting ` distributional information from PAISA. In particular, CombiNet uses a pattern-based approach to extract a wide range of multiword expressions, such as phrasal lexemes, collocations, and usual combinations. POS n-grams ` and then are automatically extracted from PAISA, ranked according to different types of association measures (e.g., pointwise mutual information, log-likelihood ratios, etc.). Extending the LexIt methodology (Lenci et al., 2012), CombiNet also extracts distributional profiles from the parsed ` including the following types of layer of PAISA, information: Figure 1: Dependency diagram Targeted at novice language learners of Italian, a filter for automatically restricting search results to sentences of limited complexity has been integrated into each search component. When activated, search results are automatically filtered based on a combination of the complexity measures introduced in section 4.3. 5.3 Technical details 1. syntactic slots (subject, complements, modiThe PAISA` online interface has been developed in sev"
W14-0406,W06-2920,0,0.0317161,"minated using a black-list that had been manually populated following inspection of earlier corpus versions. 4.1.2 Table 1: Average text length by source In September 2009, the Wikimedia Foundation decided to release the content of their wikis under CC BY-SA8 , so we decided to download the large and varied amount of texts made available through the Italian versions of these websites. This was done using the Wikipedia Extractor9 on official dumps10 of Wikipedia, Wikinews, Wikisource, Wikibooks, Wikiversity and Wikivoyage. The annotated corpus adheres to the standard CoNLL column-based format (Buchholz and Marsi, 2006), is encoded in UTF-8. 4 Corpus Creation 4.1 Collecting and cleaning web data The web pages for PAISA` were selected in two ways: part of the corpus collection was made through CC-focused web crawling, and another part through a targeted collection of documents from specific websites. 4.1.1 Targeted 4.2 Linguistic annotation and tools adaptation The corpus was automatically annotated with lemma, part-of-speech and dependency information, using state-of-the-art annotation tools for Italian. Part-of-speech tagging was performed with the Part-Of-Speech tagger described in Dell’Orletta (2009) and"
W14-0406,P06-1043,0,0.104212,"Missing"
W14-0406,rehm-etal-2008-towards,0,0.0276103,"ikipedia.org/wiki/ Vocabolario_di_alto_uso), together with high frequent function words not contained in those two lists. We used this domain adaptation approach for 39 pecially for the harvested14 subcorpus that was downloaded as described in section 4.1. We therefore carried out some experiments with the ultimate aim to enrich the corpus with metadata about text genre, topic and function, using automated techniques. In order to gain some insights into the com` we first conducted some manposition of PAISA, ual investigations. Drawing on existing literature on web genres (e.g. (Santini, 2005; Rehm et al., 2008; Santini et al., 2010)) and text classification according to text function and topic (e.g. (Sharoff, 2006)), we developed a tentative three-fold taxonomy to be used for text classification. Following four cycles of sample manual annotation by three annotators, categories were adjusted in order to ` web documents better reflect the nature of PAISA’s (cf. (Sharoff, 2010) about differences between domains covered in the BNC and in the web-derived ukWaC). Details about the taxonomy are provided in Borghetti et al. (2011). Then, we started to cross-check whether the devised taxonomy was ` composii"
W14-0406,W11-0314,1,0.903945,"Missing"
W14-0406,W13-1906,1,0.877398,"Missing"
W14-0406,J03-3001,0,0.0172091,"lity, incrementally augmented with new texts and annotation metadata for intelligent indexing and browsing. These requirements brought us to design a resource that was (1) freely available and freely re-publishable, (2) comprehensively covering contemporary common language and cultural content and (3) enhanced with a rich set of automaticallyannotated linguistic information to enable advanced querying and retrieving of data. On top 2 Related Work The world wide web, with its inexhaustible amount of natural language data, has become an established source for efficiently building large corpora (Kilgarriff and Grefenstette, 2003). Tools are available that make it convenient to bootstrap corpora from the web based on mere seed term lists, such as the BootCaT toolkit (Baroni and Bernardini, 2004). The huge corpora created by the WaCky project (Baroni et al., 2009) are an example of such an approach. A large number of papers have recently been published on the harvesting, cleaning and processing of web corpora.1 However, freely available, large, contemporary, linguistically annotated, easily accessible web corpora are still missing for many languages; but cf. e.g. (G´en´ereux et al., 2012) and the Common Crawl Foundation"
W15-4208,N09-1003,0,0.0525288,"Missing"
W15-4208,W11-2501,1,0.772704,"onym questions of the TOEFL as a benchmark in the synonyms identification task. Although good results in such set (Rapp, 2003) may have a strong impact on the audience, its small size and the fact that it contains only synonyms cannot make it an accurate benchmark to evaluate DSMs. For what concerns antonymy, based on similar principles to the TOEFL, Mohammed et al. (2008) proposes a dataset containing 950 closest-opposite questions, where five alternatives are provided for every target word. Their data are collected starting from 162 questions in the Graduate Record Examination (GRE). BLESS (Baroni and Lenci, 2011) contains several relations, such as hypernymy, co-hyponymy, meronymy, event, attribute, etc. This dataset covers 200 concrete and unambiguous concepts divided in 17 categories (e.g. vehicle, ground mammal, etc.). Every concept is linked through the various semantic relations to several relata (which can be either nouns, adjectives or verbs). Unfortunately this dataset does not contain synonymy and antonymy related pairs. With respect to entailment, Baroni et al.(2012) Related Work Up to now, DSMs performance has typically been evaluated against benchmarks developed for purposes other than DSM"
W15-4208,E12-1004,0,0.733867,"Missing"
W15-4208,P99-1008,0,0.0361593,"similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates). DSMs do not provide by themselves a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a"
W15-4208,W09-0215,0,0.154042,"Missing"
W15-4208,D08-1103,0,0.0551206,"Missing"
W15-4208,P06-1015,0,0.154748,", in fact, can be similar in many ways. Dog and animal are similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates). DSMs do not provide by themselves a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synony"
W15-4208,2003.mtsummit-papers.42,0,0.0194325,"inconsistencies in the way semantic relations have been encoded. Simply looking at the hypernymy relation (Cruse, 1986), for example, we can see that it is used in both a taxonomical (i.e. dog is a hyponym of animal) and a vague and debatable way (i.e. silly is a hyponym of child). ConceptNet (Liu and Singh, 2004) may be considered even less homogeneous, given its size and the automatic way in which it was developed. Landauer and Dumais (1997) introduces the 80 multiple-choice synonym questions of the TOEFL as a benchmark in the synonyms identification task. Although good results in such set (Rapp, 2003) may have a strong impact on the audience, its small size and the fact that it contains only synonyms cannot make it an accurate benchmark to evaluate DSMs. For what concerns antonymy, based on similar principles to the TOEFL, Mohammed et al. (2008) proposes a dataset containing 950 closest-opposite questions, where five alternatives are provided for every target word. Their data are collected starting from 162 questions in the Graduate Record Examination (GRE). BLESS (Baroni and Lenci, 2011) contains several relations, such as hypernymy, co-hyponymy, meronymy, event, attribute, etc. This data"
W15-4208,gheorghita-pierrel-2012-towards,0,0.149614,"Missing"
W15-4208,E14-4008,1,0.740615,"a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth"
W15-4208,C92-2082,0,0.607621,"lations. Words, in fact, can be similar in many ways. Dog and animal are similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates). DSMs do not provide by themselves a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pa"
W15-4208,Y14-1018,1,0.947947,"a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth"
W15-4208,W09-2415,0,0.0355443,"Missing"
W15-4208,W14-5814,0,0.117044,"Missing"
W15-4208,Q14-1041,0,0.0527536,"used for either filtering the pairs or performing an in-depth analysis of the results. The tuples were extracted from a combination of ConceptNet 5.0 and WordNet 4.0, and subsequently filtered through automatic methods and crowdsourcing in order to ensure their quality. The dataset is freely downloadable1 . An extension in RDF format, including also scripts for data processing, is under development. 1 Introduction Distributional Semantic Models (DSMs) represent lexical meaning in vector spaces by encoding corpora derived word co-occurrences in vectors (Sahlgren, 2006; Turney and Pantel, 2010; Lapesa and Evert, 2014). These models are based on the assumption that meaning can be inferred from the contexts in which terms occur. Such assumption is 1 The resource is available http://colinglab.humnet.unipi.it/resources/ and https://github.com/esantus at at 64 Proceedings of the 4th Workshop on Linked Data in Linguistics (LDL-2015), pages 64–69, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2009; Weeds et al., 2004; Weeds and Weir, 2003). Both the abovementioned approaches need to rely on datasets containing semantic relations"
W15-4208,W03-1011,0,0.421294,"corpora derived word co-occurrences in vectors (Sahlgren, 2006; Turney and Pantel, 2010; Lapesa and Evert, 2014). These models are based on the assumption that meaning can be inferred from the contexts in which terms occur. Such assumption is 1 The resource is available http://colinglab.humnet.unipi.it/resources/ and https://github.com/esantus at at 64 Proceedings of the 4th Workshop on Linked Data in Linguistics (LDL-2015), pages 64–69, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2009; Weeds et al., 2004; Weeds and Weir, 2003). Both the abovementioned approaches need to rely on datasets containing semantic relations for training and/or evaluation. EVALution is a dataset designed to support DSMs on both processes. This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis of the results. The qu"
W15-4208,C04-1146,0,0.471585,"spaces by encoding corpora derived word co-occurrences in vectors (Sahlgren, 2006; Turney and Pantel, 2010; Lapesa and Evert, 2014). These models are based on the assumption that meaning can be inferred from the contexts in which terms occur. Such assumption is 1 The resource is available http://colinglab.humnet.unipi.it/resources/ and https://github.com/esantus at at 64 Proceedings of the 4th Workshop on Linked Data in Linguistics (LDL-2015), pages 64–69, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2009; Weeds et al., 2004; Weeds and Weir, 2003). Both the abovementioned approaches need to rely on datasets containing semantic relations for training and/or evaluation. EVALution is a dataset designed to support DSMs on both processes. This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis"
W15-4208,S12-1012,1,0.886767,"ked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis of the results. The tuples were extract"
W15-4208,W06-1104,0,0.011487,"di Pisa Pisa, Italy Chu-Ren Huang The Hong Kong Polytechnic University Hong Kong alessandro.lenci@ling.unipi.it churen.huang@polyu.edu.hk Abstract typically referred to as the distributional hypothesis (Harris, 1954). DSMs are broadly used in Natural Language Processing (NLP) because they allow systems to automatically acquire lexical semantic knowledge in a fully unsupervised way and they have been proved to outperform other semantic models in a large number of tasks, such as the measurement of lexical semantic similarity and relatedness. Their geometric representation of semantic distance (Zesch and Gurevych, 2006) allows its calculation through mathematical measures, such as the vector cosine. A related but more complex task is the identification of semantic relations. Words, in fact, can be similar in many ways. Dog and animal are similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates). DSMs do not provide by themselves a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them u"
W16-1803,J08-4004,0,0.0284634,"Missing"
W16-1803,J09-1005,0,0.171604,"Missing"
W16-1803,W03-1812,0,0.139194,"Missing"
W16-1803,W07-1101,0,0.0922958,"Missing"
W16-1803,baroni-etal-2004-introducing,0,0.0267148,"Missing"
W16-1803,Q14-1041,0,0.0503663,"Missing"
W16-1803,P99-1041,0,0.27006,"Missing"
W16-1803,W03-1810,0,0.14248,"Missing"
W16-1803,D13-1145,0,0.160417,"Missing"
W16-1803,J93-1007,0,0.822485,"Missing"
W16-1803,H05-1113,0,0.0538878,"Missing"
W16-1803,W05-1006,0,0.076568,"Missing"
W16-1803,J90-1003,0,\N,Missing
W16-4102,J10-4006,1,0.953047,"ding times in a test set extracted from the Dundee Corpus (Kennedy et al., 2003). Their results showed that the semantic component improves the predictions, compared to models based only on syntactic information. Building on the work of Mitchell et al. (2010) and Mitchell (2011), Sayeed et al. (2015) tested a similar model on a multimodal language corpus (the AMI Meeting corpus; see Carletta (2007)), being able to predict spoken word pronunciation duration. A totally different perspective was adopted by Lenci (2011): starting from the method for thematic fit estimations that was introduced in Baroni and Lenci (2010), the author presented a compositional distributional model for reproducing the expectation update on the filler of the patient slot of a verb, depending on how the agent slot had been saturated (for example, if the agent of the verb to check is journalist, likely patients will be things that journalists typically check, such as source, spelling etc.). Lenci tried to model explicitly the process through which we modify our predictions on upcoming linguistic input on the basis of our event knowledge: the saturation of an argument slot imposes new semantic constraints on the other positions yet"
W16-4102,F13-1017,1,0.875421,"future, we plan to extend our experimentations to a wider range of psycholinguistic datasets, in order to see how the model can deal with a larger number of complexity sources and linguistic structures. Hopefully, future extensions of this model will also present a more global notion of complexity and will integrate information coming from different linguistic domains. It would be interesting, for example, to combine the predictions of our model of semantic complexity with constraint-based frameworks for the estimation of syntactic difficulty, such as Blache’s Property Grammars (Blache, 2011; Blache, 2013), and to see how they correlate with experimental data. There are many other aspects in language processing that, at the moment, our model leaves aside. Future extensions, in our view, should also account for the role played by attention,12 since several linguistic devices (prosodic cues, non-canonical syntactic structures etc.) can be used to signal informationally relevant parts of the message to the listeners/readers, helping them in the allocation of processing resources and thus influencing complexity (Hagoort, 2016). At the best of our knowledge, such issues have still to be convincingly"
W16-4102,D16-1205,1,0.917059,"- IN, etc.),7 and values are distributional vectors of dependent lexemes.8 The latter can be conceived as “out-of-context” distributional vector encoding of lexical items. Any type of distributional representation can be used to this purpose (e.g., explicit vectors, low-dimensionality dense embeddings, etc.). The following is a representation of an event e ∈ GEK, extracted from the sentence The student reads the book.: −−−−−→ −−→ −−→ (2) [EV EN T NSUBJ:student HEAD:read DOBJ:book] Unlike previous syntax-based DSMs, we extract from corpora syntactic joint contexts, besides single dependencies (Chersoni et al., 2016). A syntactic joint context includes the whole set of dependencies of a given lexical head, which we assume as a surface representation of an event. Each event in GEK may be cued by several lexical items, as part of their semantic content, albeit with different strength depending on their statistical distribution. For instance, the event in (2) is cued by the noun student, the verb read, and the noun book. We assume GEK to be hierarchically structured, according to various levels of event schematicity. In fact, all events in GEK can be underspecified. Without any need to add in GEK any specifi"
W16-4102,P13-2152,0,0.0128344,"task on the Bicknell dataset (Bicknell et al., 2010). 2 Related work Some of the previous works applying Distributional Semantic Models (henceforth DSMs) to sentence processing focused on the problem of computing a semantic surprisal index for the words of the sentence, on the basis of what Hale (2001) has proposed for syntax, and defined as the negative logarithm of the probability of a word given its previous linguistic context. The higher the surprisal of a word, the lower its predictability, and high surprisal values have been shown to correlate with an increase in processing difficulty (Frank et al., 2013; Smith and Levy, 2013). Mitchell et al. (2010) proposed a model to compute surprisal, based on the product of a trigram language model and of a semantic component, based in turn on the weighted dot product of the semantic vector of a target word and of a history vector, representing its prior context. The authors interpolated their model with the output of an incremental parser and they evaluated it on the task of predicting word reading times in a test set extracted from the Dundee Corpus (Kennedy et al., 2003). Their results showed that the semantic component improves the predictions, compa"
W16-4102,N01-1021,0,0.310494,"en the different objects of the construction itself. 13 In the following sections, we will present a global distributional semantic complexity score combining event activation and unification costs. As a first evaluation of our framework, we will use the semantic complexity score in a difficulty estimation task on the Bicknell dataset (Bicknell et al., 2010). 2 Related work Some of the previous works applying Distributional Semantic Models (henceforth DSMs) to sentence processing focused on the problem of computing a semantic surprisal index for the words of the sentence, on the basis of what Hale (2001) has proposed for syntax, and defined as the negative logarithm of the probability of a word given its previous linguistic context. The higher the surprisal of a word, the lower its predictability, and high surprisal values have been shown to correlate with an increase in processing difficulty (Frank et al., 2013; Smith and Levy, 2013). Mitchell et al. (2010) proposed a model to compute surprisal, based on the product of a trigram language model and of a semantic component, based in turn on the weighted dot product of the semantic vector of a target word and of a history vector, representing i"
W16-4102,W11-0607,1,0.900623,"he output of an incremental parser and they evaluated it on the task of predicting word reading times in a test set extracted from the Dundee Corpus (Kennedy et al., 2003). Their results showed that the semantic component improves the predictions, compared to models based only on syntactic information. Building on the work of Mitchell et al. (2010) and Mitchell (2011), Sayeed et al. (2015) tested a similar model on a multimodal language corpus (the AMI Meeting corpus; see Carletta (2007)), being able to predict spoken word pronunciation duration. A totally different perspective was adopted by Lenci (2011): starting from the method for thematic fit estimations that was introduced in Baroni and Lenci (2010), the author presented a compositional distributional model for reproducing the expectation update on the filler of the patient slot of a verb, depending on how the agent slot had been saturated (for example, if the agent of the verb to check is journalist, likely patients will be things that journalists typically check, such as source, spelling etc.). Lenci tried to model explicitly the process through which we modify our predictions on upcoming linguistic input on the basis of our event know"
W16-4102,P10-1021,0,0.225822,"al., 2010). 2 Related work Some of the previous works applying Distributional Semantic Models (henceforth DSMs) to sentence processing focused on the problem of computing a semantic surprisal index for the words of the sentence, on the basis of what Hale (2001) has proposed for syntax, and defined as the negative logarithm of the probability of a word given its previous linguistic context. The higher the surprisal of a word, the lower its predictability, and high surprisal values have been shown to correlate with an increase in processing difficulty (Frank et al., 2013; Smith and Levy, 2013). Mitchell et al. (2010) proposed a model to compute surprisal, based on the product of a trigram language model and of a semantic component, based in turn on the weighted dot product of the semantic vector of a target word and of a history vector, representing its prior context. The authors interpolated their model with the output of an incremental parser and they evaluated it on the task of predicting word reading times in a test set extracted from the Dundee Corpus (Kennedy et al., 2003). Their results showed that the semantic component improves the predictions, compared to models based only on syntactic informati"
W16-4102,P15-1074,0,0.782809,"m language model and of a semantic component, based in turn on the weighted dot product of the semantic vector of a target word and of a history vector, representing its prior context. The authors interpolated their model with the output of an incremental parser and they evaluated it on the task of predicting word reading times in a test set extracted from the Dundee Corpus (Kennedy et al., 2003). Their results showed that the semantic component improves the predictions, compared to models based only on syntactic information. Building on the work of Mitchell et al. (2010) and Mitchell (2011), Sayeed et al. (2015) tested a similar model on a multimodal language corpus (the AMI Meeting corpus; see Carletta (2007)), being able to predict spoken word pronunciation duration. A totally different perspective was adopted by Lenci (2011): starting from the method for thematic fit estimations that was introduced in Baroni and Lenci (2010), the author presented a compositional distributional model for reproducing the expectation update on the filler of the patient slot of a verb, depending on how the agent slot had been saturated (for example, if the agent of the verb to check is journalist, likely patients will"
W16-5302,J10-4006,1,0.756164,"lement our proposal we need two kinds of information: the distributional signature of a set of verbs and their relative frequency with a set of syntactic Cxns. We extracted the latter from VALEX (Korhonen et al., 2006), an automatically built subcategorization lexicon that encodes information for 6,397 English verbs. From this list we selected, for each of the four Cxns used by Johnson and Goldberg (2013) reported in Table 1, the set of 75 top associated verbs. To model the distributional behavior of our verbs we built a syntax-based DSM (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010), that is a space in which the linguistic expressions are characterized on the basis of the parsed text dependency paths in which they occur. For instance, given the sentence The cat ate my homework, in a syntax-based model the distributional entry for −→ the verb eat is represented with the dependency:filler patterns subj:cat, obj:homework. We extracted the raw co-occurrence statistics from the extended arcs of the American English section of the Google Books Syntactic Ngrams corpus (Goldberg and Orwant, 2013), a 146.2B tokens corpus built from 1.4M books. Verbs failing to reach the minimal t"
W16-5302,W16-2513,0,0.0131382,"outside the scope of this paper, but we take this result as an additional confirmation of the goodness of our proposal. 5 Conclusion We proposed a simple unsupervised corpus-based model that represents the meaning of a syntactic construction as the weighted centroid of the vectors encoding the distributional behavior of its prototypical verbs. Given the exploratory nature of this work, we did not explore the full parameter space of our model, an issue that follow-up studies could investigate, e.g. by comparing the alternative DSM implementations ability to model the priming effect magnitude (Ettinger and Linzen, 2016). Our model and experimental results show that distributional semantics is able to provide a usage-based representation of the semantic content of argument constructions, which is consistent with the available evidence concerning the psycholinguistic reality of construction semantics (Bencini and Goldberg, 2000; Kaschak and Glenberg, 2000; Kako, 2006; Goldwater and Markman, 2009; Johnson and Goldberg, 2013) and how this knowledge is acquired (Goldberg, 1999; Casenhiser and Goldberg, 2005; Kidd et al., 2010). At the same time, the increment in descriptive and explanatory power obtained by movin"
W16-5302,S13-1035,0,0.0140373,"bs we built a syntax-based DSM (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010), that is a space in which the linguistic expressions are characterized on the basis of the parsed text dependency paths in which they occur. For instance, given the sentence The cat ate my homework, in a syntax-based model the distributional entry for −→ the verb eat is represented with the dependency:filler patterns subj:cat, obj:homework. We extracted the raw co-occurrence statistics from the extended arcs of the American English section of the Google Books Syntactic Ngrams corpus (Goldberg and Orwant, 2013), a 146.2B tokens corpus built from 1.4M books. Verbs failing to reach the minimal threshold of 500 occurrences were discarded. The raw co-occurrence matrix has been weighted with Positive Local Mutual Information (Evert, 2008, PLMI) to calculate the strength of association between a verb and a syntactic pattern. PLMI is defined as the log ratio between the joint probability of a target v and a context c and their marginal probabilities, multiplied by their joint frequency, setting to zero all the negative results:   p(c, v) P LM I(c, v) = max 0, f (c, v) · log2 (3) p(c) · p(v) PLMI correspo"
W16-5302,korhonen-etal-2006-large,0,0.0167678,"del is to account for the association between a Cxn and a target verb as a function of their geometric distance in the distributional semantic space. Given the exploratory nature of the work presented in these pages, we did not tune all the possible settings and hyperparameters of our DSM. Rather, whenever possible we relied on what is the common practice in the literature or on our experience. To implement our proposal we need two kinds of information: the distributional signature of a set of verbs and their relative frequency with a set of syntactic Cxns. We extracted the latter from VALEX (Korhonen et al., 2006), an automatically built subcategorization lexicon that encodes information for 6,397 English verbs. From this list we selected, for each of the four Cxns used by Johnson and Goldberg (2013) reported in Table 1, the set of 75 top associated verbs. To model the distributional behavior of our verbs we built a syntax-based DSM (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010), that is a space in which the linguistic expressions are characterized on the basis of the parsed text dependency paths in which they occur. For instance, given the sentence The cat ate my homew"
W16-5302,Q14-1041,0,0.0554454,"Ms are typically built by searching all the occurrences of a target expression in a corpus, identifying its contexts of occurrence and representing the target-by-contexts frequencies as a matrix. Contexts can be words, syntactic relations, lexicalized patterns, documents and so on, while the vectors composing the final matrix are assumed to be the distributional representation of the semantics of the target elements. Distributional vectors can be used to evaluate the semantic distance between lexical elements by means of geometric methods (Bullinaria and Levy, 2007; Bullinaria and Levy, 2012; Lapesa and Evert, 2014) or manipulated to represent more complex linguistic entities (Baroni, 2013). Our model implements the idea that the meaning of a syntactic Cxn is intimately related to the semantics of its typical verbs. It is a two step process, that starts by identifying the typical verbs that occur in − our target syntactic Cxn and building their distributional → v vectors. We calculated the weighted centroid −−→ of these verb vectors in order to build a CXN vector encoding the distributional properties of Cxn. The notion of centroid is the generalization of the notion of mean to multidimensional spaces. I"
W16-5302,P98-2127,0,0.0852662,"rature or on our experience. To implement our proposal we need two kinds of information: the distributional signature of a set of verbs and their relative frequency with a set of syntactic Cxns. We extracted the latter from VALEX (Korhonen et al., 2006), an automatically built subcategorization lexicon that encodes information for 6,397 English verbs. From this list we selected, for each of the four Cxns used by Johnson and Goldberg (2013) reported in Table 1, the set of 75 top associated verbs. To model the distributional behavior of our verbs we built a syntax-based DSM (Grefenstette, 1994; Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010), that is a space in which the linguistic expressions are characterized on the basis of the parsed text dependency paths in which they occur. For instance, given the sentence The cat ate my homework, in a syntax-based model the distributional entry for −→ the verb eat is represented with the dependency:filler patterns subj:cat, obj:homework. We extracted the raw co-occurrence statistics from the extended arcs of the American English section of the Google Books Syntactic Ngrams corpus (Goldberg and Orwant, 2013), a 146.2B tokens corpus built from"
W16-5302,J07-2002,0,0.145249,"Missing"
W16-5302,E14-1025,0,0.0169493,"log ratio between the joint probability of a target v and a context c and their marginal probabilities, multiplied by their joint frequency, setting to zero all the negative results:   p(c, v) P LM I(c, v) = max 0, f (c, v) · log2 (3) p(c) · p(v) PLMI corresponds to the Positive Pointwise Mutual Information score (Church and Hanks, 1991) between the verb and the context, weighted by their joint frequency, and differs from PPMI in avoiding the bias towards low-frequency events. To ignore unwanted variance and to reduce the processing cost we adopted the context selection strategy proposed by Polajnar and Clark (2014) and limited the distributional characterization of each verb to its 240 top-associated contexts. In the final step we fed equation 1 with all the previously collected statistics on each group of 75 top-associated verbs, thus obtaining the distributional signature of our target Cxns that will be tested in the remaining of the paper. 3 Jabberwocky sentences prime associated verbs The starting point of the reflections by Johnson and Goldberg (2013, henceforth JG) is the psycholinguistic literature showing that speakers associate semantic knowledge to argument structures, independently of the lin"
W16-5302,C98-2122,0,\N,Missing
W16-5309,P14-1023,0,0.234255,". Licence details: http:// semantic relation. Participants were provided with training and test datasets extracted from EVALution 1.0 (Santus et al., 2015b), as well as a scoring script for evaluating the output of their systems. The shared task has been intended and designed as a “friendly competition”: the goal was to identify strengths and weaknesses of various methods, rather than just “crowning” the best-performing model. In total, seven systems participated in the shared task. Most of them exploited Distributional Semantic Models (DSMs), either of the count-based or word-embedding type (Baroni et al., 2014). Most of them relied on distance or nearest neighbors in subtask 1, and on machine learning classifiers (e.g., Support Vector Machine (SVM), Convolutional Neural Network (CNN) and Random Forest (RF)) in subtask 2. Some systems enriched the DSM representation by adopting patterns (e.g., LexNet, the best system in subtask 2) or extracting distributional properties with unsupervised measures (e.g., ROOT18). This paper reports the results achieved by the participating systems, providing insights about their respective strengths and weaknesses. It is organized as follows. Section 2 surveys similar"
W16-5309,S15-2151,0,0.0565631,"Missing"
W16-5309,S16-1168,0,0.0528522,"Missing"
W16-5309,S07-1003,0,0.0961225,"Missing"
W16-5309,N16-2002,1,0.850221,"as a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns the relative merits and demerits of countbased distributional models and word embeddings (which are obtained by training neural networks rather than counting co-occurrence frequencies). While the latter seem to outperform the former in several tasks such as similarity estimation (Baroni et al., 2014), both types of models are subject to variation at the level of individual linguistic relations (Gladkova et al., 2016). Levy et al. (2015a) have also shown that optimization of hyperparameters can make a bigger difference than the choice between different models. Finally, very recently, several scholars have investigated the possibility of integrating different kinds of information. Kiela et al. (2015) have used image generality for hypernymy detection, while Shwartz et al. (2016) have tried to identify the same relation by combining pattern-based and distributional information. 3 3.1 Shared task Task description The CogALex-V shared task was conducted as a “friendly competition” where participants had access"
W16-5309,C92-2082,0,0.387764,"for the Identification of Semantic Relations Up to this date, several corpus-based approaches to the identification of semantic relations have been proposed. Most of them, however, focus on a single semantic relation with the ambitious objective of isolating it from all the others. Dealing with multiple relations has been found particularly challenging, and few systems have attempted multi-class classifications. The exceptions include Turney (2008) and Pantel and Pennacchiotti (2006). Early approaches rely on lexical-syntactic patterns (e.g. “tools such as hammers”). After the seminal work of Hearst (1992) who sketched methods for pattern discovery, Snow et al. (2004) adopted machine learning over dependency-paths-based features. While these approaches focused on hypernyms, Pantel and Pennacchiotti (2006) introduced Espresso, able to identify several semantic relations (i.e. hypernymy, part-of, succession, reaction and production) as well as to maximize recall by using the Web and precision by assessing the reliability of the patterns. Other pattern-based approaches to synonymy and antonymy are reported by Lin et al. (2003), Turney (2008), Wang et al. (2010) and Lobanova et al. (2010). The majo"
W16-5309,S10-1006,0,0.0774016,"Missing"
W16-5309,P15-2020,0,0.0161834,"(which are obtained by training neural networks rather than counting co-occurrence frequencies). While the latter seem to outperform the former in several tasks such as similarity estimation (Baroni et al., 2014), both types of models are subject to variation at the level of individual linguistic relations (Gladkova et al., 2016). Levy et al. (2015a) have also shown that optimization of hyperparameters can make a bigger difference than the choice between different models. Finally, very recently, several scholars have investigated the possibility of integrating different kinds of information. Kiela et al. (2015) have used image generality for hypernymy detection, while Shwartz et al. (2016) have tried to identify the same relation by combining pattern-based and distributional information. 3 3.1 Shared task Task description The CogALex-V shared task was conducted as a “friendly competition” where participants had access to both training and testing datasets, released on the 8th and the 27th of September 2016, respectively. The participants were asked to evaluate the output of their system with the official evaluation script, released with the test set together with random and majority baselines. Each"
W16-5309,Q14-1041,1,0.84926,"the corpus. In subtask 1, LexNet is combined with vector cosine (calculated on word2vec embeddings trained on Google News) through weights that were learned on a validation set. In subtask 2, in order to avoid a bias towards the majority class RANDOM, a Multi-Layer Perceptron (MLP) is trained and applied only on pairs that were classified as related in subtask 1. The third system, Mach5, investigates the structure and hyperparameters of two traditional dependency-filtered and dependency-structured DSMs trained on a Web corpus of 9.5 billion words. The author sets most parameters according to Lapesa and Evert (2014), focusing on feature selection and optimization of SVD dimensions. Distance information is used directly in subtask 1, while for subtask 2 a linear SVM classifier is applied to 1200-dimensional vectors representing partial Euclidean distance in the two SVD-reduced spaces. Given the competitive results in subtask 1 and the much lower performance achieved in subtask 2, it is evident that Mach5 was optimized for identifying non-random pairs rather than for recognizing and discriminating specific semantic relations. The other systems include ROOT18, which relies on several unsupervised features e"
W16-5309,S12-1012,1,0.87775,"The major limitation of pattern-based approaches is that they require words to co-occur in the same sentence, strongly impacting the recall. Distributional approaches have therefore been adopted to reduce such limitations. They are based on the Distributional Hypothesis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general seman"
W16-5309,Q15-1016,0,0.0399784,"nsisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns the relative merits and demerits of countbased distributional models and word embeddings (which are obtained by training neural networks rather than counting co-occurrence frequencies). While the latter seem to outperform the former in several tasks such as similarity estimation (Baroni et al., 2014), both types of models are subject to variation at the level of individual linguistic r"
W16-5309,N15-1098,0,0.0544365,"nsisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns the relative merits and demerits of countbased distributional models and word embeddings (which are obtained by training neural networks rather than counting co-occurrence frequencies). While the latter seem to outperform the former in several tasks such as similarity estimation (Baroni et al., 2014), both types of models are subject to variation at the level of individual linguistic r"
W16-5309,J10-3003,0,0.034749,"valuation metrics, the seven participating systems and their results. The best performing system in subtask 1 is GHHH (F1 = 0.790), while the best system in subtask 2 is LexNet (F1 = 0.445). The dataset and the task description are available at https://sites.google.com/site/cogalex2016/home/shared-task. 1 Introduction Determining automatically if words are semantically related, and in what way, is important for Natural Language Processing (NLP) applications such as thesaurus generation (Grefenstette, 1994), ontology learning (Zouaq and Nkambou, 2008), paraphrase generation and identification (Madnani and Dorr, 2010), as well as for drawing inferences (Martinez-G´omez et al., 2016). Many NLP applications make use of handcrafted resources such as WordNet (Fellbaum, 1998). However, creating these resources is expensive and time-consuming; they are available for only a few languages, and their coverage inevitably lags behind the lexical and conceptual proliferation. In the last decades, a number of corpus-based approaches have investigated the possibility of identifying lexical semantic relations by observing word usage. Even though these methods are still far from being able to provide a comprehensive model"
W16-5309,P16-4015,0,0.0228902,"Missing"
W16-5309,P16-2074,0,0.0481365,"rth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns"
W16-5309,P06-1015,0,0.206698,"well as further information about the shared task are available at https://sites.google. com/site/cogalex2016/home/shared-task. 70 2.2 Methods for the Identification of Semantic Relations Up to this date, several corpus-based approaches to the identification of semantic relations have been proposed. Most of them, however, focus on a single semantic relation with the ambitious objective of isolating it from all the others. Dealing with multiple relations has been found particularly challenging, and few systems have attempted multi-class classifications. The exceptions include Turney (2008) and Pantel and Pennacchiotti (2006). Early approaches rely on lexical-syntactic patterns (e.g. “tools such as hammers”). After the seminal work of Hearst (1992) who sketched methods for pattern discovery, Snow et al. (2004) adopted machine learning over dependency-paths-based features. While these approaches focused on hypernyms, Pantel and Pennacchiotti (2006) introduced Espresso, able to identify several semantic relations (i.e. hypernymy, part-of, succession, reaction and production) as well as to maximize recall by using the Web and precision by assessing the reliability of the patterns. Other pattern-based approaches to sy"
W16-5309,D14-1162,0,0.0882794,"nal publicly available word embeddings trained on huge corpora (Google News, Common Crawl and Wikipedia + Gigaword 5). The authors found that linear regression works better in subtask 1 (i.e. binary 74 classification), while multi-task CNN performs better in subtask 2, which involves multi-class classification. Analogy was instead found less appropriate for semantic relation identification. LexNet relies on Wikipedia + Gigaword 5 and Google News corpora, leveraging the combination of distributional and path-based information. The authors merged the 50-dimensional GloVe pre-trained embeddings (Pennington et al., 2014) for the words in the pairs with the average embedding vector – created using a LSTM (Hochreiter and Schmidhuber, 1997) – of all the dependency paths that connect them in the corpus. In subtask 1, LexNet is combined with vector cosine (calculated on word2vec embeddings trained on Google News) through weights that were learned on a validation set. In subtask 2, in order to avoid a bias towards the majority class RANDOM, a Multi-Layer Perceptron (MLP) is trained and applied only on pairs that were classified as related in subtask 1. The third system, Mach5, investigates the structure and hyperpa"
W16-5309,D16-1234,0,0.0642002,"Distributional Hypothesis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongo"
W16-5309,C14-1097,0,0.0666826,"hey are based on the Distributional Hypothesis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al.,"
W16-5309,W15-4208,1,0.913468,"m gladkova@phiz.c.u-tokyo.ac.jp Stefan Evert FAU Erlangen-N¨urnberg, Germany stefan.evert@fau.de Alessandro Lenci University of Pisa, Italy alessandro.lenci@unipi.it Abstract The shared task of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V) aims at providing a common benchmark for testing current corpus-based methods for the identification of lexical semantic relations (synonymy, antonymy, hypernymy, part-whole meronymy) and at gaining a better understanding of their respective strengths and weaknesses. The shared task uses a challenging dataset extracted from EVALution 1.0 (Santus et al., 2015b), which contains word pairs holding the above-mentioned relations as well as semantically unrelated control items (random). The task is split into two subtasks: (i) identification of related word pairs vs. unrelated ones; (ii) classification of the word pairs according to their semantic relation. This paper describes the subtasks, the dataset, the evaluation metrics, the seven participating systems and their results. The best performing system in subtask 1 is GHHH (F1 = 0.790), while the best system in subtask 2 is LexNet (F1 = 0.445). The dataset and the task description are available at ht"
W16-5309,L16-1722,1,0.853809,"sis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the N"
W16-5309,P16-1226,0,0.369399,"occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched pairs (Santus et al., 2016). One of the ongoing disputes in the NLP community concerns the relative merits an"
W16-5309,C08-1114,0,0.0585021,"and test data as well as further information about the shared task are available at https://sites.google. com/site/cogalex2016/home/shared-task. 70 2.2 Methods for the Identification of Semantic Relations Up to this date, several corpus-based approaches to the identification of semantic relations have been proposed. Most of them, however, focus on a single semantic relation with the ambitious objective of isolating it from all the others. Dealing with multiple relations has been found particularly challenging, and few systems have attempted multi-class classifications. The exceptions include Turney (2008) and Pantel and Pennacchiotti (2006). Early approaches rely on lexical-syntactic patterns (e.g. “tools such as hammers”). After the seminal work of Hearst (1992) who sketched methods for pattern discovery, Snow et al. (2004) adopted machine learning over dependency-paths-based features. While these approaches focused on hypernyms, Pantel and Pennacchiotti (2006) introduced Espresso, able to identify several semantic relations (i.e. hypernymy, part-of, succession, reaction and production) as well as to maximize recall by using the Web and precision by assessing the reliability of the patterns."
W16-5309,C14-1212,0,0.0769981,"such limitations. They are based on the Distributional Hypothesis (Harris, 1954; Firth, 1957) that words occurring in similar contexts also bear similar meaning. Distributional approaches can be (i) unsupervised, generally consisting of mathematical functions that implement linguistic hypotheses about how and which contexts are relevant to identify specific relations (Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2014); or (ii) supervised, generally consisting of algorithms that automatically learn some distributional information about the words holding a specific relation (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Santus et al., 2016; Nguyen et al., 2016; Shwartz et al., 2016). While unsupervised approaches are commonly outperformed by supervised ones, the latter – which rely on distributional word vectors, either concatenated or combined through algebraic functions – seem to learn specific lexical properties of the words in the pairs rather than the general semantic relation existing between them (Weeds et al., 2014; Levy et al., 2015b). This has a negative impact on their performance on previously unseen words, lexically split datasets and unseen switched p"
W16-5322,baroni-etal-2004-introducing,0,0.0537492,"are significantly higher than those for non-canonical pairs. 4.1 Data The distributional analysis was performed on the 138 pairs with co-occurrence frequency ≥ 5 and mutually produced by the subjects. Eight pairs were removed because one of the members was not included in the distributional model used for this analysis (cf. below) or were erroneously lemmatized (i.e., as past participle forms). Therefore, the distributional analysis was performed on 130 pairs, 62 canonical and 68 non-canonical. 4.2 Procedure Noun-adjective co-occurrences were automatically extracted from La Repubblica corpus (Baroni et al., 2004) with LexIt (Lenci, 2014)4. All the nouns each adjective in the test pairs modifies or is a predicate of were collected. Co-occurrences were weighted with Positive PMI (PPMI) and represented as a multidimensional vector for each adjective. The cosine was used to measure the distributional similarity of each test pair (Turney and Pantel, 2010). Therefore, the higher the cosine of an antonymic pair, the more its members tend to co-occur with the same nouns. 4.3 Results The overall mean cosine is 0.11 (sd 0.07) (see Table 2). Considering the two groups separately a relevant difference can be note"
W16-5322,J91-1001,0,\N,Missing
W17-6524,baroni-etal-2004-introducing,0,0.00843404,"quisition. Few studies tried to automatically build multilingual SCFs lexica. To the best of our knowledge, there have been few experiments in multilingual verb lexicon with syntactic and semantic information, mostly establishing multilingual links manually (Civit et al., 2005; Hellan et al., 2014). 3 The LexIt Framework LexIt (Lenci et al., 2012) is a computational framework whose aim is to automatically extract distributional information about the argument structure of predicates. It was originally developed to extract information on Italian verbs, nouns and adjectives from “La Repubblica” (Baroni et al., 2004) corpus (ca. 331 millions tokens) and from a “dump” of the Italian section of Wikipedia (ca. 152 millions of tokens). The database resulting from this previous work is freely browsable.2 The whole framework aims at processing linguistic information from a dependency-parsed corpus and then storing the results into a database where each predicate is associated with a distributional profile, i.e. a data structure that combines several statistical information about the combinatorial behaviour of the lemma. This profile is articulated into: 1. a syntactic profile, specifying the syntactic arguments"
W17-6524,J10-4007,0,0.0443432,"Missing"
W17-6524,C04-1104,0,0.038021,"002; Erk et al., 2010) and diathesis alternation (McCarthy, 2001). The approach consists in automatically infering subcategorization frames directly from the corpus, with or without a predefined list of possible frames. The literature reports a large number of automatically built subcategorization lexica, among which VALEX for English verbs (Korhonen et al., 2006), LexSchem (Messiant et al., 2008) and LexFr (Rambelli et al., 2016) for French verbs, LexIt for Italian verbs, nouns and adjectives (Lenci et al., 2012). SCFs ac208 quisition has been investigated also for languages such as Chinese (Han et al., 2004) and Japanese (Marchal, 2015). These resources have been of particular interest to classify verbs on the basis of their syntactic and semantic properties, producing several taxonomies comparable to VerbNet (Kipper-Schuler, 2005). Despite the importance of these resources, existing lexica only focus on a single language with a specific syntactic frame representation, strongly dependent on the corpus used for acquisition. Few studies tried to automatically build multilingual SCFs lexica. To the best of our knowledge, there have been few experiments in multilingual verb lexicon with syntactic and"
W17-6524,hellan-etal-2014-multival,0,0.0307835,"verbs on the basis of their syntactic and semantic properties, producing several taxonomies comparable to VerbNet (Kipper-Schuler, 2005). Despite the importance of these resources, existing lexica only focus on a single language with a specific syntactic frame representation, strongly dependent on the corpus used for acquisition. Few studies tried to automatically build multilingual SCFs lexica. To the best of our knowledge, there have been few experiments in multilingual verb lexicon with syntactic and semantic information, mostly establishing multilingual links manually (Civit et al., 2005; Hellan et al., 2014). 3 The LexIt Framework LexIt (Lenci et al., 2012) is a computational framework whose aim is to automatically extract distributional information about the argument structure of predicates. It was originally developed to extract information on Italian verbs, nouns and adjectives from “La Repubblica” (Baroni et al., 2004) corpus (ca. 331 millions tokens) and from a “dump” of the Italian section of Wikipedia (ca. 152 millions of tokens). The database resulting from this previous work is freely browsable.2 The whole framework aims at processing linguistic information from a dependency-parsed corpu"
W17-6524,Y09-1003,0,0.0151959,"res of Italian predicates. Practical issues that arose when building argument structure representations for typologically different languages will also be discussed. 1 Introduction The argument structure of predicates is a key research area in Natural Language Processing (NLP), as verb valency has a decisive impact on sentence structure. Since including information about the syntactic-semantic realization of predicate arguments in a lexicon proved to benefit many NLP applications, e.g. recognition of textual entailment, information retrieval, machine translation and word-sense disambiguation (Korhonen, 2009), research in the (semi-)automatic acquisition of argument structure information from corpora has become widespread. Meanwhile, the last years have also witnessed a growing interest in multilingual studies and evaluation campaigns to test the quality and the robustness of parsing software. By combining these two computational linguistic topics, our work is oriented towards the elabothierry.poibeau@ens.fr ration of a cross-language subcategorization lexicon, i.e. an automatically-built resource that encodes combinatorial properties of verbs at the syntax-semantics interface. This resource will"
W17-6524,lenci-etal-2012-lexit,1,0.895893,", Italy Paris, France g.rambelli1@studenti.unipi.it alessandro.lenci@unipi.it Abstract This paper introduces UDLex, a computational framework for the automatic extraction of argument structures for several languages. By exploiting the versatility of the Universal Dependency annotation scheme, our system acquires subcategorization frames directly from a dependency parsed corpus, regardless of the input language. It thus uses a universal set of language-independent rules to detect verb dependencies in a sentence. In this paper we describe how the system has been developed by adapting the LexIt (Lenci et al., 2012) framework, originally designed to describe argument structures of Italian predicates. Practical issues that arose when building argument structure representations for typologically different languages will also be discussed. 1 Introduction The argument structure of predicates is a key research area in Natural Language Processing (NLP), as verb valency has a decisive impact on sentence structure. Since including information about the syntactic-semantic realization of predicate arguments in a lexicon proved to benefit many NLP applications, e.g. recognition of textual entailment, information re"
W17-6524,W08-1301,0,0.0143417,"Missing"
W17-6524,de-marneffe-etal-2014-universal,0,0.0398988,"Missing"
W17-6524,petrov-etal-2012-universal,0,0.0789985,"Missing"
W17-6524,P07-1115,0,0.0467982,"Missing"
W17-6524,L16-1148,1,0.83588,"Missing"
W17-6524,schulte-im-walde-2002-subcategorisation,0,0.186944,"Missing"
W17-6524,messiant-etal-2008-lexschem,1,0.792363,"tomatic methods have been developed for the identification of verb subcategorization frames (SCFs) (Korhonen, 2002; Messiant et al., 2010; Schulte im Walde, 2009), selectional preferences (Resnik, 1996; Light and Greiff, 2002; Erk et al., 2010) and diathesis alternation (McCarthy, 2001). The approach consists in automatically infering subcategorization frames directly from the corpus, with or without a predefined list of possible frames. The literature reports a large number of automatically built subcategorization lexica, among which VALEX for English verbs (Korhonen et al., 2006), LexSchem (Messiant et al., 2008) and LexFr (Rambelli et al., 2016) for French verbs, LexIt for Italian verbs, nouns and adjectives (Lenci et al., 2012). SCFs ac208 quisition has been investigated also for languages such as Chinese (Han et al., 2004) and Japanese (Marchal, 2015). These resources have been of particular interest to classify verbs on the basis of their syntactic and semantic properties, producing several taxonomies comparable to VerbNet (Kipper-Schuler, 2005). Despite the importance of these resources, existing lexica only focus on a single language with a specific syntactic frame representation, strongly depen"
W17-6524,I08-3008,0,0.0417221,"ically derive verb subcategorization frames regardless of the specificities of the input language. For our purpose, we decided to exploit Universal Dependencies1 (UD) annotations: UD is developed by the UD community with the final goal of creating a cross-linguistically consistent treebank annotation scheme for many languages (Nivre, 2015). The actual UD design combines the (universal) Stanford dependencies (de Marneffe and Manning, 2008; de Marneffe et al., 2014), the Google universal part-of-speech tags (UPOS) (Petrov et al., 2012) and the Interset interlingua for morpho-syntactic tag sets (Zeman and Resnik, 2008). The aim of our project is twofold: on the one hand, we want to test if UD relations are sufficient to describe argument structure for some representative languages, and on the other hand we want to create a multilingual subcategorization lexicon to carry out a contrastive study regarding argument structures, i.e., the analysis of the syntactic realization patterns of verbs arguments across languages. For instance, we would like to know if synonymous predicates across languages occur with similar or different morpho-syntactic frames, or if the same valency frame in two languages is instantiat"
W17-6524,poibeau-messiant-2008-still,1,\N,Missing
W17-6803,J10-4006,1,0.660569,"their structural roles, which are computed later. For example, the difference between typical agents and typical patients, according to this account, would not be included in the representation of an event. In the last few years, a related issue has been debated in the field of distributional semantics, i.e. whether there is any added value in using structured representations of linguistic contexts over bag-ofwords ones (e.g., contexts represented as co-occurrence windows). While structured models have been shown to outperform the latter in a number of semantic tasks (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014), some bag-of-word models proved to be extremely competitive, at least under certain parameter settings (Baroni et al., 2014). A recent paper by Lapesa and Evert (2017) explicitly addressed the question of whether using structured distributional semantic models is worth the effort, by comparing the performance of syntax-based and window-based distributional models on four different tasks. The authors showed that, even after extensive parameter tuning, the former have a significant advantage only in one task out of four (i.e., noun clustering). Interestingly, in the di"
W17-6803,W16-4102,1,0.853824,"was measured as the cosine similarity between its vector, and the context-vector obtained by averaging the vectors of the preceding words. Ettinger and colleagues tested their method on the sentences used in the ERP study by Federmeier and Kutas (1999), in which three different conditions were defined, and they observed that the context-target similarity scores across conditions were following the same pattern of the N400 amplitudes of the original experiment. Thus, this work shows how data on N400 variations can be modeled even by means of vectors with minimal or no syntactic information. 2 Chersoni et al. (2016) presented a research work testing a similar method on the Bicknell dataset. However, their model does not really update argument expectations on the basis of other arguments, computing instead a global score of semantic coherence for the entire event representation, on the basis of the mutual typicality between all the participants. 3 Experiments Rationale. Baroni and Lenci (2010) computed the thematic fit for a candidate f iller (e.g., policeman) in an argument slot (e.g., agent) of an input lexical item (e.g., arrest) as the similarity score between the vector of the candidate f iller and a"
W17-6803,P13-4006,0,0.0270416,"of the Wacky (Baroni et al., 2009) corpus. Both were parsed with the Maltparser (Nivre and Hall, 2005). From this concatenation, we built a dependency-based DSMs, where the tuples are weighted by means of Positive Local Mutual Information (PLMI, Evert (2004)). Given the cooccurrence count Otrf of the target t, the syntactic relation r and the filler f , we computed the expected count Etrf (i.e., the simple joint probability of indipendent variables, corresponding to the product of the probabilities of the single events).4 4 The DSM were built by means of the scripts of the DISSECT framework (Dinu et al., 2013) The PLMI for each target-relation-filler tuple is computed as follows:  LM I(t, r, f ) = log Otrf Etrf  ∗ Otrf P LM I(t, r, f ) = max(LM I(t, r, f ), 0) (3) (4) Our DSM contains 28,817 targets (i.e., all nouns and verbs with frequency above 1000 in the training corpora), and all syntactic relations were included.5 We also built a window-based DSM to extract cooccurrence information for the BOW model, counting only the co-occurrences between the nouns and the verbs of the list above within a word window of width 2. Prototypes The prototypes of all models were built out of the vectors of the"
W17-6803,N15-1003,0,0.287639,"tional information coming from the agent and from the predicate of an agent-verb-patient triple (e.g., butcher–cut–meat), generating a prototype vector which represents the expectations on the patient filler, given the agent filler. The triples of the Bicknell dataset (Bicknell et al., 2010), which were used for the first time to evaluate such a model, are still today, at the best of our knowledge, the only existing gold standard for this type of task. Although the ‘structured-approach’ to thematic fit was influential for a number of other works (Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015; Sayeed et al., 2016; Santus et al., 2017), the task of modeling the update of the argument expectations has received relatively little attention. An exception is the work by Tilk et al. (2016), who trained a neural network on a role-labeled corpus in order to optimize the distributional representation for thematic fit estimation. Their model was also tested on the task of the composition and update of argument expectations, where it was able to achieve a performance comparable to Lenci (2011) on the triples of the Bicknell dataset.2 Notice that both the models of Lenci (2011) and Tilk et al."
W17-6803,E17-2063,0,0.0110316,"n of an event. In the last few years, a related issue has been debated in the field of distributional semantics, i.e. whether there is any added value in using structured representations of linguistic contexts over bag-ofwords ones (e.g., contexts represented as co-occurrence windows). While structured models have been shown to outperform the latter in a number of semantic tasks (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014), some bag-of-word models proved to be extremely competitive, at least under certain parameter settings (Baroni et al., 2014). A recent paper by Lapesa and Evert (2017) explicitly addressed the question of whether using structured distributional semantic models is worth the effort, by comparing the performance of syntax-based and window-based distributional models on four different tasks. The authors showed that, even after extensive parameter tuning, the former have a significant advantage only in one task out of four (i.e., noun clustering). Interestingly, in the discussion they leave open the question of whether their results can generalize to linguistically challenging task such as the prediction of thematic fit ratings. In this paper, we specifically in"
W17-6803,W11-0607,1,0.786635,"lity by means of a Local Mutual Information score (Evert, 2004) computed between verb, arguments and syntactic relations. The basic assumption is that the higher the distributional similarity of a candidate argument with a role prototype, the higher its predictability as a filler for that role will be. As a gold standard, the authors used the human-elicited thematic fit ratings collected by McRae et al. (1998) and Pad´o (2007), and they evaluated the performance by measuring the correlation between these ratings and the scores generated by the model (as already proposed by Erk et al. (2010)). Lenci (2011) later extended this ‘structured-approach’ to account for the dynamic update of the expectations on an argument, which depends on how other roles in the sentences are filled. For instance, given the agent butcher the expected patient of the verb cut is likely to be meat, while given the agent coiffeur the expected patient of the same verb is likely to be hair. By means of the same DM tensor, this study tested an additive and a multiplicative model (Mitchell and Lapata, 2010) to compose the distributional information coming from the agent and from the predicate of an agent-verb-patient triple ("
W17-6803,P14-2050,0,0.0132482,"which are computed later. For example, the difference between typical agents and typical patients, according to this account, would not be included in the representation of an event. In the last few years, a related issue has been debated in the field of distributional semantics, i.e. whether there is any added value in using structured representations of linguistic contexts over bag-ofwords ones (e.g., contexts represented as co-occurrence windows). While structured models have been shown to outperform the latter in a number of semantic tasks (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014), some bag-of-word models proved to be extremely competitive, at least under certain parameter settings (Baroni et al., 2014). A recent paper by Lapesa and Evert (2017) explicitly addressed the question of whether using structured distributional semantic models is worth the effort, by comparing the performance of syntax-based and window-based distributional models on four different tasks. The authors showed that, even after extensive parameter tuning, the former have a significant advantage only in one task out of four (i.e., noun clustering). Interestingly, in the discussion they leave open t"
W17-6803,J07-2002,0,0.0547974,"Missing"
W17-6803,D17-1068,1,0.772801,"from the predicate of an agent-verb-patient triple (e.g., butcher–cut–meat), generating a prototype vector which represents the expectations on the patient filler, given the agent filler. The triples of the Bicknell dataset (Bicknell et al., 2010), which were used for the first time to evaluate such a model, are still today, at the best of our knowledge, the only existing gold standard for this type of task. Although the ‘structured-approach’ to thematic fit was influential for a number of other works (Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015; Sayeed et al., 2016; Santus et al., 2017), the task of modeling the update of the argument expectations has received relatively little attention. An exception is the work by Tilk et al. (2016), who trained a neural network on a role-labeled corpus in order to optimize the distributional representation for thematic fit estimation. Their model was also tested on the task of the composition and update of argument expectations, where it was able to achieve a performance comparable to Lenci (2011) on the triples of the Bicknell dataset.2 Notice that both the models of Lenci (2011) and Tilk et al. (2016) necessarily rely on the hypothesis"
W17-6803,W16-2518,0,0.0126561,"g from the agent and from the predicate of an agent-verb-patient triple (e.g., butcher–cut–meat), generating a prototype vector which represents the expectations on the patient filler, given the agent filler. The triples of the Bicknell dataset (Bicknell et al., 2010), which were used for the first time to evaluate such a model, are still today, at the best of our knowledge, the only existing gold standard for this type of task. Although the ‘structured-approach’ to thematic fit was influential for a number of other works (Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015; Sayeed et al., 2016; Santus et al., 2017), the task of modeling the update of the argument expectations has received relatively little attention. An exception is the work by Tilk et al. (2016), who trained a neural network on a role-labeled corpus in order to optimize the distributional representation for thematic fit estimation. Their model was also tested on the task of the composition and update of argument expectations, where it was able to achieve a performance comparable to Lenci (2011) on the triples of the Bicknell dataset.2 Notice that both the models of Lenci (2011) and Tilk et al. (2016) necessarily r"
W17-6803,D16-1017,0,0.0643391,"nt filler, given the agent filler. The triples of the Bicknell dataset (Bicknell et al., 2010), which were used for the first time to evaluate such a model, are still today, at the best of our knowledge, the only existing gold standard for this type of task. Although the ‘structured-approach’ to thematic fit was influential for a number of other works (Sayeed and Demberg, 2014; Sayeed et al., 2015; Greenberg et al., 2015; Sayeed et al., 2016; Santus et al., 2017), the task of modeling the update of the argument expectations has received relatively little attention. An exception is the work by Tilk et al. (2016), who trained a neural network on a role-labeled corpus in order to optimize the distributional representation for thematic fit estimation. Their model was also tested on the task of the composition and update of argument expectations, where it was able to achieve a performance comparable to Lenci (2011) on the triples of the Bicknell dataset.2 Notice that both the models of Lenci (2011) and Tilk et al. (2016) necessarily rely on the hypothesis that the arguments are structurally distinct, since they are trained either on argument tuples containing fine-grained dependency information, or on se"
W17-6803,P14-1023,0,\N,Missing
W18-4603,J10-4006,1,0.767075,"lous ones, and in particular, they have never been tested on the task of modeling their differences in processing complexity. In this paper, we compare two different models of thematic fit by testing their ability of identifying violations of selectional restrictions in two datasets from the experimental studies. 1 Introduction In recent years, Distributional Semantic Models (henceforth DSMs) have been at the core of one of the most active research areas in NLP, and have been applied to a wide variety of tasks. Among these, distributional modeling of selectional preferences (Erk et al., 2010; Baroni and Lenci, 2010) has been quite popular in computational psycholinguistics, since the similarity estimated by DSMs works very well for predicting the thematic fit between an argument and a verb. That is to say, the more the argument vector is similar to some kind of vector representation of the ideal filler of the verb slot (it can be either an abstract prototype, or a cluster of exemplars), the more the argument will satisfy the semantic requirements of the slot. The notion of thematic fit, as it has been proposed by the recent psycholinguistic research 1 , is related to, but not totally equivalent to the cl"
W18-4603,W16-4102,1,0.810858,"ince the former refers to a gradient compatibility between verb and role, whereas the latter conceives such compatibility as as boolean constraint evaluated on discrete semantic features (Lebani and Lenci, 2018). The distributional models of thematic fit have been evaluated by comparing the plausibility scores produced by the models with human-elicited judgements (Erk et al., 2010; Baroni and Lenci, 2010; Greenberg et al., 2015; Santus et al., 2017), showing significant correlations. Moreover, they have been used to predict the composition and the update of argument expectations (Lenci, 2011; Chersoni et al., 2016), and for modeling reading times of experimental studies on complement coercion (Zarcone et al., 2013). However, an issue regarding their evaluation has not been addressed yet, i.e. their ability of capturing different levels of implausibility. 2 Our processing system is sensitive to minimal variations in predictability between highly unpredictable word combinations, and such sensitivity has been shown to have an influence on reading times (Smith and Levy, 2013). Moreover, word combinations that are simply rare and/or unlikely and word combinations that are semantically deviant have been shown"
W18-4603,P13-4006,0,0.0327901,"ompared two different models of thematic fit. B&L2010 is a ’classical’ model of thematic fit, and it consists of a direct reimplementation of Baroni and Lenci (2010): since we are scoring sentences which differ for the degree of typicality of the verb-object combination, the scores assigned by this model will be the thematic fit scores θ of the object of each sentence given the verb and the patient role. In Equation 3, t is the target verb and c is a word occurring as an object (obj) of t: − −c |obj, → θ=→ t 3 (3) We used the scripts of the DISSECT framework to build the distributional space (Dinu et al., 2013). As context words, we took into account only the 20K words of our target list, in order to limit the size of the distributional space. 5 Obviously, including all the syntactic relations would have hugely increased the dimensionality of the vector space. Therefore, we took into account only the following relations: subject, direct and indirect object, prepositional complement. For each relation, we also considered its inverse: for example, the target apple-v has a dimension eat-v:obj-1, meaning that apple occurs as a direct object of eat-v. 6 In the literature, 20 is a common choice for the nu"
W18-4603,J10-4007,0,0.0667553,"Missing"
W18-4603,N15-1003,0,0.204835,"The notion of thematic fit, as it has been proposed by the recent psycholinguistic research 1 , is related to, but not totally equivalent to the classical notion of selectional preferences, since the former refers to a gradient compatibility between verb and role, whereas the latter conceives such compatibility as as boolean constraint evaluated on discrete semantic features (Lebani and Lenci, 2018). The distributional models of thematic fit have been evaluated by comparing the plausibility scores produced by the models with human-elicited judgements (Erk et al., 2010; Baroni and Lenci, 2010; Greenberg et al., 2015; Santus et al., 2017), showing significant correlations. Moreover, they have been used to predict the composition and the update of argument expectations (Lenci, 2011; Chersoni et al., 2016), and for modeling reading times of experimental studies on complement coercion (Zarcone et al., 2013). However, an issue regarding their evaluation has not been addressed yet, i.e. their ability of capturing different levels of implausibility. 2 Our processing system is sensitive to minimal variations in predictability between highly unpredictable word combinations, and such sensitivity has been shown to"
W18-4603,W11-0607,1,0.920445,"references, since the former refers to a gradient compatibility between verb and role, whereas the latter conceives such compatibility as as boolean constraint evaluated on discrete semantic features (Lebani and Lenci, 2018). The distributional models of thematic fit have been evaluated by comparing the plausibility scores produced by the models with human-elicited judgements (Erk et al., 2010; Baroni and Lenci, 2010; Greenberg et al., 2015; Santus et al., 2017), showing significant correlations. Moreover, they have been used to predict the composition and the update of argument expectations (Lenci, 2011; Chersoni et al., 2016), and for modeling reading times of experimental studies on complement coercion (Zarcone et al., 2013). However, an issue regarding their evaluation has not been addressed yet, i.e. their ability of capturing different levels of implausibility. 2 Our processing system is sensitive to minimal variations in predictability between highly unpredictable word combinations, and such sensitivity has been shown to have an influence on reading times (Smith and Levy, 2013). Moreover, word combinations that are simply rare and/or unlikely and word combinations that are semantically"
W18-4603,nivre-etal-2006-maltparser,0,0.0505108,"the other hand, we have full coverage for the Pylkk¨anen dataset. DSM We built a dependency-based DSM by using the data in the BNC corpus (Leech, 1992) and in the Wacky corpus (Baroni et al., 2009). Both the corpora were POS-tagged with the Tree Tagger (Schmid, 23 Verb and Role Agent of to play Agent of to arrest Patient of to eat Patient of to shoot Fillers actor, gamer, violinist cop, policeman, superhero pizza, sandwich, ice-cream enemy, soldier, prey Table 1: Verb roles and examples of fillers extracted by means of a corresponding syntactic relation. 1994) and parsed with the Maltparser (Nivre et al., 2006). 3 We extracted all the dependencies for the 20K most frequent words in the corpora, including the words of our datasets. Every co-occurrence between a target word and another context word in a given syntactic relation was weighted by means of Positive Local Mutual Information (Evert, 2004). 4 Given a target t, a relation r and a context word c occurring in the relation r with the target (e.g. t = bark, r = sbj, c = dog), we computed both their cooccurrence Otrc , and the expected co-occurrence Etrc under the assumption of statistical independence. The Positive Local Mutual Information (hence"
W18-4603,D17-1068,1,0.844114,"it, as it has been proposed by the recent psycholinguistic research 1 , is related to, but not totally equivalent to the classical notion of selectional preferences, since the former refers to a gradient compatibility between verb and role, whereas the latter conceives such compatibility as as boolean constraint evaluated on discrete semantic features (Lebani and Lenci, 2018). The distributional models of thematic fit have been evaluated by comparing the plausibility scores produced by the models with human-elicited judgements (Erk et al., 2010; Baroni and Lenci, 2010; Greenberg et al., 2015; Santus et al., 2017), showing significant correlations. Moreover, they have been used to predict the composition and the update of argument expectations (Lenci, 2011; Chersoni et al., 2016), and for modeling reading times of experimental studies on complement coercion (Zarcone et al., 2013). However, an issue regarding their evaluation has not been addressed yet, i.e. their ability of capturing different levels of implausibility. 2 Our processing system is sensitive to minimal variations in predictability between highly unpredictable word combinations, and such sensitivity has been shown to have an influence on r"
W18-4603,C94-1027,0,0.25286,"Missing"
W18-4603,W11-1301,0,0.0188244,"ns, and such sensitivity has been shown to have an influence on reading times (Smith and Levy, 2013). Moreover, word combinations that are simply rare and/or unlikely and word combinations that are semantically deviant have been shown to have different consequences on processing complexity (Paczynski and Kuperberg, 2012; Warren et al., 2015). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 See McRae and Matsuki (2009) for an overview. 2 A partial exception is the study on semantic deviance by Vecchi et al. (2011). However, they focus on the acceptability of adjectival phrases, rather than on selectional preferences. 20 Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing, pages 20–29 Santa Fe, New Mexico, USA, August 25, 2018. From this point of view, thematic fit models represent an interesting alternative to the traditional probabilistic ones: they use distributional information about typical arguments to create an abstract representation of the ”ideal” filler of the argument slot, and thus they are more capable of generalizing to the unseen. In other words, it does n"
W18-4603,W13-0216,1,0.863802,"Missing"
W19-3312,W16-4102,1,0.845153,"similar distributional behaviour like Caused-Motion and Dative. As for frames, there has been some work on using distributional similarity between vectors for their unsupervised induction (Ustalov et al., 2018), for comparing frames across languages (Sikos and Pad´o, 2018), and even for the automatic identification of the semantic relations holding between them (Botschen et al., 2017). Modeling sentence comprehension A trend in computational semantics regards the application of DSMs to sentence processing (Mitchell et al., 2010; Lenci, 2011; Sayeed et al., 2015; Johns and Jones, 2015, i.a.). Chersoni et al. (2016, 2017) propose a Distributional Model of sentence comprehension inspired by the general principles of the Memory, Unification and Control framework (Hagoort, 2013, 2015). The memory component includes events in GEK with feature structures containing information directly extracted from parsed sentences in corpora: attributes are syntactic dependencies, while values are distributional vectors of dependent lexemes. Then, they model semantic composition as an event construction and update function F, whose aim is to build a coherent semantic representation by integrating the GEK cued by the lingu"
W19-3312,S17-1021,1,0.872718,"Missing"
W19-3312,P98-1013,0,0.190996,"ic University giulia.rambelli@phd.unipi.it emmanuelechersoni@gmail.com Philippe Blache Aix-Marseille University blache@lpl-aix.fr Chu-Ren Huang The Hong Kong Polytechnic University churen.huang@polyu.edu.hk Alessandro Lenci University of Pisa alessandro.lenci@unipi.it Abstract Obj1 Obj2]). It is worth stressing that, even if the concept of construction is based on the idea that linguistic properties actually emerge from language use, CxG theories have typically preferred to model the semantic content of constructions in terms of hand-made, formal representations like those of Frame Semantics (Baker et al., 1998). This leaves open the issue of how semantic representations can be learned from empirical evidence, and how do they relate to the usage-based nature of Cxs. In fact, for a usage-based model of grammar based on a strong syntax-semantics parallelism, it would be desirable to be grounded on a framework allowing to learn the semantic content of Cxs from language use. In this paper, we propose a new type of semantic representation of Construction Grammar that combines constructions with the vector representations used in Distributional Semantics. We introduce a new framework, Distributional Constr"
W19-3312,J10-4006,1,0.660784,"riples and identify frames as clustered triples. Of course, a limit of this approach is that it only uses subject and object arguments, while frames are generally associated with a wider variety of roles. Lebani and Lenci (2018) instead provide a distributional representation of verb-specific semantic roles as clusters of features automatically induced from corpora. In this paper, we assume that at least some aspects of semantic roles can be derived from combining (e.g., with summation) the distributional vectors of their most prototypical fillers, following an approach widely explored in DS (Baroni and Lenci, 2010; Erk et al., 2010; Sayeed et al., −−−→ 2016; Santus et al., 2017). For instance, the buyer role in the COMMERCIAL TRANSACTION frame can be taken as a vector encoding the properties of the typical nouns filling this role. We are aware that this solution is just an approximation of the content of frames elements. How to satisfactorily characterize semantic frames and roles using DS is in fact still an open research question. 3.3  ditransitive-cx  commercial-transaction-fr     −−−→     BUYER buyer       − − − →   *SELLER seller +      −−−→   SEM    GOODS goods"
W19-3312,W19-2913,0,0.0656285,"Missing"
W19-3312,W17-2618,0,0.0139057,"between linguistic items represented in a vector space. For example, Busso et al. (2018) built a semantic space for several Italian argument constructions and then computed the similarity of their vectors, observing that some Cxs have similar distributional behaviour like Caused-Motion and Dative. As for frames, there has been some work on using distributional similarity between vectors for their unsupervised induction (Ustalov et al., 2018), for comparing frames across languages (Sikos and Pad´o, 2018), and even for the automatic identification of the semantic relations holding between them (Botschen et al., 2017). Modeling sentence comprehension A trend in computational semantics regards the application of DSMs to sentence processing (Mitchell et al., 2010; Lenci, 2011; Sayeed et al., 2015; Johns and Jones, 2015, i.a.). Chersoni et al. (2016, 2017) propose a Distributional Model of sentence comprehension inspired by the general principles of the Memory, Unification and Control framework (Hagoort, 2013, 2015). The memory component includes events in GEK with feature structures containing information directly extracted from parsed sentences in corpora: attributes are syntactic dependencies, while values"
W19-3312,W11-0607,1,0.805924,"ed the similarity of their vectors, observing that some Cxs have similar distributional behaviour like Caused-Motion and Dative. As for frames, there has been some work on using distributional similarity between vectors for their unsupervised induction (Ustalov et al., 2018), for comparing frames across languages (Sikos and Pad´o, 2018), and even for the automatic identification of the semantic relations holding between them (Botschen et al., 2017). Modeling sentence comprehension A trend in computational semantics regards the application of DSMs to sentence processing (Mitchell et al., 2010; Lenci, 2011; Sayeed et al., 2015; Johns and Jones, 2015, i.a.). Chersoni et al. (2016, 2017) propose a Distributional Model of sentence comprehension inspired by the general principles of the Memory, Unification and Control framework (Hagoort, 2013, 2015). The memory component includes events in GEK with feature structures containing information directly extracted from parsed sentences in corpora: attributes are syntactic dependencies, while values are distributional vectors of dependent lexemes. Then, they model semantic composition as an event construction and update function F, whose aim is to build a"
W19-3312,J15-1004,0,0.0290206,"ugust 1st, 2019 2019 Association for Computational Linguistics has never formulated a systematic proposal for deriving representations of constructional meaning from corpus data. Previous literature has mostly focused either on the automatic identification of constructions on the basis of their formal features, or on modeling the meaning of a specific CxG. connection with usage-based theoretical frameworks. To the best of our knowledge, existing attempts of linking DS with models of grammar have rather targeted formal theories like Montague Grammar and Categorial Grammar (Baroni et al., 2014; Grefenstette and Sadrzadeh, 2015). To sum up, both CxG and DS share the assumption that linguistic structures naturally emerge from language usage, and that a representation of both form and meaning of any linguistic item can be modeled through its distributional statistics, and more generally, with the quantitative information derived from corpus data. However, these two models still live in parallel worlds. On the one hand, CxG is a model of grammar in search for a consistent usage-based model of meaning, and, conversely, DS is a computational framework to build semantic representations in search for an empirically adequate"
W19-3312,W19-0129,0,0.0233673,"umption that constructional meanings for argument Cxs arise from the meaning of high frequency verbs that co-occur with them (Goldberg, 1999; Casenhiser and Goldberg, 2005; Barak and Goldberg, 2017), they compute distributional vectors for CxS as the centroids of the vectors of their typical verbs, and use them to model the psycholinguistic data about construction priming in Johnson and Goldberg (2013). This representation of construction meaning has also been applied to study valency coercion by Busso et al. (2018). Following a parallel research line on probing tasks for distributed vectors, Kann et al. (2019) investigate whether word and sentence embeddings encode the grammatical distinctions necessary for inferring the idiosyncratic frame-selectional properties of verbs. Their findings show that, at least 112 3.1 Constructions construction always involve a possession interpretation (more precisely the transfer of something to somebody), represented in the TRANSFER frame. Differently from standard SBCG formalization of Cxs, we add the distributional feature DSVECTOR into the semantic layer in order to integrate lexical distributional representations. The semantic structure of a lexical item can be"
W19-3312,P10-1021,0,0.0386834,"uctions and then computed the similarity of their vectors, observing that some Cxs have similar distributional behaviour like Caused-Motion and Dative. As for frames, there has been some work on using distributional similarity between vectors for their unsupervised induction (Ustalov et al., 2018), for comparing frames across languages (Sikos and Pad´o, 2018), and even for the automatic identification of the semantic relations holding between them (Botschen et al., 2017). Modeling sentence comprehension A trend in computational semantics regards the application of DSMs to sentence processing (Mitchell et al., 2010; Lenci, 2011; Sayeed et al., 2015; Johns and Jones, 2015, i.a.). Chersoni et al. (2016, 2017) propose a Distributional Model of sentence comprehension inspired by the general principles of the Memory, Unification and Control framework (Hagoort, 2013, 2015). The memory component includes events in GEK with feature structures containing information directly extracted from parsed sentences in corpora: attributes are syntactic dependencies, while values are distributional vectors of dependent lexemes. Then, they model semantic composition as an event construction and update function F, whose aim"
W19-3312,D17-1068,1,0.851266,"of this approach is that it only uses subject and object arguments, while frames are generally associated with a wider variety of roles. Lebani and Lenci (2018) instead provide a distributional representation of verb-specific semantic roles as clusters of features automatically induced from corpora. In this paper, we assume that at least some aspects of semantic roles can be derived from combining (e.g., with summation) the distributional vectors of their most prototypical fillers, following an approach widely explored in DS (Baroni and Lenci, 2010; Erk et al., 2010; Sayeed et al., −−−→ 2016; Santus et al., 2017). For instance, the buyer role in the COMMERCIAL TRANSACTION frame can be taken as a vector encoding the properties of the typical nouns filling this role. We are aware that this solution is just an approximation of the content of frames elements. How to satisfactorily characterize semantic frames and roles using DS is in fact still an open research question. 3.3  ditransitive-cx  commercial-transaction-fr     −−−→     BUYER buyer       − − − →   *SELLER seller +      −−−→   SEM    GOODS goods         − − − − →    MONEY money     −−→ PLACE shop F"
W19-3312,P15-1074,0,0.0159296,"rity of their vectors, observing that some Cxs have similar distributional behaviour like Caused-Motion and Dative. As for frames, there has been some work on using distributional similarity between vectors for their unsupervised induction (Ustalov et al., 2018), for comparing frames across languages (Sikos and Pad´o, 2018), and even for the automatic identification of the semantic relations holding between them (Botschen et al., 2017). Modeling sentence comprehension A trend in computational semantics regards the application of DSMs to sentence processing (Mitchell et al., 2010; Lenci, 2011; Sayeed et al., 2015; Johns and Jones, 2015, i.a.). Chersoni et al. (2016, 2017) propose a Distributional Model of sentence comprehension inspired by the general principles of the Memory, Unification and Control framework (Hagoort, 2013, 2015). The memory component includes events in GEK with feature structures containing information directly extracted from parsed sentences in corpora: attributes are syntactic dependencies, while values are distributional vectors of dependent lexemes. Then, they model semantic composition as an event construction and update function F, whose aim is to build a coherent semantic re"
W19-3312,W16-2518,0,0.0248713,"Missing"
W19-3312,W16-1803,1,0.770156,"DING frame In a similar way, events can instantiate an abstract construction dynamically, according to the context. The different lexicalization of the AGENT and the RECIPIENT in the ditransitive construction causes a different selection of the THEME. For example, the fact that the sentence fragment The teacher gives students ... could be completed as in (2) expresses a distributional restriction that can be encoded as an event capturing the co-occurrences 115 tive meaning cannot be decomposed. In computational semantics, a large literature has been aiming at modeling idiomaticity using DSMs. Senaldi et al. (2016) carried out an idiom type identification task representing Italian V-NP and V-PP Cxs as vectors. They observed that the vectors of VN and AN idioms are less similar to the vectors of lexical variants of these expressions with respect to the vectors of compositional constructions. (Cordeiro et al., 2019) realized a framework for predict compound compositionality using DSMs, evaluating to what extent they capture idiomaticity compared to human judgments. Results revealed a high agreement between the models and human predictions, suggesting that they are able to incorporate information about idi"
W19-3312,W18-3813,0,0.0668121,"Missing"
W19-3312,P18-2010,0,0.0640524,"h i     SYN CAT 1 V       n o     LIN 2 ≺ 1 ≺ 3 ≺ 4  FORM   PROPERTIES   n L    L o   3; 3 4 ADJ 1     D E   ARG - ST 2 NPx[subj] , 3 NPy[obl] , 4 NPz[obj]              transfer-fr     *    +      AGENT x FRAMES         RECIPIENT y   SEM             THEME z        −−−−−−−−→  DS - VECTOR Figure 1: Description of read verb ditransitive Figure 2: Description of ditransitive Cx  ata (2015) use distributional representations to induce embeddings for predicates and their arguments. Ustalov et al. (2018) propose a different methodology for unsupervised semantic frame induction. They build embeddings as the concatenations of subject-verb-object triples and identify frames as clustered triples. Of course, a limit of this approach is that it only uses subject and object arguments, while frames are generally associated with a wider variety of roles. Lebani and Lenci (2018) instead provide a distributional representation of verb-specific semantic roles as clusters of features automatically induced from corpora. In this paper, we assume that at least some aspects of semantic roles can be derived fr"
W19-3312,D15-1295,0,0.0438303,"Missing"
W99-0407,1995.iwpt-1.8,0,0.0809344,"representations. It is then crucial to make it sure that constituencybased representations, or any other variants thereof, be mappable onto the functional reference annotation recta-scheme. The same point is convincingly argued for by Lin (1998), who also provides an algorithm for mapping a constituency-based representation onto a dependency-based format. To show that the requirement of intertranslatability is satisfied by FAME, we consider here four different analyses for the sentence John tried to open the window together with their translation equivalent in the FAME format: 1. ANLT Parser (Briscoe & Carroll, 1995) - traditional PSG representation: (Tp (V2 (N2 (Ni (NO John_NPl))) (VI (VO tried_VVD) (VI (YO to_T0) (Vl (VO open_VV0) (N2 (DT the_AT)(NI (NO window_NNl)) )))))). F A M E equivalent: subj ( t r y , John) arg (try, open. &lt;introducer=""to"">) dobj (open, window) 2. Fast Partial Parser (Grefenstette, 1994): SUBJ ( t r y , John) DOBJ (open, window) SUBJ (open, John) MODIF (open, try). FAME equivalent: subj (try, John) dobj (open, window) subj (open, John) mod(open, try) 3. Finite State Constraint G r a m m a r Parser (Karlsson et al., 1995): John N SUBJ tried V MVMAINC"" to INFMARK open V_INF MV OBJ"""
W99-0407,W96-0209,0,0.0629999,"6). By assuming that all levels are, in a sense, primitive, rather than some of them being derivative of others, one provides considerable leeway for radically different definitions of functional relations to be cast into a common, albeit redundant, core of required infor•mation. We will return to this point in section 3 of the paper. To be more concrete, a binary functional relationship can be represented formally as consisting of the following types of information: • it is comparatively easy and ""fair"" to evaluate since it overcomes some of the shortcomings of constituency-based evaluation (Carroll and Briscoe, 1996; Carroll et al., 1998; Sampson, 1998; Lin, 1998); • it represents a very informative ""lowest common ground"" of a variety of different syntactic annotation schemes (Lin, 1998); • it is naturally multi-lingual, as functional relations probably represent the most significant level of syntactic analysis at which crosslanguage comparability makes sense; • it permits joint evaluation of systems dealing with both spoken and written language. Spoken data are typically fraught with cases of disfluency, anacoluthon, syntactic incompleteness and any sort of non-canonical syntactic structure (Antoine, 19"
W99-0407,H94-1020,0,0.21444,"Missing"
W99-0407,H91-1005,0,0.382317,"on and recall are to be gauged jointly relative to all such levels. To be concrete, let us first show a full version of the FAME standard representation for the sentence John tried to open the window (cf. Section 2.2): i. ( t r y , John) 44 i i . &lt;try,John> i i i . subj adaptation to different domains and different languages. Nonetheless, the formalisms used for syntax and semantics must have a certain degree of similarity and some additional knowledge about the relationships between syntax and semantics is necessary. An example is provided by what has been done in the ESPRIT SUNDIAL project (Peckam, 1991), where Syntax is defined using a dependency grammar augmented with morphological agreement rules; Semantics is declared through case frames (Fillmore, 1968; Fillmore, 1985) using a conceptual graph formalism. An additional bulk of knowledge, called mapping knowledge, specifies possible links between the symbols of the dependency grammar and the concepts of case frames. In this way syntactic and semantic controls are performed at the same time, avoiding the generation of parse trees that must afterwards be validated semantically. The FAME meta-scheme fits in comparatively well with this approa"
Y14-1018,J10-4006,1,0.804961,"of the contrasting pairs in GRE closest-to-opposite questions are not listed as opposites in WordNet”. Copyright 2014 by Enrico Santus, Qin Lu, Alessandro Lenci and Chu-Ren Huang 28th Pacific Asia Conference on Language, Information and Computation pages 135–144 !135 PACLIC 28 The automatic identification of semantic relations is a core task in computational semantics. Distributional Semantic Models (DSMs) have often been exploited for their well known ability to identify semantically similar lexemes using corpus-derived co-occurrences encoded as distributional vectors (Santus et al., 2014a; Baroni and Lenci, 2010; Turney and Pantel, 2010; Padó and Lapata, 2007; Sahlgren, 2006). These models are based on the Distributional Hypothesis (Harris, 1954) and represent lexical semantic similarity in function of distributional similarity, which can be measured by vector cosine (Turney and Pantel, 2010). However, these models are characterized by a major shortcoming. That is, they are not able to discriminate among different kinds of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponym"
Y14-1018,P10-1018,0,0.0497849,"Missing"
Y14-1018,2020.inlg-1.14,0,0.0238601,"Missing"
Y14-1018,C92-2082,0,0.398782,"gether with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a distributional interpretation of the so-called paradox of simultaneous similarity and difference between the antonyms (Cruse, 1986). According to this paradox, antonyms are similar to synonyms"
Y14-1018,J91-1001,0,0.690848,"l need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a distributional interpretation of the so-called paradox of simultaneous similarity and difference between the antonyms (Cruse, 1986). According to this paradox, antonyms are similar to synonyms in every dimension of meaning except one. Our hypothesis is that the different dimension of meaning is a salient one and it can be identified with DSMs and exploited for discriminating antonyms from synonyms. The rest of the paper is organized as follows. Section 2 gives the definition and illustrates the various types of antonyms. Section 3 gives a brief o"
Y14-1018,D13-1169,0,0.0689646,"Missing"
Y14-1018,S12-1012,1,0.925115,"are characterized by a major shortcoming. That is, they are not able to discriminate among different kinds of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponyms like house, meronyms like brick, antonyms like shack, together with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Ch"
Y14-1018,W11-2128,0,0.0412439,"Missing"
Y14-1018,H05-1067,0,0.0744627,"Missing"
Y14-1018,D08-1103,0,0.428385,"makes use of Average Precision to estimate the extent and salience of the intersection among the most descriptive contexts of two target words. Evaluation shows that the proposed method is able to distinguish antonyms and synonyms with high accuracy across different parts of speech, including nouns, adjectives and verbs. APAnt outperforms the vector cosine and a baseline model implementing the cooccurrence hypothesis. 1 Introduction Antonymy is one of the fundamental relations shaping the organization of the semantic lexicon and its identification is very challenging for computational models (Mohammad et al., 2008; Deese, 1965; Deese, 1964). Yet, antonymy is essential for many Natural Language Processing (NLP) applications, such as Information Retrieval (IR), Ontology Learning (OL), Machine Translation (MT), Sentiment Analysis (SA) and Dialogue Systems (Roth and Schulte im Walde, 2014; Mohammad et al., 2013). In particular, the automatic identification of semantic opposition is a crucial component for the detection and generation of paraphrases (Marton et al., 2011), the understanding of contradictions (de Marneffe et al., 2008) and the detection of humor (Mihalcea and Strapparava, 2005). Several exist"
Y14-1018,J07-2002,0,0.0433936,"ite questions are not listed as opposites in WordNet”. Copyright 2014 by Enrico Santus, Qin Lu, Alessandro Lenci and Chu-Ren Huang 28th Pacific Asia Conference on Language, Information and Computation pages 135–144 !135 PACLIC 28 The automatic identification of semantic relations is a core task in computational semantics. Distributional Semantic Models (DSMs) have often been exploited for their well known ability to identify semantically similar lexemes using corpus-derived co-occurrences encoded as distributional vectors (Santus et al., 2014a; Baroni and Lenci, 2010; Turney and Pantel, 2010; Padó and Lapata, 2007; Sahlgren, 2006). These models are based on the Distributional Hypothesis (Harris, 1954) and represent lexical semantic similarity in function of distributional similarity, which can be measured by vector cosine (Turney and Pantel, 2010). However, these models are characterized by a major shortcoming. That is, they are not able to discriminate among different kinds of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponyms like house, meronyms like brick, antonyms like"
Y14-1018,P06-1015,0,0.714471,"e brick, antonyms like shack, together with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a distributional interpretation of the so-called paradox of simultaneous similarity and difference between the antonyms (Cruse, 1986). According to this paradox, antonyms are simi"
Y14-1018,P14-2086,0,0.361997,"Missing"
Y14-1018,W14-5814,0,0.121769,"Missing"
Y14-1018,C02-1061,0,0.103734,"Missing"
Y14-1018,C08-1114,0,0.435435,"r lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponyms like house, meronyms like brick, antonyms like shack, together with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a dis"
Y14-1018,E14-4008,1,0.918306,"wo lexemes are antonyms or synonyms by looking at the extent and salience of this intersection: the broader and more salient the intersection, the higher the probability that the lexemes are synonyms; vice versa the narrower and less salient the intersection, the higher the probability that the lexemes are antonyms. To verify this hypothesis, we select the N most salient contexts of the two target words (N=1001). We define the salience of a context for a specific target word by ranking the contexts through Local Mutual Information (LMI; Evert, 2005) and picking the first N, as already done by Santus et al. (2014a). Once the N most salient contexts for the two target words have been identified, we verify the extent and the salience of the contexts shared by both the target words. We predict that synonyms share a significantly higher number of salient contexts than antonyms. To estimate the extent and the salience of the shared contexts, we adapt the Average Precision measure (AP; Voorhees and Harman, 1999), a common Information Retrieval (IR) evaluation metric already used by Kotlerman et al. (2010) to identify lexical entailment. In IR systems, this measure is used to evaluate the ranked documents re"
Y14-1018,J13-3004,0,\N,Missing
Y14-1018,P08-1118,0,\N,Missing
Y16-2021,N09-1003,0,0.387296,"Missing"
Y16-2021,J10-4006,1,0.894609,"Missing"
Y16-2021,P14-1023,0,0.703018,"shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (Bullinaria and Levy, 2012), against which APSyn still obtains competitive performances. The results are also discussed in relation to the state-of-the-art DSMs, as reported in Hill et al. (2015). In such comparison, the best settings of our models outperform the word embeddings in almost all datasets. A pilot study was also carried out to investigate whether APSyn is scalable. Results prove its high performance also when calculated on large corpora, such as those used by Baroni et al. (2014). On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine. Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (Turney, 2001; Agirre et al., 2009; Hill et al., 2015), we test the ability of the models to quantify genuine semantic similarity. 2 2.1 Background DSMs, Measures of Association and Dimensionality Reduction Count-based DSMs are built in an unsupervised way. Starting from large preprocessed corpora, a matrix M(m×n) is built, in which each row is a vector representing a"
Y16-2021,J90-1003,0,0.702972,"of the Distributional Hypothesis (DH), which claims that words occurring in the same contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Sahlgren, 2008). Such hypothesis provides the theoretical ground for Distributional Semantic Models (DSMs), which represent word meaning by means of high-dimensional vectors encoding corpus-extracted co-occurrences between targets and their linguistic contexts (Turney and Pantel, 2010). Traditional DSMs initialize vectors with cooccurrence frequencies. Statistical measures, such as Positive Pointwise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These ﬁrst-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word"
Y16-2021,W16-2506,0,0.162529,"ting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models learn meaning representations through neural network training: the vectors dimensions are set to maximize the probability for the contexts that typically occur with the target word. Vector Cosine is generally adopted by both types of models as a similarity measure. However, this metric has been found to suffer from several problems (Li and Han, 2013; Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are actually shared by the vectors. Finally, Cosine is affected by the hubness effect (Dinu et al., 2014; Schn30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 229 abel et al., 2015), i.e. the fact that words with high frequency tend to be universal neighbours. Even though other measures have been proposed in the literature (Deza and Deza, 2009), Vector Cosine is still by far the most popular one (Turney and Pantel"
Y16-2021,J15-4004,0,0.0945737,", 2010). However, in a recent paper of Santus et al. (2016b), the authors have claimed that Vector Cosine is outperformed by APSyn (Average Precision for Synonymy), a metric based on the extent of the intersection between the most salient contexts of two target words. The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets. In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation - namely WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). For comparison, Vector Cosine is also calculated on several countbased DSMs. We implement a total of twenty-eight models with different parameters settings, each of which differs according to corpus size, context window width, weighting scheme and SVD application. The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (Bullinaria and Levy, 2012), against which APSyn still obtains competitive performances. The results are also discussed in relation to the state-of-the-art DSMs, as reported in Hill et al. (201"
Y16-2021,P12-1092,0,0.686827,"wise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These ﬁrst-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models learn meaning representations through neural network training: the vectors dimensions are set to maximize the probability for the contexts that typically occur with the target word. Vector Cosine is generally adopted by both types of models as a similarity measure. However, this metric has been found to suffer from several problems (Li and Han, 2013; Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are actually shared by the"
Y16-2021,W14-1503,0,0.0315309,"0.335 0.519 0.525 0.564 0.546 0.562 0.553 WSim (REL) 2 3 0.167 0.175 0.251 0.269 0.378 0.396 0.051 0.084 0.141 0.151 0.325 0.323 0.259 0.241 0.261 0.284 0.233 0.27 0.337 0.397 0.361 0.382 0.287 0.309 Table 3: Spearman correlation scores for our eight models trained on RCV1, in the two subsets of WordSim353. might depend on the different type of similarity encoded in SimLex-999 (i.e. genuine similarity). On top of it, despite Hill et al. (2015)’s claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014), we need to mention that window 5 was abandoned because of its low performance. With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by Schnabel et al. (2015), using the words of the SimLex-999 dataset as query words and collecting for each of them the top 1000 nearest neighbors. Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora. Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list. It can be easily noticed that the highest ranke"
Y16-2021,Q15-1016,0,0.382486,"s that words occurring in the same contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Sahlgren, 2008). Such hypothesis provides the theoretical ground for Distributional Semantic Models (DSMs), which represent word meaning by means of high-dimensional vectors encoding corpus-extracted co-occurrences between targets and their linguistic contexts (Turney and Pantel, 2010). Traditional DSMs initialize vectors with cooccurrence frequencies. Statistical measures, such as Positive Pointwise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These ﬁrst-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models lear"
Y16-2021,L16-1723,1,0.788938,"higher values and the inability of considering how many features are actually shared by the vectors. Finally, Cosine is affected by the hubness effect (Dinu et al., 2014; Schn30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 229 abel et al., 2015), i.e. the fact that words with high frequency tend to be universal neighbours. Even though other measures have been proposed in the literature (Deza and Deza, 2009), Vector Cosine is still by far the most popular one (Turney and Pantel, 2010). However, in a recent paper of Santus et al. (2016b), the authors have claimed that Vector Cosine is outperformed by APSyn (Average Precision for Synonymy), a metric based on the extent of the intersection between the most salient contexts of two target words. The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets. In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation - namely WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). For comparison, Vector Cosine is also"
Y16-2021,D15-1036,0,0.145843,"andauer and Dumais, 1997; Jarmasz and Szpakowicz, 2004; Mikolov et al., 2013; Levy et al., 2015), looks at the normalized correlation between the dimensions of two word vectors, w1 and w2 and returns a score between -1 and 1. It is described by the following equation: n × f2i i=1 f1 i n i=1 f1 i × i=1 f2 i cos(w1 , w2 ) = n (3) where fi x is the i-th dimension in the vector x. Despite its extensive usage, Vector Cosine has been recently criticized for its hyper sensibility to features with high values and for the inability of identifying the actual feature intersection (Li and Han, 2013; Schnabel et al., 2015). Recalling an example by Li and Han (2013), the Vector Cosine for the toy-vectors a = [1, 2, 0] and b = [0, 1, 0] (i.e. 0.8944) is unexpectedly higher than the one for a and c = [2, 1, 0] (i.e. 0.8000), and even higher than the one for the toy-vectors a and d = [1, 2, 1] (i.e. 0.6325), which instead share a larger feature intersection. Since the Vector Cosine is a distance measure, it is also subject to the hubness problem, which was shown by Radovanovic et al. (2010) to be an inherent property of data distributions in highdimensional vector space. The problem consists in the fact that vector"
Y16-2021,P10-1040,0,0.0618988,"uch as Positive Pointwise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These ﬁrst-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models learn meaning representations through neural network training: the vectors dimensions are set to maximize the probability for the contexts that typically occur with the target word. Vector Cosine is generally adopted by both types of models as a similarity measure. However, this metric has been found to suffer from several problems (Li and Han, 2013; Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are act"
zarcone-lenci-2008-computational,baroni-etal-2004-introducing,0,\N,Missing
zarcone-lenci-2008-computational,J96-1002,0,\N,Missing
zarcone-lenci-2008-computational,P07-1113,0,\N,Missing
zarcone-lenci-2008-computational,J00-4004,0,\N,Missing
