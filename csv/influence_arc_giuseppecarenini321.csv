2020.aacl-main.51,D19-1327,0,0.104124,"has been initially and more intensely studied in the field of multidocument summarization (Lloret and Sanz, 2013), because important sentences selected from multiple documents (about the same topic) are more 1 Our code can be found here - http://www.cs. ubc.ca/cs-research/lci/research-groups/ natural-language-processing/ likely to be redundant than sentences from the same document, generating a non-redundant summary should still be one of the goals for single document summarization (Lin et al., 2009). Generally speaking, there is a trade-off between importance and diversity (non-redundancy) (Jung et al., 2019), which is reflected in the two phases, sentence scoring and sentence selection (Zhou et al., 2018) in which extractive summarization task can be naturally decomposed. The former typically scores sentences based on importance, while the latter selects sentences based on their scores, but also possibly taking other factors (including redundancy) into account. Traditionally, in non-neural approaches the tradeoff between importance and redundancy has been carefully considered, with sentence selection picking sentences by optimizing an objective function that balances the two aspects (Carbonell an"
2020.aacl-main.51,D18-1208,0,0.0870834,"Missing"
2020.aacl-main.51,D19-5413,0,0.02468,"edundancy significantly. 1 1 Introduction Summarization is the task of shortening a given document(s) while maintaining the most important information. In general, a good summarizer should generate a summary that is syntactically accurate, semantically correct, coherent, and non-redundant (Saggion and Poibeau, 2013). While extractive methods tend to have better performance on the first two aspects, they are typically less coherent and more redundant than abstractive ones, where new sentences are often generated by sentence fusion and compression, which helps detecting and removing redundancy (Lebanoff et al., 2019). Although eliminating redundancy has been initially and more intensely studied in the field of multidocument summarization (Lloret and Sanz, 2013), because important sentences selected from multiple documents (about the same topic) are more 1 Our code can be found here - http://www.cs. ubc.ca/cs-research/lci/research-groups/ natural-language-processing/ likely to be redundant than sentences from the same document, generating a non-redundant summary should still be one of the goals for single document summarization (Lin et al., 2009). Generally speaking, there is a trade-off between importance"
2020.aacl-main.51,D19-1623,0,0.017389,"y loss Lce (in blue), and an RL mechanism and Carenini (2019), we set word length limit of (in green) whose loss is Lrd . The neural model is the generated summaries as 200 on both datasets. then trained on a mixed objective loss L with γ as 6 We tune the hyperparameter λ and β in the rethe scaling factor. Zooming on the details of the spective methods on the validation set, and set RL component, it first generates a summary Sˆ by applying the MMR selection described for MMR- λ = 0.6, β = 0.3 for both datasets. Following Select, which is to greedily pick sentences accord- previous work (e.g., Li et al. (2019)), γ was set to 0.99. For training MMR-Select+, the learning ing to MMR-score, as well as the corresponding rate is lr = 1e − 6; we start with the pretrained label assignment Yˆ = {yˆ1 , yˆ2 , ..., yˆn } (yˆi = 1 if si is selected, yˆi = 0 otherwise). Then, the ex- ExtSumm-LG model. As for the evaluation metric, we use ROUGE scores as the measurement of impected reward is computed based on the ROUGE score between Sˆ and the gold-standard human ab- portance while using the Unique N-gram Ratio and NID defined in Section 3 as the measurements of stractive summary S weighted by the probability red"
2020.aacl-main.51,W04-1013,0,0.0675511,"common basic model (Xiao and Carenini, 2019). To summarize, our main contributions in this paper are: we first examine popular datasets, and show that redundancy is a more serious problem when summarizing long documents (e.g., scientific papers) than short ones (e.g. news). Secondly, we not only reorganize and re-implement existing neural methods for redundancy reduction, but we also propose three new general and flexible methods. Finally, in a series of experiments, we compare existing and proposed methods on long documents (i.e., the Pubmed and arXiv datasets), with respect to ROUGE scores (Lin, 2004) and redundancy scores (Peyrard et al., 2017; Feigenblat et al., 2017). As a preview, empirical results reveal that the proposed methods achieve state-of-the-art performance on ROUGE scores, on the two scientific paper datasets, while also reducing the redundancy significantly. 2 Related Work In traditional extractive summarization, the process is treated as a discrete optimization problem balancing between importance scores and redundancy scores, with techniques like Maximal Marginal Relevance(MMR)(Carbonell and Goldstein, 1998), redundancy-aware feature-based sentence classifiers (Ren et al."
2020.aacl-main.51,D19-1387,0,0.389523,"18) in 516 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 516–528 c December 4 - 7, 2020. 2020 Association for Computational Linguistics an implicit way. However, whether these strategies actually help reducing redundancy is still an open empirical question. The only neural attempt of explicitly reduce redundancy in the sentence selection phase is the Trigram Blocking technique, used in recent extractive summarization models on news datasets (e.g., (Liu and Lapata, 2019)). However, the effectiveness of such strategy on the summarization of long documents has not been tested. Finally, a very recent work by Bi et al. (2020) attempts to reduce redundancy in more sophisticated ways, but still focusing on news. Furthermore, since it relies on BERT, such model is unsuitable to deal with long documents (with over 3,000 words). To address this rather confusing situation, characterized by unclear connections between all the proposed neural models, by their limited focus on short documents, and by spotty evaluations, in this paper we systematically organize existing re"
2020.aacl-main.51,K16-1028,0,0.099659,"Missing"
2020.aacl-main.51,D18-1206,0,0.0598977,"documents are more redundant 4 Redundancy Reduction Methods We systematically organize neural redundancy reduction methods into three categories, and compare prototypical methods from each category. A The decoder is designed to implicitly take redundancy into account. entropy(D) N ID = 1 − log(|D|) B In the sentence scoring phase, explicitly learn to reduce the redundancy. Note that higher NID indicates more redundancy. When we compare the redundancy of long vs. short documents with respect to these two metrics on four popular datasets for summarization (CNNDM (Nallapati et al., 2016b), Xsum (Narayan et al., 2018), Pubmed and arXiv (Cohan et al., 2018)), we observe that long documents are substantially more redundant than short ones (as it was already pointed out in the past (Stewart and Carbonell, 1998)). Table 1 shows the basic statistics of each dataset, along with the average NID 2 In this paper, all the unique n-gram ratios are shown in percentage. # Doc. 203k 270k 115k 201k C In the sentence selection phase, select sentences with less redundancy. In this section, we describe different methods from each category. To compare them in a fair way, we build all of them on a basic ExtSum-LG model (see §"
2020.aacl-main.51,D14-1162,0,0.0826772,"Missing"
2020.aacl-main.51,X98-1025,0,0.196722,"h category. A The decoder is designed to implicitly take redundancy into account. entropy(D) N ID = 1 − log(|D|) B In the sentence scoring phase, explicitly learn to reduce the redundancy. Note that higher NID indicates more redundancy. When we compare the redundancy of long vs. short documents with respect to these two metrics on four popular datasets for summarization (CNNDM (Nallapati et al., 2016b), Xsum (Narayan et al., 2018), Pubmed and arXiv (Cohan et al., 2018)), we observe that long documents are substantially more redundant than short ones (as it was already pointed out in the past (Stewart and Carbonell, 1998)). Table 1 shows the basic statistics of each dataset, along with the average NID 2 In this paper, all the unique n-gram ratios are shown in percentage. # Doc. 203k 270k 115k 201k C In the sentence selection phase, select sentences with less redundancy. In this section, we describe different methods from each category. To compare them in a fair way, we build all of them on a basic ExtSum-LG model (see §4.1), by modifying the decoder and the loss function in the sentence selection phase or the sentence selection algorithm. In Table 2, we summarize the architecture (Encoder, Decoder, Loss Functi"
2020.aacl-main.51,D19-1298,1,0.916892,"lly, in non-neural approaches the tradeoff between importance and redundancy has been carefully considered, with sentence selection picking sentences by optimizing an objective function that balances the two aspects (Carbonell and Goldstein, 1998; Ren et al., 2016). In contrast, more recent works on neural extractive summarization models has so far over-emphasized sentence importance and the corresponding scoring phase, while paying little attention to how to reduce redundancy in the selection phase, where they simply apply a greedy algorithm to select sentences (e.g.,Cheng and Lapata (2016); Xiao and Carenini (2019)). Notice that this is especially problematic for long documents, where redundancy tends to be a more serious problem, as we have observed in key datasets. Improving redundancy reduction in neural extractive summarization for long documents is a major goal of this paper. Indeed, some recently proposed neural methods aim to reduce redundancy, but they either do that implicitly or inflexibly and only focusing on short documents (e.g., news). For instance, some models learn to reduce redundancy when predicting the scores (Nallapati et al., 2016a), or jointly learn to score and select sentences (Z"
2020.aacl-main.51,P18-1061,0,0.127608,"and Sanz, 2013), because important sentences selected from multiple documents (about the same topic) are more 1 Our code can be found here - http://www.cs. ubc.ca/cs-research/lci/research-groups/ natural-language-processing/ likely to be redundant than sentences from the same document, generating a non-redundant summary should still be one of the goals for single document summarization (Lin et al., 2009). Generally speaking, there is a trade-off between importance and diversity (non-redundancy) (Jung et al., 2019), which is reflected in the two phases, sentence scoring and sentence selection (Zhou et al., 2018) in which extractive summarization task can be naturally decomposed. The former typically scores sentences based on importance, while the latter selects sentences based on their scores, but also possibly taking other factors (including redundancy) into account. Traditionally, in non-neural approaches the tradeoff between importance and redundancy has been carefully considered, with sentence selection picking sentences by optimizing an objective function that balances the two aspects (Carbonell and Goldstein, 1998; Ren et al., 2016). In contrast, more recent works on neural extractive summariza"
2020.aacl-main.63,Q19-1011,0,0.204484,"Missing"
2020.aacl-main.63,2020.acl-main.29,0,0.0164066,"1 outperforms SOTA approaches when trained and tested on three datasets. We also the robustness of our proposed model in domain transfer setting by training a model on a large-scale dataset and testing it on four challenging real-world benchmarks. Furthermore, we apply our proposed strategy to two other languages (German and Chinese), and show its effectiveness in multilingual scenarios. 1 Table 1: A Wikipedia sample article about City Marcus covering three topics: T1, T2 and T3 Introduction Topic segmentation is a fundamental NLP task that has received considerable attention in recent years (Barrow et al., 2020; Glavas and Somasundaran, 2020; Lukasik et al., 2020). It can reveal important aspects of a document semantic structure by splitting the document into topical-coherent textual units. Taking the Wikipedia article in Table 1 as an example, without the section marks, a reliable topic segmenter should be able to detect the correct boundaries within the text and chunk this article into the topical-coherent units T1, T2 and T3. The results of topic segmentation can further benefit other key downstream NLP tasks such as document summarization (Mitra et al., 1997; Riedl and Biemann, 2012a; Xiao and C"
2020.aacl-main.63,P05-1018,0,0.294493,"dict the sentence topics, the learned topic embeddings can be utilized for topic segmentation. However, one critical flaw of their method is that it requires a complicated pre-processing pipeline, which includes topic extraction and synset clustering, whose errors can propagate to the main topic segmentation task. In contrast, our proposal only requires the plain content of the training data without any complex pre-processing. Coherence Modeling Early works on coherence modeling merely predict the coherence score for documents by tracking the patterns of entities’ grammatical role transition (Barzilay and Lapata, 2005, 2008). More recently, researchers started modeling the coherence for sentence pairs by their semantic similarities and used them for higher level coherence prediction or even other tasks, including topic segmentation. Wang et al. (2017) demonstrated the strong relation between text-pair coherence modeling and topic segmentation. They assumed that (1) a pair of texts from the same document should be ranked more coherent than a pair of texts from different documents; (2) a pair of texts from the same segment should be ranked more coherent than a pair of texts from different segments of a docum"
2020.aacl-main.63,J08-1001,0,0.156715,"Missing"
2020.aacl-main.63,N09-1042,0,0.589699,"Missing"
2020.aacl-main.63,A00-2004,0,0.54522,"anisms to the basic model, we conduct three sets of experiments for evaluation: Datasets Data for Intra-Domain Evaluation High quality training dataset for topic segmentation usually satisfies the following criteria: (1) large size; (2) cover a variety of topics; (3) contains real documents with reliable segmentation either from human annotations or already specified in the documents e.g., sections. In order to comprehensively evaluate the effectiveness of our context modeling strategy when dealing with data of different quality, we train and test models on the following three datasets: CHOI (Choi, 2000) whose articles are synthesized artificially by stitching together different sources (i.e., they were not written as one document by one author). Hence, it does not really reflect naturally occurring topic drifts. While the quality of this dataset is low, it is an early but popular benchmark for topic segmentation evaluation. We include this dataset to allow comparison with the previous work. RULES (Bertrand et al., 2018) is a dataset collected from the U.S. Federal Register issues3 . When U.S. federal agencies make changes to regulations or other policies, they must publish a document called"
2020.aacl-main.63,N19-1423,0,0.0169066,"network, we applied an attention mechanism (Yang et al., 2016) to make the model better capture task-wise sentence semantics. The benefit of this enhancement is verified empirically by the results in Table 2. As it can be seen, replacing the max-pooling with the attention based BiLSTM sentence encoder yields better performance. Enhancing Generality with BERT Embeddings In order to better deal with unseen text in test data and hence improve the model’s generality, we utilize a pre-trained BERT sentence encoder2 which complements our sentence encoding network. The transformer-based BERT model (Devlin et al., 2019) was trained on multi-billion sentences publicly available on the web for several generic sentence-level semantic tasks, such as Natural Language Inference and Question Answering, which implies that it can arguably capture more general aspects of sentence semantics in a reliable way. To combine task-specific information with generic semantic signals from BERT, we simply concatenate the BERT sentence embeddings with the sentence embeddings derived from our encoder. Such concatenation then becomes the input of the next level 2 github.com/hanxiao/bert-as-service. For languages other than English,"
2020.aacl-main.63,D08-1035,0,0.619186,"tream NLP tasks such as document summarization (Mitra et al., 1997; Riedl and Biemann, 2012a; Xiao and Carenini, 2019), question answering (Oh et al., 2007; Diefenbach et al., 2018), machine reading (van Dijk, 1981; 1 Our code will be publicly available at www.cs. ubc.ca/cs-research/lci/research-groups/ natural-language-processing/ Saha et al., 2019) and dialogue modeling (Xu et al., 2020; Zhang et al., 2020). A wide variety of techniques have been proposed for topic segmentation. Early unsupervised models exploit word statistic overlaps (Hearst, 1997; Galley et al., 2003), Bayesian contexts (Eisenstein and Barzilay, 2008) or semantic relatedness graphs (Glavaˇs et al., 2016) to measure the lexical or semantic cohesion between the sentences or paragraphs and infer the segment boundaries from them. More recently, several works have framed topic segmentation as neural supervised learning, because of the remarkable success achieved by such models in most NLP tasks (Wang et al., 2016, 2017; Sehikh et al., 2017; Koshorek et al., 2018; Arnold et al., 2019). Despite minor architectural differences, most of these neural solutions adopt Recurrent Neural Network (Schuster and Paliwal, 1997) and its variants (RNNs) as the"
2020.aacl-main.63,P03-1071,0,0.857524,"ation can further benefit other key downstream NLP tasks such as document summarization (Mitra et al., 1997; Riedl and Biemann, 2012a; Xiao and Carenini, 2019), question answering (Oh et al., 2007; Diefenbach et al., 2018), machine reading (van Dijk, 1981; 1 Our code will be publicly available at www.cs. ubc.ca/cs-research/lci/research-groups/ natural-language-processing/ Saha et al., 2019) and dialogue modeling (Xu et al., 2020; Zhang et al., 2020). A wide variety of techniques have been proposed for topic segmentation. Early unsupervised models exploit word statistic overlaps (Hearst, 1997; Galley et al., 2003), Bayesian contexts (Eisenstein and Barzilay, 2008) or semantic relatedness graphs (Glavaˇs et al., 2016) to measure the lexical or semantic cohesion between the sentences or paragraphs and infer the segment boundaries from them. More recently, several works have framed topic segmentation as neural supervised learning, because of the remarkable success achieved by such models in most NLP tasks (Wang et al., 2016, 2017; Sehikh et al., 2017; Koshorek et al., 2018; Arnold et al., 2019). Despite minor architectural differences, most of these neural solutions adopt Recurrent Neural Network (Schuste"
2020.aacl-main.63,S16-2016,0,0.71927,"Missing"
2020.aacl-main.63,2020.cl-1.3,0,0.0263075,"0.6 41.5 39.3 7.7 9.6 7.0 6.1† 6.3† 5.8† SECTION 51.3 39.5 44.9 12.6 12.7 13.6 11.3 10.4† 10.0† 9.7† MEAN 50.4 33.9 30.3 7.1 9.3 6.4 5.7 5.7 5.3 Table 5: Pk error score on three datasets. Results in bold indicate the best performance across all comparisons. Underlined results indicate the best performance in the bottom section. † indicates the result is significantly different (p &lt; 0.05) from basic model. also contains articles about cities and diseases. The section marks are used as the ground truth labels. SECTION-ZH which was randomly generated from the Chinese Wikipedia dump4 mentioned in Hao and Paul (2020). As before, section marks are also used here as ground truth boundaries. The statistical details of these two datasets can be found in Table 4. 4.2 Baselines These include two popular unsupervised topic segmentation methods, BayesSeg (Eisenstein and Barzilay, 2008) and GraphSeg (Glavaˇs et al., 2016), as well as the three recently proposed supervised neural models, TextSeg (Koshorek et al., 2018) (from which we derive our basic model), Sector (Arnold et al., 2019) and Hierarchical Transformer (labeled Transformer in the tables) (Glavas and Somasundaran, 2020). We use the original implementati"
2020.aacl-main.63,J97-1003,0,0.829253,"topic segmentation can further benefit other key downstream NLP tasks such as document summarization (Mitra et al., 1997; Riedl and Biemann, 2012a; Xiao and Carenini, 2019), question answering (Oh et al., 2007; Diefenbach et al., 2018), machine reading (van Dijk, 1981; 1 Our code will be publicly available at www.cs. ubc.ca/cs-research/lci/research-groups/ natural-language-processing/ Saha et al., 2019) and dialogue modeling (Xu et al., 2020; Zhang et al., 2020). A wide variety of techniques have been proposed for topic segmentation. Early unsupervised models exploit word statistic overlaps (Hearst, 1997; Galley et al., 2003), Bayesian contexts (Eisenstein and Barzilay, 2008) or semantic relatedness graphs (Glavaˇs et al., 2016) to measure the lexical or semantic cohesion between the sentences or paragraphs and infer the segment boundaries from them. More recently, several works have framed topic segmentation as neural supervised learning, because of the remarkable success achieved by such models in most NLP tasks (Wang et al., 2016, 2017; Sehikh et al., 2017; Koshorek et al., 2018; Arnold et al., 2019). Despite minor architectural differences, most of these neural solutions adopt Recurrent N"
2020.aacl-main.63,P19-1403,0,0.0224973,"Related Work Topic Segmentation Early unsupervised models exploit the lexical overlaps of sentences to measure the lexical cohesion between sentences or paragraphs (Hearst, 1997; Galley et al., 2003; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012b). Then, by moving two sliding windows over the text, the cohesion between successive text units could be measured and a cohesion drop would signal a segment boundary. Even if these models do not require any training data, they only show limited performance in practice and are not general enough to handle the temporal change of the languages (Huang and Paul, 2019). More recently, neural-based supervised methods have been devised for topic segmentation because of their more accurate predictions and greater efficiency. One line of research frames topic segmentation as a sequence labeling problem and builds neural models to predict segment boundaries directly. Wang et al. (2016) proposed a simple BiLSTM model to label if a sentence is a segment boundary or not. They demonstrated that along with engineered features based on cue phrases (eg., ‘first of all’, ‘second’), their model can achieve marginally better performance than early unsupervised methods. La"
2020.aacl-main.63,D19-1235,1,0.876406,"Missing"
2020.aacl-main.63,2020.emnlp-main.603,1,0.790202,"Missing"
2020.aacl-main.63,N18-2075,0,0.140115,"riety of techniques have been proposed for topic segmentation. Early unsupervised models exploit word statistic overlaps (Hearst, 1997; Galley et al., 2003), Bayesian contexts (Eisenstein and Barzilay, 2008) or semantic relatedness graphs (Glavaˇs et al., 2016) to measure the lexical or semantic cohesion between the sentences or paragraphs and infer the segment boundaries from them. More recently, several works have framed topic segmentation as neural supervised learning, because of the remarkable success achieved by such models in most NLP tasks (Wang et al., 2016, 2017; Sehikh et al., 2017; Koshorek et al., 2018; Arnold et al., 2019). Despite minor architectural differences, most of these neural solutions adopt Recurrent Neural Network (Schuster and Paliwal, 1997) and its variants (RNNs) as their main framework. On the one hand, RNNs are appropriate because topic segmentation can be modelled as a sequence labeling task where each sentence is either the end of a segment or not. On the other hand, this choice makes these neural models limited in how to model the context. Because some sophisticated RNNs (eg., 626 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Comput"
2020.aacl-main.63,2020.emnlp-main.380,0,0.14792,"Missing"
2020.aacl-main.63,P06-1004,0,0.569958,"Missing"
2020.aacl-main.63,W97-0707,0,0.388011,"siderable attention in recent years (Barrow et al., 2020; Glavas and Somasundaran, 2020; Lukasik et al., 2020). It can reveal important aspects of a document semantic structure by splitting the document into topical-coherent textual units. Taking the Wikipedia article in Table 1 as an example, without the section marks, a reliable topic segmenter should be able to detect the correct boundaries within the text and chunk this article into the topical-coherent units T1, T2 and T3. The results of topic segmentation can further benefit other key downstream NLP tasks such as document summarization (Mitra et al., 1997; Riedl and Biemann, 2012a; Xiao and Carenini, 2019), question answering (Oh et al., 2007; Diefenbach et al., 2018), machine reading (van Dijk, 1981; 1 Our code will be publicly available at www.cs. ubc.ca/cs-research/lci/research-groups/ natural-language-processing/ Saha et al., 2019) and dialogue modeling (Xu et al., 2020; Zhang et al., 2020). A wide variety of techniques have been proposed for topic segmentation. Early unsupervised models exploit word statistic overlaps (Hearst, 1997; Galley et al., 2003), Bayesian contexts (Eisenstein and Barzilay, 2008) or semantic relatedness graphs (Gla"
2020.aacl-main.63,N12-1064,0,0.145991,"in recent years (Barrow et al., 2020; Glavas and Somasundaran, 2020; Lukasik et al., 2020). It can reveal important aspects of a document semantic structure by splitting the document into topical-coherent textual units. Taking the Wikipedia article in Table 1 as an example, without the section marks, a reliable topic segmenter should be able to detect the correct boundaries within the text and chunk this article into the topical-coherent units T1, T2 and T3. The results of topic segmentation can further benefit other key downstream NLP tasks such as document summarization (Mitra et al., 1997; Riedl and Biemann, 2012a; Xiao and Carenini, 2019), question answering (Oh et al., 2007; Diefenbach et al., 2018), machine reading (van Dijk, 1981; 1 Our code will be publicly available at www.cs. ubc.ca/cs-research/lci/research-groups/ natural-language-processing/ Saha et al., 2019) and dialogue modeling (Xu et al., 2020; Zhang et al., 2020). A wide variety of techniques have been proposed for topic segmentation. Early unsupervised models exploit word statistic overlaps (Hearst, 1997; Galley et al., 2003), Bayesian contexts (Eisenstein and Barzilay, 2008) or semantic relatedness graphs (Glavaˇs et al., 2016) to mea"
2020.aacl-main.63,W12-3307,0,0.888317,"in recent years (Barrow et al., 2020; Glavas and Somasundaran, 2020; Lukasik et al., 2020). It can reveal important aspects of a document semantic structure by splitting the document into topical-coherent textual units. Taking the Wikipedia article in Table 1 as an example, without the section marks, a reliable topic segmenter should be able to detect the correct boundaries within the text and chunk this article into the topical-coherent units T1, T2 and T3. The results of topic segmentation can further benefit other key downstream NLP tasks such as document summarization (Mitra et al., 1997; Riedl and Biemann, 2012a; Xiao and Carenini, 2019), question answering (Oh et al., 2007; Diefenbach et al., 2018), machine reading (van Dijk, 1981; 1 Our code will be publicly available at www.cs. ubc.ca/cs-research/lci/research-groups/ natural-language-processing/ Saha et al., 2019) and dialogue modeling (Xu et al., 2020; Zhang et al., 2020). A wide variety of techniques have been proposed for topic segmentation. Early unsupervised models exploit word statistic overlaps (Hearst, 1997; Galley et al., 2003), Bayesian contexts (Eisenstein and Barzilay, 2008) or semantic relatedness graphs (Glavaˇs et al., 2016) to mea"
2020.aacl-main.63,D17-1139,0,0.697876,"020. 2020 Association for Computational Linguistics LSTM, GRU) are able to preserve long-distance information (Lipton et al., 2015; Sehikh et al., 2017; Wang et al., 2018), which can largely help language models. But for topic segmentation, it is critical to supervise the model to focus more on the local context. As illustrated in Table 1, the prediction of the segment boundary between T1 and T2 hardly depends on the content in T3. Bringing in excessive long-distance signals may cause unnecessary noise and hurt performance. Moreover, text coherence has strong relation with topic segmentation (Wang et al., 2017; Glavas and Somasundaran, 2020). For instance, in Table 1, sentence pairs from the same segment (like &lt;S1, S2&gt; or &lt;S3, S4&gt;) are more coherent than sentence pairs across segments (like S2 and S3). Arguably, with a proper way of modeling the coherence between adjacent sentences, a topic segmenter can be further enhanced. In this paper, we propose to enhance a state-ofthe-art (SOTA) topic segmenter (Koshorek et al., 2018) based on hierarchical attention BiLSTM network to better model the local context of a sentence in two complementary ways. First, we add a coherence-related auxiliary task to ma"
2020.aacl-main.63,P16-1218,0,0.0292013,"liary task module and its integration in our segmenter is shown in red in Figure 2. li = 1 if sentences in this pair are from the same segment, and li = 0 otherwise. The embeddings ei and ei+1 of adjacent sentences pairs &lt; si , si+1 &gt; used for coherence computing are calculated from → − BiLSTM forward and backward hidden states h ← − and h , following the equations below: → − −−→ ei = tanh(We ( hi − hi−1 ) + be ) (2) ←−− ←−− ei+1 = tanh(We (hi+1 − hi+2 ) + be ) (3) 3.4 However, notice that instead of using the conven→ − ← − tional [ hi ; hi ] as the embedding of sentence i, here, similarly to Wang and Chang (2016), we subtract forward/backward states to focus on the semantics of sentences in the current sentence pair. The semantic coherence between two sentence embeddings is then computed as the sigmoid of their cosine similarity: Cohi = σ(cos(ei , ei+1 )) (4) We use binary cross-entropy loss to formulate the objective of our auxiliary task. For a document with k sentences, the loss can be calculated as: L2 = − k−1 X i=1,li =1 log Cohi − k−1 X log(1 − Cohi ) i=1,li =0 (5) which penalizes high Coh across segments and low Coh within segments. Combining Equation 1 and 5, we form the loss function of our n"
2020.aacl-main.63,D18-1116,0,0.0358623,"Missing"
2020.aacl-main.63,D19-1298,1,0.821745,"al., 2020; Glavas and Somasundaran, 2020; Lukasik et al., 2020). It can reveal important aspects of a document semantic structure by splitting the document into topical-coherent textual units. Taking the Wikipedia article in Table 1 as an example, without the section marks, a reliable topic segmenter should be able to detect the correct boundaries within the text and chunk this article into the topical-coherent units T1, T2 and T3. The results of topic segmentation can further benefit other key downstream NLP tasks such as document summarization (Mitra et al., 1997; Riedl and Biemann, 2012a; Xiao and Carenini, 2019), question answering (Oh et al., 2007; Diefenbach et al., 2018), machine reading (van Dijk, 1981; 1 Our code will be publicly available at www.cs. ubc.ca/cs-research/lci/research-groups/ natural-language-processing/ Saha et al., 2019) and dialogue modeling (Xu et al., 2020; Zhang et al., 2020). A wide variety of techniques have been proposed for topic segmentation. Early unsupervised models exploit word statistic overlaps (Hearst, 1997; Galley et al., 2003), Bayesian contexts (Eisenstein and Barzilay, 2008) or semantic relatedness graphs (Glavaˇs et al., 2016) to measure the lexical or semanti"
2020.aacl-main.63,N16-1174,0,0.0211578,"pk−1 } for a document with k sentences: k−1 X L1 = − [yi log pi + (1 − yi ) log(1 − pi )] (1) i=1 Looking at the details of the architecture in Figure 1, our basic model constitutes a strong baseline by extending the segmenter presented in Koshorek et al. (2018) in two ways (colored parts); namely, by improving the sentence encoder with an attention mechanism (orange) and with BERT embeddings (blue). Enhancing Task-Specific Sentence Representations - While Koshorek et al. (2018) applied maxpooling to build sentence embeddings from sentence encoding network, we applied an attention mechanism (Yang et al., 2016) to make the model better capture task-wise sentence semantics. The benefit of this enhancement is verified empirically by the results in Table 2. As it can be seen, replacing the max-pooling with the attention based BiLSTM sentence encoder yields better performance. Enhancing Generality with BERT Embeddings In order to better deal with unseen text in test data and hence improve the model’s generality, we utilize a pre-trained BERT sentence encoder2 which complements our sentence encoding network. The transformer-based BERT model (Devlin et al., 2019) was trained on multi-billion sentences pub"
2020.aacl-main.63,Q19-1017,0,0.0605306,"Missing"
2020.aacl-main.67,D14-1162,0,0.0833887,"neural techniques have dominated recent research. Li and Jurafsky (2017) applied Recurrent Neural Networks (RNNs) to model the coherent generation of the h_right RST Network Recursive adapted from (Tai et al., 2015). E Coherence Embedding σ 3 3.1 Method RST-Recursive We parse silver-standard RST trees for documents using the CODRA (Joty et al., 2015) RST parser, which we then employ as input to our recursive neural model, RST-Recursive. The overall procedure 665 for RST-Recursive is shown in Figure 1. Given a document of n EDUs E1:n with each EDU Ei represented as a list of GloVe embeddings (Pennington et al., 2014), we use an LSTM to process each Ei , using the final hidden state as the EDU embedding ei = LSTM(Ei ) for each leaf i of the document’s RST tree. Afterwards, we apply a recursive LSTM architecture (Figure 2) that traverses the RST tree bottom-up. At each node s, we use the children’s sub-tree embeddings [hl , cl , rl ] and [hr , cr , rr ] to form the node’s sub-tree embedding: [hs , cs ] = TreeLSTM([hl , cl , rl ], [hr , cr , rr ]) (1) where hl /cl and hr /cr are the LSTM hidden and cell states from the left and right sub-trees respectively. The relation embeddings of the children sub-trees,"
2020.aacl-main.67,prasad-etal-2008-penn,0,0.125277,"Missing"
2020.aacl-main.67,P15-1150,0,0.0401543,"Missing"
2020.aacl-main.67,P17-1121,0,0.0168083,"uation of Text Centering Theory (Grosz et al., 1994) states that subsequent sentences in coherent texts are likely to continue to focus on the same entities (i.e., subjects, objects, etc.) as within the previous sentences. Building on top of this, Barzilay and Lapata (2008) were the first to propose the Entity-Grid model that constructs a two-dimensional array Gn,m for a text of n sentences and m entities, which are used to estimate transition probabilities for entity occurrence patterns. More recently, Elsner and Charniak (2011) extended Entity-Grid using entity-specific features, while Tien Nguyen and Joty (2017) used a Convolutional Neural Network (CNN) on top of Entity-Grid to learn more hierarchical patterns. On the other hand, feature-free deep neural techniques have dominated recent research. Li and Jurafsky (2017) applied Recurrent Neural Networks (RNNs) to model the coherent generation of the h_right RST Network Recursive adapted from (Tai et al., 2015). E Coherence Embedding σ 3 3.1 Method RST-Recursive We parse silver-standard RST trees for documents using the CODRA (Joty et al., 2015) RST parser, which we then employ as input to our recursive neural model, RST-Recursive. The overall procedur"
2020.codi-1.13,P14-2052,0,0.406579,"ild in the parent’s relation. Alternatively, if both children are equally important, 124 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 124–134 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 both are represented as Nuclei. While other popular theories of discourse exist (most notably PDTB (Prasad et al., 2008)), RST along with its human-annotated RST-DT treebank (Carlson et al., 2002) have been leveraged in the past to improve extractive summarizations, with either unsupervised (Hirao et al., 2013; Kikuchi et al., 2014), or supervised (Xu et al., 2020) methods. In this paper, we explore a novel, equally important application for discourse information in extractive summarization, namely to reduce the number of parameters. Instead of exploiting discourse trees as an additional source of information on top of neural models, we use the information as a prior to reduce the number of parameters of existing neural models. This is critical not only to reduce the risk of over-fitting but also to create smaller models that are easier to interpret and deploy. Not surprisingly, reducing the number of parameters has beco"
2020.codi-1.13,N18-2097,0,0.029818,"rees, previously converted from the RST constituency trees, aiming to generate a more coherent summary. Based on this idea of trimming the dependencytree, Kikuchi et al. (2014) propose another method of trimming nested trees, composed into two levels: a document-tree considering the structure of the document and a sentence-tree considering the structure within each sentence. More recently, further work along this line started to incorporate discourse structures into supervised summarization with the goal to better leverage the (linguistic) structure of a document. Xiao and Carenini (2019) and Cohan et al. (2018) thereby use the natural structure of scientific papers (i.e. sections) to improve the inputs of the sequence models, better encoding long documents using a structural prior. They empirically show that such structure effectively improves performance. Moreover, Xu et al. (2020) propose a graphbased discourse-aware extractive summarization method incorporating the dependency trees converted from RST trees on top of the BERTSUM model (Liu and Lapata, 2019) and the document co-reference graph. The results show consistent improvements, implying a close, bidirectional relationship between downstream"
2020.codi-1.13,D19-1445,0,0.111799,"e models really learn? Such that better priors can be provided and less parameters are required and (2) Are all the model parameters necessary? To identify which parameters can be safely removed. Recently, researchers have explored these questions especially in the context of transformer models. With respect to what is learned in such models, several experiments reveal that the information captured by the multi-head self-attention in the popular BERT model (i.e., the learned attention weights) generally align well with syntactic and semantic relations within sentences (Vig and Belinkov, 2019; Kovaleva et al., 2019). Regarding the second question, building on previous work exploring how to prune large neural models while keeping the performance comparable to the original model (Michel et al., 2019), very recently Tay et al. (2020) has proposed the ”Synthesizer” framework, comparing the performance when replacing the dot-product selfattention in the original transformer model with other, less parameterized, attention types. Inspired by these two lines of research on transformer-based models, namely the identification of a close connection between learned attention weights and linguistic structures, and th"
2020.codi-1.13,N19-1423,0,0.0281589,"tance of a unit during training. In recent years, the role of attention within NLP further solidified with researchers exploring new variants, such as multi-head self-attention, as used in transformers (Vaswani et al., 2017). Generally, larger transformer models with more attentionheads (and therefore more parameters) achieve better performance for many tasks (Vaswani et al., 2017). In the context of explaining the internal workings of neural models, Kovaleva et al. (2019) has recently focused on transformer-style models, investigating the role of individual attention-heads in the BERT model (Devlin et al., 2019). Analyzing the capacity to capture different linguistic information within the self-attention module, they find that information represented across attention-heads is oftentimes redundant, thus showing potential to prune those parameters. Following these findings, Raganato et al. (2020) define a combination of fixed, position-based attention heads and a single learnable dot-product self-attention head. They empirically show that this hybrid approach reduces the spatial complexity of the model, while retaining the original performance. In addition, the hybrid model improves the performance in"
2020.codi-1.13,W16-3616,0,0.109194,", obtained from the constituency representation. 4.1 Dependency-based Nuclearity Attributes (D-Tree) Inspired by previous work using dependency trees to support the summarization task (Marcu, 1999; Hirao et al., 2013; Xu et al., 2020), we first convert the original constituency-tree, obtained with the RST-DT trained discourse parser (Wang et al., 2017), into the respective dependency tree and subsequently generate the final matrix-representation. In the first step, we follow the constituency-todependency conversion algorithm proposed by Hirao et al. (2013) (shown superior for summarization in Hayashi et al. (2016)). While this algorithm ensures a near-bijective conversion (see Morey et al. (2018)), the resulting dependency trees do not necessarily have single-rooted sentence sub-trees.To account for this, we apply the post-editing method proposed in Hayashi et al. (2016). To use the newly generated dependency tree in the “Synthesizer” transformer model, we generate the self-attention matrix from the tree structure by following a standard Graph Theory approach (Xu et al., 2020). Head-dependent relations in the tree are represented as binary values (1 indicating a relation, 0 representing no connection)"
2020.codi-1.13,D15-1166,0,0.0609065,"stic structures, and the potential for safely reducing attention parameters, we propose a document-level discourse-based attention method for extractive summarization. With this new, discourse-inspired approach, we reduce the size of the attention module, the core component of the transformer model, while keeping the modelperformance competitive to comparable, fully parameterized models on both EDU and sentence level. 2 2.1 Related Work Attention Methods Attention mechanisms have become a widely used component of many modern neural NLP models. Originally proposed by Bahdanau et al. (2014) and Luong et al. (2015) for machine translation, the general idea behind attention is based on the intuition that not all textual units within a sequence contribute equally to the result. Thus, the attention value is introduced to learn how to assess the importance of a unit during training. In recent years, the role of attention within NLP further solidified with researchers exploring new variants, such as multi-head self-attention, as used in transformers (Vaswani et al., 2017). Generally, larger transformer models with more attentionheads (and therefore more parameters) achieve better performance for many tasks ("
2020.codi-1.13,D13-1158,0,0.320816,"n the ’Satellite’ child in the parent’s relation. Alternatively, if both children are equally important, 124 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 124–134 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 both are represented as Nuclei. While other popular theories of discourse exist (most notably PDTB (Prasad et al., 2008)), RST along with its human-annotated RST-DT treebank (Carlson et al., 2002) have been leveraged in the past to improve extractive summarizations, with either unsupervised (Hirao et al., 2013; Kikuchi et al., 2014), or supervised (Xu et al., 2020) methods. In this paper, we explore a novel, equally important application for discourse information in extractive summarization, namely to reduce the number of parameters. Instead of exploiting discourse trees as an additional source of information on top of neural models, we use the information as a prior to reduce the number of parameters of existing neural models. This is critical not only to reduce the risk of over-fitting but also to create smaller models that are easier to interpret and deploy. Not surprisingly, reducing the number"
2020.codi-1.13,D19-1235,1,0.819188,"ture of scientific papers (i.e. sections) to improve the inputs of the sequence models, better encoding long documents using a structural prior. They empirically show that such structure effectively improves performance. Moreover, Xu et al. (2020) propose a graphbased discourse-aware extractive summarization method incorporating the dependency trees converted from RST trees on top of the BERTSUM model (Liu and Lapata, 2019) and the document co-reference graph. The results show consistent improvements, implying a close, bidirectional relationship between downstream tasks and discourse parsing. Huber and Carenini (2019, 2020) show that sentiment information can be used to infer discourse trees with promising performance. They further mention extractive summarization as another important downstream task with strong potential connections to the document’s discourse, motivating the bidirectional use of available information. This paper employs a rather different objective from aforementioned work combining discourse and summarization. Instead of leveraging additional discourse information to enhance the model performance, we strive to create a summarization model with significantly less parameters, hence being"
2020.codi-1.13,2020.emnlp-main.603,1,0.845908,"Missing"
2020.codi-1.13,J15-3002,1,0.876366,"ain the respective sentence-level selfattention matrix, given the EDU-level self-attention matrix Ae of the three matrix-generation approaches defined above, we define an indicatormatrix I ∈ RN S×N E . N S and N E are thereby the number of sentences and EDUs in the document. Iij = 1 if and only if EDU j belongs to sentence i. The sentence-level self-attention matrix As is then defined as As = IAe I T Generating the sentence-level self-attention matrices directly from the EDU-level self-attention matrices, instead of the tree-representation itself, avoids the problem of potentially leaky EDUs (Joty et al., 2015), as sentences with leaky EDUs (having naturally high attention values between them) will continue to be tightly connected. 5 Experiments 5.1 Experimental Setup Dataset: We use the popular CNN/DM dataset (Nallapati et al., 2016), a standard corpus for extractive summarization. Key dimensions of the dataset with corresponding statistics are in Table 1. Based on the average number of units selected #token 546 #EDU 70.2 #Sent 27.2 #EDU(O.) 6.4 #Sent(O.) 3.1 Table 1: Statistics of the CNNDM dataset. O. means the average number of units in the oracle by the oracle4 on EDU- and sentence-level, we de"
2020.codi-1.13,D19-1387,0,0.0945324,"structures into supervised summarization with the goal to better leverage the (linguistic) structure of a document. Xiao and Carenini (2019) and Cohan et al. (2018) thereby use the natural structure of scientific papers (i.e. sections) to improve the inputs of the sequence models, better encoding long documents using a structural prior. They empirically show that such structure effectively improves performance. Moreover, Xu et al. (2020) propose a graphbased discourse-aware extractive summarization method incorporating the dependency trees converted from RST trees on top of the BERTSUM model (Liu and Lapata, 2019) and the document co-reference graph. The results show consistent improvements, implying a close, bidirectional relationship between downstream tasks and discourse parsing. Huber and Carenini (2019, 2020) show that sentiment information can be used to infer discourse trees with promising performance. They further mention extractive summarization as another important downstream task with strong potential connections to the document’s discourse, motivating the bidirectional use of available information. This paper employs a rather different objective from aforementioned work combining discourse"
2020.codi-1.13,J18-2001,0,0.0514146,"butes (D-Tree) Inspired by previous work using dependency trees to support the summarization task (Marcu, 1999; Hirao et al., 2013; Xu et al., 2020), we first convert the original constituency-tree, obtained with the RST-DT trained discourse parser (Wang et al., 2017), into the respective dependency tree and subsequently generate the final matrix-representation. In the first step, we follow the constituency-todependency conversion algorithm proposed by Hirao et al. (2013) (shown superior for summarization in Hayashi et al. (2016)). While this algorithm ensures a near-bijective conversion (see Morey et al. (2018)), the resulting dependency trees do not necessarily have single-rooted sentence sub-trees.To account for this, we apply the post-editing method proposed in Hayashi et al. (2016). To use the newly generated dependency tree in the “Synthesizer” transformer model, we generate the self-attention matrix from the tree structure by following a standard Graph Theory approach (Xu et al., 2020). Head-dependent relations in the tree are represented as binary values (1 indicating a relation, 0 representing no connection) in the selfattention matrix, where each column of the matrix identifies the head and"
2020.codi-1.13,K16-1028,0,0.124477,"Missing"
2020.codi-1.13,prasad-etal-2008-penn,0,0.0123931,"nts). An additional nuclearity attribute is assigned to each child, representing the importance of the subtree in the local constituent, i.e. the ’Nucleus’ child plays a more important role than the ’Satellite’ child in the parent’s relation. Alternatively, if both children are equally important, 124 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 124–134 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 both are represented as Nuclei. While other popular theories of discourse exist (most notably PDTB (Prasad et al., 2008)), RST along with its human-annotated RST-DT treebank (Carlson et al., 2002) have been leveraged in the past to improve extractive summarizations, with either unsupervised (Hirao et al., 2013; Kikuchi et al., 2014), or supervised (Xu et al., 2020) methods. In this paper, we explore a novel, equally important application for discourse information in extractive summarization, namely to reduce the number of parameters. Instead of exploiting discourse trees as an additional source of information on top of neural models, we use the information as a prior to reduce the number of parameters of existi"
2020.codi-1.13,2020.findings-emnlp.49,0,0.0203595,"s (and therefore more parameters) achieve better performance for many tasks (Vaswani et al., 2017). In the context of explaining the internal workings of neural models, Kovaleva et al. (2019) has recently focused on transformer-style models, investigating the role of individual attention-heads in the BERT model (Devlin et al., 2019). Analyzing the capacity to capture different linguistic information within the self-attention module, they find that information represented across attention-heads is oftentimes redundant, thus showing potential to prune those parameters. Following these findings, Raganato et al. (2020) define a combination of fixed, position-based attention heads and a single learnable dot-product self-attention head. They empirically show that this hybrid approach reduces the spatial complexity of the model, while retaining the original performance. In addition, the hybrid model improves the performance in the low-resource case. Broadening these results, Tay et al. (2020) further investigate the contribution of the self-attention mechanism. In their proposed “Synthesizer” model, they present a generalized version of the transformer, exploring alternative attention types, generally requirin"
2020.codi-1.13,W19-4808,0,0.0209252,"stions: (1) What do these models really learn? Such that better priors can be provided and less parameters are required and (2) Are all the model parameters necessary? To identify which parameters can be safely removed. Recently, researchers have explored these questions especially in the context of transformer models. With respect to what is learned in such models, several experiments reveal that the information captured by the multi-head self-attention in the popular BERT model (i.e., the learned attention weights) generally align well with syntactic and semantic relations within sentences (Vig and Belinkov, 2019; Kovaleva et al., 2019). Regarding the second question, building on previous work exploring how to prune large neural models while keeping the performance comparable to the original model (Michel et al., 2019), very recently Tay et al. (2020) has proposed the ”Synthesizer” framework, comparing the performance when replacing the dot-product selfattention in the original transformer model with other, less parameterized, attention types. Inspired by these two lines of research on transformer-based models, namely the identification of a close connection between learned attention weights and lingu"
2020.codi-1.13,P17-2029,0,0.165544,"ibutes, we propose three distinct tree-tomatrix encodings focusing on: the nuclearityattribute, through a dependency-tree transformation; the plain discourse-structure, derived from the original constituency structure; and a nuclearityaugmented discourse structure, obtained from the constituency representation. 4.1 Dependency-based Nuclearity Attributes (D-Tree) Inspired by previous work using dependency trees to support the summarization task (Marcu, 1999; Hirao et al., 2013; Xu et al., 2020), we first convert the original constituency-tree, obtained with the RST-DT trained discourse parser (Wang et al., 2017), into the respective dependency tree and subsequently generate the final matrix-representation. In the first step, we follow the constituency-todependency conversion algorithm proposed by Hirao et al. (2013) (shown superior for summarization in Hayashi et al. (2016)). While this algorithm ensures a near-bijective conversion (see Morey et al. (2018)), the resulting dependency trees do not necessarily have single-rooted sentence sub-trees.To account for this, we apply the post-editing method proposed in Hayashi et al. (2016). To use the newly generated dependency tree in the “Synthesizer” trans"
2020.codi-1.13,D18-1116,0,0.0488704,"Missing"
2020.codi-1.13,D19-1298,1,0.828285,"-based method on dependency trees, previously converted from the RST constituency trees, aiming to generate a more coherent summary. Based on this idea of trimming the dependencytree, Kikuchi et al. (2014) propose another method of trimming nested trees, composed into two levels: a document-tree considering the structure of the document and a sentence-tree considering the structure within each sentence. More recently, further work along this line started to incorporate discourse structures into supervised summarization with the goal to better leverage the (linguistic) structure of a document. Xiao and Carenini (2019) and Cohan et al. (2018) thereby use the natural structure of scientific papers (i.e. sections) to improve the inputs of the sequence models, better encoding long documents using a structural prior. They empirically show that such structure effectively improves performance. Moreover, Xu et al. (2020) propose a graphbased discourse-aware extractive summarization method incorporating the dependency trees converted from RST trees on top of the BERTSUM model (Liu and Lapata, 2019) and the document co-reference graph. The results show consistent improvements, implying a close, bidirectional relatio"
2020.codi-1.13,2020.acl-main.451,0,0.606528,"ively, if both children are equally important, 124 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 124–134 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 both are represented as Nuclei. While other popular theories of discourse exist (most notably PDTB (Prasad et al., 2008)), RST along with its human-annotated RST-DT treebank (Carlson et al., 2002) have been leveraged in the past to improve extractive summarizations, with either unsupervised (Hirao et al., 2013; Kikuchi et al., 2014), or supervised (Xu et al., 2020) methods. In this paper, we explore a novel, equally important application for discourse information in extractive summarization, namely to reduce the number of parameters. Instead of exploiting discourse trees as an additional source of information on top of neural models, we use the information as a prior to reduce the number of parameters of existing neural models. This is critical not only to reduce the risk of over-fitting but also to create smaller models that are easier to interpret and deploy. Not surprisingly, reducing the number of parameters has become increasingly important in the"
2020.codi-1.17,D15-1263,0,0.0206243,"acting as the leaves of the tree. Adjacent EDUs and constituents are hierarchically aggregated to form (possibly non-binary) constituents, with internal nodes containing (1) a nuclearity label, defining the importance of that subtree (rooted at the internal node) in the local context and (2) a relation label, defining the type of semantic connection between the two subtrees (e.g., Elaboration, Background). Previous research has shown that the use of RSTstyle discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015; Gerani et al., 2014; Ji and Smith, 2017). And more recently, it has been found that RST discourse structures can complement learned contextual embeddings (e.g., BERT (Devlin et al., 2018)), in tasks where linguistic information on complete documents is critical, such as argumentation analysis (Chakrabarty et al., 2019). In this work, we present preliminary results of investigating the benefits of coreference resolution features for RST discourse parsing. From the theoretical perspective, it has long been established (Asher and Lascarides, 2003) tha"
2020.codi-1.17,D19-1291,0,0.0201688,"the two subtrees (e.g., Elaboration, Background). Previous research has shown that the use of RSTstyle discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015; Gerani et al., 2014; Ji and Smith, 2017). And more recently, it has been found that RST discourse structures can complement learned contextual embeddings (e.g., BERT (Devlin et al., 2018)), in tasks where linguistic information on complete documents is critical, such as argumentation analysis (Chakrabarty et al., 2019). In this work, we present preliminary results of investigating the benefits of coreference resolution features for RST discourse parsing. From the theoretical perspective, it has long been established (Asher and Lascarides, 2003) that discourse structure can impose constraints on mention antecedent distributions, with these constraints being derived from the role of each discourse unit (sen160 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 160–167 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 tenc"
2020.codi-1.17,P98-1044,0,0.14607,"e benefits of coreference resolution features for RST discourse parsing. From the theoretical perspective, it has long been established (Asher and Lascarides, 2003) that discourse structure can impose constraints on mention antecedent distributions, with these constraints being derived from the role of each discourse unit (sen160 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 160–167 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 tence or EDU) with respect to the global discourse. The Veins theory (Cristea et al., 1998) is the most known formalization of anaphoric constraints with respect to RST tree structures, involving assigning to each EDU a subset of preceding EDUs defined by the nuclearity attributes of the EDU’s parent nodes in the document’s discourse tree (see Appendix A for the exact definition). These constrains act as a domain of referential accessibility where the antecedents must reside, for otherwise the discourse would be considered incoherent. As an example of this phenomenon, consider the discourse structure in Figure 1. In principle, a reader could apply commonsense knowledge to resolve th"
2020.codi-1.17,N18-1013,0,0.0163832,"the three proposed discourse parsing models which differ in the levels of coupling with the coreference model. See Figure 2 for the visual comparison. SpanBERT-NoCoref: in addition to the organizational features, our baseline system utilizes only the output SpanBERT-contextualized word embeddings. To predict each Stack-Queue action, a full document is passed through SpanBERT in a non-overlapping sliding window fashion, as per Joshi et al. (2020), so that the context of full document can be considered for each parsing action to account for possible context-sensitivity of discourse structures (Dai and Huang, 2018). The node representation vQ1 for the first Queue element is computed as the mean of the first and the last word 162 Model HILDA(2010) DPLP(2014) CODRA(2015) Two-Stage(2017) Transition-Syntax(2018) D2P2S2E (Ensemble)(2020) SpanBERT-NoCoref SpanBERT-CorefFeats SpanBERT-MultiTask Human (2017) Structure 82.6 82.0 82.6 86.0 85.5 87.0 87.8 ± 0.2 88.1 ± 0.3 87.9 ± 0.2 88.3 Nuclearity 66.6 68.2 68.3 72.4 73.1 74.6 75.8 ± 0.2 76.1 ± 0.6 75.9 ± 0.6 77.3 Relation 54.6 57.8 55.8 59.7 60.2 60.0 63.4 ± 0.3 63.6 ± 0.3 63.3 ± 0.7 65.4 Table 1: RST-Parseval micro precision for structure, nuclearity and relati"
2020.codi-1.17,D19-1295,0,0.0194298,"constrains must be defined softly, at least in the context of RST theory. To explore these ideas computationally with respect to modern neural models, we investigate the utility of automatically extracted coreference features and discourse-coreference shared representations in the context and for the benefit of neural RST discourse parsing. Our strong baseline SpanBERT-NoCoref utilizes SpanBERT (Joshi et al., 2020) as in the current SOTA coreference resolver, without utilizing any direct coreference information. Next, our SpanBERT-CorefFeats considers the output of coreference resolver as per Dai and Huang (2019), letting us test the benefit of predicted and so possibly noisy coreference features. Finally, our more sophisticated SpanBERTMultitask model learns discourse parsing together with coreference resolution in the neural multitask learning fashion, sharing the SpanBERT contextual word encoder for both models. 2 Related Work Dai and Huang (2019) have already explored the benefit of using coreference information for neural PDTB implicit discourse relation classification, in a way similar to our SpanBERT-CorefFeats model. In our study, we also explore the use of shared encoder architecture for both"
2020.codi-1.17,N19-1423,0,0.0161597,"performance, reporting however the scores of an ensemble of five independent runs of their proposed model instead of single-model results. In this work we follow the shift-reduce strategy and apply SpanBERT-Base (Joshi et al., 2020; Wolf et al., 2020), which we introduce below, for encoding the document contents. The field of coreference resolution has recently been dominated by deep learning models. The current SOTA model by Joshi et al. (2020) is built upon the neural coreference resolver of (Lee et al., 2018) by incorporating SpanBERT language model, which modifies the commonly used BERT (Devlin et al., 2019) architecture with a novel span masking pretraining objective. In our work, we reimplemented their coreference resolver in PyTorch (Paszke et al., 2019). Our code for both models is available1 . 3 Shift-Reduce Architecture All our proposed parsers share the same basic shiftreduce architecture, consisting of a Queue, which is initially filled with documents EDUs in order 161 1 http://www.cs.ubc.ca/ cs-research/lci/research-groups/ natural-language-processing/index.html vS2 ... si si+1 vS1 si+2 ... sj sj+1 ... ... vS2 SpanBERT ... w&apos;i w&apos;i+1 w&apos;i+2 ... w&apos;j w&apos;j+1 ... ... ... wi wi+1 wi+2 S2 ... wj"
2020.codi-1.17,D14-1168,1,0.801242,"are hierarchically aggregated to form (possibly non-binary) constituents, with internal nodes containing (1) a nuclearity label, defining the importance of that subtree (rooted at the internal node) in the local context and (2) a relation label, defining the type of semantic connection between the two subtrees (e.g., Elaboration, Background). Previous research has shown that the use of RSTstyle discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015; Gerani et al., 2014; Ji and Smith, 2017). And more recently, it has been found that RST discourse structures can complement learned contextual embeddings (e.g., BERT (Devlin et al., 2018)), in tasks where linguistic information on complete documents is critical, such as argumentation analysis (Chakrabarty et al., 2019). In this work, we present preliminary results of investigating the benefits of coreference resolution features for RST discourse parsing. From the theoretical perspective, it has long been established (Asher and Lascarides, 2003) that discourse structure can impose constraints on mention anteceden"
2020.codi-1.17,P14-1002,0,0.0830427,"Missing"
2020.codi-1.17,P17-1092,0,0.0144914,"ggregated to form (possibly non-binary) constituents, with internal nodes containing (1) a nuclearity label, defining the importance of that subtree (rooted at the internal node) in the local context and (2) a relation label, defining the type of semantic connection between the two subtrees (e.g., Elaboration, Background). Previous research has shown that the use of RSTstyle discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015; Gerani et al., 2014; Ji and Smith, 2017). And more recently, it has been found that RST discourse structures can complement learned contextual embeddings (e.g., BERT (Devlin et al., 2018)), in tasks where linguistic information on complete documents is critical, such as argumentation analysis (Chakrabarty et al., 2019). In this work, we present preliminary results of investigating the benefits of coreference resolution features for RST discourse parsing. From the theoretical perspective, it has long been established (Asher and Lascarides, 2003) that discourse structure can impose constraints on mention antecedent distributions, with"
2020.codi-1.17,2020.tacl-1.5,0,0.163365,"the antecedent boundaries defined by Veins Theory are often too restrictive, suggesting that while discourse structures can be useful for predicting coreference structures and vice versa, these mutual constrains must be defined softly, at least in the context of RST theory. To explore these ideas computationally with respect to modern neural models, we investigate the utility of automatically extracted coreference features and discourse-coreference shared representations in the context and for the benefit of neural RST discourse parsing. Our strong baseline SpanBERT-NoCoref utilizes SpanBERT (Joshi et al., 2020) as in the current SOTA coreference resolver, without utilizing any direct coreference information. Next, our SpanBERT-CorefFeats considers the output of coreference resolver as per Dai and Huang (2019), letting us test the benefit of predicted and so possibly noisy coreference features. Finally, our more sophisticated SpanBERTMultitask model learns discourse parsing together with coreference resolution in the neural multitask learning fashion, sharing the SpanBERT contextual word encoder for both models. 2 Related Work Dai and Huang (2019) have already explored the benefit of using coreferenc"
2020.codi-1.17,J15-3002,1,0.895514,"Missing"
2020.codi-1.17,N18-2108,0,0.0433181,"Missing"
2020.codi-1.17,D17-1136,0,0.0160166,"an of the first and the last word 162 Model HILDA(2010) DPLP(2014) CODRA(2015) Two-Stage(2017) Transition-Syntax(2018) D2P2S2E (Ensemble)(2020) SpanBERT-NoCoref SpanBERT-CorefFeats SpanBERT-MultiTask Human (2017) Structure 82.6 82.0 82.6 86.0 85.5 87.0 87.8 ± 0.2 88.1 ± 0.3 87.9 ± 0.2 88.3 Nuclearity 66.6 68.2 68.3 72.4 73.1 74.6 75.8 ± 0.2 76.1 ± 0.6 75.9 ± 0.6 77.3 Relation 54.6 57.8 55.8 59.7 60.2 60.0 63.4 ± 0.3 63.6 ± 0.3 63.3 ± 0.7 65.4 Table 1: RST-Parseval micro precision for structure, nuclearity and relation prediction on RST-DT corpus. Scores for previous approaches are from either Morey et al. (2017) or the original papers. embedding of the EDU that this Queue element represents. vS1 and vS2 are computed as the means of the first and the last word embeddings of the nuclear EDU of S1 and S2 , as each non-leaf node in an RST structure encodes a relation between nuclear EDUs of its children (Morey et al., 2018). SpanBERT-CorefFeats: with this architecture variant, we attempt to assess the benefit of coreference features generated by the coreference resolver for RST parsing. Given a document with n words, the coreference features will be used to update the initial (not contextualized) SpanBER"
2020.codi-1.17,J18-2001,0,0.218777,".3 72.4 73.1 74.6 75.8 ± 0.2 76.1 ± 0.6 75.9 ± 0.6 77.3 Relation 54.6 57.8 55.8 59.7 60.2 60.0 63.4 ± 0.3 63.6 ± 0.3 63.3 ± 0.7 65.4 Table 1: RST-Parseval micro precision for structure, nuclearity and relation prediction on RST-DT corpus. Scores for previous approaches are from either Morey et al. (2017) or the original papers. embedding of the EDU that this Queue element represents. vS1 and vS2 are computed as the means of the first and the last word embeddings of the nuclear EDU of S1 and S2 , as each non-leaf node in an RST structure encodes a relation between nuclear EDUs of its children (Morey et al., 2018). SpanBERT-CorefFeats: with this architecture variant, we attempt to assess the benefit of coreference features generated by the coreference resolver for RST parsing. Given a document with n words, the coreference features will be used to update the initial (not contextualized) SpanBERT word embeddings w1:n , which will later be passed to SpanBERT. Specifically, for a given document we apply the pre-trained coreference parser of Joshi et al. (2020) to extract the document’s coreference clusters C1 , C2 , ..., each of which are equivalence classes representing different mentions of the same ent"
2020.codi-1.17,W17-5535,1,0.806415,"of the tree. Adjacent EDUs and constituents are hierarchically aggregated to form (possibly non-binary) constituents, with internal nodes containing (1) a nuclearity label, defining the importance of that subtree (rooted at the internal node) in the local context and (2) a relation label, defining the type of semantic connection between the two subtrees (e.g., Elaboration, Background). Previous research has shown that the use of RSTstyle discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015; Gerani et al., 2014; Ji and Smith, 2017). And more recently, it has been found that RST discourse structures can complement learned contextual embeddings (e.g., BERT (Devlin et al., 2018)), in tasks where linguistic information on complete documents is critical, such as argumentation analysis (Chakrabarty et al., 2019). In this work, we present preliminary results of investigating the benefits of coreference resolution features for RST discourse parsing. From the theoretical perspective, it has long been established (Asher and Lascarides, 2003) that discourse structur"
2020.codi-1.17,N18-1202,0,0.0233171,"ar bottom-up shiftreduce method, adopted from syntactic parsing. Wang et al. (2017) uses hand-crafted features and the shift-reduce method predicted by two separate Support-Vector-Machines (SVMs) for structureand nuclearity-prediction and relation-estimation. The neural model by Yu et al. (2018) uses a similar topology, but instead relies entirely on LSTMs for automatic feature extraction and on a single multilayer-perceptron (MLP) for classifying all possible actions. Top-down approaches to discourse parsing are also quite promising, with recent work of Kobayashi et al. (2020) applying ELMO (Peters et al., 2018) for computing span representations and achieving the new absolute SOTA performance, reporting however the scores of an ensemble of five independent runs of their proposed model instead of single-model results. In this work we follow the shift-reduce strategy and apply SpanBERT-Base (Joshi et al., 2020; Wolf et al., 2020), which we introduce below, for encoding the document contents. The field of coreference resolution has recently been dominated by deep learning models. The current SOTA model by Joshi et al. (2020) is built upon the neural coreference resolver of (Lee et al., 2018) by incorpo"
2020.codi-1.17,W17-3603,0,0.116341,"ny proficient English speaker would call such a discourse ill-formed and incoherent, due to the fact that it breaks the discourse-imposed antecedent scope. In general, anaphora can only be resolved with respect to the most salient (sentence 1 in Figure 1) units of the preceding discourse (Asher and Lascarides, 2003). For our purposes, this means that having access to a document’s coreference structure might be beneficial to the task of predicting the discourse structure, since the coreference structure can constrain the discourse parser’s solution space. However, as shown in a corpus study by Zeldes (2017), the antecedent boundaries defined by Veins Theory are often too restrictive, suggesting that while discourse structures can be useful for predicting coreference structures and vice versa, these mutual constrains must be defined softly, at least in the context of RST theory. To explore these ideas computationally with respect to modern neural models, we investigate the utility of automatically extracted coreference features and discourse-coreference shared representations in the context and for the benefit of neural RST discourse parsing. Our strong baseline SpanBERT-NoCoref utilizes SpanBERT"
2020.coling-main.16,Q18-1002,0,0.21559,"20) in section 3.1. For phase two, we focus on our novel sentiment predictor in section 3.2. The inference phase is straightforward and will be limited to the description in Figure 2 (c) for brevity. 3.1 Sentiment Inspired Discourse Trees The approach to generate “silver-standard” partial discourse trees (incorporating structure and nuclearity) from distant sentiment supervision (Huber and Carenini, 2019; Huber and Carenini, 2020) comprises two major components. First, documents are annotated for sentiment and importance at the EDU-level using a neural Multiple-Instance Learning (MIL) method (Angelidis and Lapata, 2018), solely utilizing documentlevel supervision signals given in the original corpus. In particular, MIL infers a sentiment polarity label px within the interval of [−1, 1] for each EDU x, depending on the distribution of words/EDUs within and between documents. Using the neural model by Angelidis and Lapata (2018), an additional attention mechanism is internally used to weight the importance of EDUs for the overall document sentiment. The attention-weight ax in the interval [0, 1] of EDU x is also extracted from the model and subsequently used as an importance score when aggregating sub-trees. N"
2020.coling-main.16,D15-1263,0,0.161564,"ion on the task of sentiment analysis. We therefore decide to build our framework on the HAN model (Yang et al., 2016), which is the most established, yet recent approach in the field, previously re-implemented and tested in many studies. We inject discourse information using TreeLSTMs (Tai et al., 2015), which are also well-established compared to tree-transformers, for which architectural variants and results are still preliminary (e.g. Shiv and Quirk (2019), Nguyen et al. (2020)). (3) Combining Discourse Parsing and Sentiment Analysis has been previously explored in multiple lines of work (Bhatia et al., 2015; Hogenboom et al., 2015; Nejat et al., 2017; Ji and Smith, 2017). Architecture-wise, the most closely related approach to our new model has been proposed by Ji and Smith (2017), where discourse trees generated by the DPLP parser (Ji and Eisenstein, 2014) trained on RST-DT are used in a recursive neural network to predict sentiment for multiple corpora. In their evaluation, the authors show slight improvements compared to the sequential HAN model. These initial positive results are a key motivation for our work, in which we aim to further improve the performance, especially on long documents,"
2020.coling-main.16,P16-1139,0,0.0466197,"documents that examine positive and negative aspects of a subject matter in complex rhetorical structures, like the ones shown in Figure 1. More specifically, in this work, we generate complete and hierarchical RST-style discourse trees (Mann and Thompson, 1988) with leaf nodes representing clause-like document fragments, called elementary discourse units (EDUs) and internal tree nodes labelled with a nuclearity assignment (Nucleus, Satellite), encoding the importance of a node in its local subtree1 . To incorporate these RST-style discourse structures, we employ a hybrid approach inspired by Bowman et al. (2016) and Choi et al. (2018), integrating a TreeLSTM (Tai et al., 2015) with the well-established Hierarchical Attention Network model (HAN) (Yang et al., 2016). From Ji and Smith (2017), we further adopt a non-competitive tree attention mechanism that is shown to be more appropriate in this context2 . This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 Discourse relation are not considered in this work. 2 We did not apply tree-transformers to the task, as in spite of recent proposals (e.g. Shiv and Q"
2020.coling-main.16,C14-1008,0,0.049301,"analysis component (Liu et al., 2019a). While early “bag-of-word” sentiment prediction models (Taboada et al., 2011) and their extensions (Wilson et al., 2009) already show promising results on the task, they all share one inherit limitation: Due to the absence of temporal information, they are not able to fully capture the semantics (and therefore the sentiment) of long texts, where different meanings oftentimes directly emerge from the word order, underlying syntax and discourse structures. Recent models for sentiment analysis address this limitation by leveraging sequential paradigms (Dos Santos and Gatti, 2014; Kim, 2014; Tai et al., 2015; Adhikari et al., 2019b), simple hierarchical information (Yang et al., 2016), complex syntactic structures on sentence level (Socher et al., 2013) or discourse structures of multi-sentential text (Ji and Smith, 2017). This paper follows the last line of aforementioned research, by developing a framework to exploit automatically generated, large-scale, domain-related discourse structures for sentiment prediction. Arguably, such framework can be especially beneficial for long documents that examine positive and negative aspects of a subject matter in complex rhetor"
2020.coling-main.16,D14-1168,1,0.871382,"gnificantly improved when using discourse trees generated by distant supervision on sentiment, compared to the traditionally acquired RST-DT discourse corpus. Using an additional ensemble method, we can further improve the performance and, even if only by a small margin, significantly outperform individual models. 2 Related Work This work is located at the intersection of recent approaches on discourse parsing and sentiment analysis and mostly influenced by four lines of research: (1) RST-style Discourse Parsing is a valuable upstream task for many downstream models (e.g. Ji and Smith (2017), Gerani et al. (2014)). Different approaches either separate discourse parsing “vertically” into sub-tasks on sentence-level, paragraph-level and document-level (Joty et al., 2015; Ji and Eisenstein, 2014), or “horizontally”, separating the prediction of structure and nuclearity from the relation computation (Wang et al., 2017). Furthermore, approaches have been explored to aggregate documents bottom-up using CKY (Joty et al., 2015) or employing local shift-reduce strategies, predicting the tree-structure through a sequence of actions based on linguistic features (Ji and Eisenstein, 2014; Subba and Di Eugenio, 200"
2020.coling-main.16,2020.coling-main.337,1,0.708179,"a simple threshold, based on the document length, improves the overall performance, showing statistically significant results. We compare our newly developed model with the well-established HAN model. In future work, we plan to compare the standard DocBERT model (Adhikari et al., 2019a) and discourse-inspired versions of it, to further solidify the findings in this work. We also plan to generate other large-scale datasets according to Huber and Carenini (2020) and evaluate our model on further “silver-standard” discourse treebanks. Using a neural discourse parser, such as Yu et al. (2018) or Guz et al. (2020) to train on MEGA-DT is another extension of this work. Besides the task of sentiment analysis, extractive summarization has recently been shown to align well with discourse structures in a transformer framework (Xiao et al., 2020), giving rise to potential improvements using the DAH model on this task. As another extension, we intend to look into more sophisticated ways to ensemble the sequential- and discourse tree-based models. Acknowledgments We thank the anonymous reviewers and the UBC-NLP group for their insightful comments and suggestions. This research was supported by the Language & S"
2020.coling-main.16,W16-3616,0,0.0491265,"Missing"
2020.coling-main.16,D19-1235,1,0.505811,"]9 , [It was so clean, nice, and new -]10 , [TV ’s on every cardio machine.]11 , [When i came back to this location,]12 , [I felt bad.]13 Due to space limitations a larger version as well as the corresponding full text for the right example is shown in Appendix A. Aiming to enhance the task of sentiment analysis by using discourse, it seems intuitive to employ domain-related discourse structures. Therefore, instead of using the standard RST-DT discourse treebank in the news domain (Carlson et al., 2002), we decide to infer discourse structures automatically learned from sentiment annotations (Huber and Carenini, 2019) on our discourse-augmented Yelp’13 treebank called MEGA-DT (Huber and Carenini, 2020). This way, our framework goes from sentiment to sentiment, in the sense that the discourse structures used to improve the sentiment predictions are generated through distant supervision from sentiment itself. Our hypothesis is that a parser trained on a large “silver-standard” discourse treebank automatically generated from sentiment will generate more useful discourse trees for sentiment prediction than one trained on a small and generic treebank, even if such treebank is human-annotated for RST discourse s"
2020.coling-main.16,2020.emnlp-main.603,1,0.484269,"i came back to this location,]12 , [I felt bad.]13 Due to space limitations a larger version as well as the corresponding full text for the right example is shown in Appendix A. Aiming to enhance the task of sentiment analysis by using discourse, it seems intuitive to employ domain-related discourse structures. Therefore, instead of using the standard RST-DT discourse treebank in the news domain (Carlson et al., 2002), we decide to infer discourse structures automatically learned from sentiment annotations (Huber and Carenini, 2019) on our discourse-augmented Yelp’13 treebank called MEGA-DT (Huber and Carenini, 2020). This way, our framework goes from sentiment to sentiment, in the sense that the discourse structures used to improve the sentiment predictions are generated through distant supervision from sentiment itself. Our hypothesis is that a parser trained on a large “silver-standard” discourse treebank automatically generated from sentiment will generate more useful discourse trees for sentiment prediction than one trained on a small and generic treebank, even if such treebank is human-annotated for RST discourse structures. In a series of experiments we show that while our novel approach to discour"
2020.coling-main.16,P14-1002,0,0.400318,"nsemble method, we can further improve the performance and, even if only by a small margin, significantly outperform individual models. 2 Related Work This work is located at the intersection of recent approaches on discourse parsing and sentiment analysis and mostly influenced by four lines of research: (1) RST-style Discourse Parsing is a valuable upstream task for many downstream models (e.g. Ji and Smith (2017), Gerani et al. (2014)). Different approaches either separate discourse parsing “vertically” into sub-tasks on sentence-level, paragraph-level and document-level (Joty et al., 2015; Ji and Eisenstein, 2014), or “horizontally”, separating the prediction of structure and nuclearity from the relation computation (Wang et al., 2017). Furthermore, approaches have been explored to aggregate documents bottom-up using CKY (Joty et al., 2015) or employing local shift-reduce strategies, predicting the tree-structure through a sequence of actions based on linguistic features (Ji and Eisenstein, 2014; Subba and Di Eugenio, 2009; Wang et al., 2017) or dense representations (Yu et al., 2018). Empirically, Wang et al. (2017) show that the combination of horizontal separation with a shift-reduce parsing framewo"
2020.coling-main.16,P17-1092,0,0.247013,"the absence of temporal information, they are not able to fully capture the semantics (and therefore the sentiment) of long texts, where different meanings oftentimes directly emerge from the word order, underlying syntax and discourse structures. Recent models for sentiment analysis address this limitation by leveraging sequential paradigms (Dos Santos and Gatti, 2014; Kim, 2014; Tai et al., 2015; Adhikari et al., 2019b), simple hierarchical information (Yang et al., 2016), complex syntactic structures on sentence level (Socher et al., 2013) or discourse structures of multi-sentential text (Ji and Smith, 2017). This paper follows the last line of aforementioned research, by developing a framework to exploit automatically generated, large-scale, domain-related discourse structures for sentiment prediction. Arguably, such framework can be especially beneficial for long documents that examine positive and negative aspects of a subject matter in complex rhetorical structures, like the ones shown in Figure 1. More specifically, in this work, we generate complete and hierarchical RST-style discourse trees (Mann and Thompson, 1988) with leaf nodes representing clause-like document fragments, called elemen"
2020.coling-main.16,J15-3002,1,0.893221,"ing an additional ensemble method, we can further improve the performance and, even if only by a small margin, significantly outperform individual models. 2 Related Work This work is located at the intersection of recent approaches on discourse parsing and sentiment analysis and mostly influenced by four lines of research: (1) RST-style Discourse Parsing is a valuable upstream task for many downstream models (e.g. Ji and Smith (2017), Gerani et al. (2014)). Different approaches either separate discourse parsing “vertically” into sub-tasks on sentence-level, paragraph-level and document-level (Joty et al., 2015; Ji and Eisenstein, 2014), or “horizontally”, separating the prediction of structure and nuclearity from the relation computation (Wang et al., 2017). Furthermore, approaches have been explored to aggregate documents bottom-up using CKY (Joty et al., 2015) or employing local shift-reduce strategies, predicting the tree-structure through a sequence of actions based on linguistic features (Ji and Eisenstein, 2014; Subba and Di Eugenio, 2009; Wang et al., 2017) or dense representations (Yu et al., 2018). Empirically, Wang et al. (2017) show that the combination of horizontal separation with a sh"
2020.coling-main.16,N19-1347,0,0.0147199,"results are a key motivation for our work, in which we aim to further improve the performance, especially on long documents, by not only training the discourse parser on a larger and more appropriate treebank (i.e. MEGA-DT), but also by improving the sentiment prediction, replacing recursive neural networks with superior TreeLSTMs, tightly integrated with HAN. (4) (Discourse) Tree Learning tries to automatically infer discourse trees from large amounts of data. In popular approaches, trees are inferred directly while learning a neural model for a downstream task, such as text classification (Karimi and Tang, 2019) or extractive summarization (Liu et al., 2019b). Along this line of research, we previously proposed a similar objective in Huber and Carenini (2019), automatically generating discourse trees from distant supervision of a downstream task (sentiment analysis). However, we employed a rather different approach. Instead of trying to induce discourse trees directly during training of a neural network, we propose a dedicated system, comprising of well-established methods, to directly generate discourse trees. With the resulting large-scale, sentiment influenced discourse treebank called MEGA-DT, we"
2020.coling-main.16,D14-1181,0,0.0449486,"et al., 2019a). While early “bag-of-word” sentiment prediction models (Taboada et al., 2011) and their extensions (Wilson et al., 2009) already show promising results on the task, they all share one inherit limitation: Due to the absence of temporal information, they are not able to fully capture the semantics (and therefore the sentiment) of long texts, where different meanings oftentimes directly emerge from the word order, underlying syntax and discourse structures. Recent models for sentiment analysis address this limitation by leveraging sequential paradigms (Dos Santos and Gatti, 2014; Kim, 2014; Tai et al., 2015; Adhikari et al., 2019b), simple hierarchical information (Yang et al., 2016), complex syntactic structures on sentence level (Socher et al., 2013) or discourse structures of multi-sentential text (Ji and Smith, 2017). This paper follows the last line of aforementioned research, by developing a framework to exploit automatically generated, large-scale, domain-related discourse structures for sentiment prediction. Arguably, such framework can be especially beneficial for long documents that examine positive and negative aspects of a subject matter in complex rhetorical struct"
2020.coling-main.16,N19-1173,0,0.0984327,"erformance for long documents, even beyond previous approaches using well-established discourse parsers trained on human annotated data. We show that a simple ensemble approach can further enhance performance by selectively using discourse, depending on the document length. 1 Introduction Predicting whether a given word, sentence or document expresses a positive, neutral or negative sentiment is a fundamental task in Natural Language Processing (NLP). For instance, a recent survey of text mining papers from 1992-2017 has found that out of 4, 346 papers, 467 had a sentiment analysis component (Liu et al., 2019a). While early “bag-of-word” sentiment prediction models (Taboada et al., 2011) and their extensions (Wilson et al., 2009) already show promising results on the task, they all share one inherit limitation: Due to the absence of temporal information, they are not able to fully capture the semantics (and therefore the sentiment) of long texts, where different meanings oftentimes directly emerge from the word order, underlying syntax and discourse structures. Recent models for sentiment analysis address this limitation by leveraging sequential paradigms (Dos Santos and Gatti, 2014; Kim, 2014; Ta"
2020.coling-main.16,J18-2001,0,0.013702,"re influential in the computation of the final document representation, as motivated by the examples in Figure 1. There are two crucial decisions on how to incorporate the discourse-guided tree aggregation: (1) The tree representation. Although discourse parsing typically processes constituency tree-structures, most successful downstream applications of discourse parsing benefit from dependency discourse trees (e.g., Marcu (2000), Ji and Smith (2017), Shiv and Quirk (2019)). Even though both tree representations are conveying the same information and near-isomorphic conversions are available (Morey et al., 2018), we believe that this is because of the different role that nuclearity plays in the tree-representations. In particular, while in constituency trees nuclearity is an attribute of internal tree-nodes, head-dependent relations in the dependency tree are fundamentally shaped by the nuclearity attribution. This more explicit representation of nuclearity can benefit downstream applications. For this reason, we are converting the RST constituency trees into dependency representations (see left of Figure 4). (2) The aggregation approach has a significant impact on the performance of the model. In th"
2020.coling-main.16,W17-5535,1,0.832426,"refore decide to build our framework on the HAN model (Yang et al., 2016), which is the most established, yet recent approach in the field, previously re-implemented and tested in many studies. We inject discourse information using TreeLSTMs (Tai et al., 2015), which are also well-established compared to tree-transformers, for which architectural variants and results are still preliminary (e.g. Shiv and Quirk (2019), Nguyen et al. (2020)). (3) Combining Discourse Parsing and Sentiment Analysis has been previously explored in multiple lines of work (Bhatia et al., 2015; Hogenboom et al., 2015; Nejat et al., 2017; Ji and Smith, 2017). Architecture-wise, the most closely related approach to our new model has been proposed by Ji and Smith (2017), where discourse trees generated by the DPLP parser (Ji and Eisenstein, 2014) trained on RST-DT are used in a recursive neural network to predict sentiment for multiple corpora. In their evaluation, the authors show slight improvements compared to the sequential HAN model. These initial positive results are a key motivation for our work, in which we aim to further improve the performance, especially on long documents, by not only training the discourse parser on"
2020.coling-main.16,D14-1162,0,0.0824484,"Missing"
2020.coling-main.16,D13-1170,0,0.00995484,"sing results on the task, they all share one inherit limitation: Due to the absence of temporal information, they are not able to fully capture the semantics (and therefore the sentiment) of long texts, where different meanings oftentimes directly emerge from the word order, underlying syntax and discourse structures. Recent models for sentiment analysis address this limitation by leveraging sequential paradigms (Dos Santos and Gatti, 2014; Kim, 2014; Tai et al., 2015; Adhikari et al., 2019b), simple hierarchical information (Yang et al., 2016), complex syntactic structures on sentence level (Socher et al., 2013) or discourse structures of multi-sentential text (Ji and Smith, 2017). This paper follows the last line of aforementioned research, by developing a framework to exploit automatically generated, large-scale, domain-related discourse structures for sentiment prediction. Arguably, such framework can be especially beneficial for long documents that examine positive and negative aspects of a subject matter in complex rhetorical structures, like the ones shown in Figure 1. More specifically, in this work, we generate complete and hierarchical RST-style discourse trees (Mann and Thompson, 1988) with"
2020.coling-main.16,N09-1064,0,0.650065,"Missing"
2020.coling-main.16,J11-2001,0,0.0388432,"stablished discourse parsers trained on human annotated data. We show that a simple ensemble approach can further enhance performance by selectively using discourse, depending on the document length. 1 Introduction Predicting whether a given word, sentence or document expresses a positive, neutral or negative sentiment is a fundamental task in Natural Language Processing (NLP). For instance, a recent survey of text mining papers from 1992-2017 has found that out of 4, 346 papers, 467 had a sentiment analysis component (Liu et al., 2019a). While early “bag-of-word” sentiment prediction models (Taboada et al., 2011) and their extensions (Wilson et al., 2009) already show promising results on the task, they all share one inherit limitation: Due to the absence of temporal information, they are not able to fully capture the semantics (and therefore the sentiment) of long texts, where different meanings oftentimes directly emerge from the word order, underlying syntax and discourse structures. Recent models for sentiment analysis address this limitation by leveraging sequential paradigms (Dos Santos and Gatti, 2014; Kim, 2014; Tai et al., 2015; Adhikari et al., 2019b), simple hierarchical information (Yang e"
2020.coling-main.16,P15-1150,0,0.771396,"19a). While early “bag-of-word” sentiment prediction models (Taboada et al., 2011) and their extensions (Wilson et al., 2009) already show promising results on the task, they all share one inherit limitation: Due to the absence of temporal information, they are not able to fully capture the semantics (and therefore the sentiment) of long texts, where different meanings oftentimes directly emerge from the word order, underlying syntax and discourse structures. Recent models for sentiment analysis address this limitation by leveraging sequential paradigms (Dos Santos and Gatti, 2014; Kim, 2014; Tai et al., 2015; Adhikari et al., 2019b), simple hierarchical information (Yang et al., 2016), complex syntactic structures on sentence level (Socher et al., 2013) or discourse structures of multi-sentential text (Ji and Smith, 2017). This paper follows the last line of aforementioned research, by developing a framework to exploit automatically generated, large-scale, domain-related discourse structures for sentiment prediction. Arguably, such framework can be especially beneficial for long documents that examine positive and negative aspects of a subject matter in complex rhetorical structures, like the one"
2020.coling-main.16,D15-1167,0,0.365165,"Missing"
2020.coling-main.16,P17-2029,0,0.334385,"dels. 2 Related Work This work is located at the intersection of recent approaches on discourse parsing and sentiment analysis and mostly influenced by four lines of research: (1) RST-style Discourse Parsing is a valuable upstream task for many downstream models (e.g. Ji and Smith (2017), Gerani et al. (2014)). Different approaches either separate discourse parsing “vertically” into sub-tasks on sentence-level, paragraph-level and document-level (Joty et al., 2015; Ji and Eisenstein, 2014), or “horizontally”, separating the prediction of structure and nuclearity from the relation computation (Wang et al., 2017). Furthermore, approaches have been explored to aggregate documents bottom-up using CKY (Joty et al., 2015) or employing local shift-reduce strategies, predicting the tree-structure through a sequence of actions based on linguistic features (Ji and Eisenstein, 2014; Subba and Di Eugenio, 2009; Wang et al., 2017) or dense representations (Yu et al., 2018). Empirically, Wang et al. (2017) show that the combination of horizontal separation with a shift-reduce parsing framework achieves 186 competitive performance, reaching state-of-the-art results on the structure-prediction task. In this work, w"
2020.coling-main.16,J09-3003,0,0.0235934,"n annotated data. We show that a simple ensemble approach can further enhance performance by selectively using discourse, depending on the document length. 1 Introduction Predicting whether a given word, sentence or document expresses a positive, neutral or negative sentiment is a fundamental task in Natural Language Processing (NLP). For instance, a recent survey of text mining papers from 1992-2017 has found that out of 4, 346 papers, 467 had a sentiment analysis component (Liu et al., 2019a). While early “bag-of-word” sentiment prediction models (Taboada et al., 2011) and their extensions (Wilson et al., 2009) already show promising results on the task, they all share one inherit limitation: Due to the absence of temporal information, they are not able to fully capture the semantics (and therefore the sentiment) of long texts, where different meanings oftentimes directly emerge from the word order, underlying syntax and discourse structures. Recent models for sentiment analysis address this limitation by leveraging sequential paradigms (Dos Santos and Gatti, 2014; Kim, 2014; Tai et al., 2015; Adhikari et al., 2019b), simple hierarchical information (Yang et al., 2016), complex syntactic structures"
2020.coling-main.16,2020.codi-1.13,1,0.743554,"Missing"
2020.coling-main.16,N16-1174,0,0.818625,"2011) and their extensions (Wilson et al., 2009) already show promising results on the task, they all share one inherit limitation: Due to the absence of temporal information, they are not able to fully capture the semantics (and therefore the sentiment) of long texts, where different meanings oftentimes directly emerge from the word order, underlying syntax and discourse structures. Recent models for sentiment analysis address this limitation by leveraging sequential paradigms (Dos Santos and Gatti, 2014; Kim, 2014; Tai et al., 2015; Adhikari et al., 2019b), simple hierarchical information (Yang et al., 2016), complex syntactic structures on sentence level (Socher et al., 2013) or discourse structures of multi-sentential text (Ji and Smith, 2017). This paper follows the last line of aforementioned research, by developing a framework to exploit automatically generated, large-scale, domain-related discourse structures for sentiment prediction. Arguably, such framework can be especially beneficial for long documents that examine positive and negative aspects of a subject matter in complex rhetorical structures, like the ones shown in Figure 1. More specifically, in this work, we generate complete and"
2020.coling-main.16,C18-1047,0,0.0887408,"e parsing “vertically” into sub-tasks on sentence-level, paragraph-level and document-level (Joty et al., 2015; Ji and Eisenstein, 2014), or “horizontally”, separating the prediction of structure and nuclearity from the relation computation (Wang et al., 2017). Furthermore, approaches have been explored to aggregate documents bottom-up using CKY (Joty et al., 2015) or employing local shift-reduce strategies, predicting the tree-structure through a sequence of actions based on linguistic features (Ji and Eisenstein, 2014; Subba and Di Eugenio, 2009; Wang et al., 2017) or dense representations (Yu et al., 2018). Empirically, Wang et al. (2017) show that the combination of horizontal separation with a shift-reduce parsing framework achieves 186 competitive performance, reaching state-of-the-art results on the structure-prediction task. In this work, we demonstrate the potential of this discourse parser trained on a large-scale sentiment-dependent treebank (MEGA-DT) to generate discourse trees for sentiment prediction, enhancing the performance on long and diverse documents. (2) Neural Sentiment Analysis is a common sub-task in many real world systems with Kim (2014) being the first to show the effect"
2020.coling-main.337,D15-1263,0,0.115286,"arger (possibly non-binary) constituents, with internal nodes containing (1) a nuclearity label, defining the importance of the subtree (rooted at the internal node) in the local context and (2) a relation label, defining the type of semantic connection between the two subtrees (e.g., Elaboration, Background). In this work, we focus on structure and nuclearity prediction, not taking relations into account. Previous research has shown that the use of RST-style discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015; Gerani et al., 2014; Ji and Smith, 2017). More recently, it has also been suggested that discourse structures obtained in an RST-style manner can further be complementary to learned contextual embeddings, like the popular BERT approach (Devlin et al., 2018). Combining both approaches has shown to support tasks where linguistic information on complete documents is critical, such as argumentation analysis (Chakrabarty et al., 2019). Even though discourse parsers appear to enhance the performance on a variety of tasks, the full potential of using more"
2020.coling-main.337,E17-1028,0,0.349712,"more severe, as data-driven approaches cannot be applied to their full potential. The combination of these two limitations has been one of the main reasons for the limited application of neural discourse parsing for more diverse downstream tasks. While there have been neural discourse This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3794 Proceedings of the 28th International Conference on Computational Linguistics, pages 3794–3805 Barcelona, Spain (Online), December 8-13, 2020 parsers proposed (Braud et al., 2017; Yu et al., 2018; Mabona et al., 2019), they still cannot consistently outperform traditional approaches when applied to the RST-DT dataset, where the amount of training data is arguably insufficient for such data-intensive approaches. In this work, we alleviate the restrictions to the effective and efficient use of discourse as mentioned above by introducing a novel approach combining a newly proposed large-scale discourse treebank with our data-driven neural discourse parsing strategy. More specifically, we employ the novel MEGA-DT “silver-standard” discourse treebank published by Huber and"
2020.coling-main.337,D19-1291,0,0.0220075,"parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015; Gerani et al., 2014; Ji and Smith, 2017). More recently, it has also been suggested that discourse structures obtained in an RST-style manner can further be complementary to learned contextual embeddings, like the popular BERT approach (Devlin et al., 2018). Combining both approaches has shown to support tasks where linguistic information on complete documents is critical, such as argumentation analysis (Chakrabarty et al., 2019). Even though discourse parsers appear to enhance the performance on a variety of tasks, the full potential of using more linguistically inspired approaches for downstream applications has not been unleashed yet. The main open challenges of integrating discourse into more NLP downstream tasks and to deliver even greater benefits have been a combination of (1) discourse parsing being a difficult task itself, with an inherently high degree of ambiguity and uncertainty and (2) the lack of large-scale annotated datasets, rendering the initial problem more severe, as data-driven approaches cannot b"
2020.coling-main.337,P14-1048,0,0.0279408,"the original Parseval measure as proposed in Morey et al. (2017), evaluated on the RST-DT and Instr-DT corpora. Best performance per column is bold. (subscripts on results indicate standard deviation, — non-published values) The DPLP parser (Ji and Eisenstein, 2014) is a traditional discourse parser utilizing an SVM-classifier within the shift-reduce framework solely based on linear projections of lexical features. The CODRA model (Joty et al., 2015) uses an optimal CKY-based chart parser in combination with Dynamic Conditional Random Fields (CRF), separated on sentence-level. The gCRF model (Feng and Hirst, 2014) follows a similar approach but utilizes a greedy strategy. The Two-Stage parser proposed by Wang et al. (2017) is the current SOTA system on the RST-DT structure prediction. The model uses two separate linear SVM classifiers. We use the public codebase3 provided by Wang et al. (2017) and remove the relation classification module for our experiments. Transition-Syntax: the parser by Yu et al. (2018) is a neural shift-reduce parser utilizing LSTMs to generate EDU embeddings. In addition, they apply a neural dependency parser for extracting syntactic features. Cross-Lingual is the neural shift-r"
2020.coling-main.337,P19-1062,0,0.0172599,", trying to generate large-scale discourse treebanks through automated annotations from downstream tasks, such as sentiment analysis (Huber and Carenini, 2020), text classification (Liu and Lapata, 2018), summarization (Liu et al., 2019a) and fake news detection (Karimi and Tang, 2019). The majority of these approaches follows the intuition that discourse trees can be inferred from downstream tasks by predicting latent representations during the learning process of the task itself (Liu and Lapata, 2018; Karimi and Tang, 2019; Liu et al., 2019a) in an end-to-end manner. However, recent work by Ferracane et al. (2019) has shown that the trees resulting from this approach are not only poorly aligned with general discourse structures, but furthermore are oftentimes too shallow. A rather different strategy has been employed by Huber and Carenini (2020), trying to explicitly generate a discourse augmented treebank through distant supervision from sentiment annotations in combination with Multiple-Instance Learning (MIL) and a CKY bottom-up tree generation approach. While the resulting MEGA-DT dataset has only been proposed and released recently, the authors show promising results in their work, reaching the be"
2020.coling-main.337,D14-1168,1,0.852353,"taining (1) a nuclearity label, defining the importance of the subtree (rooted at the internal node) in the local context and (2) a relation label, defining the type of semantic connection between the two subtrees (e.g., Elaboration, Background). In this work, we focus on structure and nuclearity prediction, not taking relations into account. Previous research has shown that the use of RST-style discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015; Gerani et al., 2014; Ji and Smith, 2017). More recently, it has also been suggested that discourse structures obtained in an RST-style manner can further be complementary to learned contextual embeddings, like the popular BERT approach (Devlin et al., 2018). Combining both approaches has shown to support tasks where linguistic information on complete documents is critical, such as argumentation analysis (Chakrabarty et al., 2019). Even though discourse parsers appear to enhance the performance on a variety of tasks, the full potential of using more linguistically inspired approaches for downstream applications h"
2020.coling-main.337,2020.emnlp-main.603,1,0.664676,"al., 2017; Yu et al., 2018; Mabona et al., 2019), they still cannot consistently outperform traditional approaches when applied to the RST-DT dataset, where the amount of training data is arguably insufficient for such data-intensive approaches. In this work, we alleviate the restrictions to the effective and efficient use of discourse as mentioned above by introducing a novel approach combining a newly proposed large-scale discourse treebank with our data-driven neural discourse parsing strategy. More specifically, we employ the novel MEGA-DT “silver-standard” discourse treebank published by Huber and Carenini (2020) containing over 250,000 discourse annotated documents from the Yelp’13 sentiment dataset (Tang et al., 2015), nearly three orders of magnitude larger than commonly used RST-style annotated discourse treebanks (RST-DT (Carlson et al., 2002), Instructional-DT (Subba and Di Eugenio, 2009)). Given this new dataset with a previously unseen number of full RST-style discourse trees, we revisit the task of neural discourse parsing, which has been previously attempted by Yu et al. (2018) and others with rather limited success. We believe that one reason why previous neural models could not yet consist"
2020.coling-main.337,P14-1002,0,0.70239,"only been proposed and released recently, the authors show promising results in their work, reaching the best inter-domain performance when comparing their dataset against RST-DT and the Instruction-DT. This leads us to believe that their treebank does not only learn sentiment-related information, but can also be used to infer general discourse structures on a large scale. We will further evaluate this in section 4.7. 3 Neural Discourse Parsing We follow the well-established bottom-up shift-reduce aggregation principle, as previously shown effective for traditional discourse parsers such as (Ji and Eisenstein, 2014; Wang et al., 2017) as well as neural approaches (Braud et al., 2017; Yu et al., 2018). In this section, we will first introduce the general principle of shift-reduce parsing and define the necessary data structures and actions available to our system. Based on the general description, we will subsequently describe the approach taken to execute a single step in the linear-time model. 3.1 General Shift-Reduce Architecture The transition-based shift-reduce parsing architecture traditionally consists of two data structures (a queue and a stack), interacting through a set of possible actions cate"
2020.coling-main.337,P17-1092,0,0.048806,"ity label, defining the importance of the subtree (rooted at the internal node) in the local context and (2) a relation label, defining the type of semantic connection between the two subtrees (e.g., Elaboration, Background). In this work, we focus on structure and nuclearity prediction, not taking relations into account. Previous research has shown that the use of RST-style discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015; Gerani et al., 2014; Ji and Smith, 2017). More recently, it has also been suggested that discourse structures obtained in an RST-style manner can further be complementary to learned contextual embeddings, like the popular BERT approach (Devlin et al., 2018). Combining both approaches has shown to support tasks where linguistic information on complete documents is critical, such as argumentation analysis (Chakrabarty et al., 2019). Even though discourse parsers appear to enhance the performance on a variety of tasks, the full potential of using more linguistically inspired approaches for downstream applications has not been unleashed"
2020.coling-main.337,J15-3002,1,0.925585,"2) or Instructional-DT (Subba and Di Eugenio, 2009): (1) Top-down discourse parsers, splitting the document into non-overlapping text-constituents starting from the representation of the complete discourse down to individual EDUs, assigning the two resulting sub-spans a nuclearity attribute and predicting the relation holding between the sub-trees (Lin et al., 2019). (2) Bottom-up parsing, starting from the discourse-segmented list of EDUs and aggregating two adjacent units in every step. This approach is mostly realized using the CKY dynamic programming strategy to obtain optimal trees as in Joty et al. (2015) and Li et al. (2016) or using a greedy method (Hernault et al., 2010). (3) A frequently used and more locally inspired approach of bottom-up discourse parsing using the linear shift-reduce framework, adopted from previous work in syntactic parsing. While the current traditional state-of-the-art discourse parser by Wang et al. (2017) uses the bottom-up shift-reduce method predicted by two separate Support Vector Machines (SVMs) for 3795 structure/nuclearity prediction and relation estimation, neural models (Yu et al., 2018; Braud et al., 2017) utilize multi-layer perceptrons (MLP) for classify"
2020.coling-main.337,N19-1347,0,0.244615,"18; Braud et al., 2017) utilize multi-layer perceptrons (MLP) for classifying possible actions. In our work we also follow the bottom-up shift-reduce strategy, with the detailed description of our system provided in the following section. Besides the active research area on discourse parsing, a second line of work has emerged recently, trying to generate large-scale discourse treebanks through automated annotations from downstream tasks, such as sentiment analysis (Huber and Carenini, 2020), text classification (Liu and Lapata, 2018), summarization (Liu et al., 2019a) and fake news detection (Karimi and Tang, 2019). The majority of these approaches follows the intuition that discourse trees can be inferred from downstream tasks by predicting latent representations during the learning process of the task itself (Liu and Lapata, 2018; Karimi and Tang, 2019; Liu et al., 2019a) in an end-to-end manner. However, recent work by Ferracane et al. (2019) has shown that the trees resulting from this approach are not only poorly aligned with general discourse structures, but furthermore are oftentimes too shallow. A rather different strategy has been employed by Huber and Carenini (2020), trying to explicitly gene"
2020.coling-main.337,D16-1035,0,0.018657,"(Subba and Di Eugenio, 2009): (1) Top-down discourse parsers, splitting the document into non-overlapping text-constituents starting from the representation of the complete discourse down to individual EDUs, assigning the two resulting sub-spans a nuclearity attribute and predicting the relation holding between the sub-trees (Lin et al., 2019). (2) Bottom-up parsing, starting from the discourse-segmented list of EDUs and aggregating two adjacent units in every step. This approach is mostly realized using the CKY dynamic programming strategy to obtain optimal trees as in Joty et al. (2015) and Li et al. (2016) or using a greedy method (Hernault et al., 2010). (3) A frequently used and more locally inspired approach of bottom-up discourse parsing using the linear shift-reduce framework, adopted from previous work in syntactic parsing. While the current traditional state-of-the-art discourse parser by Wang et al. (2017) uses the bottom-up shift-reduce method predicted by two separate Support Vector Machines (SVMs) for 3795 structure/nuclearity prediction and relation estimation, neural models (Yu et al., 2018; Braud et al., 2017) utilize multi-layer perceptrons (MLP) for classifying possible actions."
2020.coling-main.337,P19-1410,0,0.0122771,"ming initial attempts to apply deep learning and neural networks to the task. Independent of the specific approach used, three general methodologies have been followed to learn discourse trees from small datasets, such as RST-DT (Carlson et al., 2002) or Instructional-DT (Subba and Di Eugenio, 2009): (1) Top-down discourse parsers, splitting the document into non-overlapping text-constituents starting from the representation of the complete discourse down to individual EDUs, assigning the two resulting sub-spans a nuclearity attribute and predicting the relation holding between the sub-trees (Lin et al., 2019). (2) Bottom-up parsing, starting from the discourse-segmented list of EDUs and aggregating two adjacent units in every step. This approach is mostly realized using the CKY dynamic programming strategy to obtain optimal trees as in Joty et al. (2015) and Li et al. (2016) or using a greedy method (Hernault et al., 2010). (3) A frequently used and more locally inspired approach of bottom-up discourse parsing using the linear shift-reduce framework, adopted from previous work in syntactic parsing. While the current traditional state-of-the-art discourse parser by Wang et al. (2017) uses the botto"
2020.coling-main.337,Q18-1005,0,0.0192625,"ucture/nuclearity prediction and relation estimation, neural models (Yu et al., 2018; Braud et al., 2017) utilize multi-layer perceptrons (MLP) for classifying possible actions. In our work we also follow the bottom-up shift-reduce strategy, with the detailed description of our system provided in the following section. Besides the active research area on discourse parsing, a second line of work has emerged recently, trying to generate large-scale discourse treebanks through automated annotations from downstream tasks, such as sentiment analysis (Huber and Carenini, 2020), text classification (Liu and Lapata, 2018), summarization (Liu et al., 2019a) and fake news detection (Karimi and Tang, 2019). The majority of these approaches follows the intuition that discourse trees can be inferred from downstream tasks by predicting latent representations during the learning process of the task itself (Liu and Lapata, 2018; Karimi and Tang, 2019; Liu et al., 2019a) in an end-to-end manner. However, recent work by Ferracane et al. (2019) has shown that the trees resulting from this approach are not only poorly aligned with general discourse structures, but furthermore are oftentimes too shallow. A rather different"
2020.coling-main.337,N19-1173,0,0.409219,"ion estimation, neural models (Yu et al., 2018; Braud et al., 2017) utilize multi-layer perceptrons (MLP) for classifying possible actions. In our work we also follow the bottom-up shift-reduce strategy, with the detailed description of our system provided in the following section. Besides the active research area on discourse parsing, a second line of work has emerged recently, trying to generate large-scale discourse treebanks through automated annotations from downstream tasks, such as sentiment analysis (Huber and Carenini, 2020), text classification (Liu and Lapata, 2018), summarization (Liu et al., 2019a) and fake news detection (Karimi and Tang, 2019). The majority of these approaches follows the intuition that discourse trees can be inferred from downstream tasks by predicting latent representations during the learning process of the task itself (Liu and Lapata, 2018; Karimi and Tang, 2019; Liu et al., 2019a) in an end-to-end manner. However, recent work by Ferracane et al. (2019) has shown that the trees resulting from this approach are not only poorly aligned with general discourse structures, but furthermore are oftentimes too shallow. A rather different strategy has been employed by Hu"
2020.coling-main.337,D19-1233,0,0.219031,"s cannot be applied to their full potential. The combination of these two limitations has been one of the main reasons for the limited application of neural discourse parsing for more diverse downstream tasks. While there have been neural discourse This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3794 Proceedings of the 28th International Conference on Computational Linguistics, pages 3794–3805 Barcelona, Spain (Online), December 8-13, 2020 parsers proposed (Braud et al., 2017; Yu et al., 2018; Mabona et al., 2019), they still cannot consistently outperform traditional approaches when applied to the RST-DT dataset, where the amount of training data is arguably insufficient for such data-intensive approaches. In this work, we alleviate the restrictions to the effective and efficient use of discourse as mentioned above by introducing a novel approach combining a newly proposed large-scale discourse treebank with our data-driven neural discourse parsing strategy. More specifically, we employ the novel MEGA-DT “silver-standard” discourse treebank published by Huber and Carenini (2020) containing over 250,00"
2020.coling-main.337,D17-1136,0,0.163382,"Parser (- Features) Our Parser (- LM Pretraining) Human (2017) Structure RST-DT Instr-DT 64.10 — 68.60 — 65.10 — 62.70 — 70.97 58.86 67.10 — 72.43 64.55 ±0.27 ±1.12 ±0.95 72.94 ±0.61 65.41 ±0.18 59.89 ±0.30 54.68 70.61 ±0.72 ±1.74 63.32 ±0.42 65.78 ±2.97 53.50 78.7 — Nuclearity RST-DT Instr-DT 54.20 — 55.90 — 55.50 — 54.50 — 57.97 40.00 57.40 — 61.38 44.41 ±0.56 ±1.37 ±0.90 61.86 ±1.11 46.59 ±0.55 33.28 ±1.83 28.36 58.37 ±0.65 ±1.83 44.41 ±0.58 48.93 ±2.69 32.82 66.8 — Table 1: Micro-averaged F1-scores for structure and nuclearity prediction using the original Parseval measure as proposed in Morey et al. (2017), evaluated on the RST-DT and Instr-DT corpora. Best performance per column is bold. (subscripts on results indicate standard deviation, — non-published values) The DPLP parser (Ji and Eisenstein, 2014) is a traditional discourse parser utilizing an SVM-classifier within the shift-reduce framework solely based on linear projections of lexical features. The CODRA model (Joty et al., 2015) uses an optimal CKY-based chart parser in combination with Dynamic Conditional Random Fields (CRF), separated on sentence-level. The gCRF model (Feng and Hirst, 2014) follows a similar approach but utilizes a"
2020.coling-main.337,J18-2001,0,0.338723,"ng is an important upstream task within the area of Natural Language Processing (NLP) which has been an active field of research over the last decades. In this work, we focus on discourse representations for the English language, where most research has been surrounding one of the two main theories behind discourse, the Rhetorical Structure Theory (RST) proposed by Mann and Thompson (1988) or interpreting discourse according to PDTB (Prasad et al., 2008). While both theories have their strengths, the application of the RST theory, encoding documents into complete constituency discourse trees (Morey et al., 2018), has been shown to have many crucial implications on real world problems. A tree is defined on a set of EDUs (Elementary Discourse Units), approximately aligning with clause-like sentence fragments, acting as the leaves of the tree. Adjacent EDUs or sub-trees are hierarchically aggregated to form larger (possibly non-binary) constituents, with internal nodes containing (1) a nuclearity label, defining the importance of the subtree (rooted at the internal node) in the local context and (2) a relation label, defining the type of semantic connection between the two subtrees (e.g., Elaboration, B"
2020.coling-main.337,W17-5535,1,0.831228,"inary) constituents, with internal nodes containing (1) a nuclearity label, defining the importance of the subtree (rooted at the internal node) in the local context and (2) a relation label, defining the type of semantic connection between the two subtrees (e.g., Elaboration, Background). In this work, we focus on structure and nuclearity prediction, not taking relations into account. Previous research has shown that the use of RST-style discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015; Gerani et al., 2014; Ji and Smith, 2017). More recently, it has also been suggested that discourse structures obtained in an RST-style manner can further be complementary to learned contextual embeddings, like the popular BERT approach (Devlin et al., 2018). Combining both approaches has shown to support tasks where linguistic information on complete documents is critical, such as argumentation analysis (Chakrabarty et al., 2019). Even though discourse parsers appear to enhance the performance on a variety of tasks, the full potential of using more linguistically insp"
2020.coling-main.337,prasad-etal-2008-penn,0,0.0440026,"provides even larger performance benefits, suggesting a novel and promising research direction in the field of discourse analysis. 1 Introduction Discourse parsing is an important upstream task within the area of Natural Language Processing (NLP) which has been an active field of research over the last decades. In this work, we focus on discourse representations for the English language, where most research has been surrounding one of the two main theories behind discourse, the Rhetorical Structure Theory (RST) proposed by Mann and Thompson (1988) or interpreting discourse according to PDTB (Prasad et al., 2008). While both theories have their strengths, the application of the RST theory, encoding documents into complete constituency discourse trees (Morey et al., 2018), has been shown to have many crucial implications on real world problems. A tree is defined on a set of EDUs (Elementary Discourse Units), approximately aligning with clause-like sentence fragments, acting as the leaves of the tree. Adjacent EDUs or sub-trees are hierarchically aggregated to form larger (possibly non-binary) constituents, with internal nodes containing (1) a nuclearity label, defining the importance of the subtree (ro"
2020.coling-main.337,N09-1064,0,0.652652,"Missing"
2020.coling-main.337,D15-1167,0,0.188351,"hen applied to the RST-DT dataset, where the amount of training data is arguably insufficient for such data-intensive approaches. In this work, we alleviate the restrictions to the effective and efficient use of discourse as mentioned above by introducing a novel approach combining a newly proposed large-scale discourse treebank with our data-driven neural discourse parsing strategy. More specifically, we employ the novel MEGA-DT “silver-standard” discourse treebank published by Huber and Carenini (2020) containing over 250,000 discourse annotated documents from the Yelp’13 sentiment dataset (Tang et al., 2015), nearly three orders of magnitude larger than commonly used RST-style annotated discourse treebanks (RST-DT (Carlson et al., 2002), Instructional-DT (Subba and Di Eugenio, 2009)). Given this new dataset with a previously unseen number of full RST-style discourse trees, we revisit the task of neural discourse parsing, which has been previously attempted by Yu et al. (2018) and others with rather limited success. We believe that one reason why previous neural models could not yet consistently outperform more traditional approaches, heavily relying on feature engineering (Wang et al., 2017), is"
2020.coling-main.337,P17-2029,0,0.0944857,"set (Tang et al., 2015), nearly three orders of magnitude larger than commonly used RST-style annotated discourse treebanks (RST-DT (Carlson et al., 2002), Instructional-DT (Subba and Di Eugenio, 2009)). Given this new dataset with a previously unseen number of full RST-style discourse trees, we revisit the task of neural discourse parsing, which has been previously attempted by Yu et al. (2018) and others with rather limited success. We believe that one reason why previous neural models could not yet consistently outperform more traditional approaches, heavily relying on feature engineering (Wang et al., 2017), is the lack of generalisation when using deep learning approaches on the small RST-DT dataset, containing only 385 discourse annotated documents. This makes us believe that using a more advanced neural discourse parser in combination with a large training dataset can lead to significant performance gains. Admittedly, even though MEGADT contains a huge number of datapoints to train on, it has been automatically annotated, potentially introducing noise and biases, which can negatively influence the performance of our newly proposed neural discourse parser when solely trained on this dataset. A"
2020.coling-main.337,C18-1047,0,0.290777,"-driven approaches cannot be applied to their full potential. The combination of these two limitations has been one of the main reasons for the limited application of neural discourse parsing for more diverse downstream tasks. While there have been neural discourse This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3794 Proceedings of the 28th International Conference on Computational Linguistics, pages 3794–3805 Barcelona, Spain (Online), December 8-13, 2020 parsers proposed (Braud et al., 2017; Yu et al., 2018; Mabona et al., 2019), they still cannot consistently outperform traditional approaches when applied to the RST-DT dataset, where the amount of training data is arguably insufficient for such data-intensive approaches. In this work, we alleviate the restrictions to the effective and efficient use of discourse as mentioned above by introducing a novel approach combining a newly proposed large-scale discourse treebank with our data-driven neural discourse parsing strategy. More specifically, we employ the novel MEGA-DT “silver-standard” discourse treebank published by Huber and Carenini (2020)"
2020.dt4tp-1.4,Q18-1002,0,0.0200505,"ion (Liu et al., 2019). In essence, a neural framework is designed so that a discourse tree for a document is induced while training on the target downstream task. However, even if these approaches demonstrated improvements over non-tree-based models, subsequent studies have shown that the resulting latent discourse dependency trees are often trivial and too shallow (Ferracane et al., 2019). In contrast, recent work on distant supervision from sentiment (Huber and Carenini, 2020) indicates that large treebanks of discourse trees can be generated by combining neural multiple-instance learning (Angelidis and Lapata, 2018) with a CKY-inspired algorithm (Jurafsky and Martin, 2014). Since a series of experiments in inter-domain discourse parsing have certified the high-quality of these treebanks, we use one of such treebaks, called MEGA-DT, for training and testing our data-driven text structuring approach. 3 Novel Task and Methods Our novel task for text structuring receives as input a set of n EDUs and returns both an ordering and a discourse structure for that set. We first describe how the EDUs are encoded, as this is the initial step for all the approaches we consider. Then, after discussing a basic method f"
2020.dt4tp-1.4,P19-1080,0,0.0210675,"course treebanks that have only recently become available. Further, we propose a new evaluation metric that is arguably more suitable for our new task compared to existing content ordering metrics. Finally, we empirically show that our approach outperforms competitive alternatives on the proposed measure and is equivalent in performance with respect to previously established measures. 1 Introduction Natural Language Generation (NLG) plays a fundamental role in data-to-text tasks like automatically producing soccer, weather and financial reports (Chen and Mooney, 2008; Plachouras et al., 2016; Balakrishnan et al., 2019), as well as in text-to-text generation tasks like summarization (Nenkova and McKeown, 2012). Generally speaking, NLG involves three key steps (Gatt and Krahmer, 2017): first there is content determination which selects what information units should be conveyed, secondly there is text structuring, which is responsible for properly structuring and ordering those units; and finally microplanning-realization that aggregates information units into sentences and paragraphs that are then verbalized. The focus of this paper is on the text structuring step, which is critical for the overall performanc"
2020.dt4tp-1.4,D18-1465,0,0.122262,"treebank are constituency trees (see 3141 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3141–3152 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 1(b)), we face the additional challenge of turning them into dependency trees (see Figure 1(a)) before the learning process can start (Hayashi et al., 2016). In a comprehensive evaluation, we compare our solution to three baselines along with a competitive approach based on pointer networks (Vinyals et al., 2015), which is the established method of choice not only for sentence ordering (Cui et al., 2018), but also for basic domain-specific text structuring in data-to-text applications (Puduppully et al., 2019). In particular, the comparison involves training and testing the different models on the MEGADT treebank (Huber and Carenini, 2020), containing ≈250,000 discourse trees obtained by distant supervision from a the Yelp’13 corpus of customer reviews (Tang et al., 2015). With respect to evaluation metrics, we found the current ways of measuring content ordering (e.g., Kendall’s τ ) to be inadequate to capture the quality of long sequences of relatively short information units (i.e., sequenc"
2020.dt4tp-1.4,P19-1062,0,0.0191186,"s the induction of latent tree structures over documents. Some of these works aim at obtaining better document representations for tasks such as text classification (Karimi and Tang, 2019) and single-document extractive summarization (Liu et al., 2019). In essence, a neural framework is designed so that a discourse tree for a document is induced while training on the target downstream task. However, even if these approaches demonstrated improvements over non-tree-based models, subsequent studies have shown that the resulting latent discourse dependency trees are often trivial and too shallow (Ferracane et al., 2019). In contrast, recent work on distant supervision from sentiment (Huber and Carenini, 2020) indicates that large treebanks of discourse trees can be generated by combining neural multiple-instance learning (Angelidis and Lapata, 2018) with a CKY-inspired algorithm (Jurafsky and Martin, 2014). Since a series of experiments in inter-domain discourse parsing have certified the high-quality of these treebanks, we use one of such treebaks, called MEGA-DT, for training and testing our data-driven text structuring approach. 3 Novel Task and Methods Our novel task for text structuring receives as inpu"
2020.dt4tp-1.4,W16-3616,0,0.072552,"e resulting structures can be highly useful for subsequent NLG pipeline stages such as aggregation, and for downstream tasks like text simplification (Zhong et al., 2019). Our approach is trainable end-to-end, but since the discourse trees in the training treebank are constituency trees (see 3141 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3141–3152 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 1(b)), we face the additional challenge of turning them into dependency trees (see Figure 1(a)) before the learning process can start (Hayashi et al., 2016). In a comprehensive evaluation, we compare our solution to three baselines along with a competitive approach based on pointer networks (Vinyals et al., 2015), which is the established method of choice not only for sentence ordering (Cui et al., 2018), but also for basic domain-specific text structuring in data-to-text applications (Puduppully et al., 2019). In particular, the comparison involves training and testing the different models on the MEGADT treebank (Huber and Carenini, 2020), containing ≈250,000 discourse trees obtained by distant supervision from a the Yelp’13 corpus of customer r"
2020.dt4tp-1.4,2020.emnlp-main.603,1,0.912247,"the building blocks of any discourse structure (see Figure 1(a)(left)). In other words, we assume that the system is given a set of EDUs (with cardinality possibly &gt; 100) as input and returns their ordering, as well as the unlabelled RST dependency discourse tree structure for a document consisting of this set of EDUs, as illustrated in Figure 1(a). Our data-driven approach relies on the very recent availability of large treebanks containing hundreds of thousands of (silver-standard) discourse trees that can be automatically generated by distant supervision following the approach presented by Huber and Carenini (2020). We formulate the problem as one of the dependency tree induction, repurposing existing solutions (Ma and Hovy, 2017; Vinyals et al., 2015) to perform an RST-based text structuring where both EDU ordering and tree building are executed simultaneously (Reiter and Dale, 2000). The resulting structures can be highly useful for subsequent NLG pipeline stages such as aggregation, and for downstream tasks like text simplification (Zhong et al., 2019). Our approach is trainable end-to-end, but since the discourse trees in the training treebank are constituency trees (see 3141 Findings of the Associa"
2020.dt4tp-1.4,N19-1347,0,0.0186135,"g smaller semantic units they arguably require more finegrained processing. Furthermore, our task goes beyond ordering by also requiring the synergistic and simultaneous step of generating the RST discourse structure for the EDUs. To address these challenges, more powerful techniques for tree induction are needed on top of pointer networks. (c) Document discourse tree structure induction: The third related line of research involves the induction of latent tree structures over documents. Some of these works aim at obtaining better document representations for tasks such as text classification (Karimi and Tang, 2019) and single-document extractive summarization (Liu et al., 2019). In essence, a neural framework is designed so that a discourse tree for a document is induced while training on the target downstream task. However, even if these approaches demonstrated improvements over non-tree-based models, subsequent studies have shown that the resulting latent discourse dependency trees are often trivial and too shallow (Ferracane et al., 2019). In contrast, recent work on distant supervision from sentiment (Huber and Carenini, 2020) indicates that large treebanks of discourse trees can be generated by com"
2020.dt4tp-1.4,D07-1015,0,0.0411525,"Laplacian matrix L of G: (P P2 n if i = j i0 =1 l=1 Ai0 ,j,l , Li,j = P2 (14) otherwise l=1 −Ai,j,l (8) where W1 ∈ Rd×d×2 , W2 ∈ Rd×2 and W3 ∈ Rd×2 , b ∈ R2 are learnable bilinear, linear and bias terms. Once the tensor M is predicted, it is used for inferring an initial dependency structure. Learning M : The objective is to maximize the probability of the correct tree structure y: P exp (vi ,vj ,l)∈y Mi,j,l P (y|e1:n , θ) = (9) Z(e1:n , θ) where Z(e1:n , θ) = X y∈T (e1:n ) exp  X Mi,j,l i,j,l∈y and replace its first row by r(v): ( ˆ i,j = ri (v), if i = 1 L Li,j otherwise It can be shown (Koo et al., 2007) that the deterˆ is in fact equal to the normalizing minant of L constant that we need: (vi ,vj ,l)∈y (10) with T (e1:n ) denoting all possible trees from a set of EDUs e1:n . Since the number |T (e1:n ) |of possible trees grows exponentially with the number of EDUs, we need an efficient way of computing Z(e1:n , θ). Following (Koo et al., 2007), we achieve this goal by first computing the weighted adjacency matrix A(M ) ∈ Rn×n×2 for left-child and right-child edges: ( 0, if i = j Ai,j,l = (11) exp{Mi,j,l } otherwise as well as the root scores for each node: ri (v) = exp{M LP (vi )} (15) (12)"
2020.dt4tp-1.4,J06-4002,0,0.0872805,"ormation unit from its correct position. These include Kendall’s τ , Position Accuracy (POS) and Perfect Match Ratio (PMR). Then, we propose a new, more sophisticated metric, which is arguably much more appropriate for longer sequences of relatively short information units (i.e., sequences of EDUs of long multisentential text). This metric, that we call Blocked Kendall’τ rewards a correctly ordered sub-sequence even if its location is shifted as a single block. Kendall’s τ : a metric of rank correlation, widely used for information ordering evaluation; found to correlate with human judgement (Lapata, 2006). It is computed as follows: 1 X τoˆi |D| where # of transpositions  n (19) 2 Position Accuracy (POS) computes the average proportion of EDUs that are in their correct absolute position according to the gold ordering. 1 X count(ˆ oi = oi ) |D| length(oi ) (20) oi ∈D (22) The number of transpositions can be at least zero (ifthe sequence is perfectly ordered) and at most n 2 , if the sequence is in reversed order. Thus, Blocked Kendall’s τ has the same range [−1, 1] and is lower bounded by the standard Kendall’s τ , with the key advantage of rewarding correct blocks of EDUs. We also note that"
2020.dt4tp-1.4,N19-1173,0,0.055163,"Missing"
2020.dt4tp-1.4,I17-1007,0,0.362015,"set of EDUs (with cardinality possibly &gt; 100) as input and returns their ordering, as well as the unlabelled RST dependency discourse tree structure for a document consisting of this set of EDUs, as illustrated in Figure 1(a). Our data-driven approach relies on the very recent availability of large treebanks containing hundreds of thousands of (silver-standard) discourse trees that can be automatically generated by distant supervision following the approach presented by Huber and Carenini (2020). We formulate the problem as one of the dependency tree induction, repurposing existing solutions (Ma and Hovy, 2017; Vinyals et al., 2015) to perform an RST-based text structuring where both EDU ordering and tree building are executed simultaneously (Reiter and Dale, 2000). The resulting structures can be highly useful for subsequent NLG pipeline stages such as aggregation, and for downstream tasks like text simplification (Zhong et al., 2019). Our approach is trainable end-to-end, but since the discourse trees in the training treebank are constituency trees (see 3141 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3141–3152 c November 16 - 20, 2020. 2020 Association for Comput"
2020.dt4tp-1.4,J18-2001,0,0.0249433,"o∗ |s, θ) (7) θ s∈D During inference, since an exhaustive search over the most likely ordering is intractable, we use beam search for finding a suboptimal solution. 3.3 Performing the whole task: Our DepStructurer The first design choice in addressing the task of simultaneously structuring and ordering a set of EDUs is whether the system should learn how to build dependency or constituency discourse trees (see Figure 1 (a)-(b) for corresponding examples). We decided to aim for dependency discourse structures for two key reasons. Not only have they been shown to be more general and expressive (Morey et al., 2018), but there are also readily available graph-based methods for learning and inference of dependency trees (Ma and Hovy, 2017) that when properly combined enable structure and ordering prediction to benefit from each other. However, since the only large-scale discourse treebank for training (MEGA-DT) contains constituency trees, we first convert them into dependency ones. For this, we follow the protocol of (Hayashi et al., 2016), which effectively resolves the ambiguity involved in converting multinuclear constituency units. Notice that we want dependency trees that fully specify the ordering"
2020.dt4tp-1.4,D19-1321,0,0.0199655,"data entries. The follow-up work of Puduppully et al. (2019), instead of conditioning on all records, arguably performs better text structuring by first selecting and then ordering the entries of a data table using Pointer network architecture (Vinyals et al., 2015). That way, the surface realization module considers previously generated text and only one new data table entry at a time. Their model was extended by Iso et al. (2019), with an additional GRU for tracking the entities that the model already referred to in the past. Pursuing a rather different approach to improve text structuring, Shao et al. (2019) proposed a hierarchical latent-variable model where the problem is decomposed into dependent sub-tasks, aggregating groups of data table entries into sentences first and then generating the sentences sequentially, conditioned on the plan and already generated sentences. Overall, these last three models significantly outperform the initial approach of Wiseman et al. (2017) both in terms of fluency and coverage, with increasing sophistication of the text structuring module yielding bigger gains, confirming that text structuring is indeed crucial for generating coherent long documents. The task"
2020.dt4tp-1.4,D15-1167,0,0.301771,"omprehensive evaluation, we compare our solution to three baselines along with a competitive approach based on pointer networks (Vinyals et al., 2015), which is the established method of choice not only for sentence ordering (Cui et al., 2018), but also for basic domain-specific text structuring in data-to-text applications (Puduppully et al., 2019). In particular, the comparison involves training and testing the different models on the MEGADT treebank (Huber and Carenini, 2020), containing ≈250,000 discourse trees obtained by distant supervision from a the Yelp’13 corpus of customer reviews (Tang et al., 2015). With respect to evaluation metrics, we found the current ways of measuring content ordering (e.g., Kendall’s τ ) to be inadequate to capture the quality of long sequences of relatively short information units (i.e., sequences of EDUs of long multi-sentential text). Thus, we propose a novel evaluation measure, Blocked Kendall’s τ , that we argue should be used for our new NLG task of ordering and structuring a possibly large set of EDUs, because it critically measures how well semantically close units are clustered together in the correct order. To summarize the contributions of this paper: ("
2020.emnlp-main.603,Q18-1002,0,0.313499,"the lack of annotated data in many domains and for many real-world tasks, methods to automatically generate reliable, fine-grained data-labels have been explored for many years. One promising approach in this area is Multiple Instance Learning (MIL) (Keeler et al., 1991). The general task of MIL is to retrieve fine-grained information (called instance-labels) from high-level supervision (called bag-labels), using correlations of discriminative features within and between bags to predict labels for instances. With the recent rise of deep-learning, neural MIL approaches have also been proposed (Angelidis and Lapata, 2018). We previously combined the two lines of related work described above to create discourse structures from large-scale datasets, solely using document-level supervision (Huber and Carenini, 2019). When applied to the auxiliary task of sentiment analysis, we generated a “silver-standard” discourse structure treebank using the neural MIL model by Angelidis and Lapata (2018) in combination with a sentiment-guided CKY-style treeconstruction algorithm, generating near-optimal discourse trees in bottom-up fashion (Jurafsky and Martin, 2014). Although our approach has shown clear benefits, it is inap"
2020.emnlp-main.603,D15-1263,0,0.360885,"relying on a non-scalable CKY solution, which cannot be applied to many real-world datasets with especially long documents. Introduction Discourse parsing is an important Natural Language Processing (NLP) task, aiming to uncover the hidden structure underlying coherent documents, as described by theories of discourse like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) or PDTB (Prasad et al., 2008). Not only has discourse parsing been shown to enhance key downstream tasks, such as text classification (Ji and Smith, 2017), summarization (Gerani et al., 2014) and sentiment analysis (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015), but it also appears to complement contextual embeddings, like BERT (Devlin et al., 2018), in tasks where discourse information is critical, such as argumentation analysis (Chakrabarty et al., 2019). Traditionally, RST-style discourse parsing builds a complete, hierarchical constituency tree for a document (Morey et al., 2018), where leaf nodes are clause-like sentence fragments, called elementarydiscourse-units (EDUs), while internal tree nodes In this work, we propose a significant extension to this line of research by introducing a scalable solu"
2020.emnlp-main.603,D19-1291,0,0.0350144,"n structure underlying coherent documents, as described by theories of discourse like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) or PDTB (Prasad et al., 2008). Not only has discourse parsing been shown to enhance key downstream tasks, such as text classification (Ji and Smith, 2017), summarization (Gerani et al., 2014) and sentiment analysis (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015), but it also appears to complement contextual embeddings, like BERT (Devlin et al., 2018), in tasks where discourse information is critical, such as argumentation analysis (Chakrabarty et al., 2019). Traditionally, RST-style discourse parsing builds a complete, hierarchical constituency tree for a document (Morey et al., 2018), where leaf nodes are clause-like sentence fragments, called elementarydiscourse-units (EDUs), while internal tree nodes In this work, we propose a significant extension to this line of research by introducing a scalable solution for documents of arbitrary length and further moving beyond just predicting the tree-structure by incorporating the nuclearity attribute, oftentimes critical in informing downstream tasks (Marcu, 2000; Ji and Smith, 2017; Shiv and Quirk, 2"
2020.emnlp-main.603,N18-2097,0,0.0723123,"Missing"
2020.emnlp-main.603,N16-1024,0,0.19766,"s, domain dependent discourse trees are induced during the neural training process. While either method has shown to improve the performance on the downstream task itself, subsequent research by Ferracane et al. (2019) indicates that the induced trees are often trivial and shallow, and 7443 do not represent valid discourse structures. The third stream of related work is on leveraging heuristic search algorithms in NLP tasks involving trees. For syntactic parsing, Vinyals et al. (2015) and Fried et al. (2017) show that a static, small beam size (e.g. 10) already achieves good performance, with Dyer et al. (2016) delivering promising results by using greedy decoding. As a recent example for discourse parsing, Mabona et al. (2019) successfully combine standard beam-search with shift-reduce parsing using two parallel beams for shift and reduce actions. Overall, recent work shows that beam-search approaches and their possible extensions can effectively address scalability issues in multiple parsing scenarios. In this paper, we extend the standard beam-search approach with a stochastic exploration-exploitation trade-off, as used in Reinforcement Learning, where signals also tend to be sparse and noisy. 3"
2020.emnlp-main.603,P14-1048,0,0.081065,"ull RST-style discourse trees, separated into training- and test-set with a 90-10 split. RST-DT by Carlson et al. (2002), containing news articles alongside with full RST-style discourse trees, in the standard 90-10 train-test split. The two automatically annotated treebanks are: Yelp13-DT, generated according to our previously proposed unconstrained CKY approach as described in Huber and Carenini (2019). We use the pre-segmented version of the Yelp’13 customer review dataset by Angelidis and Lapata (2018), separated into EDUs by applying the publicly available discourse segmenter proposed in Feng and Hirst (2014). Yelp13-DT contains short documents with ≤ 20 EDUs, only considering two nuclearity classes (namely N-S and S-N). MEGA-DT, our novel treebank, is also generated 4.2 Discourse Parsers To interpret our results in the context of existing work, we consider a diverse set of top-performing discourse parsers. Previous work by Morey et al. (2017) compares a set of competitive parsers, including DPLP (Ji and Eisenstein, 2014), gCRF (Feng and Hirst, 2014), CODRA (Joty et al., 2015) and Li et al. (2016). We further add the Two-Stage discourse parser by Wang et al. (2017) and the neural approach by Yu et"
2020.emnlp-main.603,P19-1062,0,0.0832656,"s not scale to long documents and cannot predict nuclearity- and relation-labels. Addressing these limitations is a major motivation of this paper. Further efforts to automatically generate discourse trees from auxiliary tasks have been mostly focused on latent tree induction, generating trees from text classification (Karimi and Tang, 2019) or summarization tasks (Liu et al., 2019). For both approaches, domain dependent discourse trees are induced during the neural training process. While either method has shown to improve the performance on the downstream task itself, subsequent research by Ferracane et al. (2019) indicates that the induced trees are often trivial and shallow, and 7443 do not represent valid discourse structures. The third stream of related work is on leveraging heuristic search algorithms in NLP tasks involving trees. For syntactic parsing, Vinyals et al. (2015) and Fried et al. (2017) show that a static, small beam size (e.g. 10) already achieves good performance, with Dyer et al. (2016) delivering promising results by using greedy decoding. As a recent example for discourse parsing, Mabona et al. (2019) successfully combine standard beam-search with shift-reduce parsing using two pa"
2020.emnlp-main.603,P17-2025,0,0.0305723,"Missing"
2020.emnlp-main.603,D14-1168,1,0.842887,"relation labels; and (ii) Applicability, by relying on a non-scalable CKY solution, which cannot be applied to many real-world datasets with especially long documents. Introduction Discourse parsing is an important Natural Language Processing (NLP) task, aiming to uncover the hidden structure underlying coherent documents, as described by theories of discourse like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) or PDTB (Prasad et al., 2008). Not only has discourse parsing been shown to enhance key downstream tasks, such as text classification (Ji and Smith, 2017), summarization (Gerani et al., 2014) and sentiment analysis (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015), but it also appears to complement contextual embeddings, like BERT (Devlin et al., 2018), in tasks where discourse information is critical, such as argumentation analysis (Chakrabarty et al., 2019). Traditionally, RST-style discourse parsing builds a complete, hierarchical constituency tree for a document (Morey et al., 2018), where leaf nodes are clause-like sentence fragments, called elementarydiscourse-units (EDUs), while internal tree nodes In this work, we propose a significant extension to this lin"
2020.emnlp-main.603,D19-1235,1,0.244115,"component. Experiments on multiple datasets indicate that a discourse parser trained on our MEGA-DT treebank delivers promising inter-domain performance gains when compared to parsers trained on human-annotated discourse corpora. 1 A key limitation for further research in RST-style discourse parsing is the scarcity of training data. Only a few human annotated discourse treebanks exist, each only containing a few hundred documents. Although our recent efforts using distant supervision from sentiment to generate large-scale discourse treebanks have already partly addressed this dire situation (Huber and Carenini, 2019), the previously proposed solution is still limited in: (i) Scope, by only building the RST constituency structure without nuclearity and relation labels; and (ii) Applicability, by relying on a non-scalable CKY solution, which cannot be applied to many real-world datasets with especially long documents. Introduction Discourse parsing is an important Natural Language Processing (NLP) task, aiming to uncover the hidden structure underlying coherent documents, as described by theories of discourse like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) or PDTB (Prasad et al., 2008). Not"
2020.emnlp-main.603,P14-1002,0,0.515338,"t our new approach to distant supervision from sentiment can generate large-scale, high-quality treebanks, with MEGADT being the best publicly available resource for training a discourse parser in domains where no gold-standard discourse annotation is available. 2 Related Work The most closely related line of work is RST-style discourse parsing, with the goal to obtain a complete discourse tree, including structure, nuclearity and relations. Based on the observation that these three aspects are correlated, most previous work has explored models to learn them jointly (e.g., Joty et al. (2015); Ji and Eisenstein (2014); Yu et al. (2018)). However, while this strategy seems intuitive, the state-of-the-art (SOTA) system on structure-prediction by Wang et al. (2017) applies a rather different strategy, first jointly predicting structure and nuclearity and then subsequently pre1 Our new Discourse Treebank and the code to generate further “silver-standard” discourse treebanks can be found at: https://www.cs.ubc.ca/ cs-research/lci/research-groups/ natural-language-processing/ dicting relations. The main motivation behind this separation is the large number of possible output classes when predicting these three a"
2020.emnlp-main.603,P17-1092,0,0.455818,"ncy structure without nuclearity and relation labels; and (ii) Applicability, by relying on a non-scalable CKY solution, which cannot be applied to many real-world datasets with especially long documents. Introduction Discourse parsing is an important Natural Language Processing (NLP) task, aiming to uncover the hidden structure underlying coherent documents, as described by theories of discourse like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) or PDTB (Prasad et al., 2008). Not only has discourse parsing been shown to enhance key downstream tasks, such as text classification (Ji and Smith, 2017), summarization (Gerani et al., 2014) and sentiment analysis (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015), but it also appears to complement contextual embeddings, like BERT (Devlin et al., 2018), in tasks where discourse information is critical, such as argumentation analysis (Chakrabarty et al., 2019). Traditionally, RST-style discourse parsing builds a complete, hierarchical constituency tree for a document (Morey et al., 2018), where leaf nodes are clause-like sentence fragments, called elementarydiscourse-units (EDUs), while internal tree nodes In this work, we propos"
2020.emnlp-main.603,J15-3002,1,0.902016,"l, this suggests that our new approach to distant supervision from sentiment can generate large-scale, high-quality treebanks, with MEGADT being the best publicly available resource for training a discourse parser in domains where no gold-standard discourse annotation is available. 2 Related Work The most closely related line of work is RST-style discourse parsing, with the goal to obtain a complete discourse tree, including structure, nuclearity and relations. Based on the observation that these three aspects are correlated, most previous work has explored models to learn them jointly (e.g., Joty et al. (2015); Ji and Eisenstein (2014); Yu et al. (2018)). However, while this strategy seems intuitive, the state-of-the-art (SOTA) system on structure-prediction by Wang et al. (2017) applies a rather different strategy, first jointly predicting structure and nuclearity and then subsequently pre1 Our new Discourse Treebank and the code to generate further “silver-standard” discourse treebanks can be found at: https://www.cs.ubc.ca/ cs-research/lci/research-groups/ natural-language-processing/ dicting relations. The main motivation behind this separation is the large number of possible output classes whe"
2020.emnlp-main.603,N19-1347,0,0.166105,"(2018) in combination with a sentiment-guided CKY-style treeconstruction algorithm, generating near-optimal discourse trees in bottom-up fashion (Jurafsky and Martin, 2014). Although our approach has shown clear benefits, it is inapplicable to many real-world datasets, as it does not scale to long documents and cannot predict nuclearity- and relation-labels. Addressing these limitations is a major motivation of this paper. Further efforts to automatically generate discourse trees from auxiliary tasks have been mostly focused on latent tree induction, generating trees from text classification (Karimi and Tang, 2019) or summarization tasks (Liu et al., 2019). For both approaches, domain dependent discourse trees are induced during the neural training process. While either method has shown to improve the performance on the downstream task itself, subsequent research by Ferracane et al. (2019) indicates that the induced trees are often trivial and shallow, and 7443 do not represent valid discourse structures. The third stream of related work is on leveraging heuristic search algorithms in NLP tasks involving trees. For syntactic parsing, Vinyals et al. (2015) and Fried et al. (2017) show that a static, smal"
2020.emnlp-main.603,D16-1035,0,0.0670775,"a (2018), separated into EDUs by applying the publicly available discourse segmenter proposed in Feng and Hirst (2014). Yelp13-DT contains short documents with ≤ 20 EDUs, only considering two nuclearity classes (namely N-S and S-N). MEGA-DT, our novel treebank, is also generated 4.2 Discourse Parsers To interpret our results in the context of existing work, we consider a diverse set of top-performing discourse parsers. Previous work by Morey et al. (2017) compares a set of competitive parsers, including DPLP (Ji and Eisenstein, 2014), gCRF (Feng and Hirst, 2014), CODRA (Joty et al., 2015) and Li et al. (2016). We further add the Two-Stage discourse parser by Wang et al. (2017) and the neural approach by Yu et al. (2018) into our final evaluation. Due to the top performance of the parser by Wang et al. (2017) on the structure-prediction of the widely used RST-DT corpus, and even more importantly, due to the separation of the relation computation from the structure/nuclearity prediction, we use the parser by Wang et al. (2017) in our inter-domain experiments. 4.3 Preliminary Evaluation We run a set of preliminary evaluations on a randomly selected subset containing 10,000 documents from the Yelp’13"
2020.emnlp-main.603,N19-1173,0,0.181831,"KY-style treeconstruction algorithm, generating near-optimal discourse trees in bottom-up fashion (Jurafsky and Martin, 2014). Although our approach has shown clear benefits, it is inapplicable to many real-world datasets, as it does not scale to long documents and cannot predict nuclearity- and relation-labels. Addressing these limitations is a major motivation of this paper. Further efforts to automatically generate discourse trees from auxiliary tasks have been mostly focused on latent tree induction, generating trees from text classification (Karimi and Tang, 2019) or summarization tasks (Liu et al., 2019). For both approaches, domain dependent discourse trees are induced during the neural training process. While either method has shown to improve the performance on the downstream task itself, subsequent research by Ferracane et al. (2019) indicates that the induced trees are often trivial and shallow, and 7443 do not represent valid discourse structures. The third stream of related work is on leveraging heuristic search algorithms in NLP tasks involving trees. For syntactic parsing, Vinyals et al. (2015) and Fried et al. (2017) show that a static, small beam size (e.g. 10) already achieves goo"
2020.emnlp-main.603,D19-1233,0,0.164135,"Missing"
2020.emnlp-main.603,D17-1136,0,0.214322,"nstrained CKY approach as described in Huber and Carenini (2019). We use the pre-segmented version of the Yelp’13 customer review dataset by Angelidis and Lapata (2018), separated into EDUs by applying the publicly available discourse segmenter proposed in Feng and Hirst (2014). Yelp13-DT contains short documents with ≤ 20 EDUs, only considering two nuclearity classes (namely N-S and S-N). MEGA-DT, our novel treebank, is also generated 4.2 Discourse Parsers To interpret our results in the context of existing work, we consider a diverse set of top-performing discourse parsers. Previous work by Morey et al. (2017) compares a set of competitive parsers, including DPLP (Ji and Eisenstein, 2014), gCRF (Feng and Hirst, 2014), CODRA (Joty et al., 2015) and Li et al. (2016). We further add the Two-Stage discourse parser by Wang et al. (2017) and the neural approach by Yu et al. (2018) into our final evaluation. Due to the top performance of the parser by Wang et al. (2017) on the structure-prediction of the widely used RST-DT corpus, and even more importantly, due to the separation of the relation computation from the structure/nuclearity prediction, we use the parser by Wang et al. (2017) in our inter-domai"
2020.emnlp-main.603,J18-2001,0,0.644829,"on, 1988) or PDTB (Prasad et al., 2008). Not only has discourse parsing been shown to enhance key downstream tasks, such as text classification (Ji and Smith, 2017), summarization (Gerani et al., 2014) and sentiment analysis (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015), but it also appears to complement contextual embeddings, like BERT (Devlin et al., 2018), in tasks where discourse information is critical, such as argumentation analysis (Chakrabarty et al., 2019). Traditionally, RST-style discourse parsing builds a complete, hierarchical constituency tree for a document (Morey et al., 2018), where leaf nodes are clause-like sentence fragments, called elementarydiscourse-units (EDUs), while internal tree nodes In this work, we propose a significant extension to this line of research by introducing a scalable solution for documents of arbitrary length and further moving beyond just predicting the tree-structure by incorporating the nuclearity attribute, oftentimes critical in informing downstream tasks (Marcu, 2000; Ji and Smith, 2017; Shiv and Quirk, 2019). Inspired by the recent success of heuristic search in NLP tasks involving trees (e.g., Fried et al. (2017); Mabona et al. (2"
2020.emnlp-main.603,K16-1028,0,0.112967,"Missing"
2020.emnlp-main.603,D15-1167,0,0.846236,"y the recent success of heuristic search in NLP tasks involving trees (e.g., Fried et al. (2017); Mabona et al. (2019)), we develop a beam-search strategy implementing an exploration-exploitation trade-off, as commonly used in reinforcementlearning (RL) (Poole and Mackworth, 2010). Remarkably, by following this heuristic approach, any large corpus annotated with sentiment can be turned into a discourse treebank on which a domain/genre specific discourse parser can be trained. As a case study for this process, we annotate, evaluate and publicly release a new discourseaugmented Yelp ’13 corpus (Tang et al., 2015) 7442 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7442–7457, c November 16–20, 2020. 2020 Association for Computational Linguistics called MEGA-DT1 (comprising ≈250,000 documents) with nuclearity attributed “silver-standard” discourse trees, solely leveraging the corpus’ document-level sentiment annotation. To evaluate the quality of our newly proposed MEGA-DT corpus, we conduct a series of experiments. We train the top-performing discourse parser by Wang et al. (2017) on MEGA-DT and compare its performance with the same parser trained on previ"
2020.emnlp-main.603,P17-2029,0,0.435228,"ate, evaluate and publicly release a new discourseaugmented Yelp ’13 corpus (Tang et al., 2015) 7442 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7442–7457, c November 16–20, 2020. 2020 Association for Computational Linguistics called MEGA-DT1 (comprising ≈250,000 documents) with nuclearity attributed “silver-standard” discourse trees, solely leveraging the corpus’ document-level sentiment annotation. To evaluate the quality of our newly proposed MEGA-DT corpus, we conduct a series of experiments. We train the top-performing discourse parser by Wang et al. (2017) on MEGA-DT and compare its performance with the same parser trained on previously proposed treebanks. Specifically, we compare our discourse-annotated dataset against a smaller “silver-standard” treebank (Huber and Carenini, 2019) containing around ≈100,000 documents with ≤20 EDUs and two standard human annotated corpora in the news domain (RSTDT) (Carlson et al., 2002) and in the instructional domain (Subba and Di Eugenio, 2009). Results indicate that while training a parser on MEGA-DT does not yet match the performance of training and testing on the same treebank (intradomain), it does push"
2020.emnlp-main.603,C18-1047,0,0.496078,"ant supervision from sentiment can generate large-scale, high-quality treebanks, with MEGADT being the best publicly available resource for training a discourse parser in domains where no gold-standard discourse annotation is available. 2 Related Work The most closely related line of work is RST-style discourse parsing, with the goal to obtain a complete discourse tree, including structure, nuclearity and relations. Based on the observation that these three aspects are correlated, most previous work has explored models to learn them jointly (e.g., Joty et al. (2015); Ji and Eisenstein (2014); Yu et al. (2018)). However, while this strategy seems intuitive, the state-of-the-art (SOTA) system on structure-prediction by Wang et al. (2017) applies a rather different strategy, first jointly predicting structure and nuclearity and then subsequently pre1 Our new Discourse Treebank and the code to generate further “silver-standard” discourse treebanks can be found at: https://www.cs.ubc.ca/ cs-research/lci/research-groups/ natural-language-processing/ dicting relations. The main motivation behind this separation is the large number of possible output classes when predicting these three aspects together. T"
2020.emnlp-main.603,W17-5535,1,0.836189,"able CKY solution, which cannot be applied to many real-world datasets with especially long documents. Introduction Discourse parsing is an important Natural Language Processing (NLP) task, aiming to uncover the hidden structure underlying coherent documents, as described by theories of discourse like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) or PDTB (Prasad et al., 2008). Not only has discourse parsing been shown to enhance key downstream tasks, such as text classification (Ji and Smith, 2017), summarization (Gerani et al., 2014) and sentiment analysis (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015), but it also appears to complement contextual embeddings, like BERT (Devlin et al., 2018), in tasks where discourse information is critical, such as argumentation analysis (Chakrabarty et al., 2019). Traditionally, RST-style discourse parsing builds a complete, hierarchical constituency tree for a document (Morey et al., 2018), where leaf nodes are clause-like sentence fragments, called elementarydiscourse-units (EDUs), while internal tree nodes In this work, we propose a significant extension to this line of research by introducing a scalable solution for documents o"
2020.emnlp-main.603,prasad-etal-2008-penn,0,0.030696,"Huber and Carenini, 2019), the previously proposed solution is still limited in: (i) Scope, by only building the RST constituency structure without nuclearity and relation labels; and (ii) Applicability, by relying on a non-scalable CKY solution, which cannot be applied to many real-world datasets with especially long documents. Introduction Discourse parsing is an important Natural Language Processing (NLP) task, aiming to uncover the hidden structure underlying coherent documents, as described by theories of discourse like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) or PDTB (Prasad et al., 2008). Not only has discourse parsing been shown to enhance key downstream tasks, such as text classification (Ji and Smith, 2017), summarization (Gerani et al., 2014) and sentiment analysis (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015), but it also appears to complement contextual embeddings, like BERT (Devlin et al., 2018), in tasks where discourse information is critical, such as argumentation analysis (Chakrabarty et al., 2019). Traditionally, RST-style discourse parsing builds a complete, hierarchical constituency tree for a document (Morey et al., 2018), where leaf nodes a"
2020.emnlp-main.603,P18-2124,0,0.0592129,"Missing"
2020.emnlp-main.603,D16-1264,0,0.0831617,"Missing"
2020.emnlp-main.603,N09-1064,0,0.677477,"Missing"
2020.findings-emnlp.281,Q18-1002,0,0.160821,"ion (Liu et al., 2019). In essence, a neural framework is designed so that a discourse tree for a document is induced while training on the target downstream task. However, even if these approaches demonstrated improvements over non-tree-based models, subsequent studies have shown that the resulting latent discourse dependency trees are often trivial and too shallow (Ferracane et al., 2019). In contrast, recent work on distant supervision from sentiment (Huber and Carenini, 2020) indicates that large treebanks of discourse trees can be generated by combining neural multiple-instance learning (Angelidis and Lapata, 2018) with a CKY-inspired algorithm (Jurafsky and Martin, 2014). Since a series of experiments in inter-domain discourse parsing have certified the high-quality of these treebanks, we use one of such treebaks, called MEGA-DT, for training and testing our data-driven text structuring approach. 3 Novel Task and Methods Our novel task for text structuring receives as input a set of n EDUs and returns both an ordering and a discourse structure for that set. We first describe how the EDUs are encoded, as this is the initial step for all the approaches we consider. Then, after discussing a basic method f"
2020.findings-emnlp.281,P19-1080,0,0.0285717,"course treebanks that have only recently become available. Further, we propose a new evaluation metric that is arguably more suitable for our new task compared to existing content ordering metrics. Finally, we empirically show that our approach outperforms competitive alternatives on the proposed measure and is equivalent in performance with respect to previously established measures. 1 Introduction Natural Language Generation (NLG) plays a fundamental role in data-to-text tasks like automatically producing soccer, weather and financial reports (Chen and Mooney, 2008; Plachouras et al., 2016; Balakrishnan et al., 2019), as well as in text-to-text generation tasks like summarization (Nenkova and McKeown, 2012). Generally speaking, NLG involves three key steps (Gatt and Krahmer, 2017): first there is content determination which selects what information units should be conveyed, secondly there is text structuring, which is responsible for properly structuring and ordering those units; and finally microplanning-realization that aggregates information units into sentences and paragraphs that are then verbalized. The focus of this paper is on the text structuring step, which is critical for the overall performanc"
2020.findings-emnlp.281,D18-1465,0,0.145266,"treebank are constituency trees (see 3141 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3141–3152 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 1(b)), we face the additional challenge of turning them into dependency trees (see Figure 1(a)) before the learning process can start (Hayashi et al., 2016). In a comprehensive evaluation, we compare our solution to three baselines along with a competitive approach based on pointer networks (Vinyals et al., 2015), which is the established method of choice not only for sentence ordering (Cui et al., 2018), but also for basic domain-specific text structuring in data-to-text applications (Puduppully et al., 2019). In particular, the comparison involves training and testing the different models on the MEGADT treebank (Huber and Carenini, 2020), containing ≈250,000 discourse trees obtained by distant supervision from a the Yelp’13 corpus of customer reviews (Tang et al., 2015). With respect to evaluation metrics, we found the current ways of measuring content ordering (e.g., Kendall’s τ ) to be inadequate to capture the quality of long sequences of relatively short information units (i.e., sequenc"
2020.findings-emnlp.281,P19-1062,0,0.0192511,"s the induction of latent tree structures over documents. Some of these works aim at obtaining better document representations for tasks such as text classification (Karimi and Tang, 2019) and single-document extractive summarization (Liu et al., 2019). In essence, a neural framework is designed so that a discourse tree for a document is induced while training on the target downstream task. However, even if these approaches demonstrated improvements over non-tree-based models, subsequent studies have shown that the resulting latent discourse dependency trees are often trivial and too shallow (Ferracane et al., 2019). In contrast, recent work on distant supervision from sentiment (Huber and Carenini, 2020) indicates that large treebanks of discourse trees can be generated by combining neural multiple-instance learning (Angelidis and Lapata, 2018) with a CKY-inspired algorithm (Jurafsky and Martin, 2014). Since a series of experiments in inter-domain discourse parsing have certified the high-quality of these treebanks, we use one of such treebaks, called MEGA-DT, for training and testing our data-driven text structuring approach. 3 Novel Task and Methods Our novel task for text structuring receives as inpu"
2020.findings-emnlp.281,W16-3616,0,0.16821,"e resulting structures can be highly useful for subsequent NLG pipeline stages such as aggregation, and for downstream tasks like text simplification (Zhong et al., 2019). Our approach is trainable end-to-end, but since the discourse trees in the training treebank are constituency trees (see 3141 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3141–3152 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 1(b)), we face the additional challenge of turning them into dependency trees (see Figure 1(a)) before the learning process can start (Hayashi et al., 2016). In a comprehensive evaluation, we compare our solution to three baselines along with a competitive approach based on pointer networks (Vinyals et al., 2015), which is the established method of choice not only for sentence ordering (Cui et al., 2018), but also for basic domain-specific text structuring in data-to-text applications (Puduppully et al., 2019). In particular, the comparison involves training and testing the different models on the MEGADT treebank (Huber and Carenini, 2020), containing ≈250,000 discourse trees obtained by distant supervision from a the Yelp’13 corpus of customer r"
2020.findings-emnlp.281,2020.emnlp-main.603,1,0.866187,"the building blocks of any discourse structure (see Figure 1(a)(left)). In other words, we assume that the system is given a set of EDUs (with cardinality possibly &gt; 100) as input and returns their ordering, as well as the unlabelled RST dependency discourse tree structure for a document consisting of this set of EDUs, as illustrated in Figure 1(a). Our data-driven approach relies on the very recent availability of large treebanks containing hundreds of thousands of (silver-standard) discourse trees that can be automatically generated by distant supervision following the approach presented by Huber and Carenini (2020). We formulate the problem as one of the dependency tree induction, repurposing existing solutions (Ma and Hovy, 2017; Vinyals et al., 2015) to perform an RST-based text structuring where both EDU ordering and tree building are executed simultaneously (Reiter and Dale, 2000). The resulting structures can be highly useful for subsequent NLG pipeline stages such as aggregation, and for downstream tasks like text simplification (Zhong et al., 2019). Our approach is trainable end-to-end, but since the discourse trees in the training treebank are constituency trees (see 3141 Findings of the Associa"
2020.findings-emnlp.281,N19-1347,0,0.0181234,"g smaller semantic units they arguably require more finegrained processing. Furthermore, our task goes beyond ordering by also requiring the synergistic and simultaneous step of generating the RST discourse structure for the EDUs. To address these challenges, more powerful techniques for tree induction are needed on top of pointer networks. (c) Document discourse tree structure induction: The third related line of research involves the induction of latent tree structures over documents. Some of these works aim at obtaining better document representations for tasks such as text classification (Karimi and Tang, 2019) and single-document extractive summarization (Liu et al., 2019). In essence, a neural framework is designed so that a discourse tree for a document is induced while training on the target downstream task. However, even if these approaches demonstrated improvements over non-tree-based models, subsequent studies have shown that the resulting latent discourse dependency trees are often trivial and too shallow (Ferracane et al., 2019). In contrast, recent work on distant supervision from sentiment (Huber and Carenini, 2020) indicates that large treebanks of discourse trees can be generated by com"
2020.findings-emnlp.281,D07-1015,0,0.0218306,"Laplacian matrix L of G: (P P2 n if i = j i0 =1 l=1 Ai0 ,j,l , Li,j = P2 (14) otherwise l=1 −Ai,j,l (8) where W1 ∈ Rd×d×2 , W2 ∈ Rd×2 and W3 ∈ Rd×2 , b ∈ R2 are learnable bilinear, linear and bias terms. Once the tensor M is predicted, it is used for inferring an initial dependency structure. Learning M : The objective is to maximize the probability of the correct tree structure y: P exp (vi ,vj ,l)∈y Mi,j,l P (y|e1:n , θ) = (9) Z(e1:n , θ) where Z(e1:n , θ) = X y∈T (e1:n ) exp  X Mi,j,l i,j,l∈y and replace its first row by r(v): ( ˆ i,j = ri (v), if i = 1 L Li,j otherwise It can be shown (Koo et al., 2007) that the deterˆ is in fact equal to the normalizing minant of L constant that we need: (vi ,vj ,l)∈y (10) with T (e1:n ) denoting all possible trees from a set of EDUs e1:n . Since the number |T (e1:n ) |of possible trees grows exponentially with the number of EDUs, we need an efficient way of computing Z(e1:n , θ). Following (Koo et al., 2007), we achieve this goal by first computing the weighted adjacency matrix A(M ) ∈ Rn×n×2 for left-child and right-child edges: ( 0, if i = j Ai,j,l = (11) exp{Mi,j,l } otherwise as well as the root scores for each node: ri (v) = exp{M LP (vi )} (15) (12)"
2020.findings-emnlp.281,J06-4002,0,0.0812052,"ormation unit from its correct position. These include Kendall’s τ , Position Accuracy (POS) and Perfect Match Ratio (PMR). Then, we propose a new, more sophisticated metric, which is arguably much more appropriate for longer sequences of relatively short information units (i.e., sequences of EDUs of long multisentential text). This metric, that we call Blocked Kendall’τ rewards a correctly ordered sub-sequence even if its location is shifted as a single block. Kendall’s τ : a metric of rank correlation, widely used for information ordering evaluation; found to correlate with human judgement (Lapata, 2006). It is computed as follows: 1 X τoˆi |D| where # of transpositions  n (19) 2 Position Accuracy (POS) computes the average proportion of EDUs that are in their correct absolute position according to the gold ordering. 1 X count(ˆ oi = oi ) |D| length(oi ) (20) oi ∈D (22) The number of transpositions can be at least zero (ifthe sequence is perfectly ordered) and at most n 2 , if the sequence is in reversed order. Thus, Blocked Kendall’s τ has the same range [−1, 1] and is lower bounded by the standard Kendall’s τ , with the key advantage of rewarding correct blocks of EDUs. We also note that"
2020.findings-emnlp.281,N19-1173,0,0.0475226,"Missing"
2020.findings-emnlp.281,I17-1007,0,0.34644,"set of EDUs (with cardinality possibly &gt; 100) as input and returns their ordering, as well as the unlabelled RST dependency discourse tree structure for a document consisting of this set of EDUs, as illustrated in Figure 1(a). Our data-driven approach relies on the very recent availability of large treebanks containing hundreds of thousands of (silver-standard) discourse trees that can be automatically generated by distant supervision following the approach presented by Huber and Carenini (2020). We formulate the problem as one of the dependency tree induction, repurposing existing solutions (Ma and Hovy, 2017; Vinyals et al., 2015) to perform an RST-based text structuring where both EDU ordering and tree building are executed simultaneously (Reiter and Dale, 2000). The resulting structures can be highly useful for subsequent NLG pipeline stages such as aggregation, and for downstream tasks like text simplification (Zhong et al., 2019). Our approach is trainable end-to-end, but since the discourse trees in the training treebank are constituency trees (see 3141 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3141–3152 c November 16 - 20, 2020. 2020 Association for Comput"
2020.findings-emnlp.281,J18-2001,0,0.024861,"o∗ |s, θ) (7) θ s∈D During inference, since an exhaustive search over the most likely ordering is intractable, we use beam search for finding a suboptimal solution. 3.3 Performing the whole task: Our DepStructurer The first design choice in addressing the task of simultaneously structuring and ordering a set of EDUs is whether the system should learn how to build dependency or constituency discourse trees (see Figure 1 (a)-(b) for corresponding examples). We decided to aim for dependency discourse structures for two key reasons. Not only have they been shown to be more general and expressive (Morey et al., 2018), but there are also readily available graph-based methods for learning and inference of dependency trees (Ma and Hovy, 2017) that when properly combined enable structure and ordering prediction to benefit from each other. However, since the only large-scale discourse treebank for training (MEGA-DT) contains constituency trees, we first convert them into dependency ones. For this, we follow the protocol of (Hayashi et al., 2016), which effectively resolves the ambiguity involved in converting multinuclear constituency units. Notice that we want dependency trees that fully specify the ordering"
2020.findings-emnlp.281,D19-1321,0,0.0250734,"data entries. The follow-up work of Puduppully et al. (2019), instead of conditioning on all records, arguably performs better text structuring by first selecting and then ordering the entries of a data table using Pointer network architecture (Vinyals et al., 2015). That way, the surface realization module considers previously generated text and only one new data table entry at a time. Their model was extended by Iso et al. (2019), with an additional GRU for tracking the entities that the model already referred to in the past. Pursuing a rather different approach to improve text structuring, Shao et al. (2019) proposed a hierarchical latent-variable model where the problem is decomposed into dependent sub-tasks, aggregating groups of data table entries into sentences first and then generating the sentences sequentially, conditioned on the plan and already generated sentences. Overall, these last three models significantly outperform the initial approach of Wiseman et al. (2017) both in terms of fluency and coverage, with increasing sophistication of the text structuring module yielding bigger gains, confirming that text structuring is indeed crucial for generating coherent long documents. The task"
2020.findings-emnlp.281,D15-1167,0,0.536931,"omprehensive evaluation, we compare our solution to three baselines along with a competitive approach based on pointer networks (Vinyals et al., 2015), which is the established method of choice not only for sentence ordering (Cui et al., 2018), but also for basic domain-specific text structuring in data-to-text applications (Puduppully et al., 2019). In particular, the comparison involves training and testing the different models on the MEGADT treebank (Huber and Carenini, 2020), containing ≈250,000 discourse trees obtained by distant supervision from a the Yelp’13 corpus of customer reviews (Tang et al., 2015). With respect to evaluation metrics, we found the current ways of measuring content ordering (e.g., Kendall’s τ ) to be inadequate to capture the quality of long sequences of relatively short information units (i.e., sequences of EDUs of long multi-sentential text). Thus, we propose a novel evaluation measure, Blocked Kendall’s τ , that we argue should be used for our new NLG task of ordering and structuring a possibly large set of EDUs, because it critically measures how well semantically close units are clustered together in the correct order. To summarize the contributions of this paper: ("
2020.nlpcovid19-2.18,D19-3001,0,0.0223974,"nd that time. 5 How Do Twitter Users React to COVID-19? Understanding people’s reactions to the COVID19 pandemic is important to public health agencies because it informs how public health agencies should frame their health messaging. To capture sentiment revealed in tweets towards important aspects of COVID-19, we use ABSA. In our work, aspects can include public health interventions or issues associated with COVID-19 such as “socialdistancing”, “reopening”, and “masks”. We investigate people’s opinion (positive/negative) towards these aspects. We use ABSApp, a weakly-supervised ABSA system (Pereg et al., 2019). We choose ABSApp because it does not require labeled data for training, and allows manually editing domain-specific aspect and opinion lexicons produced by the method. This feature is particularly beneficial for us because we can select/add aspects public health agencies are interested in. After training the tweet data using ABSApp, we obtained 806 aspect terms and 211 opinion terms. Editing the lexicons by public health experts resulted in 545 aspect terms (e.g., “vaccines”, “economy”, and “masks”) and 60 domain specific opinion terms (e.g., “infectious”- negative, and “professional”- posit"
2021.acl-long.302,N19-1408,0,0.0115451,"e the discourse trees learned from sentiment for the sentiment analysis task itself and, similarly, rely on the discourse trees learned from summarization to benefit the summarization task. 3911 4.1.1 Sentiment Analysis In order to predict the sentiment of a document in W-RST-Sent based on its weighted discourse tree, we need to introduce an additional source of information to be aggregated according to such tree. Here, we choose word embeddings, as commonly used as an initial transformation in many models tackling the sentiment prediction task (Kim, 2014; Tai et al., 2015; Yang et al., 2016; Adhikari et al., 2019; Huber and Carenini, 2020a). To avoid introducing additional confounding factors through sophisticated tree aggregation approaches (e.g. TreeLSTMs (Tai et al., 2015)), we select a simple method, aiming to directly compare the inferred tree-structures and allowing us to better assess the performance differences originating from the weight/nuclearity attribution (see right step in Figure 3). More specifically, we start by computing the average word-embedding for each leaf node leaf i (here containing a single EDU) in the discourse tree. j&lt;|leaf i | X leaf i = Emb(wordji )/|leaf i | j=0 With |le"
2021.acl-long.302,Q18-1002,0,0.016884,"ear combination has been previously learned on the training portion of the dataset. tween summarization and discourse. In particular, in Xiao et al. (2021), we demonstrate that the selfattention matrix learned during the training of a transformer-based summarizer captures valid aspects of constituency and dependency discourse trees. To summarize, building on our previous work on creating discourse trees through distant supervision, we take a first step towards generating weighted discourse trees from the sentiment analysis and summarization tasks. 3 W-RST Treebank Generation model proposed by Angelidis and Lapata (2018) on a corpus with document-level sentiment goldlabels, internally annotating each input-unit (in our case EDUs) with a sentiment- and attention-score. After the MIL model is trained (center), a tuple (si , ai ) containing a sentiment score si and an attention ai is extracted for each EDU i. Based on these tuples representing leaf nodes, the CKY algorithm (Jurafsky and Martin, 2014) is applied to find the tree structure to best align with the overall document sentiment, through a bottom-up aggregation approach defined as3 : sl ∗ al + sr ∗ ar al + ar Given the intuition from above, we combine in"
2021.acl-long.302,D15-1263,0,0.0217838,"ree structure, often referred to as the tree span. (2) A nuclearity Arguably, the weakest aspect of an RST representation is the nuclearity assessment, which makes a too coarse differentiation between primary and secondary importance of sub-trees. However, despite its binary assignment of importance and even though the nuclearity attribute is only one of three components of an RST tree, it has major implications for many downstream tasks, as already shown early on by Marcu (1999), using the nuclearity attribute as the key signal in extractive summarization. Further work in sentiment analysis (Bhatia et al., 2015) also showed the importance of nuclearity for the task by first converting the constituency tree into a dependency tree (more aligned with the nuclearity attribute) and then using that tree to predict sentiment more accurately. Both of these results indicate that nuclearity, even in the coarse RST version, already contains valuable information. Hence, we believe that this coarsegrained classification is reasonable when manually annotating discourse, but see it as a major point of improvement, if a more fine-grained assessment could be correctly assigned. We therefore explore the potential of a"
2021.acl-long.302,P14-1048,0,0.0196493,"de describing the relationship between the sub-trees of a node (e.g., Contrast, Evidence, Contradiction). Figure 1: Document wsj 0639 from the RST-DT corpus with inconsistent importance differences between N-S attributions. (The top-level satellite is clearly more central to the overall context than the lower-level satellite. However, both are similarly assigned the satellite attribution by at least one annotator). Top relation: Annotator 1: N-S, Annotator 2: N-N. computational models of discourse by simply applying machine learning methods to RST annotated treebanks (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Li et al., 2016; Wang et al., 2017; Yu et al., 2018), we rely on very recent empirical studies showing that weighted “silver-standard” discourse trees can be inferred from auxiliary tasks such as sentiment analysis (Huber and Carenini, 2020b) and summarization (Xiao et al., 2021). In our evaluation, we assess both, computational benefits and linguistic insights. In particular, we find that automatically generated, weighted discourse trees can benefit key NLP downstream tasks. We further show that real-valued importance scores (at least partially) align with human annotatio"
2021.acl-long.302,D14-1168,1,0.798529,"e to predict sentiment more accurately. Both of these results indicate that nuclearity, even in the coarse RST version, already contains valuable information. Hence, we believe that this coarsegrained classification is reasonable when manually annotating discourse, but see it as a major point of improvement, if a more fine-grained assessment could be correctly assigned. We therefore explore the potential of assigning a weighted nuclearity attribute in this paper. While plenty of studies have highlighted the important role of discourse for real-world downstream tasks, including summarization, (Gerani et al., 2014; Xu et al., 2020; Xiao et al., 2020), sentiment analysis (Bhatia et al., 2015; Hogenboom et al., 2015; Nejat et al., 2017) and text classification (Ji and Smith, 2017), more critical to our approach is very recent work exploring such connection in the opposite direction. In Huber and Carenini (2020b), we exploit sentiment related information to generate “silver-standard” nuclearity annotated discourse trees, showing their potential on the domain-transfer discourse parsing task. Crucially for our purposes, this approach internally generates real-valued importance-weights for trees. For the tas"
2021.acl-long.302,W19-2708,0,0.0347731,"Missing"
2021.acl-long.302,2020.findings-emnlp.281,1,0.763668,"capture uncertainty in human annotators, implying some alignment of the importance distributions with linguistic ambiguity. 2 Related Work First introduced by Mann and Thompson (1988), the Rhetorical Structure Theory (RST) has been one of the primary guiding theories for discourse analysis (Carlson et al., 2002; Subba and Di Eugenio, 2009; Zeldes, 2017; Gessler et al., 2019; Liu and Zeldes, 2019), discourse parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Li et al., 2016; Wang et al., 2017; Yu et al., 2018), and text planning (Torrance, 2015; Gatt and Krahmer, 2018; Guz and Carenini, 2020). The RST framework thereby comprehensively describes the organization of a document, guided by the author’s communicative goals, encompassing three components: (1) A projective constituency tree structure, often referred to as the tree span. (2) A nuclearity Arguably, the weakest aspect of an RST representation is the nuclearity assessment, which makes a too coarse differentiation between primary and secondary importance of sub-trees. However, despite its binary assignment of importance and even though the nuclearity attribute is only one of three components of an RST tree, it has major impli"
2021.acl-long.302,2020.emnlp-main.603,1,0.751811,"ly more central to the overall context than the lower-level satellite. However, both are similarly assigned the satellite attribution by at least one annotator). Top relation: Annotator 1: N-S, Annotator 2: N-N. computational models of discourse by simply applying machine learning methods to RST annotated treebanks (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Li et al., 2016; Wang et al., 2017; Yu et al., 2018), we rely on very recent empirical studies showing that weighted “silver-standard” discourse trees can be inferred from auxiliary tasks such as sentiment analysis (Huber and Carenini, 2020b) and summarization (Xiao et al., 2021). In our evaluation, we assess both, computational benefits and linguistic insights. In particular, we find that automatically generated, weighted discourse trees can benefit key NLP downstream tasks. We further show that real-valued importance scores (at least partially) align with human annotations and can interestingly also capture uncertainty in human annotators, implying some alignment of the importance distributions with linguistic ambiguity. 2 Related Work First introduced by Mann and Thompson (1988), the Rhetorical Structure Theory (RST) has been"
2021.acl-long.302,P14-1002,0,0.0252689,"ute for every internal node describing the relationship between the sub-trees of a node (e.g., Contrast, Evidence, Contradiction). Figure 1: Document wsj 0639 from the RST-DT corpus with inconsistent importance differences between N-S attributions. (The top-level satellite is clearly more central to the overall context than the lower-level satellite. However, both are similarly assigned the satellite attribution by at least one annotator). Top relation: Annotator 1: N-S, Annotator 2: N-N. computational models of discourse by simply applying machine learning methods to RST annotated treebanks (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Li et al., 2016; Wang et al., 2017; Yu et al., 2018), we rely on very recent empirical studies showing that weighted “silver-standard” discourse trees can be inferred from auxiliary tasks such as sentiment analysis (Huber and Carenini, 2020b) and summarization (Xiao et al., 2021). In our evaluation, we assess both, computational benefits and linguistic insights. In particular, we find that automatically generated, weighted discourse trees can benefit key NLP downstream tasks. We further show that real-valued importance scores (at least partially) alig"
2021.acl-long.302,P17-1092,0,0.0180385,"believe that this coarsegrained classification is reasonable when manually annotating discourse, but see it as a major point of improvement, if a more fine-grained assessment could be correctly assigned. We therefore explore the potential of assigning a weighted nuclearity attribute in this paper. While plenty of studies have highlighted the important role of discourse for real-world downstream tasks, including summarization, (Gerani et al., 2014; Xu et al., 2020; Xiao et al., 2020), sentiment analysis (Bhatia et al., 2015; Hogenboom et al., 2015; Nejat et al., 2017) and text classification (Ji and Smith, 2017), more critical to our approach is very recent work exploring such connection in the opposite direction. In Huber and Carenini (2020b), we exploit sentiment related information to generate “silver-standard” nuclearity annotated discourse trees, showing their potential on the domain-transfer discourse parsing task. Crucially for our purposes, this approach internally generates real-valued importance-weights for trees. For the task of extractive summarization, we follow our intuition given in Xiao et al. (2020) and Xiao et al. (2021), exploiting the connection be3909 Figure 2: Three phases of ou"
2021.acl-long.302,J15-3002,1,0.832775,"tionship between the sub-trees of a node (e.g., Contrast, Evidence, Contradiction). Figure 1: Document wsj 0639 from the RST-DT corpus with inconsistent importance differences between N-S attributions. (The top-level satellite is clearly more central to the overall context than the lower-level satellite. However, both are similarly assigned the satellite attribution by at least one annotator). Top relation: Annotator 1: N-S, Annotator 2: N-N. computational models of discourse by simply applying machine learning methods to RST annotated treebanks (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Li et al., 2016; Wang et al., 2017; Yu et al., 2018), we rely on very recent empirical studies showing that weighted “silver-standard” discourse trees can be inferred from auxiliary tasks such as sentiment analysis (Huber and Carenini, 2020b) and summarization (Xiao et al., 2021). In our evaluation, we assess both, computational benefits and linguistic insights. In particular, we find that automatically generated, weighted discourse trees can benefit key NLP downstream tasks. We further show that real-valued importance scores (at least partially) align with human annotations and can interest"
2021.acl-long.302,D14-1181,0,0.00246271,"trees with nuclearity. Specifically, we leverage the discourse trees learned from sentiment for the sentiment analysis task itself and, similarly, rely on the discourse trees learned from summarization to benefit the summarization task. 3911 4.1.1 Sentiment Analysis In order to predict the sentiment of a document in W-RST-Sent based on its weighted discourse tree, we need to introduce an additional source of information to be aggregated according to such tree. Here, we choose word embeddings, as commonly used as an initial transformation in many models tackling the sentiment prediction task (Kim, 2014; Tai et al., 2015; Yang et al., 2016; Adhikari et al., 2019; Huber and Carenini, 2020a). To avoid introducing additional confounding factors through sophisticated tree aggregation approaches (e.g. TreeLSTMs (Tai et al., 2015)), we select a simple method, aiming to directly compare the inferred tree-structures and allowing us to better assess the performance differences originating from the weight/nuclearity attribution (see right step in Figure 3). More specifically, we start by computing the average word-embedding for each leaf node leaf i (here containing a single EDU) in the discourse tree"
2021.acl-long.302,D16-1035,0,0.0151594,"e sub-trees of a node (e.g., Contrast, Evidence, Contradiction). Figure 1: Document wsj 0639 from the RST-DT corpus with inconsistent importance differences between N-S attributions. (The top-level satellite is clearly more central to the overall context than the lower-level satellite. However, both are similarly assigned the satellite attribution by at least one annotator). Top relation: Annotator 1: N-S, Annotator 2: N-N. computational models of discourse by simply applying machine learning methods to RST annotated treebanks (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Li et al., 2016; Wang et al., 2017; Yu et al., 2018), we rely on very recent empirical studies showing that weighted “silver-standard” discourse trees can be inferred from auxiliary tasks such as sentiment analysis (Huber and Carenini, 2020b) and summarization (Xiao et al., 2021). In our evaluation, we assess both, computational benefits and linguistic insights. In particular, we find that automatically generated, weighted discourse trees can benefit key NLP downstream tasks. We further show that real-valued importance scores (at least partially) align with human annotations and can interestingly also captur"
2021.acl-long.302,K16-1028,0,0.0542651,"Missing"
2021.acl-long.302,2020.coling-main.16,1,0.708509,"ly more central to the overall context than the lower-level satellite. However, both are similarly assigned the satellite attribution by at least one annotator). Top relation: Annotator 1: N-S, Annotator 2: N-N. computational models of discourse by simply applying machine learning methods to RST annotated treebanks (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Li et al., 2016; Wang et al., 2017; Yu et al., 2018), we rely on very recent empirical studies showing that weighted “silver-standard” discourse trees can be inferred from auxiliary tasks such as sentiment analysis (Huber and Carenini, 2020b) and summarization (Xiao et al., 2021). In our evaluation, we assess both, computational benefits and linguistic insights. In particular, we find that automatically generated, weighted discourse trees can benefit key NLP downstream tasks. We further show that real-valued importance scores (at least partially) align with human annotations and can interestingly also capture uncertainty in human annotators, implying some alignment of the importance distributions with linguistic ambiguity. 2 Related Work First introduced by Mann and Thompson (1988), the Rhetorical Structure Theory (RST) has been"
2021.acl-long.302,W17-5535,1,0.841498,"eady contains valuable information. Hence, we believe that this coarsegrained classification is reasonable when manually annotating discourse, but see it as a major point of improvement, if a more fine-grained assessment could be correctly assigned. We therefore explore the potential of assigning a weighted nuclearity attribute in this paper. While plenty of studies have highlighted the important role of discourse for real-world downstream tasks, including summarization, (Gerani et al., 2014; Xu et al., 2020; Xiao et al., 2020), sentiment analysis (Bhatia et al., 2015; Hogenboom et al., 2015; Nejat et al., 2017) and text classification (Ji and Smith, 2017), more critical to our approach is very recent work exploring such connection in the opposite direction. In Huber and Carenini (2020b), we exploit sentiment related information to generate “silver-standard” nuclearity annotated discourse trees, showing their potential on the domain-transfer discourse parsing task. Crucially for our purposes, this approach internally generates real-valued importance-weights for trees. For the task of extractive summarization, we follow our intuition given in Xiao et al. (2020) and Xiao et al. (2021), exploiting the c"
2021.acl-long.302,D14-1162,0,0.0838232,"Missing"
2021.acl-long.302,prasad-etal-2008-penn,0,0.0248492,"l Language Processing (NLP) should balance and integrate findings from machine learning approaches with insights and theories from linguistics. With the enormous success of data-driven approaches over the last decades, this balance has arguably and excessively shifted, with linguistic theories playing a less and less critical role. Even more importantly, there are only little attempts made to improve such theories in light of recent empirical results. In the context of discourse, two main theories have emerged in the past: The Rhetorical Structure Theory (RST) (Carlson et al., 2002) and PDTB (Prasad et al., 2008). In this paper, we focus on RST, exploring whether the underlying theory can be refined in a data-driven manner. In general, RST postulates a complete discourse tree for a given document. To obtain this formal representation as a projective consituency tree, a given document is first separated into so called Elementary Discourse Units (or short EDUs), representing clause-like sentence fragments of the input ∗ Equal contribution. document. Afterwards, the discourse tree is built by hierarchically aggregating EDUs into larger constituents annotated with an importance indicator (in RST called nu"
2021.acl-long.302,N09-1064,0,0.10326,"Missing"
2021.acl-long.302,P15-1150,0,0.00941589,"nuclearity. Specifically, we leverage the discourse trees learned from sentiment for the sentiment analysis task itself and, similarly, rely on the discourse trees learned from summarization to benefit the summarization task. 3911 4.1.1 Sentiment Analysis In order to predict the sentiment of a document in W-RST-Sent based on its weighted discourse tree, we need to introduce an additional source of information to be aggregated according to such tree. Here, we choose word embeddings, as commonly used as an initial transformation in many models tackling the sentiment prediction task (Kim, 2014; Tai et al., 2015; Yang et al., 2016; Adhikari et al., 2019; Huber and Carenini, 2020a). To avoid introducing additional confounding factors through sophisticated tree aggregation approaches (e.g. TreeLSTMs (Tai et al., 2015)), we select a simple method, aiming to directly compare the inferred tree-structures and allowing us to better assess the performance differences originating from the weight/nuclearity attribution (see right step in Figure 3). More specifically, we start by computing the average word-embedding for each leaf node leaf i (here containing a single EDU) in the discourse tree. j&lt;|leaf i | X le"
2021.acl-long.302,D15-1167,0,0.0775985,"Missing"
2021.acl-long.302,2020.acl-main.451,0,0.136898,"t more accurately. Both of these results indicate that nuclearity, even in the coarse RST version, already contains valuable information. Hence, we believe that this coarsegrained classification is reasonable when manually annotating discourse, but see it as a major point of improvement, if a more fine-grained assessment could be correctly assigned. We therefore explore the potential of assigning a weighted nuclearity attribute in this paper. While plenty of studies have highlighted the important role of discourse for real-world downstream tasks, including summarization, (Gerani et al., 2014; Xu et al., 2020; Xiao et al., 2020), sentiment analysis (Bhatia et al., 2015; Hogenboom et al., 2015; Nejat et al., 2017) and text classification (Ji and Smith, 2017), more critical to our approach is very recent work exploring such connection in the opposite direction. In Huber and Carenini (2020b), we exploit sentiment related information to generate “silver-standard” nuclearity annotated discourse trees, showing their potential on the domain-transfer discourse parsing task. Crucially for our purposes, this approach internally generates real-valued importance-weights for trees. For the task of extractive s"
2021.acl-long.302,N16-1174,0,0.0129146,"fically, we leverage the discourse trees learned from sentiment for the sentiment analysis task itself and, similarly, rely on the discourse trees learned from summarization to benefit the summarization task. 3911 4.1.1 Sentiment Analysis In order to predict the sentiment of a document in W-RST-Sent based on its weighted discourse tree, we need to introduce an additional source of information to be aggregated according to such tree. Here, we choose word embeddings, as commonly used as an initial transformation in many models tackling the sentiment prediction task (Kim, 2014; Tai et al., 2015; Yang et al., 2016; Adhikari et al., 2019; Huber and Carenini, 2020a). To avoid introducing additional confounding factors through sophisticated tree aggregation approaches (e.g. TreeLSTMs (Tai et al., 2015)), we select a simple method, aiming to directly compare the inferred tree-structures and allowing us to better assess the performance differences originating from the weight/nuclearity attribution (see right step in Figure 3). More specifically, we start by computing the average word-embedding for each leaf node leaf i (here containing a single EDU) in the discourse tree. j&lt;|leaf i | X leaf i = Emb(wordji )"
2021.acl-long.302,N16-1000,0,0.199358,"Missing"
2021.acl-long.302,P17-2029,0,0.0205227,"node (e.g., Contrast, Evidence, Contradiction). Figure 1: Document wsj 0639 from the RST-DT corpus with inconsistent importance differences between N-S attributions. (The top-level satellite is clearly more central to the overall context than the lower-level satellite. However, both are similarly assigned the satellite attribution by at least one annotator). Top relation: Annotator 1: N-S, Annotator 2: N-N. computational models of discourse by simply applying machine learning methods to RST annotated treebanks (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Joty et al., 2015; Li et al., 2016; Wang et al., 2017; Yu et al., 2018), we rely on very recent empirical studies showing that weighted “silver-standard” discourse trees can be inferred from auxiliary tasks such as sentiment analysis (Huber and Carenini, 2020b) and summarization (Xiao et al., 2021). In our evaluation, we assess both, computational benefits and linguistic insights. In particular, we find that automatically generated, weighted discourse trees can benefit key NLP downstream tasks. We further show that real-valued importance scores (at least partially) align with human annotations and can interestingly also capture uncertainty in hu"
2021.acl-long.302,2020.codi-1.13,1,0.737494,". Both of these results indicate that nuclearity, even in the coarse RST version, already contains valuable information. Hence, we believe that this coarsegrained classification is reasonable when manually annotating discourse, but see it as a major point of improvement, if a more fine-grained assessment could be correctly assigned. We therefore explore the potential of assigning a weighted nuclearity attribute in this paper. While plenty of studies have highlighted the important role of discourse for real-world downstream tasks, including summarization, (Gerani et al., 2014; Xu et al., 2020; Xiao et al., 2020), sentiment analysis (Bhatia et al., 2015; Hogenboom et al., 2015; Nejat et al., 2017) and text classification (Ji and Smith, 2017), more critical to our approach is very recent work exploring such connection in the opposite direction. In Huber and Carenini (2020b), we exploit sentiment related information to generate “silver-standard” nuclearity annotated discourse trees, showing their potential on the domain-transfer discourse parsing task. Crucially for our purposes, this approach internally generates real-valued importance-weights for trees. For the task of extractive summarization, we fol"
2021.acl-long.302,2021.naacl-main.326,1,0.790564,"Missing"
2021.acl-short.119,P16-1046,0,0.0199042,"s. In this paper, we introduce a novel technique1 to demote lead bias and make the summarizer focus more on the content semantics. Experiments on two news corpora with different degrees of lead bias show that our method can effectively demote the model’s learned lead bias and improve its generality on out-ofdistribution data, with little to no performance loss on in-distribution data. 1 Introduction Neural extractive summarization, which produces a short summary for a document by selecting a set of representative sentences, has shown great potential in real-world applications, including news (Cheng and Lapata, 2016; Nallapati et al., 2017) and scientific paper summarization (Cohan et al., 2018; Xiao and Carenini, 2019). Typically, a generalpurpose extractive summarizer learns to select the most important sentences from a document to form the summary by considering their content salience, informativeness and redundancy. However, when restricted to a specific domain, the summarizer can learn to exploit particular biases in the data, the most famous of which is the lead bias in news (Nenkova et al., 2011; Hong and Nenkova, 2014); namely that sentences at the beginning of a news article are more likely to c"
2021.acl-short.119,D19-1418,0,0.307524,"can be only applied to RL-based summarizers, is to add an explicit auxiliary loss to directly balance position with content. Alternatively, Zhong et al. (2019b) and Wang et al. (2019) investigate strategies to train the summarizer on multiple news datasets with different degrees of lead bias, but this may still be problematic when we apply the trained summarizer to the documents with lead bias not covered in the training data. Outside the summarization area, methods have also been proposed to eliminate data biases for other NLP tasks like text classification or entailment (Kumar et al., 2019; Clark et al., 2019, 2020). Inspired by Kumar et al. (2019), we have developed an alternating adversarial learning technique to demote the summarizer lead bias, but also maintain the performance on the in-distribution 948 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 948–954 August 1–6, 2021. ©2021 Association for Computational Linguistics data. We introduce a position prediction component as an adversary, and optimize it along with the neural extractive summarizer in an alte"
2021.acl-short.119,2020.findings-emnlp.272,0,0.0819094,"Missing"
2021.acl-short.119,N18-2097,0,0.0176923,"ummarizer focus more on the content semantics. Experiments on two news corpora with different degrees of lead bias show that our method can effectively demote the model’s learned lead bias and improve its generality on out-ofdistribution data, with little to no performance loss on in-distribution data. 1 Introduction Neural extractive summarization, which produces a short summary for a document by selecting a set of representative sentences, has shown great potential in real-world applications, including news (Cheng and Lapata, 2016; Nallapati et al., 2017) and scientific paper summarization (Cohan et al., 2018; Xiao and Carenini, 2019). Typically, a generalpurpose extractive summarizer learns to select the most important sentences from a document to form the summary by considering their content salience, informativeness and redundancy. However, when restricted to a specific domain, the summarizer can learn to exploit particular biases in the data, the most famous of which is the lead bias in news (Nenkova et al., 2011; Hong and Nenkova, 2014); namely that sentences at the beginning of a news article are more likely to contain summary-worthy information. As a result, not surprisingly, such bias is s"
2021.acl-short.119,D19-1620,0,0.680375,"learn to exploit particular biases in the data, the most famous of which is the lead bias in news (Nenkova et al., 2011; Hong and Nenkova, 2014); namely that sentences at the beginning of a news article are more likely to contain summary-worthy information. As a result, not surprisingly, such bias is strongly captured by neural extractive summarizers for news, for which the sentence positional information tends to dominate the actual content of ∗ 1 The first two authors contributed equally to this work. https://github.com/lxing532/Debiasing the sentence in model prediction (Jung et al., 2019; Grenander et al., 2019; Zhong et al., 2019a,b). While learning a summarizer reflecting the biases in the training dataset is completely fine when the summarizer is going to be deployed to summarize documents having similar biases, it would be problematic when the model was applied to deal with documents coming from a mixture of datasets with different degrees of such biases. In this paper, we address this problem in the context of the lead bias in the news domain by exploring ways in which an extractive summarizer for news can be trained so that it learns to balance the lead bias with the content of the sentences,"
2021.acl-short.119,E14-1075,0,0.0289462,"entences, has shown great potential in real-world applications, including news (Cheng and Lapata, 2016; Nallapati et al., 2017) and scientific paper summarization (Cohan et al., 2018; Xiao and Carenini, 2019). Typically, a generalpurpose extractive summarizer learns to select the most important sentences from a document to form the summary by considering their content salience, informativeness and redundancy. However, when restricted to a specific domain, the summarizer can learn to exploit particular biases in the data, the most famous of which is the lead bias in news (Nenkova et al., 2011; Hong and Nenkova, 2014); namely that sentences at the beginning of a news article are more likely to contain summary-worthy information. As a result, not surprisingly, such bias is strongly captured by neural extractive summarizers for news, for which the sentence positional information tends to dominate the actual content of ∗ 1 The first two authors contributed equally to this work. https://github.com/lxing532/Debiasing the sentence in model prediction (Jung et al., 2019; Grenander et al., 2019; Zhong et al., 2019a,b). While learning a summarizer reflecting the biases in the training dataset is completely fine whe"
2021.acl-short.119,D19-1327,0,0.0844876,"the summarizer can learn to exploit particular biases in the data, the most famous of which is the lead bias in news (Nenkova et al., 2011; Hong and Nenkova, 2014); namely that sentences at the beginning of a news article are more likely to contain summary-worthy information. As a result, not surprisingly, such bias is strongly captured by neural extractive summarizers for news, for which the sentence positional information tends to dominate the actual content of ∗ 1 The first two authors contributed equally to this work. https://github.com/lxing532/Debiasing the sentence in model prediction (Jung et al., 2019; Grenander et al., 2019; Zhong et al., 2019a,b). While learning a summarizer reflecting the biases in the training dataset is completely fine when the summarizer is going to be deployed to summarize documents having similar biases, it would be problematic when the model was applied to deal with documents coming from a mixture of datasets with different degrees of such biases. In this paper, we address this problem in the context of the lead bias in the news domain by exploring ways in which an extractive summarizer for news can be trained so that it learns to balance the lead bias with the co"
2021.acl-short.119,D18-1208,0,0.0542121,"Missing"
2021.acl-short.119,D19-1425,0,0.154167,"n. The other, which can be only applied to RL-based summarizers, is to add an explicit auxiliary loss to directly balance position with content. Alternatively, Zhong et al. (2019b) and Wang et al. (2019) investigate strategies to train the summarizer on multiple news datasets with different degrees of lead bias, but this may still be problematic when we apply the trained summarizer to the documents with lead bias not covered in the training data. Outside the summarization area, methods have also been proposed to eliminate data biases for other NLP tasks like text classification or entailment (Kumar et al., 2019; Clark et al., 2019, 2020). Inspired by Kumar et al. (2019), we have developed an alternating adversarial learning technique to demote the summarizer lead bias, but also maintain the performance on the in-distribution 948 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 948–954 August 1–6, 2021. ©2021 Association for Computational Linguistics data. We introduce a position prediction component as an adversary, and optimize it along with the neural extractive s"
2021.acl-short.119,W04-1013,0,0.0182855,"32 30.98 16.79 18.29↑ 16.22↓ 16.67 16.98↑ 17.20↑⇑ XSum R2 RL 1.60 11.96 8.98 23.51 1.84 12.33 2.53↑ 13.45↑ 1.62↓ 11.90↓ 1.91↑ 12.28 1.96↑ 12.48↑ 1.99↑⇑ 12.63↑⇑ Mean 9.96 21.16 10.32 11.42 9.91 10.29 10.47 10.61 Table 1: The ROUGE-1/2/L F1 scores and “Mean” (mean of ROUGE-1/2/L) on CNN/DM and XSum test data. The best and second best performances over the basic transformer are in bold and underlined. ↑/↓ indicates the results are significantly higher/lower than Basic Transformer and ⇑/⇓ indicates the results are significantly higher/lower than Shuffling (p < 0.01 with bootstrap resampling test (Lin, 2004)). Model Lead-3 Oracle Basic Transformer – No Position Encoding – Only Position Encoding Learned-Mixin Shuffling Our Method Dearly 49.33 49.51 44.30 16.07 48.65∗†‡ 40.45 42.69 42.67 Dmiddle 30.90 47.02 31.91 16.88 30.97 31.82 31.91 32.18∗†‡ 3.3 Dlate 19.80 43.81 22.65 18.59 19.70 22.70 22.99∗†‡ 22.85∗† Table 2: Avg. of ROUGE-1/2/L F1 scores on Dearly , Dmiddle and Dlate . Results significantly better than Basic Transformer on ROUGE-1/2/L are marked with ∗, †, and ‡ respectively. is a method proposed lately for summarization lead bias demoting, and Learned-Mixin (Clark et al., 2019), which is a"
2021.acl-short.119,D18-1206,0,0.262079,"ics data. We introduce a position prediction component as an adversary, and optimize it along with the neural extractive summarizer in an alternating manner. Furthermore, in contrast with Grenander et al. (2019) and Wang et al. (2019), our proposal is model-independent and only requires one type of news dataset as training input. In this paper, we apply our proposed method to a biased transformer-based extractive summarizer (Vaswani et al., 2017) trained on CNN/DM training set (Hermann et al., 2015) and conduct experiments on two test sets with different degrees of lead bias: CNN/DM and XSum (Narayan et al., 2018), for in-distribution and generality evaluation respectively. The experimental results indicate that our proposed “debiasing” method can effectively demote the lead bias learned by the neural news summarizer and improve its generalizability, while still mostly maintaining the model’s performance on the data with a similar lead bias. 2 Proposed Debiasing Method Our method aims to demote the lead bias learned by the summarizer and encourage it to select content based more on the semantics covered in sentences. As shown in Figure 1, our method comprises two components: one for Summarization (red)"
2021.acl-short.119,P11-5003,0,0.0600632,"et of representative sentences, has shown great potential in real-world applications, including news (Cheng and Lapata, 2016; Nallapati et al., 2017) and scientific paper summarization (Cohan et al., 2018; Xiao and Carenini, 2019). Typically, a generalpurpose extractive summarizer learns to select the most important sentences from a document to form the summary by considering their content salience, informativeness and redundancy. However, when restricted to a specific domain, the summarizer can learn to exploit particular biases in the data, the most famous of which is the lead bias in news (Nenkova et al., 2011; Hong and Nenkova, 2014); namely that sentences at the beginning of a news article are more likely to contain summary-worthy information. As a result, not surprisingly, such bias is strongly captured by neural extractive summarizers for news, for which the sentence positional information tends to dominate the actual content of ∗ 1 The first two authors contributed equally to this work. https://github.com/lxing532/Debiasing the sentence in model prediction (Jung et al., 2019; Grenander et al., 2019; Zhong et al., 2019a,b). While learning a summarizer reflecting the biases in the training datas"
2021.acl-short.119,D18-1191,0,0.0245713,"Missing"
2021.acl-short.119,D19-1298,1,0.932137,"on the content semantics. Experiments on two news corpora with different degrees of lead bias show that our method can effectively demote the model’s learned lead bias and improve its generality on out-ofdistribution data, with little to no performance loss on in-distribution data. 1 Introduction Neural extractive summarization, which produces a short summary for a document by selecting a set of representative sentences, has shown great potential in real-world applications, including news (Cheng and Lapata, 2016; Nallapati et al., 2017) and scientific paper summarization (Cohan et al., 2018; Xiao and Carenini, 2019). Typically, a generalpurpose extractive summarizer learns to select the most important sentences from a document to form the summary by considering their content salience, informativeness and redundancy. However, when restricted to a specific domain, the summarizer can learn to exploit particular biases in the data, the most famous of which is the lead bias in news (Nenkova et al., 2011; Hong and Nenkova, 2014); namely that sentences at the beginning of a news article are more likely to contain summary-worthy information. As a result, not surprisingly, such bias is strongly captured by neural"
2021.acl-short.119,2020.aacl-main.51,1,0.79244,"Missing"
2021.acl-short.119,2020.codi-1.13,1,0.765534,"and improve its generalizability, while still mostly maintaining the model’s performance on the data with a similar lead bias. 2 Proposed Debiasing Method Our method aims to demote the lead bias learned by the summarizer and encourage it to select content based more on the semantics covered in sentences. As shown in Figure 1, our method comprises two components: one for Summarization (red) and the other for sentence Position Prediction (green). 2.1 Summarization Component Following previous work, we formulate extractive summarization as a sequence labeling task (Xiao and Carenini, 2019, 2020; Xiao et al., 2020). For a document d = {s1 , s2 , ..., sk }, each sentence will be assigned a score α ∈ [0, 1]. The summary will then be formed with the highest scored sentences. We adopt a transformer-based model (Vaswani et al., 2017) as our basic “biased” summarization component (red in Fig. 1), as shown to be heavily impacted by the lead bias (Zhong et al., 2019a). This component contains a transformerbased encoder Encθt and a multilayer perceptron (MLP) decoder Decθs , parameterized by θt and θs respectively. We use the averaged word embedding from Glove as sentence embedding as suggested in Kedzie et al."
2021.acl-short.119,2020.aacl-main.63,1,0.838819,"Missing"
2021.acl-short.119,P19-1100,0,0.0392854,"Missing"
2021.acl-short.119,D19-5410,0,0.215995,"lar biases in the data, the most famous of which is the lead bias in news (Nenkova et al., 2011; Hong and Nenkova, 2014); namely that sentences at the beginning of a news article are more likely to contain summary-worthy information. As a result, not surprisingly, such bias is strongly captured by neural extractive summarizers for news, for which the sentence positional information tends to dominate the actual content of ∗ 1 The first two authors contributed equally to this work. https://github.com/lxing532/Debiasing the sentence in model prediction (Jung et al., 2019; Grenander et al., 2019; Zhong et al., 2019a,b). While learning a summarizer reflecting the biases in the training dataset is completely fine when the summarizer is going to be deployed to summarize documents having similar biases, it would be problematic when the model was applied to deal with documents coming from a mixture of datasets with different degrees of such biases. In this paper, we address this problem in the context of the lead bias in the news domain by exploring ways in which an extractive summarizer for news can be trained so that it learns to balance the lead bias with the content of the sentences, resulting in a model"
2021.deelio-1.10,2020.acl-main.494,0,0.0298066,"Missing"
2021.deelio-1.10,W14-4012,0,0.167441,"Missing"
2021.deelio-1.10,D18-1537,0,0.0238155,"ese models are generally useful for simple prediction tasks, yet it is difficult to apply them to complicated tasks. To interpret complex models used for complex tasks, one can examine how prediction 97 changes between two different inputs (Shrikumar et al., 2017; Lundberg and Lee, 2017) or by locally perturbing an input (Ribeiro et al., 2016). However, a recent and popular method in NLP has been the use of an attention mechanism, which was found to be effective in helping interpret complex models by highlighting which inputs are informative to prediction (Wang et al., 2016; Lin et al., 2017; Ghaeini et al., 2018; Seo et al., 2016). Along the lines of work using attention for interpretation, our model improves attention-based interpretability by using high-level concept information. To our knowledge, no prior work used external high-level concept information for better interpretability. 3 high-level ontology for organizing a great number of concepts in the biomedical domain, which provides unified access to many different biomedical resources. On top of the UMLS, the UMLS semantic network (McCray, 2003) implements an upperlevel conceptual layer for all UMLS concepts. This semantic network categorizes"
2021.deelio-1.10,P15-1010,0,0.0732323,"Missing"
2021.deelio-1.10,N18-2041,0,0.0187336,"etability. Section 6 concludes. 2 2.1 representation. KnowBERT (Peters et al., 2019) trained BERT for entity linkers and language modeling in a multitask setting to incorporate entity representation. K-BERT (Liu et al., 2020) injected triples from knowledge graphs into a sentence to obtain an extended tree-form input for BERT. Although all these prior models incorporated external knowledge into advanced neural architectures to improve model performance, they didn’t pay much attention to interpretability benefits. There have been a few knowledge-infused models that considered interpretability. Kumar et al. (2018) proposed a two-level attention network for sentiment analysis using knowledge graph embedding generated using WordNet (Fellbaum, 2012) and top-k similar words. Although this work mentions interpretability, it did not show whether/how the model can help interpretability. Margatina et al. (2019) incorporated existing psycho-linguistic and affective knowledge from human experts for sentiment related tasks. This work only showed attention heatmap for an example. Our work is distinguished from others in that KW-ATTN is designed in consideration of not only accuracy but also interpretability of the"
2021.deelio-1.10,P19-1385,0,0.128866,"an extended tree-form input for BERT. Although all these prior models incorporated external knowledge into advanced neural architectures to improve model performance, they didn’t pay much attention to interpretability benefits. There have been a few knowledge-infused models that considered interpretability. Kumar et al. (2018) proposed a two-level attention network for sentiment analysis using knowledge graph embedding generated using WordNet (Fellbaum, 2012) and top-k similar words. Although this work mentions interpretability, it did not show whether/how the model can help interpretability. Margatina et al. (2019) incorporated existing psycho-linguistic and affective knowledge from human experts for sentiment related tasks. This work only showed attention heatmap for an example. Our work is distinguished from others in that KW-ATTN is designed in consideration of not only accuracy but also interpretability of the model. For this reason, KW-ATTN allows separately and flexibly attending to the words and/or concepts so that important concepts for prediction can be included in prediction explanations, adding an extra layer of interpretation. We also perform human evaluation to see the effect of incorporati"
2021.deelio-1.10,Q14-1019,0,0.0255635,"ith high-level concepts from two knowledge bases: BabelNet and UMLS. 3.1.1 Expression “Mom” “Diagnosed” “Stage 3 ovarian cancer” BabelNet BabelNet (Navigli and Ponzetto, 2012) is a constantly growing semantic network which connects concepts and named entities in a large network of semantic relations, currently made up of about 16 million entries, called Babel synsets. In our study, we use the hypernyms of Babel synsets as additional higher-level concept information for the raw words or phrases in text. We first map texts with concepts in Babel synsets using an entity linking toolkit, Babelfy (Moro et al., 2014), and then retrieve hypernyms, high-level concepts, of the concepts using BabelNet APIs. Table 1 shows example annotations for the sentence “My mom was diagnosed with stage 3 ovarian cancer.” Expression “Mom” “diagnosed” “state” “ovarian cancer” Table 2: MetaMap annotations for UMLS concepts 3.2 Incorporating High-Level Concepts To incorporate high-level concept information into a NN model, we design a new attention mechanism, KW-ATTN, which allows giving separate but complementary attentions to a word and its corresponding concept. To test KW-ATTN, we choose a one-level RNN architecture with"
2021.deelio-1.10,N18-1097,0,0.0204489,"re, swing, best, looking” KW same length “java as a(n) object-oriented_programming_language, ide as a(n) application, php as a(n) free_software” KW replacement “object-oriented_programming_language, application, free_software, swing, best, looking” Table 7: Examples of different types of explanations used for human evaluation. We assume that attention can be used for prediction explanations based on (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019). We choose to ask about the validity of a given prediction unlike prior work that asked to guess a model’s prediction based on an explanation (Nguyen, 2018; Chen et al., 2020). Although we acknowledge that the model’s prediction may bias the annotators, we choose this approach since humans have high-level concepts as background knowledge. Humans do not require external additional concept information for guessing a correct topic label among multiple topic options especially when the given topic options are distinct from each other. For example, although the high-level concept “athletics” is not given for the word “baseball” in an explanation, humans would not have a problem with classifying it into the sports category when given topic options are"
2021.deelio-1.10,D19-1005,0,0.0212546,"n accuracy when added on top of the pretrained BERT model. Additionally, our attention analysis on patient need data annotated with BabelNet and UMLS indicates that choice of external knowledge impacts the model’s performance. (3) Lastly, our human evaluation using crowdsourcing suggests our model improves interpretability. Section 2 relates prior work to ours. Section 3 explains our method. Section 4 evaluates our model on two different tasks in terms of classification accuracy. Section 5 describes our human evaluation on interpretability. Section 6 concludes. 2 2.1 representation. KnowBERT (Peters et al., 2019) trained BERT for entity linkers and language modeling in a multitask setting to incorporate entity representation. K-BERT (Liu et al., 2020) injected triples from knowledge graphs into a sentence to obtain an extended tree-form input for BERT. Although all these prior models incorporated external knowledge into advanced neural architectures to improve model performance, they didn’t pay much attention to interpretability benefits. There have been a few knowledge-infused models that considered interpretability. Kumar et al. (2018) proposed a two-level attention network for sentiment analysis us"
2021.deelio-1.10,N16-3020,0,0.0505504,"g explanations or justifications behind a model’s prediction can further assist in decision making or the task at hand. To provide interpretability, researchers have used inherently interpretable models such as sparse linear regression models, decision trees, or rule sets. These models are generally useful for simple prediction tasks, yet it is difficult to apply them to complicated tasks. To interpret complex models used for complex tasks, one can examine how prediction 97 changes between two different inputs (Shrikumar et al., 2017; Lundberg and Lee, 2017) or by locally perturbing an input (Ribeiro et al., 2016). However, a recent and popular method in NLP has been the use of an attention mechanism, which was found to be effective in helping interpret complex models by highlighting which inputs are informative to prediction (Wang et al., 2016; Lin et al., 2017; Ghaeini et al., 2018; Seo et al., 2016). Along the lines of work using attention for interpretation, our model improves attention-based interpretability by using high-level concept information. To our knowledge, no prior work used external high-level concept information for better interpretability. 3 high-level ontology for organizing a great"
2021.deelio-1.10,P19-1282,0,0.020804,"ncept “java, yields, best, language, results, built” KW same number “java as a(n) object-oriented_programming_language, ide as a(n) application, php as a(n) free_software, swing, best, looking” KW same length “java as a(n) object-oriented_programming_language, ide as a(n) application, php as a(n) free_software” KW replacement “object-oriented_programming_language, application, free_software, swing, best, looking” Table 7: Examples of different types of explanations used for human evaluation. We assume that attention can be used for prediction explanations based on (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019). We choose to ask about the validity of a given prediction unlike prior work that asked to guess a model’s prediction based on an explanation (Nguyen, 2018; Chen et al., 2020). Although we acknowledge that the model’s prediction may bias the annotators, we choose this approach since humans have high-level concepts as background knowledge. Humans do not require external additional concept information for guessing a correct topic label among multiple topic options especially when the given topic options are distinct from each other. For example, although the high-level concept “athletics” is no"
2021.deelio-1.10,E17-1014,0,0.0201012,"incorporation of external semantic knowledge into neural models for text classification. Wang et al. (2017) proposed a framework based on convolutional neural networks that combines explicit and implicit representations of short text for classification by conceptualizing a short text as a set of relevant concepts using a large taxonomy knowledge base. Yang and Mitchell (2017) proposed KBLSTM, a RNN model that uses continuous representations of knowledge bases for machine reading. Xu et al. (2017) incorporated background knowledge with the format of entity-attribute for conversation modeling. Stanovsky et al. (2017) overrided word embeddings with DBpedia concept embeddings, and used RNNs for recognizing mentions of adverse drug reaction in social media. More advanced neural architectures such as transformers has been also benefited by external knowledge. (Zhong et al., 2019) proposed a Knowledge Enriched Transformer (KET), where contextual utterances are interpreted using hierarchical self-attention and external commonsense knowledge is dynamically leveraged using a contextaware affective graph attention mechanism. ERNIE (Zhang et al., 2019) integrated entity embeddings pretrained on a knowledge graph wi"
2021.deelio-1.10,D16-1058,0,0.0372154,"els, decision trees, or rule sets. These models are generally useful for simple prediction tasks, yet it is difficult to apply them to complicated tasks. To interpret complex models used for complex tasks, one can examine how prediction 97 changes between two different inputs (Shrikumar et al., 2017; Lundberg and Lee, 2017) or by locally perturbing an input (Ribeiro et al., 2016). However, a recent and popular method in NLP has been the use of an attention mechanism, which was found to be effective in helping interpret complex models by highlighting which inputs are informative to prediction (Wang et al., 2016; Lin et al., 2017; Ghaeini et al., 2018; Seo et al., 2016). Along the lines of work using attention for interpretation, our model improves attention-based interpretability by using high-level concept information. To our knowledge, no prior work used external high-level concept information for better interpretability. 3 high-level ontology for organizing a great number of concepts in the biomedical domain, which provides unified access to many different biomedical resources. On top of the UMLS, the UMLS semantic network (McCray, 2003) implements an upperlevel conceptual layer for all UMLS conc"
2021.deelio-1.10,D19-1002,0,0.021984,"planation Type Example No concept “java, yields, best, language, results, built” KW same number “java as a(n) object-oriented_programming_language, ide as a(n) application, php as a(n) free_software, swing, best, looking” KW same length “java as a(n) object-oriented_programming_language, ide as a(n) application, php as a(n) free_software” KW replacement “object-oriented_programming_language, application, free_software, swing, best, looking” Table 7: Examples of different types of explanations used for human evaluation. We assume that attention can be used for prediction explanations based on (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019). We choose to ask about the validity of a given prediction unlike prior work that asked to guess a model’s prediction based on an explanation (Nguyen, 2018; Chen et al., 2020). Although we acknowledge that the model’s prediction may bias the annotators, we choose this approach since humans have high-level concepts as background knowledge. Humans do not require external additional concept information for guessing a correct topic label among multiple topic options especially when the given topic options are distinct from each other. For example, although the high-level"
2021.deelio-1.10,P17-1132,0,0.0216008,"We also perform human evaluation to see the effect of incorporating high-level concepts on interpretation rather than just showing a few visualization examples. Related Work Knowledge-infused Neural Networks There has been a growing interest in incorporation of external semantic knowledge into neural models for text classification. Wang et al. (2017) proposed a framework based on convolutional neural networks that combines explicit and implicit representations of short text for classification by conceptualizing a short text as a set of relevant concepts using a large taxonomy knowledge base. Yang and Mitchell (2017) proposed KBLSTM, a RNN model that uses continuous representations of knowledge bases for machine reading. Xu et al. (2017) incorporated background knowledge with the format of entity-attribute for conversation modeling. Stanovsky et al. (2017) overrided word embeddings with DBpedia concept embeddings, and used RNNs for recognizing mentions of adverse drug reaction in social media. More advanced neural architectures such as transformers has been also benefited by external knowledge. (Zhong et al., 2019) proposed a Knowledge Enriched Transformer (KET), where contextual utterances are interprete"
2021.deelio-1.10,N16-1174,0,0.0604892,"e “My mom was diagnosed with stage 3 ovarian cancer.” Expression “Mom” “diagnosed” “state” “ovarian cancer” Table 2: MetaMap annotations for UMLS concepts 3.2 Incorporating High-Level Concepts To incorporate high-level concept information into a NN model, we design a new attention mechanism, KW-ATTN, which allows giving separate but complementary attentions to a word and its corresponding concept. To test KW-ATTN, we choose a one-level RNN architecture with an attention mechanism (1L), a hierarchical RNN architecture with an attention mechanism (2L) as in Hierarchical Attention Network (HAN) (Yang et al., 2016), and a pretrained BERT (Devlin et al., 2018). Our 2L model architecture is shown in Figure 1. The whole architecture begins with words in each sentence as input. They are embedded and encoded using a word encoder, and then the resulting hidden representations move forward to a wordconcept attention layer after being concatenated with the corresponding concept embeddings. This part is different from common RNN architectures for text classification, where only the hidden representations from the word encoder are used for a word-level attention layer. Then, the output of this attention layer is"
2021.deelio-1.10,P19-1139,0,0.0277501,"h the format of entity-attribute for conversation modeling. Stanovsky et al. (2017) overrided word embeddings with DBpedia concept embeddings, and used RNNs for recognizing mentions of adverse drug reaction in social media. More advanced neural architectures such as transformers has been also benefited by external knowledge. (Zhong et al., 2019) proposed a Knowledge Enriched Transformer (KET), where contextual utterances are interpreted using hierarchical self-attention and external commonsense knowledge is dynamically leveraged using a contextaware affective graph attention mechanism. ERNIE (Zhang et al., 2019) integrated entity embeddings pretrained on a knowledge graph with corresponding entity mentions in the text to augment the text 2.2 Interpretability Interpretability is the ability to explain or present a model in an understandable way to humans (DoshiVelez and Kim, 2017). This interpretability is beneficial for developers to understand the model, help identify and possibly fix issues with the model, or to enhance the model. It is crucial for application end users because knowing explanations or justifications behind a model’s prediction can further assist in decision making or the task at ha"
2021.deelio-1.10,D19-1016,0,0.0389379,"Missing"
2021.emnlp-demo.26,2020.acl-demos.22,0,0.136829,"anatory rithms that compute the importance of model capabilities of the attention mechanism (Jain and components and different parts of the input Wallace, 2019; Wiegreffe and Pinter, 2019), the sequence. Case studies and feedback from visualization of its weights has also been shown to a user focus group indicate that the framebe beneficial in discovering learnt features (Clark work is useful, and suggest several improveet al., 2019; Voita et al., 2019), with promising rements. Our framework is available at: https: cent work focusing on Transformers (Vig, 2019; //github.com/raymondzmc/T3-Vis. Hoover et al., 2020). Besides the works on exploring what has been 1 Introduction learnt in the pretrained models, there are also sevApproaches through neural networks have made significant progress in the field of NLP, with Trans- eral visualization tools developed to show saliency former models (Vaswani et al., 2017) rapidly be- scores generated by gradient-based (Simonyan et al., 2013; Bach et al., 2015; Shrikumar et al., coming the dominant architecture due to their efficient parallel training and ability to effec- 2017) or perturbation-based interpretation methods (Ribeiro et al., 2016; Li et al., 2016), whi"
2021.emnlp-demo.26,N19-1357,0,0.0326178,"Missing"
2021.emnlp-demo.26,2020.tacl-1.5,0,0.0200055,"ed (Simonyan et al., 2013; Bach et al., 2015; Shrikumar et al., coming the dominant architecture due to their efficient parallel training and ability to effec- 2017) or perturbation-based interpretation methods (Ribeiro et al., 2016; Li et al., 2016), which tivelymodel long sequences. Following the release can help with visualizing the relative importance of BERT (Devlin et al., 2019) along with other Transformer-based models pretrained on large cor- of individual tokens in the input with respect to a target prediction (Wallace et al., 2019; Johnson pora (Liu et al., 2019; Lewis et al., 2020; Joshi et al., 2020; Tenney et al., 2020). However, only a et al., 2020; Lee et al., 2020), the most successful few studies have instead focused on visualizing the strategy on many NLP leaderboards has been to overall training dynamics, where support is critical directly fine-tune such models on the downstream tasks (e.g., summarization, classification). How- for identifying mislabeled examples or failure cases (Liu et al., 2018; Xiang et al., 2019; Swayamdipta ever, despite the strong empirical performance of et al., 2020) this strategy, understanding and interpreting the training and fine-tuning processes rema"
2021.emnlp-demo.26,D19-1445,0,0.051724,"Missing"
2021.emnlp-demo.26,2021.eacl-demos.17,0,0.0444025,"Missing"
2021.emnlp-demo.26,2020.acl-main.703,0,0.026206,"ated by gradient-based (Simonyan et al., 2013; Bach et al., 2015; Shrikumar et al., coming the dominant architecture due to their efficient parallel training and ability to effec- 2017) or perturbation-based interpretation methods (Ribeiro et al., 2016; Li et al., 2016), which tivelymodel long sequences. Following the release can help with visualizing the relative importance of BERT (Devlin et al., 2019) along with other Transformer-based models pretrained on large cor- of individual tokens in the input with respect to a target prediction (Wallace et al., 2019; Johnson pora (Liu et al., 2019; Lewis et al., 2020; Joshi et al., 2020; Tenney et al., 2020). However, only a et al., 2020; Lee et al., 2020), the most successful few studies have instead focused on visualizing the strategy on many NLP leaderboards has been to overall training dynamics, where support is critical directly fine-tune such models on the downstream tasks (e.g., summarization, classification). How- for identifying mislabeled examples or failure cases (Liu et al., 2018; Xiang et al., 2019; Swayamdipta ever, despite the strong empirical performance of et al., 2020) this strategy, understanding and interpreting the training and fine-t"
2021.emnlp-demo.26,D19-1387,0,0.0280687,"initial evidence on the effectiveness of different visualization components, and serve as examples for how our framework can be used. 3.2.1 Pattern Exploration for an Extractive Summarizer NLP researchers in our group, who work on summarization, applied T3 -Vis to the extractive summarization task, which aims to compress a document 224 Figure 4: The self-attention distribution of token “photo” in the Instance Analysis View. Figure 5: A misclassified example within a cluster of well-classified example. by selecting its most informative sentences. BERTSum, which is fine-tuned from a BERT model (Liu and Lapata, 2019), is one of the top-performing models for extractive summarization, but why and how it works remains a mystery. With our interface, the researchers explored patterns captured by the model that played important roles in model predictions. They performed an analysis on the CNN/Daily Mail dataset (Hermann et al., 2015), which is arguably the most popular benchmark for summarization tasks. 3.2.2 Error Analysis for Topic Classification Other researchers in our group explored the interface for error analysis to identify possible improvements of a BERT-based model for topic classification. The Yahoo"
2021.emnlp-demo.26,2021.ccl-1.108,0,0.0873086,"Missing"
2021.emnlp-demo.26,2020.emnlp-main.746,0,0.0811936,"Missing"
2021.emnlp-demo.26,2020.emnlp-demos.15,0,0.445592,"2013; Bach et al., 2015; Shrikumar et al., coming the dominant architecture due to their efficient parallel training and ability to effec- 2017) or perturbation-based interpretation methods (Ribeiro et al., 2016; Li et al., 2016), which tivelymodel long sequences. Following the release can help with visualizing the relative importance of BERT (Devlin et al., 2019) along with other Transformer-based models pretrained on large cor- of individual tokens in the input with respect to a target prediction (Wallace et al., 2019; Johnson pora (Liu et al., 2019; Lewis et al., 2020; Joshi et al., 2020; Tenney et al., 2020). However, only a et al., 2020; Lee et al., 2020), the most successful few studies have instead focused on visualizing the strategy on many NLP leaderboards has been to overall training dynamics, where support is critical directly fine-tune such models on the downstream tasks (e.g., summarization, classification). How- for identifying mislabeled examples or failure cases (Liu et al., 2018; Xiang et al., 2019; Swayamdipta ever, despite the strong empirical performance of et al., 2020) this strategy, understanding and interpreting the training and fine-tuning processes remains a critiIn essence,"
2021.emnlp-demo.26,2020.blackboxnlp-1.4,0,0.0335257,"Missing"
2021.emnlp-demo.26,N16-3020,0,0.06069,"com/raymondzmc/T3-Vis. Hoover et al., 2020). Besides the works on exploring what has been 1 Introduction learnt in the pretrained models, there are also sevApproaches through neural networks have made significant progress in the field of NLP, with Trans- eral visualization tools developed to show saliency former models (Vaswani et al., 2017) rapidly be- scores generated by gradient-based (Simonyan et al., 2013; Bach et al., 2015; Shrikumar et al., coming the dominant architecture due to their efficient parallel training and ability to effec- 2017) or perturbation-based interpretation methods (Ribeiro et al., 2016; Li et al., 2016), which tivelymodel long sequences. Following the release can help with visualizing the relative importance of BERT (Devlin et al., 2019) along with other Transformer-based models pretrained on large cor- of individual tokens in the input with respect to a target prediction (Wallace et al., 2019; Johnson pora (Liu et al., 2019; Lewis et al., 2020; Joshi et al., 2020; Tenney et al., 2020). However, only a et al., 2020; Lee et al., 2020), the most successful few studies have instead focused on visualizing the strategy on many NLP leaderboards has been to overall training dynami"
2021.emnlp-demo.26,P19-3007,0,0.215667,"despite some limitations regarding the explanatory rithms that compute the importance of model capabilities of the attention mechanism (Jain and components and different parts of the input Wallace, 2019; Wiegreffe and Pinter, 2019), the sequence. Case studies and feedback from visualization of its weights has also been shown to a user focus group indicate that the framebe beneficial in discovering learnt features (Clark work is useful, and suggest several improveet al., 2019; Voita et al., 2019), with promising rements. Our framework is available at: https: cent work focusing on Transformers (Vig, 2019; //github.com/raymondzmc/T3-Vis. Hoover et al., 2020). Besides the works on exploring what has been 1 Introduction learnt in the pretrained models, there are also sevApproaches through neural networks have made significant progress in the field of NLP, with Trans- eral visualization tools developed to show saliency former models (Vaswani et al., 2017) rapidly be- scores generated by gradient-based (Simonyan et al., 2013; Bach et al., 2015; Shrikumar et al., coming the dominant architecture due to their efficient parallel training and ability to effec- 2017) or perturbation-based interpretatio"
2021.emnlp-demo.26,P19-1580,0,0.0224885,"et al., 2019). Similarly, den states, attention) through interactive visualization, and allows a suite of built-in algodespite some limitations regarding the explanatory rithms that compute the importance of model capabilities of the attention mechanism (Jain and components and different parts of the input Wallace, 2019; Wiegreffe and Pinter, 2019), the sequence. Case studies and feedback from visualization of its weights has also been shown to a user focus group indicate that the framebe beneficial in discovering learnt features (Clark work is useful, and suggest several improveet al., 2019; Voita et al., 2019), with promising rements. Our framework is available at: https: cent work focusing on Transformers (Vig, 2019; //github.com/raymondzmc/T3-Vis. Hoover et al., 2020). Besides the works on exploring what has been 1 Introduction learnt in the pretrained models, there are also sevApproaches through neural networks have made significant progress in the field of NLP, with Trans- eral visualization tools developed to show saliency former models (Vaswani et al., 2017) rapidly be- scores generated by gradient-based (Simonyan et al., 2013; Bach et al., 2015; Shrikumar et al., coming the dominant architec"
2021.emnlp-demo.26,D19-3002,0,0.0340555,"Missing"
2021.emnlp-demo.26,D19-1002,0,0.0200907,"6; Kahng erties and behaviours. Our framework offers et al., 2017), with Aken et al. (2020) visualizing an intuitive overview that allows the user to the differences of token representations from differexplore different facets of the model (e.g., hident layers of BERT (Devlin et al., 2019). Similarly, den states, attention) through interactive visualization, and allows a suite of built-in algodespite some limitations regarding the explanatory rithms that compute the importance of model capabilities of the attention mechanism (Jain and components and different parts of the input Wallace, 2019; Wiegreffe and Pinter, 2019), the sequence. Case studies and feedback from visualization of its weights has also been shown to a user focus group indicate that the framebe beneficial in discovering learnt features (Clark work is useful, and suggest several improveet al., 2019; Voita et al., 2019), with promising rements. Our framework is available at: https: cent work focusing on Transformers (Vig, 2019; //github.com/raymondzmc/T3-Vis. Hoover et al., 2020). Besides the works on exploring what has been 1 Introduction learnt in the pretrained models, there are also sevApproaches through neural networks have made significan"
2021.naacl-main.326,K16-1028,0,0.131844,"Missing"
2021.naacl-main.326,2020.tacl-1.15,0,0.0243209,"In this paper, we also tackle the data sparsity problem in discourse parsing, however, using a significantly different approach. First, instead of relying on sentiment, we leverage the task of extractive summarization. Second, instead of a method for distant supervision, we propose an unsupervised approach. The area of unsupervised RST-style discourse parsing has been mostly underlooked in the past, with recent neural approaches either taking advantage of pre-trained language models to predict discourse (Kobayashi et al., 2019) or using pre-trained syntactic parsers and linguistic knowledge (Nishida and Nakayama, 2020) to infer discourse trees in an unsupervsied manner. Similarly. our proposal only relies on a pre-trained neural summarization model to generate discourse trees. Recent neural summarization models are typically based on transformers (Liu and Lapata, 2019a; Zhang et al., 2019). One advantage of these mod4140 els is that they learn the relationship between input units explicitly using the dot-product self-attention, which allows for some degree of exploration of the inner working of these complex and distributed models. Here, we investigate if the attention matrices of a transformer-based summar"
2021.naacl-main.326,W18-5431,0,0.0287894,"tion have also been confirmed for neural summarizers, e.g. in Xiao and Carenini (2019) and Cohan et al. (2018), using the structure of scientific papers (i.e. sections), and in Xu et al. (2020), successfully incorporating RST-style discourse and co-reference information in the BERTSUM summarizer (Liu and Lapata, 2019b). applied to new documents. While our focus is on discourse, extracting syntactic constituency and dependency trees from transformer-based models has been recently attempted in both, machine translation and language modelling. In machine translation, Mareˇcek and Rosa (2019) and Raganato and Tiedemann (2018) show that trained translation models can capture syntactic information within their attention heads, using the CKY and CLE algorithms, respectively. In pre-trained language models, Wu et al. (2020) propose a parameter-free probing method to construct syntactic dependency trees based on a pretrained BERT model, only briefly elaborating on possible applications to discourse. In contrast to our work, they do not directly use attention heads, but instead build an impact matrix based on the distance between token representations. Furthermore, while their BERT-based model cannot deal with long sequ"
2021.naacl-main.326,N09-1064,0,0.642753,"Missing"
2021.naacl-main.326,P17-2029,0,0.120186,"onstituency trees, they can be converted into dependency trees with near isomorphic transformations. In this work, we infer both, constituency and dependency trees. Over the past decades, RST discourse parsing has been mainly focusing on supervised models, typically trained and tested within the same domain using human annotated discourse treebanks, such as RST-DT (Carlson et al., 2002), Instruction-DT (Subba and Di Eugenio, 2009) or GUM (Zeldes, 2017). The intra-domain performance of these supervised models has consistently improved, with a mix of traditional models by Joty et al. (2015) and Wang et al. (2017), and neural models (Yu et al., 2018) reaching state-of-the-art (SOTA) results. Yet, these approaches do not generalize well inter-domain (Huber and Carenini, 2020), likely due to the limited amount of available training data. Huber and Carenini (2019) recently tackled this data-sparsity issue through automatically generated discourse structures from distant supervision, showing that sentiment information can be used to infer discourse trees. Improving on their initial results, Huber and Carenini (2020) published a large-scale, distantly supervised discourse corpus (MEGA-DT), showing that a pa"
2021.naacl-main.326,2020.acl-main.383,0,0.0248437,"orporating RST-style discourse and co-reference information in the BERTSUM summarizer (Liu and Lapata, 2019b). applied to new documents. While our focus is on discourse, extracting syntactic constituency and dependency trees from transformer-based models has been recently attempted in both, machine translation and language modelling. In machine translation, Mareˇcek and Rosa (2019) and Raganato and Tiedemann (2018) show that trained translation models can capture syntactic information within their attention heads, using the CKY and CLE algorithms, respectively. In pre-trained language models, Wu et al. (2020) propose a parameter-free probing method to construct syntactic dependency trees based on a pretrained BERT model, only briefly elaborating on possible applications to discourse. In contrast to our work, they do not directly use attention heads, but instead build an impact matrix based on the distance between token representations. Furthermore, while their BERT-based model cannot deal with long sequences, our two-level encoder can effectively deal with sequences of any length, which is critical in discourse. 3 3.1 Our Model Framework Overview Our main goal is to show the ability of a previousl"
2021.naacl-main.326,D19-1298,1,0.839917,"an be used to derive discourse trees for arbitrary documents. Marcu (1999) pioneered the idea to directly apply RST-style discourse parsing to extractive summarization, and empirically showed that RST discourse information can benefit the summarization task, by simply extracting EDUs along the nucleus path. This initial success was followed by further work on leveraging discourse parsing in summarization, including McDonald (2007), Hirao et al. (2013), and Kikuchi et al. (2014). More recently, the benefits of discourse for summarization have also been confirmed for neural summarizers, e.g. in Xiao and Carenini (2019) and Cohan et al. (2018), using the structure of scientific papers (i.e. sections), and in Xu et al. (2020), successfully incorporating RST-style discourse and co-reference information in the BERTSUM summarizer (Liu and Lapata, 2019b). applied to new documents. While our focus is on discourse, extracting syntactic constituency and dependency trees from transformer-based models has been recently attempted in both, machine translation and language modelling. In machine translation, Mareˇcek and Rosa (2019) and Raganato and Tiedemann (2018) show that trained translation models can capture syntact"
2021.naacl-main.326,2020.codi-1.13,1,0.827899,"ations. Furthermore, while their BERT-based model cannot deal with long sequences, our two-level encoder can effectively deal with sequences of any length, which is critical in discourse. 3 3.1 Our Model Framework Overview Our main goal is to show the ability of a previously trained summarization model to be directly applied to the task of RST-style discourse parsing. Along this line, we explore the relationship between information learned by the transformer-based sumarizer and the task of discourse parsing. We leverage the synergies between units learned in the transformer model by following Xiao et al. (2020), previously proposing the use of a transformer documentencoder on top of a pretrained BERT EDU encoder. This standard summarization model is presented in Figure 1 (left). In the transformer-based document encoder, each head internally contains a selfattention matrix, learned during the training of the summarization model, representing the relationship Admittedly, Liu and Lapata (2018) and Liu et al. between EDUs (Figure 1 (center)). In this paper, (2019b) presented preliminary work on inferring we analyze these learned self-attention matrices, discourse structures from attention mechanisms, n"
2021.naacl-main.326,2020.acl-main.451,0,0.277984,"-style discourse parsing to extractive summarization, and empirically showed that RST discourse information can benefit the summarization task, by simply extracting EDUs along the nucleus path. This initial success was followed by further work on leveraging discourse parsing in summarization, including McDonald (2007), Hirao et al. (2013), and Kikuchi et al. (2014). More recently, the benefits of discourse for summarization have also been confirmed for neural summarizers, e.g. in Xiao and Carenini (2019) and Cohan et al. (2018), using the structure of scientific papers (i.e. sections), and in Xu et al. (2020), successfully incorporating RST-style discourse and co-reference information in the BERTSUM summarizer (Liu and Lapata, 2019b). applied to new documents. While our focus is on discourse, extracting syntactic constituency and dependency trees from transformer-based models has been recently attempted in both, machine translation and language modelling. In machine translation, Mareˇcek and Rosa (2019) and Raganato and Tiedemann (2018) show that trained translation models can capture syntactic information within their attention heads, using the CKY and CLE algorithms, respectively. In pre-trained"
2021.naacl-main.326,C18-1047,0,0.0150751,"d into dependency trees with near isomorphic transformations. In this work, we infer both, constituency and dependency trees. Over the past decades, RST discourse parsing has been mainly focusing on supervised models, typically trained and tested within the same domain using human annotated discourse treebanks, such as RST-DT (Carlson et al., 2002), Instruction-DT (Subba and Di Eugenio, 2009) or GUM (Zeldes, 2017). The intra-domain performance of these supervised models has consistently improved, with a mix of traditional models by Joty et al. (2015) and Wang et al. (2017), and neural models (Yu et al., 2018) reaching state-of-the-art (SOTA) results. Yet, these approaches do not generalize well inter-domain (Huber and Carenini, 2020), likely due to the limited amount of available training data. Huber and Carenini (2019) recently tackled this data-sparsity issue through automatically generated discourse structures from distant supervision, showing that sentiment information can be used to infer discourse trees. Improving on their initial results, Huber and Carenini (2020) published a large-scale, distantly supervised discourse corpus (MEGA-DT), showing that a parser trained on such treebank deliver"
2021.naacl-main.326,P19-1499,0,0.0180417,"an unsupervised approach. The area of unsupervised RST-style discourse parsing has been mostly underlooked in the past, with recent neural approaches either taking advantage of pre-trained language models to predict discourse (Kobayashi et al., 2019) or using pre-trained syntactic parsers and linguistic knowledge (Nishida and Nakayama, 2020) to infer discourse trees in an unsupervsied manner. Similarly. our proposal only relies on a pre-trained neural summarization model to generate discourse trees. Recent neural summarization models are typically based on transformers (Liu and Lapata, 2019a; Zhang et al., 2019). One advantage of these mod4140 els is that they learn the relationship between input units explicitly using the dot-product self-attention, which allows for some degree of exploration of the inner working of these complex and distributed models. Here, we investigate if the attention matrices of a transformer-based summarizer effectively capture discourse information (i.e., how strongly EDUs are related) and therefore can be used to derive discourse trees for arbitrary documents. Marcu (1999) pioneered the idea to directly apply RST-style discourse parsing to extractive summarization, and emp"
2021.sigdial-1.18,Q19-1011,0,0.0239381,"Missing"
2021.sigdial-1.18,P05-1018,0,0.114139,"real-world dialogues to make our proposal more suitable for conversational data. Another line of research explores casting DTS as a topic tracking problem (Khan et al., 2015; Takanobu et al., 2018), with the predefined conversation topics as part of the supervisory signals. Even though they have achieved SOTA performance on the in-distribution data, their reliability on the out-of-distribution data is rather poor. In contrast, our proposal does not require any prior knowledge (i.e., predefined topics) as input, so it is more transferable to out-of-distribution data. Coherence Scoring Early on Barzilay and Lapata (2005, 2008) observed that particular patterns of grammatical role transition for entities can reveal the coherence of monologue documents. Hence, they proposed the entity-grid approach by using entity role transitions mined from documents as the features for document coherence scoring. Later, Cervone and Riccardi (2020) explored the potential of the entity-grid approach on conversational data and further proved that it was also suitable for dialogues. However, one key limitation of the entity-grid model is that by excessively relying on the identification of entity tokens and their corresponding r"
2021.sigdial-1.18,J08-1001,0,0.203892,"Missing"
2021.sigdial-1.18,D18-1547,0,0.0317331,"Missing"
2021.sigdial-1.18,2020.sigdial-1.21,0,0.0363869,"ormance on the in-distribution data, their reliability on the out-of-distribution data is rather poor. In contrast, our proposal does not require any prior knowledge (i.e., predefined topics) as input, so it is more transferable to out-of-distribution data. Coherence Scoring Early on Barzilay and Lapata (2005, 2008) observed that particular patterns of grammatical role transition for entities can reveal the coherence of monologue documents. Hence, they proposed the entity-grid approach by using entity role transitions mined from documents as the features for document coherence scoring. Later, Cervone and Riccardi (2020) explored the potential of the entity-grid approach on conversational data and further proved that it was also suitable for dialogues. However, one key limitation of the entity-grid model is that by excessively relying on the identification of entity tokens and their corresponding roles, its performance can be reduced by errors from other NLP pre-processing tasks, like coreference resolution, which can be very noisy. In order to resolve this limitation, researchers have explored scoring a document coherence by measuring and aggregating the coherence of its adjacent text pairs (e.g., Xu et al."
2021.sigdial-1.18,N19-1423,0,0.00594605,"ing real daily communications and covering various topics (proposed in Li et al. (2017) and Wang et al. (2021)). In particular, all the adjacent utterance pairs are firstly extracted to form the positive sample set. Then for each positive sample, the corresponding negative samples are generated by replacing the subsequent turn in the positive sample with (1) an non-adjacent turn randomly picked from the same dialogue, and (2) a turn randomly picked from another dialogue talking about another topic. Once the training corpus is ready, we re-purpose the Next Sentence Prediction (NSP) BERT model (Devlin et al., 2019) as the basic framework of our utteracepair coherence scoring model. After fine-tuning the pretrained NSP BERT on our automatically generated training corpus with the marginal ranking loss, the resulting model can then be applied to produce the topical coherence score for all the consecutive utterance pairs in any given dialogue. Such scores can finally be used for the inference of topic segmentation for that dialogue. We empirically test the popular TextTiling algorithm (Hearst, 1997) enhanced by the supervisory signal provided by our learned utterance-pair coherence scoring model on two lang"
2021.sigdial-1.18,2020.aacl-main.28,0,0.0157013,"enefit of our proposal taking dialogue flows into consideration in the training process. Consider (U7, U8) as an example, the first three segmenters tend to assign relatively high depth score (low coherence) to this utterance pair due to the very little content overlap between them. However, our method manages to assign this pair the minimal depth score. This is because such utterance pair is a Questions-Inform in the Dialog Flow, thus even if there is very limited content in common, the two utterances should still very likely belong to the same topic segment. 5 generation (Qiao et al., 2020; Ji et al., 2020b) and summarization (Ji et al., 2020a). Acknowledgments We thank the anonymous reviewers and the UBCNLP group for their insightful comments and suggestions. This research was supported by the Language & Speech Innovation Lab of Cloud BU, Huawei Technologies Co., Ltd. Conclusions and Future Work This paper addresses a key limitation of unsupervised dialogue topic segmenters, namely their inability to model topical coherence among utterances in the dialogue. To this end, we leverage signals learned from a neural utterance-pair coherence scoring model based on fine-tuning NSP BERT. With no data"
2021.sigdial-1.18,N13-1019,0,0.0250537,"elated Work Dialogue Topic Segmentation (DTS) Similar to the topic segmentation for monologue, dialogue topic segmentation aims to segment a dialogue session into the topical-coherent units. Therefore, a wide variety of approaches which were originally proposed for monologue topic segmentation, have also been widely applied to conversational corpora. Early approaches, due to lack of training data, are usually unsupervised and exploit the word co-occurrence statistics (Hearst, 1997; Galley et al., 2003; Eisenstein and Barzilay, 2008) or sentences’ topical distribution (Riedl and Biemann, 2012; Du et al., 2013) to measure the sentence similarity between turns, so that topical or semantic changes can be detected. More recently, with the availability of large-scale corpora sampled from Wikipedia, by taking the section mark as the ground-truth segment boundary (Koshorek et al., 2018; Arnold et al., 2019), there has been a rapid growth in supervised approaches for monologue topic segmentation, especially neural-based approaches (Koshorek et al., 2018; Badjatiya et al., 2018; Arnold et al., 2019). These supervised solutions are favored by researchers due to their more robust performance and efficiency. H"
2021.sigdial-1.18,2020.emnlp-main.54,0,0.011371,"enefit of our proposal taking dialogue flows into consideration in the training process. Consider (U7, U8) as an example, the first three segmenters tend to assign relatively high depth score (low coherence) to this utterance pair due to the very little content overlap between them. However, our method manages to assign this pair the minimal depth score. This is because such utterance pair is a Questions-Inform in the Dialog Flow, thus even if there is very limited content in common, the two utterances should still very likely belong to the same topic segment. 5 generation (Qiao et al., 2020; Ji et al., 2020b) and summarization (Ji et al., 2020a). Acknowledgments We thank the anonymous reviewers and the UBCNLP group for their insightful comments and suggestions. This research was supported by the Language & Speech Innovation Lab of Cloud BU, Huawei Technologies Co., Ltd. Conclusions and Future Work This paper addresses a key limitation of unsupervised dialogue topic segmenters, namely their inability to model topical coherence among utterances in the dialogue. To this end, we leverage signals learned from a neural utterance-pair coherence scoring model based on fine-tuning NSP BERT. With no data"
2021.sigdial-1.18,D08-1035,0,0.510939,"rent together than the utterances about different topics (Hearst, 1997; Purver et al., 2006). Hence, effectively modeling the coherence among utterances becomes the key ingredient of a successful DTS model. However, the performances of the prior unsupervised DTS models are usually limited since the coherence measurements between utterances 167 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 167–177 July 29–31, 2021. ©2021 Association for Computational Linguistics are typically based on surface features (eg,. lexical overlap) (Hearst, 1997; Eisenstein and Barzilay, 2008) or word-level semantics (Song et al., 2016; Xu et al., 2021). Even though these features are easy to extract and thus make models more generally applicable, they can only reflect the coherence between utterances in a rather shallow way. More recently, there is work departing from the unsupervised setting by casting DTS as a weakly supervised learning task and utilizing a RL-based neural model as the basic framework (Takanobu et al., 2018). However, while this approach has been at least partially successful on goal-oriented dialogues when provided with predefined in-domain topics, it cannot de"
2021.sigdial-1.18,W17-5506,0,0.0444586,"Missing"
2021.sigdial-1.18,N18-2075,0,0.120757,"opically coherent pieces. An example is given in Table 1. Topic transition happens after Turn-4 and Turn-6, where the topic is correspondingly switched from “the requirement of the insurance coverage” to “the information presented on the insurance card”, and then to “the way of submitting the insurance card”. Dialogue topic segmentation plays a vital role for a variety of downstream dialogue-related NLP tasks, such as dialogue generation (Li et al., 2016), summarization (Bokaei et al., 2016) and response prediction (Xu et al., 2021). Different from the monologue topic segmentation (MTS) task (Koshorek et al., 2018; Xing et al., 1 Our code, proposed fine-tuned models and data can be found at https://github.com/lxing532/ Dialogue-Topic-Segmenter. 2020), the shortage of labeled dialogue corpora has always been a very serious problem for DTS. Collecting annotations about topic shifting between the utterances of dialogues is highly expensive and time-consuming. Hence, most of the proposed labeled datasets for DTS are typically used for model evaluation rather than training. They are either small in size (Xu et al., 2021) or artificially generated and possibly noisy (Feng et al., 2020). Because of the lack o"
2021.sigdial-1.18,2020.emnlp-main.652,0,0.0753663,"Missing"
2021.sigdial-1.18,P03-1071,0,0.172493,"s of our proposal seems to come from better capturing topical relations and consideration for dialogue flows. 2 Related Work Dialogue Topic Segmentation (DTS) Similar to the topic segmentation for monologue, dialogue topic segmentation aims to segment a dialogue session into the topical-coherent units. Therefore, a wide variety of approaches which were originally proposed for monologue topic segmentation, have also been widely applied to conversational corpora. Early approaches, due to lack of training data, are usually unsupervised and exploit the word co-occurrence statistics (Hearst, 1997; Galley et al., 2003; Eisenstein and Barzilay, 2008) or sentences’ topical distribution (Riedl and Biemann, 2012; Du et al., 2013) to measure the sentence similarity between turns, so that topical or semantic changes can be detected. More recently, with the availability of large-scale corpora sampled from Wikipedia, by taking the section mark as the ground-truth segment boundary (Koshorek et al., 2018; Arnold et al., 2019), there has been a rapid growth in supervised approaches for monologue topic segmentation, especially neural-based approaches (Koshorek et al., 2018; Badjatiya et al., 2018; Arnold et al., 2019)"
2021.sigdial-1.18,S16-2016,0,0.0330835,"Missing"
2021.sigdial-1.18,J97-1003,0,0.334776,"en the utterances of dialogues is highly expensive and time-consuming. Hence, most of the proposed labeled datasets for DTS are typically used for model evaluation rather than training. They are either small in size (Xu et al., 2021) or artificially generated and possibly noisy (Feng et al., 2020). Because of the lack of training data, most previously proposed methods for DTS follow the unsupervised paradigm. The common assumption behind these unsupervised methods is that the utterances associated with the same topic should be more coherent together than the utterances about different topics (Hearst, 1997; Purver et al., 2006). Hence, effectively modeling the coherence among utterances becomes the key ingredient of a successful DTS model. However, the performances of the prior unsupervised DTS models are usually limited since the coherence measurements between utterances 167 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 167–177 July 29–31, 2021. ©2021 Association for Computational Linguistics are typically based on surface features (eg,. lexical overlap) (Hearst, 1997; Eisenstein and Barzilay, 2008) or word-level semantics (Song et al., 2"
2021.sigdial-1.18,2020.emnlp-main.742,0,0.0387958,"th score 2 means the pair of utterances are less topically related to each other. The threshold τ to identify segment boundaries is computed from the mean µ and standard deviation σ of depth scores: τ = µ− σ2 . A pair of utterances with the depth score over τ will be select to have a segment boundary in between. Next, we describe our novel training data generation strategy and the architecture of our new utterance-pair coherence scoring model, which are the two key contributions of this paper. 3.1 Training Data for Coherence Scoring We follow previous work (Wang et al., 2017; Xu et al., 2019; Huang et al., 2020) to optimize the 169 utterance-pair coherence scoring model (described in Section 3.2) with marginal ranking loss. Formally, the coherence scoring model CS receives two utterances (u1 , u2 ) as input and return the coherence score c = CS(u1 , u2 ), which reflects the topical relevance of this pair of utterances. Due to the lack of corpora labeled with ground-truth coherence scores, we follow the strategy in Wang et al. (2017) to train CS based on the pairwise ranking with ordering relations of coherence between utterance pairs as supervisory signals. In order to create the training data labele"
2021.sigdial-1.18,D16-1127,0,0.0383512,"ceived considerable attention in recent years. In essence, DTS aims to reveal the topic structure of a dialogue by segmenting the dialogue session into its topically coherent pieces. An example is given in Table 1. Topic transition happens after Turn-4 and Turn-6, where the topic is correspondingly switched from “the requirement of the insurance coverage” to “the information presented on the insurance card”, and then to “the way of submitting the insurance card”. Dialogue topic segmentation plays a vital role for a variety of downstream dialogue-related NLP tasks, such as dialogue generation (Li et al., 2016), summarization (Bokaei et al., 2016) and response prediction (Xu et al., 2021). Different from the monologue topic segmentation (MTS) task (Koshorek et al., 2018; Xing et al., 1 Our code, proposed fine-tuned models and data can be found at https://github.com/lxing532/ Dialogue-Topic-Segmenter. 2020), the shortage of labeled dialogue corpora has always been a very serious problem for DTS. Collecting annotations about topic shifting between the utterances of dialogues is highly expensive and time-consuming. Hence, most of the proposed labeled datasets for DTS are typically used for model evalua"
2021.sigdial-1.18,I17-1099,0,0.33203,"tively capture the deeper semantic (topical) relations between them. Due to the absence of supervision, we propose a simple yet effective strategy to generate a training corpus for the utterance-pair coherence scoring task, with the paired coherent/notutterance pairs as datapoints. Then, after applying such strategy, we use the resulting corpus to train an utterance-pair coherence scoring model with the relative ranking objective (Li, 2011). In practice, we create a training corpus from large conversational datasets containing real daily communications and covering various topics (proposed in Li et al. (2017) and Wang et al. (2021)). In particular, all the adjacent utterance pairs are firstly extracted to form the positive sample set. Then for each positive sample, the corresponding negative samples are generated by replacing the subsequent turn in the positive sample with (1) an non-adjacent turn randomly picked from the same dialogue, and (2) a turn randomly picked from another dialogue talking about another topic. Once the training corpus is ready, we re-purpose the Next Sentence Prediction (NSP) BERT model (Devlin et al., 2019) as the basic framework of our utteracepair coherence scoring model"
2021.sigdial-1.18,J02-1002,0,0.213798,"nsecutive utterance pairs. TeT + CLS (Xu et al., 2021): TextTiling enhanced by the pretrained BERT sentence encoder, by using output embeddings of BERT encoder to compute semantic similarity for consecutive utterance pairs. TeT + NSP: TextTiling enhanced by the pretrained BERT for Next Sentence Prediction (NSP), by leveraging the output probability to represent the semantic coherence for consecutive utterance pairs. 4.3 Evaluation Metrics We apply three standard metrics to evaluate the performances of our proposal and baselines. They are: Pk error score (Beeferman et al., 1999), WinDiff (WD) (Pevzner and Hearst, 2002) and F1 score (macro). Pk and WD are both calculated based on the overlap between ground-truth segments and model’s predictions within a certain size sliding window. Since they are both penalty metrics, lower score indicates better performance. F1 is the standard armonic mean of precision and recall, with higher scores indicating better performance 4.4 Experimental Setup We fine-tune the utterance-pair coherence scoring model on BERTbase which consists of 12 layers and 12 heads in each layer. The hidden dimension of BERTbase is 768. Training is executed with AdamW (Loshchilov and Hutter, 2019)"
2021.sigdial-1.18,P06-1003,0,0.0902248,"Missing"
2021.sigdial-1.18,2020.findings-emnlp.299,0,0.0390347,"tion is about the benefit of our proposal taking dialogue flows into consideration in the training process. Consider (U7, U8) as an example, the first three segmenters tend to assign relatively high depth score (low coherence) to this utterance pair due to the very little content overlap between them. However, our method manages to assign this pair the minimal depth score. This is because such utterance pair is a Questions-Inform in the Dialog Flow, thus even if there is very limited content in common, the two utterances should still very likely belong to the same topic segment. 5 generation (Qiao et al., 2020; Ji et al., 2020b) and summarization (Ji et al., 2020a). Acknowledgments We thank the anonymous reviewers and the UBCNLP group for their insightful comments and suggestions. This research was supported by the Language & Speech Innovation Lab of Cloud BU, Huawei Technologies Co., Ltd. Conclusions and Future Work This paper addresses a key limitation of unsupervised dialogue topic segmenters, namely their inability to model topical coherence among utterances in the dialogue. To this end, we leverage signals learned from a neural utterance-pair coherence scoring model based on fine-tuning NSP BE"
2021.sigdial-1.18,W12-3307,0,0.0300566,"n for dialogue flows. 2 Related Work Dialogue Topic Segmentation (DTS) Similar to the topic segmentation for monologue, dialogue topic segmentation aims to segment a dialogue session into the topical-coherent units. Therefore, a wide variety of approaches which were originally proposed for monologue topic segmentation, have also been widely applied to conversational corpora. Early approaches, due to lack of training data, are usually unsupervised and exploit the word co-occurrence statistics (Hearst, 1997; Galley et al., 2003; Eisenstein and Barzilay, 2008) or sentences’ topical distribution (Riedl and Biemann, 2012; Du et al., 2013) to measure the sentence similarity between turns, so that topical or semantic changes can be detected. More recently, with the availability of large-scale corpora sampled from Wikipedia, by taking the section mark as the ground-truth segment boundary (Koshorek et al., 2018; Arnold et al., 2019), there has been a rapid growth in supervised approaches for monologue topic segmentation, especially neural-based approaches (Koshorek et al., 2018; Badjatiya et al., 2018; Arnold et al., 2019). These supervised solutions are favored by researchers due to their more robust performance"
2021.sigdial-1.18,2021.naacl-main.28,0,0.0228452,"rance-Pair Coherence Scoring Model As illustrated in Figure 1(a), we choose the Next Sentence Prediction (NSP) BERT (Devlin et al., 2019) (trained for the Next Sentence Prediction task) as the basic framework of our utterance-pair coherence scoring model due to the similarity of these two tasks5 . They both take a pair of sentences/utterances as input and only a topically re5 Instead of NSP BERT (a cross-encoder), we could have also modelled such pairwise scoring with a bi-encoder, which first encodes each utterance independently. We eventually selected the cross-encoder due to the results in Thakur et al. (2021) showing that cross-encoders usually outperform biencoders for pairwise sentence scoring. lated sentence should be predicted as the appropriate next sentence. We first initialize the model with BERTbase , which was pretrained on multibillion publicly available data. At the fine-tuning stage, we expect the model to learn to discriminate the positive utterance pairs from their corresponding negative pairs. More specifically, the − positive (si , t+ i ) and negative (si , ti ) as instances are fed into the model respectively in the form of +/− ([CLS]||si ||[SEP]||ti ||[SEP]), where ||denotes the"
2021.sigdial-1.18,D17-1139,0,0.220046,"has been at least partially successful on goal-oriented dialogues when provided with predefined in-domain topics, it cannot deal effectively with more general open-domain dialogues. To alleviate the aforementioned limitations in previous work, in this paper, we still cast DTS as an unsupervised learning task to make it applicable to dialogues from diverse domains and resources. However, instead of merely utilizing shallow features for coherence prediction, we leverage the supervised information from the text-pair coherence scoring task (i.e., measuring the coherence of adjacent textual units (Wang et al., 2017; Xu et al., 2019; Wang et al., 2020)), which can more effectively capture the deeper semantic (topical) relations between them. Due to the absence of supervision, we propose a simple yet effective strategy to generate a training corpus for the utterance-pair coherence scoring task, with the paired coherent/notutterance pairs as datapoints. Then, after applying such strategy, we use the resulting corpus to train an utterance-pair coherence scoring model with the relative ranking objective (Li, 2011). In practice, we create a training corpus from large conversational datasets containing real da"
2021.sigdial-1.18,2020.emnlp-main.533,0,0.0897162,"Missing"
2021.sigdial-1.18,2020.aacl-main.63,1,0.820371,"Missing"
2021.sigdial-1.18,W17-4406,1,0.848811,"Missing"
2021.sigdial-1.18,P19-1067,0,0.336139,"artially successful on goal-oriented dialogues when provided with predefined in-domain topics, it cannot deal effectively with more general open-domain dialogues. To alleviate the aforementioned limitations in previous work, in this paper, we still cast DTS as an unsupervised learning task to make it applicable to dialogues from diverse domains and resources. However, instead of merely utilizing shallow features for coherence prediction, we leverage the supervised information from the text-pair coherence scoring task (i.e., measuring the coherence of adjacent textual units (Wang et al., 2017; Xu et al., 2019; Wang et al., 2020)), which can more effectively capture the deeper semantic (topical) relations between them. Due to the absence of supervision, we propose a simple yet effective strategy to generate a training corpus for the utterance-pair coherence scoring task, with the paired coherent/notutterance pairs as datapoints. Then, after applying such strategy, we use the resulting corpus to train an utterance-pair coherence scoring model with the relative ranking objective (Li, 2011). In practice, we create a training corpus from large conversational datasets containing real daily communication"
C16-1245,H91-1060,0,0.402425,"Missing"
C16-1245,D14-1124,1,0.919762,"requires a filtering step to ensure that only “good quality” unlabeled documents can be used for enrichment and re-training. We propose and evaluate two ways to perform the filtering. The first is to use an agreement score between the two parsers. The second is to use only the confidence score of the faster parser. Our empirical results show that agreement score can help to boost the performance on infrequent relations, and that the confidence score is a viable approximation of the agreement score for infrequent relations. 1 Introduction Discourse parsing is widely used in text understanding (Allen et al., 2014), sentiment analysis (Bhatia et al., 2015) and other NLP tasks (Guzm´an et al., 2014) (Gerani et al., 2016). A multi-sentential discourse parser takes a document as input, and returns its discourse structure that shows how clauses and sentences are related in the document, via the use of various discourse relations. For instance, the benchmark RST-DT dataset (Carlson et al., 2001) uses 18 discourse relations. Studies in the past decade on discourse parsing, such as (Ji and Eisenstein, 2014), (Feng and Hirst, 2014), and (Joty et al., 2015), greatly improved the performance of discourse parsing"
C16-1245,D15-1263,0,0.0514061,"Missing"
C16-1245,W01-1605,0,0.441667,"Missing"
C16-1245,P14-1048,0,0.423584,"relations. 1 Introduction Discourse parsing is widely used in text understanding (Allen et al., 2014), sentiment analysis (Bhatia et al., 2015) and other NLP tasks (Guzm´an et al., 2014) (Gerani et al., 2016). A multi-sentential discourse parser takes a document as input, and returns its discourse structure that shows how clauses and sentences are related in the document, via the use of various discourse relations. For instance, the benchmark RST-DT dataset (Carlson et al., 2001) uses 18 discourse relations. Studies in the past decade on discourse parsing, such as (Ji and Eisenstein, 2014), (Feng and Hirst, 2014), and (Joty et al., 2015), greatly improved the performance of discourse parsing in general. However, it has been observed that the performance across the discourse relations varies significantly (Joty et al., 2015), and that poor performance may be linked to underfitting, i.e., a lack of training data (Feng and Hirst, 2014). In this paper, we investigate the underfitting hypothesis and study how to improve the situation. Different discourse relations are usually unevenly distributed in a dataset, and some of them occur much less frequently than other relations. We call the former the infreque"
C16-1245,P14-1065,0,0.0608648,"Missing"
C16-1245,D10-1039,0,0.0191785,"T-DT, PDTB and unlabeled data together through multi-task learning process, and gets performance improvements on relatively infrequent relations, though they only apply their scheme on the four coarse top-level relations. Their scheme is based on retrieving more training instances from unlabeled data through cue phrases. This approach of using explicit examples to predict implicit examples has been shown to produce mixed results (Sporleder and Lascarides, 2008). Moreover, (Joty et al., 2015) has shown that there are many more features beyond cue phrases that are useful for discourse parsing. (Hernault et al., 2010) proposes a feature vector extension approach to improve classification of infrequent discourse relations. The approach is based on word co-occurrence. Partly because a simple discourse parser was used, their approach is shown to produce only minimal improvements in performance. Unlike (Liu et al., 2016) and (Hernault et al., 2010), we aim to exploit more advanced parsers with higher performance, and also keep the finer-granularity of the relations, especially focusing on the infrequent relations. We employ the idea of co-training, which is first introduced by (Blum and Mitchell, 1998) with it"
C16-1245,P14-1002,0,0.384944,"eement score for infrequent relations. 1 Introduction Discourse parsing is widely used in text understanding (Allen et al., 2014), sentiment analysis (Bhatia et al., 2015) and other NLP tasks (Guzm´an et al., 2014) (Gerani et al., 2016). A multi-sentential discourse parser takes a document as input, and returns its discourse structure that shows how clauses and sentences are related in the document, via the use of various discourse relations. For instance, the benchmark RST-DT dataset (Carlson et al., 2001) uses 18 discourse relations. Studies in the past decade on discourse parsing, such as (Ji and Eisenstein, 2014), (Feng and Hirst, 2014), and (Joty et al., 2015), greatly improved the performance of discourse parsing in general. However, it has been observed that the performance across the discourse relations varies significantly (Joty et al., 2015), and that poor performance may be linked to underfitting, i.e., a lack of training data (Feng and Hirst, 2014). In this paper, we investigate the underfitting hypothesis and study how to improve the situation. Different discourse relations are usually unevenly distributed in a dataset, and some of them occur much less frequently than other relations. We call"
C16-1245,P13-1048,1,0.846668,"of the SR-parser. The rationale is that while the CODRA parser is generally more accurate than the SR-parser, the SRparser is two orders of magnitude faster. If a high-enough threshold on the confidence score of the SR-parser is used for enrichment, Section 4 will investigate whether the confidence score is a good approximation of the agreement score. If this approach is successful, an even larger number of unlabeled documents can be parsed rapidly to be used for re-training. 2 Related work Recent discourse parsers have improved the overall performance of discourse parsing in different ways. (Joty et al., 2013) (Joty et al., 2015) proposed a two-stage document-level discourse parser CODRA, which builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. This approach achieves good performance in discourse relation labeling. Based on their idea, (Feng and Hirst, 2014) developed a similar but much faster model that adopts a greedy bottom-up approach, with two linear-chain CRFs applied in cascade as local classifiers. On the other hand, (Ji and Eisenstein, 2014)"
C16-1245,J15-3002,1,0.675759,"iscourse parsing is widely used in text understanding (Allen et al., 2014), sentiment analysis (Bhatia et al., 2015) and other NLP tasks (Guzm´an et al., 2014) (Gerani et al., 2016). A multi-sentential discourse parser takes a document as input, and returns its discourse structure that shows how clauses and sentences are related in the document, via the use of various discourse relations. For instance, the benchmark RST-DT dataset (Carlson et al., 2001) uses 18 discourse relations. Studies in the past decade on discourse parsing, such as (Ji and Eisenstein, 2014), (Feng and Hirst, 2014), and (Joty et al., 2015), greatly improved the performance of discourse parsing in general. However, it has been observed that the performance across the discourse relations varies significantly (Joty et al., 2015), and that poor performance may be linked to underfitting, i.e., a lack of training data (Feng and Hirst, 2014). In this paper, we investigate the underfitting hypothesis and study how to improve the situation. Different discourse relations are usually unevenly distributed in a dataset, and some of them occur much less frequently than other relations. We call the former the infrequent relations. For example"
C16-1245,P13-1047,0,0.013075,"a cue phrase like “because”, while the latter are not and consequentially are more difficult to identify. Several studies have been conducted to tackle the problem of classifying implicit relations which do not have many explicit features and examples. (Zhou et al., 2010) presents a method to predict the missing connective based on a language model trained on an unlabeled corpus. The predicted connective is then used as a feature to classify the implicit relation. (McKeown and Biran, 2013) tackles the feature sparsity problem by aggregating implicit relations into larger groups. And recently (Lan et al., 2013) combines different data through multi-task learning. The method performs implicit and explicit relation classification in the PDTB framework as two tasks and relies on multi-task learning to obtain higher performance. (Liu et al., 2016) proposes a multi-task neural networks that combines RST-DT, PDTB and unlabeled data together through multi-task learning process, and gets performance improvements on relatively infrequent relations, though they only apply their scheme on the four coarse top-level relations. Their scheme is based on retrieving more training instances from unlabeled data throug"
C16-1245,J93-2004,0,0.0536379,"Missing"
C16-1245,P13-2013,0,0.0172133,"implicit relations. A key distinction in discourse parsing is between explicit and implicit relations. The former are signaled by a cue phrase like “because”, while the latter are not and consequentially are more difficult to identify. Several studies have been conducted to tackle the problem of classifying implicit relations which do not have many explicit features and examples. (Zhou et al., 2010) presents a method to predict the missing connective based on a language model trained on an unlabeled corpus. The predicted connective is then used as a feature to classify the implicit relation. (McKeown and Biran, 2013) tackles the feature sparsity problem by aggregating implicit relations into larger groups. And recently (Lan et al., 2013) combines different data through multi-task learning. The method performs implicit and explicit relation classification in the PDTB framework as two tasks and relies on multi-task learning to obtain higher performance. (Liu et al., 2016) proposes a multi-task neural networks that combines RST-DT, PDTB and unlabeled data together through multi-task learning process, and gets performance improvements on relatively infrequent relations, though they only apply their scheme on"
C16-1245,prasad-etal-2008-penn,0,0.117319,"Missing"
C16-1245,P09-1027,0,0.0469914,"oduce only minimal improvements in performance. Unlike (Liu et al., 2016) and (Hernault et al., 2010), we aim to exploit more advanced parsers with higher performance, and also keep the finer-granularity of the relations, especially focusing on the infrequent relations. We employ the idea of co-training, which is first introduced by (Blum and Mitchell, 1998) with its application in helping the search engine better classify “academic course home page”. Similar co-training efforts have been found effective in many NLP problems when only a small amount of labeled data is available. For example, (Wan, 2009) proposes a co-training approach for cross-lingual 2605 sentiment classification, while (Li and Nenkova, 2015) applies co-training on predicting sentence specificity. 3 Our Enrichment Approach The workflow of our enrichment approach is shown in Figure 1. First we use the labeled data to provide initial training of the two parsers. Then each parser is used to produce a discourse tree for each unlabeled document. After that, we apply a filtering step to select those “high quality” discourse trees, which are added to the original labeled data to form the “enriched training data” to re-train the t"
C16-1245,C10-2172,0,0.0227699,"-art discourse parsers still perform badly on infrequent relations due to insufficient training examples. The problem of lacking training examples also impacts other aspects of discourse parsing, for example parsing implicit relations. A key distinction in discourse parsing is between explicit and implicit relations. The former are signaled by a cue phrase like “because”, while the latter are not and consequentially are more difficult to identify. Several studies have been conducted to tackle the problem of classifying implicit relations which do not have many explicit features and examples. (Zhou et al., 2010) presents a method to predict the missing connective based on a language model trained on an unlabeled corpus. The predicted connective is then used as a feature to classify the implicit relation. (McKeown and Biran, 2013) tackles the feature sparsity problem by aggregating implicit relations into larger groups. And recently (Lan et al., 2013) combines different data through multi-task learning. The method performs implicit and explicit relation classification in the PDTB framework as two tasks and relies on multi-task learning to obtain higher performance. (Liu et al., 2016) proposes a multi-"
C16-2001,P15-2113,1,0.887768,"Missing"
C16-2001,S16-1138,1,0.853194,"Missing"
C16-2001,D15-1068,1,0.89046,"Missing"
C16-2001,P03-1054,0,0.0264421,"larity value using a similarity matrix. The similarity and the embeddings along with other additional similarity features are then passed through a hidden layer and next to the output layer for classification. The qe and ce are learned by backpropagating the (cross entropy) errors from the output layer. qe and ce vectors are finally concatenated and used as features in our SVM model. Tree kernels We use tree kernels to measure the syntactic similarity between the question and the comment. First, we produce shallow syntactic trees for the question and for the comment using the Stanford parser (Klein and Manning, 2003). Following Severyn and Moschitti (2012), we link the two trees by connecting nodes such as NP, PP, VP, when there is at least one lexical overlap between the corresponding phrases of the trees, and we mark those links using a specific tag. The kernel function K is defined as: K((t1 , t2 ), (c1 , c2 )) = T K(t1 , c1 )+T K(t2 , c2 ), where T K(t, c) is a tree kernel function operating over a pair of question (t) and comment (c) trees.3 Classification Performance We evaluated our comment classifier on the SemEval-2016 Task 3 test set with the official scorer, obtaining the following results: MAP"
C16-2001,S15-2047,1,0.903863,"Missing"
C16-2001,S15-2036,1,0.910076,"Missing"
D08-1081,P08-1041,1,0.843747,"tion of the email that behaves as a unit in a fine-grain representation of the conversation structure. A fragment sometimes consists of an entire email and sometimes a portion of an email. For example, if a given email has the structure A &gt;B C where B is a quoted section in the middle of the email, then there are three email fragments in total: two new fragments A and C separated by one quoted fragment B. Sentences in a fragment are weighted according to the Clue Word Score (CWS) measure, a lexical cohesion metric based on the recurrence of words in parent and child nodes. In subsequent work, Carenini et al. (2008) determined that subjectivity detection (i.e., whether the sentence contains sentiments or opinions from the author) gave additional improvement for email thread summaries. Also on the Enron corpus, Zajic et al. (2008) compared Collective Message Summarization (CMS) to Individual Message Summarization (IMS) and found the former to be a more effective technique for summarizing email data. CMS essentially treats thread summarization as a multi-document summarization problem, while IMS summarizes individual emails in the thread and then concatenates them to form a thread summary. In our work desc"
D08-1081,W06-1643,0,0.13655,"question/answer detection. More recently, researchers have investigated the utility of employing speech-specific features for summarization, including prosodic information. Murray et al. (2005a; 2005b) compared purely textual summarization approaches with featurebased approaches incorporating prosodic features, with human judges favoring the feature-based approaches. In subsequent work (2006; 2007), they began to look at additional speech-specific characteristics such as speaker status, discourse markers and high-level meta comments in meetings, i.e. comments that refer to the meeting itself. Galley (2006) used skip-chain Conditional Random Fields to model pragmatic dependencies between paired meeting utterances (e.g. QUESTION-ANSWER relations), and used a combination of lexical, prosodic, structural and discourse features to rank utterances by importance. Galley found that while the most useful single feature class was lexical features, a combination of acoustic, durational and structural features exhibited comparable performance according to Pyramid evaluation. 2.2 Email Summarization Work on email summarization can be divided into summarization of individual email messages and summarization"
D08-1081,W01-0719,0,0.0128549,"ds to model pragmatic dependencies between paired meeting utterances (e.g. QUESTION-ANSWER relations), and used a combination of lexical, prosodic, structural and discourse features to rank utterances by importance. Galley found that while the most useful single feature class was lexical features, a combination of acoustic, durational and structural features exhibited comparable performance according to Pyramid evaluation. 2.2 Email Summarization Work on email summarization can be divided into summarization of individual email messages and summarization of entire email threads. Muresan et al. (2001) took the approach of summarizing individual email messages, first using linguistic techniques to extract noun phrases and then employing machine learning methods to label the extracted noun phrases as salient or not. Corston-Oliver et al. (2004) focused on identifying speech acts within a given email, with a particular interest in task-related sentences. 774 Rambow et al. (2004) addressed the challenge of summarizing entire threads by treating it as a binary sentence classification task. They considered three types of features: basic features that simply treat the email as text (e.g. tf.idf,"
D08-1081,W05-0905,1,0.460894,"Missing"
D08-1081,N06-1047,1,0.563948,"primary reasons. First, the differing summarization annotations in the AMI and Enron corpora naturally lend themselves to slightly divergent metrics, one based on extractabstract links and the other based on the essential/option/uninformative distinction. Second, and more importantly, using these two metrics allow us to compare our results with state-of-the-art results in the two fields of speech summarization and email summarization. In future work we plan to use a single evaluation metric. 3.3.1 Evaluating Meeting Summaries To evaluate meeting summaries we use the weighted f-measure metric (Murray et al., 2006). This evaluation scheme relies on the multiple human annotated summary links described in Section 3.2.2. Both weighted precision and recall share the same numerator num = N M X X L(si , aj ) (1) i=1 j=1 Weighted precision is defined as: precision = num N ·M (2) and weighted recall is given by num recall = PO PN i=1 j=1 L(si , aj ) (3) where O is the total number of DAs in the meeting, N is the number of annotators, and the denominator represents the total number of links made between DAs and abstract sentences by all annotators. The weighted f-measure is calculated as the harmonic mean of wei"
D08-1081,N04-1019,0,0.0666674,"Missing"
D08-1081,N04-4027,0,0.229977,"ed comparable performance according to Pyramid evaluation. 2.2 Email Summarization Work on email summarization can be divided into summarization of individual email messages and summarization of entire email threads. Muresan et al. (2001) took the approach of summarizing individual email messages, first using linguistic techniques to extract noun phrases and then employing machine learning methods to label the extracted noun phrases as salient or not. Corston-Oliver et al. (2004) focused on identifying speech acts within a given email, with a particular interest in task-related sentences. 774 Rambow et al. (2004) addressed the challenge of summarizing entire threads by treating it as a binary sentence classification task. They considered three types of features: basic features that simply treat the email as text (e.g. tf.idf, which scores words highly if they are frequent in the document but rare across all documents), features that consider the thread to be a sequence of turns (e.g. the position of the turn in the thread), and email-specific features such as number of recipients and subject line similarity. Carenini et al. (2007) took an approach to thread summarization using the Enron corpus (descri"
D08-1081,J02-4003,0,0.00801496,"ion we give a brief overview of previous research on meeting summarization and email summarization, respectively. 773 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 773–782, c Honolulu, October 2008. 2008 Association for Computational Linguistics 2.1 Meeting Summarization Among early work on meeting summarization, Waibel et al. (1998) implemented a modified version of the Maximal Marginal Relevance algorithm (Carbonell and Goldstein, 1998) applied to speech transcripts, presenting the user with the n best sentences in a meeting browser interface. Zechner (2002) investigated summarizing several genres of speech, including spontaneous meeting speech. Though relevance detection in his work relied largely on tf.idf scores, Zechner also explored cross-speaker information linking and question/answer detection. More recently, researchers have investigated the utility of employing speech-specific features for summarization, including prosodic information. Murray et al. (2005a; 2005b) compared purely textual summarization approaches with featurebased approaches incorporating prosodic features, with human judges favoring the feature-based approaches. In subse"
D09-1140,P08-1092,0,0.0158019,"icant patterns must occur at least once in the irrelevant text. This last rule is meant to prevent 1 http://ir.dcs.gla.ac.uk/test collections/blog06info.html p(r|t) 0.99 0.99 0.98 0.97 0.95 0.95 0.95 us from learning completely blog-specific patterns such as posted by NN or linked to DT. In the end, more than 20,000 patterns were learned from the blog data. While manual inspection does show that many undesirable patterns were extracted, among the highest-scoring patterns are many sensible subjective trigrams such as those indicated in Table 3. This approach is similar in spirit to the work of Biadsy et al. (2008) on unsupervised biography production. Without access to labeled biographical data, the authors chose to use sentences from Wikipedia biographies as their positive set and sentences from newswire articles as their negative set, on the assumption that most of the Wikipedia sentences would be relevant to biographies and most of the newswire sentences would not. 3.2 Deriving VIN Features For our machine learning experiments, we derive, for each sentence, features indicating the presence of the significant VIN patterns. Patterns are binned according to their conditional probability range (i.e., 0."
D09-1140,H92-1022,0,0.044506,"h syntactic parsers may perform poorly. Also, our learned trigram patterns range from fully instantiated to completely uninstantiated. For example, we might find that the pattern RB JJ NN is a very good indicator of subjective utterances because it matches a variety of scenarios where people are ascribing qualities to things, e.g. really bad movie, horribly overcooked steak. Notice that we do not see our approach and AutoSlog-TS as mutually exclusive, and indeed we demonstrate through these experiments that they can be effectively combined. Our approach begins by running the Brill POS tagger (Brill, 1992) over all sentences in a document. We then extract all of the word trigrams from the document, and represent each trigram using every possible instantiation. Because we are working at the trigram level, and each unit of the trigram can be a word or its POS tag there are 23 = 8 representations in each trigram’s instantiation set. To continue the example from above, the instantiation set for the trigram really great idea is given in Table 1. As we scan down the instantiation set, we can see that the level of abstraction increases until it is completely uninstantiated. It is this multilevel abstr"
D09-1140,D08-1081,1,0.794728,"detection, we also recognize that there are many additional features specific for characterizing multimodal conversations that may correlate well with subjectivity and polarity. Such features include structural characteristics like the position of a sentence in a turn and the position of a turn in the conversation, and participant features relating to dominance or leadership. For example, it may be that subjective sentences are more likely to come at the end of a conversation, or that a person who dominates the conversation may utter more negative sentences. We use the feature set provided by Murray and Carenini (2008), which they used for automatic summarization of conversations and which are shown in Table 4. Many of the features are based on so-called Sprob and T prob term-weights, the former of which weights words based on their distributions across conversation participants and the latter of which similarly weights words based on their distributions across conversation turns. Other features include word entropy of the candidate sentence, lexical cohesion of the sentence with the greater conversation, and structural features indicating position of the candidate sentence in the turn and in the conversati"
D09-1140,D08-1049,0,0.775848,"sation features capture structural characteristics of multimodal conversations as well as participant information. We test our approach in two sets of experiments. The goal of the first set of experiments is to discriminate subjective from non-subjective utterances, comparing the novel approach to existing state-of-the-art techniques. In the second set of experiments, the goal is to discriminate positivesubjective and negative-subjective utterances, establishing their polarity. In both sets of experiments, we assess the impact of features relating to conversation structure. 2 Related Research Raaijmakers et al. (2008) have approached the problem of detecting subjectivity in meeting speech by using a variety of multimodal features such as prosodic features, word n-grams, character n-grams and phoneme n-grams. For subjectivity detection, they found that a combination of all features was best, while prosodic features were less useful for discriminating between positive and negative utterances. They found character n-grams to be particularly useful. Riloff and Wiebe (2004) presented a method for learning subjective extraction patterns from a large amount of data, which takes subjective and nonsubjective text a"
D09-1140,W03-1014,0,0.931233,"sense that their extraction patterns are partly instantiated. However, the AutoSlog-TS approach relies on deriving syntactic structure with the Sundance shallow parser (Riloff and Phillips, 2004). We hypothesize that our trigram approach may be more robust to disfluent and fragmented meeting speech and emails 1349 1 really really really RB really RB RB RB 2 great great JJ great JJ great JJ JJ 3 idea NN idea idea NN NN idea NN bility is greater than 0.65 and the pattern occurs more than five times in the entire document set (slightly higher than probability &gt;= 0.60 and f requency &gt;= 2 used by Riloff and Wiebe (2003)). Table 1: Sample Instantiation Set on which syntactic parsers may perform poorly. Also, our learned trigram patterns range from fully instantiated to completely uninstantiated. For example, we might find that the pattern RB JJ NN is a very good indicator of subjective utterances because it matches a variety of scenarios where people are ascribing qualities to things, e.g. really bad movie, horribly overcooked steak. Notice that we do not see our approach and AutoSlog-TS as mutually exclusive, and indeed we demonstrate through these experiments that they can be effectively combined. Our appro"
D09-1140,W06-1652,0,0.335575,"t 2009. 2009 ACL and AFNLP low dependency parser (Riloff and Phillips, 2004). They are extracted by exhaustively applying syntactic templates such as &lt; subj &gt; passive-verb and active-verb &lt; dobj &gt; to a training corpus, with an extracted pattern for every instantiation of the syntactic template. These patterns are scored according to probability of relevance given the pattern and frequency of the pattern. Because these patterns are based on syntactic structure, they can represent subjective expressions that are not fixed word sequences and would therefore be missed by a simple n-gram approach. Riloff et al. (2006) explore feature subsumption for opinion detection, where a given feature may subsume another feature representationally if the strings matched by the first feature include all of the strings matched by the second feature. To give their own example, the unigram happy subsumes the bigram very happy. The first feature will behaviorally subsume the second if it representationally subsumes the second and has roughly the same information gain, within an acceptable margin. They show that they can improve opinion analysis results by modeling these relations and reducing the feature set. Our approach"
D09-1140,2007.sigdial-1.5,0,0.0605843,"Missing"
D09-1140,wilson-2008-annotating,0,0.103249,"d Learning of Patterns from Conversation Data The first learning strategy is to apply the abovedescribed methods to the annotated conversation data, learning the positive patterns by comparing positive-subjective utterances to all other utterances, and learning the negative patterns by comparing the negative-subjective utterances to all other utterances, using the described methods. This results in 759 significant positive patterns and 67 significant negative patterns. This difference in pattern numbers can be explained by negative utterances being less common in the AMI meetings, as noted by Wilson (2008). It may be that people are less comfortable in expressing negative sentiments in face-to-face conversations, particularly when the meeting participants do not know each other well (in the AMI scenario meetings, many participants were meeting each other for the first time). But there may be a further explanation for why we learn many more positive than negative patterns. When conversation participants do express negative sentiments, they may couch those sentiments in more euphemistic or guarded terms compared with positive sentiments. Table 2 gives examples of significant positive and negative"
D09-1140,W03-1017,0,0.114359,"he strings matched by the second feature. To give their own example, the unigram happy subsumes the bigram very happy. The first feature will behaviorally subsume the second if it representationally subsumes the second and has roughly the same information gain, within an acceptable margin. They show that they can improve opinion analysis results by modeling these relations and reducing the feature set. Our approach for learning subjective patterns like Raaijmakers et al. relies on n-grams, but like Riloff et al. moves beyond fixed sequences of words by varying levels of lexical instantiation. Yu and Hatzivassiloglou (2003) addressed three challenges in the news article domain: discriminating between objective documents and subjective documents such as editorials, detecting subjectivity at the sentence level, and determining polarity at the sentence level. They found that the latter two tasks were substantially more difficult than classification at the document level. Of particular relevance here is that they found that partof-speech (POS) features were especially useful for assigning polarity scores, with adjectives, adverbs and verbs comprising the best set of POS tags. This work inspired us to look at general"
D10-1038,N09-1042,0,0.0127496,"hey achieve similar results as (Galley et al., 2003), with the supervised approach outperforming LCSeg. However, for the subtopic level, LCSeg performs significantly better than the supervised one. In our work, we show how LCSeg performs when applied to the temporal ordering of the emails in a thread. We also propose its extension to leverage the finer conversation structure of emails. The probabilistic generative topic models, such as LDA and its variants (e.g., (Blei et al., 2003), (Steyvers and Griffiths, 2007)), have proven to be successful for topic segmentation in both monologue (e.g., (Chen et al., 2009)) and dialog (e.g., (Georgescul et al., 2008)). (Purver et al., 2006) uses a variant of LDA for the tasks of segmenting meeting transcripts and extracting the associated topic labels. However, their approach for segmentation does not perform better than LCSeg. In our work, we show how the general LDA performs when applied to email conversations and describe how it can be extended to exploit the conversation structure of emails. Several approaches have been proposed to capture an email conversation . Email programs (e.g., Gmail, Yahoomail) group emails into threads using headers. However, our a"
D10-1038,W01-0514,0,0.0348819,"2003) to conversations/emails, we consider a topic is something about which the participant(s) discuss or argue or Topic segmentation is often considered a prerequisite for other higher-level conversation analysis and applications of the extracted structure are broad, encompassing: summarization (Harabagiu and Lacatusu, 2005), information extraction and ordering (Allan, 2002), information retrieval (Dias et al., 2007), and intelligent user interfaces (Dredze et al., 2008). While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi et al., 2001)) and synchronous dialogs (e.g., (Galley et al., 2003), (Hsueh et al., 2006)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email). Therefore, there is no reliable annotation scheme, no standard corpus, and no agreedupon metrics available. Also, it is our key hypothesis that, because of its asynchronous nature, and the use of quotation (Crystal, 2001), topics in an email thread often do not change in a sequential way. As a result, we do not expect models which have proved successful in monologue or dialog to be as effective when they are applied to e"
D10-1038,A00-2004,0,0.129285,"Missing"
D10-1038,P08-1095,0,0.36039,"renini and Gabriel Murray and Raymond T. Ng {rjoty, carenini, gabrielm, rng}@cs.ubc.ca Department of Computer Science University of British Columbia Vancouver, BC, V6T 1Z4, Canada Abstract express their opinions. For example, in the email thread shown in Figure 1, according to the majority of our annotators, participants discuss three topics (e.g., ‘telecon cancellation’, ‘TAG document’, and ‘responding to I18N’). Multiple topics seem to occur naturally in social interactions, whether synchronous (e.g., chats, meetings) or asynchronous (e.g., emails, blogs) conversations. In multi-party chat (Elsner and Charniak, 2008) report an average of 2.75 discussions active at a time. In our email corpus, we found an average of 2.5 topics per thread. This work concerns automatic topic segmentation of email conversations. We present a corpus of email threads manually annotated with topics, and evaluate annotator reliability. To our knowledge, this is the first such email corpus. We show how the existing topic segmentation models (i.e., Lexical Chain Segmenter (LCSeg) and Latent Dirichlet Allocation (LDA)) which are solely based on lexical information, can be applied to emails. By pointing out where these methods fail a"
D10-1038,P03-1071,0,0.663951,"LDA for segmenting an email thread into topical clusters and incorporating conversation structure into these models improves the performance significantly. 1 Introduction With the ever increasing popularity of emails and web technologies, it is very common for people to discuss issues, events, agendas or tasks by email. Effective processing of the email contents can be of great strategic value. In this paper, we study the problem of topic segmentation for emails, i.e., grouping the sentences of an email thread into a set of coherent topical clusters. Adapting the standard definition of topic (Galley et al., 2003) to conversations/emails, we consider a topic is something about which the participant(s) discuss or argue or Topic segmentation is often considered a prerequisite for other higher-level conversation analysis and applications of the extracted structure are broad, encompassing: summarization (Harabagiu and Lacatusu, 2005), information extraction and ordering (Allan, 2002), information retrieval (Dias et al., 2007), and intelligent user interfaces (Dredze et al., 2008). While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi e"
D10-1038,I08-2133,0,0.094015,"et al., 2003), with the supervised approach outperforming LCSeg. However, for the subtopic level, LCSeg performs significantly better than the supervised one. In our work, we show how LCSeg performs when applied to the temporal ordering of the emails in a thread. We also propose its extension to leverage the finer conversation structure of emails. The probabilistic generative topic models, such as LDA and its variants (e.g., (Blei et al., 2003), (Steyvers and Griffiths, 2007)), have proven to be successful for topic segmentation in both monologue (e.g., (Chen et al., 2009)) and dialog (e.g., (Georgescul et al., 2008)). (Purver et al., 2006) uses a variant of LDA for the tasks of segmenting meeting transcripts and extracting the associated topic labels. However, their approach for segmentation does not perform better than LCSeg. In our work, we show how the general LDA performs when applied to email conversations and describe how it can be extended to exploit the conversation structure of emails. Several approaches have been proposed to capture an email conversation . Email programs (e.g., Gmail, Yahoomail) group emails into threads using headers. However, our annotations show that topics change at a finer"
D10-1038,J97-1003,0,0.315662,"ing to two measures: ‘number of words in the chain’ and ‘compactness of the chain’. The more compact (in terms of number of sentences) and the more populated chains get higher scores. The algorithm then works with two adjacent analysis windows, each of a fixed size k which is empirically determined. For each sentence boundary, LCSeg computes the cosine similarity (or lexical cohesion function) at the transition between the two windows. Low similarity indicates low lexical cohesion, and a sharp change signals a high probability of an actual topic boundary. This method is similar to TextTiling (Hearst, 1997) except that the similarity is computed based on the scores of the ‘lexical chains’ instead of ‘term counts’. In order to apply LCSeg on email threads we arrange the emails based on their temporal relation (i.e., arrival time) and apply the LCSeg algorithm to get the topic boundaries. 9 One can also consider other lexical semantic relations (e.g., synonym, hypernym, hyponym) in lexical chaining. However, Galley et al., (Galley et al., 2003) uses only repetition relation as previous research results (e.g., (Choi, 2000)) account only for repetition. From: Brian To: rdf core Subject: 20030220 tel"
D10-1038,E06-1035,0,0.309155,"h the participant(s) discuss or argue or Topic segmentation is often considered a prerequisite for other higher-level conversation analysis and applications of the extracted structure are broad, encompassing: summarization (Harabagiu and Lacatusu, 2005), information extraction and ordering (Allan, 2002), information retrieval (Dias et al., 2007), and intelligent user interfaces (Dredze et al., 2008). While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi et al., 2001)) and synchronous dialogs (e.g., (Galley et al., 2003), (Hsueh et al., 2006)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email). Therefore, there is no reliable annotation scheme, no standard corpus, and no agreedupon metrics available. Also, it is our key hypothesis that, because of its asynchronous nature, and the use of quotation (Crystal, 2001), topics in an email thread often do not change in a sequential way. As a result, we do not expect models which have proved successful in monologue or dialog to be as effective when they are applied to email conversations. Our contributions in this paper aim to remedy 388 Procee"
D10-1038,P06-1004,0,0.41829,"inition of topic (Galley et al., 2003) to conversations/emails, we consider a topic is something about which the participant(s) discuss or argue or Topic segmentation is often considered a prerequisite for other higher-level conversation analysis and applications of the extracted structure are broad, encompassing: summarization (Harabagiu and Lacatusu, 2005), information extraction and ordering (Allan, 2002), information retrieval (Dias et al., 2007), and intelligent user interfaces (Dredze et al., 2008). While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi et al., 2001)) and synchronous dialogs (e.g., (Galley et al., 2003), (Hsueh et al., 2006)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email). Therefore, there is no reliable annotation scheme, no standard corpus, and no agreedupon metrics available. Also, it is our key hypothesis that, because of its asynchronous nature, and the use of quotation (Crystal, 2001), topics in an email thread often do not change in a sequential way. As a result, we do not expect models which have proved successful in monologue or dialog to be as effective when"
D10-1038,P06-1003,0,0.0691643,"Missing"
D12-1083,P05-1022,0,0.0184229,"sible nuclearity statuses. (Subba and Eugenio, 2009) and also treat the reversals of non-commutative relations as separate relations. That is, PREPARATION-ACT and ACTPREPARATION are two different relations. Attaching the nuclearity statuses to these relations gives 70 distinct relations in the Instructional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies, we test our models on those specific test sets. In addition, we s"
D12-1083,A00-2018,0,0.0951801,"du/page/software 910 Not all relations take all the possible nuclearity statuses. (Subba and Eugenio, 2009) and also treat the reversals of non-commutative relations as separate relations. That is, PREPARATION-ACT and ACTPREPARATION are two different relations. Attaching the nuclearity statuses to these relations gives 70 distinct relations in the Instructional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies"
D12-1083,J03-4003,0,0.0271471,"structional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies, we test our models on those specific test sets. In addition, we show more general performance based on 10-fold cross validation. 5.3 Parsing based on Manual Segmentation First, we present the results of our discourse parser based on manual segmentation. The parsing performance is assessed using the unlabeled (i.e., span) and labeled (i.e., nuclear"
D12-1083,P07-1033,0,0.0448921,"sting to observe how much our full system is affected by an automatic segmenter on both RST-DT and the Instructional corpus (see Table 2 and Table 5). Nevertheless, taking into account the segmentation results in Table 4, this is not surprising because previous studies (Soricut and Marcu, 2003) have already shown that automatic segmentation is the primary impediment to high accuracy discourse parsing. This demonstrates the need for a more accurate segmentation model in the Instructional genre. A promising future direction would be to apply effective domain adaptation methods (e.g., easyadapt (Daume, 2007)) to improve the segmentation performance in the Instructional domain by leveraging the rich data in RST-DT. 5.6 Error Analysis and Discussion The results in Table 2 suggest that given a manually segmented discourse, our sentence-level discourse parser finds the unlabeled (i.e., span) discourse tree and assigns the nuclearity statuses to the spans at a performance level close to human annotators. We, therefore, look more closely into the performance of our parser on the hardest task of relation labeling. Figure 6 shows the confusion matrix for the relation labeling task using manual segmentati"
D12-1083,P09-1075,0,0.722715,"s are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursively also subject to this relation linking. A span linked by a rhetorical relation can be either a NUCLEUS or a SATELLITE depending on how central the message is to the author. Discourse analysis in RST involves two subtasks: (i) breaking the Previous studies on discourse analysis have been quite successful in identifying what machine learning approaches and what features are more useful for automatic discourse segmentation and parsing (Soricut and Marcu, 2003; Subba and Eugenio, 2009; duVerle and Prendinger, 2009). However, all the proposed solutions suffer from at least one of the following two key limitations: first, they make strong independence assumptions on the structure and the labels of the resulting DT, and typically model the construction of the DT and the labeling of the relations separately; second, they apply a greedy, suboptimal algorithm to build the structure of the DT. In this paper, we propose a new sentence-level discourse parser that addresses both limitations. The crucial component is a probabilistic discriminative parsing model, expressed as a Dynamic Conditional Random Field (DCR"
D12-1083,P08-1109,0,0.0649235,"Missing"
D12-1083,P07-1062,0,0.905371,"an EDU boundary after a token wk , we find the lowest constituent in the lexicalized syntactic tree that spans over tokens wi . . . wj such that i≤k&lt;j. The production that expands this constituent in the tree and its different variations, form the feature set. For example in Figure 5, the production NP(efforts)→PRP$(its)NNS(efforts)↑S(to) and its different variations depending on whether they include the lexical heads and how many nonterminals (up to two) to consider before and after the potential EDU boundary (↑), are used to determine the existence of a boundary after the word efforts (see (Fisher and Roark, 2007) for details). SPADE uses these features in a generative way, meaning that, it inserts an EDU boundary if the relative frequency (i.e., Maximum Likelihood Estimate (MLE)) of a potential boundary given the production in the training corpus is greater than 0.5. If the production has not been observed frequently enough, it uses its other variations to perform further smoothing. In contrast, we compute the MLE estimates for a production and its other variations, and use those as features with/without binarizing the values. Shallow syntactic parse (or Chunk) and POS tags have been shown to possess"
D12-1083,I11-1120,0,0.0322917,"Missing"
D12-1083,P95-1037,0,0.216158,"ations in the Instructional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies, we test our models on those specific test sets. In addition, we show more general performance based on 10-fold cross validation. 5.3 Parsing based on Manual Segmentation First, we present the results of our discourse parser based on manual segmentation. The parsing performance is assessed using the unlabeled (i.e., span) and labele"
D12-1083,J00-3005,0,0.853402,"evel discourse analysis. Our framework comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin. 1 Figure 1: Discourse structure of a sentence in RST-DT. Introduction Automatic discourse analysis has been shown to be critical in several fundamental Natural Language Processing (NLP) tasks including text generation (Prasad et al., 2005), summarization (Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, posits a tree representation of a discourse, known as a Discourse Tree (DT), as exemplified by the sample DT shown in Figure 1. The leaves of a DT correspond to contiguous atomic text spans, also called Elementary Discourse Units (EDUs) (three in the example). The adjacent EDUs are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursi"
D12-1083,prasad-etal-2008-penn,0,0.0610095,"Missing"
D12-1083,N03-1028,0,0.115076,"Missing"
D12-1083,N03-1030,0,0.514526,"ts (EDUs) (three in the example). The adjacent EDUs are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursively also subject to this relation linking. A span linked by a rhetorical relation can be either a NUCLEUS or a SATELLITE depending on how central the message is to the author. Discourse analysis in RST involves two subtasks: (i) breaking the Previous studies on discourse analysis have been quite successful in identifying what machine learning approaches and what features are more useful for automatic discourse segmentation and parsing (Soricut and Marcu, 2003; Subba and Eugenio, 2009; duVerle and Prendinger, 2009). However, all the proposed solutions suffer from at least one of the following two key limitations: first, they make strong independence assumptions on the structure and the labels of the resulting DT, and typically model the construction of the DT and the labeling of the relations separately; second, they apply a greedy, suboptimal algorithm to build the structure of the DT. In this paper, we propose a new sentence-level discourse parser that addresses both limitations. The crucial component is a probabilistic discriminative parsing mod"
D12-1083,H05-1033,0,0.358704,"k comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin. 1 Figure 1: Discourse structure of a sentence in RST-DT. Introduction Automatic discourse analysis has been shown to be critical in several fundamental Natural Language Processing (NLP) tasks including text generation (Prasad et al., 2005), summarization (Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, posits a tree representation of a discourse, known as a Discourse Tree (DT), as exemplified by the sample DT shown in Figure 1. The leaves of a DT correspond to contiguous atomic text spans, also called Elementary Discourse Units (EDUs) (three in the example). The adjacent EDUs are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursively also subject to this relation linking. A span li"
D12-1083,N09-1064,0,0.621581,"Missing"
D12-1083,J05-2005,0,0.230786,"Missing"
D12-1083,J93-2004,0,\N,Missing
D14-1124,W11-0702,0,0.422008,"iterations. By better understanding how debate arises and propagates in a conversation, we may also gain insight into how authors’ opinions on a topic can be influenced over time. The long term goal of our research is to lay the foundations for understanding argumentative structure in conversations, which could be applied to NLP tasks such as summarization, information Motivated by the widespread consensus in both computational and theoretical linguistics on the utility of discourse markers for signalling pragmatic functions such as disagreement and personal opinions (Webber and Prasad, 2008; Abbott et al., 2011; J. E. Fox-Tree, 2010), we introduce a new set of features based on the Discourse Tree (DT) of a conversational text. Discourse Trees were formalized by Mann and Thompson (1988) as part of their Rhetorical Structure Theory (RST) to represent the structure of discourse. Although this theory is for monologic discourse, we propose to treat conversational dialogue as a collection of linked monologues, and subsequently build a relation graph describing both rhetorical connections within user posts, as well as between different users. Features obtained from this graph offer significant improvements"
D14-1124,P12-1042,0,0.0260897,"Missing"
D14-1124,W08-1106,1,0.645268,"in the text. Therefore, we also include an aggregated count of all 17 discourse markers in each fifth of the posts in a topic (e.g. the count of all 17 discourse markers in the first fifth of every post in the topic). Altogether, there are 5 aggregated discourse marker features in addition to the 17 frequency count features. 4.3 Sentiment Features Sentiment polarity features have been shown to be useful in argument detection (Mishne and Glance, 2006). For this work, we use four sentiment scoring categories: the variance, average score, number of negative sentences, and controversiality score (Carenini and Cheung, 2008) of sentences in a post. These are determined using SoCAL (Taboada et al., 2011), which gives each sentence a polarity score and has been shown to work well on user-generated content. Overall, we have two main classes of sentiment features. The first type splits all the posts in a topic into 4 sections corresponding to the sentences in each quarter of the post. The sentiment scores described above are then applied to each section of the posts (e.g. one feature is the number of negative sentences in the first quarter of each post). As a separate feature, we also include the scores on just the f"
D14-1124,W12-3810,0,0.0429109,"Missing"
D14-1124,W13-1105,0,0.095247,"Missing"
D14-1124,W13-4006,0,0.0625365,"as been used to rank articles according to controversiality, although no quantitative evaluation of this approach has been given, and, unlike in our analysis, they did not consider any other features (Gomez et al., 2008). Content based features such as polarity and cosine similarity have also been used to study influence, controversy and opinion changes on microblogging sites such as Twitter (Lin et al., 2013; Popescu and Pennacchiotti, 2010). The simplified task of detecting disagreement between just two users (either a question/response pair (Abbott et al., 2011) or two adjacent paragraphs (Misra and Walker, 2013)) has also been recently approached on the ARGUE corpus. Abbott et al. (2011) use discourse markers, generalized dependency features, punctuation and structural features, while Misra and Walker (2013) focus on n-grams indicative of denial, hedging and agreement, as well as cue words and punctuation. Most similar to our work is that by Mishne and Glance (2006). They performed a general analysis of weblog comments, using punctuation, quotes, lexicon counts, subjectivity, polarity and referrals to detect disputative and non-disputative comments. Referrals and questions, as well as polarity measur"
D14-1124,P12-1034,0,0.0306446,"Missing"
D14-1124,P04-1085,0,0.351529,"Missing"
D14-1124,C10-2100,0,0.086373,"collections of news documents on a particular topic, as well as reviews (Choi et al., 2010; Awadallah et al., 2012; Popescu et al., 2005). In the asynchronous domain, there has been recent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recent work has focused on identifying users’ attitudes towards each other (Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been used to rank articles according to controversiality, although no quantitative evaluation of this approach has been given, and, unlike in our analysis, they did not consider any other features (Gomez et al., 2008). Content based f"
D14-1124,H05-1043,0,0.0691039,"Missing"
D14-1124,D10-1121,0,0.026349,"Missing"
D14-1124,D08-1049,0,0.0187745,"Missing"
D14-1124,P13-1162,0,0.016659,"Somasundaran and Wiebe, 2010; Thomas et al., 2006), it is often difficult to outperform a unigram/bigram model in the task of disagreement and argument detection. In this analysis, because of the very small number of samples, we do not consider dependency or part-ofspeech features, but do make a comparison with a filtered unigram/bigram model. In the filtering, we remove stop words and any words that occur in fewer than three topics. This helps to prevent topic specific words from being selected, and limits the number of possible matches slightly. Additionally, we use a lexicon of bias-words (Recasens et al., 2013) to extract a bias-word frequency score over all posts in the topic as a separate feature. Punctuation Like many other features already described, frequency counts of ‘?’,‘!’,‘”’,‘”, and ‘.’ are found for each fifth of the post (the first fifth, second fifth, etc.). These counts are then aggregated across all posts for a total of 5×5 = 25 features. 4.8 Referrals Referrals have been found to help with the detection of disagreement (Mishne and Glance, 2006), especially with respect to other authors. Since there are no direct referrals to previous authors in this corpus, references to variations"
D14-1124,2007.sigdial-1.5,0,0.0294421,"Missing"
D14-1124,P13-1048,1,0.796369,"nd to threads of comments. Each comment has a timestamp at the minute resolution as well as author information (although it is possible to post on the forum anonymously). Additionally, other users can give different posts scores (in the range -1 to 5) as well as categorizing posts under “funny”, “interesting”, “informative”, “insightful”, “flamebait”, “off topic”, or “troll”. This user moderation, as well as the 1 formalized reply-to structure between comments, makes Slashdot attractive over other internet forums as it allows for high-quality and structured conversations. In a previous study, Joty et al. (2013) selected 20 articles and their associated comments to be annotated for topic segmentation boundaries and labels by an expert Slashdot contributor. They define a topic as a subset of the utterances in a conversation, while a topic label describes what the given topic is about (e.g., Physics in videogames). Of the 98 annotated topics from their dataset, we filtered out those with only one contributing user, for a total of 95 topics. Next, we developed a Human Intelligence Task (HIT) using the crowdsourcing platform Crowdflower.2 The objective of this task was to both develop a corpus for testin"
D14-1124,P09-1026,0,0.0694304,"to be useful for argument labelling and detection in articles on the topic of healthcare. Additionally, discourse markers and sentiment features have been found to assist with disagreement detection in collections of news documents on a particular topic, as well as reviews (Choi et al., 2010; Awadallah et al., 2012; Popescu et al., 2005). In the asynchronous domain, there has been recent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recent work has focused on identifying users’ attitudes towards each other (Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been used to rank articles accordi"
D14-1124,W10-0214,0,0.164449,"lling and detection in articles on the topic of healthcare. Additionally, discourse markers and sentiment features have been found to assist with disagreement detection in collections of news documents on a particular topic, as well as reviews (Choi et al., 2010; Awadallah et al., 2012; Popescu et al., 2005). In the asynchronous domain, there has been recent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recent work has focused on identifying users’ attitudes towards each other (Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been used to rank articles according to controversiality, althoug"
D14-1124,J11-2001,0,0.00463822,"rs in each fifth of the posts in a topic (e.g. the count of all 17 discourse markers in the first fifth of every post in the topic). Altogether, there are 5 aggregated discourse marker features in addition to the 17 frequency count features. 4.3 Sentiment Features Sentiment polarity features have been shown to be useful in argument detection (Mishne and Glance, 2006). For this work, we use four sentiment scoring categories: the variance, average score, number of negative sentences, and controversiality score (Carenini and Cheung, 2008) of sentences in a post. These are determined using SoCAL (Taboada et al., 2011), which gives each sentence a polarity score and has been shown to work well on user-generated content. Overall, we have two main classes of sentiment features. The first type splits all the posts in a topic into 4 sections corresponding to the sentences in each quarter of the post. The sentiment scores described above are then applied to each section of the posts (e.g. one feature is the number of negative sentences in the first quarter of each post). As a separate feature, we also include the scores on just the first sentence, as Mishne and Glance (2006) previously found this to be beneficia"
D14-1124,W06-1639,0,0.216059,"atures have been well documented in the literature to provide useful information about whether or not arguments exist, especially in online conversations that may be more informative than subjective (Biyani et al., 2014; Yin et al., 2012). In this work, length features include the length of the post in sentences, the average number of words per sentence, the average number of sentences per post, the number of contributing authors, the rate of posting, and the total amount of time of the conversation. This results in a total of 9 features. 4.7 As noted previously (Somasundaran and Wiebe, 2010; Thomas et al., 2006), it is often difficult to outperform a unigram/bigram model in the task of disagreement and argument detection. In this analysis, because of the very small number of samples, we do not consider dependency or part-ofspeech features, but do make a comparison with a filtered unigram/bigram model. In the filtering, we remove stop words and any words that occur in fewer than three topics. This helps to prevent topic specific words from being selected, and limits the number of possible matches slightly. Additionally, we use a lexicon of bias-words (Recasens et al., 2013) to extract a bias-word freq"
D14-1124,P11-2065,0,0.0903692,"Missing"
D14-1124,W12-3710,0,0.310451,"torical relations to be useful for argument labelling and detection in articles on the topic of healthcare. Additionally, discourse markers and sentiment features have been found to assist with disagreement detection in collections of news documents on a particular topic, as well as reviews (Choi et al., 2010; Awadallah et al., 2012; Popescu et al., 2005). In the asynchronous domain, there has been recent work in disagreement detection, especially as it pertains to stance identification. Content based features, including sentiment, duration, and discourse markers have been used for this task (Yin et al., 2012; Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). The structure of a conversation has also been used, although these approaches have focused on simple rules for disagreement identification (Murakami and Raymond, 2010), or have assumed that adjacent posts always disagree (Agrawal et al., 2003). More recent work has focused on identifying users’ attitudes towards each other (Hassan et al., 2010), influential users and posts (Nguyen et al., 2014), as well as identifying subgroups of users who share viewpoints (Abu-Jbara et al., 2010). In Slashdot, the h-index of a discussion has been"
D14-1124,H05-2017,0,\N,Missing
D14-1168,C08-2002,0,0.01477,"to a small number of aspects and our aspect ordering method takes advantage of rhetorical information and does not require any training data. Our method relies on the discourse structure and discourse relations of reviews to infer the importance of aspects as well as the association between them (e.g., which aspects relate to each other). Researchers have recently started using the discourse structure of text in sentiment analysis and have shown its advantage in improving sentiment classification accuracy (e.g., (Lazaridou et al., 2013; Trivedi and Eisenstein, 2013; Somasundaran et al., 2009; Asher et al., 2008)). However, to the best of our knowledge, none of the existing works have looked into exploiting discourse structure in abstractive review summarization. In our work, importance of aspects, derived from the reviews’ discourse structure and relations, is used to rank and select aspects to be included in the summary. More specifically, we start with the most important (highest ranked) aspects to generate a summary and add more aspects to the system until a summary of desired length is obtained. Aspect association is considered to better explain how the opinions on aspects affect each other (e.g."
D14-1168,P99-1071,0,0.0199615,"007; Titov and McDonald, 2008)). However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal. Di Fabbrizio et al., 2014). This finding is also supported by a previous study in the context of summarizing news articles (Barzilay et al., 1999). To the best of our knowledge, there are only three previous works on abstractive opinion summarization (Ganesan et al., 2010; Carenini et al., 2013; Di Fabbrizio et al., 2014). The first work (Ganesan et al., 2010) proposes a graph-based method for generating ultra concise opinion summaries that are more suitable for viewing on devices with small screens. This method does not provide a well-formed grammatical abstract and the generated summary only contains words that occur in the original texts. Therefore, this approach is more extractive than abstractive. Another limitation is that the gen"
D14-1168,W08-1106,1,0.59509,"number of users whose opinions contributed to the evaluation of the aspect. Polarity verbs: for each aspect, a polarity verb is selected based on the average sentiment polarity strength for that aspect. Although the average, in most cases, can be a good metric to evaluate the polarity of an aspect, it fails when the distribution of evaluations is centered on zero, for instance, if there are equal numbers of positive and negative evaluations (i.e., controversial). To partially solve this problem, we first check whether the aspect evaluation is controversial by applying the formula proposed by (Carenini and Cheung, 2008). In the case of controversiality, our microplanner selects a lexical item to express the controversiality of the aspect. In other cases, we use the average and select the polarity verb based on that. Connectives: in order to form more fluent and readable sentences and to increase the language variability, we randomly select our connectives from the list shown in Table 1. Moreover, when a parent aspect (excluding the root in AHT) has two children, they are connected by one of the coordinating conjunction “[and, similarly]” if they agree on polarity, and they will be connected by a choice of “["
D14-1168,W14-4408,0,0.476022,"Missing"
D14-1168,C10-1039,0,0.886339,"mary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal. Di Fabbrizio et al., 2014). This finding is also supported by a previous study in the context of summarizing news articles (Barzilay et al., 1999). To the best of our knowledge, there are only three previous works on abstractive opinion summarization (Ganesan et al., 2010; Carenini et al., 2013; Di Fabbrizio et al., 2014). The first work (Ganesan et al., 2010) proposes a graph-based method for generating ultra concise opinion summaries that are more suitable for viewing on devices with small screens. This method does not provide a well-formed grammatical abstract and the generated summary only contains words that occur in the original texts. Therefore, this approach is more extractive than abstractive. Another limitation is that the generated summaries do not contain any information about the distribution of opinions. In the second work, (Carenini et al., 2013"
D14-1168,P13-1048,1,0.698154,"cture Theory (RST) (Mann and Thompson, 1988) is one of the most popular. RST divides a text into minimal atomic units, called Elementary Discourse Units (EDUs). It then forms a tree representation of a discourse called a Discourse Tree (DT) using rhetorical relations (e.g., Elaboration, Explanation, etc) as edges, and EDUs as leaves. EDUs linked by a rhetorical relation are also distinguished based on their relative importance in conveying the author’s message: nucleus is the central part, whereas satellite is the peripheral part. We use a publicly available state-of-the-art discourse parser (Joty et al., 2013)1 to generate a DT for each product review. Figure 1 (a) and (b) show DTs for two sample reviews where dotted edges identify the satellite spans. DT1 in Figure 1 (a) shows that review R1 consists of three EDUs with two relations Elaboration and Background between them. It also shows that the first EDU (i.e. I love camera) is the nucleus (shown by solid line) of the relation Elaboration and so the rest of the document (EDUs 2 and 3) is less important and aims at elaborating on what the author meant in the first EDU. Similarly, the structure shows that the third EDU is mentioned as background in"
D14-1168,P13-1160,0,0.0382663,"On the other hand, in contrast with Starlet-H, we do not limit the input reviews to a small number of aspects and our aspect ordering method takes advantage of rhetorical information and does not require any training data. Our method relies on the discourse structure and discourse relations of reviews to infer the importance of aspects as well as the association between them (e.g., which aspects relate to each other). Researchers have recently started using the discourse structure of text in sentiment analysis and have shown its advantage in improving sentiment classification accuracy (e.g., (Lazaridou et al., 2013; Trivedi and Eisenstein, 2013; Somasundaran et al., 2009; Asher et al., 2008)). However, to the best of our knowledge, none of the existing works have looked into exploiting discourse structure in abstractive review summarization. In our work, importance of aspects, derived from the reviews’ discourse structure and relations, is used to rank and select aspects to be included in the summary. More specifically, we start with the most important (highest ranked) aspects to generate a summary and add more aspects to the system until a summary of desired length is obtained. Aspect association is co"
D14-1168,E09-1059,0,0.0479103,"d the rhetorical relations between them using a PageRank algorithm, and transform the selected subgraph into an aspect tree. Finally, we generate a natural language summary by applying a template-based NLG framework. Quantitative and qualitative analysis of the results, based on two user studies, show that our approach significantly outperforms extractive and abstractive baselines. 1 Introduction Most existing works on sentiment summarization focus on predicting the overall rating on an entity (Pang et al., 2002; Pang and Lee, 2004) or estimating ratings for product features (Lu et al., 2009; Lerman et al., 2009; Snyder and Barzilay, 2007; Titov and McDonald, 2008)). However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal. Di Fabbrizio et al., 2014). This finding is also supported by a previous study in the context of sum"
D14-1168,I13-1065,0,0.0238591,"ins words that occur in the original texts. Therefore, this approach is more extractive than abstractive. Another limitation is that the generated summaries do not contain any information about the distribution of opinions. In the second work, (Carenini et al., 2013) addresses some of the aforementioned problems and generates well-formed grammatical abstracts that describe the distribution of opinion over the entity and its features. However, for each product, this approach requires a feature taxonomy handcrafted by humans as an input, which is not scalable. To partially address this problem (Mukherjee and Joshi, 2013) has proposed a method for the automatic generation of a product attribute hierarchy that leverages ConceptNet (Liu and Singh, 2004). However, the resulting ontology tree has been used only for sentiment classification and not for classification. In the third and most recent study, (Di Fabbrizio et al., 2014) proposed Starlet-H as a hybrid abstractive/extractive sentiment summarizer. StarletH uses extractive summarization techniques to select salient quotes from the input reviews and embeds them into the abstractive summary to exemplify, justify or provide evidence for the aggregate positive o"
D14-1168,P04-1035,0,0.140788,"ate a graph. We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm, and transform the selected subgraph into an aspect tree. Finally, we generate a natural language summary by applying a template-based NLG framework. Quantitative and qualitative analysis of the results, based on two user studies, show that our approach significantly outperforms extractive and abstractive baselines. 1 Introduction Most existing works on sentiment summarization focus on predicting the overall rating on an entity (Pang et al., 2002; Pang and Lee, 2004) or estimating ratings for product features (Lu et al., 2009; Lerman et al., 2009; Snyder and Barzilay, 2007; Titov and McDonald, 2008)). However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal. Di Fabbrizio et al."
D14-1168,W02-1011,0,0.0334496,"rse trees and generate a graph. We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm, and transform the selected subgraph into an aspect tree. Finally, we generate a natural language summary by applying a template-based NLG framework. Quantitative and qualitative analysis of the results, based on two user studies, show that our approach significantly outperforms extractive and abstractive baselines. 1 Introduction Most existing works on sentiment summarization focus on predicting the overall rating on an entity (Pang et al., 2002; Pang and Lee, 2004) or estimating ratings for product features (Lu et al., 2009; Lerman et al., 2009; Snyder and Barzilay, 2007; Titov and McDonald, 2008)). However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal"
D14-1168,radev-etal-2004-mead,0,0.0208993,"Missing"
D14-1168,N07-1038,0,0.0283479,"tions between them using a PageRank algorithm, and transform the selected subgraph into an aspect tree. Finally, we generate a natural language summary by applying a template-based NLG framework. Quantitative and qualitative analysis of the results, based on two user studies, show that our approach significantly outperforms extractive and abstractive baselines. 1 Introduction Most existing works on sentiment summarization focus on predicting the overall rating on an entity (Pang et al., 2002; Pang and Lee, 2004) or estimating ratings for product features (Lu et al., 2009; Lerman et al., 2009; Snyder and Barzilay, 2007; Titov and McDonald, 2008)). However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal. Di Fabbrizio et al., 2014). This finding is also supported by a previous study in the context of summarizing news articles (Bar"
D14-1168,D09-1018,0,0.00828789,"ot limit the input reviews to a small number of aspects and our aspect ordering method takes advantage of rhetorical information and does not require any training data. Our method relies on the discourse structure and discourse relations of reviews to infer the importance of aspects as well as the association between them (e.g., which aspects relate to each other). Researchers have recently started using the discourse structure of text in sentiment analysis and have shown its advantage in improving sentiment classification accuracy (e.g., (Lazaridou et al., 2013; Trivedi and Eisenstein, 2013; Somasundaran et al., 2009; Asher et al., 2008)). However, to the best of our knowledge, none of the existing works have looked into exploiting discourse structure in abstractive review summarization. In our work, importance of aspects, derived from the reviews’ discourse structure and relations, is used to rank and select aspects to be included in the summary. More specifically, we start with the most important (highest ranked) aspects to generate a summary and add more aspects to the system until a summary of desired length is obtained. Aspect association is considered to better explain how the opinions on aspects af"
D14-1168,P08-1036,0,0.0116134,"PageRank algorithm, and transform the selected subgraph into an aspect tree. Finally, we generate a natural language summary by applying a template-based NLG framework. Quantitative and qualitative analysis of the results, based on two user studies, show that our approach significantly outperforms extractive and abstractive baselines. 1 Introduction Most existing works on sentiment summarization focus on predicting the overall rating on an entity (Pang et al., 2002; Pang and Lee, 2004) or estimating ratings for product features (Lu et al., 2009; Lerman et al., 2009; Snyder and Barzilay, 2007; Titov and McDonald, 2008)). However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal. Di Fabbrizio et al., 2014). This finding is also supported by a previous study in the context of summarizing news articles (Barzilay et al., 1999). To the"
D14-1168,N13-1100,0,0.0350483,"ntrast with Starlet-H, we do not limit the input reviews to a small number of aspects and our aspect ordering method takes advantage of rhetorical information and does not require any training data. Our method relies on the discourse structure and discourse relations of reviews to infer the importance of aspects as well as the association between them (e.g., which aspects relate to each other). Researchers have recently started using the discourse structure of text in sentiment analysis and have shown its advantage in improving sentiment classification accuracy (e.g., (Lazaridou et al., 2013; Trivedi and Eisenstein, 2013; Somasundaran et al., 2009; Asher et al., 2008)). However, to the best of our knowledge, none of the existing works have looked into exploiting discourse structure in abstractive review summarization. In our work, importance of aspects, derived from the reviews’ discourse structure and relations, is used to rank and select aspects to be included in the summary. More specifically, we start with the most important (highest ranked) aspects to generate a summary and add more aspects to the system until a summary of desired length is obtained. Aspect association is considered to better explain how"
D14-1168,C00-2137,0,0.0194329,"Missing"
D19-1235,P17-1067,0,0.0357023,"Missing"
D19-1235,Q18-1002,0,0.474261,"t information, the structure nevertheless resembles a well aligned discourse tree. It can be observed that EDUs with negative sentiment are generally located at a higher level in the tree, while for example EDUs 5 and , 6 with positive sentiment, are at the bottom of a deep subtree. This way, EDUs with negative sentiment strongly influence the overall sentiment, while EDUs 5 and 6 only have little impact. At the same time, semantically related EDUs generally have a shorter distance than semantically unrelated EDUs. Our approach combines a neural variant of multiple-instance learning (MILNet) (Angelidis and Lapata, 2018), with an optimal CKY-style tree generation algorithm (Jurafsky and Martin, 2014). First, MILNet computes fine-grained sentiment values and un-normalized attention scores (Ji and Smith, 2017) on EDU-level, by solely relying on distant supervision signals from document-level annotations. These annotations are abundantly available from several published open source datasets such as Yelp’13 (Tang et al., 2015), IMDB (Diao et al., 2014) or Amazon (Zhang et al., 2015). Then, the sentiment values and attention scores are aggregated to guide the discourse-tree construction, optimized on the document"
D19-1235,D15-1263,0,0.61684,"dataset and compare it with parsers trained on human-annotated corpora (news domain RST-DT and Instructional domain). Results indicate that while our parser does not yet match the performance of a parser trained and tested on the same dataset (intra-domain), it does perform remarkably well on the much more difficult and arguably more useful task of inter-domain discourse structure prediction, where the parser is trained on one domain and tested/applied on another one. 1 Introduction Discourse parsing is a fundamental NLP task known to enhance key downstream tasks, such as sentiment analysis (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015), text classification (Ji and Smith, 2017) and summarization (Gerani et al., 2014). In essence, a discourse parser should reveal the structure underlying coherent text as postulated by a discourse theory, of which the two most popular are Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) and PDTB (Prasad et al., 2008). In this paper, we focus on RST-style parsing, but the proposed approach is theory agnostic and could be applied to PDTB as well. The RST discourse theory assumes a complete hierarchical discourse tree for a given document, w"
D19-1235,E17-1028,0,0.0415436,"here leaf nodes are clause-like sentence fragments, called elementary-discourse-units (EDUs), while internal tree nodes are labelled with discourse relations. In addition, each node is given a nuclearity attribute, which encodes the importance of the node in its local context. In the past decade, traditional, probabilistic approaches, such as Support Vector Machines (SVM) (Hernault et al., 2010; Ji and Eisenstein, 2014) and Conditional Random Fields (CRF) (Joty et al., 2015; Feng and Hirst, 2014), have dominated the field. More recently, neural approaches (Braud et al., 2016; Li et al., 2016; Braud et al., 2017; Yu et al., 2018; Liu and Lapata, 2017, 2018) have been explored, with limited success (Morey et al., 2017; Ferracane et al., 2019). The main reason why recent advances in deep learning have not enhanced discourse parsing to the same extend as they have revolutionized many other areas of NLP is the small amount of training data available. Existing corpora in English (Carlson et al., 2002; Subba and Di Eugenio, 2009) only comprise of a few hundred annotated documents, each typically containing a few dozen EDUs, strictly limiting the application of deep learning methodologies. Although in princ"
D19-1235,C16-1179,0,0.0162325,"iscourse tree for a given document, where leaf nodes are clause-like sentence fragments, called elementary-discourse-units (EDUs), while internal tree nodes are labelled with discourse relations. In addition, each node is given a nuclearity attribute, which encodes the importance of the node in its local context. In the past decade, traditional, probabilistic approaches, such as Support Vector Machines (SVM) (Hernault et al., 2010; Ji and Eisenstein, 2014) and Conditional Random Fields (CRF) (Joty et al., 2015; Feng and Hirst, 2014), have dominated the field. More recently, neural approaches (Braud et al., 2016; Li et al., 2016; Braud et al., 2017; Yu et al., 2018; Liu and Lapata, 2017, 2018) have been explored, with limited success (Morey et al., 2017; Ferracane et al., 2019). The main reason why recent advances in deep learning have not enhanced discourse parsing to the same extend as they have revolutionized many other areas of NLP is the small amount of training data available. Existing corpora in English (Carlson et al., 2002; Subba and Di Eugenio, 2009) only comprise of a few hundred annotated documents, each typically containing a few dozen EDUs, strictly limiting the application of deep lear"
D19-1235,P12-1007,0,0.0253313,"ation of MILNet is trained on the document-level sentiment. Next, shown in Figure 3, we again (a) segment the document into EDUs and use (b) the MIL network to generate fine-grain sentiment and importance scores. Then in (c), we prepare those scores to be used in (d), the CKY-like parser, which generates an optimal RST discourse-tree for the document, based on the EDU-level scores and the gold label document sentiment. 3.1 Segmentation and Preprocessing We initially separate the sentiment documents into a disjoint sequence of EDUs. The segmentation is obtained using the discourse segmenter by Feng and Hirst (2012) as generated and published by Angelidis and Lapata (2018). We preprocess the EDUs by removing infrequent- and stop-words and subsequently apply lemmatization. 3.2 Multiple-Instance Learning (MIL) Our MIL model is closely related to the methodology described in Angelidis and Lapata (2018), as well as the papers by Yang et al. (2016) and Ji and Smith (2017). The computation is based on the initial segmentation described in section 3.1 and is shown in further detail in Figure 4. Our model consists of two levels of Recurrent Neural Networks (RNN) inspired by Yang et al. (2016) and a sentiment- an"
D19-1235,P14-1048,0,0.0929611,"applied to PDTB as well. The RST discourse theory assumes a complete hierarchical discourse tree for a given document, where leaf nodes are clause-like sentence fragments, called elementary-discourse-units (EDUs), while internal tree nodes are labelled with discourse relations. In addition, each node is given a nuclearity attribute, which encodes the importance of the node in its local context. In the past decade, traditional, probabilistic approaches, such as Support Vector Machines (SVM) (Hernault et al., 2010; Ji and Eisenstein, 2014) and Conditional Random Fields (CRF) (Joty et al., 2015; Feng and Hirst, 2014), have dominated the field. More recently, neural approaches (Braud et al., 2016; Li et al., 2016; Braud et al., 2017; Yu et al., 2018; Liu and Lapata, 2017, 2018) have been explored, with limited success (Morey et al., 2017; Ferracane et al., 2019). The main reason why recent advances in deep learning have not enhanced discourse parsing to the same extend as they have revolutionized many other areas of NLP is the small amount of training data available. Existing corpora in English (Carlson et al., 2002; Subba and Di Eugenio, 2009) only comprise of a few hundred annotated documents, each typic"
D19-1235,P19-1062,0,0.0924755,"elled with discourse relations. In addition, each node is given a nuclearity attribute, which encodes the importance of the node in its local context. In the past decade, traditional, probabilistic approaches, such as Support Vector Machines (SVM) (Hernault et al., 2010; Ji and Eisenstein, 2014) and Conditional Random Fields (CRF) (Joty et al., 2015; Feng and Hirst, 2014), have dominated the field. More recently, neural approaches (Braud et al., 2016; Li et al., 2016; Braud et al., 2017; Yu et al., 2018; Liu and Lapata, 2017, 2018) have been explored, with limited success (Morey et al., 2017; Ferracane et al., 2019). The main reason why recent advances in deep learning have not enhanced discourse parsing to the same extend as they have revolutionized many other areas of NLP is the small amount of training data available. Existing corpora in English (Carlson et al., 2002; Subba and Di Eugenio, 2009) only comprise of a few hundred annotated documents, each typically containing a few dozen EDUs, strictly limiting the application of deep learning methodologies. Although in principle new corpora could be created, the annotation process is expensive and time consuming. It requires sophisticated linguistic expe"
D19-1235,D14-1168,1,0.871436,"indicate that while our parser does not yet match the performance of a parser trained and tested on the same dataset (intra-domain), it does perform remarkably well on the much more difficult and arguably more useful task of inter-domain discourse structure prediction, where the parser is trained on one domain and tested/applied on another one. 1 Introduction Discourse parsing is a fundamental NLP task known to enhance key downstream tasks, such as sentiment analysis (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015), text classification (Ji and Smith, 2017) and summarization (Gerani et al., 2014). In essence, a discourse parser should reveal the structure underlying coherent text as postulated by a discourse theory, of which the two most popular are Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) and PDTB (Prasad et al., 2008). In this paper, we focus on RST-style parsing, but the proposed approach is theory agnostic and could be applied to PDTB as well. The RST discourse theory assumes a complete hierarchical discourse tree for a given document, where leaf nodes are clause-like sentence fragments, called elementary-discourse-units (EDUs), while internal tree nodes are lab"
D19-1235,P14-1002,0,0.609403,"on RST-style parsing, but the proposed approach is theory agnostic and could be applied to PDTB as well. The RST discourse theory assumes a complete hierarchical discourse tree for a given document, where leaf nodes are clause-like sentence fragments, called elementary-discourse-units (EDUs), while internal tree nodes are labelled with discourse relations. In addition, each node is given a nuclearity attribute, which encodes the importance of the node in its local context. In the past decade, traditional, probabilistic approaches, such as Support Vector Machines (SVM) (Hernault et al., 2010; Ji and Eisenstein, 2014) and Conditional Random Fields (CRF) (Joty et al., 2015; Feng and Hirst, 2014), have dominated the field. More recently, neural approaches (Braud et al., 2016; Li et al., 2016; Braud et al., 2017; Yu et al., 2018; Liu and Lapata, 2017, 2018) have been explored, with limited success (Morey et al., 2017; Ferracane et al., 2019). The main reason why recent advances in deep learning have not enhanced discourse parsing to the same extend as they have revolutionized many other areas of NLP is the small amount of training data available. Existing corpora in English (Carlson et al., 2002; Subba and Di"
D19-1235,P17-1092,0,0.405479,"T-DT and Instructional domain). Results indicate that while our parser does not yet match the performance of a parser trained and tested on the same dataset (intra-domain), it does perform remarkably well on the much more difficult and arguably more useful task of inter-domain discourse structure prediction, where the parser is trained on one domain and tested/applied on another one. 1 Introduction Discourse parsing is a fundamental NLP task known to enhance key downstream tasks, such as sentiment analysis (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015), text classification (Ji and Smith, 2017) and summarization (Gerani et al., 2014). In essence, a discourse parser should reveal the structure underlying coherent text as postulated by a discourse theory, of which the two most popular are Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) and PDTB (Prasad et al., 2008). In this paper, we focus on RST-style parsing, but the proposed approach is theory agnostic and could be applied to PDTB as well. The RST discourse theory assumes a complete hierarchical discourse tree for a given document, where leaf nodes are clause-like sentence fragments, called elementary-discourse-units ("
D19-1235,J15-3002,1,0.901332,"ostic and could be applied to PDTB as well. The RST discourse theory assumes a complete hierarchical discourse tree for a given document, where leaf nodes are clause-like sentence fragments, called elementary-discourse-units (EDUs), while internal tree nodes are labelled with discourse relations. In addition, each node is given a nuclearity attribute, which encodes the importance of the node in its local context. In the past decade, traditional, probabilistic approaches, such as Support Vector Machines (SVM) (Hernault et al., 2010; Ji and Eisenstein, 2014) and Conditional Random Fields (CRF) (Joty et al., 2015; Feng and Hirst, 2014), have dominated the field. More recently, neural approaches (Braud et al., 2016; Li et al., 2016; Braud et al., 2017; Yu et al., 2018; Liu and Lapata, 2017, 2018) have been explored, with limited success (Morey et al., 2017; Ferracane et al., 2019). The main reason why recent advances in deep learning have not enhanced discourse parsing to the same extend as they have revolutionized many other areas of NLP is the small amount of training data available. Existing corpora in English (Carlson et al., 2002; Subba and Di Eugenio, 2009) only comprise of a few hundred annotate"
D19-1235,D16-1035,0,0.181655,"given document, where leaf nodes are clause-like sentence fragments, called elementary-discourse-units (EDUs), while internal tree nodes are labelled with discourse relations. In addition, each node is given a nuclearity attribute, which encodes the importance of the node in its local context. In the past decade, traditional, probabilistic approaches, such as Support Vector Machines (SVM) (Hernault et al., 2010; Ji and Eisenstein, 2014) and Conditional Random Fields (CRF) (Joty et al., 2015; Feng and Hirst, 2014), have dominated the field. More recently, neural approaches (Braud et al., 2016; Li et al., 2016; Braud et al., 2017; Yu et al., 2018; Liu and Lapata, 2017, 2018) have been explored, with limited success (Morey et al., 2017; Ferracane et al., 2019). The main reason why recent advances in deep learning have not enhanced discourse parsing to the same extend as they have revolutionized many other areas of NLP is the small amount of training data available. Existing corpora in English (Carlson et al., 2002; Subba and Di Eugenio, 2009) only comprise of a few hundred annotated documents, each typically containing a few dozen EDUs, strictly limiting the application of deep learning methodologie"
D19-1235,D17-1133,0,0.0608944,"nce fragments, called elementary-discourse-units (EDUs), while internal tree nodes are labelled with discourse relations. In addition, each node is given a nuclearity attribute, which encodes the importance of the node in its local context. In the past decade, traditional, probabilistic approaches, such as Support Vector Machines (SVM) (Hernault et al., 2010; Ji and Eisenstein, 2014) and Conditional Random Fields (CRF) (Joty et al., 2015; Feng and Hirst, 2014), have dominated the field. More recently, neural approaches (Braud et al., 2016; Li et al., 2016; Braud et al., 2017; Yu et al., 2018; Liu and Lapata, 2017, 2018) have been explored, with limited success (Morey et al., 2017; Ferracane et al., 2019). The main reason why recent advances in deep learning have not enhanced discourse parsing to the same extend as they have revolutionized many other areas of NLP is the small amount of training data available. Existing corpora in English (Carlson et al., 2002; Subba and Di Eugenio, 2009) only comprise of a few hundred annotated documents, each typically containing a few dozen EDUs, strictly limiting the application of deep learning methodologies. Although in principle new corpora could be created, the"
D19-1235,Q18-1005,0,0.062435,"Missing"
D19-1235,E12-1062,0,0.0328097,"g on the level in the discourse tree. Ji and Smith (2017) use discourse trees generated by a discourse parser (Ji and Eisenstein, 2014) to inform a recursive neural network and automatically learn the model weights for sentiment prediction. In this paper, we exploit the relation between sentiment analysis and discourse parsing in the opposite direction by using sentiment annotations to create discourse structures. The third area of related work is distant supervision aimed at automatically generating fine-grained annotations. Distant supervision has previously been used to retrieve sentiment (Marchetti-Bowick and Chambers, 2012; Tabassum et al., 2016) and emotion classes (AbdulMageed and Ungar, 2017) from opinionated text, showing the potential of distant supervision with user generated content. A common technique for distant supervision is multiple-instance learning (Keeler and Rumelhart, 1992), where the general idea is to retrieve fine-grained information from high-level signals. High-level signals are called bags and fine-grained information is referred to as instances. The task is defined as the generation of instance labels solely based on the given bag labels. We follow the approach by Angelidis and Lapata (2"
D19-1235,D17-1136,0,0.163151,"l tree nodes are labelled with discourse relations. In addition, each node is given a nuclearity attribute, which encodes the importance of the node in its local context. In the past decade, traditional, probabilistic approaches, such as Support Vector Machines (SVM) (Hernault et al., 2010; Ji and Eisenstein, 2014) and Conditional Random Fields (CRF) (Joty et al., 2015; Feng and Hirst, 2014), have dominated the field. More recently, neural approaches (Braud et al., 2016; Li et al., 2016; Braud et al., 2017; Yu et al., 2018; Liu and Lapata, 2017, 2018) have been explored, with limited success (Morey et al., 2017; Ferracane et al., 2019). The main reason why recent advances in deep learning have not enhanced discourse parsing to the same extend as they have revolutionized many other areas of NLP is the small amount of training data available. Existing corpora in English (Carlson et al., 2002; Subba and Di Eugenio, 2009) only comprise of a few hundred annotated documents, each typically containing a few dozen EDUs, strictly limiting the application of deep learning methodologies. Although in principle new corpora could be created, the annotation process is expensive and time consuming. It requires soph"
D19-1235,W17-5535,1,0.884278,"it with parsers trained on human-annotated corpora (news domain RST-DT and Instructional domain). Results indicate that while our parser does not yet match the performance of a parser trained and tested on the same dataset (intra-domain), it does perform remarkably well on the much more difficult and arguably more useful task of inter-domain discourse structure prediction, where the parser is trained on one domain and tested/applied on another one. 1 Introduction Discourse parsing is a fundamental NLP task known to enhance key downstream tasks, such as sentiment analysis (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015), text classification (Ji and Smith, 2017) and summarization (Gerani et al., 2014). In essence, a discourse parser should reveal the structure underlying coherent text as postulated by a discourse theory, of which the two most popular are Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) and PDTB (Prasad et al., 2008). In this paper, we focus on RST-style parsing, but the proposed approach is theory agnostic and could be applied to PDTB as well. The RST discourse theory assumes a complete hierarchical discourse tree for a given document, where leaf nodes are"
D19-1235,C18-1047,0,0.126993,"clause-like sentence fragments, called elementary-discourse-units (EDUs), while internal tree nodes are labelled with discourse relations. In addition, each node is given a nuclearity attribute, which encodes the importance of the node in its local context. In the past decade, traditional, probabilistic approaches, such as Support Vector Machines (SVM) (Hernault et al., 2010; Ji and Eisenstein, 2014) and Conditional Random Fields (CRF) (Joty et al., 2015; Feng and Hirst, 2014), have dominated the field. More recently, neural approaches (Braud et al., 2016; Li et al., 2016; Braud et al., 2017; Yu et al., 2018; Liu and Lapata, 2017, 2018) have been explored, with limited success (Morey et al., 2017; Ferracane et al., 2019). The main reason why recent advances in deep learning have not enhanced discourse parsing to the same extend as they have revolutionized many other areas of NLP is the small amount of training data available. Existing corpora in English (Carlson et al., 2002; Subba and Di Eugenio, 2009) only comprise of a few hundred annotated documents, each typically containing a few dozen EDUs, strictly limiting the application of deep learning methodologies. Although in principle new corpora"
D19-1235,D14-1162,0,0.0834009,"Missing"
D19-1235,prasad-etal-2008-penn,0,0.026285,"e structure prediction, where the parser is trained on one domain and tested/applied on another one. 1 Introduction Discourse parsing is a fundamental NLP task known to enhance key downstream tasks, such as sentiment analysis (Bhatia et al., 2015; Nejat et al., 2017; Hogenboom et al., 2015), text classification (Ji and Smith, 2017) and summarization (Gerani et al., 2014). In essence, a discourse parser should reveal the structure underlying coherent text as postulated by a discourse theory, of which the two most popular are Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) and PDTB (Prasad et al., 2008). In this paper, we focus on RST-style parsing, but the proposed approach is theory agnostic and could be applied to PDTB as well. The RST discourse theory assumes a complete hierarchical discourse tree for a given document, where leaf nodes are clause-like sentence fragments, called elementary-discourse-units (EDUs), while internal tree nodes are labelled with discourse relations. In addition, each node is given a nuclearity attribute, which encodes the importance of the node in its local context. In the past decade, traditional, probabilistic approaches, such as Support Vector Machines (SVM)"
D19-1235,N09-1064,0,0.44335,"Missing"
D19-1235,D16-1030,0,0.0363752,"Missing"
D19-1235,D15-1167,0,0.429027,"e time, semantically related EDUs generally have a shorter distance than semantically unrelated EDUs. Our approach combines a neural variant of multiple-instance learning (MILNet) (Angelidis and Lapata, 2018), with an optimal CKY-style tree generation algorithm (Jurafsky and Martin, 2014). First, MILNet computes fine-grained sentiment values and un-normalized attention scores (Ji and Smith, 2017) on EDU-level, by solely relying on distant supervision signals from document-level annotations. These annotations are abundantly available from several published open source datasets such as Yelp’13 (Tang et al., 2015), IMDB (Diao et al., 2014) or Amazon (Zhang et al., 2015). Then, the sentiment values and attention scores are aggregated to guide the discourse-tree construction, optimized on the document gold-label sentiment, using optimal CKY-style parsing. Following this approach, we generate a new corpus annotated with “silver standard” discourse trees, which comprises of 100k documents (two orders of magnitude more than any existing corpora). To test the quality of our new corpus, we run a series of experiments, where we train the top performing discourse parser by Wang et al. (2017) on our corpus for d"
D19-1235,P17-2029,0,0.435286,"ts such as Yelp’13 (Tang et al., 2015), IMDB (Diao et al., 2014) or Amazon (Zhang et al., 2015). Then, the sentiment values and attention scores are aggregated to guide the discourse-tree construction, optimized on the document gold-label sentiment, using optimal CKY-style parsing. Following this approach, we generate a new corpus annotated with “silver standard” discourse trees, which comprises of 100k documents (two orders of magnitude more than any existing corpora). To test the quality of our new corpus, we run a series of experiments, where we train the top performing discourse parser by Wang et al. (2017) on our corpus for discourse structure prediction and compare it with the same parser trained on human annotated corpora in the news domain (RSTDT) and in the instructional domain. Results indicate that while training a parser on our corpus does not yet match the performance of a parser trained and tested on the same dataset (intra-domain), it does perform remarkably well on the significantly more difficult and arguably more useful task of inter-domain discourse structure prediction, where the parser is trained on one domain and tested/applied on another one. Our results on inter-domain discou"
D19-1235,N16-1174,0,0.0622553,"or the document, based on the EDU-level scores and the gold label document sentiment. 3.1 Segmentation and Preprocessing We initially separate the sentiment documents into a disjoint sequence of EDUs. The segmentation is obtained using the discourse segmenter by Feng and Hirst (2012) as generated and published by Angelidis and Lapata (2018). We preprocess the EDUs by removing infrequent- and stop-words and subsequently apply lemmatization. 3.2 Multiple-Instance Learning (MIL) Our MIL model is closely related to the methodology described in Angelidis and Lapata (2018), as well as the papers by Yang et al. (2016) and Ji and Smith (2017). The computation is based on the initial segmentation described in section 3.1 and is shown in further detail in Figure 4. Our model consists of two levels of Recurrent Neural Networks (RNN) inspired by Yang et al. (2016) and a sentiment- and attention-module. The computational flow in the model is defined 2308 Figure 2: First stage, training the MIL model on the document-level sentiment prediction task Figure 3: Second stage, using the neural MIL model to retrieve fine-grained sentiment and attention scores (star/triangle), used for the CKY computation to generate the"
D19-1298,N18-1150,0,0.0345933,"ntly introduce two large-scale datasets of long and structured scientific papers obtained from arXiv and PubMed. These two new datasets contain much longer documents than all the news datasets (See Table 1) and are therefore ideal test-beds for the method we present in this paper. Neural Abstractive summarization on long documents While most current neural abstractive summarization models have focused on summarizing relatively short news articles (e.g., (See et al., 2017)), few researchers have started to investigate the summarization of longer documents by exploiting their natural structure. Celikyilmaz et al. (2018) present an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. The encoding task is divided across several collaborating agents, each is responsible for a subsection of text through a multi-layer LSTM with word attention. Their model seems however overly complicated when it comes to the extractive summarization task, where word attention is arguably much less critical. So, we do not consider this model further in this paper. Cohan et al. (2018) also propose a model for abstractive summarization taking the structure of documents"
D19-1298,P16-1046,0,0.386743,"s redundant, they may end up including misleading or even utterly false statements, because the methods to extract and aggregate information form the source document are still rather noisy. In this work, we focus on extracting informative sentences from a given document (without dealing with redundancy), especially when the document is relatively long (e.g., scientific articles). Most recent works on neural extractive summarization have been rather successful in generating summaries of short news documents (around 650 words/document) (Nallapati et al., 2016) by applying neural Seq2Seq models (Cheng and Lapata, 2016). However when it comes to long documents, these models tend to struggle with longer sequences because at each decoding step, the decoder needs to learn to construct a context vector capturing relevant information from all the tokens in the source sequence (Shao et al., 2017). Long documents typically cover multiple topics. In general, the longer a document is, the more topics are discussed. As a matter of fact, when humans write long documents they organize them in chapters, sections etc.. Scientific papers are an example of longer documents and they follow a standard discourse structure desc"
D19-1298,W14-4012,0,0.107278,"Missing"
D19-1298,N18-2097,0,0.434916,"fectively summarizing long documents. In particular, while overall we outperform the baseline and previous approaches only by a narrow margin on both datasets, the benefit of our method become much stronger as we apply it to longer documents. Furthermore, in an ablation study to assess the relative contributions of the global and the local model we found that, rather surprisingly, the benefits of our model seem to come exclusively from modeling the local context, even for the longest documents.VI (iii) In order to evaluate our approach, we have created oracle labels for both Pubmed and arXiv (Cohan et al., 2018), by applying a greedy oracle labeling algorithm. The two datasets annotated with extractive labels will be made public.1 success of neural sequence models in other NLP tasks, Cheng and Lapata (2016) propose a novel approach to extractive summarization based on neural networks and continuous sentence features, which outperforms traditional methods on the DailyMail dataset. In particular, they develop a general encoder-decoder architecture, where a CNN is used as sentence encoder, a uni-directional LSTM as document encoder, with another uni-directional LSTM as decoder. To decrease the number of"
D19-1298,D15-1045,0,0.0981935,"ourse categories (e.g. Background, Hypothesis, Motivation, etc.) at the sentence level. The recent work most similar to ours is (Collins et al., 2017) where, in order to determine whether a sentence should be included in the summary, they directly use the section each sentence appears in as a categorical feature with values like Highlight, Abstract, Introduction, etc.. In this paper, instead of using sections as categorical features, we rely on a distributed representation of the semantic information within each section, as the local context of each sentence. In a very different line of work, Cohan and Goharian (2015) form the summary by also exploiting information on how the target paper is cited in other papers. Currently, we do not use any information from citation contexts. 2.3 Datasets for long documents Dernoncourt et al. (2018) provide a comprehensive overview of the current datasets for summarization. Noticeably, most of the larger-scale summarization datasets consists of relatively short documents, like CNN/DailyMail (Nallapati et al., 2016) and New York Times (Sandhaus, 2008). One exception is (Cohan et al., 2018) that recently introduce two large-scale datasets of long and structured scientific"
D19-1298,K17-1021,0,0.38891,"(Shao et al., 2017). Long documents typically cover multiple topics. In general, the longer a document is, the more topics are discussed. As a matter of fact, when humans write long documents they organize them in chapters, sections etc.. Scientific papers are an example of longer documents and they follow a standard discourse structure describing the problem, methodology, experiments/results, and finally conclusions (Suppe, 1998). To the best of our knowledge only one previous work in extractive summarization has explicitly leveraged section information to guide the generation of summaries (Collins et al., 2017). However, the only information about sections fed into their 3011 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3011–3021, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics sentence classifier is a categorical feature with values like Highlight, Abstract, Introduction, etc., depending on which section the sentence appears in. In contrast, in order to exploit section information, in this paper we propose to capture a distributed represen"
D19-1298,D16-1001,0,0.0815413,"entence appears in. In contrast, in order to exploit section information, in this paper we propose to capture a distributed representation of both the global (the whole document) and the local context (e.g., the section/topic) when deciding if a sentence should be included in the summary Our main contributions are as follows: (i) In order to capture the local context, we are the first to apply LSTM-minus to text summarization. LSTM-minus is a method for learning embeddings of text spans, which has achieved good performance in dependency parsing (Wang and Chang, 2016), in constituency parsing (Cross and Huang, 2016), as well as in discourse parsing (Liu and Lapata, 2017). With respect to more traditional methods for capturing local context, which rely on hierarchical structures, LSTM-minus produces simpler models i.e. with less parameters, and therefore faster to train and less prone to overfitting. (ii) We test our method on the Pubmed and arXiv datasets and results appear to support our goal of effectively summarizing long documents. In particular, while overall we outperform the baseline and previous approaches only by a narrow margin on both datasets, the benefit of our method become much stronger as"
D19-1298,W14-3348,0,0.0093799,"6 9.93 13.89 8.52 10.22 15.37 19.17 18.53 18.78 19.74 19.70 12.28 27.48 ROUGE-L 33.43 29.70 34.59 27.38 29.69 35.21 30.89 30.17 30.36 31.48 31.43 25.17 38.66 METEOR 20.56 20.34 20.42 20.83 20.83 16.19 23.60 Table 3: Results on the Pubmed dataset. See caption of Table 2 above for details on compared models and notation. by selecting the top ranked sentences by model probability p(yi |W, b), until the length limit is met or exceeded. Based on the average length of abstracts in these two datasets, we set the length limit to 200 words. We use ROUGE scores6 (Lin and Hovy, 2003) and METEOR scores7 (Denkowski and Lavie, 2014) between the model results and ground-truth abstractive summaries as evaluation metric. The unigram and bigram overlap (ROUGE-1,2) are intended to measure the informativeness, while longest common subsequence (ROUGE-L) captures fluency to some extent (Cheng and Lapata, 2016). METEOR was originally proposed to evaluate translation systems by measuring the alignment between the system output and reference translations. As such, it can also be used as an automatic evaluation metric for summarization (Kedzie et al., 2018). The performance of all models on arXiv and Pubmed is shown in Table 2 and T"
D19-1298,L18-1509,0,0.0318888,"the summary, they directly use the section each sentence appears in as a categorical feature with values like Highlight, Abstract, Introduction, etc.. In this paper, instead of using sections as categorical features, we rely on a distributed representation of the semantic information within each section, as the local context of each sentence. In a very different line of work, Cohan and Goharian (2015) form the summary by also exploiting information on how the target paper is cited in other papers. Currently, we do not use any information from citation contexts. 2.3 Datasets for long documents Dernoncourt et al. (2018) provide a comprehensive overview of the current datasets for summarization. Noticeably, most of the larger-scale summarization datasets consists of relatively short documents, like CNN/DailyMail (Nallapati et al., 2016) and New York Times (Sandhaus, 2008). One exception is (Cohan et al., 2018) that recently introduce two large-scale datasets of long and structured scientific papers obtained from arXiv and PubMed. These two new datasets contain much longer documents than all the news datasets (See Table 1) and are therefore ideal test-beds for the method we present in this paper. Neural Abstra"
D19-1298,N19-1423,0,0.0253089,"in (Nallapati et al., 2017), CNN in (Cheng and Lapata, 2016), and Average Word Embedding in (Kedzie et al., 2018). Kedzie et al. (2018) experiment with all the three methods and conclude that Word Embedding Averaging is as good or better than either RNNs or CNNs for sentence embedding across different domains and summarizer architectures. Thus, we use the Average Word Embedding as our sentence encoder, by which a sentence embedding is simply the average of its word embeddings, i.e. w se = n 1X emb(wi ), se ∈ Rdemb . n w 0 Besides, we also tried the popular pre-trained BERT sentence embedding (Devlin et al., 2019), but initial results were rather poor. So we do not pursue this possibility any further. 3.2 Document Encoder At the document level, a bi-directional recurrent neural network (Schuster and Paliwal, 1997) is often used to encode all the sentences sequentially forward and backward, with such model achieving remarkable success in machine translation (Bahdanau et al., 2015). As units, we selected gated recurrent units (GRU) (Cho et al., 2014), in light of favorable results shown in (Chung et al., 2014). The GRU is represented with the standard reset, update, and new gates. The output of the bi-di"
D19-1298,D18-1208,0,0.167246,"aintaining the accuracy, Nallapati et al. (2017) present SummaRuNNer, a simple RNN-based sequence classifier without decoder, outperforming or matching the model of (Cheng and Lapata, 2016). They take content, salience, novelty, and position of each sentence into consideration when deciding if a sentence should be included in the extractive summary. Yet, they do not capture any aspect of the topical structure, as we do in this paper. So their approach would arguably suffer when applied to long documents, likely containing multiple and diverse topics. While SummaRuNNer was tested only on news, Kedzie et al. (2018) carry out a comprehensive set of experiments with deep learning models of extractive summarization across different domains, i.e. news, personal stories, meetings, and medical articles, as well as across different neural architectures, in order to better understand the general pros and cons of different design choices. They find that non auto-regressive sentence extraction performs as well or better than auto-regressive extraction in all domains, where by auto-regressive sentence extraction they mean using previous predictions to inform future predictions. Furthermore, they find that the Aver"
D19-1298,D18-1205,0,0.0327197,"Missing"
D19-1298,D13-1070,0,0.054298,"th 656 693 530 3016 4938 avg. summ. length 43 52 38 203 220 2.4 Table 1: Comparison of news datasets and scientific paper datasets(Cohan et al., 2018), the length is in terms of the number of words classifier to select content from a scientific paper based on the rhetorical status of each sentence (e.g., whether it specified a research goal, or some generally accepted scientific background knowledge, etc.). More recently, researchers have extended this work by applying more sophisticated classifiers to identify more fine-grain rhetorical categories, as well as by exploiting citation contexts. Liakata et al. (2013) propose the CoreSC discourse-driven content, which relies on CRFs and SVMs, to classify the discourse categories (e.g. Background, Hypothesis, Motivation, etc.) at the sentence level. The recent work most similar to ours is (Collins et al., 2017) where, in order to determine whether a sentence should be included in the summary, they directly use the section each sentence appears in as a categorical feature with values like Highlight, Abstract, Introduction, etc.. In this paper, instead of using sections as categorical features, we rely on a distributed representation of the semantic informati"
D19-1298,N03-1020,0,0.444996,".89 44.81 44.85 35.63 55.05 ROUGE-2 11.36 9.93 13.89 8.52 10.22 15.37 19.17 18.53 18.78 19.74 19.70 12.28 27.48 ROUGE-L 33.43 29.70 34.59 27.38 29.69 35.21 30.89 30.17 30.36 31.48 31.43 25.17 38.66 METEOR 20.56 20.34 20.42 20.83 20.83 16.19 23.60 Table 3: Results on the Pubmed dataset. See caption of Table 2 above for details on compared models and notation. by selecting the top ranked sentences by model probability p(yi |W, b), until the length limit is met or exceeded. Based on the average length of abstracts in these two datasets, we set the length limit to 200 words. We use ROUGE scores6 (Lin and Hovy, 2003) and METEOR scores7 (Denkowski and Lavie, 2014) between the model results and ground-truth abstractive summaries as evaluation metric. The unigram and bigram overlap (ROUGE-1,2) are intended to measure the informativeness, while longest common subsequence (ROUGE-L) captures fluency to some extent (Cheng and Lapata, 2016). METEOR was originally proposed to evaluate translation systems by measuring the alignment between the system output and reference translations. As such, it can also be used as an automatic evaluation metric for summarization (Kedzie et al., 2018). The performance of all model"
D19-1298,D17-1133,0,0.0948487,"ion information, in this paper we propose to capture a distributed representation of both the global (the whole document) and the local context (e.g., the section/topic) when deciding if a sentence should be included in the summary Our main contributions are as follows: (i) In order to capture the local context, we are the first to apply LSTM-minus to text summarization. LSTM-minus is a method for learning embeddings of text spans, which has achieved good performance in dependency parsing (Wang and Chang, 2016), in constituency parsing (Cross and Huang, 2016), as well as in discourse parsing (Liu and Lapata, 2017). With respect to more traditional methods for capturing local context, which rely on hierarchical structures, LSTM-minus produces simpler models i.e. with less parameters, and therefore faster to train and less prone to overfitting. (ii) We test our method on the Pubmed and arXiv datasets and results appear to support our goal of effectively summarizing long documents. In particular, while overall we outperform the baseline and previous approaches only by a narrow margin on both datasets, the benefit of our method become much stronger as we apply it to longer documents. Furthermore, in an abl"
D19-1298,W04-3252,0,0.795563,"l or better than auto-regressive extraction in all domains, where by auto-regressive sentence extraction they mean using previous predictions to inform future predictions. Furthermore, they find that the Average Word Embedding sentence encoder works at least as well as encoders based on CNN and RNN. In light of these findings, our model is not auto-regressive and uses the Average Word Embedding encoder. 2 2.2 Related work 2.1 Extractive summarization Traditional extractive summarization methods are mostly based on explicit surface features (Radev et al., 2004), relying on graph-based methods (Mihalcea and Tarau, 2004), or on submodular maximization (Tixier et al., 2017). Benefiting from the 1 The data and code are available at https: //github.com/Wendy-Xiao/Extsumm_local_ global_context. Extractive summarization on Scientific papers Research on summarizing scientific articles has a long history (Nenkova et al., 2011). Earlier on, it was realized that summarizing scientific papers requires different approaches than what was used for summarizing news articles, due to differences in document length, writing style and rhetorical structure. For instance, (Teufel and Moens, 2002) presented a supervised Naive Bay"
D19-1298,K16-1028,0,0.129815,"Missing"
D19-1298,W01-0100,0,0.762473,"Missing"
D19-1298,P17-1099,0,0.666678,"t documents, like CNN/DailyMail (Nallapati et al., 2016) and New York Times (Sandhaus, 2008). One exception is (Cohan et al., 2018) that recently introduce two large-scale datasets of long and structured scientific papers obtained from arXiv and PubMed. These two new datasets contain much longer documents than all the news datasets (See Table 1) and are therefore ideal test-beds for the method we present in this paper. Neural Abstractive summarization on long documents While most current neural abstractive summarization models have focused on summarizing relatively short news articles (e.g., (See et al., 2017)), few researchers have started to investigate the summarization of longer documents by exploiting their natural structure. Celikyilmaz et al. (2018) present an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. The encoding task is divided across several collaborating agents, each is responsible for a subsection of text through a multi-layer LSTM with word attention. Their model seems however overly complicated when it comes to the extractive summarization task, where word attention is arguably much less critical. So, we do no"
D19-1298,J02-4002,0,0.0436875,"elying on graph-based methods (Mihalcea and Tarau, 2004), or on submodular maximization (Tixier et al., 2017). Benefiting from the 1 The data and code are available at https: //github.com/Wendy-Xiao/Extsumm_local_ global_context. Extractive summarization on Scientific papers Research on summarizing scientific articles has a long history (Nenkova et al., 2011). Earlier on, it was realized that summarizing scientific papers requires different approaches than what was used for summarizing news articles, due to differences in document length, writing style and rhetorical structure. For instance, (Teufel and Moens, 2002) presented a supervised Naive Bayes 3012 Datasets CNN Daily Mail NY Times PubMed arXiv # docs 92K 219K 655K 133K 215K avg. doc. length 656 693 530 3016 4938 avg. summ. length 43 52 38 203 220 2.4 Table 1: Comparison of news datasets and scientific paper datasets(Cohan et al., 2018), the length is in terms of the number of words classifier to select content from a scientific paper based on the rhetorical status of each sentence (e.g., whether it specified a research goal, or some generally accepted scientific background knowledge, etc.). More recently, researchers have extended this work by app"
D19-1298,W17-4507,0,0.103478,", where by auto-regressive sentence extraction they mean using previous predictions to inform future predictions. Furthermore, they find that the Average Word Embedding sentence encoder works at least as well as encoders based on CNN and RNN. In light of these findings, our model is not auto-regressive and uses the Average Word Embedding encoder. 2 2.2 Related work 2.1 Extractive summarization Traditional extractive summarization methods are mostly based on explicit surface features (Radev et al., 2004), relying on graph-based methods (Mihalcea and Tarau, 2004), or on submodular maximization (Tixier et al., 2017). Benefiting from the 1 The data and code are available at https: //github.com/Wendy-Xiao/Extsumm_local_ global_context. Extractive summarization on Scientific papers Research on summarizing scientific articles has a long history (Nenkova et al., 2011). Earlier on, it was realized that summarizing scientific papers requires different approaches than what was used for summarizing news articles, due to differences in document length, writing style and rhetorical structure. For instance, (Teufel and Moens, 2002) presented a supervised Naive Bayes 3012 Datasets CNN Daily Mail NY Times PubMed arXiv"
D19-1298,D14-1162,0,0.0913015,"bmed and arXiv datasets, the extractive summaries are missing. So we follow the work of (Kedzie et al., 2018) on extractive summary labeling, constructing gold label sequences by greedily optimizing ROUGE-1 on the gold-standard abstracts, which are available for each article. 4 The algorithm is shown in Appendix A. 4.3 Implementation Details We train our model using the Adam optimizer (Kingma and Ba, 2015) with learning rate 0.0001 and a drop out rate of 0.3. We use a mini-batch with a batch size of 32 documents, and the size of the GRU hidden states is 300. For word embeddings, we use GloVe (Pennington et al., 2014) with dimension 300, pre-trained on the Wikipedia and Gigaword. The vocabulary size of our model is 50000. All the above parameters were set based on (Kedzie et al., 2018) without any fine-tuning. Again following (Kedzie et al., 2018), we train each model for 50 epochs, and the best model is selected with early stopping on the validation set according to Rouge-2 F-score. 4.4 Models for Comparison We perform a systematic comparison with previous work in extractive summarization. For completeness, we also compare with recent neural abstractive approaches. In all the experiments, we use the same"
D19-1298,radev-etal-2004-mead,0,0.102783,"Missing"
D19-1298,P16-1218,0,0.167976,"oduction, etc., depending on which section the sentence appears in. In contrast, in order to exploit section information, in this paper we propose to capture a distributed representation of both the global (the whole document) and the local context (e.g., the section/topic) when deciding if a sentence should be included in the summary Our main contributions are as follows: (i) In order to capture the local context, we are the first to apply LSTM-minus to text summarization. LSTM-minus is a method for learning embeddings of text spans, which has achieved good performance in dependency parsing (Wang and Chang, 2016), in constituency parsing (Cross and Huang, 2016), as well as in discourse parsing (Liu and Lapata, 2017). With respect to more traditional methods for capturing local context, which rely on hierarchical structures, LSTM-minus produces simpler models i.e. with less parameters, and therefore faster to train and less prone to overfitting. (ii) We test our method on the Pubmed and arXiv datasets and results appear to support our goal of effectively summarizing long documents. In particular, while overall we outperform the baseline and previous approaches only by a narrow margin on both datasets,"
D19-1298,W05-0908,0,0.220429,"to measure the informativeness, while longest common subsequence (ROUGE-L) captures fluency to some extent (Cheng and Lapata, 2016). METEOR was originally proposed to evaluate translation systems by measuring the alignment between the system output and reference translations. As such, it can also be used as an automatic evaluation metric for summarization (Kedzie et al., 2018). The performance of all models on arXiv and Pubmed is shown in Table 2 and Table 3, respectively. Follow the work (Kedzie et al., 2018), we use the approximate randomization as the statistical significance test method (Riezler and Maxwell, 2005) with a Bonferroni correction for multiple comparisons, at the confidence level 0.01 (p < 0.01). As we can see in these tables, on both datasets, the neural extractive models outperforms the traditional extractive models on informativeness (ROUGE-1,2) by a wide margin, but results are mixed on ROUGE-L. Presumably, this is due to the neural training process, which relies on a goal standard based on ROUGE-1. Exploring other training schemes and/or a combination of 6 We use a modified version of rouge papier, a python wrapper of ROUGE-1.5.5, https://github. com/kedz/rouge_papier. The command line"
D19-1349,W13-0102,0,0.361098,"ign detection (Paul and Dredze, 2014) and medical issue analysis (Huang et al., 2015, 2017). To reliably utilize topic models trained for these tasks, we need to evaluate them carefully and ensure that they have as high quality as possible. When topic models are used in an extrinsic task, like text categorization, they can be assessed by measuring how effectively they contribute to that task (Chen et al., 2016; Huang et al., 2015). However, when they are generated for human consumption, their evaluation is more challenging. In such cases, interpretability is critical, and Chang et al. (2009); Aletras and Stevenson (2013) have shown that the standard way to evaluate the output of a probabilistic model, by measuring perplexity on held-out data (Wallach et al., 1 Our code and data are available here. 2009), does not imply that the inferred topics are human-interpretable. A topic inferred by LDA is typically represented by the set of words with the highest probability given the topic. With this characteristic, we can evaluate the topic quality by determining how coherent the set of topic words is. While a variety of techniques (Section 2) have been geared towards measuring the topic quality in this way, in this p"
D19-1349,D18-1098,0,0.0454009,"s the measure of Inter-Annotator Agreement (IAA) for three datasets. The average human rating score/IAA for 20NG, Wiki and NYT are 2.91/0.71, 3.23/0.82 and 3.06/0.69, respectively. 5.2 Experimental Design Topic Modeling Following the settings in Xing and Paul (2018), we ran the LDA Gibbs samplers for 2,000 iterations (Griffiths and Steyvers, 2004) for each datasets, with 1,000 burn-in iterations, collecting samples every 10 iterations for the final 1,000 iterations. The set of estimates Θ thus contains 100 samples. Estimator Training was performed following the cross-domain training strategy (Bhatia et al., 2018). With the ground truth (human judgments), we train the estimator on all topics over one dataset, and test it on another (one-to-one). To enlarge the training set, we also train the estimator on two datasets merged together and test 6 https://www.figure-eight.com/ it on the third one (two-to-one). Given the limited amount of data and the need for interpretability, we experimented only with non-neural classifiers, including linear regression, nearest neighbors regression, Bayesian regression, and Support Vector Regression (SVR) using sklearn (Pedregosa et al., 2011); we report the results with"
D19-1349,D08-1038,0,0.0607881,"e variability of the posterior distributions. Compared to several existing baselines for automatic topic evaluation, the proposed metric achieves state-of-the-art correlations with human judgments of topic quality in experiments on three corpora.1 We additionally demonstrate that topic quality estimation can be further improved using a supervised estimator that combines multiple metrics. 1 Introduction Latent Dirichlet Allocation (LDA) (Blei et al., 2003) topic modeling has been widely used for NLP tasks which require the extraction of latent themes, such as scientific article topic analysis (Hall et al., 2008), news media tracking (Roberts et al., 2013), online campaign detection (Paul and Dredze, 2014) and medical issue analysis (Huang et al., 2015, 2017). To reliably utilize topic models trained for these tasks, we need to evaluate them carefully and ensure that they have as high quality as possible. When topic models are used in an extrinsic task, like text categorization, they can be assessed by measuring how effectively they contribute to that task (Chen et al., 2016; Huang et al., 2015). However, when they are generated for human consumption, their evaluation is more challenging. In such case"
D19-1349,N18-1099,1,0.901672,"e of work exploits the co-occurrence statistics indirectly. Aletras and Stevenson (2013) devised a new method by mapping the topic words into a semantic space and then computing the pairwise distributional similarity (DS) of words in that space. However, the semantic space is still built on PMI or NPMI. Roder et al. (2015) studied a unifying framework to explore a set of co-occurrence based topic quality measures and their parameters, identifying two complex combinations, (named CV and CP in that paper2 ), as the best performers on their test corpora. Posterior Based Method Recently, Xing and Paul (2018) analyzed how the posterior of LDA parameters vary during Gibbs sampling inference (Geman and Geman, 1984; Griffiths and Steyvers, 2004) and proposed a new topic quality measurement named Topic Stability. The Gibbs sampling for LDA generates estimates for two distributions: for topics given a document (θ), and for words given a topic (φ). Topic stability considers φ and is defined as: stability(Φk ) = 1 X sim(φk , φ¯k ) (1) |Φk | φk ∈Φk The stability of topic k is computed as the mean cosine similarity between the mean (φ¯k ) of all the sampled topic k’s distribution estimates (Φk ) and topic"
D19-1349,Y15-1064,0,0.0259867,"ves state-of-the-art correlations with human judgments of topic quality in experiments on three corpora.1 We additionally demonstrate that topic quality estimation can be further improved using a supervised estimator that combines multiple metrics. 1 Introduction Latent Dirichlet Allocation (LDA) (Blei et al., 2003) topic modeling has been widely used for NLP tasks which require the extraction of latent themes, such as scientific article topic analysis (Hall et al., 2008), news media tracking (Roberts et al., 2013), online campaign detection (Paul and Dredze, 2014) and medical issue analysis (Huang et al., 2015, 2017). To reliably utilize topic models trained for these tasks, we need to evaluate them carefully and ensure that they have as high quality as possible. When topic models are used in an extrinsic task, like text categorization, they can be assessed by measuring how effectively they contribute to that task (Chen et al., 2016; Huang et al., 2015). However, when they are generated for human consumption, their evaluation is more challenging. In such cases, interpretability is critical, and Chang et al. (2009); Aletras and Stevenson (2013) have shown that the standard way to evaluate the output"
D19-1349,E14-1056,0,0.0390238,"to evaluate the quality of LDA topic models: Co-occurrence Based Methods and Posterior Based Methods. Co-occurrence Based Methods Most prominent topic quality evaluations use various pairwise co-occurrence statistics to estimate topic’s semantic similarity. Mimno et al. (2011) proposed the Coherence metric, which is the summation of the conditional probability of each topic word given all other words. Newman et al. (2010) showed that the summation of the pairwise pointwise mutual information (PMI) of all possible topic word pairs is also an effective metric to assess topic quality. Later, in Lau et al. (2014), PMI was replaced 3471 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3471–3477, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Figure 1: Two example topics and their distributions of µ, σ and cv from the NYT corpus. Two topics are: Topic1 (in blue): {financial, banks, bank, money, debt, fund, loans, investors, funds, hedge}. Topic2 (in orange): {world, one, like, good, even, know, think, get, many, got}. Their human rating scores are"
D19-1349,D11-1024,0,0.0895915,"Missing"
D19-1349,N10-1012,0,0.237904,"opic word co-occurrence. Our novel estimator further improves the topic quality assessment on two out of the three corpora we have. 2 Automatic Topic Quality Evaluation There are two common ways to evaluate the quality of LDA topic models: Co-occurrence Based Methods and Posterior Based Methods. Co-occurrence Based Methods Most prominent topic quality evaluations use various pairwise co-occurrence statistics to estimate topic’s semantic similarity. Mimno et al. (2011) proposed the Coherence metric, which is the summation of the conditional probability of each topic word given all other words. Newman et al. (2010) showed that the summation of the pairwise pointwise mutual information (PMI) of all possible topic word pairs is also an effective metric to assess topic quality. Later, in Lau et al. (2014), PMI was replaced 3471 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3471–3477, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Figure 1: Two example topics and their distributions of µ, σ and cv from the NYT corpus. Two topics are: Topic1 (in blu"
E06-1039,P03-1048,0,0.465527,"ize evaluative text about a single entity should rely on a preliminary phase of information extraction from the target corpus. In particular, the summarizer should at least know for each document: what features of the entity were evaluated, the polarity of the evaluations and their strengths. In this paper, we explore this hypothesis by considering two alternative approaches. First, we developed a sentence-extraction based summarizer that uses the information extracted from the corpus to select and rank sentences from the corpus. We implemented this system, called MEAD*, by 305 adapting MEAD (Radev et al., 2003), an opensource framework for multi-document summarization. Second, we developed a summarizer that produces summaries primarily by generating language from the information extracted from the corpus. We implemented this system, called the Summarizer of Evaluative Arguments (SEA), by adapting the Generator of Evaluative Arguments (GEA) (Carenini and Moore, expected 2006) a framework for generating user tailored evaluative arguments. We have performed an empirical formative evaluation of MEAD* and SEA in a user study. In this evaluation, we also tested the effectiveness of human generated summari"
E06-1039,W04-3256,0,0.025122,"ectronics. Summaries of this literature could be of great strategic value to product designers, planners and manufacturers. There are other equally important commercial applications, such as the summarization of travel logs, and non-commercial applications, such as the summarization of candidate reviews. The general problem we consider in this paper is how to effectively summarize a large corpora of evaluative text about a single entity (e.g., a product). In contrast, most previous work on multidocument summarization has focused on factual text (e.g., news (McKeown et al., 2002), biographies (Zhou et al., 2004)). For factual documents, the goal of a summarizer is to select the most important facts and present them in a sensible ordering while avoiding repetition. Previous work has shown that this can be effectively achieved by carefully extracting and ordering the most informative sentences from the original documents in a domain-independent way. Notice however that when the source documents are assumed to contain inconsistent information (e.g., conflicting reports of a natural disaster (White et al., 2002)), a different approach is needed. The summarizer needs first to extract the information from"
E06-1039,H05-1044,0,\N,Missing
E06-1039,H05-2017,0,\N,Missing
E06-1039,H05-1043,0,\N,Missing
I17-1062,N09-1023,0,0.766484,"ads. This metric measures ”how well In-Thread Classifier We construct a classifier, referred to as the inthread classifier, which leverages the output of all previous stages of the pipeline to predict the probability of a message belonging to a thread. Given a message and a thread, we generate a feature vector using the features described in Table 2. The model used for classification, built in scikit618 Figure 2. The proposed pipeline for disentangling a chat transcript into threads. Throughout the diagram, t represents a thread and m denotes a message. we extract whole conversations intact” (Wang and Oard, 2009). We also use the lock score to measure local agreement. For a particular message, the previous k messages are either in the same or a different thread. The lock score is computed by considering, for each message, the previous k messages and counting the matches in the same/different thread assignments across annotations. The third and final metric used to score annotation agreement is the Shen-F score, as defined by Shen et al. (2006) which measures how well related messages are grouped. The Shen-F score is defined as: F = X ni i n maxj (F (i, j)) treating the annotation with higher entropy a"
I17-1062,J10-3004,0,0.0883252,"inst previous work. 1 Brian: Katie: Jenny: Jenny: Katie: Brian: i need a new tv show to watch psych/house of cards/breaking bad sound like things you might enjoy oh I should probably renew my Netflix I forgot my laprop at home D: Katie, that sucks... Are you going to go back home to get it? laptop* try Black Mirror Figure 1. An example of a multiparticipant chat with two threads of dialogue. Our work is novel because it approaches the problem of thread disentanglement by attempting to predict immediate reply relationships between messages. The potential benefits of this idea were discussed by Elsner and Charniak (2010) and Uthus and Aha (2013), but the idea has not yet been explored. Furthermore, Elsner and Charniak (2010) suggest that this approach ”might yield more reliable annotations”. Additionally, we explore the usage of unlabeled data for the purpose of identifying semantic relationships between messages. Previous attempts at semantic modeling by Elsner and Charniak (2010) and Adams and Martel (2010) have not been very effective, however recent accomplishments in next utterance prediction (Lowe et al., 2015) can be leveraged for the purpose of thread disentanglement. The main contributions of this pa"
I17-1062,W15-4640,0,0.2921,"relationships between messages. The potential benefits of this idea were discussed by Elsner and Charniak (2010) and Uthus and Aha (2013), but the idea has not yet been explored. Furthermore, Elsner and Charniak (2010) suggest that this approach ”might yield more reliable annotations”. Additionally, we explore the usage of unlabeled data for the purpose of identifying semantic relationships between messages. Previous attempts at semantic modeling by Elsner and Charniak (2010) and Adams and Martel (2010) have not been very effective, however recent accomplishments in next utterance prediction (Lowe et al., 2015) can be leveraged for the purpose of thread disentanglement. The main contributions of this paper are as follows. Introduction The problem of thread disentanglement is a precursor to high-level analysis of multiparticipant chats (Carenini et al., 2011). A typical chat consists of multiple simultaneous and distinct conversations, with Elsner and Charniak (2010) observing an average of 2.75 simultaneous threads of dialogue. Since a conversation does not necessarily entail a contiguous sequence of messages, the interwoven threads must be identified and segmented prior to any high-level analysis o"
J15-3002,W97-0703,0,0.675541,"Missing"
J15-3002,N07-1054,0,0.06076,"Missing"
J15-3002,A00-2018,0,0.26586,"Missing"
J15-3002,P05-1022,0,0.0217243,"DT. To cope with this limitation, we use the inferred (posterior) probabilities from our CRF parsing models in a probabilistic CKY-like bottom–up parsing algorithm (Jurafsky and Martin 2008), which is non-greedy and optimal. Furthermore, a simple modification of this parsing algorithm allows us to generate k-best (i.e., the k highest probability) parse hypotheses for each input text that could then be used in a reranker to improve over the initial ranking using additional (global) features of the discourse tree as evidence, a strategy that has been successfully explored in syntactic parsing (Charniak and Johnson 2005; Collins and Koo 2005). Third, most of the existing discourse parsers do not discriminate between intrasentential parsing (i.e., building the DTs for the individual sentences) and multisentential parsing (i.e., building the DT for the whole document). However, we argue that distinguishing between these two parsing conditions can result in more effective parsing. Two separate parsing models could exploit the fact that rhetorical relations 387 Computational Linguistics Volume 41, Number 3 are distributed differently intra-sententially versus multi-sententially. Also, they could independently ch"
J15-3002,N13-1136,0,0.0236685,"Missing"
J15-3002,P14-1085,0,0.0101241,"Missing"
J15-3002,J03-4003,0,0.122136,"Missing"
J15-3002,J05-1003,0,0.0266255,"Missing"
J15-3002,P98-1044,0,0.549926,"Missing"
J15-3002,P07-1033,0,0.0425522,"Missing"
J15-3002,P02-1057,0,0.0224856,"Missing"
J15-3002,P09-1075,0,0.0185577,"Missing"
J15-3002,C96-1058,0,0.204552,"also propose a greedy post-editing step based on an additional feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them a significant gain in performance. In a different approach, Li et al. (2014) propose a discourse-level dependency structure to capture direct relationships between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful application of deep learning to numerous NLP problems including syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b), and various tagging tasks (Collobert et al. 2011), a couple of recent studies in discourse parsing also use deep neural networks (DNNs) and related feature representation methods. Inspired by the work of Socher et al. (2013a, 2013b), Li, Li, and"
J15-3002,P12-1007,0,0.0128696,"like ours assigns a probability to every possible DT. The parsing algorithm then picks the most probable DTs. The existing discourse parsers (Marcu 1999; Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) described in Section 2 use parsing models that disregard the structural interdependencies between the DT constituents. However, we hypothesize that, like syntactic parsing, discourse parsing is also a structured prediction problem, which involves predicting multiple variables (i.e., the structure and the relation labels) that depend on each other (Smith 2011). Recently, Feng and Hirst (2012) also found these interdependencies to be critical for parsing performance. To capture the structural dependencies between the DT constituents, CODRA uses undirected conditional graphical models (i.e., CRFs) as its parsing models. To find the most probable DT, unlike most previous studies (Marcu 1999; Subba and Di-Eugenio 2009; Hernault et al. 2010), which adopt a greedy solution, CODRA applies an optimal CKY parsing algorithm to the inferred posterior probabilities (obtained from the CRFs) of all possible DT constituents. Furthermore, the parsing algorithm allows CODRA to generate a list of k"
J15-3002,P14-1048,0,0.443102,"rendingerlab.net/hilda/. 391 Computational Linguistics Volume 41, Number 3 parsing, and both scenarios use a single uniform parsing model. Second, they take a greedy (i.e., sub-optimal) approach to construct a DT. Third, they disregard sequential dependencies between DT constituents. Furthermore, HILDA considers the structure and the labels of a DT separately. Our discourse parser CODRA, as described in the next section, addresses all these limitations. More recent work than ours also attempts to address some of the above-mentioned limitations of the existing discourse parsers. Similar to us, Feng and Hirst (2014) generate a document-level DT in two stages, where a multi-sentential parsing follows an intra-sentential one. At each stage, they iteratively use two separate linear-chain CRFs (Lafferty, McCallum, and Pereira 2001) in a cascade: one for predicting the presence of rhetorical relations between adjacent discourse units in a sequence, and the other to predict the relation label between the two most probable adjacent units to be merged as selected by the previous CRF. While they use CRFs to take into account the sequential dependencies between DT constituents, they use them greedily during parsin"
J15-3002,P08-1109,0,0.0641963,"Missing"
J15-3002,P07-1062,0,0.561158,"Missing"
J15-3002,P03-1071,0,0.19933,"Missing"
J15-3002,I11-1120,0,0.0273248,"Missing"
J15-3002,D14-1027,1,0.887188,"Missing"
J15-3002,P14-1065,1,0.88264,"Missing"
J15-3002,D12-1108,0,0.0168877,"Missing"
J15-3002,W05-1506,0,0.0257676,"Missing"
J15-3002,P14-1002,0,0.474952,"se tree to get the vector representation for the EDU. Adjacent discourse units are then merged hierarchically to get the vector representations for the higher order discourse units. In every step, the merging is done using one binary (structure) and one multi-class (relation) classifier, each having a three-layer neural network architecture. The cost function for training the model is given by these two cascaded classifiers applied at different levels of the DT. Similar to our method, they use the classifier probabilities in a CKY-like parsing algorithm to find the global optimal DT. Finally, Ji and Eisenstein (2014) present a feature representation learning method in a shift–reduce discourse parser (Marcu 1999). Unlike DNNs, which learn non-linear feature transformations in a maximum likelihood model, they learn linear transformations of features in a max margin classification model. 3. Overview of Our Rhetorical Analysis Framework CODRA takes as input a raw text and produces a discourse tree that describes the text in terms of coherence relations that hold between adjacent discourse units (i.e., clauses, sentences) in the text. An example DT generated by an online demo of CODRA 392 Joty, Carenini, and N"
J15-3002,D12-1083,1,0.878223,"Missing"
J15-3002,P13-1048,1,0.84536,"Missing"
J15-3002,W14-3352,1,0.659695,"Missing"
J15-3002,P08-1026,0,0.0681691,"Missing"
J15-3002,P13-1160,0,0.045068,"Missing"
J15-3002,D14-1220,0,0.0643375,"predicting the presence of rhetorical relations between adjacent discourse units in a sequence, and the other to predict the relation label between the two most probable adjacent units to be merged as selected by the previous CRF. While they use CRFs to take into account the sequential dependencies between DT constituents, they use them greedily during parsing to achieve efficiency. They also propose a greedy post-editing step based on an additional feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them a significant gain in performance. In a different approach, Li et al. (2014) propose a discourse-level dependency structure to capture direct relationships between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful appl"
J15-3002,P14-1003,0,0.081651,"predicting the presence of rhetorical relations between adjacent discourse units in a sequence, and the other to predict the relation label between the two most probable adjacent units to be merged as selected by the previous CRF. While they use CRFs to take into account the sequential dependencies between DT constituents, they use them greedily during parsing to achieve efficiency. They also propose a greedy post-editing step based on an additional feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them a significant gain in performance. In a different approach, Li et al. (2014) propose a discourse-level dependency structure to capture direct relationships between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful appl"
J15-3002,W10-4327,0,0.24445,"Missing"
J15-3002,W14-3336,0,0.0224322,"Missing"
J15-3002,P95-1037,0,0.637605,"Missing"
J15-3002,P99-1047,0,0.527878,"tances stripped of their original discourse cues do not generalize well to implicit cases because they are linguistically quite different. Note that this approach to identifying discourse relations in the absence of manually labeled data does not fully solve the parsing problem (i.e., building DTs); rather, it only attempts to identify a small subset of coarser relations between two (flat) text segments (i.e., a tagging problem). Arguably, to perform a complete rhetorical analysis, one needs to use supervised machine learning techniques based on human-annotated data. 2.2 Supervised Approaches Marcu (1999) applies supervised machine learning techniques to build a discourse segmenter and a shift–reduce discourse parser. Both the segmenter and the parser rely on C4.5 decision tree classifiers (Poole and Mackworth 2010) to learn the rules automatically from the data. The discourse segmenter mainly uses discourse cues, shallowsyntactic (i.e., POS tags) and contextual features (i.e., neighboring words and their POS tags). To learn the shift–reduce actions, the discourse parser encodes five types of features: lexical (e.g., discourse cues), shallow-syntactic, textual similarity, operational (previous"
J15-3002,J00-3005,0,0.288845,"Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. Law. Computational Linguistics Volume 41, Number 3 text has a coherence structure (Halliday and Hasan 1976; Hobbs 1979), which logically binds its clauses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories"
J15-3002,P02-1047,0,0.224664,"Missing"
J15-3002,P07-1075,0,0.018781,"ther to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Representation Theory, which is dr"
J15-3002,P05-1012,0,0.0239027,"ased on an additional feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them a significant gain in performance. In a different approach, Li et al. (2014) propose a discourse-level dependency structure to capture direct relationships between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful application of deep learning to numerous NLP problems including syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b), and various tagging tasks (Collobert et al. 2011), a couple of recent studies in discourse parsing also use deep neural networks (DNNs) and related feature representation methods. Inspired by the work of Socher et al. (2013a, 2013b), Li, Li, and Hovy (2014) propose a recursive DNN for discourse"
J15-3002,H05-1066,0,0.0645416,"Missing"
J15-3002,J91-1002,0,0.755208,"Missing"
J15-3002,P04-1035,0,0.00658881,"Missing"
J15-3002,P09-2004,0,0.0811038,"Missing"
J15-3002,prasad-etal-2008-penn,0,0.224284,"Missing"
J15-3002,P08-2062,0,0.0639658,"Missing"
J15-3002,N03-1028,0,0.198151,"Missing"
J15-3002,J02-4004,0,0.155312,"Missing"
J15-3002,P13-1045,0,0.0207601,"between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful application of deep learning to numerous NLP problems including syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b), and various tagging tasks (Collobert et al. 2011), a couple of recent studies in discourse parsing also use deep neural networks (DNNs) and related feature representation methods. Inspired by the work of Socher et al. (2013a, 2013b), Li, Li, and Hovy (2014) propose a recursive DNN for discourse parsing. However, as in Socher et al. (2013a, 2013b), word vectors (i.e., embeddings) are not learned explicitly for the task, rather they are taken from Collobert et al. (2011). Given the vectors of the words in an EDU, their model first composes them hiera"
J15-3002,D13-1170,0,0.00463377,"Missing"
J15-3002,N03-1030,0,0.577106,"Missing"
J15-3002,W04-3210,0,0.0680903,"Missing"
J15-3002,H05-1033,0,0.493898,"of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. Law. Computational Linguistics Volume 41, Number 3 text has a coherence structure (Halliday and Hasan 1976; Hobbs 1979), which logically binds its clauses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different vie"
J15-3002,W04-0213,0,0.0304719,"Missing"
J15-3002,N09-1064,0,0.0322482,"Missing"
J15-3002,J11-2001,0,0.00514583,"Missing"
J15-3002,J02-4002,0,0.140023,"uses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Repres"
J15-3002,J05-2005,0,0.100595,"Missing"
J15-3002,P14-1019,0,0.0600359,"Missing"
J15-3002,J93-2004,0,\N,Missing
J15-3002,S07-1106,1,\N,Missing
J15-3002,C98-1044,0,\N,Missing
J98-3004,P84-1065,0,0.17828,"Missing"
J98-3004,W96-0406,0,0.0800088,"l estate sales data), lacking any obvious graphical depiction. Second, although our long term goal is to generate coordinated multimedia explanations using informational graphics and natural language, our focus in this paper was on generating effective natural language explanations about the graphical presentations. In order to do this, the system had to explicitly reason about the perceptual complexity of the presentation. Generating such captions is an important component of constructing multimedia explanations involving integrative graphical displays. The POSTGRAPttE system (Fasciano 1996; Fasciano and Lapalme 1996) is the closest related research effort. As in our work, PoSTGRAPI-IE generates statistical graphics and accompanying captions. However, the issues considered in our work differ from those in POSTGRAPI-IE in several ways and both the text and the graphics generated by PosTGRAPHE emphasize aspects orthogonal to the ones considered in our project. For instance, PoSTGRAPI-IE can take as input a list of aspects that should be conveyed by the presentation. (These goals are represented in the system as a predefined set of templates, such as, ""show the evolution of &lt;attribute-name-I} with respect to"
J98-3004,J95-2003,0,0.0573025,"Missing"
J98-3004,W96-0403,0,0.0172925,"the events suggests that one discuss the asking 10 This is conventionalfor languagesthat are written from left to right, and may be differentin other languages that are written from right to left. 453 Computational Linguistics Volume 24, Number 3 price of a house before the selling price, this would lead to mentioning the right edge before the left edge, contrary to the default ordering. 5.3 Aggregation Module Once the speech acts are ordered, they are passed to the aggregation module. In the general case, aggregation in natural language is a very difficult problem (Dalianis 1996; Shaw 1995; Huang and Fiedler 1996). Fortunately, our generation task requires a type of aggregation that is relatively straightforward. Our aggregation strategy only conjoins pairs of contiguous propositions about the same grapheme type in the same space. The module checks for grapheme types rather than specific graphemes to cover circumstances where, for instance, a chart may have a number of grey and black bars (which are different graphemes of the same type). This enables the system to generate text of the form ""The grey bars indicate the selling price of the house, whereas the black bars indicate the asking price."" When tw"
J98-3004,A92-1007,0,0.093667,"ark blue lines on the left). The number of troops was 400,000 in the earliest segments, 100,000 in the later segments, and 10,000 in the last segments. The city and date of each battle is shown by the labels of a yellow diamond, which shows the battle&apos;s location. This caption can help users understand the various attributes and the underlying relations between them--conveyed so succinctly by the graphic. Although several projects have focused on the question of how such intelligent graphical presentations can be automatically generated (e.g., Casner 1991; Mackinlay 1986; Roth and Hefley 1993; Kerpedjiev 1992), they have not addressed the problem of generating the accompanying textual explanations. Without this ability, automatic graphical presentation systems will necessarily be limited to generating conventionalized graphics that do not use novel means to express complex relationships among data attributes, or risk generating displays that users will find difficult to fully comprehend and utilize. In designing our framework for generating natural language captions we have adapted and integrated work in natural language generation (NLG) by a number of researchers--including ourselves--in different"
J98-3004,P90-1013,0,0.31129,"t shows the neighborhood (1) (2) (3) The system would need to realize that position was the only attribute of the mark being used for a mapping, and position is always clear in a graph and need not explicitly be mentioned; thus resulting in statement (2). However, since the mark is the only grapheme used in the graph, the system could leave off mentioning the mark as well, thus resulting in statement (3). There are two ways of dealing with this issue: (i) The system could apply iterative refinements of the referring expressions generated by the planner, as done in the local brevity algorithm (Reiter 1990). However, this single case would have substantially increased the computational cost of generating referring expressions in all cases, without significantly improving any of the other (perfectly appropriate) referring expressions generated by the module. (ii) The system could recognize this specific situation at a higher level and process the speech acts appropriately to avoid this situation completely. Thus, rather than considering this situation as a problem of generating an appropriate expression for the concept position of the mark in the third chart, we have chosen to push this problem u"
J98-3004,P95-1053,0,0.0231098,"ing between the events suggests that one discuss the asking 10 This is conventionalfor languagesthat are written from left to right, and may be differentin other languages that are written from right to left. 453 Computational Linguistics Volume 24, Number 3 price of a house before the selling price, this would lead to mentioning the right edge before the left edge, contrary to the default ordering. 5.3 Aggregation Module Once the speech acts are ordered, they are passed to the aggregation module. In the general case, aggregation in natural language is a very difficult problem (Dalianis 1996; Shaw 1995; Huang and Fiedler 1996). Fortunately, our generation task requires a type of aggregation that is relatively straightforward. Our aggregation strategy only conjoins pairs of contiguous propositions about the same grapheme type in the same space. The module checks for grapheme types rather than specific graphemes to cover circumstances where, for instance, a chart may have a number of grey and black bars (which are different graphemes of the same type). This enables the system to generate text of the form ""The grey bars indicate the selling price of the house, whereas the black bars indicate t"
J98-3004,W94-0302,1,0.906609,"ing referring expressions, example generation, and linearization. Given the applied nature of our work, in selecting specific NLG techniques we followed a parsimonious approach. For each subtask we selected the simplest technique that was capable, in conjunction with the behavior of the other subtasks, of producing coherent text that could express the propositions we needed to convey. The generation process starts with content selection. For this process, we use LONCBOW, a domain-independent discourse planner originally developed as part of a project aimed at generating tutorial explanations (Young and Moore 1994). Using plan operators that encode discourse strategies devised for the task of generating captions, the planner determines what information should be included in the captions (and consequently what should be left out), and how to organize the selected information. Operator constraints analyze the structure of the graphic presentation and the perceptual complexity of the graphical display to enable the planner to select and apply appropriate strategies. The output of the text planning stage is then further processed by a microplanner, a sequence of modules implementing inter- and Figurative ma"
J98-3004,J94-2003,0,\N,Missing
J98-3004,C94-1086,0,\N,Missing
N10-1132,J05-3002,0,0.025921,"ormation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We describe both ILP approaches in Section 4. 3 Interpretation - Ontology Mapping The view that summarization consists of stages of interpretation, transformation and generation was laid out by Sparck-Jones (1999). Popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (Knight and Marcu, 2000; Clarke and Lapata, 2006) or sentence merging (Barzilay and McKeown, 2005). In contrast, in this work we clearly separate interpretation from transformation. The most relevant research to ours is by Kleinbauer et al. (2007), similarly focused on meeting abstraction. They create an ontology for the AMI scenario meeting corpus (Carletta et al., 2005), described in Section 5.1. The system uses topic segments and topic labels, and for each topic segment in the meeting a sentence is generated that describes the most frequently mentioned content items 895 Source document interpretation in our system relies on a simple conversation ontology. The ontology is written in OWL/"
N10-1132,W95-0110,0,0.8253,"Missing"
N10-1132,P06-2019,0,0.0138112,"mail-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We describe both ILP approaches in Section 4. 3 Interpretation - Ontology Mapping The view that summarization consists of stages of interpretation, transformation and generation was laid out by Sparck-Jones (1999). Popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (Knight and Marcu, 2000; Clarke and Lapata, 2006) or sentence merging (Barzilay and McKeown, 2005). In contrast, in this work we clearly separate interpretation from transformation. The most relevant research to ours is by Kleinbauer et al. (2007), similarly focused on meeting abstraction. They create an ontology for the AMI scenario meeting corpus (Carletta et al., 2005), described in Section 5.1. The system uses topic segments and topic labels, and for each topic segment in the meeting a sentence is generated that describes the most frequently mentioned content items 895 Source document interpretation in our system relies on a simple conve"
N10-1132,W07-2322,0,0.00573558,"ned by the sentence-ontology mapping. Our evaluation shows that the sentences we select are highly informative to generate abstract summaries, and that our content selection method outperforms several greedy selection approaches. The system described thus far may appear extractive in nature, as the transformation step is identifying informative sentences in the conversation. However, these selected sentences correspond to &lt; participant, relation, entity &gt; triples in the ontology, for which we can subsequently generate novel text by creating linguistic annotations of the conversation ontology (Galanis and Androutsopolous, 2007). Even without the generation step, the approach described above allows us to create structured extracts by grouping sentences according to specific phenomena such as action items and decisions. The knowledge represented by the ontology enables us to significantly improve sentence selection according to intrinsic measures and to generate structured output that we hypothesize will be more useful to an end user compared with a generic unstructured extract. Future work will focus on the generation component and on applying the summarization system to conversations in other modalities such as blog"
N10-1132,W06-1643,0,0.309442,"systems differ in two major respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; and the ontology used by Kleinbauer et al. is specific not just to meetings but to the AMI scenario meetings, while our ontology applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described b"
N10-1132,2007.sigdial-1.40,0,0.0732247,"Missing"
N10-1132,W07-2324,0,0.0784425,"etation - Ontology Mapping The view that summarization consists of stages of interpretation, transformation and generation was laid out by Sparck-Jones (1999). Popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (Knight and Marcu, 2000; Clarke and Lapata, 2006) or sentence merging (Barzilay and McKeown, 2005). In contrast, in this work we clearly separate interpretation from transformation. The most relevant research to ours is by Kleinbauer et al. (2007), similarly focused on meeting abstraction. They create an ontology for the AMI scenario meeting corpus (Carletta et al., 2005), described in Section 5.1. The system uses topic segments and topic labels, and for each topic segment in the meeting a sentence is generated that describes the most frequently mentioned content items 895 Source document interpretation in our system relies on a simple conversation ontology. The ontology is written in OWL/RDF and contains two core upper-level classes: Participant and Entity. When additional information is available about participant roles in a given do"
N10-1132,P07-1103,0,0.0651761,"Missing"
N10-1132,2007.sigdial-1.4,0,0.0814173,"ic not just to meetings but to the AMI scenario meetings, while our ontology applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We describe both ILP approaches in Section 4. 3 Interpretation - Ontology Mapping The view that summarization consists of stages of interpretation, transformation and generation was laid out by Sparck-Jones (1999). Popular approaches to text extracti"
N10-1132,D08-1049,0,0.0222984,"logy applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We describe both ILP approaches in Section 4. 3 Interpretation - Ontology Mapping The view that summarization consists of stages of interpretation, transformation and generation was laid out by Sparck-Jones (1999). Popular approaches to text extraction essentially collapse interpretation and transformation into one step, with"
N10-1132,N04-4027,0,0.165604,"ajor respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; and the ontology used by Kleinbauer et al. is specific not just to meetings but to the AMI scenario meetings, while our ontology applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We descr"
N10-1132,wilson-2008-annotating,0,0.0996881,"Missing"
N10-1132,P05-1037,0,0.0233221,"content items from the ontology, while our summarizer is fully automatic; and the ontology used by Kleinbauer et al. is specific not just to meetings but to the AMI scenario meetings, while our ontology applies to conversations in general. While the work by Kleinbauer et al. is among the earliest research on abstracting multi-party dialogues, much attention in recent years has been paid to extractive summarization of such conversations, including meetings (Galley, 2006), emails (Rambow et al., 2004; Carenini et al., 2007), telephone conversations (Zhu and Penn, 2006) and internet relay chats (Zhou and Hovy, 2005). Recent research has addressed the challenges of detecting decisions (Hsueh et al., 2007), action items (Purver et al., 2007; Murray and Renals, 2008) and subjective sentences (Raaijmakers et al., 2008). In our work we perform all of these tasks but rely on general conversational features without recourse to meeting-specific or email-specific features. Our approach to transformation is an adaptation of an ILP sentence selection algorithm described by Xie et al. (2009). We describe both ILP approaches in Section 4. 3 Interpretation - Ontology Mapping The view that summarization consists of sta"
N13-1018,P12-3014,0,0.101141,"e chunks to our extracted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2 ), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in ph2 , and vice-versa"
N13-1018,N06-1046,0,0.0173089,"imal”, or “giggle” and “chuckle” can be replaced by “laugh”. The motivation behind the generalization step is to enrich the common terms between the phrases in favor of increasing the chance that they could merge to a single phrase. This also helps to move beyond the limitation of original lexical choices. 2.3.2 Phrase merging The goal is to merge the phrases that are connected, and to generate a human readable phrase that contains more information than a single extracted phrase. Several approaches have been proposed to aggregate and merge sentences in Natural Language Generation (NLG) (e.g. (Barzilay and Lapata, 2006; Cheng and Mellish, 2000)), however most of them use syntactic structure of the sentences. To merge phrases at the lexical level, we set few common linguistically motivated aggregation patterns such as: simple conjunction, and conjunction via shared participants (Reiter and Dale, 2000). Table 2 demonstrates the merging patterns, where wij is the jth word (or segment) in phrase i, cw is the common word (or segment) in both phrases and CP OS is the common part-of-speech tag of the corresponding word. To illustrate, pattern 1 looks for the first segment of each phrase (wi1 ). If they are same (c"
N13-1018,P11-1062,0,0.568264,"acted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2 ), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in ph2 , and vice-versa (the “unknown” cases i"
N13-1018,P07-1069,0,0.0201103,"extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1 , 1 http://slashdot.org and possible phrases that can be extracted from the topic cluster using different approaches. This example demonstrates that although most approaches (Mei et al., 2007; Lau et al., 2011; Branavan et al., 2007) advocate extracting phrase-level topic labels from the text segments, topically related text segments do not always contain one keyword or key phrase that can capture the meaning of the topic. As shown in this example, such labels do not exist in the original text and cannot be extracted using the existing probabilistic models (e.g., (Mei et al., 2007)). The same problem can be observed with many other examples. This suggests the idea of aggregating and generating topic labels, instead of simply extracting them, as a challenging scenario for this field of research. Moreover, to generate a lab"
N13-1018,W00-1425,0,0.0099463,"ckle” can be replaced by “laugh”. The motivation behind the generalization step is to enrich the common terms between the phrases in favor of increasing the chance that they could merge to a single phrase. This also helps to move beyond the limitation of original lexical choices. 2.3.2 Phrase merging The goal is to merge the phrases that are connected, and to generate a human readable phrase that contains more information than a single extracted phrase. Several approaches have been proposed to aggregate and merge sentences in Natural Language Generation (NLG) (e.g. (Barzilay and Lapata, 2006; Cheng and Mellish, 2000)), however most of them use syntactic structure of the sentences. To merge phrases at the lexical level, we set few common linguistically motivated aggregation patterns such as: simple conjunction, and conjunction via shared participants (Reiter and Dale, 2000). Table 2 demonstrates the merging patterns, where wij is the jth word (or segment) in phrase i, cw is the common word (or segment) in both phrases and CP OS is the common part-of-speech tag of the corresponding word. To illustrate, pattern 1 looks for the first segment of each phrase (wi1 ). If they are same (cwi1 ) and share the same P"
N13-1018,W04-3205,0,0.0376809,"for each pair of phrases. In order to adapt the similarity scores to the entailment score, we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2 . In this way, we can check the portion of information/facts in ph2 which is covered by ph1 . The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the sim182 ilarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind u"
N13-1018,W10-1751,0,0.0329095,"t the similarity scores to the entailment score, we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2 . In this way, we can check the portion of information/facts in ph2 which is covered by ph1 . The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the sim182 ilarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind using different entailment features is tha"
N13-1018,D10-1038,1,0.842685,"tional datasets. Our interest in dealing with conversational texts derives from two reasons. First, the huge amount of textual data generated everyday in these conversations validates the need of text analysis frameworks to process such conversational texts effectively. Second, conversational texts pose challenges to the traditional techniques, including redundancies, disfluencies, higher language variabilities and ill-formed sentence structure (Liu et al., 2011). Our conversational datasets are from two different asynchronous media: email and blog. For email, we use the dataset presented in (Joty et al., 2010), where three individuals annotated the publicly available BC3 email corpus (Ulrich et al., 2008) with topics. The corpus contains 40 email threads (or conversations) at an average of 5 emails per thread. On average it has 26.3 sentences and 2.5 topics per thread. A topic has an average length of 12.6 sentences. In total, the three annotators found 269 topics in a cor184 pus of 1,024 sentences. There are no publicly available blog corpora annotated with topics. For this study, we build our own blog corpus containing 20 blog conversations of various lengths from Slashdot, each annotated with to"
N13-1018,C10-1065,0,0.0588898,"the following sections. 2.1 Phrase extraction We tokenize and preprocess each cluster in the collection of topic clusters with lemmas, stems, part-ofspeech tags, sense tags and chunks. We also extract n-grams up to length 5 which do not start or end with a stop word. In this phase, we do not include any frequency count feature in our candidate extraction pipeline. Once we have built the candidates pool, the next step is to identify a subset containing the most significant of those candidates. Since most top systems in key phrase extraction use supervised approaches, we follow the same method (Kim et al., 2010b; Medelyan et al., 2008; Frank et al., 1999). Initially, we consider a set of features used in the other systems to determine whether a phrase is likely to be a key phrase. However, since our dataset is conversational (more details in Section 3), and the text segments are not long, we aim for a classifier with high recall. Thus, we only use TFxIDF (Salton and McGill, 1986), position of the first occurrence (Frank et al., 1999) and phrase length as our features. We merge the training and test data released for SemEval-2010 Task #5 (Kim et al., 2010b), which consists of 244 scientific articles"
N13-1018,S10-1004,0,0.127914,"the following sections. 2.1 Phrase extraction We tokenize and preprocess each cluster in the collection of topic clusters with lemmas, stems, part-ofspeech tags, sense tags and chunks. We also extract n-grams up to length 5 which do not start or end with a stop word. In this phase, we do not include any frequency count feature in our candidate extraction pipeline. Once we have built the candidates pool, the next step is to identify a subset containing the most significant of those candidates. Since most top systems in key phrase extraction use supervised approaches, we follow the same method (Kim et al., 2010b; Medelyan et al., 2008; Frank et al., 1999). Initially, we consider a set of features used in the other systems to determine whether a phrase is likely to be a key phrase. However, since our dataset is conversational (more details in Section 3), and the text segments are not long, we aim for a classifier with high recall. Thus, we only use TFxIDF (Salton and McGill, 1986), position of the first occurrence (Frank et al., 1999) and phrase length as our features. We merge the training and test data released for SemEval-2010 Task #5 (Kim et al., 2010b), which consists of 244 scientific articles"
N13-1018,W07-2324,0,0.0140014,"ng. We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms. 1 Table 1: Topic labeling example. Introduction Given text segments about the same topic written in different ways (i.e., language variability), topic labeling deals with the problem of automatically generating semantically meaningful labels for those text segments. The potential of integrating topic labeling as a prerequisite for higher-level analysis has been reported in several areas, such as summarization (Harabagiu and Lacatusu, 2010; Kleinbauer et al., 2007; Dias et al., 2007), information extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1 , 1 http://slashdot.org and possible phrases that can be extracted from the topic cluster using different approaches. This example demonstrates that although most approaches (Me"
N13-1018,P11-1154,0,0.028743,"007), information extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1 , 1 http://slashdot.org and possible phrases that can be extracted from the topic cluster using different approaches. This example demonstrates that although most approaches (Mei et al., 2007; Lau et al., 2011; Branavan et al., 2007) advocate extracting phrase-level topic labels from the text segments, topically related text segments do not always contain one keyword or key phrase that can capture the meaning of the topic. As shown in this example, such labels do not exist in the original text and cannot be extracted using the existing probabilistic models (e.g., (Mei et al., 2007)). The same problem can be observed with many other examples. This suggests the idea of aggregating and generating topic labels, instead of simply extracting them, as a challenging scenario for this field of research. Mor"
N13-1018,N10-1045,1,0.855593,"keywords. We add those chunks to our extracted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2 ), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in"
N13-1018,P11-1134,1,0.874875,"we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2 . In this way, we can check the portion of information/facts in ph2 which is covered by ph1 . The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the sim182 ilarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind using different entailment features is that combining various scores will yield a be"
N13-1018,P12-2024,1,0.873874,"tiveness of our feature set for this lexical entailment model. The reason that we gained a very high accuracy is because our selected sentences are a subset of RTE6 and RTE7 with a shorter length (less number of words) which makes the entailment recognition task much easier than recognizing entailment between paragraphs or complex long sentences. 2.2.3 Graph edge labeling We set the edge labeling problem as a two-way classification task. Two-way classification casts multidirectional entailment as a unidirectional problem, where each pair is analyzed checking for entailment in both directions (Mehdad et al., 2012). In this condition, each original test example is correctly classified if both pairs originated from it are correctly judged (“YES-YES” for bidirectional,“YESNO” and “NO-YES” for unidirectional entailment and “NO-NO” for unknown cases). Two-way classification represents an intuitive solution to capture multidimensional entailment relations. Moreover, since our training examples are labeled with binary judgments, we are not able to train a three-way classifier. 2.2.4 Identification and selection Assigning all entailment relations between the extracted phrase pairs, we are aiming at identifying"
N13-1018,D11-1062,1,0.854152,"the following three steps. 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge3 and Crosslingual textual entailment for content synchronization4 (Negri et al., 2012). However, such datasets cannot directly support our application. Specifically, our entailment graph is built over the extracted phrases (with max. length of 5 tokens per phrase), while the RTE datasets are composed of longer sentences and paragraphs (Bentivogli et al., 2009; Negri et al., 2011). In order to collect a dataset which is more similar to the goal of our entailment framework, we decide to select a subset of the sixth and seventh RTE challenge main task (i.e., RTE within a Corpus). Our 3 4 http://pascallin.ecs.soton.ac.uk/Challenges/RTE/ http://www.cs.york.ac.uk/semeval-2013/task8/ dataset choice is based on the following reasons: i) the length of sentence pairs in RTE6 and RTE7 is shorter than the others, and ii) RTE6 and RTE7 main task datasets are originally created for summarization purpose which is closer to our work. We sort the RTE6 and RTE7 dataset pairs based on t"
N13-1018,S12-1053,1,0.898214,"Missing"
N13-1018,C00-2137,0,0.0330731,"ggregation Extraction+Entailment+ Aggregation 12.2 15.1 15.6 18.5 30.8 35.5 33.3 37.6 17.9 20.4 38.7 41.6 ing R-f1 and Sem-R-f1 metrics suggests the need for more flexible automatic evaluation methods for this task. Moreover, although the same trend of improvement is observed in blog and email corpora, the differences between their performance suggest the investigation of specialized methods for various conversational modalities. Table 4: Results for candidate topic labels on blog and email corpora. 8 The statistical significance tests was calculated by approximate randomization described in (Yeh, 2000). 186 0.8 Sem-R-f1 icantly8 in both datasets. On the blog corpus, our key phrase extraction method (Extraction-BL) fails to beat the other baselines (Lead-BL and Freq-BL) in majority of cases (except R-f1 for Lead-BL). However, in the email dataset, it improves the performance over both baselines in both evaluation metrics. This might be due to the shorter topic clusters (in terms of number of sentences) in email corpus which causes a smaller number of phrases to be extracted. We also observe the effectiveness of the aggregation phase. In all cases, there is a significant improvement (p &lt; 0.05"
N13-1018,W04-3252,0,\N,Missing
N13-1018,N10-1146,1,\N,Missing
N13-1018,W07-1401,0,\N,Missing
P00-1020,W00-1407,1,0.654664,"sually for the sake of brevity. According to argumentation theory, the selection of what evidence to mention in an argument should be based on a measure of the evidence strength of support (or opposition) to the main claim of the argument (Mayberry and Golden 1996). Furthermore, argumentation theory suggests that for evaluative arguments the measure of evidence strength should be based on a model of the intended reader’s values and preferences. Following argumentation theory, we have designed an argumentative strategy for generating evaluative arguments that are properly arranged and concise (Carenini and Moore 2000). In our strategy, we assume that the reader’s values and preferences are represented as an additive multiattribute value function (AMVF), a conceptualization based on multiattribute utility theory (MAUT)(Clemen 1996). This allows us to adopt and extend a measure of evidence strength proposed in previous work on explaining decision theoretic advice based on an AMVF (Klein1994). Figure 1 Sample additive multiattribute value function (AMVF) The argumentation strategy has been implemented as part of a complete argument generator. Other modules of the generator include a microplanner, which perfor"
P00-1020,J97-1004,0,\N,Missing
P00-1020,J98-3001,0,\N,Missing
P08-1041,W97-0703,0,0.0755653,"s and measure the similarity between every pair of sentences. Graph-ranking algorithms, e.g., PageRank (Brin and Page, 1998), are then applied to rank those sentences. Our method is different from them. First, instead of using the complete graph, we build the graph based on the conversation structure. Second, we try various ways to compute the similarity among sentences and the ranking of the sentences. Several studies in the NLP literature have explored the reoccurrence of similar words within one document due to text cohesion. The idea has been formalized in the construct of lexical chains (Barzilay and Elhadad, 1997). While our approach and lexical chains both rely on lexical cohesion, they are quite different with respect to the kind of linkages considered. Lexical chain is only based on similarities between lexical items in contiguous sentences. In contrast, in our approach, the linkage is based on the existing conversation structure. In our approach, the “chain” is not only “lexical” but also “conversational”, and typically spans over several emails. 3 Extracting Conversations from Multiple Emails In this section, we first review how to build a fragment quotation graph through an example. Then we exten"
P08-1041,esuli-sebastiani-2006-sentiwordnet,0,0.0602895,"the Page-Rank score of a node (sentence) s. d is the dumping factor, which is initialized to 0.85 as suggested in the Page-Rank algorithm. In this way, the rank of a sentence is evaluated globally based on the graph. 5 Summarization with Subjective Opinions Other than the conversation structure, the measures of cohesion and the graph-based summarization methods we have proposed, the importance of a sentence in emails can be captured from other aspects. In many applications, it has been shown that sentences with subjective meanings are paid more attention than factual ones(Pang and Lee, 2004)(Esuli and Sebastiani, 2006). We evaluate whether this is also the case in emails, especially when the conversation is about decision making, giving advice, providing feedbacks, etc. A large amount of work has been done on determining the level of subjectivity of text (Shanahan et al., 2005). In this paper we follow a very simple approach that, if successful, could be extended in future work. More specifically, in order to assess the degree of subjectivity of a sentence s, we count the frequency of words and phrases in s that are likely to bear subjective opinions. The assumption is that the more subjective words s conta"
P08-1041,I05-2011,0,0.0123244,"ing them up as the final sentence score. X P R(s) = (1 − d) + d ∗ si ∈child(s) weight(s, si ) ∗ P R(si ) + X weight(sj , s) ∗ P R(sj ) sj ∈parent(s) weight(s, si ) + si ∈child(s) X (4) weight(sj , s) sj ∈parent(s) As to the subjective words and phrases, we consider the following two lists generated by researchers in this area. • OpF ind: The list of subjective words in (Wilson et al., 2005). The major source of this list is from (Riloff and Wiebe, 2003) with additional words from other sources. This list contains 8,220 words or phrases in total. • OpBear: The list of opinion bearing words in (Kim and Hovy, 2005). This list contains 27,193 words or phrases in total. 6 X Empirical Evaluation 6.1 Dataset Setup There are no publicly available annotated corpora to test email summarization techniques. So, the first step in our evaluation was to develop our own corpus. We use the Enron email dataset, which is the largest public email dataset. In the 10 largest inbox folders in the Enron dataset, there are 296 email conversations. Since we are studying summarizing email conversations, we required that each selected conversation contained at least 4 emails. In total, 39 conversations satisfied this requiremen"
P08-1041,P04-1035,0,0.0334633,"quation 4. P R(s) is the Page-Rank score of a node (sentence) s. d is the dumping factor, which is initialized to 0.85 as suggested in the Page-Rank algorithm. In this way, the rank of a sentence is evaluated globally based on the graph. 5 Summarization with Subjective Opinions Other than the conversation structure, the measures of cohesion and the graph-based summarization methods we have proposed, the importance of a sentence in emails can be captured from other aspects. In many applications, it has been shown that sentences with subjective meanings are paid more attention than factual ones(Pang and Lee, 2004)(Esuli and Sebastiani, 2006). We evaluate whether this is also the case in emails, especially when the conversation is about decision making, giving advice, providing feedbacks, etc. A large amount of work has been done on determining the level of subjectivity of text (Shanahan et al., 2005). In this paper we follow a very simple approach that, if successful, could be extended in future work. More specifically, in order to assess the degree of subjectivity of a sentence s, we count the frequency of words and phrases in s that are likely to bear subjective opinions. The assumption is that the m"
P08-1041,N04-3012,0,0.0411919,"g edge (su , sv ). We explore three types of cohesion measures: (1) clue words that are based on stems, (2) semantic distance based on WordNet 356 3.3.2 Semantic Similarity Based on WordNet Other than stems, when people reply to previous messages they may also choose some semantically related words, such as synonyms and antonyms, e.g., “talk” vs. “discuss”. Based on this observation, we propose to use semantic similarity to measure the cohesion between two sentences. We use the wellknown lexical database WordNet to get the semantic similarity of two words. Specifically, we use the package by (Pedersen et al., 2004), which includes several methods to compute the semantic similarity. Among those methods, we choose “lesk” and “jcn”, which are considered two of the best methods in (Jurafsky and Martin, 2008). Similar to the clue words, we measure the semantic similarity of two sentences by the total semantic similarity of the words in both sentences. This is described in the following equation. weight(su , sv ) = X X σ(wi , wj ), (2) wi ∈su wj ∈sv 3.3.3 Cosine Similarity Cosine similarity is a popular metric to compute the similarity of two text units. To do so, each sentence is represented as a word vector"
P08-1041,N04-4027,0,0.308866,"at case, rather than Summarizing email conversations is challenging due to the characteristics of emails, especially the conversational nature. Most of the existing methods dealing with email conversations use the email thread to represent the email conversation structure, which is not accurate in many cases (Yeh and Harnly, 2006). Meanwhile, most existing email summarization approaches use quantitative features to describe the conversation structure, e.g., number of recipients and responses, and apply some general multi-document summarization methods to extract some sentences as the summary (Rambow et al., 2004) (Wan and McKeown, 2004). Although such methods consider the conversation structure somehow, they simplify the conversation structure into several features and do not fully utilize it into the summarization process. In contrast, in this paper, we propose new summarization approaches by sentence extraction, which rely on a fine-grain representation of the conversation structure. We first build a sentence quotation graph by content analysis. This graph not only captures the conversation structure more accurately, especially for selective quotations, but it also represents the conversation struct"
P08-1041,W03-1014,0,0.0489519,"sentence. In addition, we can combine SubjScore with any of the sentence scores based on the sentence quotation graph. In this paper, we use a simple approach by adding them up as the final sentence score. X P R(s) = (1 − d) + d ∗ si ∈child(s) weight(s, si ) ∗ P R(si ) + X weight(sj , s) ∗ P R(sj ) sj ∈parent(s) weight(s, si ) + si ∈child(s) X (4) weight(sj , s) sj ∈parent(s) As to the subjective words and phrases, we consider the following two lists generated by researchers in this area. • OpF ind: The list of subjective words in (Wilson et al., 2005). The major source of this list is from (Riloff and Wiebe, 2003) with additional words from other sources. This list contains 8,220 words or phrases in total. • OpBear: The list of opinion bearing words in (Kim and Hovy, 2005). This list contains 27,193 words or phrases in total. 6 X Empirical Evaluation 6.1 Dataset Setup There are no publicly available annotated corpora to test email summarization techniques. So, the first step in our evaluation was to develop our own corpus. We use the Enron email dataset, which is the largest public email dataset. In the 10 largest inbox folders in the Enron dataset, there are 296 email conversations. Since we are study"
P08-1041,C04-1128,0,0.0507663,"or email threads (Rambow et al., 2004). They described each sentence in an email conversations by a set of features and used machine learning to classify whether or not a sentence should be included into the summary. Their experiments showed that features about emails and the email thread could significantly improve the accuracy of summarization. Wan et al. proposed a summarization approach for decision-making email discussions (Wan and McKeown, 2004). They extracted the issue and response sentences from an email thread as a summary. Similar to the issue-response relationship, Shrestha et al.(Shrestha and McKeown, 2004) proposed methods to identify the question-answer pairs from an email thread. Once again, their results showed that including features about the email 354 thread could greatly improve the accuracy. Similar results were obtained by Corston-Oliver et al. They studied how to identify “action” sentences in email messages and use those sentences as a summary(Corston-Oliver et al., 2004). All these approaches used the email thread as a coarse representation of the underlying conversation structure. In our recent study (Carenini et al., 2007), we built a fragment quotation graph to represent an email"
P08-1041,C04-1079,0,0.136156,"ummarizing email conversations is challenging due to the characteristics of emails, especially the conversational nature. Most of the existing methods dealing with email conversations use the email thread to represent the email conversation structure, which is not accurate in many cases (Yeh and Harnly, 2006). Meanwhile, most existing email summarization approaches use quantitative features to describe the conversation structure, e.g., number of recipients and responses, and apply some general multi-document summarization methods to extract some sentences as the summary (Rambow et al., 2004) (Wan and McKeown, 2004). Although such methods consider the conversation structure somehow, they simplify the conversation structure into several features and do not fully utilize it into the summarization process. In contrast, in this paper, we propose new summarization approaches by sentence extraction, which rely on a fine-grain representation of the conversation structure. We first build a sentence quotation graph by content analysis. This graph not only captures the conversation structure more accurately, especially for selective quotations, but it also represents the conversation structure at the finer granula"
P08-1041,P07-1070,0,0.0104739,"ch has a coarser granularity than the sentence level. For email summarization by sentence extraction, the fragment granularity may be inadequate. Second, we only adopted one cohesion measure (clue words that are based on stemming), and did not consider more sophisticated ones such as semantically similar words. Third, we did not consider subjective opinions. Finally, we did not compared CWS to other possible graph-based approaches as we propose in this paper. Other than for email summarization, other document summarization methods have adopted graphranking algorithms for summarization, e.g., (Wan et al., 2007), (Mihalcea and Tarau, 2004) and (Erkan and Radev, 2004). Those methods built a complete graph for all sentences in one or multiple documents and measure the similarity between every pair of sentences. Graph-ranking algorithms, e.g., PageRank (Brin and Page, 1998), are then applied to rank those sentences. Our method is different from them. First, instead of using the complete graph, we build the graph based on the conversation structure. Second, we try various ways to compute the similarity among sentences and the ranking of the sentences. Several studies in the NLP literature have explored t"
P08-1041,H05-2018,0,0.0285689,"ubjScore(s) alone can be used to evaluate the importance of a sentence. In addition, we can combine SubjScore with any of the sentence scores based on the sentence quotation graph. In this paper, we use a simple approach by adding them up as the final sentence score. X P R(s) = (1 − d) + d ∗ si ∈child(s) weight(s, si ) ∗ P R(si ) + X weight(sj , s) ∗ P R(sj ) sj ∈parent(s) weight(s, si ) + si ∈child(s) X (4) weight(sj , s) sj ∈parent(s) As to the subjective words and phrases, we consider the following two lists generated by researchers in this area. • OpF ind: The list of subjective words in (Wilson et al., 2005). The major source of this list is from (Riloff and Wiebe, 2003) with additional words from other sources. This list contains 8,220 words or phrases in total. • OpBear: The list of opinion bearing words in (Kim and Hovy, 2005). This list contains 27,193 words or phrases in total. 6 X Empirical Evaluation 6.1 Dataset Setup There are no publicly available annotated corpora to test email summarization techniques. So, the first step in our evaluation was to develop our own corpus. We use the Enron email dataset, which is the largest public email dataset. In the 10 largest inbox folders in the Enro"
P08-1041,W04-3252,0,\N,Missing
P08-1041,J00-4006,0,\N,Missing
P13-1048,J00-3005,0,0.763712,"and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast"
P13-1048,P12-1007,0,0.355874,"istinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard. This paper aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not consider the sequential dependencies between the DT constituents, which has been recently shown to be critical (Feng a"
P13-1048,J91-1002,0,0.302844,"the inference much faster (i.e., complexity of O(M 2 )). Breaking the chain structure also allows us to balance the data for training (equal number instances with S=1 and S=0), which dramatically reduces the learning time of the model. We apply our model to all possible adjacent units at all levels for the multi-sentential case, and 490 Lexico-syntactic features dominance sets (Soricut and Marcu, 2003) are very effective for intra-sentential parsing. We include syntactic labels and lexical heads of head and attachment nodes along with their dominance relationship as features. Lexical chains (Morris and Hirst, 1991) are sequences of semantically related words that can indicate topic shifts. Features extracted from lexical chains have been shown to be useful for finding paragraph-level discourse structure (Sporleder and Lascarides, 2004). We compute lexical chains for a document following the approach proposed in (Galley and McKeown, 2003), that extracts lexical chains after performing word sense disambiguation. Following (Joty et al., 2012), we also encode contextual and rhetorical sub-structure features in our models. The rhetorical sub-structure features incorporate hierarchical dependencies between DT"
P13-1048,P07-1062,0,0.741307,", which in turn are also subject to this relation linking. Discourse units linked by a rhetorical relation are further distinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard. This paper aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not"
P13-1048,N03-1030,0,0.527984,"me-Unit Contrast Explanation Figure 2: Distributions of six most frequent relations in intra-sentential and multi-sentential parsing scenarios. frequent relations on a development set containing 20 randomly selected documents from RST-DT. Notice that relations Attribution and Same-Unit are more frequent than Joint in intra-sentential case, whereas Joint is more frequent than the other two in multi-sentential case. On the other hand, different kinds of features are applicable and informative for intra-sentential vs. multi-sentential parsing. For example, syntactic features like dominance sets (Soricut and Marcu, 2003) are extremely useful for sentence-level parsing, but are not even applicable in multi-sentential case. Likewise, lexical chain features (Sporleder and Lascarides, 2004), that are useful for multi-sentential parsing, are not applicable at the sentence level. Based on these observations, our discourse parsing framework comprises two separate modules: an intra-sentential parser and a multisentential parser (Figure 3). First, the intrasentential parser produces one or more discourse sub-trees for each sentence. Then, the multisentential parser generates a full DT for the document from these sub-t"
P13-1048,D12-1083,1,0.239984,"subject to this relation linking. Discourse units linked by a rhetorical relation are further distinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard. This paper aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not consider the sequen"
P13-1048,H05-1033,0,0.0311665,"rsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast), forming larger discourse units (represented by int"
P13-1048,C04-1007,0,0.820285,"ndow covering two adjacent sentences and by then consolidating the results produced by over2 Related work The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al., 2004). These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on. However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004). Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features. Hernault et al., (2010) presents the publicly available HILDA parser. Given the EDUs in a doc487 30 25 20 15 10 5 0 Multi-sentential Intra-sentential Elaboration Joint Algorithm Sentences segmented into EDUs Algorithm Document-level discourse tree model model Intra-sentential parser Multi-sentential parser Figure 3: Discourse parsing framework. Attribution Same-Unit Contrast Explanation Figure 2: Distributions"
P13-1048,W10-4327,0,0.250002,"rasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaborat"
P13-1048,W04-0213,0,0.0487185,"Missing"
P13-1048,N09-1064,0,0.688998,"en the discourse unit containing EDUs i through m and the unit containing EDUs m+1 through j. For example, the DT for the second sentence in Figure 1 can be represented as ument, HILDA iteratively employs two Support Vector Machine (SVM) classifiers in pipeline to build the DT. In each iteration, a binary classifier first decides which of the adjacent units to merge, then a multi-class classifier connects the selected units with an appropriate relation label. They evaluate their approach on the RST-DT corpus (Carlson et al., 2002) of news articles. On a different genre of instructional texts, Subba and Di-Eugenio (2009) propose a shift-reduce parser that relies on a classifier for relation labeling. Their classifier uses Inductive Logic Programming (ILP) to learn first-order logic rules from a set of features including compositional semantics. In this work, we address the limitations of these models (described in Section 1) introducing our novel discourse parser. 3 Our Discourse Parsing Framework Given a document with sentences already segmented into EDUs, the discourse parsing problem is determining which discourse units (EDUs or larger units) to relate (i.e., the structure), and how to relate them (i.e., t"
P13-1048,P02-1047,0,0.0642381,"h these cases, builds sentence-level sub-trees by applying the intra-sentential parser on a sliding window covering two adjacent sentences and by then consolidating the results produced by over2 Related work The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al., 2004). These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on. However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004). Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features. Hernault et al., (2010) presents the publicly available HILDA parser. Given the EDUs in a doc487 30 25 20 15 10 5 0 Multi-sentential Intra-sentential Elaboration Joint Algorithm Sentences segmented into EDUs Algorithm Document-level discourse tree model model Intra-sentential parser Multi-sentential pars"
P13-1048,C04-1048,0,\N,Missing
P14-1115,P12-3014,0,0.0213774,"re scored, the top scored utterances are selected to be sent to the next step. We estimate the percentage of the retrieved utterances based on the development set. 1222 2.2 Redundancy Removal Utterances selected in previous step often include redundant information, which is semantically equivalent but may vary in lexical choices. By identifying the semantic relations between the sentences, we can discover what information in one sentence is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences. Similar to earlier work (Berant et al., 2011; Adler et al., 2012), we set this problem as a variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). Using entailment in this phase is motivated by taking advantage of semantic relations instead of pure statistical methods (e.g., Maximal Marginal Relevance) and shown to be more effective (Mehdad et al., 2013a). We follow the same practice as (Mehdad et al., 2013a) to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones. 2.3 Abstract Generation In this phase, our goal is to gener"
P14-1115,J05-3002,0,0.0966105,"tical methods (e.g., Maximal Marginal Relevance) and shown to be more effective (Mehdad et al., 2013a). We follow the same practice as (Mehdad et al., 2013a) to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones. 2.3 Abstract Generation In this phase, our goal is to generate understandable informative abstract sentences that capture the content of the source sentences and represents the information needs defined by queries. There are several ways of generating abstract sentences (e.g. (Barzilay and McKeown, 2005; Liu and Liu, 2009; Ganesan et al., 2010; Murray et al., 2010)); however, most of them rely heavily on the sentence structure. We believe that such approaches are suboptimal, especially in dealing with conversational data, because multiparty written conversations are often poorly structured. Instead, we apply an approach that does not rely on syntax, nor on a standard NLG architecture. Moreover, since dealing with user queries efficiency is an important aspect, we aim for an approach that is also motivated by the speed with which the abstracts are obtained. We perform the task of abstract gen"
P14-1115,P11-1062,0,0.0217429,"all the utterances are scored, the top scored utterances are selected to be sent to the next step. We estimate the percentage of the retrieved utterances based on the development set. 1222 2.2 Redundancy Removal Utterances selected in previous step often include redundant information, which is semantically equivalent but may vary in lexical choices. By identifying the semantic relations between the sentences, we can discover what information in one sentence is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences. Similar to earlier work (Berant et al., 2011; Adler et al., 2012), we set this problem as a variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). Using entailment in this phase is motivated by taking advantage of semantic relations instead of pure statistical methods (e.g., Maximal Marginal Relevance) and shown to be more effective (Mehdad et al., 2013a). We follow the same practice as (Mehdad et al., 2013a) to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones. 2.3 Abstract Generation In this phase,"
P14-1115,C10-1037,0,0.12376,"in generated abstract sentences. This task can be viewed as sentence clustering, where each sentence cluster can provide the content for an abstract sentence. We use the K-mean clustering algorithm by cosine similarity as a distance function between sentence vectors composed of tf.idf scores. Also notice that the lexical similarity between sentences in one cluster facilitates both the construction of the word graph and finding the best path in the word graph, as described next. 2.3.2 Word Graph In order to construct a word graph, we adopt the method recently proposed by (Mehdad et al., 2013a; Filippova, 2010) with some optimizations. Below, we show how the word graph is applied to generate the abstract sentences. Let G = (W, L) be a directed graph with the set of nodes W representing words and a set of directed edges L representing the links between words. Given a cluster of related sentences S = {s1 , s2 , ..., sn }, a word graph is constructed by iteratively adding sentences to it. In the first step, the graph represents one sentence plus the start and end symbols. A node is added to the graph for each word in the sentence, and words adjacent are linked with directed edges. When adding a new sen"
P14-1115,C10-1039,0,0.0651639,") and shown to be more effective (Mehdad et al., 2013a). We follow the same practice as (Mehdad et al., 2013a) to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones. 2.3 Abstract Generation In this phase, our goal is to generate understandable informative abstract sentences that capture the content of the source sentences and represents the information needs defined by queries. There are several ways of generating abstract sentences (e.g. (Barzilay and McKeown, 2005; Liu and Liu, 2009; Ganesan et al., 2010; Murray et al., 2010)); however, most of them rely heavily on the sentence structure. We believe that such approaches are suboptimal, especially in dealing with conversational data, because multiparty written conversations are often poorly structured. Instead, we apply an approach that does not rely on syntax, nor on a standard NLG architecture. Moreover, since dealing with user queries efficiency is an important aspect, we aim for an approach that is also motivated by the speed with which the abstracts are obtained. We perform the task of abstract generation in three steps, as follows: 2.3.1"
P14-1115,P07-2049,0,0.022691,"ances should carry the essence of the original text; and ii) utterances should be relevant to the query. To fulfill such requirements we define the concepts of signature terms and query terms. 2.1.1 Signature Terms Signature terms are generally indicative of the content of a document or collection of documents. To identify such terms, we can use frequency, word probability, standard statistic tests, information-theoretic measures or log-likelihood ratio. In this work, we use log-likelihood ratio to extract the signature terms from chat logs, since log-likelihood ratio leads to better results (Gupta et al., 2007). We use a method described in (Lin and Hovy, 2000) in order to identify such terms and their associated weight. Example 2 demonstrates a chat log and associated signature terms. 2.1.2 Query Terms Query terms are indicative of the content in a phrasal query. In order to identify such terms, we first extract all content terms from the query. Then, following previous studies (e.g., (Gonzalo Utterance Scoring To estimate the utterance score, we view both the query terms and the signature terms as the terms that should appear in a human query-based summary. To achieve this, the most relevant (summ"
P14-1115,C00-1072,0,0.458782,"To address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization. Our key contributions in this work are as follows: 1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on users phrasal queries, instead of well-formed questions. As a by-product of our approach, we also propose an extractive summarization model based on phrasal queries to select the summary-worthy sentences in the conversation based on query terms and signature terms (Lin and Hovy, 2000). 2) We propose a novel ranking strategy to select the best path in the constructed word graph by taking the query content, overall information content and grammaticality (i.e., fluency) of the sentence into consideration. 3) Although most of the current summarization approaches use supervised algorithms as a part of their system (e.g., (Wang et al., 2013)), our method can be totally unsupervised and does not depend on human annotation. 4) Although different conversational modalities (e.g., email vs. chat vs. meeting) underline domain-specific characteristics, in this work, we take advantage o"
P14-1115,P09-2066,0,0.0197216,"Marginal Relevance) and shown to be more effective (Mehdad et al., 2013a). We follow the same practice as (Mehdad et al., 2013a) to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones. 2.3 Abstract Generation In this phase, our goal is to generate understandable informative abstract sentences that capture the content of the source sentences and represents the information needs defined by queries. There are several ways of generating abstract sentences (e.g. (Barzilay and McKeown, 2005; Liu and Liu, 2009; Ganesan et al., 2010; Murray et al., 2010)); however, most of them rely heavily on the sentence structure. We believe that such approaches are suboptimal, especially in dealing with conversational data, because multiparty written conversations are often poorly structured. Instead, we apply an approach that does not rely on syntax, nor on a standard NLG architecture. Moreover, since dealing with user queries efficiency is an important aspect, we aim for an approach that is also motivated by the speed with which the abstracts are obtained. We perform the task of abstract generation in three st"
P14-1115,N13-1018,1,0.933465,"s and associated human-written query-based summaries for a chat log. rization focus on news or other well-written documents, while research on summarizing multiparty written conversations (e.g., chats, emails) has been limited. This is because traditional NLP approaches developed for formal texts often are not satisfactory when dealing with multiparty written conversations, which are typically in a casual style and do not display a clear syntactic structure with proper grammar and spelling. Even though some works try to address the problem of summarizing multiparty written conversions (e.g., (Mehdad et al., 2013b; Wang and Cardie, 2013; Murray et al., 2010; Zhou and Hovy, 2005; Gillick et al., 2009)), they do so in a generic way (not querybased) and focus on only one conversational domain (e.g., meetings). Moreover, most of the proposed systems for conversation summarization are extractive. To address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization. Our key contributions in this work are as follows: 1) To the best of our knowledge, our framework is the first abstractive system that generates s"
P14-1115,W13-2117,1,0.874493,"Missing"
P14-1115,W98-0705,0,0.224458,"Missing"
P14-1115,W10-4211,1,0.939486,"mmaries for a chat log. rization focus on news or other well-written documents, while research on summarizing multiparty written conversations (e.g., chats, emails) has been limited. This is because traditional NLP approaches developed for formal texts often are not satisfactory when dealing with multiparty written conversations, which are typically in a casual style and do not display a clear syntactic structure with proper grammar and spelling. Even though some works try to address the problem of summarizing multiparty written conversions (e.g., (Mehdad et al., 2013b; Wang and Cardie, 2013; Murray et al., 2010; Zhou and Hovy, 2005; Gillick et al., 2009)), they do so in a generic way (not querybased) and focus on only one conversational domain (e.g., meetings). Moreover, most of the proposed systems for conversation summarization are extractive. To address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization. Our key contributions in this work are as follows: 1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on users phrasal queries, inst"
P14-1115,W11-0501,0,0.0224959,"used for evaluating our query-based summarization system. Most available conversational corpora do not contain any human written summaries, or the gold standard human written summaries are generic (Carletta et al., 2005; Joty et al., 2013). In this work, we use available corpora for emails and chats for written conversations, while for spoken conversation, we employ an available corpus in multiparty meeting conversations. Chat: to the best of our knowledge, the only publicly available chat logs with human written summaries can be downloaded from the GNUe Traffic archive (Zhou and Hovy, 2005; Uthus and Aha, 2011; Uthus and Aha, 2013). Each chat log has a human created summary in the form of a digest. Each digest summarizes IRC logs for a period and consists of few summaries over each chat log with a unique title for the associated human written summary. In this way, the title of each summary 1224 can be counted as a phrasal query and the corresponding summary is considered as the querybased abstract of the associated chat log including only the information most relevant to the title. Therefore, we can use the human-written querybased abstract as gold standards and evaluate our system automatically. O"
P14-1115,P13-1137,0,0.0333719,"Missing"
P14-1115,P13-1136,0,0.028782,"stead of well-formed questions. As a by-product of our approach, we also propose an extractive summarization model based on phrasal queries to select the summary-worthy sentences in the conversation based on query terms and signature terms (Lin and Hovy, 2000). 2) We propose a novel ranking strategy to select the best path in the constructed word graph by taking the query content, overall information content and grammaticality (i.e., fluency) of the sentence into consideration. 3) Although most of the current summarization approaches use supervised algorithms as a part of their system (e.g., (Wang et al., 2013)), our method can be totally unsupervised and does not depend on human annotation. 4) Although different conversational modalities (e.g., email vs. chat vs. meeting) underline domain-specific characteristics, in this work, we take advantage of their underlying similarities to generalize away from specific modalities and determine effective method for query-based summarization of multimodal conversations. We evaluate our system over GNUe Traffic archive2 Internet Relay Chat (IRC) logs, AMI meetings corpus (Carletta et al., 2005) and BC3 emails dataset (Ulrich et al., 2008). Automatic evaluation"
P14-1115,C00-2137,0,0.019505,"Missing"
P14-1115,P05-1037,0,0.0294878,"g. rization focus on news or other well-written documents, while research on summarizing multiparty written conversations (e.g., chats, emails) has been limited. This is because traditional NLP approaches developed for formal texts often are not satisfactory when dealing with multiparty written conversations, which are typically in a casual style and do not display a clear syntactic structure with proper grammar and spelling. Even though some works try to address the problem of summarizing multiparty written conversions (e.g., (Mehdad et al., 2013b; Wang and Cardie, 2013; Murray et al., 2010; Zhou and Hovy, 2005; Gillick et al., 2009)), they do so in a generic way (not querybased) and focus on only one conversational domain (e.g., meetings). Moreover, most of the proposed systems for conversation summarization are extractive. To address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization. Our key contributions in this work are as follows: 1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on users phrasal queries, instead of well-formed qu"
P14-1115,W04-3252,0,\N,Missing
P19-4003,W04-3240,0,0.0668594,"Missing"
P19-4003,P09-1075,0,0.038371,"rization (Ji and Smith, 2017), and sentiment analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structur"
P19-4003,D14-1220,0,0.0247646,"re emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse analysis tasks (a) The tasks: relation sense identification and scope disambiguation (b) Statistical"
P19-4003,N16-1037,0,0.0230558,"zation). Again, evaluation metrics and approaches will be discussed and compared. We conclude with an interactive discussion of future challenges for discourse analMotivation Discourse analysis has been a fundamental problem in the ACL community, where the focus is to develop tools to automatically model language phenomena that go beyond the individual sentences. With the ongoing neural revolution, as the methods become more effective and flexible, analysis and interpretability beyond the sentence-level is of particular interests for many core language processing tasks like language modeling (Ji et al., 2016) and applications such as machine translation and its evaluation (Sennrich, 2018; L¨aubli et al., 2018; Joty et al., 2017), text categorization (Ji and Smith, 2017), and sentiment analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics"
P19-4003,D16-1035,0,0.061192,"Missing"
P19-4003,P17-1092,0,0.0197289,"nalMotivation Discourse analysis has been a fundamental problem in the ACL community, where the focus is to develop tools to automatically model language phenomena that go beyond the individual sentences. With the ongoing neural revolution, as the methods become more effective and flexible, analysis and interpretability beyond the sentence-level is of particular interests for many core language processing tasks like language modeling (Ji et al., 2016) and applications such as machine translation and its evaluation (Sennrich, 2018; L¨aubli et al., 2018; Joty et al., 2017), text categorization (Ji and Smith, 2017), and sentiment analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009)"
P19-4003,P19-1410,1,0.833143,"e computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse analysis tasks (a) The tasks: relation sense identification and scope disambiguation (b) Statistical models (Pitler and Nenkova, 2009; Ziheng et al., 2014) (c) Neural models (Ji and Eise"
P19-4003,P11-1100,0,0.0508656,"Missing"
P19-4003,J15-3002,1,0.895098,"timent analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse"
P19-4003,D18-1464,0,0.0138067,"-to structure, speech act recognition 5. Final remarks (a) Tree vs. graph structure (b) Discourse Graphbank C. Coffee Break [15 mins] D. Coherence Models & Applications of Discourse [45 mins] 4. Applications of discourse analysis B. Coherence Structure, Corpora & Discourse Parsing [45 mins] 1. Overview of coherence models (a) Entity grid and its extensions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Guinaudeau and Strube, 2013) (b) Discourse relation based model (Lin et al., 2011; Pitler and Nenkova, 2008) (c) Neural coherence models (Mohiuddin et al., 2018; Li and Jurafsky, 2017; Mesgar and Strube, 2018) (d) Coherence models for conversations (Elsner and Charniak, 2011a; Mohiuddin et al., 2018) 1. Discourse theories & coherence relations (a) Rhetorical Structure Theory (RST) & RST Treebank (Carlson et al., 2002) & Instructional domain (Subba and Di Eugenio, 2009) (b) Discourse Lexicalized Tree Adjoining Grammar (D-LTAG) & Penn Discourse Treebank (PDTB) (Prasad et al., 2005) 2. Discourse connectives & unsupervised relation identification 2. Evaluation tasks (a) Role of connectives in RST & PDTB (b) Identifying discourse connectives (c) Implicit and explicit relations (a) Sentence ordering (Dis"
P19-4003,P16-1165,1,0.769595,"9 (QA track) and EMNLP-2019 (Discourse track) and a senior program committee member for IJCAI 2019. Shafiq is a recipient of NSERC CGS-D scholarship and Microsoft Research Excellent Intern award. 1. Conversational structures (a) Speech (or dialog) acts in synchronous and asynchronous conversations (b) Reply-to (thread) structure in asynchronous conversations (Carenini et al., 2007) (c) Conversation disentanglement in synchronous conversations 2. Computational models (a) Speech act recognition models (Stolcke et al., 2000; Cohen et al., 2004; Ritter et al., 2010; Joty et al., 2011; Paul, 2012; Joty and Hoque, 2016; Mohiuddin et al., 2019) (b) Thread reconstruction models (Shen et al., 2006; Wang et al., 2008, 2011a,b) (c) Conversation disentanglement models (Elsner and Charniak, 2008, 2011a) 3. Evaluation & Summary of results F. Future Challenges [15 mins] Dr. Giuseppe Carenini2 is a Professor in Computer Science at UBC. Giuseppe has broad interdisciplinary interests. His work on NLP and information visualization to support decision making has been published in over 100 peer-reviewed papers (including best paper at UMAP-14 and ACMTiiS-14). He was the area chair for ACL’09 “Sentiment Analysis, Opinion M"
P19-4003,P18-1052,1,0.842032,"versational structure ⇒ Disentanglement & reply-to structure, speech act recognition 5. Final remarks (a) Tree vs. graph structure (b) Discourse Graphbank C. Coffee Break [15 mins] D. Coherence Models & Applications of Discourse [45 mins] 4. Applications of discourse analysis B. Coherence Structure, Corpora & Discourse Parsing [45 mins] 1. Overview of coherence models (a) Entity grid and its extensions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Guinaudeau and Strube, 2013) (b) Discourse relation based model (Lin et al., 2011; Pitler and Nenkova, 2008) (c) Neural coherence models (Mohiuddin et al., 2018; Li and Jurafsky, 2017; Mesgar and Strube, 2018) (d) Coherence models for conversations (Elsner and Charniak, 2011a; Mohiuddin et al., 2018) 1. Discourse theories & coherence relations (a) Rhetorical Structure Theory (RST) & RST Treebank (Carlson et al., 2002) & Instructional domain (Subba and Di Eugenio, 2009) (b) Discourse Lexicalized Tree Adjoining Grammar (D-LTAG) & Penn Discourse Treebank (PDTB) (Prasad et al., 2005) 2. Discourse connectives & unsupervised relation identification 2. Evaluation tasks (a) Role of connectives in RST & PDTB (b) Identifying discourse connectives (c) Implicit"
P19-4003,D17-1134,0,0.0175241,"n & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse analysis tasks (a) The tasks: relation sense identification and scope disambiguation (b) Statistical models (Pitler and Nenkova, 2009; Ziheng et al., 2014) (c) Neural models (Ji and Eisenstein, 2015; Lan et al., 2017) (d) Evaluation & Discussion (a) Coherence structure ⇒ Discourse segmentation & parsing (b) Coherence models ⇒ Coherence evaluation (c) Topic structure ⇒ Topic segmentation & labeling [not covered in this tutorial] (d) Coreference structure ⇒ Coreference resolution [not covered in this tutorial] (e) Conversational structure ⇒ Disentanglement & reply-to structure, speech act recognition 5. Final remarks (a) Tree vs. graph structure (b) Discourse Graphbank C. Coffee Break [15 mins] D. Coherence Models & Applications of Discourse [45 mins] 4. Applications of discourse analysis B. Coherence Struct"
P19-4003,N19-1134,1,0.796203,"-2019 (Discourse track) and a senior program committee member for IJCAI 2019. Shafiq is a recipient of NSERC CGS-D scholarship and Microsoft Research Excellent Intern award. 1. Conversational structures (a) Speech (or dialog) acts in synchronous and asynchronous conversations (b) Reply-to (thread) structure in asynchronous conversations (Carenini et al., 2007) (c) Conversation disentanglement in synchronous conversations 2. Computational models (a) Speech act recognition models (Stolcke et al., 2000; Cohen et al., 2004; Ritter et al., 2010; Joty et al., 2011; Paul, 2012; Joty and Hoque, 2016; Mohiuddin et al., 2019) (b) Thread reconstruction models (Shen et al., 2006; Wang et al., 2008, 2011a,b) (c) Conversation disentanglement models (Elsner and Charniak, 2008, 2011a) 3. Evaluation & Summary of results F. Future Challenges [15 mins] Dr. Giuseppe Carenini2 is a Professor in Computer Science at UBC. Giuseppe has broad interdisciplinary interests. His work on NLP and information visualization to support decision making has been published in over 100 peer-reviewed papers (including best paper at UMAP-14 and ACMTiiS-14). He was the area chair for ACL’09 “Sentiment Analysis, Opinion Mining, and Text Classific"
P19-4003,N03-1030,0,0.333163,"; Joty et al., 2017), text categorization (Ji and Smith, 2017), and sentiment analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse"
P19-4003,D17-1136,0,0.0218375,"ls and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse analysis tasks (a) The tasks: relation sense identification and scope disambiguation (b) Statistical models (Pitler and Nenkova"
P19-4003,J00-3003,0,0.132393,"Missing"
P19-4003,W17-5535,1,0.832508,"undamental problem in the ACL community, where the focus is to develop tools to automatically model language phenomena that go beyond the individual sentences. With the ongoing neural revolution, as the methods become more effective and flexible, analysis and interpretability beyond the sentence-level is of particular interests for many core language processing tasks like language modeling (Ji et al., 2016) and applications such as machine translation and its evaluation (Sennrich, 2018; L¨aubli et al., 2018; Joty et al., 2017), text categorization (Ji and Smith, 2017), and sentiment analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model"
P19-4003,N09-1064,0,0.0816957,"Missing"
P19-4003,D12-1009,0,0.0198669,"for ACL-2019 (QA track) and EMNLP-2019 (Discourse track) and a senior program committee member for IJCAI 2019. Shafiq is a recipient of NSERC CGS-D scholarship and Microsoft Research Excellent Intern award. 1. Conversational structures (a) Speech (or dialog) acts in synchronous and asynchronous conversations (b) Reply-to (thread) structure in asynchronous conversations (Carenini et al., 2007) (c) Conversation disentanglement in synchronous conversations 2. Computational models (a) Speech act recognition models (Stolcke et al., 2000; Cohen et al., 2004; Ritter et al., 2010; Joty et al., 2011; Paul, 2012; Joty and Hoque, 2016; Mohiuddin et al., 2019) (b) Thread reconstruction models (Shen et al., 2006; Wang et al., 2008, 2011a,b) (c) Conversation disentanglement models (Elsner and Charniak, 2008, 2011a) 3. Evaluation & Summary of results F. Future Challenges [15 mins] Dr. Giuseppe Carenini2 is a Professor in Computer Science at UBC. Giuseppe has broad interdisciplinary interests. His work on NLP and information visualization to support decision making has been published in over 100 peer-reviewed papers (including best paper at UMAP-14 and ACMTiiS-14). He was the area chair for ACL’09 “Sentime"
P19-4003,D08-1020,0,0.0631394,"rence resolution [not covered in this tutorial] (e) Conversational structure ⇒ Disentanglement & reply-to structure, speech act recognition 5. Final remarks (a) Tree vs. graph structure (b) Discourse Graphbank C. Coffee Break [15 mins] D. Coherence Models & Applications of Discourse [45 mins] 4. Applications of discourse analysis B. Coherence Structure, Corpora & Discourse Parsing [45 mins] 1. Overview of coherence models (a) Entity grid and its extensions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Guinaudeau and Strube, 2013) (b) Discourse relation based model (Lin et al., 2011; Pitler and Nenkova, 2008) (c) Neural coherence models (Mohiuddin et al., 2018; Li and Jurafsky, 2017; Mesgar and Strube, 2018) (d) Coherence models for conversations (Elsner and Charniak, 2011a; Mohiuddin et al., 2018) 1. Discourse theories & coherence relations (a) Rhetorical Structure Theory (RST) & RST Treebank (Carlson et al., 2002) & Instructional domain (Subba and Di Eugenio, 2009) (b) Discourse Lexicalized Tree Adjoining Grammar (D-LTAG) & Penn Discourse Treebank (PDTB) (Prasad et al., 2005) 2. Discourse connectives & unsupervised relation identification 2. Evaluation tasks (a) Role of connectives in RST & PDTB"
P19-4003,D11-1002,0,0.22444,"Missing"
P19-4003,P09-2004,0,0.0347257,"orey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse analysis tasks (a) The tasks: relation sense identification and scope disambiguation (b) Statistical models (Pitler and Nenkova, 2009; Ziheng et al., 2014) (c) Neural models (Ji and Eisenstein, 2015; Lan et al., 2017) (d) Evaluation & Discussion (a) Coherence structure ⇒ Discourse segmentation & parsing (b) Coherence models ⇒ Coherence evaluation (c) Topic structure ⇒ Topic segmentation & labeling [not covered in this tutorial] (d) Coreference structure ⇒ Coreference resolution [not covered in this tutorial] (e) Conversational structure ⇒ Disentanglement & reply-to structure, speech act recognition 5. Final remarks (a) Tree vs. graph structure (b) Discourse Graphbank C. Coffee Break [15 mins] D. Coherence Models & Applicati"
P19-4003,P17-2029,0,0.0256295,"f challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse analysis tasks (a) The tasks: relation sense identification and scope disambiguation (b) Statistical models (Pitler and Nenkova, 2009; Ziheng et al., 2014) (c) Neural"
W00-1402,J98-3002,0,0.253934,"Missing"
W00-1402,J95-2003,0,0.0226764,"Missing"
W00-1402,J97-1004,0,\N,Missing
W00-1402,W00-1407,1,\N,Missing
W00-1402,W98-1415,0,\N,Missing
W00-1402,J98-3001,0,\N,Missing
W00-1407,W00-1402,1,0.904203,"n her model of preferences (i.e., the argumentative The argumentation strategy has been intent is equal to the expected value, so MD=0) 6. implemented as a set of plan operators. Using We now examine the strategy in detail, after these operators the Longbow discourse planner introducing necessary, terminology. The subject (Young and Moore 1994) selects and arranges the content of the argument. We have applied 5 a,.or~, is an alternative such that Vo v~,(a,,,,r~,)=O, our strategy in a system that serves as a realwhereas abL.,is an alternative suchthat Vo vo(abe.¢~)=l estate personal assistant (Carenini 2000a). The 6 An alternative strategy, for generating arguments system presents information about houses whose argumentative intent was-greater (or lower) available on the market in graphical format. The than the expected value, could also be defined in our user explores this information by means of framework. However, this strategy should boost the interactive techniques, and can request a natural evaluation of supporting evidence and include only weak counterarguments, or hide them overall (the opposite if the target value was lower than the expected value) 7 The steps in the strategy are marked"
W00-1407,J98-3002,0,0.0179712,"Missing"
W08-1106,P99-1071,0,0.179453,"iding a context in which the need for generationbased methods is especially great. 1 Introduction There are two main approaches to the task of summarization—extraction and abstraction (Hahn and Mani, 2000). Extraction involves concatenating extracts taken from the corpus into a summary, whereas abstraction involves generating novel sentences from information extracted from the corpus. It has been observed that in the context of multidocument summarization of news articles, extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al., 1999). However, there has been little work identifying specific factors which might affect the performance of each strategy in summarizing evaluative documents 2 Related Work There has been little work comparing extractive and abstractive multi-document summarization. A previous study on summarizing evaluative text (Carenini et. al, 2006) showed that extraction and abstraction performed about equally well, though for different reasons. The study, however, did not 1 Authors are listed in alphabetical order. 33 look at the effect of the controversiality of the corpus on the relative performance of th"
W08-1106,E06-1039,1,0.901475,"(Lin, 2004), which gives a score based on the similarity in the sequences of words between a human-written model summary and the machine summary. While ROUGE scores have been shown to often correlate quite well with human judgements (Nenkova et al., 2007), they do not provide insights into the specific strengths and weaknesses of the summary. The method of summarization evaluation used in this work is to ask users to complete a questionnaire about summaries that they are presented with. The questionnaire consists of questions asking for Likert ratings and is adapted from the questionnaire in (Carenini et al., 2006). 3 Representative Systems In our user study, we compare an abstractive and an extractive multi-document summarizer that are both developed specifically for the evaluative domain. These summarizers have been found to produce quantitatively similar results, and both significantly outperform a baseline summarizer, which is the MEAD summarization framework with all options set to the default (Radev et al., 2000). Both summarizers rely on information extraction from the corpus. First, sentences with opinions need to be identified, along with the features of the entity that are evaluated, the stren"
W08-1106,W00-1407,1,0.765382,"e user. This mapping provides a better conceptual organization of the CFs 34 by grouping together semantically similar CFs, such as jpeg picture and jpeg slide show under the UDF JPEG. For the purposes of our study, feature extraction, polarity/strength identification and the mapping from CFs to UDFs are not done automatically as in (Hu and Liu, 2004) and (Carenini et al, 2005). Instead, “gold standard” annotations by humans are used in order to focus on the effect of the summarization strategy. 3.1 SEA summaries, which is adapted from GEA and is based on guidelines from argumentation theory (Carenini and Moore, 2000), sometimes sounded unnatural. We found that controversially rated UDF features (roughly balanced positive and negative evaluations) were treated as contrasts to those which were uncontroversially rated (either mostly positive, or mostly negative evaluations). In SEA, contrast relations between features are realized by cue phrases signalling contrast such as “however” and “although”. These cue phrases appear to signal a contrast that is too strong for the relation between controversial and uncontroversial features. An example of a SEA summary suffering from this problem can be found in Figure"
W08-1106,W02-2116,0,0.0558891,"Missing"
W08-1106,W04-1013,0,0.0527875,"For example, summaries which misrepresented the polarity of the evaluations for a certain feature were not penalized, and human summaries sometimes produced contradictory statements about the distribution of the opinions. In one case, one model summary claimed that a feature is positively rated, while another claimed the opposite, whereas the machine summary indicated that this feature drew mixed reviews. Clearly, only one of these positions should be regarded as correct. Further work is needed to resolve these problems. There are also automatic methods for summary evaluation, such as ROUGE (Lin, 2004), which gives a score based on the similarity in the sequences of words between a human-written model summary and the machine summary. While ROUGE scores have been shown to often correlate quite well with human judgements (Nenkova et al., 2007), they do not provide insights into the specific strengths and weaknesses of the summary. The method of summarization evaluation used in this work is to ask users to complete a questionnaire about summaries that they are presented with. The questionnaire consists of questions asking for Likert ratings and is adapted from the questionnaire in (Carenini et"
W08-1106,N04-1019,0,0.0120131,"a summarizer for its intended purpose. (e.g. (McKeown et al., 2005)) This approach, however, is less applicable in this work because we are interested in evaluating specific properties of the summary such as the grammaticality and the content, which may be difficult to evaluate with an overall task-based approach. Furthermore, the design of the task may intrinsically favour abstractive or extractive summarization. As an extreme example, asking for a list of specific comments from users would clearly favour extractive summarization. Another method for summary evaluation is the Pyramid method (Nenkova and Passonneau, 2004), which takes into account the fact that human summaries with different content can be equally informative. Multiple human summaries are taken to be models, and chunks of meaning known as Summary Content Units (SCU) are manually identified. Peer summaries are evaluated based on how many SCUs they share with the model summaries, and the number of model summaries in which these SCUs are found. Although this method has been tested in DUC 2006 and DUC 2005 (Passonneau et al., 2006), (Passonneau et al., 2005) in the domain of news articles, it has not been tested for evaluative text. A pilot study"
W08-1106,W00-0403,0,0.0706408,"rs to complete a questionnaire about summaries that they are presented with. The questionnaire consists of questions asking for Likert ratings and is adapted from the questionnaire in (Carenini et al., 2006). 3 Representative Systems In our user study, we compare an abstractive and an extractive multi-document summarizer that are both developed specifically for the evaluative domain. These summarizers have been found to produce quantitatively similar results, and both significantly outperform a baseline summarizer, which is the MEAD summarization framework with all options set to the default (Radev et al., 2000). Both summarizers rely on information extraction from the corpus. First, sentences with opinions need to be identified, along with the features of the entity that are evaluated, the strength, and polarity (positive or negative) of the evaluation. For instance, in a corpus of customer reviews, the sentence “Excellent picture quality - on par with my Pioneer, Panasonic, and JVC players.” contains an opinion on the feature picture quality of a DVD player, and is a very positive evaluation (+3 on a scale from -3 to +3). We rely on methods from previous work for these tasks (Hu and Liu, 2004). Onc"
W08-1106,N03-1020,0,\N,Missing
W09-2804,W04-1013,0,0.00250549,"s in a UDF hierarchy, and sampled from these distributions to generate new UDF hierarchies and evaluations. In total, we generated 36 sets of data, which covered a realistic set of possible scenarios in term of feature hierarchy structures as well as in term of distribution of evaluations for each feature. Table 2 presents some statistics on the generated data sets. 4.2 Building a Human Performance Model We adopt the evaluation approach that a good content selection strategy should perform similarly to humans, which is the view taken by existing summarization evaluation schemes such as ROUGE (Lin, 2004) and the Pyramid method (Nenkova et al., 2007). For evaluating our content selection strategy, we conducted a user study asking human participants to perform a selection task to create “gold standard” selections. Participants viewed and selected UDF features using a Treemap information visualization. See Figure 2 for an example. We recruited 25 university students or graduates, who were each presented with 19 to 20 of the cases we generated as described above. Each case represented a different hypothetical product, which was represented by a UDF hierarchy, as well as P/S evaluations from -3 to"
W09-2804,P05-1018,0,0.0328204,"any meaningful conclusion from the dataset.2 Thus, we stochastically θ(u) = imp pos(u)/(imp pos(u)+imp neg(u)) The distribution-based multiplier E(u, v) is the Jensen-Shannon divergence from Ber(θ(u)) to Ber(θ(v)), plus one for multiplicative identity when the divergence is zero. E(u, v) = JS(θ(u), θ(v)) + 1 The final formula for the information coverage cost is thus 2 Using a constructed dataset based on real data where no resources or agreed-upon evaluation methodology yet exists has been done in other NLP tasks such as topic boundary detection (Reynar, 1994) and local coherence modelling (Barzilay and Lapata, 2005). We are encouraged, however, that subsequent to our experiment, more resources for opinion anald(u, v) = dir moi(v) × T (u, v) × E(u, v) Consider the following example consisting of four-node UDF tree and importance scores. 11 # Features # Evaluated Features # Children (depth 0) # Children (depth 1 fertile) mean 55.3889 21.6667 11.3056 5.5495 std. 8.5547 5.9722 0.7753 1.7724 visualizing data in the customer review domain, even for novice users (Carenini et al., 2006). In a Treemap, the feature hierarchy is represented by nested rectangles, with parent features being larger rectangles, and chi"
W09-2804,P04-1035,0,0.0062224,"lements, and returning the items with the highest scores. In GEA (Generator of Evaluative Arguments), evaluative arguments are generated to describe an entity as positive or negative (Carenini and Moore, 2006). An entity is decomposed into a hierarchy of features, and a relevance score is independently calculated for each feature, based on the preferences of the user and the value of that feature for the product. Content selection involves selecting the most relevant features for the current user. There is also work in sentiment analysis relying on optimization or clustering-based approaches. Pang and Lee (2004) frame the problem of detecting subjective sentences as finding the minimum cut in a graph representation of the sentences. They produce compressed versions of movie reviews using just the subjective sentences, which retain the polarity information of the review. Gamon et al. (2005) use a heuristic approach to cluster sentences drawn from car reviews, grouping sentences that share common terms, especially those salient in the domain such as ‘drive’ or ‘handling’. The resulting clusters are displayed by a Treemap visualization. Our work is most similar to the content selection method of the mul"
W09-2804,W08-1106,1,0.839395,"d in the summarization process. For example, in multi-modal summarization, complex information can be more effectively conveyed by combining graphics and text (Tufte et al., 1998). While graphics can present large amounts of data compactly and support the discovery of trends and relationships, text is much more effective at explaining key points about the data. In another case specific to opinion summarization, the controversiality of the opinions in a corpus was found to correlate with the type of text summary, with abstractive summarization being preferred when the controversiality is high (Carenini and Cheung, 2008). We first test whether our optimization-based approach can achieve reasonable performance on content selection alone. As a contribution of this paper, we compare our optimization-based approach to a previously proposed heuristic method. Because our approach replaces a set of myopic decisions with an extensively studied procedure (the p-median problem) that is able to find a global solution, we hypothesized our approach would produce better selections. The results of our study indicate that our optimization-based content selection strategy performs about as well as the heuristic method. These"
W09-2804,P94-1050,0,0.110698,"of products, which limits our ability to draw any meaningful conclusion from the dataset.2 Thus, we stochastically θ(u) = imp pos(u)/(imp pos(u)+imp neg(u)) The distribution-based multiplier E(u, v) is the Jensen-Shannon divergence from Ber(θ(u)) to Ber(θ(v)), plus one for multiplicative identity when the divergence is zero. E(u, v) = JS(θ(u), θ(v)) + 1 The final formula for the information coverage cost is thus 2 Using a constructed dataset based on real data where no resources or agreed-upon evaluation methodology yet exists has been done in other NLP tasks such as topic boundary detection (Reynar, 1994) and local coherence modelling (Barzilay and Lapata, 2005). We are encouraged, however, that subsequent to our experiment, more resources for opinion anald(u, v) = dir moi(v) × T (u, v) × E(u, v) Consider the following example consisting of four-node UDF tree and importance scores. 11 # Features # Evaluated Features # Children (depth 0) # Children (depth 1 fertile) mean 55.3889 21.6667 11.3056 5.5495 std. 8.5547 5.9722 0.7753 1.7724 visualizing data in the customer review domain, even for novice users (Carenini et al., 2006). In a Treemap, the feature hierarchy is represented by nested rectang"
W09-2804,J08-1001,0,\N,Missing
W10-2603,P07-1056,0,0.10648,"produced in each iteration of selftraining. Thus, one of the issues addressed in this paper is to asses whether self-training is useful for domain adaptation. A more sophisticated semi-supervised domain adaptation method is structural correspondence learning (SCL). SCL uses unlabeled data to determine correspondences between features in the two domains by correlating them with so-called pivot features, which are features exhibiting similar behaviors in the source and target domains. Blitzer applied this algorithm successfully to POS tagging (Blitzer et al., 2006) and sentiment classification (Blitzer et al., 2007). SCL seems promising for other tasks as well, for example parse disambiguation (Plank, 2009). We give an overview first of work on supervised and semi-supervised domain adaptation, then of research on summarization of conversations. 2.1 Semi-supervised Domain Adaptation Supervised Domain Adaptation Many domain adaptation methods have been proposed for the supervised case, where a small amount of labeled data in the target domain is used along with a larger amount of labeled source data. Two baseline approaches are to train only on the source data or only on target training data. One way of us"
W10-2603,W06-1615,0,0.0557686,"sing the reranker to select the candidate instances produced in each iteration of selftraining. Thus, one of the issues addressed in this paper is to asses whether self-training is useful for domain adaptation. A more sophisticated semi-supervised domain adaptation method is structural correspondence learning (SCL). SCL uses unlabeled data to determine correspondences between features in the two domains by correlating them with so-called pivot features, which are features exhibiting similar behaviors in the source and target domains. Blitzer applied this algorithm successfully to POS tagging (Blitzer et al., 2006) and sentiment classification (Blitzer et al., 2007). SCL seems promising for other tasks as well, for example parse disambiguation (Plank, 2009). We give an overview first of work on supervised and semi-supervised domain adaptation, then of research on summarization of conversations. 2.1 Semi-supervised Domain Adaptation Supervised Domain Adaptation Many domain adaptation methods have been proposed for the supervised case, where a small amount of labeled data in the target domain is used along with a larger amount of labeled source data. Two baseline approaches are to train only on the source"
W10-2603,P07-1033,0,0.346247,"ined on the target data. They find improvement in a capitalizer that adapts using out-ofdomain and a small amount of in-domain data versus only training on out-of-domain WSJ data. Similar to the prior method, Daume’s MEGA model also trains a MEMM. It achieves domain adaptation through hyperparameters that indicate whether an instance is generated by a source, target, or general distribution, and finds the optimal values of the parameters through conditional EM (Daume and Marcu, 2006). A simpler method of domain adaptation, that achieves a performance similar to prior and MEGA, was proposed by Daume (2007) and successfully applied to a variety of NPL sequence labeling problems, such as named entity recognition, shallow parsing, and part-of-speech (POS) tagging. Furthermore, this approach is straightforward to apply by copying feature values so there is a source version, a target version, and a general version of the feature, and was found to be faster to train than MEGA and prior. For all these reasons, we use Daume’s method and not the other two in our experiments. 2.3 Summarization We would like to use domain adaptation to aid in summarizing multi-party conversations hailing from different mo"
W10-2603,N04-1001,0,0.0226708,"are to train only on the source data or only on target training data. One way of using information from both domains is merging the source and target labeled data sets and training a model on the combination. A method inspired by boosting is to take a linear combination of the predictions of two classifiers, one trained on the source and one trained on the target training data. Another simple method is to train a predictor on the source data, run it on the target data, and then use its predictions on each instance as additional features for a target-trained model. This was first introduced by Florian et al. (2004), who applied it to multilingual named entity recognition. The prior method of domain adaptation by Chelba and Acero (2006) involves using the source data to find optimal parameter values of a maximum entropy model on that data, and then setting these as a prior on the values of a model trained on the target data. They find improvement in a capitalizer that adapts using out-ofdomain and a small amount of in-domain data versus only training on out-of-domain WSJ data. Similar to the prior method, Daume’s MEGA model also trains a MEMM. It achieves domain adaptation through hyperparameters that in"
W10-2603,W06-1643,0,0.0674048,"erent modalities. This contrasts with much of previous work on summarization of conversations, which has focused on domainspecific features (e.g., Rambow et al, 2004). We will treat summarization as a supervised binary classification problem where the sentences of a conversation are rated by their informativeness and a subset is selected to form an extractive summary. Research in meeting summarization relevant to our task has investigated the utility of employing a large feature set including prosodic information, speaker status, lexical and structural discourse features (Murray et al., 2006; Galley, 2006). For email summarization, we view an 17 email thread as a conversation. For summarizing email threads, Rambow (2004) used lexical features such as tf.idf, features that considered the thread to be a sequence of turns, and emailspecific features such as number of recipients and the subject line. Asynchronous multi-party conversations were successfully represented for summarization through a small number of conversational features by Murray and Carenini (2008). This paved the way to cross-domain conversation summarization by representing both email threads and meetings with a set of common conv"
W10-2603,N06-1020,0,0.237561,"ere each sen16 Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 16–22, c Uppsala, Sweden, 15 July 2010. 2010 Association for Computational Linguistics 2 Related Work 2.2 Because unlabeled data is usually much easier to collect than labeled data in a new domain, semisupervised domain adaptation methods that exploit unlabeled data are potentially very useful. In self-training, a training set is used that is originally composed of labeled data, and repeatedly augmented with the highest confidence predictions on unlabeled data. McClosky et al. (2006) apply this in a domain adaptation setting for parsing: with only unlabeled data in the target Brown domain, and labeled and unlabeled datasets in the news domain (WSJ and NANC respectively), a self-trained reranking parser performs almost as well as a parser trained only on Brown labeled data. However, McClosky concludes that self-training alone is not beneficial, and most of the improvements they get over previous work on domain adaptation for parsing are due to using the reranker to select the candidate instances produced in each iteration of selftraining. Thus, one of the issues addressed"
W10-2603,D08-1081,1,0.934797,"lity of employing a large feature set including prosodic information, speaker status, lexical and structural discourse features (Murray et al., 2006; Galley, 2006). For email summarization, we view an 17 email thread as a conversation. For summarizing email threads, Rambow (2004) used lexical features such as tf.idf, features that considered the thread to be a sequence of turns, and emailspecific features such as number of recipients and the subject line. Asynchronous multi-party conversations were successfully represented for summarization through a small number of conversational features by Murray and Carenini (2008). This paved the way to cross-domain conversation summarization by representing both email threads and meetings with a set of common conversational features. The work we present here investigates using data from both emails and meetings in summarizing emails, and compares using conversational versus lexical features. 3 notators authoring abstracts and linking email thread sentences to the abstract sentences. The Enron email corpus 2 : a collection of emails released as part of the investigation into the Enron corporation, it has become a popular corpus for NLP research due to being realistic,"
W10-2603,N10-1132,1,0.816443,"pants. For an email thread, a turn consists of a single email fragment in the exchange. Similarly, for meetings, a turn is a sequence of dialogue acts by the same speaker. The conversational features, which are described in detail in (Murray and Carenini, 2008), include sentence length, sentence position in the conversation and in the current turn, pause-style features, lexical cohesion, centroid scores, and features that measure how terms cluster between conversation participants and conversation turns. Lexical features: We derive an extensive set of lexical features, originally proposed in (Murray et al., 2010) from the AMI and BC3 datasets, and then compute their occurrence in the Enron corpus. After throwing out features that occur less than five times, we end up with approximately 200,000 features. The features derived are: character trigrams, word bigrams, POS tag bigrams, word pairs, POS pairs, and varying instantiation ngram (VIN) features. For word pairs, we extract the ordered pairs of words that occur in the same sentence, and similarly for POS pairs. To derive VIN features, we take each word bigram w1,w2 and further represent it as two patterns p1,w2 and w1,p2 each consisting of a word and"
W10-2603,N06-1047,1,0.636416,"ons hailing from different modalities. This contrasts with much of previous work on summarization of conversations, which has focused on domainspecific features (e.g., Rambow et al, 2004). We will treat summarization as a supervised binary classification problem where the sentences of a conversation are rated by their informativeness and a subset is selected to form an extractive summary. Research in meeting summarization relevant to our task has investigated the utility of employing a large feature set including prosodic information, speaker status, lexical and structural discourse features (Murray et al., 2006; Galley, 2006). For email summarization, we view an 17 email thread as a conversation. For summarizing email threads, Rambow (2004) used lexical features such as tf.idf, features that considered the thread to be a sequence of turns, and emailspecific features such as number of recipients and the subject line. Asynchronous multi-party conversations were successfully represented for summarization through a small number of conversational features by Murray and Carenini (2008). This paved the way to cross-domain conversation summarization by representing both email threads and meetings with a set"
W10-2603,E09-3005,0,0.0164474,"s whether self-training is useful for domain adaptation. A more sophisticated semi-supervised domain adaptation method is structural correspondence learning (SCL). SCL uses unlabeled data to determine correspondences between features in the two domains by correlating them with so-called pivot features, which are features exhibiting similar behaviors in the source and target domains. Blitzer applied this algorithm successfully to POS tagging (Blitzer et al., 2006) and sentiment classification (Blitzer et al., 2007). SCL seems promising for other tasks as well, for example parse disambiguation (Plank, 2009). We give an overview first of work on supervised and semi-supervised domain adaptation, then of research on summarization of conversations. 2.1 Semi-supervised Domain Adaptation Supervised Domain Adaptation Many domain adaptation methods have been proposed for the supervised case, where a small amount of labeled data in the target domain is used along with a larger amount of labeled source data. Two baseline approaches are to train only on the source data or only on target training data. One way of using information from both domains is merging the source and target labeled data sets and trai"
W10-2603,N04-4027,0,0.119734,"eech (POS) tagging. Furthermore, this approach is straightforward to apply by copying feature values so there is a source version, a target version, and a general version of the feature, and was found to be faster to train than MEGA and prior. For all these reasons, we use Daume’s method and not the other two in our experiments. 2.3 Summarization We would like to use domain adaptation to aid in summarizing multi-party conversations hailing from different modalities. This contrasts with much of previous work on summarization of conversations, which has focused on domainspecific features (e.g., Rambow et al, 2004). We will treat summarization as a supervised binary classification problem where the sentences of a conversation are rated by their informativeness and a subset is selected to form an extractive summary. Research in meeting summarization relevant to our task has investigated the utility of employing a large feature set including prosodic information, speaker status, lexical and structural discourse features (Murray et al., 2006; Galley, 2006). For email summarization, we view an 17 email thread as a conversation. For summarizing email threads, Rambow (2004) used lexical features such as tf.id"
W10-2603,W10-2608,0,\N,Missing
W10-4211,J05-3002,0,0.0675543,"acts in terms of coherence and usability according to human ratings. In general, users rate abstract-style summaries much more highly than extracts for these conversations. 2 Related Research Automatic summarizaton has been described as consisting of interpretation, transformation and generation (Jones, 1999). Popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (Knight and Marcu, 2000; Clarke and Lapata, 2006) or sentence merging (Barzilay and McKeown, 2005). In contrast, in this work we clearly separate interpretation from transformation and incorporate an NLG component to generate new text to describe meeting conversations. While extraction remains the most common approach to text summarization, one application in which abstractive summarization is widely used is data-to-text generation. Summarization is critical for data-to-text generation because the amount of collected data may be massive. Examples of such applications include the summarization of intensive care unit data in the medical domain (Portet et al., 2009) and data from gas turbine"
W10-4211,W08-1106,1,0.76396,"audit (Murray et al., 2009) , finding that human abstracts are a challenging gold-standard in terms of enabling participants to work quickly and correctly identify the relevant information. For that task, automatic extracts and the semi-automatic abstracts of Kleinbauer et al. (2007) were found to be competitive with one another in terms of user satisfaction and resultant task scores. Other research on comparing extracts and abstracts has found that an automatic abstractor outperforms a generic extractor in the domains of technical articles (Saggion and Lapalme, 2002) and evaluative reviews (Carenini and Cheung, 2008), and that human-written abstracts were rated best overall. 3 Interpretation - Ontology Mapping Source document interpretation in our system relies on a general conversation ontology. The ontology is written in OWL/RDF and contains upperlevel classes such as Participant, Entity, Utterance, and DialogueAct. When additional information is available about participant roles in a given domain, Participant subclasses such as ProjectManager can be utilized. Object properties connect instances of ontology classes; for example, the following entry in the ontology states that the object property hasSpea"
W10-4211,W95-0110,0,0.340451,"Missing"
W10-4211,P06-2019,0,0.00998099,"stract summaries significantly outperform extracts in terms of coherence and usability according to human ratings. In general, users rate abstract-style summaries much more highly than extracts for these conversations. 2 Related Research Automatic summarizaton has been described as consisting of interpretation, transformation and generation (Jones, 1999). Popular approaches to text extraction essentially collapse interpretation and transformation into one step, with generation either being ignored or consisting of postprocessing techniques such as sentence compression (Knight and Marcu, 2000; Clarke and Lapata, 2006) or sentence merging (Barzilay and McKeown, 2005). In contrast, in this work we clearly separate interpretation from transformation and incorporate an NLG component to generate new text to describe meeting conversations. While extraction remains the most common approach to text summarization, one application in which abstractive summarization is widely used is data-to-text generation. Summarization is critical for data-to-text generation because the amount of collected data may be massive. Examples of such applications include the summarization of intensive care unit data in the medical domain"
W10-4211,2007.sigdial-1.40,0,0.0243423,"Missing"
W10-4211,W07-2324,0,0.0271466,"lication in which abstractive summarization is widely used is data-to-text generation. Summarization is critical for data-to-text generation because the amount of collected data may be massive. Examples of such applications include the summarization of intensive care unit data in the medical domain (Portet et al., 2009) and data from gas turbine sensors (Yu et al., 2007). Our approach is similar except that our input is text data in the form of conversations. We otherwise utilize a very similar architecture of pattern recognition, pattern abstraction, pattern selection and summary generation. Kleinbauer et al. (2007) carry out topic-based meeting abstraction. Our systems differ in two major respects: their summarization process uses human gold-standard annotations of topic segments, topic labels and content items from the ontology, while our summarizer is fully automatic; secondly, the ontology they used is specific not just to meetings but to the AMI scenario meetings (Carletta et al., 2005), while our ontology applies to conversations in general, allowing our approach to be extended to emails, blogs, etc. In this work we conduct a user study where participants use summaries to browse meeting transcripts"
W10-4211,P07-1103,0,0.0784241,"Missing"
W10-4211,D08-1081,1,0.686131,"the detection of items such as decisions, actions, and problems. This component uses general features that are applicable to any conversation domain. The first set of features we use for this ontology mapping are features relating to conversational structure. They include sentence length, sentence position in the conversation and in the current turn, pause-style features, lexical cohesion, centroid scores, and features that measure how terms cluster between conversation participants and conversation turns. While these features have been found to work well for generic extractive summarization (Murray and Carenini, 2008), we use additional features for capturing the more specific sentence-level phenomena of this research. These include character trigrams, word bigrams, part-of-speech bigrams, word pairs, part-of-speech pairs, and varying instantiation n-grams, described in more detail in (Murray et al., 2010). After removing features that occur fewer than five times, we end up with 218,957 total features. 3.2 Message Generation Rather than merely classifying individual sentences as decisions, action items, and so on, we also aim to detect larger patterns – or messages – within the meeting. For example, a give"
W10-4211,N10-1132,1,0.861483,"osition in the conversation and in the current turn, pause-style features, lexical cohesion, centroid scores, and features that measure how terms cluster between conversation participants and conversation turns. While these features have been found to work well for generic extractive summarization (Murray and Carenini, 2008), we use additional features for capturing the more specific sentence-level phenomena of this research. These include character trigrams, word bigrams, part-of-speech bigrams, word pairs, part-of-speech pairs, and varying instantiation n-grams, described in more detail in (Murray et al., 2010). After removing features that occur fewer than five times, we end up with 218,957 total features. 3.2 Message Generation Rather than merely classifying individual sentences as decisions, action items, and so on, we also aim to detect larger patterns – or messages – within the meeting. For example, a given participant may repeatedly make positive comments about an entity throughout the meeting, or may give contrasting opinions of an entity. In order to determine which messages are essential for summarizing meetings, three human judges conducted a detailed analysis of four development set meeti"
W10-4211,J02-4005,0,0.0347701,"d extracts and abstracts for the task of a decision audit (Murray et al., 2009) , finding that human abstracts are a challenging gold-standard in terms of enabling participants to work quickly and correctly identify the relevant information. For that task, automatic extracts and the semi-automatic abstracts of Kleinbauer et al. (2007) were found to be competitive with one another in terms of user satisfaction and resultant task scores. Other research on comparing extracts and abstracts has found that an automatic abstractor outperforms a generic extractor in the domains of technical articles (Saggion and Lapalme, 2002) and evaluative reviews (Carenini and Cheung, 2008), and that human-written abstracts were rated best overall. 3 Interpretation - Ontology Mapping Source document interpretation in our system relies on a general conversation ontology. The ontology is written in OWL/RDF and contains upperlevel classes such as Participant, Entity, Utterance, and DialogueAct. When additional information is available about participant roles in a given domain, Participant subclasses such as ProjectManager can be utilized. Object properties connect instances of ontology classes; for example, the following entry in t"
W10-4211,wilson-2008-annotating,0,0.023189,"Missing"
W12-2602,J05-3002,0,0.0880559,"topic. In Section 4, we discuss community detection algorithms and evaluation metrics that allow overlapping communities. Work on detecting adjacency pairs (Shriberg et al., 2004; Galley et al., 2004) also involves classifying sentence pairs as being somehow related. For example, if sentence B directly follows sentence A, we might determine that they have a relationship such as question-answer or request-accept. In contrast, with ACD there is no requirement that sentence pairs be adjacent or even in proximity to one another, nor must they be in a rhetorical relation. Work on sentence fusion (Barzilay and McKeown, 2005) identifies sentences containing similar or repeated information and combines them into new sentences. In contrast, in our task sentences need not contain repeated information in order to be linked. For example, two sentences could be linked to a common abstract sentence due to a more complex rhetorical relationship such as proposal-reject or question-answer. ACD is a more general problem that may incorporate elements of topic clustering, adjacency pair detection and other sentence clustering or pairing tasks. Here we try to directly learn the abstractive sentence links using lower-level featu"
W12-2602,P04-1085,0,0.0142781,"in numerous abstract sentences. From a practical standpoint, in our work on ACD we cannot use many of the methods and evaluation metrics designed for topic clustering, due to the fact that a 11 document sentence can belong to more than one abstract sentence. This leads to overlapping communities, whereas most work on topic clustering has focused primarily on disjoint communities where a sentence belongs to a single topic. In Section 4, we discuss community detection algorithms and evaluation metrics that allow overlapping communities. Work on detecting adjacency pairs (Shriberg et al., 2004; Galley et al., 2004) also involves classifying sentence pairs as being somehow related. For example, if sentence B directly follows sentence A, we might determine that they have a relationship such as question-answer or request-accept. In contrast, with ACD there is no requirement that sentence pairs be adjacent or even in proximity to one another, nor must they be in a rhetorical relation. Work on sentence fusion (Barzilay and McKeown, 2005) identifies sentences containing similar or repeated information and combines them into new sentences. In contrast, in our task sentences need not contain repeated informatio"
W12-2602,D10-1038,1,0.872301,"of abstractive summarization wherein document sentences are grouped according to whether they can be jointly realized by a common abstractive sentence. The first step of ACD, where we predict links between sentence pairs, can be seen to encompass extraction since the link is via an as-yetungenerated abstract sentence, i.e. each linked sentence is considered summary-worthy. However, the second step moves away from extraction by clustering the linked sentences from the document in order to generate abstract summary sentences. ACD also differs from topic clustering (Malioutov and Barzilay, 2006; Joty et al., 2010), though there are superficial similarities. A first observation is that topic links and abstract links are genuinely different phenomena, though sometimes related. A single abstract sentence can reference more than one topic, e.g. They talked about the interface design and the budget report, and a single topic can be referenced in numerous abstract sentences. From a practical standpoint, in our work on ACD we cannot use many of the methods and evaluation metrics designed for topic clustering, due to the fact that a 11 document sentence can belong to more than one abstract sentence. This leads"
W12-2602,P06-1004,0,0.225675,"ant or not, ACD is a sub-task of abstractive summarization wherein document sentences are grouped according to whether they can be jointly realized by a common abstractive sentence. The first step of ACD, where we predict links between sentence pairs, can be seen to encompass extraction since the link is via an as-yetungenerated abstract sentence, i.e. each linked sentence is considered summary-worthy. However, the second step moves away from extraction by clustering the linked sentences from the document in order to generate abstract summary sentences. ACD also differs from topic clustering (Malioutov and Barzilay, 2006; Joty et al., 2010), though there are superficial similarities. A first observation is that topic links and abstract links are genuinely different phenomena, though sometimes related. A single abstract sentence can reference more than one topic, e.g. They talked about the interface design and the budget report, and a single topic can be referenced in numerous abstract sentences. From a practical standpoint, in our work on ACD we cannot use many of the methods and evaluation metrics designed for topic clustering, due to the fact that a 11 document sentence can belong to more than one abstract"
W12-2602,E99-1011,0,0.0820362,"icipants play roles within a fictional company. For each meeting, an annotator first authors an abstractive summary. Multiple annotators then create extractive summaries by linking sentences from the meeting transcript to sentences within the abstract. This generates a many-to-many mapping between transcript sentences and abstract sentences, so that a given transcript sentence can relate to more than one abstract sentence and viceverse. A sample of this extractive-abstractive linking was shown in Figure 1. It is known that inter-annotator agreement can be quite low for the summarization task (Mani et al., 1999; Mani, 2001b), and this is the case with the AMI extractive summarization codings. The average κ score is 0.45. In these experiments, we use only humanauthored transcripts and plan to use speech recognition transcripts in the future. Note that our overall approach is not specific to conversations or to speech data. Step 2 is completely general, while Step 1 uses a single same-speaker feature that is specific to conversations. That feature can be dropped to make our approach completely general (or, equivalently, that binary feature can be thought of as always 1 when applied to monologic text)."
W12-2602,W01-0100,0,0.574013,"h other relevant tasks such as extractive summarization and topic clustering. In Sections 3-4, we describe the two ACD steps before we can fully discuss evaluation methods. Section 5 describes the experimental setup and corpora used, including a description of the abstractive and extractive summary annotations and the links between them. In Section 6, we give a detailed description of the Omega Index and explain how it differs from the more common Rand Index. In Sections 7-8 we present results and draw conclusions. 2 Related Work The ACD task differs from more common extractive summarization (Mani, 2001a; Jurafsky and Martin, 2008). Whereas extraction involves simply classifying sentences as important or not, ACD is a sub-task of abstractive summarization wherein document sentences are grouped according to whether they can be jointly realized by a common abstractive sentence. The first step of ACD, where we predict links between sentence pairs, can be seen to encompass extraction since the link is via an as-yetungenerated abstract sentence, i.e. each linked sentence is considered summary-worthy. However, the second step moves away from extraction by clustering the linked sentences from the d"
W12-2602,W10-4211,1,0.765138,"all/f-scores as well as the AUROC metrics. We compare our supervised classifier (labeled “Abstractive Links”) with a lower-bound where all instances are predicted as positive, leading to perfect recall and low precision. Our system scores moderately well on both precision and recall, with an average f-score of 0.54. The AUROC for the abstractive link classifier is 0.89. It is difficult to compare with previous work since, to our knowledge, nobody has previously modeled these extractive-abstractive mappings between document sentences and associated abstracts. We can compare with the results of Murray et al. (2010), however, who linked sentences by aggregating them into messages. In that work, each message is comprised of sentences that share a dialogue act type (e.g. an action item) and mention at least one common entity (e.g. remote control). Similar to our work, sentences can belong to more than one message. We assess how well their message-based approach captures these abstractive links, reporting their precision/recall/f-scores for this task in Table 1, with their system labeled “Message Links”. While their precision is above the lower-bound, the recall and f-score are extremely low. This demonstra"
W12-2602,W04-2319,0,0.0225207,"topic can be referenced in numerous abstract sentences. From a practical standpoint, in our work on ACD we cannot use many of the methods and evaluation metrics designed for topic clustering, due to the fact that a 11 document sentence can belong to more than one abstract sentence. This leads to overlapping communities, whereas most work on topic clustering has focused primarily on disjoint communities where a sentence belongs to a single topic. In Section 4, we discuss community detection algorithms and evaluation metrics that allow overlapping communities. Work on detecting adjacency pairs (Shriberg et al., 2004; Galley et al., 2004) also involves classifying sentence pairs as being somehow related. For example, if sentence B directly follows sentence A, we might determine that they have a relationship such as question-answer or request-accept. In contrast, with ACD there is no requirement that sentence pairs be adjacent or even in proximity to one another, nor must they be in a rhetorical relation. Work on sentence fusion (Barzilay and McKeown, 2005) identifies sentences containing similar or repeated information and combines them into new sentences. In contrast, in our task sentences need not conta"
W12-2602,W97-0710,0,0.192508,"tractive links as edges. The second step is to identify communities within the graph, where each community corresponds to an abstractive sentence to be generated. In this paper, we describe how the Omega Index, a metric for comparing non-disjoint clustering solutions, can be used as a summarization evaluation metric for this task. We use the Omega Index to compare and contrast several community detection algorithms. 1 Introduction Automatic summarization has long been proposed as a helpful tool for managing the massive amounts of language data in our modern lives (Luhn, 1958; Edmundson, 1969; Teufel and Moens, 1997; Carbonell and Goldstein, 1998; Radev et al., 2001). Most summarization systems are extractive, meaning that a subset of sentences from an input document forms a summary of the whole. Particular significance may be attached to the chosen sentences, e.g. that they are relevant to a provided query, generally important for understanding the overall document, or represent a particular phenomenon such 10 as action items from a meeting. In any case, extraction consists of binary classification of candidate sentences, plus post-processing steps such as sentence ranking and compression. In contrast,"
W13-2117,C10-1037,0,0.660917,"ll pipeline to generate an abstractive summary for each meeting transcript. Our system is similar to that of Murray et al. (2010) in terms of generating abstractive summaries for meeting transcripts. However, we take a lighter supervision for the content selection phase and a different approach towards the language generation phase, which does not rely on the conventional Natural Language Generation (NLG) architecture (Reiter and Dale, 2000). 2) We propose a word graph based approach to aggregate and generate the abstractive sentence summary. Our work extends the word graph method proposed by Filippova (2010) with the following novel contributions: i) We take advantage of lexical knowledge to merge similar nodes by finding their relations in WordNet; ii) We generate new sentences through generalization and aggregation of the original ones, which means that our generated sentences are not necessarily composed of the original words; and iii) We adopt a new ranking strategy to select the best path in the graph by taking the information content and the grammaticality (i.e. fluency) of the sentence into consideration. 3) In order to generate an abstract summary for a meeting, we have to be able to capt"
W13-2117,P12-3014,0,0.0439255,"our sentences from the meeting corpus. However, the collected training samples was the closest available dataset to our needs. Entailment Graph Sentences in a community often include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the sentences in each community, we can discover the information in one sentence that is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences. Similar to earlier work (Lloret et al., 2008; Mehdad et al., 2010; Berant et al., 2011; Adler et al., 2012; Mehdad et al., 2013), we set this problem as a variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). We build an entailment graph for each community of sentences, where nodes are the linked sentences and edges are the entailment relations between nodes. Given two sentences (s1 and s2 ), we aim at identifying the following cases: i) s1 and s2 express the same meaning (bidirectional entailment). In such cases one of the sentences should be eliminated; ii) s1 is more informative than s2 (unidirectional entailment). In such cases, the entailing sentence should repla"
W13-2117,C10-1039,0,0.187708,"Missing"
W13-2117,J05-3002,0,0.0288065,"Missing"
W13-2117,P11-1062,0,0.0964978,"l NLG pipeline (Reiter and Dale, 2000), we exploit 137 C “we should discuss about the remote control and its color” entails “about the remote”, “let’s talk about the remote” and “um remote’s color”, but not “remote’s size is also important”. So we can keep “we should discuss about the remote control and its color” and “remote’s size is also important” and eliminate the others. In this way, TE-based sentence identification can be designed to distinguish meaning-preserving variations from true divergence, regardless of lexical choices and structures. Similar to previous approaches in TE (e.g., (Berant et al., 2011)), we use a supervised method. To train and build the entailment graph, we perform three steps described in the following subsections. F E A x B G x D Figure 2: Building an entailment graph over sentences. Arrows and “x” represent the entailment direction and unknown cases respectively. 1 2.2 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge1 and Crosslingual textual entailment for content synchronization2 (Negri et al., 2012). However,"
W13-2117,W04-3205,0,0.00982008,"pair of sentences. Before aggregating the similarity scores to form an entailment score, we normalize the similarity scores by the length of s2 (in terms of lexical items), when checking the entailment direction from s1 to s2 . In this way, we can estimate the portion of information/facts in s2 which is covered by s1 . The first five scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymy-hyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 tokens). The rationale behind usi"
W13-2117,W10-1751,0,0.0127616,"similarity scores to form an entailment score, we normalize the similarity scores by the length of s2 (in terms of lexical items), when checking the entailment direction from s1 to s2 . In this way, we can estimate the portion of information/facts in s2 which is covered by s1 . The first five scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymy-hyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 tokens). The rationale behind using different entailment features is that"
W13-2117,N03-1020,0,0.0385472,"ank system (Garg et al., 2009) and with one abstractive baseline: 6) Original word graph model (Orig. word graph) (Filippova, 2010). In order to measure the effectiveness of different components, we also evaluated our system using human-annotated sentence communities (GC) in comparison with our community detection model (full). Moreover, we measure the performance of our system (GC) ablating the entailment module (-ent). Evaluation Metrics To evaluate performance, we use the ROUGE-1 and ROUGE-2 (unigram and bigram overlap) F1 score, which correlate well with human rankings of summary quality (Lin and Hovy, 2003). We also ignore stopwords to reduce the impact of high overlap when matching them. Furthermore, to evaluate the grammaticality of our generated summaries in comparison with the original word graph method, following common practice (Barzilay and McKeown, 2005), we randomly selected 10 meeting summaries (total 150 sentences). Then, we asked annotators to give one 3 ROUGE-2 3 4.4 5.1 3.8 4.8 4.0 4.2 Table 1: Performance of different summarization algorithms on human transcripts for meeting conversations. 5 For preprocessing our dataset we use OpenNLP3 for tokenization and part-of-speech tagging."
W13-2117,P09-2066,0,0.0317005,"Missing"
W13-2117,D11-1062,1,0.237432,"an entailment graph over sentences. Arrows and “x” represent the entailment direction and unknown cases respectively. 1 2.2 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge1 and Crosslingual textual entailment for content synchronization2 (Negri et al., 2012). However, such datasets cannot directly support our application, since the RTE datasets are often composed of longer wellformed sentences and paragraphs (Bentivogli et al., 2009; Negri et al., 2011). In order to collect a dataset that is more similar to the goal of our entailment framework, we select a subset of the sixth and seventh RTE challenge main task (i.e., RTE within a Corpus). Our dataset choice is based on the following reasons: i) the length of sentence pairs in RTE6 and RTE7 is shorter than the others, and ii) RTE6 and RTE7 main task datasets are originally created for summarization purpose, which is closer to our work. We sort the RTE6 and RTE7 dataset pairs based on the sentence length and choose the first 2000 samples with an equal number of positive and negative examples."
W13-2117,W01-0100,0,0.509626,"Missing"
W13-2117,S12-1053,1,0.458466,"Missing"
W13-2117,N10-1045,1,0.261584,"differences between our training set and our sentences from the meeting corpus. However, the collected training samples was the closest available dataset to our needs. Entailment Graph Sentences in a community often include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the sentences in each community, we can discover the information in one sentence that is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences. Similar to earlier work (Lloret et al., 2008; Mehdad et al., 2010; Berant et al., 2011; Adler et al., 2012; Mehdad et al., 2013), we set this problem as a variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). We build an entailment graph for each community of sentences, where nodes are the linked sentences and edges are the entailment relations between nodes. Given two sentences (s1 and s2 ), we aim at identifying the following cases: i) s1 and s2 express the same meaning (bidirectional entailment). In such cases one of the sentences should be eliminated; ii) s1 is more informative than s2 (unidirectional entailment). In such c"
W13-2117,P11-1134,1,0.342034,"we normalize the similarity scores by the length of s2 (in terms of lexical items), when checking the entailment direction from s1 to s2 . In this way, we can estimate the portion of information/facts in s2 which is covered by s1 . The first five scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymy-hyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 tokens). The rationale behind using different entailment features is that combining various scores will yield a bett"
W13-2117,P12-2024,1,0.82418,"n similarity scores calculated with a sentence-tosentence matching process. Each example pair of sentences (s1 and s2 ) is represented by a feature vector, where each feature is a specific similarity score estimating whether s1 entails s2 . Entailment graph edge labeling Since our training examples are labeled with binary judgments, we are not able to train a threeway classifier. Therefore, we set the edge labeling problem as a two-way classification task that casts multidirectional entailment as a unidirectional problem, where each pair is analyzed checking for entailment in both directions (Mehdad et al., 2012). In this condition, each original test example is correctly classified if both pairs originated from it are correctly judged (“YES-YES” for bidirectional,“YES-NO” and “NO-YES” for unidirectional entailment and “NO-NO” for unknown cases). Two-way classification represents an intuitive solution to capture multidimensional entailment relations. We compute 18 similarity scores for each pair of sentences. Before aggregating the similarity scores to form an entailment score, we normalize the similarity scores by the length of s2 (in terms of lexical items), when checking the entailment direction fr"
W13-2117,C00-2137,0,0.0351027,"Missing"
W13-2117,N13-1018,1,0.755572,"he meeting corpus. However, the collected training samples was the closest available dataset to our needs. Entailment Graph Sentences in a community often include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the sentences in each community, we can discover the information in one sentence that is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences. Similar to earlier work (Lloret et al., 2008; Mehdad et al., 2010; Berant et al., 2011; Adler et al., 2012; Mehdad et al., 2013), we set this problem as a variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). We build an entailment graph for each community of sentences, where nodes are the linked sentences and edges are the entailment relations between nodes. Given two sentences (s1 and s2 ), we aim at identifying the following cases: i) s1 and s2 express the same meaning (bidirectional entailment). In such cases one of the sentences should be eliminated; ii) s1 is more informative than s2 (unidirectional entailment). In such cases, the entailing sentence should replace or complement the e"
W13-2117,W10-4211,1,0.620092,"ctive task scores; likewise automatic abstractive summaries are preferred in comparison with human extracts (Murray et al., 2010). Moreover, most of the abstractive summarization approaches focus on one component of the system, such as generation (e.g., (Genest and Lapalme, 2010)) or content selection (e.g., (Murray et al., 2012)), instead of developing the full framework for abstractive summarization. To address these limitations, as the main contribution of this paper, we propose a full pipeline to generate an abstractive summary for each meeting transcript. Our system is similar to that of Murray et al. (2010) in terms of generating abstractive summaries for meeting transcripts. However, we take a lighter supervision for the content selection phase and a different approach towards the language generation phase, which does not rely on the conventional Natural Language Generation (NLG) architecture (Reiter and Dale, 2000). 2) We propose a word graph based approach to aggregate and generate the abstractive sentence summary. Our work extends the word graph method proposed by Filippova (2010) with the following novel contributions: i) We take advantage of lexical knowledge to merge similar nodes by find"
W13-2117,W12-2602,1,0.788588,"omponents, which we describe in more detail in the following sections. 2.1 Community Detection While some abstractive summary sentences are very similar to original sentences from the meeting transcript, others can be created by aggregating and merging multiple sentences into an abstract sentence. In order to generate such a sentence, we need to identify which sentences from the original meeting transcript should be combined in generated abstract sentences. This task can be considered as the first step of abstractive meeting summarization and is called “abstractive community detection (ACD)” (Murray et al., 2012). To perform ACD, we follow the same method proposed by Murray et al. (2012), in two steps: First, we classify sentence pairs according to whether or not they should be realized by a common abstractive sentence. For each pair, we extract its structural and linguistic features, and we train a logistic regression classifier over all our training data (described in Section 3.1) exploiting such features. We run the trained classier over sentence pairs, predicting abstractive links between sentences in the document. The result can be represented as an undirected graph where nodes are the sentences,"
W13-2117,W04-3252,0,\N,Missing
W13-2117,W07-1401,0,\N,Missing
W13-4017,W12-1616,0,0.100159,"s and bigrams have been shown to be useful for the task of DA modeling in previous studies (Sun, 2012; Ferschke, 2012; Kim, 2010a; Ravi, 2007; Carvalho, 2005). In addition, unigrams have been shown to be the most effective among the two. So, as the lexical feature, we include the frequency of unigrams in our feature set. Moreover, length of the utterance is another beneficial feature for DA recognition (Ferschke, 2012; Shrestha, 2004; Joty, 2011), which we add to our feature set. The speaker of an utterance has shown its utility for recognizing speech acts (Sun, 2012; Kim, 2010a; Joty, 2011). Sun and Morency (2012) specifically employ a speakeradaptation technique to demonstrate the effectiveness of this feature for DA modeling. We also include the relative position of a sentence in a post for DA modeling since most of previous studies (Ferschke, 2012; Kim, 2010a; Joty, 2011) prove the efficiency of this feature. Dialogue Act Recognition Conversational structure Adjacent utterances in a conversation have a strong correlation in terms of their dialogue acts. As an example, if speaker 1 asks a question to speaker 2, it is a high probability that the next utterance of the conversation would be an answer fr"
W13-4017,W10-4211,1,0.366658,"Missing"
W13-4017,C04-1128,0,0.0163628,"kes it possible to automatically 2 Related Work There have been several studies on supervised dialogue act (DA) modeling. To the best of our knowledge, none of them compare the performance of DA recognition on different synchronous (e.g., meeting and phone) and asynchronous (e.g., email and forum) conversations. Most of the works analyze DA modeling in a specific domain. Carvalho and Cohen (2005) propose classifying emails into their dialogue acts according to two ontologies for nouns and verbs. The ontologies are used for determining the speech acts of each single email with verb-noun pairs. Shrestha and McKeown (2004) also study the 117 Proceedings of the SIGDIAL 2013 Conference, pages 117–121, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics we build the Fragment Quotation Graph. To this end, we follow the procedure proposed by Joty et al. (2011) to extract the graph structure of a thread. problem of DA modeling in email conversations considering the two dialogue acts of question and answer. Likewise, Ravi and Kin (2007) present a DA recognition method for detecting questions and answers in educational discussions. Ferschke et al. (2012) apply DA modeling to Wikipedia disc"
W13-4017,D09-1130,0,0.0886966,"Missing"
W13-4017,E12-1079,0,0.094304,"le email with verb-noun pairs. Shrestha and McKeown (2004) also study the 117 Proceedings of the SIGDIAL 2013 Conference, pages 117–121, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics we build the Fragment Quotation Graph. To this end, we follow the procedure proposed by Joty et al. (2011) to extract the graph structure of a thread. problem of DA modeling in email conversations considering the two dialogue acts of question and answer. Likewise, Ravi and Kin (2007) present a DA recognition method for detecting questions and answers in educational discussions. Ferschke et al. (2012) apply DA modeling to Wikipedia discussions to analyze the collaborative process of editing Wikipedia pages. Kim et al. (2010a) study the task of supervised classification of dialogue acts in one-to-one online chats in the shopping domain. All these previous studies focus on DA recognition in one or two domains, and do not systematically analyze the performance of different dialog act modeling approaches on a comprehensive set of conversation domains. As far as we know, the present work is the first that proposes domain-independent supervised DA modeling techniques, and analyzes their effectiv"
W13-4017,D09-1035,0,0.0178444,"Missing"
W13-4017,W04-2319,0,\N,Missing
W13-4017,D10-1084,0,\N,Missing
W13-4017,W10-2923,0,\N,Missing
W14-3107,P03-1071,0,0.0513609,"g, we group the sentences of a blog conversation into a number of topical clusters and label each cluster by assigning a short informative topic descriptor (i.e., a keyphrase). To find the topical clusters and their associated labels, we apply the topic segmentation and labeling models recently proposed by (Joty et al., 2013b) for asynchronous conversations, and successfully evaluated on email and blog datasets. More specifically, for topic segmentation, we use their best unsupervised topic segmentation model LCSeg+FQG, which extends the generic lexical cohesion based topic segmenter (LCSeg) (Galley et al., 2003) 4.1 Tasks To understand the blog reading tasks, we reviewed the literature focusing on why and how people read blogs. From the analysis, we found that the primary goals of reading blogs include information seeking, fact checking, guidance/opinion seeking, and political surveillance (Kaye, 2005). People may also read blogs to connect to their communities of interest (Dave et al., 2004; Mishne, 2006), or just for fun/ enjoyment (Baumer et al., 2008; Kaye, 2005). Some studies have also revealed interesting behavioural patterns of blog readers. For example, people often look for variety of opinio"
W14-3107,D09-1026,0,0.00774011,"and more generic in other cases. As such, the topic model needs to be revised based 49 on user feedback to better support her analysis tasks. Thus, our goal is to support a human-inthe-loop topic modeling for asynchronous conversations via interactive visualization. There have been some recent works for incorporating user supervision in probabilistic topic models (e.g., Latent Dirichlet Allocation (LDA)) by adding constraints in the form of must-link and cannot-link (Andrzejewski et al., 2009; Hu et al., 2011), or in the form of a one-to-one mapping between LDA’s latent topics and user tags (Ramage et al., 2009). The feedback from users has been also integrated through visualizations, that steers a semi-supervised topic model (Choo et al., 2013). In contrast to the above-mentioned methods that are designed for generic documents, we are focusing on how our topic modeling approach that is specific to asynchronous conversations, can be steered by the end-users. We are planning to combine a visual interface for expressing the user’s intention via a set of actions, and a semi-supervised version of the topic model that can be iteratively refined from such user actions. A set of possible topic revision oper"
W14-3107,P11-1026,0,0.0245055,"l model and current tasks, the topic modeling results may require to be more specific in some cases, and more generic in other cases. As such, the topic model needs to be revised based 49 on user feedback to better support her analysis tasks. Thus, our goal is to support a human-inthe-loop topic modeling for asynchronous conversations via interactive visualization. There have been some recent works for incorporating user supervision in probabilistic topic models (e.g., Latent Dirichlet Allocation (LDA)) by adding constraints in the form of must-link and cannot-link (Andrzejewski et al., 2009; Hu et al., 2011), or in the form of a one-to-one mapping between LDA’s latent topics and user tags (Ramage et al., 2009). The feedback from users has been also integrated through visualizations, that steers a semi-supervised topic model (Choo et al., 2013). In contrast to the above-mentioned methods that are designed for generic documents, we are focusing on how our topic modeling approach that is specific to asynchronous conversations, can be steered by the end-users. We are planning to combine a visual interface for expressing the user’s intention via a set of actions, and a semi-supervised version of the t"
W14-3107,P13-1048,1,0.848055,"of communicating for most people, other conversational modalities such as blogs, microblogs (e.g., Twitter) and discussion fora have quickly become widely popular. Since the nature of data and tasks may vary significantly from one domain to the other, rather than trying to build an one-sizefit-all interface, we follow a design methodology that is driven by modeling the tasks and usage characteristics in a specific domain. In this work, we focus on blogs, where people can express their thoughts and engage in online discussions. Due to the large number of comments with complex thread structure (Joty et al., 2013b), mining and visualizing blog conversations can become a challenging problem. However, the visualization can be effective for other threaded discussions (e.g., news stories, Youtube comments). Users: As shown in Table 1, blog users can be categorized into two groups based on their activities: (a) participants who already contributed to the conversations, and (b) non-participants who wish to join the conversations or analyze the conversations. Depending on different user groups the tasks might vary as well, something that needs to be taken into account in the design process. For example, imag"
W14-3107,J11-2001,0,0.0150223,"FQG captures the reply relations between text fragments, which are extracted by analyzing the actual body of the comments, thus provides a finer representation of the conversation than the reply-to structure. Similarly, the topic labels are found by using their best unsupervised graph-based ranking model (i.e., BiasedCorank) that extracts representative keyphrases for each topical segment by combining informative clues from initial sentences of the segment and the fine-grain conversational structure, i.e., the FQG. For sentiment analysis, we apply the Semantic Orientation CALculator (SO-CAL) (Taboada et al., 2011), which is a lexicon-based approach (i.e., unsupervised) for determining sentiment of a text. Its performance is consistent across various domains and on completely unseen data, thus making a suitable tool for our purpose. We define five different polarity intervals (-2 to +2), and for each comment we count how many sentences fall in any of these polarity intervals to compute the polarity distribution for that comment. While designing and implementing ConVis, we have been mainly working with blog conversations from two different sources: Slashdot1 — a technology related blog site, and Daily Ko"
W14-4318,W06-3406,0,0.0638125,"Missing"
W14-4318,Y12-1050,0,0.0990102,"ed a graph-based and two probabilistic unsupervised approaches for modeling dialogue acts. By comparing those approaches, they demonstrated that the probabilistic approaches were quite effective and performed better than the graph-based one. While the following work is not done on the email domain, Kim et al. (2010) introduced a dialogue act classification on one-on-one online chat forums. To be able to capture sequential dialogue act dependency on chats, they applied a CRF model. They demonstrated that, compared with other classifiers, their CRF model performed the best. In their later work (Kim et al., 2012), they extended the domain to multi-party live chats and proposed new features for that domain. 3 Capturing Conversation Structure in Email Threads In this section, we describe how to build a fragment quotation graph which captures the conversation structure of any email thread at finer granularity. This graph was developed and shown to be effective by Carenini et al. (2011). A key assumption of this approach is that in order to effectively perform summarization and dialogue act modeling, a fine graph representation of the underlying conversation structure is needed. Here, we start with the sa"
W14-4318,W04-3240,0,0.774962,"Missing"
W14-4318,W10-4211,1,0.925676,"Missing"
W14-4318,P05-1045,0,0.029566,"gorized as both dialogue acts and extractive summarization features. In addition, we use word and POS n-grams as our features for dialogue act modeling. These features are extracted by the following process explained in Carvalho et al. (2006). However, we extend the original approach in order to further abstract n-gram features to avoid making them too sparse to be effective. In this section, we describe the derivation process in detail. A multi-step approach is used to generate word n-gram features. First, all words are tagged with the named entity using the Stanford Named Entity Recognizer (Finkel et al., 2005), and are then replaced with these tags. Second, a sequence of word-replacement tasks is applied to all email messages. Initially, some types of punctuation marks (e.g., <&gt;()[];:. and ,) and extra spaces are removed. Then, shortened phrases such as “I’m” and “We’ll” are substituted for more formal versions such as “I am” and “We will”. Next, other replacement tasks are performed. Some of them are described in Table1. In the third step, unigrams and bigrams are extracted. In this paper, unigrams and bigrams refer to all possible sequences of length one and two terms. After extracting all unigra"
W14-4318,D09-1130,0,0.277904,"POS bigram and trigram features whose scores are within the top five greatest. One example of word n-gram features for a Question dialogue act selected by this derivation method is shown in Table 2. Finally, we also include a simplified version of the ClueWordScore (CWS) developed by Carenini et al. (2007), which is listed below. Simplified CWS Feature: The number of overlaps of the content words that occur in both the current and adjacent sentences in the path, ignoring stopwords. 4.2 Dialogue Act Features The relative positions and length features have proven to be beneficial to both tasks (Jeong et al., 2009; Carenini et al., 2008). Hence, these are categorized as both dialogue acts and extractive summarization features. In addition, we use word and POS n-grams as our features for dialogue act modeling. These features are extracted by the following process explained in Carvalho et al. (2006). However, we extend the original approach in order to further abstract n-gram features to avoid making them too sparse to be effective. In this section, we describe the derivation process in detail. A multi-step approach is used to generate word n-gram features. First, all words are tagged with the named enti"
W14-4318,D10-1084,0,0.031053,"t. The details are shown in Table 3. 1 2 http://mallet.cs.umass.edu http://www.cs.ubc.ca/nest/lci/bc3.html 39 No. of Sentences 1300 No. of Extractive Summary Sentences 521 No. of S Sentences 959 No. of Q Sentences 103 No. of R Sentences 68 No. of Su Sentences 73 No. of M Sentences 97 Table 4: Detailed content of the BC3 corpus 6.2 Evaluation Metrics Here, we introduce evaluation metrics for our joint model of extractive summarization and dialogue act recognition. The CRF model has been shown to be the effective one in both dialogue act modeling and extractive summarization (Shen et al., 2007; Kim et al., 2010; Kim et al., 2012). Hence, for comparison, we implement two different CRFs, one for extractive summarization and the other for dialogue act modeling. When classifying extractive summaries using the CRF, we only use its extractive summarization features. Similarly, when modeling dialogue acts, we only use its dialogue act features. In addition, we also com137 pare our system with a non-sequential classifier, a support vector machine (SVM), with the same settings as those described above. For these implementations, we use Mallet and SVM-light package3 (Joachims, 1999). In our experiment, we fir"
W14-4318,N04-4027,0,0.583409,"ying correct summary sentences without losing performance on dialogue act modeling task. 1 Introduction Nowadays, an overwhelming amount of text information can be found on the web. Most of this information is redundant and thus the task of document summarization has attracted much attention. Since emails in particular are used for a wide variety of purposes, the process of automatically summarizing emails might be of great beneﬁt in dealing with this excessive amount of information. Much work has already been conducted on email summarization. The first research on this topic was conducted by Rambow et al. (2004), who took a supervised learning approach to extracting important sentences. A study on the supervised summarization of email threads was also performed by Ulrich et al. (2009). This study used the regression-based method for classification. There have been studies on unsupervised summarization of email threads as well. Zhou et al. (2007, 2008) proposed a graph-based unsupervised approach to email conversation summarization using clue words, i.e., recurring words contained in replies. In addition, the task of labeling sentences with dialogue acts has become important and has been employed in m"
W14-4318,W13-4017,1,0.905166,"Missing"
W14-4318,N03-1033,0,0.123333,"Missing"
W14-4318,P08-1041,1,\N,Missing
W14-4318,D08-1081,1,\N,Missing
W14-4407,N13-1030,0,0.01564,"three words. Note that these rules, which were defined based on close observation of the results obtained from our development set, greatly reduce the chance of selecting illstructured templates. Second, the remaining paths are reranked by 1) A normalized path weight and 2) A language model learned from hypernym-labeled human-authored summaries in our training data, each of which is described below. Template Fusion We further generalize the clustered templates by applying a word graph algorithm. The algorithm was originally proven to be effective in summarizing a cluster of related sentences (Boudin and Morin, 2013; Filippova, 2010; Mehdad et al., 2013). We extend it so that it can be applied to templates. Word Graph Construction In our system, a word graph is a directed graph with words or blanks serving as nodes and edges representing adjacency relations. Given a set of related templates in a group, the graph is constructed by first creating a start and end node, and then iteratively adding templates to it. When adding a new template, the algorithm first checks each word in the template to see if it can be mapped onto existing nodes in the graph. The word is mapped onto a node if the node consists of"
W14-4407,W10-4211,1,0.386939,"Missing"
W14-4407,W12-2602,1,0.856542,"meeting summarization systems (Mehdad et al. 2013; Murray et al. 2010; Wang and Cardie 2013). However, the approaches introduced in previous studies create summaries by either heavily relying on annotated data or by fusing human utterances which may contain grammatical mistakes. In this paper, we address these issues by introducing a novel summariza2. Related Work Several studies have been conducted on creating automatic abstractive meeting summarization systems. One of them includes the system proposed by Mehdad et al., (2013). Their approach first clusters human utterances into communities (Murray et al., 2012) and then builds an entailment graph over each of the latter in order to select the salient utterances. It then applies a semantic word graph algorithm to them and creates abstractive summaries. Their results show some improvement in creating informative summaries. 45 Proceedings of the 8th International Natural Language Generation Conference, pages 45–53, c Philadelphia, Pennsylvania, 19-21 June 2014. 2014 Association for Computational Linguistics However, since they create these summaries by merging human utterances, their summaries are still partially extractive. Recently, there have been s"
W14-4407,W11-0501,0,0.045407,"Missing"
W14-4407,C10-1037,0,0.179103,"hese rules, which were defined based on close observation of the results obtained from our development set, greatly reduce the chance of selecting illstructured templates. Second, the remaining paths are reranked by 1) A normalized path weight and 2) A language model learned from hypernym-labeled human-authored summaries in our training data, each of which is described below. Template Fusion We further generalize the clustered templates by applying a word graph algorithm. The algorithm was originally proven to be effective in summarizing a cluster of related sentences (Boudin and Morin, 2013; Filippova, 2010; Mehdad et al., 2013). We extend it so that it can be applied to templates. Word Graph Construction In our system, a word graph is a directed graph with words or blanks serving as nodes and edges representing adjacency relations. Given a set of related templates in a group, the graph is constructed by first creating a start and end node, and then iteratively adding templates to it. When adding a new template, the algorithm first checks each word in the template to see if it can be mapped onto existing nodes in the graph. The word is mapped onto a node if the node consists of the same word and"
W14-4407,P05-1037,0,0.0148029,"Missing"
W14-4407,W09-0613,0,0.00975681,"ome studies on creating abstract summaries of specific aspects of meetings such as decisions, actions and problems (Murray et al. 2010; Wang and Cardie, 2013). These summaries are called the Focused Meeting Summaries (Carenini et al., 2011). The system introduced by Murray et al. first classifies human utterances into specific aspects of meetings, e.g. decisions, problem, and action, and then maps them onto ontologies. It then selects the most informative subsets from these ontologies and finally generates abstractive summaries of them, utilizing a natural language generation tool, simpleNLG (Gatt and Reiter, 2009). Although their approach is essentially focused meeting summarization, after creating summaries of specific aspects, they aggregate them into one single summary covering the whole meeting. Wang and Cardie introduced a template-based focused abstractive meeting summarization system. Their system first clusters human-authored summary sentences and applies a MultipleSequence Alignment algorithm to them to generate templates. Then, given a meeting transcript to be summarized, it identifies a human utterance cluster describing a specific aspect and extracts all summary-worthy relation instances, i"
W14-4407,P13-1138,0,0.0355438,"Missing"
W14-4407,de-marneffe-etal-2006-generating,0,0.0322498,"Missing"
W14-4407,W13-2117,1,0.906723,"utomatic meeting summarization has been attracting peoples’ attention as it can save a great deal of their time and increase their productivity. The most common approaches to automatic meeting summarization have been extractive. Since extractive approaches do not require natural language generation techniques, they are arguably simpler to apply and have been extensively investigated. However, a user study conducted by Murray et al. (2010) indicates that users prefer abstractive summaries to extractive ones. Thereafter, more attention has been paid to abstractive meeting summarization systems (Mehdad et al. 2013; Murray et al. 2010; Wang and Cardie 2013). However, the approaches introduced in previous studies create summaries by either heavily relying on annotated data or by fusing human utterances which may contain grammatical mistakes. In this paper, we address these issues by introducing a novel summariza2. Related Work Several studies have been conducted on creating automatic abstractive meeting summarization systems. One of them includes the system proposed by Mehdad et al., (2013). Their approach first clusters human utterances into communities (Murray et al., 2012) and then builds an entailmen"
W14-4407,W04-3252,0,\N,Missing
W14-4407,P03-1071,0,\N,Missing
W17-2329,P03-1054,0,0.0334994,". Parts Of Speech (14) We use the Stanford Tagger (Toutanova et al., 2003) to capture the frequency of various parts of speech tags (nouns, verbs, adjectives, adverbs, pronouns, determiners, etc). Frequency counts are normalized by the number of words in the sentence, and we report the sentence average for a given post. We also count not-in-dictionary words and word-type ratios (noun to verb, pronoun to noun, etc). Context Free Grammar (45) Features which count how often a phrase structure rule occurs in a sentence, including NP→VP PP, NP→DT NP, etc. Parse trees come from the Stanford parser (Klein and Manning, 2003). Syntactic Complexity (28) Features which measure the complexity of an utterance through metrics such as the depth of the parse tree, mean length of word, sentences, T-Units and clauses and clauses per sentence. We used the L2 Syntactic Complexity Analyzer (Lu, 2010). Our methods are similar to Fraser et al. (2015), with the main difference being the dataset used and their inclusion of audio and test-specific features, which are not available in our case. To the best of our knowledge, ours is the first comparison of models trained exclusively on unstructured written samples from persons with"
W17-2329,W14-3210,0,0.329474,"al syndrome caused by neurodegenerative illnesses (e.g. Alzheimer’s Disease, vascular dementia, Lewy Body dementia). Symptoms can include memory loss, decreased reasoning ability, behavioral changes, and – relevant to our work – speech and language impairment, including fluency, word choice and sentence structure (Klimova and Kuca, 2016). Recently, there have been attempts to combine clinical information with language analysis using machine learning and NLP techniques to aid in diagnosis of dementia, and to distinguish between types of pathologies (Jarrold et al., 2014; Rentoumi et al., 2014; Orimaye et al., 2014; Fraser et al., 2015; Masrani et al., 2017). This would provide an inexpensive, non-invasive and efficient screening tool to assist in early detection, treatment and institution of supports. Yet, much of the work to date has focused on analyzing spoken language collected during formal assessment, usually with standardized exam tools. In this work we gather a corpus of blog posts publicly available online, some by people with dementia and others by the loved ones of people with dementia. We extract a variety of linguistic features from the texts, and compare multiple machine learning methods f"
W17-2329,W14-3203,0,0.0289733,"Missing"
W17-2329,W14-3204,0,0.0120586,"by 2030 (Prince, 2015). Dementia is a clinical syndrome caused by neurodegenerative illnesses (e.g. Alzheimer’s Disease, vascular dementia, Lewy Body dementia). Symptoms can include memory loss, decreased reasoning ability, behavioral changes, and – relevant to our work – speech and language impairment, including fluency, word choice and sentence structure (Klimova and Kuca, 2016). Recently, there have been attempts to combine clinical information with language analysis using machine learning and NLP techniques to aid in diagnosis of dementia, and to distinguish between types of pathologies (Jarrold et al., 2014; Rentoumi et al., 2014; Orimaye et al., 2014; Fraser et al., 2015; Masrani et al., 2017). This would provide an inexpensive, non-invasive and efficient screening tool to assist in early detection, treatment and institution of supports. Yet, much of the work to date has focused on analyzing spoken language collected during formal assessment, usually with standardized exam tools. In this work we gather a corpus of blog posts publicly available online, some by people with dementia and others by the loved ones of people with dementia. We extract a variety of linguistic features from the texts, an"
W17-2329,N03-1033,0,0.0102106,"ation level, was unavailable. From each of the three dementia blogs, we manually filtered all texts not written by the owner of the blog (e.g. fan letters) or posts containing more images than text. We were left with 1654 samples written by persons with dementia and 1151 from healthy controls. The script to download the corpus is available at https: //github.com/vadmas/blog_corpus/. 3.2 Classification Features Following Fraser et al. (2015), we extracted 101 features across six categories from each blog post. These features are described below. Parts Of Speech (14) We use the Stanford Tagger (Toutanova et al., 2003) to capture the frequency of various parts of speech tags (nouns, verbs, adjectives, adverbs, pronouns, determiners, etc). Frequency counts are normalized by the number of words in the sentence, and we report the sentence average for a given post. We also count not-in-dictionary words and word-type ratios (noun to verb, pronoun to noun, etc). Context Free Grammar (45) Features which count how often a phrase structure rule occurs in a sentence, including NP→VP PP, NP→DT NP, etc. Parse trees come from the Stanford parser (Klein and Manning, 2003). Syntactic Complexity (28) Features which measure"
W17-4502,P13-1048,1,0.943153,"techniques, such as using a mixed-model visualization to show both chronological sequence and reply relationships (Venolia and Neustaedter, 2003), thumbnail metaphor using a sequence of rectangles (Wattenberg and Millen, 2003; Kerr, 2003), and radial tree layout (Pascual-Cid and Kaltenbrunner, 2009). However, such visualizations did not focus on analysing the actual content (i.e., the text) of the conversations. On the other hand, text mining and summarization methods for conversations perform content analysis of the conversations, such as what topics are covered in a given text conversation (Joty et al., 2013b), along with what opinions the conversation participants have expressed on such topics (Taboada et al., 2011). Once the topics, opinions and conversation structure (e.g., replyrelationships between comments) are extracted, they can be used to summarize the conversations (Carenini et al., 2011). However, presenting a static/non-interactive textual summary alone is often not sufficient to satisfy the user information needs. Instead, generating a multimedia output that combines text and visualizations can be more effective, because the two can play complementary roles: while visualization can h"
W17-4502,D14-1124,1,0.860833,"the user to write any code. Such interactive environment would allow the user to have more control over the data to be represented and the interactive techniques to be supported. An early work (Yee and Hearst, 2005) attempted to organize the comments using a treemap like layout, where the parent comment is placed on top as a text block and the space below the parent node is divided between supporting and opposing statements. We plan to follow this idea in ConVis, but incorporating a higher level discourse relation analysis of the conversations along with the detection of controversial topics (Allen et al., 2014). How can we scale up our systems for big data? As social media conversational data is growing in size and complexity at an unprecedented rate, new challenges have emerged from both the computational and the visualization perspectives. In particular, we need to address the following aspects of big data, while designing visual text analytics for online conversations. Volume: Most of the existing visualizations are inadequate to handle very large amounts of raw conversational data. For example, ConVis scales with conversations with hundreds of comments; however, it is unable to deal with a very"
W17-4502,J91-1002,0,0.0292013,"are created to represent the replying relationship between fragments. If a comment does not contain any quotation, then its fragments are linked to the fragments of the comment to which it replies, capturing the original ‘reply-to’ relation. challenges, open questions, and ideas for future work in the research area of multimedia summarization for online conversations. 2 2.1 Multimedia Summarization of Online Conversations The FQG is exploited in both topic segmentation and labeling. In segmentation, each path of the FQG is considered as a separate conversation that is independently segmented (Morris and Hirst, 1991). Then, all the resulting segmentation decisions are consolidated in a final segmentation for the whole conversation. After that, topic labeling generates keyphrases to describe each topic segment in the conversation. A novel graph based ranking model is applied that intuitively boosts the rank of keyphrases that appear in the initial sentences of the segment, and/or also appear in text fragments that are central in the FQG (see (Joty et al., 2013b) for details). Our Approach To generate multimedia summary for online conversation, our primary approach was to apply human-centered design methodo"
W17-4502,J11-2001,0,0.00486738,"nships (Venolia and Neustaedter, 2003), thumbnail metaphor using a sequence of rectangles (Wattenberg and Millen, 2003; Kerr, 2003), and radial tree layout (Pascual-Cid and Kaltenbrunner, 2009). However, such visualizations did not focus on analysing the actual content (i.e., the text) of the conversations. On the other hand, text mining and summarization methods for conversations perform content analysis of the conversations, such as what topics are covered in a given text conversation (Joty et al., 2013b), along with what opinions the conversation participants have expressed on such topics (Taboada et al., 2011). Once the topics, opinions and conversation structure (e.g., replyrelationships between comments) are extracted, they can be used to summarize the conversations (Carenini et al., 2011). However, presenting a static/non-interactive textual summary alone is often not sufficient to satisfy the user information needs. Instead, generating a multimedia output that combines text and visualizations can be more effective, because the two can play complementary roles: while visualization can help the user to discover trends and relationship, text can convey key points about the results, by focusing on"
W17-4506,W10-4211,1,0.801993,"conversation summarization is an important task, since speech is the primary medium of human-human communication. Vast amounts of spoken conversation data are produced daily in call-centers. Due to this overwhelming number of conversations, call-centers can only evaluate a small percentage of the incoming calls (Stepanov et al., 2015). Automatic methods of conversation summarization have a potential to increase the capacity of the call-centers to analyze and assess their work. Earlier works on conversation summarization have mainly focused on extractive techniques. However, as pointed out in (Murray et al., 2010) and (Oya et al., 2014), abstractive summaries are preferred to extractive ones by human judges. The possible reason for this is that extractive techniques are not well suited for the conversation summarization, since there are style differ43 Proceedings of the Workshop on New Frontiers in Summarization, pages 43–47 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics Community Creation Conversation Transcript Summary Generation Conversation Transcript Slot Labeling Community Creation (Linking) Topic Segmentation Clustering Communities Speaker & Phrase Extra"
W17-4506,W14-4407,1,0.900901,"Orkan Bayer2 , Giuseppe Carenini3 , Giuseppe Riccardi2 1 SAIL, University of Southern California, Los Angeles, CA, USA 2 Signals and Interactive Systems Lab, DISI, University of Trento, Trento, Italy 3 Department of Computer Science, University of British Columbia, Vancouver, Canada singlak@usc.edu,carenini@cs.ubc.ca {evgeny.stepanov,aliorkan.bayer,giuseppe.riccardi}@unitn.it Abstract ences between spoken conversations and humanauthored summaries. Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (Mehdad et al., 2013; Oya et al., 2014). The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models. The graph paths are ranked to yield abstract sentences – a template. And these templates are selected for population with entities extracted from a conversation. Thus the abstractive summarization systems are limited to these templates generated by supervised data sources. The template selection strategy in these systems leverages on the manual links between summary and conversation sentences. Unfortunately, such manual links are rarely available."
W17-4506,de-marneffe-etal-2006-generating,0,0.0693538,"Missing"
W17-4506,pianta-etal-2008-textpro,0,0.0231992,"tion follows the approach of (Oya et al., 2014) and, starting from human-authored summaries, produces abstract templates applying slot labeling, summary clustering and template fusion steps. The information required for the template generation are part-of-speech (POS) tags, noun and verb phrase chunks, and root verbs from dependency parsing. For English, we use Illinois Chunker (Punyakanok and Roth, 2001) to identify noun phrases and extract part-of-speech tags; and the the tool of (De Marneffe et al., 2006) for generating dependency parses. For Italian, on the other hand, we use TextPro 2.0 (Pianta et al., 2008) to perform all the Natural Language Processing tasks. In the slot labeling step, noun phrases from human-authored summaries are replaced by WordNet (Fellbaum, 1998) SynSet IDs of the head nouns (right most for English). For a word, SynSet ID of the most frequent sense is selected with respect to the POS-tag. To get hypernyms for Italian we use MultiWordNet (Pianta et al., 2002). The clustering of the abstract templates generated in the previous step is performed using the WordNet hierarchy of the root verb of a sentence. The number of sentences selected for a community is set to 4, since it i"
W17-4506,W09-0505,1,0.761821,"(Mehdad et al., 2013); and our run of the system of (Oya et al., 2014). With manual communities we have Data Sets The two corpora used for the evaluation of the heuristics are AMI and LUNA. The AMI meeting corpus (Carletta et al., 2006) is a collection of 139 meeting records where groups of people are engaged in a ‘roleplay’ as a team and each speaker assumes a certain role in a team (e.g. project manager (PM)). Following (Oya et al., 2014), we removed 20 dialogs used by the authors for development, and use the remaining dialogs for the threefold cross-validation. The LUNA Human-Human corpus (Dinarelli et al., 2009) consists of 572 call-center dialogs where a client and an agent are engaged in a problem solving task over the phone. The 200 Italian LUNA dialogs have been annotated with summaries by 5 native speakers (5 summaries per dialog). For the Call Centre Conversation Summarization (CCCS) shared task (Favre et al., 2015) 45 Model Mehdad et al. (2013) Oya et al. (2014) (15 seg.) Manual Communities (H2) Top 4 turns: token (H3) Top 4 turns: SynSetID (H4) Top 4 turns: Av. WE ROUGE-2 0.040 0.068 0.072 0.076 0.077 0.079 For the 100 English LUNA dialogs, we observe the same pattern as for Italian dialogs a"
W17-4506,W15-4633,1,0.876042,"Missing"
W17-4506,P03-1071,0,0.0257184,"talian corpus (Koehn, 2005)2 . We empirically choose 300, 5, and 5 for the embedding size, window length, and word count threshold, respectively. 1 2 44 https://github.com/mmihaltz/word2vec-GoogleNews-vectors http://www.statmt.org/europarl/ 2.3 a set of 100 dialogs was manually translated to English. The conversations are equally split into training and testing sets as 100/100 for Italian, and 50/50 for English. Summary Generation The first step in summary generation is the segmentation of conversations into topics using a lexical cohesion-based domain-independent discourse segmenter – LCSeg (Galley et al., 2003). The purpose of this step is to cover all the conversation topics. Next, all possible slot ‘fillers’ are extracted from the topic segments and are ranked with respect to their frequency in the conversation. An abstract template for a segment is selected with respect to the average cosine similarity of the segment and the community linked to that template. The selected template slots are filled with the ‘fillers’ extracted earlier. 2.4 3.2 ROUGE-2 metric (Lin, 2004) is used for the evaluation. The metric considers bigram-level precision, recall and F-measure between a set of reference and hypo"
W17-4506,W04-3250,0,0.0700753,"Missing"
W17-4506,2005.mtsummit-papers.11,0,0.0207608,"the previous step is performed using the WordNet hierarchy of the root verb of a sentence. The number of sentences selected for a community is set to 4, since it is the average size of the manual community in the AMI corpus. We use word2vec tool (Mikolov et al., 2013) for learning distributed word embeddings. For English, we obtained pre-trained word embeddings trained on a part of Google News data set (about 3 billion words)1 . The model contains 300-dimensional vectors for 3 million words and phrases. For Italian, we use the word2vec to train word embeddings on the Europarl Italian corpus (Koehn, 2005)2 . We empirically choose 300, 5, and 5 for the embedding size, window length, and word count threshold, respectively. 1 2 44 https://github.com/mmihaltz/word2vec-GoogleNews-vectors http://www.statmt.org/europarl/ 2.3 a set of 100 dialogs was manually translated to English. The conversations are equally split into training and testing sets as 100/100 for Italian, and 50/50 for English. Summary Generation The first step in summary generation is the segmentation of conversations into topics using a lexical cohesion-based domain-independent discourse segmenter – LCSeg (Galley et al., 2003). The p"
W17-4506,W04-1013,0,0.032962,"Missing"
W17-4506,W13-2117,1,0.887244,"ny A. Stepanov2 , Ali Orkan Bayer2 , Giuseppe Carenini3 , Giuseppe Riccardi2 1 SAIL, University of Southern California, Los Angeles, CA, USA 2 Signals and Interactive Systems Lab, DISI, University of Trento, Trento, Italy 3 Department of Computer Science, University of British Columbia, Vancouver, Canada singlak@usc.edu,carenini@cs.ubc.ca {evgeny.stepanov,aliorkan.bayer,giuseppe.riccardi}@unitn.it Abstract ences between spoken conversations and humanauthored summaries. Abstractive conversation summarization systems, on the other hand, are mainly based on the extraction of lexical information (Mehdad et al., 2013; Oya et al., 2014). The authors cluster conversation sentences/utterances into communities to identify most relevant ones and aggregate them using word-graph models. The graph paths are ranked to yield abstract sentences – a template. And these templates are selected for population with entities extracted from a conversation. Thus the abstractive summarization systems are limited to these templates generated by supervised data sources. The template selection strategy in these systems leverages on the manual links between summary and conversation sentences. Unfortunately, such manual links are"
W17-5532,P11-1049,0,0.0655636,"Missing"
W17-5532,P08-1041,1,0.906849,"extending previous work on full-thread summarization that considers not 264 asynchronous conversations. FQGs use the fact that a given email often contains quoted material from previous emails. These quotations, or ”fragments”, can then be used to create fine-grained representations of the underlying structure of a given email thread, allowing a set of particularly informative clue words to be identified. In this paper, we also exploit FQGs in our silver standard generation system, and we use a summarizer based on clue words as a baseline in our evaluations. Furthermore, a key limitation of (Carenini et al., 2008), common to other approaches to full-thread summarization, is to consider only the input thread in the summarization process; in contrast, a user’s email history (or that of the user’s organization) can provide valuable background knowledge. The summarizer we propose in this paper addresses this limitation by taking into account background knowledge synthesized from a large number of other email threads, which we argue is especially beneficial to PT summarization as the PT can be rather short and consequently unable to provide much ground for sentence selection. ments, as well as for conversat"
W17-5532,W03-1203,0,0.0725216,"problem where sentences are labeled in/out using standard machine learning classifiers (Rambow et al., 2004; Murray and Carenini, 2008). Variations of this approach include adding sentence compression and using integer linear programming to evaluate candidate summaries and select the best ones (BergKirkpatrick et al., 2011). Sentence classification assumes sentences are independent from one another; and so to capture dependencies between sentences, the extractive summarization problem has also been recast as a sequence labeling problem using hidden Markov models and conditional random fields (Fung et al., 2003; Jin et al., 2012; Oya and Carenini, 2014). The weakness of supervised approaches is the reliance on human-annotated labeled data, which is often expensive and difficult to acquire due to privacy concerns. Our extractive approach, therefore, will focus on unsupervised extractive techniques which do not require labeled data. Another benefit of unsupervised methods is that they can serve as features for supervised methods, meaning improvements in unsupervised techniques can directly benefit supervised systems. Many unsupervised extractive summarization methods have been proposed for generic doc"
W17-5532,W10-4211,1,0.782974,"ified the mechanism by which it handles redundancy. Although in our experiments we did not find consistently significant improvements using Bayesian Surprisebased methods on partial threads, we argue that in light of the observed trends, the potential benefit of background knowledge to PT summarization (and email summarization in general) should be further investigated with larger datasets. There are multiple directions of future work. While an obvious direction is the continued development of extractive PT summarization algorithms (eg. by applying recent summarization techniques such as ILP (Murray et al., 2010) or neural network-based summarizers (Cao et al., 2015)), another is the abstractive summarization of partial threads. Yet another is the application of the silver standard algorithm to other asynchronous conversations, such as discussion forums, as well as other domains where some human annotation is available but reference summaries for different portions of the source document(s) are desired. Future work may also include finding additional Table 2: Length (in words) of the partial threads in the dataset used to define the quartile bins. We observe a number of trends in Table 3 from the expe"
W17-5532,W14-4318,1,0.836064,"n/out using standard machine learning classifiers (Rambow et al., 2004; Murray and Carenini, 2008). Variations of this approach include adding sentence compression and using integer linear programming to evaluate candidate summaries and select the best ones (BergKirkpatrick et al., 2011). Sentence classification assumes sentences are independent from one another; and so to capture dependencies between sentences, the extractive summarization problem has also been recast as a sequence labeling problem using hidden Markov models and conditional random fields (Fung et al., 2003; Jin et al., 2012; Oya and Carenini, 2014). The weakness of supervised approaches is the reliance on human-annotated labeled data, which is often expensive and difficult to acquire due to privacy concerns. Our extractive approach, therefore, will focus on unsupervised extractive techniques which do not require labeled data. Another benefit of unsupervised methods is that they can serve as features for supervised methods, meaning improvements in unsupervised techniques can directly benefit supervised systems. Many unsupervised extractive summarization methods have been proposed for generic docu• We propose an algorithm for exploiting e"
W17-5532,D13-1158,0,0.0431945,"Missing"
W17-5532,N04-4027,0,0.138213,"Missing"
W17-5532,R13-1042,0,0.0154027,"ons were used. The keyphrases were not used here, because it is not expected that most gold standard annotations will include keyphrases. Each thread was annotated by two annotators, so for each thread we have two sets of extractive sentences. The annotators were asked to select up to five sentences ”that contained the most important information in the email, and also rank the sentences in reverse order of their importance”. To serve as a background corpus that could be used for both Bayesian Surprise methods, we used a publicly available collection of threads extracted from the Enron corpus (Jamison and Gurevych, 2013), of which threads ∼43k had the metadata required (sender, recipient(s) and subject line in all emails) in order to extract the desired conversational features. 6 Evaluation over Full Threads Method BS BS-d BSE BSE-d CWS PR-MMR Full threads 0.573 0.582 0.566 0.573 0.598 0.509 Table 1: ROUGE-1 mean F-scores for full threads as compared to gold standard summaries. The results of this experiment over full threads suggest that the Bayesian Surprise-based methods perform comparably to the clue words-based summarizer, and that they all significantly outperform the PR-MMR baseline (p&lt;0.005)2 . In add"
W17-5532,P14-2055,0,0.38429,"been a growing interest in the email summarization task: given an email thread with multiple participants, provide a summary of the contents of the thread. Such 1 http://www.cs.ubc.ca/cs-research/lci/researchgroups/natural-language-processing/Software.html 263 Proceedings of the SIGDIAL 2017 Conference, pages 263–272, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics only the input thread, but also background knowledge synthesized from a large number of other email threads. In particular, we developed a summarization method based on Bayesian Surprise (Louis, 2014) which takes into account conversational features of the partial thread, as discussed in section 4. We then evaluate the system-generated summaries using our silver standards with ROUGE. tion problem previously studied for news in the Text Analysis Conferences (Dang and Owczarzak, 2008). The update summarization problem, applied to email threads, would provide a summary of only the incoming email with the assumption that the user knows and remembers the content of the preceding emails. The new NLP task of summarizing email PT is challenging, not only because new algorithms may need to be devel"
W17-5532,loza-etal-2014-building,0,0.238845,"ROUGE. tion problem previously studied for news in the Text Analysis Conferences (Dang and Owczarzak, 2008). The update summarization problem, applied to email threads, would provide a summary of only the incoming email with the assumption that the user knows and remembers the content of the preceding emails. The new NLP task of summarizing email PT is challenging, not only because new algorithms may need to be developed, but also with respect to evaluating the generated summaries. While there are publicly available datasets - including BC3 (Ulrich et al., 2008) and an Enron-derived dataset (Loza et al., 2014) - that provide gold standard summaries for completed email threads, none to our knowledge provides such summaries for PTs; such annotation by humans would be prohibitive, as it would require a summary for each partial thread (i.e., each email) in the corpus. So, a challenge we face in the evaluation of PT summaries is due to the dearth of human annotations. More specifically, given gold standard human annotations of a thread as a whole, how do we generate reference summaries of each PT against which to compare automatically generated extractive summaries? Most current summarization techniques"
W17-5532,D08-1081,1,0.745041,"rization techniques that do not take into account background information. 2 Related Work To generate PT summaries we propose an unsupervised extractive approach. Although to the best of our knowledge no one has studied PT summarization directly, there has been extensive work done in extractive summarization in general, as well as work done on email summarization specifically. Supervised methods have been proposed which turn the extractive summarization task into a binary classification problem where sentences are labeled in/out using standard machine learning classifiers (Rambow et al., 2004; Murray and Carenini, 2008). Variations of this approach include adding sentence compression and using integer linear programming to evaluate candidate summaries and select the best ones (BergKirkpatrick et al., 2011). Sentence classification assumes sentences are independent from one another; and so to capture dependencies between sentences, the extractive summarization problem has also been recast as a sequence labeling problem using hidden Markov models and conditional random fields (Fung et al., 2003; Jin et al., 2012; Oya and Carenini, 2014). The weakness of supervised approaches is the reliance on human-annotated"
W17-5535,P14-1048,0,0.0830867,"popular alternative for learning the embedding of a sentence (Kiros et al., 2015). In this setting, an encoder RNN encodes a sentence into a fixed vector representation that is then used by a decoder RNN to predict the following and preceding sentences and based on how good the predictions were, updates both the decoder and encoder RNNs. Once training is done, the encoder RNN can be used to create an embedding for any text span. In this paper, we have used the encoder Examples of effective sentence level and document level Discourse Parsers include CODRA (Joty et al., 2015) and the parser of (Feng and Hirst, 2014) . These parsers use organizational, structural, contextual, lexical and N-gram features to represent Discourse Units and apply graphical models for learning and inference (i.e. Conditional Random Fields). The performance of these parsers critically depends on a careful selection of informative and relevant features, something that is instead performed automatically in the neural models we propose in this paper. (Nakagawa et al., 2010), (Pang et al., 2008) and (Rentoumi et al., 2010), approach Sentiment Analysis using carefully engineered features as well as polarity rules. The choice of featu"
W17-5535,D14-1168,1,0.85441,"rse Tree of a sample sentence. In this sentence, the Discourse Unit “There are slow and repetitive parts,” holds a “Contrast” relationship with “but it has just enough spice to keep it interesting.”. Furthermore, we can see that the former Discourse Unit is the satellite of the relation and the later part is the Nucleus. Discourse Parsing is such a critical task in NLP because previous work has shown that information Introduction This paper focuses on studying two fundamental NLP tasks, Discourse Parsing and Sentiment Analysis. The importance of these tasks and their wide applications (e.g., (Gerani et al., 2014), (Rosenthal et al., 2014)) has initiated much interest in studying both, but no method yet exists that can come close to human performance in solving them. Discourse parsing is the task of parsing a piece of text into a tree (called a Discourse Tree), the leaves of which are typically clauses (called Elementary Discourse Units or EDUs in short) and nodes (Discourse Units) represent text spans that are concatenations of their corresponding sub1 A text span is a piece of text consisting of one or more clauses (or EDUs). 289 Proceedings of the SIGDIAL 2017 Conference, pages 289–298, c Saarbr¨uck"
W17-5535,P14-1065,0,0.0536572,"Missing"
W17-5535,J15-3002,1,0.892451,"Neural Nets (RNNs) have become a more popular alternative for learning the embedding of a sentence (Kiros et al., 2015). In this setting, an encoder RNN encodes a sentence into a fixed vector representation that is then used by a decoder RNN to predict the following and preceding sentences and based on how good the predictions were, updates both the decoder and encoder RNNs. Once training is done, the encoder RNN can be used to create an embedding for any text span. In this paper, we have used the encoder Examples of effective sentence level and document level Discourse Parsers include CODRA (Joty et al., 2015) and the parser of (Feng and Hirst, 2014) . These parsers use organizational, structural, contextual, lexical and N-gram features to represent Discourse Units and apply graphical models for learning and inference (i.e. Conditional Random Fields). The performance of these parsers critically depends on a careful selection of informative and relevant features, something that is instead performed automatically in the neural models we propose in this paper. (Nakagawa et al., 2010), (Pang et al., 2008) and (Rentoumi et al., 2010), approach Sentiment Analysis using carefully engineered features as we"
W17-5535,D15-1263,0,0.677601,"ontextual polarity label to text (sentiment analysis). Analyzing the overall polarity of a sentence is a challenging task due to the ambiguities that can be introduced by combinations of words and phrases. For example in the movie review excerpt shown in Figure 2, the phrase “There are slow and repetitive parts” has a negative sentiment. However when it is combined with the positive phrase “but it has just enough spice to keep it interesting”, it results in an overall positive sentence. It has been suggested that the information extracted from Discourse Trees can help with Sentiment Analysis (Bhatia et al., 2015) and likewise, knowing the sentiment of two pieces of text might help with the identification of discourse relationships between them (Lazaridou et al., 2013). For instance, taking the sentence in Figure 1 as an example, knowing that the two text spans “There are slow and repetitive parts” and “but it has just enough spice to keep it interesting” are in a Contrast relationship to each other, also signals that the sentiment of the two text spans is less likely (i) The development of three independent recursive neural nets: two for the key sub-tasks of discourse parsing, namely structure predict"
W17-5535,P13-1160,0,0.0258494,"e introduced by combinations of words and phrases. For example in the movie review excerpt shown in Figure 2, the phrase “There are slow and repetitive parts” has a negative sentiment. However when it is combined with the positive phrase “but it has just enough spice to keep it interesting”, it results in an overall positive sentence. It has been suggested that the information extracted from Discourse Trees can help with Sentiment Analysis (Bhatia et al., 2015) and likewise, knowing the sentiment of two pieces of text might help with the identification of discourse relationships between them (Lazaridou et al., 2013). For instance, taking the sentence in Figure 1 as an example, knowing that the two text spans “There are slow and repetitive parts” and “but it has just enough spice to keep it interesting” are in a Contrast relationship to each other, also signals that the sentiment of the two text spans is less likely (i) The development of three independent recursive neural nets: two for the key sub-tasks of discourse parsing, namely structure prediction and relation prediction; the third net for sentiment prediction. (ii) The design and experimental comparison of two alternative neural joint models, Multi"
W17-5535,D14-1220,0,0.0802718,"Missing"
W17-5535,D13-1170,0,0.346615,"simply feeding a pre-computed discourse structure in a neural model for sentiment. Learning text embeddings is a fundamental step in using Neural Nets for NLP tasks. An embedding is a fixed dimensional representation of the data (text) without the use of handpicked features. As words are the building blocks of text, previous studies have created fixed dimensional vector representations for words (Mikolov et al., 2013) that capture syntactic and semantic properties of the words. However, creating meaningful fixed dimensional vector representations for text spans is an ongoing challenge. Both (Socher et al., 2013) and (Li et al., 2014) learn the embedding of a text span in a recursive manner, given a binary tree over the text span with leaves being the words. The embedding of a parent is computed from the embedding of its two children using a non-linear projection. The embedding is then used for training the task under study (Sentiment Analysis and Discourse Parsing respectively) and updated according to how useful it was for the task. Recently Recurrent Neural Nets (RNNs) have become a more popular alternative for learning the embedding of a sentence (Kiros et al., 2015). In this setting, an encoder R"
W17-5535,W10-4327,0,0.0217927,"t learns both of these tasks in a joint model, using deep learning architectures. The main contribution of this paper is to address this gap by investigating how the two tasks can benefit from each other at the sentence level within a deep learning joint model. More specific contributions include: Figure 2: The Sentiment annotation (over Discourse Tree structure) of a sentence from Sentiment Treebank dataset contained in the resulting Discourse Tree can benefit many other NLP tasks including but not restricted to automatic summarization (e.g., (Gerani et al., 2014), (Marcu and Knight, 2001), (Louis et al., 2010)), machine translation (e.g., (Meyer and Popescu-Belis, 2012),(Guzm´an et al., 2014)) and question answering (e.g., (Verberne et al., 2007)). In contrast to traditional syntactic and semantic parsing, Discourse Parsing can generate structures that cover not only a single sentence but also multi-sentential text. However, the focus of this paper is on sentence level Discourse Parsing, leaving the study of extensions to multi-sentential text as future work. The second fundamental task we consider in this work is assigning a contextual polarity label to text (sentiment analysis). Analyzing the ove"
W17-5535,J93-2004,0,0.0590721,"Missing"
W17-5535,W12-0117,0,0.0311892,"ng deep learning architectures. The main contribution of this paper is to address this gap by investigating how the two tasks can benefit from each other at the sentence level within a deep learning joint model. More specific contributions include: Figure 2: The Sentiment annotation (over Discourse Tree structure) of a sentence from Sentiment Treebank dataset contained in the resulting Discourse Tree can benefit many other NLP tasks including but not restricted to automatic summarization (e.g., (Gerani et al., 2014), (Marcu and Knight, 2001), (Louis et al., 2010)), machine translation (e.g., (Meyer and Popescu-Belis, 2012),(Guzm´an et al., 2014)) and question answering (e.g., (Verberne et al., 2007)). In contrast to traditional syntactic and semantic parsing, Discourse Parsing can generate structures that cover not only a single sentence but also multi-sentential text. However, the focus of this paper is on sentence level Discourse Parsing, leaving the study of extensions to multi-sentential text as future work. The second fundamental task we consider in this work is assigning a contextual polarity label to text (sentiment analysis). Analyzing the overall polarity of a sentence is a challenging task due to the"
W17-5535,N10-1120,0,0.0101271,"is paper, we have used the encoder Examples of effective sentence level and document level Discourse Parsers include CODRA (Joty et al., 2015) and the parser of (Feng and Hirst, 2014) . These parsers use organizational, structural, contextual, lexical and N-gram features to represent Discourse Units and apply graphical models for learning and inference (i.e. Conditional Random Fields). The performance of these parsers critically depends on a careful selection of informative and relevant features, something that is instead performed automatically in the neural models we propose in this paper. (Nakagawa et al., 2010), (Pang et al., 2008) and (Rentoumi et al., 2010), approach Sentiment Analysis using carefully engineered features as well as polarity rules. The choice of features also plays a key role in the high performance of these models. Yet, with the rapid advancements of Neural Nets, there has been increased interest in applying them to different NLP tasks. (Socher et al., 2013) approached the problem of Sentiment Analysis by recursively assigning sentiment labels to the nodes of a binarized syntactic parse tree over a sentence. At each non-leaf node, the Sentiment Neural Net first creates a distribut"
W17-5535,S14-2009,0,0.0332061,"ntence. In this sentence, the Discourse Unit “There are slow and repetitive parts,” holds a “Contrast” relationship with “but it has just enough spice to keep it interesting.”. Furthermore, we can see that the former Discourse Unit is the satellite of the relation and the later part is the Nucleus. Discourse Parsing is such a critical task in NLP because previous work has shown that information Introduction This paper focuses on studying two fundamental NLP tasks, Discourse Parsing and Sentiment Analysis. The importance of these tasks and their wide applications (e.g., (Gerani et al., 2014), (Rosenthal et al., 2014)) has initiated much interest in studying both, but no method yet exists that can come close to human performance in solving them. Discourse parsing is the task of parsing a piece of text into a tree (called a Discourse Tree), the leaves of which are typically clauses (called Elementary Discourse Units or EDUs in short) and nodes (Discourse Units) represent text spans that are concatenations of their corresponding sub1 A text span is a piece of text consisting of one or more clauses (or EDUs). 289 Proceedings of the SIGDIAL 2017 Conference, pages 289–298, c Saarbr¨ucken, Germany, 15-17 August"
W98-0210,W96-0501,0,0.0681644,"Missing"
W98-0210,W96-0406,0,0.176725,"Missing"
W98-0210,W98-1403,1,0.80227,"Missing"
W98-1403,C94-1086,0,0.0424359,"Missing"
W98-1403,W96-0501,0,0.139461,"Missing"
W98-1403,W96-0406,0,0.161923,"Missing"
W98-1403,W98-0210,1,0.802348,"Missing"
W98-1403,P83-1007,0,0.137782,"Missing"
W98-1403,J95-3003,0,0.0529492,"Missing"
W98-1403,P97-1027,0,0.0311588,"Missing"
W98-1403,P86-1029,0,0.607367,"Missing"
W98-1403,P97-1026,0,0.125619,"Missing"
