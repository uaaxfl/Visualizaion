2021.naacl-main.127,Improving Neural {RST} Parsing Model with Silver Agreement Subtrees,2021,-1,-1,5,1,3631,naoki kobayashi,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Most of the previous Rhetorical Structure Theory (RST) parsing methods are based on supervised learning such as neural networks, that require an annotated corpus of sufficient size and quality. However, the RST Discourse Treebank (RST-DT), the benchmark corpus for RST parsing in English, is small due to the costly annotation of RST trees. The lack of large annotated training data causes poor performance especially in relation labeling. Therefore, we propose a method for improving neural RST parsing models by exploiting silver data, i.e., automatically annotated data. We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser."
2021.eacl-main.214,Context-aware Neural Machine Translation with Mini-batch Embedding,2021,-1,-1,4,1,303,makoto morishita,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"It is crucial to provide an inter-sentence context in Neural Machine Translation (NMT) models for higher-quality translation. With the aim of using a simple approach to incorporate inter-sentence information, we propose mini-batch embedding (MBE) as a way to represent the features of sentences in a mini-batch. We construct a mini-batch by choosing sentences from the same document, and thus the MBE is expected to have contextual information across sentences. Here, we incorporate MBE in an NMT model, and our experiments show that the proposed method consistently outperforms the translation capabilities of strong baselines and improves writing style or terminology to fit the document{'}s context."
2021.acl-srw.34,Zero Pronouns Identification based on Span prediction,2021,-1,-1,3,0,12484,sei iwata,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"The presence of zero-pronoun (ZP) greatly affects the downstream tasks of NLP in pro-drop languages such as Japanese and Chinese. To tackle the problem, the previous works identified ZPs as sequence labeling on the word sequence or the linearlized tree nodes of the input. We propose a novel approach to ZP identification by casting it as a query-based argument span prediction task. Given a predicate as a query, our model predicts the omission with ZP. In the experiments, our model surpassed the sequence labeling baseline."
2020.wmt-1.1,Findings of the 2020 Conference on Machine Translation ({WMT}20),2020,-1,-1,17,0,8740,loic barrault,Proceedings of the Fifth Conference on Machine Translation,0,"This paper presents the results of the news translation task and the similar language translation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages."
2020.lrec-1.443,{JP}ara{C}rawl: A Large Scale Web-Based {E}nglish-{J}apanese Parallel Corpus,2020,-1,-1,3,1,303,makoto morishita,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Recent machine translation algorithms mainly rely on parallel corpora. However, since the availability of parallel corpora remains limited, only some resource-rich language pairs can benefit from them. We constructed a parallel corpus for English-Japanese, for which the amount of publicly available parallel corpora is still limited. We constructed the parallel corpus by broadly crawling the web and automatically aligning parallel sentences. Our collected corpus, called JParaCrawl, amassed over 8.7 million sentence pairs. We show how it includes a broader range of domains and how a neural machine translation model trained with it works as a good pre-trained model for fine-tuning specific domains. The pre-training and fine-tuning approaches achieved or surpassed performance comparable to model training from the initial state and reduced the training time. Additionally, we trained the model with an in-domain dataset and JParaCrawl to show how we achieved the best performance with them. JParaCrawl and the pre-trained models are freely available online for research purposes."
2020.lrec-1.457,A Test Set for Discourse Translation from {J}apanese to {E}nglish,2020,-1,-1,1,1,3634,masaaki nagata,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We made a test set for Japanese-to-English discourse translation to evaluate the power of context-aware machine translation. For each discourse phenomenon, we systematically collected examples where the translation of the second sentence depends on the first sentence. Compared with a previous study on test sets for English-to-French discourse translation (CITATION), we needed different approaches to make the data because Japanese has zero pronouns and represents different senses in different characters. We improved the translation accuracy using context-aware neural machine translation, and the improvement mainly reflects the betterment of the translation of zero pronouns."
2020.iwslt-1.17,{U}niversity of {T}sukuba{'}s Machine Translation System for {IWSLT}20 Open Domain Translation Task,2020,-1,-1,5,1,18830,hongyi cui,Proceedings of the 17th International Conference on Spoken Language Translation,0,"In this paper, we introduce University of Tsukuba{'}s submission to the IWSLT20 Open Domain Translation Task. We participate in both ChineseâJapanese and JapaneseâChinese directions. For both directions, our machine translation systems are based on the Transformer architecture. Several techniques are integrated in order to boost the performance of our models: data filtering, large-scale noised training, model ensemble, reranking and postprocessing. Consequently, our efforts achieve 33.0 BLEU scores for ChineseâJapanese translation and 32.3 BLEU scores for JapaneseâChinese translation."
2020.findings-emnlp.77,Sequential Span Classification with Neural Semi-{M}arkov {CRF}s for Biomedical Abstracts,2020,-1,-1,5,0,8399,kosuke yamada,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Dividing biomedical abstracts into several segments with rhetorical roles is essential for supporting researchers{'} information access in the biomedical domain. Conventional methods have regarded the task as a sequence labeling task based on sequential sentence classification, i.e., they assign a rhetorical label to each sentence by considering the context in the abstract. However, these methods have a critical problem: they are prone to mislabel longer continuous sentences with the same rhetorical label. To tackle the problem, we propose sequential span classification that assigns a rhetorical label, not to a single sentence but to a span that consists of continuous sentences. Accordingly, we introduce Neural Semi-Markov Conditional Random Fields to assign the labels to such spans by considering all possible spans of various lengths. Experimental results obtained from PubMed 20k RCT and NICTA-PIBOSO datasets demonstrate that our proposed method achieved the best micro sentence-F1 score as well as the best micro span-F1 score."
2020.emnlp-main.41,A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual {BERT},2020,36,0,1,1,3634,masaaki nagata,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We present a novel supervised word alignment method based on cross-language span prediction. We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence. Since this step is equivalent to a SQuAD v2.0 style question answering task, we solve it using the multilingual BERT, which is fine-tuned on manually created gold word alignment data. It is nontrivial to obtain accurate alignment from a set of independently predicted spans. We greatly improved the word alignment accuracy by adding to the question the source token{'}s context and symmetrizing two directional predictions. In experiments using five word alignment datasets from among Chinese, Japanese, German, Romanian, French, and English, we show that our proposed method significantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining. For example, we achieved 86.7 F1 score for the Chinese-English data, which is 13.3 points higher than the previous state-of-the-art supervised method."
2020.coling-main.418,{S}pan{A}lign: Sentence Alignment Method based on Cross-Language Span Prediction and {ILP},2020,-1,-1,2,0,302,katsuki chousa,Proceedings of the 28th International Conference on Computational Linguistics,0,"We propose a novel method of automatic sentence alignment from noisy parallel documents. We first formalize the sentence alignment problem as the independent predictions of spans in the target document from sentences in the source document. We then introduce a total optimization method using integer linear programming to prevent span overlapping and obtain non-monotonic alignments. We implement cross-language span prediction by fine-tuning pre-trained multilingual language models based on BERT architecture and train them using pseudo-labeled data obtained from unsupervised sentence alignment method. While the baseline methods use sentence embeddings and assume monotonic alignment, our method can capture the token-to-token interaction between the tokens of source and target text and handle non-monotonic alignments. In sentence alignment experiments on English-Japanese, our method achieved 70.3 F1 scores, which are +8.0 points higher than the baseline method. In particular, our method improved by +53.9 F1 scores for extracting non-parallel sentences. Our method improved the downstream machine translation accuracy by 4.1 BLEU scores when the extracted bilingual sentences are used for fine-tuning a pre-trained Japanese-to-English translation model."
W19-7203,A Multi-Hop Attention for {RNN} based Neural Machine Translation,2019,0,0,6,1,18832,shohei iida,Proceedings of The 8th Workshop on Patent and Scientific Literature Translation,0,None
W19-6616,Selecting Informative Context Sentence by Forced Back-Translation,2019,0,0,6,0,23531,ryuichiro kimura,Proceedings of Machine Translation Summit XVII: Research Track,0,None
W19-5365,{NTT}{'}s Machine Translation Systems for {WMT}19 Robustness Task,2019,28,0,4,0,10697,soichiro murakami,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"This paper describes NTT{'}s submission to the WMT19 robustness task. This task mainly focuses on translating noisy text (e.g., posts on Twitter), which presents different difficulties from typical translation tasks such as news. Our submission combined techniques including utilization of a synthetic corpus, domain adaptation, and a placeholder mechanism, which significantly improved over the previous baseline. Experimental results revealed the placeholder mechanism, which temporarily replaces the non-standard tokens including emojis and emoticons with special placeholder tokens during translation, improves translation accuracy even with noisy texts."
P19-2030,Attention over Heads: A Multi-Hop Attention for Neural Machine Translation,2019,0,1,6,1,18832,shohei iida,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"In this paper, we propose a multi-hop attention for the Transformer. It refines the attention for an output symbol by integrating that of each head, and consists of two hops. The first hop attention is the scaled dot-product attention which is the same attention mechanism used in the original Transformer. The second hop attention is a combination of multi-layer perceptron (MLP) attention and head gate, which efficiently increases the complexity of the model by adding dependencies between heads. We demonstrate that the translation accuracy of the proposed multi-hop attention outperforms the baseline Transformer significantly, +0.85 BLEU point for the IWSLT-2017 German-to-English task and +2.58 BLEU point for the WMT-2017 German-to-English task. We also find that the number of parameters required for a multi-hop attention is smaller than that for stacking another self-attention layer and the proposed model converges significantly faster than the original Transformer."
P19-2056,Using Semantic Similarity as Reward for Reinforcement Learning in Sentence Generation,2019,0,0,3,0,25535,go yasui,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Traditional model training for sentence generation employs cross-entropy loss as the loss function. While cross-entropy loss has convenient properties for supervised learning, it is unable to evaluate sentences as a whole, and lacks flexibility. We present the approach of training the generation model using the estimated semantic similarity between the output and reference sentences to alleviate the problems faced by the training with cross-entropy loss. We use the BERT-based scorer fine-tuned to the Semantic Textual Similarity (STS) task for semantic similarity estimation, and train the model with the estimated scores through reinforcement learning (RL). Our experiments show that reinforcement learning with semantic similarity reward improves the BLEU scores from the baseline LSTM NMT model."
P19-1225,Answering while Summarizing: Multi-task Learning for Multi-hop {QA} with Evidence Extraction,2019,42,0,3,0,8424,kosuke nishida,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Question answering (QA) using textual sources for purposes such as reading comprehension (RC) has attracted much attention. This study focuses on the task of explainable multi-hop QA, which requires the system to return the answer with evidence sentences by reasoning and gathering disjoint pieces of the reference texts. It proposes the Query Focused Extractor (QFE) model for evidence extraction and uses multi-task learning with the QA model. QFE is inspired by extractive summarization models; compared with the existing method, which extracts each evidence sentence independently, it sequentially extracts evidence sentences by using an RNN with an attention mechanism on the question sentence. It enables QFE to consider the dependency among the evidence sentences and cover important information in the question sentence. Experimental results show that QFE with a simple RC baseline model achieves a state-of-the-art evidence extraction score on HotpotQA. Although designed for RC, it also achieves a state-of-the-art evidence extraction score on FEVER, which is a recognizing textual entailment task on a large textual database."
D19-6505,Context-aware Neural Machine Translation with Coreference Information,2019,0,0,3,0,26410,takumi ohtani,Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019),0,"We present neural machine translation models for translating a sentence in a text by using a graph-based encoder which can consider coreference relations provided within the text explicitly. The graph-based encoder can dynamically encode the source text without attending to all tokens in the text. In experiments, our proposed models provide statistically significant improvement to the previous approach of at most 0.9 points in the BLEU score on the OpenSubtitle2018 English-to-Japanese data set. Experimental results also show that the graph-based encoder can handle a longer text well, compared with the previous approach."
D19-5622,Mixed Multi-Head Self-Attention for Neural Machine Translation,2019,0,2,5,1,18830,hongyi cui,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"Recently, the Transformer becomes a state-of-the-art architecture in the filed of neural machine translation (NMT). A key point of its high-performance is the multi-head self-attention which is supposed to allow the model to independently attend to information from different representation subspaces. However, there is no explicit mechanism to ensure that different attention heads indeed capture different features, and in practice, redundancy has occurred in multiple heads. In this paper, we argue that using the same global attention in multiple heads limits multi-head self-attention{'}s capacity for learning distinct features. In order to improve the expressiveness of multi-head self-attention, we propose a novel Mixed Multi-Head Self-Attention (MMA) which models not only global and local attention but also forward and backward attention in different attention heads. This enables the model to learn distinct representations explicitly among multiple heads. In our experiments on both WAT17 English-Japanese as well as IWSLT14 German-English translation task, we show that, without increasing the number of parameters, our models yield consistent and significant improvements (0.9 BLEU scores on average) over the strong Transformer baseline."
D19-5211,{NTT} Neural Machine Translation Systems at {WAT} 2019,2019,0,0,3,1,303,makoto morishita,Proceedings of the 6th Workshop on Asian Translation,0,"In this paper, we describe our systems that were submitted to the translation shared tasks at WAT 2019. This year, we participated in two distinct types of subtasks, a scientific paper subtask and a timely disclosure subtask, where we only considered English-to-Japanese and Japanese-to-English translation directions. We submitted two systems (En-Ja and Ja-En) for the scientific paper subtask and two systems (Ja-En, texts, items) for the timely disclosure subtask. Three of our four systems obtained the best human evaluation performances. We also confirmed that our new additional web-crawled parallel corpus improves the performance in unconstrained settings."
D19-1587,Split or Merge: Which is Better for Unsupervised {RST} Parsing?,2019,0,0,6,1,3631,naoki kobayashi,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Rhetorical Structure Theory (RST) parsing is crucial for many downstream NLP tasks that require a discourse structure for a text. Most of the previous RST parsers have been based on supervised learning approaches. That is, they require an annotated corpus of sufficient size and quality, and heavily rely on the language and domain dependent corpus. In this paper, we present two language-independent unsupervised RST parsing methods based on dynamic programming. The first one builds the optimal tree in terms of a dissimilarity score function that is defined for splitting a text span into smaller ones. The second builds the optimal tree in terms of a similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F$_1$ score, which is close to the scores of the previous supervised parsers."
D19-1674,Generating Natural Anagrams: Towards Language Generation Under Hard Combinatorial Constraints,2019,0,0,4,1,20106,masaaki nishino,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"An anagram is a sentence or a phrase that is made by permutating the characters of an input sentence or a phrase. For example, {``}Trims cash{''} is an anagram of {``}Christmas{''}. Existing automatic anagram generation methods can find possible combinations of words form an anagram. However, they do not pay much attention to the naturalness of the generated anagrams. In this paper, we show that simple depth-first search can yield natural anagrams when it is combined with modern neural language models. Human evaluation results show that the proposed method can generate significantly more natural anagrams than baseline methods."
Y18-1034,Reducing Odd Generation from Neural Headline Generation,2018,0,1,6,0,4615,shun kiyono,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
W18-6421,{NTT}{'}s Neural Machine Translation Systems for {WMT} 2018,2018,0,1,3,1,303,makoto morishita,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper describes NTT{'}s neural machine translation systems submitted to the WMT 2018 English-German and German-English news translation tasks. Our submission has three main components: the Transformer model, corpus cleaning, and right-to-left n-best re-ranking techniques. Through our experiments, we identified two keys for improving accuracy: filtering noisy training sentences and right-to-left re-ranking. We also found that the Transformer model requires more training data than the RNN-based model, and the RNN-based model sometimes achieves better accuracy than the Transformer model when the corpus is small."
W18-5410,Unsupervised Token-wise Alignment to Improve Interpretation of Encoder-Decoder Models,2018,0,1,6,0,4615,shun kiyono,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},0,"Developing a method for understanding the inner workings of black-box neural methods is an important research endeavor. Conventionally, many studies have used an attention matrix to interpret how Encoder-Decoder-based models translate a given source sentence to the corresponding target sentence. However, recent studies have empirically revealed that an attention matrix is not optimal for token-wise translation analyses. We propose a method that explicitly models the token-wise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism."
P18-2097,An Empirical Study of Building a Strong Baseline for Constituency Parsing,2018,0,2,5,0,9188,jun suzuki,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper investigates the construction of a strong baseline based on general purpose sequence-to-sequence models for constituency parsing. We incorporate several techniques that were mainly developed in natural language generation tasks, e.g., machine translation and summarization, and demonstrate that the sequence-to-sequence model achieves the current top-notch parsers{'} performance (almost) without requiring any explicit task-specific knowledge or architecture of constituent parsing."
N18-2104,Pruning Basic Elements for Better Automatic Evaluation of Summaries,2018,0,0,3,0,10998,ukyo honda,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"We propose a simple but highly effective automatic evaluation measure of summarization, pruned Basic Elements (pBE). Although the BE concept is widely used for the automated evaluation of summaries, its weakness is that it redundantly matches basic elements. To avoid this redundancy, pBE prunes basic elements by (1) disregarding frequency count of basic elements and (2) reducing semantically overlapped basic elements based on word similarity. Even though it is simple, pBE outperforms ROUGE in DUC datasets in most cases and achieves the highest rank correlation coefficient in TAC 2011 AESOP task."
N18-1047,Neural Tensor Networks with Diagonal Slice Matrices,2018,0,1,5,0,27521,takahiro ishihara,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Although neural tensor networks (NTNs) have been successful in many NLP tasks, they require a large number of parameters to be estimated, which often leads to overfitting and a long training time. We address these issues by applying eigendecomposition to each slice matrix of a tensor to reduce its number of paramters. First, we evaluate our proposed NTN models on knowledge graph completion. Second, we extend the models to recursive NTNs (RNTNs) and evaluate them on logical reasoning tasks. These experiments show that our proposed models learn better and faster than the original (R)NTNs."
N18-1155,Higher-Order Syntactic Attention Network for Longer Sentence Compression,2018,0,2,4,1,3633,hidetaka kamigaito,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"A sentence compression method using LSTM can generate fluent compressed sentences. However, the performance of this method is significantly degraded when compressing longer sentences since it does not explicitly handle syntactic features. To solve this problem, we propose a higher-order syntactic attention network (HiSAN) that can handle higher-order dependency features as an attention distribution on LSTM hidden states. Furthermore, to avoid the influence of incorrect parse results, we trained HiSAN by maximizing jointly the probability of a correct output with the attention distribution. Experimental results on Google sentence compression dataset showed that our method achieved the best performance on F1 as well as ROUGE-1,2 and L scores, 83.2, 82.9, 75.8 and 82.7, respectively. In human evaluation, our methods also outperformed baseline methods in both readability and informativeness."
N18-1157,Provable Fast Greedy Compressive Summarization with Any Monotone Submodular Function,2018,0,1,4,0,29468,shinsaku sakaue,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Submodular maximization with the greedy algorithm has been studied as an effective approach to extractive summarization. This approach is known to have three advantages: its applicability to many useful submodular objective functions, the efficiency of the greedy algorithm, and the provable performance guarantee. However, when it comes to compressive summarization, we are currently missing a counterpart of the extractive method based on submodularity. In this paper, we propose a fast greedy method for compressive summarization. Our method is applicable to any monotone submodular objective function, including many functions well-suited for document summarization. We provide an approximation guarantee of our greedy algorithm. Experiments show that our method is about 100 to 400 times faster than an existing method based on integer-linear-programming (ILP) formulations and that our method empirically achieves more than 95{\%}-approximation."
D18-1450,Automatic Pyramid Evaluation Exploiting {EDU}-based Extractive Reference Summaries,2018,0,3,3,0,3632,tsutomu hirao,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"This paper tackles automation of the pyramid method, a reliable manual evaluation framework. To construct a pyramid, we transform human-made reference summaries into extractive reference summaries that consist of Elementary Discourse Units (EDUs) obtained from source documents and then weight every EDU by counting the number of extractive reference summaries that contain the EDU. A summary is scored by the correspondences between EDUs in the summary and those in the pyramid. Experiments on DUC and TAC data sets show that our methods strongly correlate with various manual evaluations."
D18-1489,Direct Output Connection for a High-Rank Language Model,2018,0,14,3,1,4614,sho takase,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also middle layers. This method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). Our proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to application tasks: machine translation and headline generation."
C18-1052,Improving Neural Machine Translation by Incorporating Hierarchical Subword Features,2018,0,5,3,1,303,makoto morishita,Proceedings of the 27th International Conference on Computational Linguistics,0,"This paper focuses on subword-based Neural Machine Translation (NMT). We hypothesize that in the NMT model, the appropriate subword units for the following three modules (layers) can differ: (1) the encoder embedding layer, (2) the decoder embedding layer, and (3) the decoder output layer. We find the subword based on Sennrich et al. (2016) has a feature that a large vocabulary is a superset of a small vocabulary and modify the NMT model enables the incorporation of several different subword units in a single embedding layer. We refer these small subword features as hierarchical subword features. To empirically investigate our assumption, we compare the performance of several different subword units and hierarchical subword features for both the encoder and decoder embedding layers. We confirmed that incorporating hierarchical subword features in the encoder consistently improves BLEU scores on the IWSLT evaluation datasets."
W17-6308,Hierarchical Word Structure-based Parsing: A Feasibility Study on {UD}-style Dependency Parsing in {J}apanese,2017,0,0,3,0,29830,takaaki tanaka,Proceedings of the 15th International Conference on Parsing Technologies,0,"In applying word-based dependency parsing such as Universal Dependencies (UD) to Japanese, the uncertainty of word segmentation emerges for defining a word unit of the dependencies. We introduce the following hierarchical word structures to dependency parsing in Japanese: morphological units (a short unit word, SUW) and syntactic units (a long unit word, LUW). An SUW can be used to segment a sentence consistently, while it is too short to represent syntactic construction. An LUW is a unit including functional multiwords and LUW-based analysis facilitates the capturing of syntactic structure and makes parsing results more precise than SUW-based analysis. This paper describes the results of a feasibility study on the ability and the effectiveness of parsing methods based on hierarchical word structure (LUW chunking+parsing) in comparison to single layer word structure (SUW parsing). We also show joint analysis of LUW-chunking and dependency parsing improves the performance of identifying predicate-argument structures, while there is not much difference between overall results of them. not much difference between overall results of them."
W17-5702,Controlling Target Features in Neural Machine Translation via Prefix Constraints,2017,0,3,2,1,31471,shunsuke takeno,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"We propose \textit{prefix constraints}, a novel method to enforce constraints on target sentences in neural machine translation. It places a sequence of special tokens at the beginning of target sentence (target prefix), while side constraints places a special token at the end of source sentence (source suffix). Prefix constraints can be predicted from source sentence jointly with target sentence, while side constraints (Sennrich et al., 2016) must be provided by the user or predicted by some other methods. In both methods, special tokens are designed to encode arbitrary features on target-side or metatextual information. We show that prefix constraints are more flexible than side constraints and can be used to control the behavior of neural machine translation, in terms of output length, bidirectional decoding, domain adaptation, and unaligned target word generation."
W17-5706,{NTT} Neural Machine Translation Systems at {WAT} 2017,2017,4,5,3,1,303,makoto morishita,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"In this year, we participated in four translation subtasks at WAT 2017. Our model structure is quite simple but we used it with well-tuned hyper-parameters, leading to a significant improvement compared to the previous state-of-the-art system. We also tried to make use of the unreliable part of the provided parallel corpus by back-translating and making a synthetic corpus. Our submitted system achieved the new state-of-the-art performance in terms of the BLEU score, as well as human evaluation."
P17-2043,Oracle Summaries of Compressive Summarization,2017,12,1,3,0,3632,tsutomu hirao,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,This paper derives an Integer Linear Programming (ILP) formulation to obtain an oracle summary of the compressive summarization paradigm in terms of ROUGE. The oracle summary is essential to reveal the upper bound performance of the paradigm. Experimental results on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries.
I17-2002,Supervised Attention for Sequence-to-Sequence Constituency Parsing,2017,14,3,6,1,3633,hidetaka kamigaito,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"The sequence-to-sequence (Seq2Seq) model has been successfully applied to machine translation (MT). Recently, MT performances were improved by incorporating supervised attention into the model. In this paper, we introduce supervised attention to constituency parsing that can be regarded as another translation task. Evaluation results on the PTB corpus showed that the bracketing F-measure was improved by supervised attention."
I17-2008,Input-to-Output Gate to Improve {RNN} Language Models,2017,17,2,3,1,4614,sho takase,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This paper proposes a reinforcing method that refines the output layers of existing Recurrent Neural Network (RNN) language models. We refer to our proposed method as Input-to-Output Gate (IOG). IOG has an extremely simple structure, and thus, can be easily combined with any RNN language models. Our experiments on the Penn Treebank and WikiText-2 datasets demonstrate that IOG consistently boosts the performance of several different types of current topline RNN language models."
E17-2047,Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization,2017,11,18,2,0,9188,jun suzuki,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark.
E17-2049,K-best Iterative {V}iterbi Parsing,2017,10,0,2,0.812616,12296,katsuhiko hayashi,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"This paper presents an efficient and optimal parsing algorithm for probabilistic context-free grammars (PCFGs). To achieve faster parsing, our proposal employs a pruning technique to reduce unnecessary edges in the search space. The key is to conduct repetitively Viterbi inside and outside parsing, while gradually expanding the search space to efficiently compute heuristic bounds used for pruning. Our experimental results using the English Penn Treebank corpus show that the proposed algorithm is faster than the standard CKY parsing algorithm. In addition, we also show how to extend this algorithm to extract k-best Viterbi parse trees."
E17-1037,Enumeration of Extractive Oracle Summaries,2017,16,3,4,0,3632,tsutomu hirao,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"To analyze the limitations and the future directions of the extractive summarization paradigm, this paper proposes an Integer Linear Programming (ILP) formulation to obtain extractive oracle summaries in terms of ROUGE-N. We also propose an algorithm that enumerates all of the oracle summaries for a set of reference summaries to exploit F-measures that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries."
W16-4615,Integrating empty category detection into preordering Machine Translation,2016,15,2,2,1,31471,shunsuke takeno,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"We propose a method for integrating Japanese empty category detection into the preordering process of Japanese-to-English statistical machine translation. First, we apply machine-learning-based empty category detection to estimate the position and the type of empty categories in the constituent tree of the source sentence. Then, we apply discriminative preordering to the augmented constituent tree in which empty categories are treated as if they are normal lexical symbols. We find that it is effective to filter empty categories based on the confidence of estimation. Our experiments show that, for the IWSLT dataset consisting of short travel conversations, the insertion of empty categories alone improves the BLEU score from 33.2 to 34.3 and the RIBES score from 76.3 to 78.7, which imply that reordering has improved For the KFTT dataset consisting of Wikipedia sentences, the proposed preordering method considering empty categories improves the BLEU score from 19.9 to 20.2 and the RIBES score from 66.2 to 66.3, which shows both translation and reordering have improved slightly."
W16-4621,{C}hinese-to-{J}apanese Patent Machine Translation based on Syntactic Pre-ordering for {WAT} 2016,2016,-1,-1,2,0.910035,1440,katsuhito sudoh,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"This paper presents our Chinese-to-Japanese patent machine translation system for WAT 2016 (Group ID: ntt) that uses syntactic pre-ordering over Chinese dependency structures. Chinese words are reordered by a learning-to-rank model based on pairwise classification to obtain word order close to Japanese. In this year{'}s system, two different machine translation methods are compared: traditional phrase-based statistical machine translation and recent sequence-to-sequence neural machine translation with an attention mechanism. Our pre-ordering showed a significant improvement over the phrase-based baseline, but, in contrast, it degraded the neural machine translation baseline."
W16-3616,Empirical comparison of dependency conversions for {RST} discourse trees,2016,23,6,3,1,12296,katsuhiko hayashi,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,None
P16-2016,Empty element recovery by spinal parser operations,2016,12,4,2,1,12296,katsuhiko hayashi,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,This paper presents a spinal parsing algorithm that can jointly detect empty elements. This method achieves state-of-theart performance on English and Japanese empty element recovery problems.
P16-2066,Phrase Table Pruning via Submodular Function Maximization,2016,18,0,3,1,20106,masaaki nishino,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Phrase table pruning is the act of removing phrase pairs from a phrase table to make it smaller, ideally removing the least useful phrases first. We propose a phrase table pruning method that formulates the task as a submodular function maximization problem, and solves it by using a greedy heuristic algorithm. The proposed method can scale with input size and long phrases, and experiments show that it achieves higher BLEU scores than state-of-the-art pruning methods."
N16-1135,Right-truncatable Neural Word Embeddings,2016,19,0,2,0,9188,jun suzuki,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
D16-1112,Neural Headline Generation on {A}bstract {M}eaning {R}epresentation,2016,13,60,5,1,4614,sho takase,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1021,Exploring Text Links for Coherent Multi-Document Summarization,2016,34,11,5,1,35692,xun wang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Summarization aims to represent source documents by a shortened passage. Existing methods focus on the extraction of key information, but often neglect coherence. Hence the generated summaries suffer from a lack of readability. To address this problem, we have developed a graph-based method by exploring the links between text to produce coherent summaries. Our approach involves finding a sequence of sentences that best represent the key information in a coherent way. In contrast to the previous methods that focus only on salience, the proposed method addresses both coherence and informativeness based on textual linkages. We conduct experiments on the DUC2004 summarization task data set. A performance comparison reveals that the summaries generated by the proposed system achieve comparable results in terms of the ROUGE metric, and show improvements in readability by human evaluation."
W15-5012,{C}hinese-to-{J}apanese Patent Machine Translation based on Syntactic Pre-ordering for{WAT} 2015,2015,5,3,2,0.983904,1440,katsuhito sudoh,Proceedings of the 2nd Workshop on {A}sian Translation ({WAT}2015),0,None
P15-2023,Discriminative Preordering Meets Kendall{'}s $\\tau$ Maximization,2015,23,7,5,1,37418,sho hoshino,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This paper explores a simple discriminative preordering model for statistical machine translation. Our model traverses binary constituent trees, and classifies whether children of each node should be reordered. The model itself is not extremely novel, but herein we introduce a new procedure to determine oracle labels so as to maximize Kendallxe2x80x99s xcfx84 . Experiments in Japanese-to-English translation revealed that our simple method is comparable with, or superior to, state-of-the-art methods in translation accuracy."
P15-2031,A Unified Learning Framework of Skip-Grams and Global Vectors,2015,15,6,2,0,9188,jun suzuki,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Log-bilinear language models such as SkipGram and GloVe have been proven to capture high quality syntactic and semantic relationships between words in a vector space. We revisit the relationship between SkipGram and GloVe models from a machine learning viewpoint, and show that these two methods are easily merged into a unified form. Then, by using the unified form, we extract the factors of the configurations that they use differently. We also empirically investigate which factor is responsible for the performance difference often observed in widely examined word similarity and analogy tasks."
P15-2039,Word-based {J}apanese typed dependency parsing with grammatical function analysis,2015,15,1,2,0.49825,29830,takaaki tanaka,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We present a novel scheme for wordbased Japanese typed dependency parser which integrates syntactic structure analysis and grammatical function analysis such as predicate-argument structure analysis. Compared to bunsetsu-based dependency parsing, which is predominantly used in Japanese NLP, it provides a natural way of extracting syntactic constituents, which is useful for downstream applications such as statistical machine translation. It also makes it possible to jointly decide dependency and predicate-argument structure, which is usually implemented as two separate steps. We convert an existing treebank to the new dependency scheme and report parsing results as a baseline for future research. We achieved a better accuracy for assigning function labels than a predicate-argument structure analyzer by using grammatical functions as dependency label."
N15-1030,Empty Category Detection With Joint Context-Label Embeddings,2015,21,2,3,1,35692,xun wang,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This paper presents a novel technique for empty category (EC) detection using distributed word representations. A joint model is learned from the labeled data to map both the distributed representations of the contexts of ECs and EC types to a low dimensional space. In the testing phase, the context of possible EC positions will be projected into the same space for empty category detection. Experiments on Chinese Treebank prove the effectiveness of the proposed method. We improve the precision by about 6 points on a subset of Chinese Treebank, which is a new state-ofthe-art performance on CTB."
N15-1049,A Dynamic Programming Algorithm for Tree Trimming-based Text Summarization,2015,24,3,5,1,20106,masaaki nishino,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Tree trimming is the problem of extracting an optimal subtree from an input tree, and sentence extraction and sentence compression methods can be formulated and solved as tree trimming problems. Previous approaches require integer linear programming (ILP) solvers to obtain exact solutions. The problem of this approach is that ILP solvers are black-boxes and have no theoretical guarantee as to their computation complexity. We propose a dynamic programming (DP) algorithm for tree trimming problems whose running time is O(NLlogN), where N is the number of tree nodes andL is the length limit. Our algorithm exploits the zero-suppressed binary decision diagram (ZDD), a data structure that represents a family of sets as a directed acyclic graph, to represent the set of subtrees in a compact form; the structure of ZDD permits the application of DP to obtain exact solutions, and our algorithm is applicable to different tree trimming problems. Moreover, experiments show that our algorithm is faster than state-of-the-art ILP solvers, and that it scales well to handle large summarization problems."
K15-2015,Hybrid Approach to {PDTB}-styled Discourse Parsing for {C}o{NLL}-2015,2015,6,3,4,1,37720,yasuhisa yoshida,Proceedings of the Nineteenth Conference on Computational Natural Language Learning - Shared Task,0,"This paper describes our end-to-end PDTB-styled discourse parser for the CoNLL-2015 shared task. We employed a machine learning-based approach to identify discourse relation between text spans for both explicit and implicit relations and employed a rule-based approach to extract arguments of the discourse relations. In particular, we focus on improving the implicit discourse relation identification. First, we extract adjacent pairs of sentences that have some discourse relationships by exploiting a two-class classifier from an entire document. Second, we assign sense labels for them by utilizing a multiple-class classifier. Our system achieved a 0.316 overall F-score for the development set, 0.249 for the testset and 0.157 for the blind testset."
D15-1156,Empty Category Detection using Path Features and Distributed Case Frames,2015,14,5,2,1,31471,shunsuke takeno,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We describe an approach for machine learning-based empty category detection that is based on the phrase structure analysis of Japanese. The problem is formalized as tree node classification, and we find that the path feature, the sequence of node labels from the current node to the root, is highly effective. We also find that the set of dot products between the word embeddings for a verb and those for case particles can be used as a substitution for case frames. Experiments show that the proposed method outperforms the previous state-of the art method by 68.6% to 73.2% in terms of F-measure."
P14-2052,Single Document Summarization based on Nested Tree Structure,2014,18,26,5,0,27810,yuta kikuchi,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Many methods of text summarization combining sentence selection and sentence compression have recently been proposed. Although the dependency between words has been used in most of these methods, the dependency between sentences, i.e., rhetorical structures, has not been exploited in such joint methods. We used both dependency between words and dependency between sentences by constructing a nested tree, in which nodes in the document tree representing dependency between sentences were replaced by a sentence tree representing dependency between words. We formulated a summarization task as a combinatorial optimization problem, in which the nested tree was trimmed without losing important content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts."
D14-1196,Dependency-based Discourse Parser for Single-Document Summarization,2014,12,39,4,1,37720,yasuhisa yoshida,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"The current state-of-the-art singledocument summarization method generates a summary by solving a Tree Knapsack Problem (TKP), which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree (DEP-DT) of a document. We can obtain a gold DEP-DT by transforming a gold Rhetorical Structure Theory-based discourse tree (RST-DT). However, there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT. To improve the ROUGE score, we propose a novel discourse parser that directly generates the DEP-DT. The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser, and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT."
2014.iwslt-papers.16,The {NAIST}-{NTT} {TED} talk treebank,2014,-1,-1,6,0,834,graham neubig,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"Syntactic parsing is a fundamental natural language processing technology that has proven useful in machine translation, language modeling, sentence segmentation, and a number of other applications related to speech translation. However, there is a paucity of manually annotated syntactic parsing resources for speech, and particularly for the lecture speech that is the current target of the IWSLT translation campaign. In this work, we present a new manually annotated treebank of TED talks that we hope will prove useful for investigation into the interaction between syntax and these speechrelated applications. The first version of the corpus includes 1,217 sentences and 23,158 words manually annotated with parse trees, and aligned with translations in 26-43 different languages. In this paper we describe the collection of the corpus, and an analysis of its various characteristics."
2014.amta-researchers.18,{J}apanese-to-{E}nglish patent translation system based on domain-adapted word segmentation and post-ordering,2014,35,0,2,1,1440,katsuhito sudoh,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"This paper presents a Japanese-to-English statistical machine translation system specialized for patent translation. Patents are practically useful technical documents, but their translation needs different efforts from general-purpose translation. There are two important problems in the Japanese-to-English patent translation: long distance reordering and lexical translation of many domain-specific terms. We integrated novel lexical translation of domain-specific terms with a syntax-based post-ordering framework that divides the machine translation problem into lexical translation and reordering explicitly for efficient syntax-based translation. The proposed lexical translation consists of a domain-adapted word segmentation and an unknown word transliteration. Experimental results show our system achieves better translation accuracy in BLEU and TER compared to the baseline methods."
Y13-1026,Effects of Parsing Errors on Pre-Reordering Performance for {C}hinese-to-{J}apanese {SMT},2013,19,3,5,1,33211,dan han,"Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation ({PACLIC} 27)",0,"Linguistically motivated reordering methods have been developed to improve word alignment especially for Statistical Machine Translation (SMT) on long distance language pairs. However, since they highly rely on the parsing accuracy, it is useful to explore the relationship between parsing and reordering. For Chinese-toJapanese SMT, we carry out a three-stage incremental comparative analysis to observe the effects of different parsing errors on reordering performance by combining empirical and descriptive approaches. For the empirical approach, we quantify the distribution of general parsing errors along with reordering qualities whereas for the descriptive approach, we extract seven influential error patterns and examine their correlation with reordering errors."
W13-4913,Constructing a Practical Constituent Parser from a {J}apanese Treebank with Function Labels,2013,19,4,2,0.49825,29830,takaaki tanaka,Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"We present an empirical study on constructing a Japanese constituent parser, which can output function labels to deal with more detailed syntactic information. Japanese syntactic parse trees are usually represented as unlabeled dependency structure between bunsetsu chunks, however, such expression is insufficient to uncover the syntactic information about distinction between complements and adjuncts and coordination structure, which is required for practical applications such as syntactic reordering of machine translation. We describe a preliminary effort on constructing a Japanese constituent parser by a Penn Treebank style treebank semi-automatically made from a dependency-based corpus. The evaluations show the parser trained on the treebank has comparable bracketing accuracy as conventional bunsetsu-based parsers, and can output such function labels as the grammatical role of the argument and the type of adnominal phrases."
W13-2806,Using unlabeled dependency parsing for pre-reordering for {C}hinese-to-{J}apanese statistical machine translation,2013,24,6,5,1,33211,dan han,Proceedings of the Second Workshop on Hybrid Approaches to Translation,0,"Chinese and Japanese have a different sentence structure. Reordering methods are effective, but need reliable parsers to extract the syntactic structure of the source sentences. However, Chinese has a loose word order, and Chinese parsers that extract the phrase structure do not perform well. We propose a framework where only POS tags and unlabeled dependency parse trees are necessary, and linguistic knowledge on structural difference can be encoded in the form of reordering rules. We show significant improvements in translation quality of sentences from news domain, when compared to state-of-the-art reordering methods."
P13-2004,Supervised Model Learning with Feature Grouping based on a Discrete Constraint,2013,23,5,2,0.776028,9188,jun suzuki,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper proposes a framework of supervised model learning that realizes feature grouping to obtain lower complexity models. The main idea of our method is to integrate a discrete constraint into model learning with the help of the dual decomposition technique. Experiments on two well-studied NLP tasks, dependency parsing and NER, demonstrate that our method can provide state-of-the-art performance even if the degrees of freedom in trained models are surprisingly small, i.e., 8 or even 2. This significant benefit enables us to provide compact model representation, which is especially useful in actual use."
P13-2038,Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information,2013,13,2,3,0.882415,3632,tsutomu hirao,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods."
I13-1147,Two-Stage Pre-ordering for {J}apanese-to-{E}nglish Statistical Machine Translation,2013,15,12,4,1,37418,sho hoshino,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,We propose a new rule-based pre-ordering method for Japanese-to-English statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES.
D13-1021,Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora,2013,8,8,3,1,1440,katsuhito sudoh,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,This paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs. The model is an extension of a Bayesian many-to-many alignment method for distinguishing nontransliteration (noise) parts in phrase pairs. It worked effectively in the experiments of bootstrapping Japanese-to-English statistical machine transliteration in patent domain using patent bilingual corpora.
D13-1139,Shift-Reduce Word Reordering for Machine Translation,2013,15,4,5,0.983989,12296,katsuhiko hayashi,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of 3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system.
D13-1158,Single-Document Summarization as a Tree Knapsack Problem,2013,17,61,5,0.882415,3632,tsutomu hirao,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Recent studies on extractive text summarization formulate it as a combinatorial optimization problem such as a Knapsack Problem, a Maximum Coverage Problem or a Budgeted Median Problem. These methods successfully improved summarization quality, but they did not consider the rhetorical relations between the textual units of a source document. Thus, summaries generated by these methods may lack logical coherence. This paper proposes a single document summarization method based on the trimming of a discourse tree. This is a two-fold process. First, we propose rules for transforming a rhetorical structure theorybased discourse tree into a dependency-based discourse tree, which allows us to take a treetrimming approach to summarization. Second, we formulate the problem of trimming a dependency-based discourse tree as a Tree Knapsack Problem, then solve it with integer linear programming (ILP). Evaluation results showed that our method improved ROUGE scores."
W12-4207,Head Finalization Reordering for {C}hinese-to-{J}apanese Machine Translation,2012,31,10,6,1,33211,dan han,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"In Statistical Machine Translation, reordering rules have proved useful in extracting bilingual phrases and in decoding during translation between languages that are structurally different. Linguistically motivated rules have been incorporated into Chinese-to-English (Wang et al., 2007) and English-to-Japanese (Isozaki et al., 2010b) translation with significant gains to the statistical translation system. Here, we carry out a linguistic analysis of the Chinese-to-Japanese translation problem and propose one of the first reordering rules for this language pair. Experimental results show substantially improvements (from 20.70 to 23.17 BLEU) when head-finalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more refined rules."
W12-4213,Zero Pronoun Resolution can Improve the Quality of {J}-{E} Translation,2012,12,12,3,1,2121,hirotoshi taira,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"In Japanese, particularly, spoken Japanese, subjective, objective and possessive cases are very often omitted. Such Japanese sentences are often translated by Japanese-English statistical machine translation to the English sentence whose subjective, objective and possessive cases are omitted, and it causes to decrease the quality of translation. We performed experiments of J-E phrase based translation using Japanese sentence, whose omitted pronouns are complemented by human. We introduced 'antecedent F-measure' as a score for measuring quality of the translated English. As a result, we found that it improves the scores of antecedent F-measure while the BLEU scores were almost unchanged. Every effectiveness of the zero pronoun resolution differs depending on the type and case of each zero pronoun."
P12-2020,A Comparative Study of Target Dependency Structures for Statistical Machine Translation,2012,21,2,5,1,6319,xianchao wu,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these non-isomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. The experiments on Chinese-to-English translation show that the HPSG parser's PASs achieved the best dependency and translation accuracies."
P12-1001,Learning to Translate with Multiple Objectives,2012,37,11,5,0.850756,5136,kevin duh,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously. Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality.n n Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization."
P12-1046,{B}ayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing,2012,31,29,4,1,7901,hiroyuki shindo,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. We aim to provide a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion. We present a novel probabilistic SR-TSG model based on the hierarchical Pitman-Yor Process to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers."
C12-2084,The Effect of Learner Corpus Size in Grammatical Error Correction of {ESL} Writings,2012,21,27,4,1,22506,tomoya mizumoto,Proceedings of {COLING} 2012: Posters,0,"English as a Second Language (ESL) learnersxe2x80x99 writings contain various grammatical errors. Previous research on automatic error correction for ESL learnersxe2x80x99 grammatical errors deals with restricted types of learnersxe2x80x99 errors. Some types of errors can be corrected by rules using heuristics, while others are difficult to correct without statistical models using native corpora and/or learner corpora. Since adding error annotation to learnersxe2x80x99 text is time-consuming, it was not until recently that large scale learner corpora became publicly available. However, little is known about the effect of learner corpus size in ESL grammatical error correction. Thus, in this paper, we investigate the effect of learner corpus size on various types of grammatical errors, using an error correction system based on phrase-based statistical machine translation (SMT) trained on a large scale errortagged learner corpus. We show that the phrase-based SMT approach is effective in correcting frequent errors that can be identified by local context, and that it is difficult for phrase-based SMT to correct errors that need long range contextual information."
W11-3506,Error Correcting Romaji-kana Conversion for {J}apanese Language Education,2011,8,3,3,0,44093,seiji kasahara,Proceedings of the Workshop on Advances in Text Input Methods ({WTIM} 2011),0,We present an approach to help editors of Japanese on a language learning SNS correct learnersxe2x80x99 sentences written in Roman characters by converting them into kana. Our system detects foreign words and converts only Japanese words even if they contain spelling errors. Experimental results show that our system achieves about 10 points higher conversion accuracy than traditional input method (IM). Error analysis reveals some tendencies of the errors specific to language learners.
P11-2036,Insertion Operator for {B}ayesian Tree Substitution Grammars,2011,15,4,3,1,7901,hiroyuki shindo,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a model that incorporates an insertion operator in Bayesian tree substitution grammars (BTSG). Tree insertion is helpful for modeling syntax patterns accurately with fewer grammar rules than BTSG. The experimental parsing results show that our model outperforms a standard PCFG and BTSG for a small dataset. For a large dataset, our model obtains comparable results to BTSG, making the number of grammar rules much smaller than with BTSG."
P11-2075,Is Machine Translation Ripe for Cross-Lingual Sentiment Classification?,2011,11,19,3,0.850756,5136,kevin duh,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"Recent advances in Machine Translation (MT) have brought forth a new paradigm for building NLP applications in low-resource scenarios. To build a sentiment classifier for a language with no labeled resources, one can translate labeled data from another language, then train a classifier on the translated text. This can be viewed as a domain adaptation problem, where labeled translations and test data have some mismatch. Various prior work have achieved positive results using this approach.n n In this opinion piece, we take a step back and make some general statements about cross-lingual adaptation problems. First, we claim that domain mismatch is not caused by MT errors, and accuracy degradation will occur even in the case of perfect MT. Second, we argue that the cross-lingual adaptation problem is qualitatively different from other (monolingual) adaptation problems in NLP; thus new adaptation algorithms ought to be considered. This paper will describe a series of carefully-designed experiments that led us to these conclusions."
P11-2112,Learning Condensed Feature Representations from Large Unsupervised Data Sets for Supervised Learning,2011,20,14,3,0.776028,9188,jun suzuki,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"This paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning. We use unsupervised data to generate informative 'condensed feature representations' from the original feature set used in supervised NLP systems. The main contribution of our method is that it can offer dense and low-dimensional feature spaces for NLP tasks while maintaining the state-of-the-art performance provided by the recently developed high-performance semi-supervised learning technique. Our method matches the results of current state-of-the-art systems with very few features, i.e., F-score 90.72 with 344 features for CoNLL-2003 NER data, and UAS 93.55 with 12.5K features for dependency parsing data derived from PTB-III."
I11-1004,Extracting Pre-ordering Rules from Predicate-Argument Structures,2011,26,20,5,1,6319,xianchao wu,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Word ordering remains as an essential problem for translating between languages with substantial structural differences, such as SOV and SVO languages. In this paper, we propose to automatically extract pre-ordering rules from predicateargument structures. A pre-ordering rule records the relative position mapping of a predicate word and its argument phrases from the source language side to the target language side. We propose 1) a lineartime algorithm to extract the pre-ordering rules from word-aligned HPSG-tree-tostring pairs and 2) a bottom-up algorithm to apply the extracted rules to HPSG trees to yield target language style source sentences. Experimental results are reported for large-scale English-to-Japanese translation, showing significant improvements of BLEU score compared with the baseline SMT systems."
I11-1017,Mining Revision Log of Language Learning {SNS} for Automated {J}apanese Error Correction of Second Language Learners,2011,12,78,3,1,22506,tomoya mizumoto,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We present an attempt to extract a largescale Japanese learnersxe2x80x99 corpus from the revision log of a language learning SNS. This corpus is easy to obtain in largescale, covers a wide variety of topics and styles, and can be a great source of knowledge for both language learners and instructors. We also demonstrate that the extracted learnersxe2x80x99 corpus of Japanese as a second language can be used as training data for learnersxe2x80x99 error correction using an SMT approach. We evaluate different granularities of tokenization to alleviate the problem of word segmentation errors caused by erroneous input from language learners. Experimental results show that the character-wise model outperforms the word-wise model."
I11-1073,Distributed Minimum Error Rate Training of {SMT} using Particle Swarm Optimization,2011,12,5,3,0.776028,9188,jun suzuki,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"The direct optimization of a translation metric is an integral part of building stateof-the-art SMT systems. Unfortunately, widely used translation metrics such as BLEU-score are non-smooth, non-convex, and non-trivial to optimize. Thus, standard optimizers such as minimum error rate training (MERT) can be extremely time-consuming, leading to a slow turnaround rate for SMT research and experimentation. We propose an alternative approach based on particle swarm optimization (PSO), which can easily exploit the fast growth of distributed computing to obtain solutions quickly. For example in our experiments on NIST 2008 Chineseto-English data with 512 cores, we demonstrate a speed increase of up to 15x and reduce the parameter tuning time from 10 hours to 40 minutes with no degradation in BLEU-score."
I11-1153,Generalized Minimum {B}ayes Risk System Combination,2011,19,11,5,0.850756,5136,kevin duh,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Minimum Bayes Risk (MBR) has been used as a decision rule for both singlesystem decoding and system combination in machine translation. For system combination, we argue that common MBR implementations are actually not correct, since probabilities in the hypothesis space cannot be reliably estimated. These implementations achieve the effect of consensus decoding (which may be beneficial in its own right), but does not reduce Bayes Risk in the true Bayesian sense. We introduce Generalized MBR, which parameterizes the loss function in MBR and allows it to be optimized in the given hypothesis space of multiple systems. This extension better approximates the true Bayes Risk decision rule and empirically improves over MBR, even in cases where the combined systems are of mixed quality."
2011.mtsummit-papers.34,Extracting Pre-ordering Rules from Chunk-based Dependency Trees for {J}apanese-to-{E}nglish Translation,2011,-1,-1,5,1,6319,xianchao wu,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.mtsummit-papers.36,Post-ordering in Statistical Machine Translation,2011,-1,-1,5,1,1440,katsuhito sudoh,Proceedings of Machine Translation Summit XIII: Papers,0,None
W10-3501,Constructing Large-Scale Person Ontology from {W}ikipedia,2010,7,3,2,0,45230,yumi shibaki,Proceedings of the 2nd Workshop on {T}he {P}eople{'}s {W}eb {M}eets {NLP}: {C}ollaboratively {C}onstructed {S}emantic {R}esources,0,"This paper presents a method for constructing a large-scale Person Ontology with category hierarchy from Wikipedia. We first extract Wikipedia category labels which represent person (hereafter, Wikipedia Person Category, WPC) by using a machine learning classifier. We then construct a WPC hierarchy by detecting is-a relations in the Wikipedia category network. We then extract the titles of Wikipedia articles which represent person (hereafter, Wikipedia person instance, WPI). Experiments show that the accuracy of WPC extraction is 99.3% precision and 98.4% recall, while that of WPI extraction is 98.2% and 98.6%, respectively. The accuracies are significantly higher than the previous methods."
W10-3302,Using Goi-Taikei as an Upper Ontology to Build a Large-Scale {J}apanese Ontology from {W}ikipedia,2010,1,4,1,1,3634,masaaki nagata,Proceedings of the 6th Workshop on {O}ntologies and {L}exical {R}esources,0,"We present a novel method for building a large-scale Japanese ontology from Wikipedia using one of the largest Japanese thesauri, Nihongo Goi-Taikei (referred to hereafter as iGoi-Taikeii) as an upper ontology. First, The leaf categories in the Goi-Taikei hierarchy are semi-automatically aligned with semantically equivalent Wikipedia categories. Then, their subcategories are created automatically by detecting is-a links in the Wikipedia category network below the junction using the knowledge dene d in Goi-Taikei above the junction. The resulting ontology has a well-dene d taxonomy in the upper level and a ne-gra ined taxonomy in the lower level with a large number of up-to-date instances. A sample evaluation shows that the precisions of the extracted categories and instances are 92.8% and 98.6%, respectively."
W10-1757,N-Best Reranking by Multitask Learning,2010,37,9,5,0.830501,5136,kevin duh,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"We propose a new framework for N-best reranking on sparse feature sets. The idea is to reformulate the reranking problem as a Multitask Learning problem, where each N-best list corresponds to a distinct task.n n This is motivated by the observation that N-best lists often show significant differences in feature distributions. Training a single reranker directly on this heteroge-nous data can be difficult.n n Our proposed meta-algorithm solves this challenge by using multitask learning (such as e1/e2 regularization) to discover common feature representations across N-best lists. This meta-algorithm is simple to implement, and its modular approach allows one to plug-in different learning algorithms from existing literature. As a proof of concept, we show statistically significant improvements on a machine translation system involving millions of features."
W10-1762,Divide and Translate: Improving Long Distance Reordering in Statistical Machine Translation,2010,27,29,5,1,1440,katsuhito sudoh,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper proposes a novel method for long distance, clause-level reordering in statistical machine translation (SMT). The proposed method separately translates clauses in the source sentence and reconstructs the target sentence using the clause translations with non-terminals. The non-terminals are placeholders of embedded clauses, by which we reduce complicated clause-level reordering into simple word-level reordering. Its translation model is trained using a bilingual corpus with clause-level alignment, which can be automatically annotated by our alignment algorithm with a syntactic parser in the source language. We achieved significant improvements of 1.4% in BLEU and 1.3% in TER by using Moses, and 2.2% in BLEU and 3.5% in TER by using our hierarchical phrase-based SMT, for the English-to-Japanese translation of research paper abstracts in the medical domain."
P10-2025,Word Alignment with Synonym Regularization,2010,13,3,3,1,7901,hiroyuki shindo,Proceedings of the {ACL} 2010 Conference Short Papers,0,We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model. Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. We design a generative model for word alignment that uses synonym information as a regularization term. The experimental results show that our proposed method significantly improves word alignment quality.
P10-2030,Predicate Argument Structure Analysis Using Transformation Based Learning,2010,14,2,3,1,2121,hirotoshi taira,Proceedings of the {ACL} 2010 Conference Short Papers,0,"Maintaining high annotation consistency in large corpora is crucial for statistical learning; however, such work is hard, especially for tasks containing semantic elements. This paper describes predicate argument structure analysis using transformation-based learning. An advantage of transformation-based learning is the readability of learned rules. A disadvantage is that the rule extraction procedure is time-consuming. We present incremental-based, transformation-based learning for semantic processing tasks. As an example, we deal with Japanese predicate argument analysis and show some tendencies of annotators for constructing a corpus with our method."
C10-1038,Enriching Dictionaries with Images from the {I}nternet - Targeting {W}ikipedia and a {J}apanese Semantic Lexicon: Lexeed -,2010,15,2,2,0.573425,44776,sanae fujita,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"We propose a simple but effective method for enriching dictionary definitions with images based on image searches. Various query expansion methods using synonyms/hypernyms (or related words) are evaluated. We demonstrate that our method is effective in obtaining highprecision images that complement dictionary entries, even for words with abstract or multiple meanings."
Y09-2046,{B}ase{NP} Supersense Tagging for {J}apanese Texts,2009,17,1,3,1,2121,hirotoshi taira,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"This paper describes baseNP supersense tagging for Japanese texts. The task extracts base noun phrases (baseNPs) from raw texts in Japanese, and labels their baseNPs with supersenses. This task has a number of applications including predicate argument structure analysis and question answering. While the definition of baseNP in English is relatively clear, its definition in Japanese has not yet been clear. In this paper, we defined Japanese baseNP analogous to English and defined Japanese supersenses using a broad-coverage Japanese thesaurus, Nihongo Goi Taikei (comprehensive outline of Japanese vocabulary). We then adopted a sequential tagging algorithm for the task, namely the averaged perceptron with HMM, and achieve high performance compared to a baseline."
Y09-2052,Utilizing Features of Verbs in Statistical Zero Pronoun Resolution for {J}apanese Speech,2009,15,0,2,0,8426,sen yoshida,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"This paper proposes a statistical zero pronoun resolution method that utilizes features of verbs. In Japanese speech, the subject is often omitted, especially when it is the first person. To resolve such zero pronouns, features related to the verbs such as functional expressions play important roles. However, recent state-of-the-art zero-pronoun resolution systems lack these features because they are mainly designed for written texts such as newspaper articles, in which first person subjects are rare. We show that a set of verbal features has the ability to distinguish first persons from others in monologue transcriptions, and this improves the accuracy of zero pronoun resolution with statistical machine learning."
D08-1055,A {J}apanese Predicate Argument Structure Analysis using Decision Lists,2008,16,29,3,1,2121,hirotoshi taira,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"This paper describes a new automatic method for Japanese predicate argument structure analysis. The method learns relevant features to assign case roles to the argument of the target predicate using the features of the words located closest to the target predicate under various constraints such as dependency types, words, semantic categories, parts of speech, functional words and predicate voices. We constructed decision lists in which these features were sorted by their learned weights. Using our method, we integrated the tasks of semantic role labeling and zero-pronoun identification, and achieved a 17% improvement compared with a baseline method in a sentence level performance analysis."
P06-1090,A Clustered Global Phrase Reordering Model for Statistical Machine Translation,2006,11,38,1,1,3634,masaaki nagata,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we present a novel global reordering model that can be incorporated into standard phrase-based statistical machine translation. Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs (Till-mann and Zhang, 2005), our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences. In principle, the global phrase reordering model is conditioned on the source and target phrases that are currently being translated, and the previously translated source and target phrases. To cope with sparseness, we use N-best phrase alignments and bilingual phrase clustering, and investigate a variety of combinations of conditioning factors. Through experiments, we show, that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task."
2006.iwslt-evaluation.11,Phrase reordering for statistical machine translation based on predicate-argument structure,2006,15,26,2,0.408163,310,mamoru komachi,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, we describe a novel phrase reordering model based on predicate-argument structure. Our phrase reordering method utilizes a general predicate-argument structure analyzer to reorder source language chunks based on predicate-argument structure. We explicitly model longdistance phrase alignments by reordering arguments and predicates. The reordering approach is applied as a preprocessing step in training phase of a phrase-based statistical MT system. We report experimental results in the evaluation campaign of IWSLT 2006."
P05-3016,Portable Translator Capable of Recognizing Characters on Signboard and Menu Captured by its Built-in Camera,2005,7,20,3,0,27528,hideharu nakajima,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"We present a portable translator that recognizes and translates phrases on signboards and menus as captured by a built-in camera. This system can be used on PDAs or mobile phones and resolves the difficulty of inputting some character sets such as Japanese and Chinese if the user doesn't know their readings. Through the high speed mobile network, small images of signboards can be quickly sent to the recognition and translation server. Since the server runs state of the art recognition and translation technology and huge dictionaries, the proposed system offers more accurate character recognition and machine translation."
2005.iwslt-1.16,{NUT}-{NTT} Statistical Machine Translation System for {IWSLT} 2005,2005,7,19,4,0,49988,kazuteru ohashi,Proceedings of the Second International Workshop on Spoken Language Translation,0,"In this paper, we present a novel distortion model for phrase-based statistical machine translation. Unlike the previous phrase distortion models whose role is to simply penalize nonmonotonic alignments[1, 2], the new model assigns the probability of relative position between two source language phrases aligned to the two adjacent target language phrases. The phrase translation probabilities and phrase distortion probabilities are calculated from the N-best phrase alignment of the training bilingual sentences. To obtain Nbest phrase alignment, we devised a novel phrase alignment algorithm based on word translation probabilities and N-best search. Experiments show that the phrase distortion model and phrase translation model improve the BLEU and NIST scores over the baseline method."
W04-3255,Efficient Decoding for Statistical Machine Translation with a Fully Expanded {WFST} Model,2004,18,8,2,0.30869,38029,hajime tsukada,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a novel method to compile statistical models for machine translation to achieve efficient decoding. In our method, each statistical submodel is represented by a weighted finite-state transducer (WFST), and all of the submodels are expanded into a composition model beforehand. Furthermore, the ambiguity of the composition model is reduced by the statistics of hypotheses while decoding. The experimental results show that the proposed model representation drastically improves the efficiency of decoding compared to the dynamic composition of the submodels, which corresponds to conventional approaches."
W03-1506,Multi-Language Named-Entity Recognition System based on {HMM},2003,15,12,2,0.666667,32919,kuniko saito,Proceedings of the {ACL} 2003 Workshop on Multilingual and Mixed-language Named Entity Recognition,0,"We introduce a multi-language named-entity recognition system based on HMM. Japanese, Chinese, Korean and English versions have already been implemented. In principle, it can analyze any other language if we have training data of the target language. This system has a common analytical engine and it can handle any language simply by changing the lexical analysis rules and statistical language model. In this paper, we describe the architecture and accuracy of the named-entity system, and report preliminary experiments on automatic bilingual named-entity dictionary construction using the Japanese and English named-entity recognizer."
2003.mtsummit-papers.56,Improving translation models by applying asymmetric learning,2003,-1,-1,2,0,42063,setsuo yamada,Proceedings of Machine Translation Summit IX: Papers,0,"The statistical Machine Translation Model has two components: a language model and a translation model. This paper describes how to improve the quality of the translation model by using the common word pairs extracted by two asymmetric learning approaches. One set of word pairs is extracted by Viterbi alignment using a translation model, the other set is extracted by Viterbi alignment using another translation model created by reversing the languages. The common word pairs are extracted as the same word pairs in the two sets of word pairs. We conducted experiments using English and Japanese. Our method improves the quality of a original translation model by 5.7{\%}. The experiments also show that the proposed learning method improves the word alignment quality independent of the training domain and the translation model. Moreover, we show that common word pairs are almost as useful as regular dictionary entries for training purposes."
W01-1413,Using the Web as a Bilingual Dictionary,2001,7,84,1,1,3634,masaaki nagata,Proceedings of the {ACL} 2001 Workshop on Data-Driven Methods in Machine Translation,0,"We present a system for extracting an English translation of a given Japanese technical term by collecting and scoring translation candidates from the web. We first show that there are a lot of partially bilingual documents in the web that could be useful for term translation, discovered by using a commercial technical term dictionary and an Internet search engine. We then present an algorithm for obtaining translation candidates based on the distance of Japanese and English terms in web documents, and report the results of a preliminary experiment."
P00-1049,Synchronous Morphological Analysis of Grapheme and Phoneme for {J}apanese {OCR},2000,8,1,1,1,3634,masaaki nagata,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"We developed a novel language model for Japanese based on grapheme-phoneme tuples, which is one order of magnitude smaller than word-based models. We also developed an alignment algorithm of graphemes and phonemes for both ordinary text and OCR output. We show, by experiment, that the combination of the grapheme-phoneme tuple ngram model and the grapheme-phoneme alignment algorithm significantly improve character recognition accuracy if both grapheme and phoneme representations are given."
P99-1036,A Part of Speech Estimation Method for {J}apanese Unknown Words using a Statistical Model of Morphology and Context,1999,17,28,1,1,3634,masaaki nagata,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that constitute a word. The point is quite simple: different character sets should be treated differently and the changes between character types are very important because Japanese script has both ideograms like Chinese (kanji) and phonograms like English (katakana). Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model. The model can achieve 96.6% tagging accuracy if unknown words are correctly segmented.
P98-2152,{J}apanese {OCR} Error Correction using Character Shape Similarity and Statistical Language Model,1998,14,18,1,1,3634,masaaki nagata,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"We present a novel OCR error correction method for languages without word delimiters that have a large character set, such as Japanese and Chinese. It consists of a statistical OCR model, an approximate word matching method using character shape similarity, and a word segmentation algorithm using a statistical language model. By using a statistical OCR model and character shape similarity, the proposed error corrector outperforms the previously published method. When the baseline character recognition accuracy is 90%, it achieves 97.4% character recognition accuracy."
C98-2147,{J}apanese {OCR} Error Correction using Character Shape Similarity and Statistical Language Model,1998,14,18,1,1,3634,masaaki nagata,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"We present a novel OCR error correction method for languages without word delimiters that have a large character set, such as Japanese and Chinese. It consists of a statistical OCR model, an approximate word matching method using character shape similarity, and a word segmentation algorithm using a statistical language model. By using a statistical OCR model and character shape similarity, the proposed error corrector outperforms the previously published method. When the baseline character recognition accuracy is 90%, it achieves 97.4% character recognition accuracy."
W97-0120,A Self-Organizing {J}apanese Word Segmenter using Heuristic Word Identification and Re-estimation,1997,11,8,1,1,3634,masaaki nagata,Fifth Workshop on Very Large Corpora,0,"We present a self-organized method to build a stochastic Japanese word segmenter from a small number of basic words and a large amount of unsegmented training text. It consists of a word-based statistical language model, an initial estimation procedure, and a re-estimation procedure. Initial word frequencies are estimated by counting all possible longest match strings between the training text and the word list. The initial word list is au~nented by identifying words in the training text using a heuristic rule based on character type. The word-based language model is then re-estimated to filter out inappropriate word hypotheses generated by the initial word identification. When the word segmeuter is trained on 3.9M character texts and 1719 initial words, its word segmentation accuracy is 86.3% recall and 82.5% precision. We find that the combination of heuristic word identi~cation and re-estimation is so effective that the initial word list need not be large. 1 I n t r o d u c t i o n Word segmentation is an important problem for Japanese because word boundaries are not marked in its writing system. Other Asian languages such as Chinese and Thai have the same problem. Any Japanese NLP application requ/res word segmentation as the first stage because there are phonological and semantic units whose pronunciation and meaning is not trivially derivable from that of the individual characters. Once word segmentation is done, all established techniques can be exploited to build practically important applications such as spelling correction [Nagata, 1996] and text retrieval [Nie and Brisebois, 1996] In a sense, Japanese word segmentation is a solved problem if (and only if) we have plenty of segmented training text. Around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programi-g procedure [Nagata, 1994, Takeuchi and Matsumoto, 1995, Yamamoto, 1996]. However, manually segmented corpora are not always available in a particular target domain and manual segmentation is very expensive. The goal of our research is unsupervised learning of Japanese word segmentation. That is, to build a Japanese word segmenter from a list of initial words and unsegmented training text. Today, it is easy to obtain a 10K-100K word list from either commercial or public domain on-line Japanese dictionaries. Gigabytes of Japanese text are readily available from newspapers, patents, HTML documents, etc.. Few works have examined unsupervised word segmentation in Japanese. Both [Yamamoto, 1996] and [Takeuchi and Matsumoto, 1995] built a word-based language model from unsegmented text"
W96-0205,Automatic Extraction of New Words from {J}apanese Texts using Generalized Forward-Backward Search,1996,8,11,1,1,3634,masaaki nagata,Conference on Empirical Methods in Natural Language Processing,0,None
C96-2136,Context-Based Spelling Correction for {J}apanese {OCR},1996,12,23,1,1,3634,masaaki nagata,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"We present a novel spelling correction method for those languages that have no delimiter between words, such as Japanese, Chinese, and Thai. It consists of an approximate word matching method and an N-best word segmentation algorithm using a statistical language model. For OCR errors, the proposed word-based correction method outperforms the conventional character-based correction method. When the baseline character recognition accuracy is 90%, it achieves 96.0% character recognition accuracy and 96.3% word segmentation accuracy, while the character recognition accuracy of character-based correction is 93.3%."
C94-1032,A Stochastic {J}apanese Morphological Analyzer Using a Forward-{DP} Backward-{A}* N-Best Search Algorithm,1994,9,86,1,1,3634,masaaki nagata,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words. It consists of a statistical language model and an efficient two-pass N-best search algorithm. The algorithm does not require delimiters between words. Thus it is suitable for written Japanese. The proposed Japanese morphological analyzer achieved 95.1% recall and 94.6% precision for open text when it was trained and tested on the ATR Corpus.
C92-3164,A Spoken Language Translation System: {SL-TRANS}2,1992,9,13,5,0,55960,tsuyoshi morimoto,{COLING} 1992 Volume 3: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
C92-1030,An Empirical Study on Rule Granularity and Unification Interleaving Toward an Efficient Unification-Based Parsing System,1992,24,9,1,1,3634,masaaki nagata,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper describes an empirical study on the optimal granularity of the phrase structure rules and the optimal strategy for interleaving CFG parsing with unification in order to implement an efficient unification-based parsing system. We claim that using medium-grained CFG phrase structure rules, which balance the computational cost of CFG parsing and unification, are a cost-effective solution for making unification-based grammar both efficient and easy to maintain. We also claim that late unification, which delays unification until a complete CFG parse is found, saves unnecessary copies of DAGs for irrelevant subparses and improves performance significantly. The effectiveness of these methods was proved in an extensive experiment. The results show that, on average, the proposed system parses 3.5 times faster than our previous one. The grammar and the parser described in this paper are fully implemented and used as the Japanese analysis module in SL-TRANS, the speech-to-speech translation system of ATR."
