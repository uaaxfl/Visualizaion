2020.bea-1.9,2020.lrec-1.883,1,0.845269,"Missing"
2020.bea-1.9,W11-2308,1,0.842233,"Missing"
2020.bea-1.9,W18-4606,0,0.0372641,"of grammatical structures’ competence with respect to predefined grades, such as the Common European Framework of Reference for Languages (CEFRL) (Zilio et al., 2018). Given the difficulty of defining a unique indicator of linguistic complexity in the context of L2 language development, a great variety of features from all linguistic levels have been used as input for supervised classification systems trained on authentic learner data for different L2s. Such is the case e.g. of Hancke and Meurers (2013) and Vajjala and L˙eo (2014), dealing with L2 German and L2 Estonian, respectively, and of Pilán and Volodina (2018), who also provided a features analysis focused on predictive features extracted from both receptive and productive texts in Swedish L2. This paper adopts this framework and presents an innovative NLP-based stylometric approach to model writing development in learners of Spanish as a second and Heritage language. Our approach relies on a wide set of linguistically motivated features extracted from students’ essays, which have already been shown relevant for a number of tasks related to modelling the ‘form’ of a text rather than the content. While the majority of previous studies on the evoluti"
2020.bea-1.9,W11-0314,1,0.840214,"Missing"
2020.bea-1.9,W14-1820,1,0.845606,"Missing"
2020.bea-1.9,P05-1025,1,0.579616,"arners, in both the first and second language context. The empirical evidence acquired from learner corpora, complemented with the increased reliability of linguistic features extracted by computational tools and machine learning approaches, has promoted a better understanding of learners’ language properties and how they change across time and increasing proficiency level (Crossley, 2020). A first line of research has focused on providing automatic ways of operationalizing sophisticated metrics of language development to alleviate the laborious manual computation of these metrics by experts (Sagae et al., 2005; Lu, 2009). A second line of research has taken the more challenging step of implementing completely data-driven approaches, which use a variety of linguistic features extracted from texts to automatically assign a learner’s language production to a given developmental level (Lubetich and Sagae, 2014). A great amount of work has been carried out in the field of second language acquisition where 92 th Proceedings of the 15 Workshop on Innovative Use of NLP for Building Educational Applications, pages 92–101 c July 10, 2020. 2020 Association for Computational Linguistics the best of our knowled"
2020.bea-1.9,W01-0521,0,0.122749,"duction in a L2 (Cimino et al., 2018). Interestingly, for all mentioned tasks the set of linguistic features plays a very important role in the classification not only of a whole document but also of each single sentence. This is the reason why, as reported in the following sections, we modelled the prediction of the development of writing skills both as document and sentence classification tasks. Although we used a state–of–the art pipeline, it is well-acknowledged that the accuracy of statistical parsers decreases when tested against texts of a different typology from that used in training (Gildea, 2001). In this respect, learners’ data are particularly challenging for general–purpose text analysis tools since they can exhibit deviation from correct and standard language; for instance, missing or anomalous use of punctuation (especially in 1st grade prompts) already impacts on the coarsest levels of text processing, i.e. sentence splitting, and thus may affect all subsequent levels of annotation. F L) Ccl = blog2 f freq(M req(CL) c, where MFW and MFL are the most frequent word form/lemma in the corpus and CW and CL are the considered ones. A first overview of how and to what extent all these"
2020.bea-1.9,L16-1680,0,0.0122333,"uency for each word form/lemma was computed exploiting the Spanish Wikipedia (dump of March 2020) using the FW) following measures: Ccw = blog2 f freq(M req(CW ) c, Linguistic Features The set of linguistic features considered as predictors of L2 written competence evolution is based on those described in Brunato et al. (2020). It includes a wide range of text properties, from raw text features, to lexical, morpho-syntactic and syntactic properties, which were extracted from different levels of linguistic annotation. For this purpose, the COWS-L2H Corpus was automatically parsed using UDPipe (Straka et al., 2016) trained on the Spanish Universal Dependency Treebank (GSD section), version 2.5. We rely on these features since it has been shown that they have a high predictive power for several tasks all aimed at modelling the linguistic form of documents. This is the case for example of the automatic readability assessment task (Dell’Orletta et al., 2011a), of the automatic classification of the textual genre of documents (Cimino et al., 2017), or also of the automatic identification of the L1 of a writer based on his/her language production in a L2 (Cimino et al., 2018). Interestingly, for all mentione"
2020.bea-1.9,C14-1203,1,0.327745,"age properties and how they change across time and increasing proficiency level (Crossley, 2020). A first line of research has focused on providing automatic ways of operationalizing sophisticated metrics of language development to alleviate the laborious manual computation of these metrics by experts (Sagae et al., 2005; Lu, 2009). A second line of research has taken the more challenging step of implementing completely data-driven approaches, which use a variety of linguistic features extracted from texts to automatically assign a learner’s language production to a given developmental level (Lubetich and Sagae, 2014). A great amount of work has been carried out in the field of second language acquisition where 92 th Proceedings of the 15 Workshop on Innovative Use of NLP for Building Educational Applications, pages 92–101 c July 10, 2020. 2020 Association for Computational Linguistics the best of our knowledge, the first data–driven study which uses linguistic features from student data to model the evolution of written language competence in Spanish as a Second Language (SSL); (ii) we show that it is possible to automatically predict the relative order of two essays written by the same student at differe"
2020.bea-1.9,W14-3509,0,0.0681894,"Missing"
2020.bea-1.9,L18-1650,0,0.0261058,"ifornia Davis alessio.miaschi@phd.unipi.it, {ssdavidson,sagae,chsanchez}@ucdavis.edu, {dominique.brunato,felice.dellorletta,giulia.venturi}@ilc.cnr.it Abstract the study of L2 writings is seen as a proxy of language ability development (Crossley, 2020). In this respect, much related work is devoted to predicting the degree of second language proficiency according to expert–based evaluation (Crossley and McNamara, 2012) or to modelling the evolution of grammatical structures’ competence with respect to predefined grades, such as the Common European Framework of Reference for Languages (CEFRL) (Zilio et al., 2018). Given the difficulty of defining a unique indicator of linguistic complexity in the context of L2 language development, a great variety of features from all linguistic levels have been used as input for supervised classification systems trained on authentic learner data for different L2s. Such is the case e.g. of Hancke and Meurers (2013) and Vajjala and L˙eo (2014), dealing with L2 German and L2 Estonian, respectively, and of Pilán and Volodina (2018), who also provided a features analysis focused on predictive features extracted from both receptive and productive texts in Swedish L2. This"
2020.lrec-1.894,W13-3520,0,0.0907265,"Missing"
2020.lrec-1.894,P17-1074,0,0.298225,"y labeling the token as correct. An alternative method of calculating agreement would be to consider only positions where at least one annotator indicated an error; however, this choice would ignore all positions at which both annotators agreed that no error exists, which is itself a form of agreement. To put our agreement values in more familiar context, we also report the F0.5 score, commonly used in GEC, using one annotator as ground-truth. In terms of both Krippendorf’s α and F0.5 , our annotators show strong agreement. Essays 448 8 24 89 3,516 be extracted using NLP tools such as ERRANT (Bryant et al., 2017). Additionally, we align the original and corrected sentences to create parallel data that can be used for training NLP systems such as grammatical error correction. To our knowledge, our corpus represents the first parallel dataset of corrected Spanish text available to researchers. As with our error annotations, we are in the process of completing additional corrections and anonymization, and will make more data publicly available as soon as practical. As can be seen in Table 3, the largest portion our currently annotated corpus come from beginning students; completing additional corrections"
2020.lrec-1.894,D18-1274,0,0.0525393,"Missing"
2020.lrec-1.894,P11-2117,0,0.0353186,"from the corpus. 6.1. Previous work in GEC The machine translation approach to GEC frames error correction as a monolingual translation task in which the source and target languages are ”with errors” and ”without errors,” respectively (Ng et al., 2014). Approaches developed originally for translation between different languages have been adapted and applied successfully to grammatical error correction (Napoles and Callison-Burch, 2017; Leacock et al., 2010, p. 95). Similar monolingual translation approaches have been used for paraphrase generation (Quirk et al., 2004) and text simplification (Coster and Kauchak, 2011). Recent work has shown neural machine translation (NMT) (Bahdanau et al., 2014) to be an effective 7240 Original Model Artificial only Parallel only Fine-tuned En 1990 se fund´o la radio p´ublica R`adio Nacional d’Andorra . 1990 se fund´o la radio p´ublica R`adio nnliocaa a˜nos d’Andorra . Noised Table 5: Artificial noising of data approach to the GEC task (Zhao et al., 2019; Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018). While framing the problem of error correction as a monolingual translation task is promising, the approach requires parallel training data (Rei et al., 2017), whic"
2020.lrec-1.894,D19-5546,0,0.403806,"Missing"
2020.lrec-1.894,N18-1055,0,0.0470342,"Missing"
2020.lrec-1.894,D18-1541,0,0.0153948,"based grammatical error correction system for Spanish learner writing. Keywords: L2 Spanish, error correction, learner corpus 1. Introduction Studies in second language acquisition benefit from large quantities of corpus data from L2 learners, which facilitate the analysis of developmental patterns with respect to their emerging grammar (Hawkins and Buttery, 2009). In addition, the development of effective NLP systems for second language instruction, such as automated grammatical error correction and automated student assessment, depends on the availability of large annotated learner corpora (Kasewa et al., 2018). While annotated learner corpora of English are widely available, large learner corpora of Spanish are less common, and as a result, the field has seen little datadriven research on the developmental processes that underlie Spanish language learning, or on the development of NLP tools to assist teachers and students of Spanish. This may come as unexpected, considering the fact that there exists a relatively high demand for learning Spanish; in 2013, fifty-one percent of students enrolled in university language courses in the United States studied Spanish (AAAS, 2016) and there are over 21 mil"
2020.lrec-1.894,P17-4012,0,0.0208734,"se of demonstrating the value of the COWS-L2H data. Our learner data, which we reserve for fine-tuning and testing, consists of 10,000 parallel uncorrected and corrected sentences drawn from COWS-L2H, with 1,400 sentences set aside for validation and 1,400 for testing. Precision 0.026 0.094 0.254 Recall 0.019 0.139 0.153 F0.5 0.024 0.101 0.224 Table 6: Model results 6.3. Model and training procedure We train a neural machine translation (NMT) model with a 4-layer bi-directional LSTM encoder and an LSTM decoder, both with 500 hidden units in each layer. We implement the model using OpenNMT-py (Klein et al., 2017). We use the Adam optimizer with a learning rate of 0.001, a dropout of 0.3, and a batch size of 64. Our decoder uses beam search with a beam size of 5. We train for 40,000 steps on 1.04 million sentences of the artificially noised Wikipedia data. After this initial training is complete, we fine tune the model for an additional 5,000 training steps on 10,000 sentences of parallel COWS-L2H data to achieve our final model. 6.4. Results We evaluate model performance using the ERRANT scorer (Bryant et al., 2017), which although designed to annotate errors in English text is capable of aligning edi"
2020.lrec-1.894,D19-5504,0,0.019312,"ys is challenging; however, our courses are generally taught using a standard variety of academic Spanish, so we expect this to be the predominant variety in the corpus. Students provide information about their linguistic background which we include as metadata in the corpus; this metadata may elucidate variability in usage resulting from students’ past experience with Spanish. The metadata also allows us to test the effects variables such as L1 on student writing. Finally, the linguistic metadata may facilitate the use of filtered subcorpora for targeted training of NLP systems; for example, Nadejde and Tetreault (2019) demonstrate that grammatical error correction systems benefit from adaptation to L1 and proficiency level. 5. Error annotation One of the primary goals of this project is to annotate grammatical errors in the corpus in a way that writing patterns typical of Spanish as a foreign language produced by student participants can be identified, catalogued, and easily utilized by researchers who use the corpus. To this end, we have begun the process of error-tagging the corpus based on specific error types; the first two error types for which we have completed annotation are gender and number agreeme"
2020.lrec-1.894,W17-5039,0,0.0128839,"tools for Spanish learners. To demonstrate the utility of the COWS-L2H corpus to NLP researchers, we implement a grammatical error correction system using the parallel error-corrected sentences from the corpus. 6.1. Previous work in GEC The machine translation approach to GEC frames error correction as a monolingual translation task in which the source and target languages are ”with errors” and ”without errors,” respectively (Ng et al., 2014). Approaches developed originally for translation between different languages have been adapted and applied successfully to grammatical error correction (Napoles and Callison-Burch, 2017; Leacock et al., 2010, p. 95). Similar monolingual translation approaches have been used for paraphrase generation (Quirk et al., 2004) and text simplification (Coster and Kauchak, 2011). Recent work has shown neural machine translation (NMT) (Bahdanau et al., 2014) to be an effective 7240 Original Model Artificial only Parallel only Fine-tuned En 1990 se fund´o la radio p´ublica R`adio Nacional d’Andorra . 1990 se fund´o la radio p´ublica R`adio nnliocaa a˜nos d’Andorra . Noised Table 5: Artificial noising of data approach to the GEC task (Zhao et al., 2019; Chollampatt and Ng, 2018; Junczys"
2020.lrec-1.894,W04-3219,0,0.0584366,"using the parallel error-corrected sentences from the corpus. 6.1. Previous work in GEC The machine translation approach to GEC frames error correction as a monolingual translation task in which the source and target languages are ”with errors” and ”without errors,” respectively (Ng et al., 2014). Approaches developed originally for translation between different languages have been adapted and applied successfully to grammatical error correction (Napoles and Callison-Burch, 2017; Leacock et al., 2010, p. 95). Similar monolingual translation approaches have been used for paraphrase generation (Quirk et al., 2004) and text simplification (Coster and Kauchak, 2011). Recent work has shown neural machine translation (NMT) (Bahdanau et al., 2014) to be an effective 7240 Original Model Artificial only Parallel only Fine-tuned En 1990 se fund´o la radio p´ublica R`adio Nacional d’Andorra . 1990 se fund´o la radio p´ublica R`adio nnliocaa a˜nos d’Andorra . Noised Table 5: Artificial noising of data approach to the GEC task (Zhao et al., 2019; Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018). While framing the problem of error correction as a monolingual translation task is promising, the approach requi"
2020.lrec-1.894,W17-5032,0,0.0607314,"Missing"
2020.lrec-1.894,N18-1057,0,0.0527919,"Missing"
2020.lrec-1.894,N19-1014,0,0.0462746,"Missing"
2021.acl-long.560,2020.acl-main.747,0,0.634302,"es, including in situations where no parallel data or prior knowledge is available. 2.3 Language vectors without parallel data The approach that is closest to ours is XLM (Conneau and Lample, 2019), which adds language embeddings to each byte pair embedding using Wikipedia data in various languages with a masked language modeling objective. However, similar to Johnson et al. (2017), the trained language embeddings only serve as an indicator to the encoder and decoder to identify input and output words in the vocabulary as belonging to different languages. In fact, in a follow up paper, XLM-R (Conneau et al., 2020), language embeddings are removed from the model for better code-switching, which suggests that the learned language embeddings may not be optimal for cross-lingual tasks. In this paper, following the finding that structural similarity is critical in multilingual language models (K et al., 2020), we generate language embeddings from a denoising autoencoder objective and demonstrate that they can be effectively used in cross-lingual zero-shot learning. 3 Generating Language Embeddings We first present the data used to generate language embeddings, then introduce our approach inspired by denoisi"
2021.acl-long.560,D18-1269,0,0.38813,"istics and the 11th International Joint Conference on Natural Language Processing, pages 7210–7225 August 1–6, 2021. ©2021 Association for Computational Linguistics proach captures typological information by comparing the information in our language embeddings to language-specific information in the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). In addition, to address the question of whether the learned language embeddings can help in downstream language tasks, we plug-in the language embeddings to cross-lingual dependency parsing and natural language inference (XNLI, Conneau et al., 2018) in a zero-shot learning setting, obtaining performance improvements. 2 Related Work Previous related research approached language representations by using prior knowledge, dense language embeddings with multilingual parallel data, or no prior knowledge about languages but having language embeddings primarily as a signal to identify different languages. 2.1 Feature-based language representations An intuitive method to represent language information is through explicit information such as known word order patterns (Ammar et al., 2016; Little, 2017), part-of-speech tag sequences (Wang and Eisner"
2021.acl-long.560,P17-4012,0,0.029775,"Missing"
2021.acl-long.560,2005.mtsummit-papers.11,0,0.113294,"urce sentence and train the language embeddings with a many-to-one neural machine translation model (Malaviya et al., 2017; Tan et al., 2019). Another method is to concatenate language embedding vectors to a character level language model (Östling and Tiedemann, 2017; Bjerva and Augenstein, 2018; Bjerva et al., 2019a). These two methods require parallel translation data such as Bible and TED Talk. Rabinovich et al. (2017) derive typological information in the form of phylogenetic trees from translation of different languages into English and French using the European Parliament speech corpus (Koehn, 2005), based on the assumption that unique language properties are present in translations (Baker et al., 1993; Toury, 1995). Bjerva et al. (2019b) abstract the translated sentences from other languages to English with part-of-speech tags, function words, dependency relation tags, and constituent tags, and train the embedding vectors by concatenating a language representation with a symbol representation. In comparison, we generate our language embeddings using no parallel corpora or linguistic annotation, which is suitable for a wider variety of languages, including in situations where no parallel"
2021.acl-long.560,2020.acl-main.703,0,0.0115383,"Table 3 indicate that our language embeddings, which capture relationships between each test language and the training language (English), are also effective in tasks involving higher-level semantic information. We observe consistent performance gains over very strong baselines in all settings and models for each language. Specifically, in the fully zero-shot setting where 5.5 Discussion Our results in each of the intrinsic and extrinsic evaluation settings demonstrate that our denoising autoencoder objective, which has been shown to be effective in various language model pre-training tasks (Lewis et al., 2020; Raffel et al., 2020), is effective for learning language embeddings that capture typological information and can be used to 7217 improve cross-lingual inference. Even though reconstructing the original sentence from a randomly ordered string is the direct training objective, our evaluation of the resulting embeddings is not based simply on word order. The grammar of a language is of course an important factor in determining the order of words in a sentence in that language, although it is not the only factor. The syntax area features in our WALS evaluation, which are largely related to relat"
2021.acl-long.560,D18-1543,0,0.0259443,"Missing"
2021.acl-long.560,2020.findings-emnlp.150,0,0.018966,"ic tasks in a zero-shot setting: cross-lingual dependency parsing and cross-lingual natural language inference1 . 1 Introduction Recent efforts to leverage multilingual datasets in language modeling (Conneau and Lample, 2019; Devlin et al., 2019) and machine translation (Johnson et al., 2017; Lu et al., 2018) highlight the potential of multilingual models that can perform well across various languages, including ones for which training sets are scarce. Most of the current multilingual research focuses on learning invariant representations or removing language-specific features after training (Libovický et al., 2020; Bjerva and Augenstein, 2021). Despite recent advances, there are still limitations. Previous work has shown that similar languages can benefit from sharing parameters, but less similar languages do not help (Zoph et al., 2016; Pires et al., 2019). However, in spite of some interests in typology (Ponti et al., 2019), identifying similar languages is nontrivial, especially for less studied ones. In addition, as Zhao et al. (2019) suggest, learning invariant representations can actually harm model performance. ∗ Equal contribution. Our learned language embeddings and code available at https://g"
2021.acl-long.560,E17-2002,0,0.152201,"ong different language representations. One way to represent language identity within a multilingual model is the use of language codes, or dense vectors representing language embeddings. If languages are represented with vectors that capture cross-lingual similarities and differences across different dimensions, this information can guide a multilingual model regarding what and how much of the information in the model should be shared among specific languages. Much of the previous research focused on generating language embeddings using prior knowledge such as word order (Ammar et al., 2016; Littell et al., 2017), using a parallel corpus (Bjerva et al., 2019b; Östling and Tiedemann, 2017), and using language codes as an indicator to distinguish input and output words in a shared vocabulary into different languages (Johnson et al., 2017; Conneau and Lample, 2019). In contrast, our work focuses on generating and using language embeddings more effectively as softsharing (de Lhoneux et al., 2018) of parameters among various languages in a single model. Furthermore, we are motivated by a more difficult setting where the properties of each language are not known in advance, and no parallel data is available"
2021.acl-long.560,W17-0120,0,0.0269008,"and natural language inference (XNLI, Conneau et al., 2018) in a zero-shot learning setting, obtaining performance improvements. 2 Related Work Previous related research approached language representations by using prior knowledge, dense language embeddings with multilingual parallel data, or no prior knowledge about languages but having language embeddings primarily as a signal to identify different languages. 2.1 Feature-based language representations An intuitive method to represent language information is through explicit information such as known word order patterns (Ammar et al., 2016; Little, 2017), part-of-speech tag sequences (Wang and Eisner, 2017), and syntactic dependencies (Östling, 2015). Littell et al. (2017) propose sparse vectors using pre-defined language features such as known typological and geographical information for a given language. However, linguistic features may not be available for less studied languages. Our proposed approach assumes no prior knowledge about each language, deriving typological information from plain text alone. Once a vector for a target language is created, it contains many typological features of the target language, and can be used for transfer"
2021.acl-long.560,W18-6309,0,0.0276246,"ture relationships among languages can be learned and subsequently leveraged in cross-lingual tasks without the use of parallel data. We generate dense embeddings for 29 languages using a denoising autoencoder, and evaluate the embeddings using the World Atlas of Language Structures (WALS) and two extrinsic tasks in a zero-shot setting: cross-lingual dependency parsing and cross-lingual natural language inference1 . 1 Introduction Recent efforts to leverage multilingual datasets in language modeling (Conneau and Lample, 2019; Devlin et al., 2019) and machine translation (Johnson et al., 2017; Lu et al., 2018) highlight the potential of multilingual models that can perform well across various languages, including ones for which training sets are scarce. Most of the current multilingual research focuses on learning invariant representations or removing language-specific features after training (Libovický et al., 2020; Bjerva and Augenstein, 2021). Despite recent advances, there are still limitations. Previous work has shown that similar languages can benefit from sharing parameters, but less similar languages do not help (Zoph et al., 2016; Pires et al., 2019). However, in spite of some interests in"
2021.acl-long.560,D15-1166,0,0.0155255,"Missing"
2021.acl-long.560,D17-1268,0,0.341129,"anguages. Our proposed approach assumes no prior knowledge about each language, deriving typological information from plain text alone. Once a vector for a target language is created, it contains many typological features of the target language, and can be used for transfer learning in downstream tasks. 2.2 Dense representation with parallel data Other previous work has also explored dense continuous representations of languages. One method is to append a language token to the beginning of a source sentence and train the language embeddings with a many-to-one neural machine translation model (Malaviya et al., 2017; Tan et al., 2019). Another method is to concatenate language embedding vectors to a character level language model (Östling and Tiedemann, 2017; Bjerva and Augenstein, 2018; Bjerva et al., 2019a). These two methods require parallel translation data such as Bible and TED Talk. Rabinovich et al. (2017) derive typological information in the form of phylogenetic trees from translation of different languages into English and French using the European Parliament speech corpus (Koehn, 2005), based on the assumption that unique language properties are present in translations (Baker et al., 1993; Tou"
2021.acl-long.560,P15-2034,0,0.0302897,"aining performance improvements. 2 Related Work Previous related research approached language representations by using prior knowledge, dense language embeddings with multilingual parallel data, or no prior knowledge about languages but having language embeddings primarily as a signal to identify different languages. 2.1 Feature-based language representations An intuitive method to represent language information is through explicit information such as known word order patterns (Ammar et al., 2016; Little, 2017), part-of-speech tag sequences (Wang and Eisner, 2017), and syntactic dependencies (Östling, 2015). Littell et al. (2017) propose sparse vectors using pre-defined language features such as known typological and geographical information for a given language. However, linguistic features may not be available for less studied languages. Our proposed approach assumes no prior knowledge about each language, deriving typological information from plain text alone. Once a vector for a target language is created, it contains many typological features of the target language, and can be used for transfer learning in downstream tasks. 2.2 Dense representation with parallel data Other previous work has"
2021.acl-long.560,E17-2102,0,0.0592204,"identity within a multilingual model is the use of language codes, or dense vectors representing language embeddings. If languages are represented with vectors that capture cross-lingual similarities and differences across different dimensions, this information can guide a multilingual model regarding what and how much of the information in the model should be shared among specific languages. Much of the previous research focused on generating language embeddings using prior knowledge such as word order (Ammar et al., 2016; Littell et al., 2017), using a parallel corpus (Bjerva et al., 2019b; Östling and Tiedemann, 2017), and using language codes as an indicator to distinguish input and output words in a shared vocabulary into different languages (Johnson et al., 2017; Conneau and Lample, 2019). In contrast, our work focuses on generating and using language embeddings more effectively as softsharing (de Lhoneux et al., 2018) of parameters among various languages in a single model. Furthermore, we are motivated by a more difficult setting where the properties of each language are not known in advance, and no parallel data is available. We investigate whether we can generate language embeddings to represent typ"
2021.acl-long.560,P19-1493,0,0.0249432,"machine translation (Johnson et al., 2017; Lu et al., 2018) highlight the potential of multilingual models that can perform well across various languages, including ones for which training sets are scarce. Most of the current multilingual research focuses on learning invariant representations or removing language-specific features after training (Libovický et al., 2020; Bjerva and Augenstein, 2021). Despite recent advances, there are still limitations. Previous work has shown that similar languages can benefit from sharing parameters, but less similar languages do not help (Zoph et al., 2016; Pires et al., 2019). However, in spite of some interests in typology (Ponti et al., 2019), identifying similar languages is nontrivial, especially for less studied ones. In addition, as Zhao et al. (2019) suggest, learning invariant representations can actually harm model performance. ∗ Equal contribution. Our learned language embeddings and code available at https://github.com/DianDYu/language_ embeddings 1 Therefore, in order to leverage language agnostic and language specific information effectively, we propose to generate language representations and examine the interactions among different language represen"
2021.acl-long.560,J19-3005,0,0.0280764,"Missing"
2021.acl-long.560,P17-1049,0,0.0178026,"sks. 2.2 Dense representation with parallel data Other previous work has also explored dense continuous representations of languages. One method is to append a language token to the beginning of a source sentence and train the language embeddings with a many-to-one neural machine translation model (Malaviya et al., 2017; Tan et al., 2019). Another method is to concatenate language embedding vectors to a character level language model (Östling and Tiedemann, 2017; Bjerva and Augenstein, 2018; Bjerva et al., 2019a). These two methods require parallel translation data such as Bible and TED Talk. Rabinovich et al. (2017) derive typological information in the form of phylogenetic trees from translation of different languages into English and French using the European Parliament speech corpus (Koehn, 2005), based on the assumption that unique language properties are present in translations (Baker et al., 1993; Toury, 1995). Bjerva et al. (2019b) abstract the translated sentences from other languages to English with part-of-speech tags, function words, dependency relation tags, and constituent tags, and train the embedding vectors by concatenating a language representation with a symbol representation. In compar"
2021.emnlp-main.37,2020.emnlp-main.23,0,0.0554063,"Missing"
2021.emnlp-main.37,D19-1461,0,0.13058,"ts where the dialog model will generate unsafe responses given the contexts and prompts. We compare our proposed Trigger_weakly, Trigger_PPO, and Trigger_PPO_adv with the original model BlenderBot. Safety Classifier We train our safety classifier fsaf ety using data collected from BAD (Xu et al., 2020). We truncate the conversation history to four utterances from both speakers following the best practice in their paper. We also ignore easy cases where the bot says “Hey do you want to talk about something else” from a safety layer during data collection. In addition, we leverage data from BBF (Dinan et al., 2019) including both singleturn and multi-turn examples. In total, we have a training corpus of 197K examples and we evaluate on the BAD validation set with 12.8K examples. We train the classifier using RoBERTa (Liu et al., 2019b). The classifier achieves an F1 score of 77.34 on unsafe examples, which is close to the number reported in Xu et al. (2020), so we did not use additional training data and framework. For the classifier used in the weakly-supervised 0 method fsaf ety , we use the same data training a multi-layer perceptron (MLP) on top of frozen BlenderBot hidden states (similar to Dathath"
2021.emnlp-main.37,W19-3646,0,0.0274545,"apparent traits such as controversial Ethical Considerations statement or hate speech, regardless of the semantics and coherence of the conversation. Our intended use case is to expose problems with For consistency, previous work suggests gen- neural (dialog) models where it is impractical to ask eration grounded by information such as per- human annotators to interact with each new model sonas (Zhang et al., 2018) and neural memories to find potential errors within a reasonable bud(Sukhbaatar et al., 2015). In terms of consistency get. More importantly, we believe that by finding detection, Dziri et al. (2019); Welleck et al. (2019); these problems automatically, we can reveal more Li et al. (2020) introduce and suggest using natural systematic problems rather than relying on simple language inference to model conversation coher- tricks as done in previous data collection. We hope ence. Recently, Nie et al. (2021) collects a large that this can help model developers to find probcontradicting human dialog corpus based on a con- lems with our proposed method so that they can versational context and show better performance solve them before deploying their trained models. than entailment-based methods"
2021.emnlp-main.37,W17-4912,0,0.0611271,"Missing"
2021.emnlp-main.37,2020.findings-emnlp.301,0,0.0242575,"ate data to improve performance of out-of-domain problem classifiers. In the future, we plan to extend our methods to other problems such as generic utterances and hallucination (Mielke et al., 2020). Safety and Consistency To make machine learning models safe to use especially with language generation, there is a long literature in safety such as hate speech (Zampieri et al., 2020) and bias (Dinan et al., 2019, 2020). Most of these works focus on abusive context detection. On a different line of research, some work introduces conditional generation to reduce toxicity (Dathathri et al., 2020; Gehman et al., 2020). These techniques mostly Acknowledgments requires some toxic classifiers, which as shown We thank Zhou Yu for early discussion, and the in Section 4.3, may not work well for a different model distribution. Recently, Xu et al. (2020) in- anonymous reviewers for their constructive suggesstructs humans to interact with neural dialog mod- tions. This work was supported by the National Sciels in an adversarial way in order to induce un- ence Foundation under Grant No. 1840191. Any opinions, findings, and conclusions or recommensafe responses from chatbots. Although classifiers trained with the int"
2021.emnlp-main.37,2020.acl-main.740,0,0.0360435,"Missing"
2021.emnlp-main.37,2020.acl-main.185,0,0.0337895,", 2020; Li et al., 2021; in the BlenderBot baseline). Song et al., 2021), Wallace et al. (2019); Sheng On the other hand, we can find that coherent nat- et al. (2020) show that some learned discrete nonural patterns such as “They should ...” and “You are sensical universal triggers can be used to generate 463 unsafe sentences. On the other hand, Gehman et al. (2020) finds toxic prompts from naturally occurring sentences. The most similar work to ours is probably directing pre-trained models into generating a list of pre-defined tokens or sentences (He and Glass, 2018; Liu et al., 2019a, 2020; He and Glass, 2020). In comparison, our task needs to generate coherent prompts according to the conversation history. Furthermore, instead of triggering pre-defined egregious responses, our proposed method is more flexible in exposing a wide range of problems such as consistency where crafting the target responses without context in advance is impossible. expose more systematic errors and is generally applicable to a wide variety of problems with trained neural models. 7 Conclusion In this paper, we propose a weakly-supervised approach and a reinforcement learning approach to automatically expose problems with"
2021.emnlp-main.37,2021.naacl-main.400,0,0.0749987,"Missing"
2021.emnlp-main.37,P16-1094,0,0.157516,"cedure is extremely expensive and is not practical for newly trained models. More importantly, systematic problems are still not revealed. Language models, including dialog models, greatly benefit from training on large amounts of data with the objective of mimicking human generated sentences (Radford et al., 2019; Brown et al., 2020; Zhang et al., 2019a; Adiwardana et al., 2020; Roller et al., 2021). However, even with carefully preprocessed training data from online sources, neural dialog models are prone to issues including generic utterances, repetition, contradiction, and lack of safety (Li et al., 2016a; Welleck et al., 2020; Li et al., 2020; Roller et al., 2021; Xu et al., 2021). Compared to modularized dialog systems which are designed to avoid these problems (Yu et al., 2019; Paranjape et al., 2020), fixing these issues with end-to-end neural models is more challenging, In this work, we propose to automatically exwhich may hinder real world use of trained mod- pose problems with neural dialog models in a more els (Wolf et al., 2017; Simonite, 2021). We argue systematic setting. Given a conversation context, that before solving these problems using simulated the goal is to generate a cohe"
2021.emnlp-main.37,D16-1127,0,0.184608,"cedure is extremely expensive and is not practical for newly trained models. More importantly, systematic problems are still not revealed. Language models, including dialog models, greatly benefit from training on large amounts of data with the objective of mimicking human generated sentences (Radford et al., 2019; Brown et al., 2020; Zhang et al., 2019a; Adiwardana et al., 2020; Roller et al., 2021). However, even with carefully preprocessed training data from online sources, neural dialog models are prone to issues including generic utterances, repetition, contradiction, and lack of safety (Li et al., 2016a; Welleck et al., 2020; Li et al., 2020; Roller et al., 2021; Xu et al., 2021). Compared to modularized dialog systems which are designed to avoid these problems (Yu et al., 2019; Paranjape et al., 2020), fixing these issues with end-to-end neural models is more challenging, In this work, we propose to automatically exwhich may hinder real world use of trained mod- pose problems with neural dialog models in a more els (Wolf et al., 2017; Simonite, 2021). We argue systematic setting. Given a conversation context, that before solving these problems using simulated the goal is to generate a cohe"
2021.emnlp-main.37,2020.acl-main.428,0,0.177743,"practical for newly trained models. More importantly, systematic problems are still not revealed. Language models, including dialog models, greatly benefit from training on large amounts of data with the objective of mimicking human generated sentences (Radford et al., 2019; Brown et al., 2020; Zhang et al., 2019a; Adiwardana et al., 2020; Roller et al., 2021). However, even with carefully preprocessed training data from online sources, neural dialog models are prone to issues including generic utterances, repetition, contradiction, and lack of safety (Li et al., 2016a; Welleck et al., 2020; Li et al., 2020; Roller et al., 2021; Xu et al., 2021). Compared to modularized dialog systems which are designed to avoid these problems (Yu et al., 2019; Paranjape et al., 2020), fixing these issues with end-to-end neural models is more challenging, In this work, we propose to automatically exwhich may hinder real world use of trained mod- pose problems with neural dialog models in a more els (Wolf et al., 2017; Simonite, 2021). We argue systematic setting. Given a conversation context, that before solving these problems using simulated the goal is to generate a coherent utterance to act data from simplifi"
2021.emnlp-main.37,2021.acl-long.353,0,0.0208102,"oblem. 3 Methodology Since the goal of the task is to expose systematic problems of pre-trained models rather than relying on simple tricks, we generate prompts using the same model in a self-chat paradigm so that when we plug in the generated prompts to the original model we get exactly the same response. Compared to recent work on instructing humans to goad chatbots where annotators have no information of how the models works in a trial-and-error blackbox attack manner, we use gradients of the model. Motivated by recent success in conditional generation without fine-tuning model parameters (Li and Liang, 2021; Yu et al., 2021), we propose to learn a trigger prompt hidden states, htrigger , while freez1 ing the original dialog model to maintain output Our code is available at https://github.com/ DianDYu/trigger distribution and generation quality. Specifically, for 457 an encoder-decoder model (or a language model), we are modeling pθ (xk |htrigger , x&lt;k , y&lt;k ) (1) where htrigger is prepended to the beginning of the conversation history and is initialized with the hidden states of the bos (beginning of sentence) token. Before any training, the distribution of pθ (xk |.) will not be modified at all"
2021.emnlp-main.37,2021.ccl-1.108,0,0.0751309,"Missing"
2021.emnlp-main.37,2020.findings-emnlp.219,0,0.0593022,"Missing"
2021.emnlp-main.37,2020.acl-main.441,0,0.0877683,"Missing"
2021.emnlp-main.37,2021.acl-long.134,0,0.357497,"g the model into generating problematic responses in a black-box attack setting. Although the data collected in this way can improve the performance of both problem classifiers and model generation, human annotators mostly rely on straightforward and intuitive strategies to collect the dataset, which may only expose superficial problems. For instance, Xu et al. (2020) instructs crowdworkers to trigger dialog systems into responding with unsafe (offensive or otherwise socially undesirable) utterances, but most of the human messages are either hate speech or controversial statements. Similarly, Nie et al. (2021) asks Mechanical Turkers to manually write contradicting dialogs for both humans and bots, or to interact with chatbots, where a frequent strategy is to ask factual questions intentionally leading to contradiction (e.g. ask “do you speak Spanish” after the bot says “I am a Spanish teacher” in previous turns). Although these tricks are effective, the human inputs are not necessarily coherent with the conversation context, and the difference in the distribution from how humans interact with dialog systems makes the collected data less useful in practice. In addition, the data collection procedur"
2021.emnlp-main.37,2020.findings-emnlp.17,0,0.0532707,"Missing"
2021.emnlp-main.37,2021.eacl-main.24,0,0.525921,"h instructs crowdworkers to goad the bots into triggering such problems. However, humans leverage superficial clues such as hate speech, while leaving systematic problems undercover. In this paper, we propose two methods including reinforcement learning to automatically trigger a dialog model into generating problematic responses. We show the effect of our methods in exposing safety and contradiction issues with state-of-the-art dialog models. 1 Introduction Kenji Sagae University of California, Davis sagae@ucdavis.edu between model designers and the dialog system during qualitative analysis (Roller et al., 2021). Recent work proposes asking annotators to converse with dialog models while goading the model into generating problematic responses in a black-box attack setting. Although the data collected in this way can improve the performance of both problem classifiers and model generation, human annotators mostly rely on straightforward and intuitive strategies to collect the dataset, which may only expose superficial problems. For instance, Xu et al. (2020) instructs crowdworkers to trigger dialog systems into responding with unsafe (offensive or otherwise socially undesirable) utterances, but most o"
2021.emnlp-main.37,2020.findings-emnlp.291,0,0.0486976,"Missing"
2021.emnlp-main.37,2020.acl-main.183,0,0.0163758,"our generated results can in turn improve the classification performance with out-of-domain data. For all our experiments, we use the state-of-theart open-domain chatbot BlenderBot (Roller et al., 2021) as our pre-trained neural dialog model. The maximum context and response lengths is set to 128 BPE tokens (Radford et al., 2019). BlenderBot is pre-trained on Reddit discussions (Baumgartner et al., 2020) with heuristic filtering and fine-tuned on human-collected clean conversational data including ConvAI2 (Zhang et al., 2018) and Blended Training and Evaluation During training, we Skill Talk (Smith et al., 2020). Because of the fine- sample contexts of three utterances from the pretuning data, the chatbot frequently deviates from processed BAD training data explained above bethe current conversation topic and asks simple ques- cause BlenderBot can only handle 128 tokens. For tions such as “do you have a pet”. This makes it evaluation, we sample contexts of the same length even harder to generate unsafe and contradictory re- from the BAD validation data. We report the avsponses given a coherent prompt. For decoding, we erage probability that the response is unsafe and follow the same procedure as in t"
2021.emnlp-main.37,2021.naacl-main.291,0,0.0474696,"Missing"
2021.emnlp-main.37,D19-1221,0,0.0240149,"enerated less coherent and less diverse, resulting in similar by Trigger_PPO significantly outperforms the n-grams. This suggests that on the one hand, we baseline (67.68 F1 vs. 59.05 F1). This indicates may need an additional reward in addition to the that our proposed method can not only expose relatively straightforward negative penalty. On the model problems, but can help problem detection other hand, with more training steps, we may be with new data distribution as well. It can also be able to discover more meaningful “universal trigused in models that require good domain-specific gers” (Wallace et al., 2019) that can trigger target classification, such as Dathathri et al. (2020). responses regardless of the context. 462 Training data DECODE valid human-bot Weakly-supervised vs. Reinforcement Learning Method Although there is a potential discrepancy between training and testing that htrigger may 0 only learn to optimize the classifier fconsis regardless of the actual task for the weakly-supervised method as explained in Section 3.1, we found that in the safety exposure task, it can still increase performance from human annotation. However, this results in much higher unsafe degree for the prompt a"
2021.emnlp-main.37,P19-1363,0,0.0212065,"as controversial Ethical Considerations statement or hate speech, regardless of the semantics and coherence of the conversation. Our intended use case is to expose problems with For consistency, previous work suggests gen- neural (dialog) models where it is impractical to ask eration grounded by information such as per- human annotators to interact with each new model sonas (Zhang et al., 2018) and neural memories to find potential errors within a reasonable bud(Sukhbaatar et al., 2015). In terms of consistency get. More importantly, we believe that by finding detection, Dziri et al. (2019); Welleck et al. (2019); these problems automatically, we can reveal more Li et al. (2020) introduce and suggest using natural systematic problems rather than relying on simple language inference to model conversation coher- tricks as done in previous data collection. We hope ence. Recently, Nie et al. (2021) collects a large that this can help model developers to find probcontradicting human dialog corpus based on a con- lems with our proposed method so that they can versational context and show better performance solve them before deploying their trained models. than entailment-based methods. However, as in Furthe"
2021.emnlp-main.37,D19-3014,1,0.843041,", greatly benefit from training on large amounts of data with the objective of mimicking human generated sentences (Radford et al., 2019; Brown et al., 2020; Zhang et al., 2019a; Adiwardana et al., 2020; Roller et al., 2021). However, even with carefully preprocessed training data from online sources, neural dialog models are prone to issues including generic utterances, repetition, contradiction, and lack of safety (Li et al., 2016a; Welleck et al., 2020; Li et al., 2020; Roller et al., 2021; Xu et al., 2021). Compared to modularized dialog systems which are designed to avoid these problems (Yu et al., 2019; Paranjape et al., 2020), fixing these issues with end-to-end neural models is more challenging, In this work, we propose to automatically exwhich may hinder real world use of trained mod- pose problems with neural dialog models in a more els (Wolf et al., 2017; Simonite, 2021). We argue systematic setting. Given a conversation context, that before solving these problems using simulated the goal is to generate a coherent utterance to act data from simplified scenarios, we need to be able as a human prompt through self-chat, which will to probe the models and expose the problems in a trigger t"
2021.emnlp-main.37,2021.findings-emnlp.194,1,0.700959,"Since the goal of the task is to expose systematic problems of pre-trained models rather than relying on simple tricks, we generate prompts using the same model in a self-chat paradigm so that when we plug in the generated prompts to the original model we get exactly the same response. Compared to recent work on instructing humans to goad chatbots where annotators have no information of how the models works in a trial-and-error blackbox attack manner, we use gradients of the model. Motivated by recent success in conditional generation without fine-tuning model parameters (Li and Liang, 2021; Yu et al., 2021), we propose to learn a trigger prompt hidden states, htrigger , while freez1 ing the original dialog model to maintain output Our code is available at https://github.com/ DianDYu/trigger distribution and generation quality. Specifically, for 457 an encoder-decoder model (or a language model), we are modeling pθ (xk |htrigger , x&lt;k , y&lt;k ) (1) where htrigger is prepended to the beginning of the conversation history and is initialized with the hidden states of the bos (beginning of sentence) token. Before any training, the distribution of pθ (xk |.) will not be modified at all. Once we generate"
2021.emnlp-main.37,P18-1205,0,0.13645,"nerating prompts that can trigger corresponding problems. In addition, we study whether our generated results can in turn improve the classification performance with out-of-domain data. For all our experiments, we use the state-of-theart open-domain chatbot BlenderBot (Roller et al., 2021) as our pre-trained neural dialog model. The maximum context and response lengths is set to 128 BPE tokens (Radford et al., 2019). BlenderBot is pre-trained on Reddit discussions (Baumgartner et al., 2020) with heuristic filtering and fine-tuned on human-collected clean conversational data including ConvAI2 (Zhang et al., 2018) and Blended Training and Evaluation During training, we Skill Talk (Smith et al., 2020). Because of the fine- sample contexts of three utterances from the pretuning data, the chatbot frequently deviates from processed BAD training data explained above bethe current conversation topic and asks simple ques- cause BlenderBot can only handle 128 tokens. For tions such as “do you have a pet”. This makes it evaluation, we sample contexts of the same length even harder to generate unsafe and contradictory re- from the BAD validation data. We report the avsponses given a coherent prompt. For decoding"
2021.emnlp-main.37,N19-1131,0,0.0644339,"Missing"
2021.findings-emnlp.194,K16-1002,0,0.0457387,"he generation process can be controlled using arbitrary attributes expressed as words or phrases. Table 1 shows text generated using the prompt The issue focused on with various control attributes. We evaluate our proposed method on sentiment and topic control and show better performance than previous state-of-the-art methods in controlling effectiveness and language quality 1 . pre-training (e.g. 55 control codes in CTRL). Another approach is to concatenate the attribute representation to the hidden states using linear transformation (Hoang et al., 2016; Fu et al., 2018) or latent variables (Bowman et al., 2016; Wang et al., 2019). These approaches require training from scratch or fine-tuning the entire pre-trained model to incorporate the external target attributes and model conditional probability (Ficler and Goldberg, 2017; Ziegler et al., 2019a; Smith et al., 2020). In addition, they always require carefully designed Kullback-Leibler (KL)-Divergence and adversarial training to generate out-of training domain text with the desirable attribute only (Romanov et al., 2019). In comparison, our proposed method does not require fine-tuning the original LM so that we can make use of the high quality pre"
2021.findings-emnlp.194,N19-1423,0,0.0329788,"Missing"
2021.findings-emnlp.194,W17-4912,0,0.0951595,"proposed method on sentiment and topic control and show better performance than previous state-of-the-art methods in controlling effectiveness and language quality 1 . pre-training (e.g. 55 control codes in CTRL). Another approach is to concatenate the attribute representation to the hidden states using linear transformation (Hoang et al., 2016; Fu et al., 2018) or latent variables (Bowman et al., 2016; Wang et al., 2019). These approaches require training from scratch or fine-tuning the entire pre-trained model to incorporate the external target attributes and model conditional probability (Ficler and Goldberg, 2017; Ziegler et al., 2019a; Smith et al., 2020). In addition, they always require carefully designed Kullback-Leibler (KL)-Divergence and adversarial training to generate out-of training domain text with the desirable attribute only (Romanov et al., 2019). In comparison, our proposed method does not require fine-tuning the original LM so that we can make use of the high quality pre-trained LM while controlling the target attributes. 2 Related Work Instead of fine-tuning the whole model, Houlsby et al. (2019) proposes to add residual adapters, Controlled text generation To interpolate a conwhich a"
2021.findings-emnlp.194,N16-1149,0,0.0243577,"e way the attributes are encoded, the end result is that the generation process can be controlled using arbitrary attributes expressed as words or phrases. Table 1 shows text generated using the prompt The issue focused on with various control attributes. We evaluate our proposed method on sentiment and topic control and show better performance than previous state-of-the-art methods in controlling effectiveness and language quality 1 . pre-training (e.g. 55 control codes in CTRL). Another approach is to concatenate the attribute representation to the hidden states using linear transformation (Hoang et al., 2016; Fu et al., 2018) or latent variables (Bowman et al., 2016; Wang et al., 2019). These approaches require training from scratch or fine-tuning the entire pre-trained model to incorporate the external target attributes and model conditional probability (Ficler and Goldberg, 2017; Ziegler et al., 2019a; Smith et al., 2020). In addition, they always require carefully designed Kullback-Leibler (KL)-Divergence and adversarial training to generate out-of training domain text with the desirable attribute only (Romanov et al., 2019). In comparison, our proposed method does not require fine-tuning the"
2021.findings-emnlp.194,Q18-1005,0,0.0260477,"he sentiment attribute representation. Thus, the target attribute representation may be diluted. To solve this problem, we propose three disentanglement methods. 3.2.1 Attribute representation with corpus representation disentanglement (AC) We propose to add a corpus domain representation d along with the attribute representation a during training. For a training corpus (such as movie reviews) with multiple attributes (such as positive and negative sentiment), d is used in all the training data while a is only used in a subset of the training data labeled with the target attribute. Similar to Liu and Lapata (2018), this can encourage the model to encode target attribute and other features separately into different representations. Specifically, the key-value pairs can be represented as K:t00 , V:t00 = [F(Ka ); Fd (Kd ); K:t ], [F(Va ); Fd (Vd ); V:t ] (3) where Fd is a separate alignment function for corpus domain representation, and Kd , Vd are from the LM encoding of corpus domain names. Compared to attributes, corpus domain names might be more abstract so we use special tokens for d (such as <movie review&gt;) and the original texts for attributes (such as athlete). At inference time, we want to genera"
2021.findings-emnlp.194,P11-1015,0,0.275395,"Missing"
2021.findings-emnlp.194,2020.findings-emnlp.219,0,0.313529,"ch ferent from adding adapters for each individual and has been commonly used in grounded generaattribute (Bapna and Firat, 2019; Ziegler et al., tion (Dinan et al., 2019; Prabhumoye et al., 2020). 2019b), our method only requires learning one Keskar et al. (2019) proposes to pre-train a large attribute alignment function for all attributes to conditional language model with available labels do controlled generation, and is more flexible at such as URLs for large LM control. This method inference time without degrading quality such as can be effective in conditional modeling, but rediversity (Madotto et al., 2020). Recently, Chan quires a substantial amount of resources for preet al. (2021) proposes to use self-supervised learntraining and is limited by the labels used during ing with hand-crafted phrases (e.g. “is perfect” to 1 represent positive sentiment), but suffers from high Our code is available at https://github.com/ DianDYu/attribute_alignment variance, low coherence and diversity in order to 2252 Attribute Representation E<movie&gt; Epositive Alignment Function director is great , EThe Edirector Eis Egreat … Eactor E’<movie&gt; E’positive great … actor Pre-trained LM Pre-trained LM <movie&gt; Wt+1 Ali"
2021.findings-emnlp.194,2020.coling-main.1,0,0.0118722,"high quality pre-trained LM while controlling the target attributes. 2 Related Work Instead of fine-tuning the whole model, Houlsby et al. (2019) proposes to add residual adapters, Controlled text generation To interpolate a conwhich are task-specific parameters to transformer trolling factor, concatenating the attribute to the inlayers for each language understanding task. Difput sequence is the most straightforward approach ferent from adding adapters for each individual and has been commonly used in grounded generaattribute (Bapna and Firat, 2019; Ziegler et al., tion (Dinan et al., 2019; Prabhumoye et al., 2020). 2019b), our method only requires learning one Keskar et al. (2019) proposes to pre-train a large attribute alignment function for all attributes to conditional language model with available labels do controlled generation, and is more flexible at such as URLs for large LM control. This method inference time without degrading quality such as can be effective in conditional modeling, but rediversity (Madotto et al., 2020). Recently, Chan quires a substantial amount of resources for preet al. (2021) proposes to use self-supervised learntraining and is limited by the labels used during ing with"
2021.findings-emnlp.194,N19-1088,0,0.120143,"tribute representation to the hidden states using linear transformation (Hoang et al., 2016; Fu et al., 2018) or latent variables (Bowman et al., 2016; Wang et al., 2019). These approaches require training from scratch or fine-tuning the entire pre-trained model to incorporate the external target attributes and model conditional probability (Ficler and Goldberg, 2017; Ziegler et al., 2019a; Smith et al., 2020). In addition, they always require carefully designed Kullback-Leibler (KL)-Divergence and adversarial training to generate out-of training domain text with the desirable attribute only (Romanov et al., 2019). In comparison, our proposed method does not require fine-tuning the original LM so that we can make use of the high quality pre-trained LM while controlling the target attributes. 2 Related Work Instead of fine-tuning the whole model, Houlsby et al. (2019) proposes to add residual adapters, Controlled text generation To interpolate a conwhich are task-specific parameters to transformer trolling factor, concatenating the attribute to the inlayers for each language understanding task. Difput sequence is the most straightforward approach ferent from adding adapters for each individual and has b"
2021.findings-emnlp.194,D13-1170,0,0.0044698,"n the training corpus and generate text conditioned on a new topic as a zero-shot setting. 4 Experiments We evaluate our proposed methods A: using attribute representation only; AC: Model A with corpus representation for disentanglement; ACK: AC with KL disentanglement; and lastly ACB: AC with Bayes disentanglement. We evaluate these models on sentiment control for thorough comparisons. We use nucleus sampling (Holtzman et al., 2020) for all the methods at inference time. Refer to Appendix A.4 for implementation details. 4.1 Sentiment control Data. We use the Stanford Sentiment Treebank (SST, Socher et al., 2013) as our training data. We choose the sentences with positive and negative sentiment to train our alignment function. We select the same 15 prompts such as “Once upon a time” that were used in prior work, which were originally randomly selected, and are listed in Appendix A.2 (Dathathri et al., 2020). log p(x|a) ∼ log p(x|a, d) − log p(x|d) (5) Baselines. We compare with five baselines. During training, we train the attribute and do- GPT2 generates unconditioned sentences given the prompts from pre-trained GPT2-medium. The genmain alignment functions (F, Fd ) by running the erated sentences are"
2021.findings-emnlp.194,N19-1015,0,0.0217332,"can be controlled using arbitrary attributes expressed as words or phrases. Table 1 shows text generated using the prompt The issue focused on with various control attributes. We evaluate our proposed method on sentiment and topic control and show better performance than previous state-of-the-art methods in controlling effectiveness and language quality 1 . pre-training (e.g. 55 control codes in CTRL). Another approach is to concatenate the attribute representation to the hidden states using linear transformation (Hoang et al., 2016; Fu et al., 2018) or latent variables (Bowman et al., 2016; Wang et al., 2019). These approaches require training from scratch or fine-tuning the entire pre-trained model to incorporate the external target attributes and model conditional probability (Ficler and Goldberg, 2017; Ziegler et al., 2019a; Smith et al., 2020). In addition, they always require carefully designed Kullback-Leibler (KL)-Divergence and adversarial training to generate out-of training domain text with the desirable attribute only (Romanov et al., 2019). In comparison, our proposed method does not require fine-tuning the original LM so that we can make use of the high quality pre-trained LM while co"
2021.findings-emnlp.194,2021.acl-long.560,1,0.71687,"sentiment classification dataset and finds that one neuron is responsible for the sentiment value in generation. Our proposed disentanglement methods, on the other hand, encourages the alignment function to encode different attributes to different representations and we leverage Bayes’ Rule to further separate attributes. In machine translation, a language representation is learned by appending a language code to the source sentence (Johnson et al., 2016) or summing with word embeddings (Conneau and Lample, 2019) to guide the translation towards the target language. Inspired by these methods (Yu et al., 2021), Attribute Alignment appends the attribute to the beginning of a sentence and learns an attribute alignment function to transform attribute representations while freezing the LM parameters, without fine-tuning the whole model in previous methods. 3 Methodology Unconditional language models are trained to optimize the probability of p(xi |x0:i−1 ) where xi is the next token and x0:i−1 are already generated tokens. For controlled generation, we need to model the conditional distribution p(xi |x0:i−1 , a) where a is the attribute for the model to condition on. Attribute representation learning L"
C08-1095,P06-4020,0,0.0112176,"CCG predicate-argument dependency structures following the CCG derivation, not directly through DAG parsing. Similarly, the HPSG parser of Miyao and Tsujii (2005) builds the HPSG predicate-argument dependency structure following unification operations during HPSG parsing. Sagae et al. (2007) use a dependency parsing combined with an HPSG parser to produce predicate-argument dependencies. However, the dependency parser is used only to produce a dependency tree backbone, which the HPSG parser then uses to produce the more general dependency graph. A similar strategy is used in the RASP parser (Briscoe et al., 2006), which builds a dependency graph through unification operations performed during a phrase structure tree parsing process. Conclusion We have presented a framework for dependency DAG parsing, using a novel algorithm for projective DAGs that extends existing shift-reduce algorithms for parsing with dependency trees, and pseudo-projective transformations applied to DAG structures. We have demonstrated that the parsing approach is effective in analysis of predicateargument structure in English using data from the HPSG Treebank (Miyao et al., 2004), and in parsing of Danish using a rich dependency"
C08-1095,W06-2920,0,0.071525,"as simple as current data-driven transition-based dependency parsing frameworks, but outputs directed acyclic graphs (DAGs). We demonstrate the benefits of DAG parsing in two experiments where its advantages over dependency tree parsing can be clearly observed: predicate-argument analysis of English and syntactic analysis of Danish with a representation that includes long-distance dependencies and anaphoric reference links. 1 Introduction Natural language parsing with data-driven dependency-based frameworks has received an increasing amount of attention in recent years (McDonald et al., 2005; Buchholz and Marsi, 2006; Nivre et al., 2006). Dependency representations directly reflect word-to-word relation † This work was conducted while the author was at the Computer Science Department of the University of Tokyo. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0). Some rights reserved. ships in a dependency graph, where the words in a sentence are the nodes, and labeled edges correspond to head-dependent syntactic relations. In addition to being inherently lexicalized, dependency analyses can be generated"
C08-1095,P02-1042,0,0.0314778,"AG parsing (that could also easily be applied to cyclic structures) using approximate inference in an edge-factored dependency model starting from dependency trees. In their model, the addition of extra arcs to the tree was learned with the parameters to build the initial tree itself, which shows the power and flexibility of approximate inference in graph-based dependency models. Other parsing approaches that produce dependency graphs that are not limited to tree structures include those based on linguisticallymotivated lexicalized grammar formalisms, such as HPSG, CCG and LFG. In particular, Clark et al. (2002) use a probabilistic model of dependency DAGs extracted from the CCGBank (Hockenmeier and Steedman, 2007) in a CCG parser that builds the CCG predicate-argument dependency structures following the CCG derivation, not directly through DAG parsing. Similarly, the HPSG parser of Miyao and Tsujii (2005) builds the HPSG predicate-argument dependency structure following unification operations during HPSG parsing. Sagae et al. (2007) use a dependency parsing combined with an HPSG parser to produce predicate-argument dependencies. However, the dependency parser is used only to produce a dependency tre"
C08-1095,D07-1024,0,0.0123265,"ent of the University of Tokyo. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0). Some rights reserved. ships in a dependency graph, where the words in a sentence are the nodes, and labeled edges correspond to head-dependent syntactic relations. In addition to being inherently lexicalized, dependency analyses can be generated efficiently and have been show to be useful in a variety of practical tasks, such as question answering (Wang et al., 2007), information extraction in biomedical text (Erkan et al., 2007; Saetre et al, 2007) and machine translation (Quirk and Corston-Oliver, 2006). However, despite rapid progress in the development of parsers for several languages (Nivre et al., 2007) and algorithms for more linguistically adequate non-projective structures (McDonald et al., 2005; Nivre and Nilsson, 2006), most of the current data-driven dependency parsing approaches are limited to producing only dependency trees, where each word has exactly one head. Although trees have desirable properties from both computational and linguistic perspectives, the structure of linguistic phenomena that goes b"
C08-1095,J07-3004,0,0.0428449,"Missing"
C08-1095,P05-1011,1,0.602649,"Missing"
C08-1095,P05-1013,0,0.0135962,"o be a DAG where all arcs can be drawn above the sentence (written sequentially in its original order) in a way such that no arcs cross and there are no covered roots (although a root is not a concept associated with DAGs, we borrow the term from trees to denote words with no heads in the sentence). However, nonprojectivity is predictably more wide-spread in DAG representations, since there are at least as many arcs as in a tree representation, and often more, including arcs that represent non-local relationships. We then discuss the application of pseudo-projective transformations (Nivre and Nilsson, 2005) and an additional arc-reversing transform to dependency DAGs. Using a shiftreduce algorithm that allows multiple heads per word and pseudo-projective transformations to754 gether forms a complete dependency DAG parsing framework. 2.1 (a) Desired output: Basic shift-reduce parsing with multiple heads X Like Nivre (2004), we consider the direction of the dependency arc to be from the head to the dependent. X Z Initial state: Stack Input tokens The basic bottom-up left-to-right dependency parsing algorithm described by Nivre (2004) keeps a list of tokens (initialized to contain the input string)"
C08-1095,N07-1050,0,0.0138519,"output in the overall parsing framework. An alternative to using pseudo-projective transformations is to develop an algorithm for DAG parsing based on the family of algorithms described by Covington (2001), in the same way the algorithms in sections 2.1 and 2.2 were developed based on the algorithms described by Nivre (2004). Although this may be straightforward, a potential drawback of such an approach is that the number of parse actions taken in a Covington-style algorithm is always quadratic on the length of the input sentence, resulting in parsers that are more costly to train and to run (Nivre, 2007). The algorithms presented here, however, behave identically to their linear runtime tree counterparts when they are trained with graphs that are limited to tree structures. Additional actions are necessary only when words with more than one head are encountered. For data sets where most words have only one head, the performance the algorithms described in sections 2.1 and 2.2 should be close to that of shiftreduce projective parsing for dependency trees. In data sets where most words have multiple heads (resulting in higher arc density), the use of a Covington-style algorithm may be advantage"
C08-1095,D07-1111,1,0.916223,"a few structures in the data contain cycles, but most of the structures in the treebank are DAGs. In the experiments presented below, the algorithm described in section 2.1 was used. We believe the use of the arc-eager algorithm described in section 2.2 would produce similar results, but this is left as future work. 3.1 Learning component The DAG parsing framework, as described so far, must decide when to apply each appropriate parser action. As with other data-driven dependency parsing approaches with shift-reduce algorithms, we use a classifier to make these decisions. Following the work of Sagae and Tsujii (2007), we use maximum entropy models for classification. During training, the DAGs are first projectivized with pseudo-projective transformations. They are then processed by the parsing algorithm, which records each action necessary to build the correct structure in the training data, along with their corresponding parser configurations (stack and input list contents). From each of these parser configurations, a set of features is extracted and used with the correct parsing action as a training example for the maximum entropy classifier. The specific features we used in both experiments are the sam"
C08-1095,E06-1011,0,0.100265,"Missing"
C08-1095,H05-1059,1,0.658499,"Missing"
C08-1095,H05-1066,0,0.645671,"pproach that is nearly as simple as current data-driven transition-based dependency parsing frameworks, but outputs directed acyclic graphs (DAGs). We demonstrate the benefits of DAG parsing in two experiments where its advantages over dependency tree parsing can be clearly observed: predicate-argument analysis of English and syntactic analysis of Danish with a representation that includes long-distance dependencies and anaphoric reference links. 1 Introduction Natural language parsing with data-driven dependency-based frameworks has received an increasing amount of attention in recent years (McDonald et al., 2005; Buchholz and Marsi, 2006; Nivre et al., 2006). Dependency representations directly reflect word-to-word relation † This work was conducted while the author was at the Computer Science Department of the University of Tokyo. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0). Some rights reserved. ships in a dependency graph, where the words in a sentence are the nodes, and labeled edges correspond to head-dependent syntactic relations. In addition to being inherently lexicalized, dependency"
C08-1095,D07-1003,0,0.0117498,"conducted while the author was at the Computer Science Department of the University of Tokyo. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0). Some rights reserved. ships in a dependency graph, where the words in a sentence are the nodes, and labeled edges correspond to head-dependent syntactic relations. In addition to being inherently lexicalized, dependency analyses can be generated efficiently and have been show to be useful in a variety of practical tasks, such as question answering (Wang et al., 2007), information extraction in biomedical text (Erkan et al., 2007; Saetre et al, 2007) and machine translation (Quirk and Corston-Oliver, 2006). However, despite rapid progress in the development of parsers for several languages (Nivre et al., 2007) and algorithms for more linguistically adequate non-projective structures (McDonald et al., 2005; Nivre and Nilsson, 2006), most of the current data-driven dependency parsing approaches are limited to producing only dependency trees, where each word has exactly one head. Although trees have desirable properties from both computational and linguistic"
C08-1095,W04-0308,0,\N,Missing
C08-1095,P07-1079,1,\N,Missing
C10-1097,P08-1024,0,0.0294485,"al model based on pause duration and trigram part-of-speech frequency. The model was constructed by identifying, from the HCRC Map Task Corpus (Anderson et al., 1991), trigrams ending with a backchannel. Fujie et al. (2004) used Hidden Markov Models to perform head nod recognition. In their paper, they combined head gesture detection with prosodic low-level features from the same person to determine strongly positive, weak positive and negative responses to yes/no type utterances. In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al., 2008). One of the most relevant works is that of Eisenstein and Davis (2007), which presents a latent conditional model for fusion of multiple modalities (speech and gestures). One of the key difference of our work is that we are explicitly modeling the micro dynamics and temporal relationship between modalities. 3 Multimodal Prediction Models Human face-to-face communication is a little like a dance, in that participants continuously adjust their behaviors based on verbal and nonverbal dis861 plays and signals. A topic of central interest in modeling such behaviors is the patterning of interlocuto"
C10-1097,P08-1097,0,0.0223883,"onfirming the importance of combining different types of multimodal features. We show that our LMDE model outperforms previous approaches based Conditional Random Fields (CRFs) and Latent-Dynamic CRFs. 2 Related Work Earlier work in multimodal language processing focused on multimodal dialogue systems where the gestures and speech may be constrained (Johnston, 1998; Jurafsky et al., 1998). Most of the research in multimodal language processing over the past decade fits within two main trends that have emerged: (1) recognition of individual multimodal actions such as speech and gestures (e.g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)). The work described in this paper can be seen from a third intermediate category where multimodal cues from one person is used to predict the social behavior of another participant. This type of predictive models has been mostly studied in the context of embodied conversational agents (Nakano et al., 2003; Nakano et al., 2007). In pa"
C10-1097,P07-1045,0,0.0663785,"Missing"
C10-1097,H94-1020,0,0.112976,"tions, we included all individual words (i.e., unigrams) spoken by the speaker during the interactions. S YNTACTIC STRUCTURE Finally, we attempt to capture syntactic information that may provide relevant cues by extracting four types of features from a syntactic dependency structure corresponding to the utterance. The syntactic structure is produced automatically using a CRF partof-speech (POS) tagger and a data-driven left-toright shift-reduce dependency parser (Sagae and Tsujii, 2007), both trained on POS tags and dependency trees extracted from the Switchboard section of the Penn Treebank (Marcus et al., 1994), converted to dependency trees using the Penn2Malt tool1 . The four syntactic features are: 864 • Part-of-speech tags for each word (e.g. noun, verb, etc.), taken from the output of the POS tagger • Grammatical function for each word (e.g. subject, object, etc.), taken directly from the dependency labels produced by the parser • Part-of-speech of the syntactic head of each word, taken from the dependency links produced by the parser • Distance and direction from each word to its syntactic head, computed from the dependency links produced by the parser 1 http://w3.msi.vxu.se/ nivre/research/Pe"
C10-1097,D09-1140,0,0.0252282,"timodal language processing focused on multimodal dialogue systems where the gestures and speech may be constrained (Johnston, 1998; Jurafsky et al., 1998). Most of the research in multimodal language processing over the past decade fits within two main trends that have emerged: (1) recognition of individual multimodal actions such as speech and gestures (e.g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)). The work described in this paper can be seen from a third intermediate category where multimodal cues from one person is used to predict the social behavior of another participant. This type of predictive models has been mostly studied in the context of embodied conversational agents (Nakano et al., 2003; Nakano et al., 2007). In particular, backchannel feedback (the nods and paraverbals such as “uh-hu” and “mm-hmm” that listeners produce as someone is speaking) has received considerable interest due to its pervasiveness across languages and conversational contexts"
C10-1097,D09-1118,0,0.0577146,"Missing"
C10-1097,P03-1070,0,0.0283658,"eech and gestures (e.g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)). The work described in this paper can be seen from a third intermediate category where multimodal cues from one person is used to predict the social behavior of another participant. This type of predictive models has been mostly studied in the context of embodied conversational agents (Nakano et al., 2003; Nakano et al., 2007). In particular, backchannel feedback (the nods and paraverbals such as “uh-hu” and “mm-hmm” that listeners produce as someone is speaking) has received considerable interest due to its pervasiveness across languages and conversational contexts and this paper addresses the problem of how to predict and generate this important class of dyadic nonverbal behavior. Several researchers have developed models to predict when backchannel should happen. In general, these results are difficult to compare as they utilize different corpora and present varying evaluation metrics. Ward"
C10-1097,P07-2031,0,0.0199849,"g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)). The work described in this paper can be seen from a third intermediate category where multimodal cues from one person is used to predict the social behavior of another participant. This type of predictive models has been mostly studied in the context of embodied conversational agents (Nakano et al., 2003; Nakano et al., 2007). In particular, backchannel feedback (the nods and paraverbals such as “uh-hu” and “mm-hmm” that listeners produce as someone is speaking) has received considerable interest due to its pervasiveness across languages and conversational contexts and this paper addresses the problem of how to predict and generate this important class of dyadic nonverbal behavior. Several researchers have developed models to predict when backchannel should happen. In general, these results are difficult to compare as they utilize different corpora and present varying evaluation metrics. Ward and Tsukahara (2000)"
C10-1097,P07-1101,0,0.0238199,"types of multimodal features. We show that our LMDE model outperforms previous approaches based Conditional Random Fields (CRFs) and Latent-Dynamic CRFs. 2 Related Work Earlier work in multimodal language processing focused on multimodal dialogue systems where the gestures and speech may be constrained (Johnston, 1998; Jurafsky et al., 1998). Most of the research in multimodal language processing over the past decade fits within two main trends that have emerged: (1) recognition of individual multimodal actions such as speech and gestures (e.g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)). The work described in this paper can be seen from a third intermediate category where multimodal cues from one person is used to predict the social behavior of another participant. This type of predictive models has been mostly studied in the context of embodied conversational agents (Nakano et al., 2003; Nakano et al., 2007). In particular, backchannel feedback (the nods and p"
C10-1097,W07-1903,0,0.0623161,"Missing"
C10-1097,D07-1111,1,0.815075,"me studies have suggested an association between lexical features and listener feedback (Cathcart et al., 2003). Using the transcriptions, we included all individual words (i.e., unigrams) spoken by the speaker during the interactions. S YNTACTIC STRUCTURE Finally, we attempt to capture syntactic information that may provide relevant cues by extracting four types of features from a syntactic dependency structure corresponding to the utterance. The syntactic structure is produced automatically using a CRF partof-speech (POS) tagger and a data-driven left-toright shift-reduce dependency parser (Sagae and Tsujii, 2007), both trained on POS tags and dependency trees extracted from the Switchboard section of the Penn Treebank (Marcus et al., 1994), converted to dependency trees using the Penn2Malt tool1 . The four syntactic features are: 864 • Part-of-speech tags for each word (e.g. noun, verb, etc.), taken from the output of the POS tagger • Grammatical function for each word (e.g. subject, object, etc.), taken directly from the dependency labels produced by the parser • Part-of-speech of the syntactic head of each word, taken from the dependency links produced by the parser • Distance and direction from eac"
C10-1097,P05-1003,0,0.12531,"sture class corresponds to a state label. (See Figure 3a). M ULTIMODAL CLASSIFIERS ( EARLY FUSION ) Our second baseline consists of two models: CRF and LDCRF (Morency et al., 2007). To train these models, we concatenate all multimodal features (lexical, syntactic, prosodic and visual) in one input vector. Graphical representation of these baseline models are given in Figure 3. CRF M IXTURE OF EXPERTS To show the importance of latent variable in our LMDE model, we trained a CRF-based mixture of discriminative experts. This model is similar to the Logarithmic Opinion Pool (LOP) CRF suggested by Smith et al. (2005). The training is performed in two steps. A graphical representation of a CRF Mixture of experts is given in the last graph of Figure 3. 5.4 Methodology We performed held-out testing by randomly selecting a subset of 11 interactions (out of 47) for the test set. The training set contains the remaining 36 dyadic interactions. All models in this paper were evaluated with the same training and test sets. Validation of all model parameters (regularization term and number of hidden states) was performed using a 3-fold cross-validation strategy on the training set. The regularization term was valida"
C10-1097,E06-1022,0,0.0674659,"Missing"
C10-1097,W98-0319,0,0.100375,"processing: (1) temporal synchrony/asynchrony between modalities, (2) micro dynamics and (3) integration of different levels of interpretation. We present an empirical evaluation on nonverbal feedback prediction (e.g., head nod) confirming the importance of combining different types of multimodal features. We show that our LMDE model outperforms previous approaches based Conditional Random Fields (CRFs) and Latent-Dynamic CRFs. 2 Related Work Earlier work in multimodal language processing focused on multimodal dialogue systems where the gestures and speech may be constrained (Johnston, 1998; Jurafsky et al., 1998). Most of the research in multimodal language processing over the past decade fits within two main trends that have emerged: (1) recognition of individual multimodal actions such as speech and gestures (e.g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)). The work described in this paper can be seen from a third intermediate category where multimodal cues"
C14-1203,A00-2018,0,0.60072,"of child language development as the carefully constructed inventory of grammatical structures in IPSyn–we first implemented an automated version of IPSyn following Sagae et al. (2005), who showed that this task can be performed nearly at the level of trained human experts. This allows us to generate IPSyn scores for a large set of child language transcripts. Our implementation differs from previous work mainly in that it uses only the tools provided in the CLAN 2152 software suite (MacWhinney, 2000), which were designed specifically for analysis of child language transcripts, instead of the Charniak (2000) parser, which was used by Sagae et al. and later by Hassanali et al. (2014) in a more recent implementation of the same general approach. We evaluated our implementation using the set of 20 manually scored transcripts described by Sagae et al. as Set A, and subsequently used to evaluate the implementation of Hassanali et al. Three transcripts were used as development data, following Sagae et al. The mean absolute difference between manually generated and automatically generated scores was 3.6, which is very similar to what has been reported by Hassanali et al. and by Sagae et al. (3.05 and 3."
C14-1203,J93-2004,0,0.0456302,"Missing"
C14-1203,W07-1001,0,0.582636,"ses where MLU plateaus, but their increased expressivity is typically associated with two severe drawbacks. The first is that their use for computation of language development scores involves identification of several specific grammatical structures in child language transcripts, a process that requires linguistic expertise and is both time-consuming and error-prone. This issue has been addressed by recent work that shows that current natural language processing techniques can be applied to automate the computation of these metrics, removing the bottleneck of manual labor (Sagae et al., 2005; Roark et al., 2007; Sahakian and Snyder, 2012). The second drawback is that these measures are language-specific, and development of a measure for a specific language requires deep expertise and careful design of an inventory of grammatical structures that researchers believe to be indicative of language development. Going beyond previous work, which addressed the first drawback of traditional metrics for child language development, we address the second, paving the way for a language-independent methodology for tracking child language development that is as expressive as current language-specific alternatives,"
C14-1203,P05-1025,1,0.749281,"to show score increases where MLU plateaus, but their increased expressivity is typically associated with two severe drawbacks. The first is that their use for computation of language development scores involves identification of several specific grammatical structures in child language transcripts, a process that requires linguistic expertise and is both time-consuming and error-prone. This issue has been addressed by recent work that shows that current natural language processing techniques can be applied to automate the computation of these metrics, removing the bottleneck of manual labor (Sagae et al., 2005; Roark et al., 2007; Sahakian and Snyder, 2012). The second drawback is that these measures are language-specific, and development of a measure for a specific language requires deep expertise and careful design of an inventory of grammatical structures that researchers believe to be indicative of language development. Going beyond previous work, which addressed the first drawback of traditional metrics for child language development, we address the second, paving the way for a language-independent methodology for tracking child language development that is as expressive as current language-sp"
C14-1203,P12-2019,0,0.537645,"us, but their increased expressivity is typically associated with two severe drawbacks. The first is that their use for computation of language development scores involves identification of several specific grammatical structures in child language transcripts, a process that requires linguistic expertise and is both time-consuming and error-prone. This issue has been addressed by recent work that shows that current natural language processing techniques can be applied to automate the computation of these metrics, removing the bottleneck of manual labor (Sagae et al., 2005; Roark et al., 2007; Sahakian and Snyder, 2012). The second drawback is that these measures are language-specific, and development of a measure for a specific language requires deep expertise and careful design of an inventory of grammatical structures that researchers believe to be indicative of language development. Going beyond previous work, which addressed the first drawback of traditional metrics for child language development, we address the second, paving the way for a language-independent methodology for tracking child language development that is as expressive as current language-specific alternatives, but without the need for ca"
C14-1203,W06-2920,0,\N,Missing
C14-1203,D07-1096,0,\N,Missing
D07-1111,J93-1002,0,0.0200513,"stack, and an LR table is consulted to determine the next parser action. In our case, the parser state is encoded as a set of features derived from the contents of the stack S and queue Q, and the next parser action is determined according to that set of features. In the deterministic case described above, the procedure used for determining parser actions (a classifier, in our case) returns a single action. If, instead, this procedure returns a list of several possible actions with corresponding probabilities, we can then parse with a model similar to the probabilistic LR models described by Briscoe and Carroll (1993), where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation. To find the most probable parse tree according to the probabilistic LR model, we use a best-first strategy. This involves an extension of the deterministic shift-reduce into a best-first shift-reduce algorithm. To describe this extension, we first introduce a new data structure Ti that represents a parser state, which includes a stack Si, a queue Qi, and a probability Pi. The deterministic algorithm is a special case of the probabilistic algorithm where we have a single p"
D07-1111,W06-2920,0,0.0818631,"ng tree voting scheme. In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-ofdomain training set, in a scheme similar to one iteration of co-training. 1 2. We generalize the standard deterministic stepwise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Ratnaparkhi (1997) and later by Sagae and Lavie (2006); Introduction There are now several approaches for multilingual dependency parsing, as demonstrated in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). The dependency parsing approach presented here extends the existing body of work mainly in four ways: 1. Although stepwise 1 dependency parsing has commonly been performed using parsing algo1 Stepwise parsing considers each step in a parsing algorithm separately, while all-pairs parsing considers entire 3. We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or chan"
D07-1111,W07-2416,0,0.0384768,"Missing"
D07-1111,W03-1018,1,0.677446,"w is inserted into the heap H. Once new states have been inserted onto H for each of the n parser actions, we move on to the next iteration of the algorithm. 3 Multilingual Parsing Experiments For each of the ten languages for which training data was provided in the multilingual track of the CoNLL 2007 shared task, we trained three LR models as follows. The first LR model for each language uses maximum entropy classification (Berger et al., 1996) to determine possible parser actions and their probabilities4. To control overfitting in the MaxEnt models, we used box-type inequality constraints (Kazama and Tsujii, 2003). The second LR model for each language also uses MaxEnt classification, but parsing is performed backwards, which is accomplished simply by reversing the input string before parsing starts. Sagae and Lavie (2006a) and Zeman and Žabokrtský (2005) have observed that reversing the direction of stepwise parsers can be beneficial in parser combinations. The third model uses support vector machines 5 (Vapnik, 1995) using the polynomial 4 Implementation by Yoshimasa Tsuruoka, available at http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/maxent/ 5 Implementation by Taku Kudo, available at http://chasen"
D07-1111,W04-3111,0,0.0185485,"Missing"
D07-1111,P05-1012,0,0.187099,"Missing"
D07-1111,W03-3017,0,0.0428782,"Missing"
D07-1111,P05-1013,0,0.0215164,"use a classifier with features derived from much of the same information contained in an LR table: the top few items on the stack, and the next few items of lookahead in the remaining input string. Additionally, following Sagae and Lavie (2006), we extend the basic deterministic LR algorithm with a bestfirst search, which results in a parsing strategy similar to generalized LR parsing (Tomita, 1987; 1990), except that we do not perform Tomita’s stack-merging operations. The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005). We use Nivre and Nilsson’s PATH scheme2. For clarity, we first describe the basic variant of the LR algorithm for dependency parsing, which is a deterministic stepwise algorithm. We then show how we extend the deterministic parser into a bestfirst probabilistic parser. 2.1 Dependency Parsing with a Data-Driven Variant of the LR Algorithm The two main data structures in the algorithm are a stack S and a queue Q. S holds subtrees of the final dependency tree for an input sentence, and Q holds the words in an input sentence. S is initialized to be empty, and Q is initialized to hold every word"
D07-1111,W97-0301,0,0.0476573,"different learners. In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme. In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-ofdomain training set, in a scheme similar to one iteration of co-training. 1 2. We generalize the standard deterministic stepwise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Ratnaparkhi (1997) and later by Sagae and Lavie (2006); Introduction There are now several approaches for multilingual dependency parsing, as demonstrated in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). The dependency parsing approach presented here extends the existing body of work mainly in four ways: 1. Although stepwise 1 dependency parsing has commonly been performed using parsing algo1 Stepwise parsing considers each step in a parsing algorithm separately, while all-pairs parsing considers entire 3. We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (200"
D07-1111,N06-2033,1,0.74441,"ilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme. In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-ofdomain training set, in a scheme similar to one iteration of co-training. 1 2. We generalize the standard deterministic stepwise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Ratnaparkhi (1997) and later by Sagae and Lavie (2006); Introduction There are now several approaches for multilingual dependency parsing, as demonstrated in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). The dependency parsing approach presented here extends the existing body of work mainly in four ways: 1. Although stepwise 1 dependency parsing has commonly been performed using parsing algo1 Stepwise parsing considers each step in a parsing algorithm separately, while all-pairs parsing considers entire 3. We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing a"
D07-1111,J87-1004,0,0.10801,"ous in parser 1045 ensembles. The main difference between our parser and a traditional LR parser is that we do not use an LR table derived from an explicit grammar to determine shift/reduce actions. Instead, we use a classifier with features derived from much of the same information contained in an LR table: the top few items on the stack, and the next few items of lookahead in the remaining input string. Additionally, following Sagae and Lavie (2006), we extend the basic deterministic LR algorithm with a bestfirst search, which results in a parsing strategy similar to generalized LR parsing (Tomita, 1987; 1990), except that we do not perform Tomita’s stack-merging operations. The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005). We use Nivre and Nilsson’s PATH scheme2. For clarity, we first describe the basic variant of the LR algorithm for dependency parsing, which is a deterministic stepwise algorithm. We then show how we extend the deterministic parser into a bestfirst probabilistic parser. 2.1 Dependency Parsing with a Data-Driven Variant of the LR Algorithm The two main data structures in the"
D07-1111,W03-3023,0,0.0851389,"Missing"
D07-1111,W05-1518,0,0.066031,"ata was provided in the multilingual track of the CoNLL 2007 shared task, we trained three LR models as follows. The first LR model for each language uses maximum entropy classification (Berger et al., 1996) to determine possible parser actions and their probabilities4. To control overfitting in the MaxEnt models, we used box-type inequality constraints (Kazama and Tsujii, 2003). The second LR model for each language also uses MaxEnt classification, but parsing is performed backwards, which is accomplished simply by reversing the input string before parsing starts. Sagae and Lavie (2006a) and Zeman and Žabokrtský (2005) have observed that reversing the direction of stepwise parsers can be beneficial in parser combinations. The third model uses support vector machines 5 (Vapnik, 1995) using the polynomial 4 Implementation by Yoshimasa Tsuruoka, available at http://www-tsujii.is.s.u-tokyo.ac.jp/~tsuruoka/maxent/ 5 Implementation by Taku Kudo, available at http://chasen.org/~taku/software/TinySVM/ and all vs. all was used for multi-class classification. kernel with degree 2. Probabilities were estimated for SVM outputs using the method described in (Platt, 1999), but accuracy improvements were not observed duri"
D07-1111,J93-2004,0,\N,Missing
D07-1111,J96-1002,0,\N,Missing
D07-1111,P06-2089,1,\N,Missing
D07-1111,D07-1096,0,\N,Missing
D07-1111,C90-1012,0,\N,Missing
georgila-etal-2012-practical,W10-4318,1,\N,Missing
I11-1150,J96-1002,0,0.022832,", and we evaluate the performance of the rules using both hand-annotated or “gold” SA (G-SA) as well as automatically assigned NLU SAs (NLU-SA), as described in Section 3.1. For the two statistical policy techniques, MaxEnt and RM, the user’s utterance may be represented in SA form or in plain text form. In the latter case, the NLU and DM modules are effectively consolidated into a single classification step. 3.3 3.1 3.4 NLU Our NLU module treats the problem of mapping an utterance text to a single SA label as a multiclass classification problem, which it solves using a maximum-entropy model (Berger et al., 1996). The utterance is represented using shallow features such as unigrams and the length of the user utterance (Sagae et al., 2009). Paraphrases of user utterances are included in the training set. 3.2 Rule-based Policy We developed our rule-based policy (Rules) by manually writing the simple rules needed to implement Amani’s dialogue policy. Given a user SA label At for turn t, the rules for determining Amani’s response Rt take one of three forms: if At = SAi then Rt = SAj if At = SAi ∧ ∃k At−k = SAl then Rt = SAj if At = SAi ∧ ¬∃k At−k = SAl then Rt = SAj MaxEnt Policy Like the NLU, the MaxEnt"
I11-1150,W11-2006,1,0.848247,"nally, as we illustrate in this paper, its performance can depend critically on the reliability of the NLU module. As an alternative, we contrast this design with a direct classification approach that relies only on textual examples and effectively combines the dialogue policy with NLU. In our case study evaluation, we find that this approach offers superior performance, owing to the high frequency of NLU errors in the two step pipeline. The research presented in this paper extends our previous work. As we summarize in Section 2, this paper relies on the same data set and evaluation metric as DeVault et al. (2011), which reports results for learned policies based on maximum entropy models. In this paper, we add a comparison to a hand-authored policy (Rules) and a new policy based on relevance models (RM). These new policies are described in Section 3. We conclude with some discussion of our new findings. 2 Research Setting and Data Set We begin by summarizing our research setting, data set, and evaluation metric. We refer the reader to DeVault et al. (2011) for additional details. We use an existing virtual human scenario designed for Tactical Questioning (TACQ) (Traum et al., 2008), where military per"
I11-1150,2005.sigdial-1.25,1,0.78644,"alogue policy is capable of high performance if perfect natural language understanding is assumed, a direct classification approach that combines the dialogue policy with NLU has practical advantages. 1 Introduction In this paper we present and evaluate a set of alternative dialogue system architectures that could be used to implement dialogue policies for conversational characters or virtual humans. The motivation for this work is to improve our understanding of the development costs and performance benefits associated with alternative system architectures for virtual human dialogue systems (Traum et al., 2005; Swartout et al., 2006; Kenny et al., 2009; Jan et al., 2009; Swartout et al., 2010). We focus on the language processing steps used in a specific virtual human system described in Section 2. We analyze the relationship between Natural Language Understanding (NLU), which maps a user’s natural language input to systemspecific semantic representations, and Dialogue Management (DM), which executes a dialogue policy that dictates what the virtual human will say or do in response to the user’s input. Traditionally, designing a two step NLU+DM pipeline involves defining semantic representations for"
I11-1150,W06-1303,1,0.84266,"viewed as a cross-language information retrieval (IR) task: we have a fixed collection of system SAs (“documents”) and a user’s input (“query”), and we need to find the best SA that matches the user’s input. This is similar to the task of searching Chinese documents using an English query, where the training data that maps user inputs to the system SAs can be viewed as a “parallel corpus” (Lavrenko et al., 2002). For our third approach we use the Relevance Model (RM) information retrieval technique first suggested by Lavrenko et al. (2002) and recently adapted to a question-answering task by Leuski et al. (2006). We have experimented with different feature sets and we found that (1) when the text data is not available, the combination of the current user SA and the last system SA is the most 1343 Utterance Features G-SA NLU-SA NLU-SA+Text Text Policy models Rules MaxEnt RM .79 .71 .73 .58 .57 .60 .65 .66 .71 Table 1: Weak accuracy results for alternative system architectures. effective; (2) when both the utterance text and SA are available, the combination of the current user SA and unigram text features from all available paraphrases works the best; and (3) when only the text is available, the unigr"
I11-1150,N09-2014,1,0.8511,"U SAs (NLU-SA), as described in Section 3.1. For the two statistical policy techniques, MaxEnt and RM, the user’s utterance may be represented in SA form or in plain text form. In the latter case, the NLU and DM modules are effectively consolidated into a single classification step. 3.3 3.1 3.4 NLU Our NLU module treats the problem of mapping an utterance text to a single SA label as a multiclass classification problem, which it solves using a maximum-entropy model (Berger et al., 1996). The utterance is represented using shallow features such as unigrams and the length of the user utterance (Sagae et al., 2009). Paraphrases of user utterances are included in the training set. 3.2 Rule-based Policy We developed our rule-based policy (Rules) by manually writing the simple rules needed to implement Amani’s dialogue policy. Given a user SA label At for turn t, the rules for determining Amani’s response Rt take one of three forms: if At = SAi then Rt = SAj if At = SAi ∧ ∃k At−k = SAl then Rt = SAj if At = SAi ∧ ¬∃k At−k = SAl then Rt = SAj MaxEnt Policy Like the NLU, the MaxEnt policy is based on a multi-class maximum-entropy classifier. It uses text-based features including unigrams and the length of th"
lavie-etal-2004-significance,niessen-etal-2000-evaluation,0,\N,Missing
lavie-etal-2004-significance,N03-2021,0,\N,Missing
lavie-etal-2004-significance,P02-1040,0,\N,Missing
lavie-etal-2004-significance,C92-2067,0,\N,Missing
lavie-etal-2004-significance,N03-1024,0,\N,Missing
lavie-etal-2004-significance,N03-1020,0,\N,Missing
lavie-etal-2004-significance,2003.mtsummit-papers.9,0,\N,Missing
lavie-etal-2004-significance,2001.mtsummit-papers.3,0,\N,Missing
N06-2033,P05-1022,0,0.227474,"Missing"
N06-2033,C96-1058,0,0.551723,"ach of the individual parsers. This is done in a two stage process of reparsing. In the first stage, m different parsers analyze an input sentence, each producing a syntactic structure. In the second stage, a parsing algorithm is applied to the original sentence, taking into account the analyses produced by each parser in the first stage. Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set. 2 Dependency Reparsing In dependency reparsing we focus on unlabeled dependencies, as described by Eisner (1996). In this scheme, the syntactic structure for a sentence with n words is a dependency tree representing head-dependent relations between pairs of words. When m parsers each output a set of dependencies (forming m dependency structures) for a given sentence containing n words, the dependencies can be combined in a simple wordby-word voting scheme, where each parser votes for the head of each of the n words in the sentence, and the head with most votes is assigned to each word. This very simple scheme guarantees that the final set of dependencies will have as many votes as possible, but it does"
N06-2033,P04-1013,0,0.0284939,"Missing"
N06-2033,W99-0623,0,0.801181,"ll is accomplished by discarding every constituent with weight below a threshold t before the search for the final parse tree starts. In the simple case where each constituent starts out with weight 1.0 (before any merging), this means that a constituent is only considered for inclusion in the final parse tree if it appears in at least t of the m initial parse trees. Intuitively, this should increase precision, since we expect that a constituent that appears in the output of more parsers to be more likely to be correct. By changing the threshold t we can control the precision/recall tradeoff. Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees. The latter scheme performed better, producing remarkable results despite its simplicity. The combination is done with a simple majority vote of whether or not constituents should appear in the combined tree. In other words, if a constituent appears at least (m + 1)/2 times in the output of the m parsers, the constituent is added to the final tree. This simple vote resulted in trees with f-score significantly higher"
N06-2033,J93-2004,0,0.0308004,"lon University Pittsburgh, PA 15213 {sagae,alavie@cs.cmu.edu} Abstract We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers. We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers. 1 Introduction Over the past decade, remarkable progress has been made in data-driven parsing. Much of this work has been fueled by the availability of large corpora annotated with syntactic structures, especially the Penn Treebank (Marcus et al., 1993). In fact, years of extensive research on training and testing parsers on the Wall Street Journal (WSJ) corpus of the Penn Treebank have resulted in the availability of several high-accuracy parsers. We present a framework for combining the output of several different accurate parsers to produce results that are superior to those of each of the individual parsers. This is done in a two stage process of reparsing. In the first stage, m different parsers analyze an input sentence, each producing a syntactic structure. In the second stage, a parsing algorithm is applied to the original sentence,"
N06-2033,H05-1066,0,0.578386,"ge should be created, the corresponding weights are simply added. As long as at least one of the m initial structures is a well-formed dependency structure, the directed graph created this way will be connected. 1 Determining the weights is discussed in section 4.1. 129 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 129–132, c New York, June 2006. 2006 Association for Computational Linguistics Once this graph is created, we reparse the sentence using a dependency parsing algorithm such as, for example, one of the algorithms described by McDonald et al. (2005). Finding the optimal dependency structure given the set of weighted dependencies is simply a matter of finding the maximum spanning tree (MST) for the directed weighted graph, which can be done using the Chu-Liu/Edmonds directed MST algorithm (Chu & Liu, 1965; Edmonds, 1967). The maximum spanning tree maximizes the votes for dependencies given the constraint that the resulting structure must be a tree. If projectivity (no crossing branches) is desired, Eisner’s (1996) dynamic programming algorithm (similar to CYK) for dependency parsing can be used instead. 3 Constituent Reparsing In constitu"
N06-2033,C04-1010,0,0.0313552,"vote resulted in trees with f-score significantly higher than the one of the best parser in the combination. However, the scheme heavily favors precision over recall. Their results on WSJ section 23 were 92.1 precision and 89.2 recall (90.61 f-score), well above the most accurate parser in their experiments (88.6 f-score). 4 Experiments In our dependency parsing experiments we used unlabeled dependencies extracted from the Penn Treebank using the same head-table as Yamada and Matsumoto (2003), using sections 02-21 as training data and section 23 as test data, following (McDonald et al., 2005; Nivre & Scholz, 2004; Yamada & Matsumoto, 2003). Dependencies extracted from section 00 were used as held-out data, and section 22 was used as additional development data. For constituent parsing, we used the section splits of the Penn Treebank as described above, as has become standard in statistical parsing research. 4.1 Dependency Reparsing Experiments Six dependency parsers were used in our combination experiments, as described below. The deterministic shift-reduce parsing algorithm of (Nivre & Scholz, 2004) was used to create two parsers2, one that processes the input sentence from left-to-right (LR), and on"
N06-2033,W05-1513,1,0.871599,"Missing"
N06-2033,W03-3023,0,0.427944,"stituent appears at least (m + 1)/2 times in the output of the m parsers, the constituent is added to the final tree. This simple vote resulted in trees with f-score significantly higher than the one of the best parser in the combination. However, the scheme heavily favors precision over recall. Their results on WSJ section 23 were 92.1 precision and 89.2 recall (90.61 f-score), well above the most accurate parser in their experiments (88.6 f-score). 4 Experiments In our dependency parsing experiments we used unlabeled dependencies extracted from the Penn Treebank using the same head-table as Yamada and Matsumoto (2003), using sections 02-21 as training data and section 23 as test data, following (McDonald et al., 2005; Nivre & Scholz, 2004; Yamada & Matsumoto, 2003). Dependencies extracted from section 00 were used as held-out data, and section 22 was used as additional development data. For constituent parsing, we used the section splits of the Penn Treebank as described above, as has become standard in statistical parsing research. 4.1 Dependency Reparsing Experiments Six dependency parsers were used in our combination experiments, as described below. The deterministic shift-reduce parsing algorithm of (N"
N06-2033,W05-1518,0,0.435934,"s. When m parsers each output a set of dependencies (forming m dependency structures) for a given sentence containing n words, the dependencies can be combined in a simple wordby-word voting scheme, where each parser votes for the head of each of the n words in the sentence, and the head with most votes is assigned to each word. This very simple scheme guarantees that the final set of dependencies will have as many votes as possible, but it does not guarantee that the final voted set of dependencies will be a well-formed dependency tree. In fact, the resulting graph may not even be connected. Zeman & Žabokrtský (2005) apply this dependency voting scheme to Czech with very strong results. However, when the constraint that structures must be well-formed is enforced, the accuracy of their results drops sharply. Instead, if we reparse the sentence based on the output of the m parsers, we can maximize the number of votes for a well-formed dependency structure. Once we have obtained the m initial dependency structures to be combined, the first step is to build a graph where each word in the sentence is a node. We then create weighted directed edges between the nodes corresponding to words for which dependencies"
N09-2014,J96-1002,0,0.0137797,"ys generators for electricity downtown • ASR (NLU input): we up apparently give you guys generators for a letter city don town • Frame (NLU output): &lt;s>.mood declarative &lt;s>.sem.agent kirk &lt;s>.sem.event deliver &lt;s>.sem.modal.possibility can &lt;s>.sem.speechact.type offer &lt;s>.sem.theme power-generator &lt;s>.sem.type event The original NLU component for this system was described in (Leuski and Traum, 2008). For the purposes of this experiment, we have developed a new NLU module and tested on several different data sets as described in the next section. Our approach is to use maximum entropy models (Berger et al., 1996) to learn a suitable mapping from features derived from the words in the ASR output to semantic frames. Given a set of examples of semantic frames with corresponding ASR output, a classifier should learn, for example, that when “generators” appears in the output of ASR, the value power-generators is likely to be present in the output frame. The specific features used by the classifier are: each word in the input string (bag-of-words representation of the input), each bigram (consecutive words), each pair of any two words in the input, and the number of words in the input string. 54 60 450 400"
N09-2014,E09-1085,0,0.121738,"ing that can be unnatural and inefficient for mixed-initiative dialogue. To achieve more flexible turn-taking with human users, for whom turn-taking and feedback at the subutterance level is natural and common, the system needs to engage in incremental processing, in which interpretation components are activated, and in some cases decisions are made, before the user utterance is complete. There is a growing body of work on incremental processing in dialogue systems. Some of this work has demonstrated overall improvements in system responsiveness and user satisfaction; e.g. (Aist et al., 2007; Skantze and Schlangen, 2009). Several 53 research groups, inspired by psycholinguistic models of human processing, have also been exploring technical frameworks that allow diverse contextual information to be brought to bear during incremental processing; e.g. (Kruijff et al., 2007; Aist et al., 2007). While this work often assumes or suggests it is possible for systems to understand partial user utterances, this premise has generally not been given detailed quantitative study. The contribution of this paper is to demonstrate and explore quantitatively the extent to which one specific dialogue system can anticipate what"
N10-2009,J96-1002,0,0.0191004,"June 2010. 2010 Association for Computational Linguistics  mood :declarative    type : event     agent : captain − kirk          sem :  event : deliver  theme : power − generator           modal : possibility : can    speech − act : type : of f er  2.2 Figure 1: AVM utterance representation. manual transcription, and a gold-standard semantic frame, allowing us to develop and evaluate a datadriven NLU approach. 2.1 NLU in SASO-EN Virtual Humans Our NLU module for the SASO-EN system, mxNLU (Sagae et al., 2009), is based on maximum entropy classification (Berger et al., 1996) , where we treat entire individual semantic frames as classes, and extract input features from ASR. The NLU output representation is an attribute-value matrix (AVM), where the attributes and values represent semantic information that is linked to a domainspecific ontology and task model (Figure 1). The AVMs are linearized, using a path-value notation, as seen in the NLU input-output example below: • Utterance (speech): we are prepared to give you guys generators for electricity downtown • ASR (NLU input): we up apparently give you guys generators for a letter city don town • Frame (NLU output"
N10-2009,W09-3902,1,0.939406,"alogue management only after the utterance is complete. This results in a rigid and often unnatural pacing where the system must wait until the user stops speaking before trying to understand and react to user input. To achieve more flexible turn-taking with human users, for whom turn-taking and feedback at the sub-utterance level is natural, the system needs the ability to start interpretation of user utterances before they are completed. We demonstrate an implementation of techniques we have developed for partial utterance understanding in virtual human dialogue systems (Sagae et al., 2009; DeVault et al., 2009) with the goal of equipping these systems with sophisticated conversational behavior, such as interruptions and non-verbal feedback. Our demonstration highlights the understanding of utterances before they are finished. It also includes an utterance completion capability, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (20"
N10-2009,N09-2014,1,0.943393,"tanding (NLU) and dialogue management only after the utterance is complete. This results in a rigid and often unnatural pacing where the system must wait until the user stops speaking before trying to understand and react to user input. To achieve more flexible turn-taking with human users, for whom turn-taking and feedback at the sub-utterance level is natural, the system needs the ability to start interpretation of user utterances before they are completed. We demonstrate an implementation of techniques we have developed for partial utterance understanding in virtual human dialogue systems (Sagae et al., 2009; DeVault et al., 2009) with the goal of equipping these systems with sophisticated conversational behavior, such as interruptions and non-verbal feedback. Our demonstration highlights the understanding of utterances before they are finished. It also includes an utterance completion capability, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed ("
N10-2009,W09-3905,0,0.0130926,"Our demonstration highlights the understanding of utterances before they are finished. It also includes an utterance completion capability, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (2009), Kruijff et al. (2007)), new possible metrics for evaluating the performance of incremental processing are being proposed (e.g. Schlangen et al. (2009)), and the advantages for dialogue system performance and usability are starting to be empirically quantified (e.g. Skantze and Schlangen (2009), Aist et al. (2007)). 2 NLU for partial utterances In previous work (Sagae et al., 2009), we presented an approach for prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding. In subsequent work (DeVault et al., 2009), we incorporated additional features of real-time incremental interpretation to develop a more nuanced prediction"
N10-2009,J09-3001,0,0.0153395,"ault et al., 2009) with the goal of equipping these systems with sophisticated conversational behavior, such as interruptions and non-verbal feedback. Our demonstration highlights the understanding of utterances before they are finished. It also includes an utterance completion capability, where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (2009), Kruijff et al. (2007)), new possible metrics for evaluating the performance of incremental processing are being proposed (e.g. Schlangen et al. (2009)), and the advantages for dialogue system performance and usability are starting to be empirically quantified (e.g. Skantze and Schlangen (2009), Aist et al. (2007)). 2 NLU for partial utterances In previous work (Sagae et al., 2009), we presented an approach for prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding. In s"
N10-2009,E09-1085,0,0.0614968,", where a virtual human can make a strategic decision to display its understanding of an unfinished user utterance by completing the utterance itself. The work we demonstrate here is part of a growing research area in which new technical approaches to incremental utterance processing are being developed (e.g. Schuler et al. (2009), Kruijff et al. (2007)), new possible metrics for evaluating the performance of incremental processing are being proposed (e.g. Schlangen et al. (2009)), and the advantages for dialogue system performance and usability are starting to be empirically quantified (e.g. Skantze and Schlangen (2009), Aist et al. (2007)). 2 NLU for partial utterances In previous work (Sagae et al., 2009), we presented an approach for prediction of semantic content from partial speech recognition hypotheses, looking at length of the speech hypothesis as a general indicator of semantic accuracy in understanding. In subsequent work (DeVault et al., 2009), we incorporated additional features of real-time incremental interpretation to develop a more nuanced prediction model that can accurately identify moments of maximal understanding within individual spoken utterances. This research was conducted in the cont"
N16-1027,J07-4004,0,0.221427,"Missing"
N16-1027,W02-2203,0,0.0729714,"Missing"
N16-1027,J07-3004,0,0.452572,"Missing"
N16-1027,D14-1107,0,0.0386462,"omputes normalized supertag probabilities. Although bidirectional LSTMs can capture long distance interactions between words, each output label is predicted independently. To explicitly model supertag interactions, our next model combines two models, the bi–LSTM and a LSTM language model (LM) over the supertags (Figure 1). At position 233 … … … hLM i +2 t i +1 ti Feed-Forward For both POS tagging and our baseline supertagging model, we use feed-forward neural networks with two hidden layers of rectified linear units (Nair and Hinton, 2010). For supertagging, we use a slightly smaller set than Lewis and Steedman (2014a), using a left and right 3-word window with suffix and capitalization features for the center word. However, unlike them, we train on the full set of supertag categories observed during training. In POS tagging, when tagging word wi , we consider only features from a window of five words, with wi at the center. For each wj with i − 2 ≤ j ≤ i + 2, we add wj lowercased and a string that encodes the basic “word shape” of wj . This is computed by replacing all sequences of uppercase letters with A, all sequences of lowercase letters with a, all sequences of digits with 9, and all sequences of ot"
N16-1027,Q14-1026,0,0.134879,"omputes normalized supertag probabilities. Although bidirectional LSTMs can capture long distance interactions between words, each output label is predicted independently. To explicitly model supertag interactions, our next model combines two models, the bi–LSTM and a LSTM language model (LM) over the supertags (Figure 1). At position 233 … … … hLM i +2 t i +1 ti Feed-Forward For both POS tagging and our baseline supertagging model, we use feed-forward neural networks with two hidden layers of rectified linear units (Nair and Hinton, 2010). For supertagging, we use a slightly smaller set than Lewis and Steedman (2014a), using a left and right 3-word window with suffix and capitalization features for the center word. However, unlike them, we train on the full set of supertag categories observed during training. In POS tagging, when tagging word wi , we consider only features from a window of five words, with wi at the center. For each wj with i − 2 ≤ j ≤ i + 2, we add wj lowercased and a string that encodes the basic “word shape” of wj . This is computed by replacing all sequences of uppercase letters with A, all sequences of lowercase letters with a, all sequences of digits with 9, and all sequences of ot"
N16-1027,N16-1026,0,0.357722,"Missing"
N16-1027,J93-2004,0,0.0922645,"Missing"
N16-1027,P11-2009,0,0.0486496,"Missing"
N16-1027,D15-1176,0,\N,Missing
N16-1027,P15-2041,0,\N,Missing
P05-1025,briscoe-carroll-2002-robust,0,0.0433703,"Missing"
P05-1025,C02-1013,0,0.025729,"Missing"
P05-1025,A00-2018,0,0.0971946,"ian MacWhinney Department of Psychology Carnegie Mellon University Pittsburgh, PA 15232 macw@cmu.edu Kenji Sagae and Alon Lavie Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15232 {sagae,alavie}@cs.cmu.edu Abstract To facilitate the use of syntactic information in the study of child language acquisition, a coding scheme for Grammatical Relations (GRs) in transcripts of parent-child dialogs has been proposed by Sagae, MacWhinney and Lavie (2004). We discuss the use of current NLP techniques to produce the GRs in this annotation scheme. By using a statistical parser (Charniak, 2000) and memorybased learning tools for classification (Daelemans et al., 2004), we obtain high precision and recall of several GRs. We demonstrate the usefulness of this approach by performing automatic measurements of syntactic development with the Index of Productive Syntax (Scarborough, 1990) at similar levels to what child language researchers compute manually. 1 Introduction Automatic syntactic analysis of natural language has benefited greatly from statistical and corpus-based approaches in the past decade. The availability of syntactically annotated data has fueled the development of high"
P05-1025,P96-1025,0,0.0209002,"nlabeled Dependency Identification Once we have isolated the text that should be analyzed in each sentence, we parse it to obtain unlabeled dependencies. Although we ultimately need labeled dependencies, our choice to produce unlabeled structures first (and label them in a later step) is motivated by available resources. Unlabeled dependencies can be readily obtained by processing constituent trees, such as those in the Penn Treebank (Marcus et al., 1993), with a set of rules to determine the lexical heads of constituents. This lexicalization procedure is commonly used in statistical parsing (Collins, 1996) and produces a dependency tree. This dependency extraction procedure from constituent trees gives us a straightforward way to obtain unlabeled dependencies: use an existing statistical parser (Charniak, 2000) trained on the Penn Treebank to produce constituent trees, and extract unlabeled dependencies using the aforementioned head-finding rules. Our target data (transcribed child language) is 199 from a very different domain than the one of the data used to train the statistical parser (the Wall Street Journal section of the Penn Treebank), but the degradation in the parser’s accuracy is acce"
P05-1025,P02-1017,0,0.0116555,"ter obtaining unlabeled dependencies as described above, we proceed to label those dependencies with the GR labels listed in Figure 2. Determining the labels of dependencies is in general an easier task than finding unlabeled dependencies in text.3 Using a classifier, we can choose one of the 30 possible GR labels for each dependency, given a set of features derived from the dependencies. Although we need manually labeled data to train the classifier for labeling dependencies, the size of this training set is far smaller than what would be necessary to train a parser to find labeled dependen3 Klein and Manning (2002) offer an informal argument that constituent labels are much more easily separable in multidimensional space than constituents/distituents. The same argument applies to dependencies and their labels. cies in one pass. We use a corpus of about 5,000 words with manually labeled dependencies to train TiMBL (Daelemans et al., 2003), a memory-based learner (set to use the k-nn algorithm with k=1, and gain ratio weighing), to classify each dependency with a GR label. We extract the following features for each dependency: • The head and dependent words; • The head and dependent parts-of-speech; • Whe"
P05-1025,J93-2004,0,0.0240471,"art-of-speech tagger (Parisse and Le Normand, 2000). This results in fairly clean sentences, accompanied by full morphological and part-of-speech analyses. 3.2 Unlabeled Dependency Identification Once we have isolated the text that should be analyzed in each sentence, we parse it to obtain unlabeled dependencies. Although we ultimately need labeled dependencies, our choice to produce unlabeled structures first (and label them in a later step) is motivated by available resources. Unlabeled dependencies can be readily obtained by processing constituent trees, such as those in the Penn Treebank (Marcus et al., 1993), with a set of rules to determine the lexical heads of constituents. This lexicalization procedure is commonly used in statistical parsing (Collins, 1996) and produces a dependency tree. This dependency extraction procedure from constituent trees gives us a straightforward way to obtain unlabeled dependencies: use an existing statistical parser (Charniak, 2000) trained on the Penn Treebank to produce constituent trees, and extract unlabeled dependencies using the aforementioned head-finding rules. Our target data (transcribed child language) is 199 from a very different domain than the one of"
P05-1025,C04-1010,0,0.0232112,"Missing"
P05-1025,sagae-etal-2004-adding,1,0.937275,"technologies. Similarly, in the study of child language, the availability of large amounts of electronically accessible empirical data in the form of child language transcripts has been shifting much of the research effort towards a corpus-based mentality. However, child language researchers have only recently begun to utilize modern NLP techniques for syntactic analysis. Although it is now common for researchers to rely on automatic morphosyntactic analyses of transcripts to obtain part-of-speech and morphological analyses, their use of syntactic parsing is rare. Sagae, MacWhinney and Lavie (2004) have proposed a syntactic annotation scheme for the CHILDES database (MacWhinney, 2000), which contains hundreds of megabytes of transcript data and has been used in over 1,500 studies in child language acquisition and developmental language disorders. This annotation scheme focuses on syntactic structures of particular importance in the study of child language. In this paper, we describe the use of existing NLP tools to parse child language transcripts and produce automatically annotated data in the format of the scheme of Sagae et al. We also validate the usefulness of the annotation scheme"
P05-1025,P95-1037,0,\N,Missing
P06-1054,W00-0730,0,0.0330866,"g., parentheses). In Chinese, such pairs are more frequent (quotes, single quotes, and book-name marks). During parsing, we note how many opening puncClassification is the key component of our parsing model. We conducted experiments with four different types of classifiers. 3.1 Classifiers Support Vector Machine: Support Vector Machine is a discriminative classification technique which solves the binary classification problem by finding a hyperplane in a high dimensional space that gives the maximum soft margin, based on the Structural Risk Minimization Principle. We used the TinySVM toolkit (Kudo and Matsumoto, 2000), with a degree 2 polynomial kernel. To train a multi-class classifier, we used the one-against-all scheme. Maximum-Entropy Classifier: In a Maximum-entropy model, the goal is to estimate a set of parameters that would maximize the entropy over distributions that satisfy certain constraints. These constraints will force the model to best account for the training data (Ratnaparkhi, 1999). Maximum-entropy models have been used for Chinese character-based parsing (Fung et al., 2004; Luo, 2003) and POS tagging (Ng and Low, 2004). In our experiments, we used Le’s Maxent toolkit (Zhang, 2004). This"
P06-1054,P03-1056,0,0.167959,"Missing"
P06-1054,W03-1025,0,0.0236967,"margin, based on the Structural Risk Minimization Principle. We used the TinySVM toolkit (Kudo and Matsumoto, 2000), with a degree 2 polynomial kernel. To train a multi-class classifier, we used the one-against-all scheme. Maximum-Entropy Classifier: In a Maximum-entropy model, the goal is to estimate a set of parameters that would maximize the entropy over distributions that satisfy certain constraints. These constraints will force the model to best account for the training data (Ratnaparkhi, 1999). Maximum-entropy models have been used for Chinese character-based parsing (Fung et al., 2004; Luo, 2003) and POS tagging (Ng and Low, 2004). In our experiments, we used Le’s Maxent toolkit (Zhang, 2004). This implementation uses the Limited-Memory Variable Metric method for parameter estimation. We trained all our models using 300 iterations with no event cut-off, and a Gaussian prior smoothing value of 2. Maxent classifiers output not only a single class label, but 427 1 2 3 4 5 6 7 8 9 10 11 A Boolean feature indicates if a closing punctuation is expected or not. A Boolean value indicates if the queue is empty or not. A Boolean feature indicates whether there is a comma separating S(1) and S(2"
P06-1054,W04-3236,0,0.0264147,"ral Risk Minimization Principle. We used the TinySVM toolkit (Kudo and Matsumoto, 2000), with a degree 2 polynomial kernel. To train a multi-class classifier, we used the one-against-all scheme. Maximum-Entropy Classifier: In a Maximum-entropy model, the goal is to estimate a set of parameters that would maximize the entropy over distributions that satisfy certain constraints. These constraints will force the model to best account for the training data (Ratnaparkhi, 1999). Maximum-entropy models have been used for Chinese character-based parsing (Fung et al., 2004; Luo, 2003) and POS tagging (Ng and Low, 2004). In our experiments, we used Le’s Maxent toolkit (Zhang, 2004). This implementation uses the Limited-Memory Variable Metric method for parameter estimation. We trained all our models using 300 iterations with no event cut-off, and a Gaussian prior smoothing value of 2. Maxent classifiers output not only a single class label, but 427 1 2 3 4 5 6 7 8 9 10 11 A Boolean feature indicates if a closing punctuation is expected or not. A Boolean value indicates if the queue is empty or not. A Boolean feature indicates whether there is a comma separating S(1) and S(2) or not. Last action given by the"
P06-1054,W00-1201,0,0.526338,"Missing"
P06-1054,C04-1010,0,0.0568874,"Missing"
P06-1054,W05-1513,1,0.807595,"itute School of Computer Science Carnegie Mellon University {mengqiu,sagae,teruko}@cs.cmu.edu Abstract accuracy just below the state-of-the-art in syntactic analysis of English, but running in linear time (Sagae and Lavie, 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). Encouraging results have also been shown recently by Cheng et al. (2004; 2005) in applying deterministic models to Chinese dependency parsing. We present a novel classifier-based deterministic parser for Chinese constituency parsing. In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie, 2005), the parsing task is transformed into a succession of classification tasks. The parser makes one pass through the input sentence. At each parse state, it consults a classifier to make shift/reduce decisions. The parser then commits to a decision and enters the next parse state. Shift/reduce decisions are made deterministically based on the local context of each parse state, and no backtracking is involved. This process can be viewed as a greedy search where only one path in the whole search space is considered. Our parser produces both dependency and constituent structures, but in this paper"
P06-1054,W03-1706,0,0.241692,"Missing"
P06-1054,C02-1126,0,0.262806,"Missing"
P06-1054,N04-1032,0,0.0468719,"Missing"
P06-1054,I05-1007,0,0.0301619,"Missing"
P06-1054,W03-3023,0,0.0789132,"Missing"
P06-1054,W99-0623,0,0.0129553,"els; on the other hand, it also indicates that our deterministic model is less resilient to POS errors. Further detailed analysis is called for, to study the extent to which POS tagging errors affects the deterministic parsing model. Table 5: Comparison of parsing speed opens up lots of possibilities for continuous improvements, both in terms of accuracy and efficiency. For example, in this paper we experimented with one method of simple voting. An alternative way of doing simple voting is to let the parsers vote on membership of constituents after each parser has produced its own parse tree (Henderson and Brill, 1999), instead of voting at each step during parsing. Our initial attempt to increase the accuracy of the DTree model by applying boosting techniques did not yield satisfactory results. In our experiment, we implemented the AdaBoost.M1 (Freund and Schapire, 1996) algorithm using resampling to vary the training set distribution. Results showed AdaBoost suffered severe overfitting problems and hurts accuracy greatly, even with a small number of samples. One possible reason for this is that our sample space is very unbalanced across the different classes. A few classes have lots of training examples w"
P06-1054,J03-4003,0,\N,Missing
P06-2089,gimenez-marquez-2004-svmtool,0,0.0547486,"Missing"
P06-2089,C04-1010,0,0.173875,"asonable and commonly held assumption is that the accuracy of deterministic classifier-based parsers can be improved if determinism is abandoned in favor of a search over a larger space of possible parses. While this assumption was shown to be true for the parser of Tsuruoka and Tsujii (2005), only a moderate improvement resulted from the addition of a non-greedy search strategy, and overall parser accuracy was still well below that of state-of-the-art statistical parsers. We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. The parser retains much of the simplicity of deterministic classifier-based parsers, but achieves results that are closer in accuracy to state-of-the-art statistical parsers. Furthermore, a simple combination of the shift-reduce parsing model with an existing generative parsing model produces results with accuracy that surpasses any that of any single (nonreranked) parser tested on the WSJ Penn Treebank, and comes close to the best results obtained with discriminative reranking (Charniak and"
P06-2089,J96-1002,0,0.0103098,"ary and binary productions. REDUCE-RIGHT-XX : represents a binary reduce action, where the root of the new subtree pushed onto S is of non-terminal type XX. Additionally, the head of the new subtree is the same as the head of the right child of the root node. To implement a parser based on the best-first algorithm, instead of just using a classifier to determine one parser action given a stack and a queue, we need a classification approach that provides us with probabilities for different parser actions associated with a given parser state. One such approach is maximum entropy classification (Berger et al., 1996), which we use in the form of a library implemented by Tsuruoka1 and used in his classifier-based parser (Tsuruoka and Tsujii, 2005). We used the same classes and the same features as Sagae and Lavie, and an additional feature that represents the previous parser action applied the current parser state (figure 1). 3 Related Work 4 As mentioned in section 2, our parsing approach can be seen as an extension of the approach of Sagae and Lavie (2005). Sagae and Lavie evaluated their deterministic classifier-based parsing framework using two classifiers: support vector machines (SVM) and k-nearest n"
P06-2089,W97-0301,0,0.76175,"SHIFT : represents a shift action; REDUCE-UNARY-XX : represents a unary reduce action, where the root of the new subtree pushed onto S is of type XX (where XX is a non-terminal symbol, typically N P , V P , P P , for example); REDUCE-LEFT-XX : represents a binary reduce action, where the root of the new sub693 tree pushed onto S is of non-terminal type XX. Additionally, the head of the new subtree is the same as the head of the left child of the root node; is based on reframing the parsing task as several sequential chunking tasks. Finally, our parser is in many ways similar to the parser of Ratnaparkhi (1997). Ratnaparkhi’s parser uses maximum-entropy models to determine the actions of a parser based to some extent on the shift-reduce framework, and it is also capable of pursuing several paths and returning the topn highest scoring parses for a sentence. However, in addition to using different features for parsing, Ratnaparkhi’s parser uses a different, more complex algorithm. The use of a more involved algorithm allows Ratnaparkhi’s parser to work with arbitrary branching trees without the need of the binarization transform employed here. It breaks the usual reduce actions into smaller pieces (CH"
P06-2089,E03-1005,0,0.0143229,"Missing"
P06-2089,J93-1002,0,0.0215226,"the top of the heap, resulting in the desired search behavior. The accuracy of deterministic parsers suggest that this may in fact be the types of probabilities a classifier would produce given features that describe the parser state, and thus the context of the parser action, specifically enough. The experiments described in section 4 support this assumption. simply as the product of the probabilities of each action in the path that resulted in that parse tree (the derivation of the tree). This produces a probabilistic shift-reduce parser that resembles a generalized probabilistic LR parser (Briscoe and Carroll, 1993), where probabilities are associated with an LR parsing table. In our case, although there is no LR table, the action probabilities are associated with several aspects of the current state of the parser, which to some extent parallel the information contained in an LR table. Instead of having an explicit LR table and pushing LR states onto the stack, the state of the parser is implicitly defined by the configurations of the stack and queue. In a way, there is a parallel between how modern PCFG-like parsers use markov grammars as a distribution that is used to determine the probability of any p"
P06-2089,W05-1513,1,0.939065,"d the state-of-theart. A reasonable and commonly held assumption is that the accuracy of deterministic classifier-based parsers can be improved if determinism is abandoned in favor of a search over a larger space of possible parses. While this assumption was shown to be true for the parser of Tsuruoka and Tsujii (2005), only a moderate improvement resulted from the addition of a non-greedy search strategy, and overall parser accuracy was still well below that of state-of-the-art statistical parsers. We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. The parser retains much of the simplicity of deterministic classifier-based parsers, but achieves results that are closer in accuracy to state-of-the-art statistical parsers. Furthermore, a simple combination of the shift-reduce parsing model with an existing generative parsing model produces results with accuracy that surpasses any that of any single (nonreranked) parser tested on the WSJ Penn Treebank, and comes close to the best results obtained with discrimina"
P06-2089,P05-1022,0,0.127155,"Missing"
P06-2089,J87-1004,0,0.0773574,"lgorithm described in section 2.1 does not handle ambiguity. By choosing a single parser action at each opportunity, the input string is parsed deterministically, and a single constituent structure is built during the parsing process from beginning to end (no other structures are even considered). A simple extension to this idea is to eliminate determinism by allowing the parser to choose several actions at each opportunity, creating different paths that lead to different parse trees. This is essentially the difference between deterministic LR parsing (Knuth, 1965) and Generalized-LR parsing (Tomita, 1987; Tomita, 1990). Furthermore, if a probability is assigned to every parser action, the probability of a parse tree can be computed • Shift: A shift action consists only of removing (shifting) the first item (part-of-speechtagged word) from W (at which point the next word becomes the new first item), and placing it on top of S. 692 the most probable parse. To obtain a list of n-best parses, we simply continue parsing once the first parse tree is found, until either n trees are found, or H is empty. We note that this approach does not use dynamic programming, and relies only on the bestfirst sea"
P06-2089,W98-1115,0,0.0212347,"parser of Sagae and Lavie (2005). That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. The algorithm considers only trees with unary and binary productions. Training the parser with arbitrary branching trees is accomplished by a simple procedure to transform those trees into trees with at most binary productions. This is done by converting each production with n children, where n > 2, into n − 1 binary productions. This binarization process is similar to the one described in (Charniak et al., 1998). Additional nonterminal nodes introduced in this conversion must be clearly marked. Transforming the parser’s output into arbitrary branching trees is accomplished using the reverse process. The deterministic parsing algorithm involves two main data structures: a stack S, and a queue W . Items in S may be terminal nodes (part-ofspeech-tagged words), or (lexicalized) subtrees of the final parse tree for the input string. Items in W are terminals (words tagged with parts-of-speech) corresponding to the input string. When parsing begins, S is empty and W is initialized by inserting every word fr"
P06-2089,A00-2018,0,0.706361,"Missing"
P06-2089,W05-1514,0,0.0911745,"Missing"
P06-2089,W03-3023,0,0.0794049,"Missing"
P06-2089,P97-1003,0,0.172477,"Missing"
P06-2089,J93-2004,0,\N,Missing
P06-2089,W00-1604,0,\N,Missing
P06-2089,J03-4003,0,\N,Missing
P06-2089,C90-1012,0,\N,Missing
P07-1079,J99-2004,0,0.09211,"minutes, since two dependency parsers are used sequentially. 5 Related work There are other approaches that combine shallow processing with deep parsing (Crysmann et al., 2002; Frank et al., 2003; Daum et al., 2003) to improve parsing efficiency. Typically, shallow parsing is used to create robust minimal recursion semantics, which are used as constraints to limit ambiguity during parsing. Our approach, in contrast, uses syntactic dependencies to achieve a significant improvement in the accuracy of wide-coverage HPSG parsing. Additionally, our approach is in many ways similar to supertagging (Bangalore and Joshi, 1999), which uses sequence labeling techniques as an efficient way to pre-compute parsing constraints (specifically, the assignment of lexical entries to input words). 6 Conclusion We have presented a novel framework for taking advantage of the strengths of a shallow parsing approach and a deep parsing approach. We have shown that by constraining the application of rules in HPSG parsing according to results from a dependency parser, we can significantly improve the accuracy of deep parsing by using shallow syntactic analyses. To illustrate how this framework allows for improvements in the accuracy"
P07-1079,W06-2920,0,0.0263757,"uistically sophisticated formalism. In addition, improvements in dependency parsing accuracy are converted directly into improvements in HPSG parsing accuracy. From the point of view of dependency parsing, the application of HPSG rules to structures generated by a surface dependency model provides a principled and linguistically motivated way to identify deep syntactic phenomena, such as long-distance dependencies, raising and control. Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized Tree Adjoin- HPSG parsing approaches in section 2."
P07-1079,E03-1052,0,0.0224732,"Missing"
P07-1079,C96-1058,0,0.019801,"directly into improvements in HPSG parsing accuracy. From the point of view of dependency parsing, the application of HPSG rules to structures generated by a surface dependency model provides a principled and linguistically motivated way to identify deep syntactic phenomena, such as long-distance dependencies, raising and control. Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized Tree Adjoin- HPSG parsing approaches in section 2. In section ing Grammar (LTAG) (Schabes et al., 1988), Head- 3, we present our framework for HPS"
P07-1079,P03-1014,0,0.0366287,"Missing"
P07-1079,P06-1088,0,0.0377423,"Missing"
P07-1079,W02-2018,0,0.00702209,"s to lexical/phrasal structures, where L = hl1 , . . . , ln i are lexical entries and 625 p(li |W ) is the supertagging probability, i.e., the probability of assignining the lexical entry li to wi (Ninomiya et al., 2006). The probability p(T |L, W ) is a maximum entropy model on HPSG parse trees, where Z is a normalization factor, and feature functions fi (T ) represent syntactic characteristics, such as head words, lengths of phrases, and applied schemas. Given the HPSG treebank as training data, the model parameters λi are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). 3 HPSG parsing with dependency constraints While a number of fairly straightforward models can be applied successfully to dependency parsing, designing and training HPSG parsing models has been regarded as a significantly more complex task. Although it seems intuitive that a more sophisticated linguistic formalism should be more difficult to parameterize properly, we argue that the difference in complexity between HPSG and dependency structures can be seen as incremental, and that the use of accurate and efficient techniques to determine the surface dependency structure of a sentence provide"
P07-1079,J93-2004,0,0.0326234,"encies that roughly correspond to deep syntax. The second step is to perform HPSG parsing, as described in section 2.2, but using the shallow dependency tree to constrain the application of HPSG rules. We now discuss these two steps in more detail. 3.1 Determining shallow dependencies in HPSG structures using dependency parsing In order to apply a data-driven dependency approach to the task of identifying the shallow dependency tree in HPSG structures, we first need a corpus of such dependency trees to serve as training data. We created a dependency training corpus based on the Penn Treebank (Marcus et al., 1993), or more specifically on the HPSG Treebank generated from the Penn Treebank (see section 2.2). For each HPSG structure in the HPSG Treebank, a dependency tree is extracted in two steps. First, the HPSG tree is converted into a CFG-style tree, simply by removing long-distance dependency links between nodes. A dependency tree is then extracted from the resulting lexicalized CFG-style tree, as is commonly done for converting constituent trees into dependency trees after the application of a headpercolation table (Collins, 1999). Once a dependency training corpus is available, it is used to train"
P07-1079,H05-1066,0,0.208417,"a more complex and linguistically sophisticated formalism. In addition, improvements in dependency parsing accuracy are converted directly into improvements in HPSG parsing accuracy. From the point of view of dependency parsing, the application of HPSG rules to structures generated by a surface dependency model provides a principled and linguistically motivated way to identify deep syntactic phenomena, such as long-distance dependencies, raising and control. Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized Tree Adjoin- HPSG parsin"
P07-1079,P05-1011,1,0.918834,"accurate, but also efficient. The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz, 2004) has been shown to offer state-of-the-art accuracy (Nivre et al., 2006) with high efficiency due to a greedy search strategy. Our approach is based on Nivre and Scholz’s approach, using support vector machines for classification of shift/reduce actions. 2.2 Wide-coverage HPSG parsing Figure 2: Extracting HPSG lexical entries from the Penn Treebank we finally obtain an HPSG parse tree that covers the entire sentence. In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). This parser has a widecoverage HPSG lexicon which is extracted from the Penn Treebank. Figure 2 illustrates their method for extraction of HPSG lexical entries. First, given a parse tree from the Penn Treebank (top), HPSGstyle constraints are added and an HPSG-style parse tree is obtained (middle). Lexical entries are then extracted from the terminal nodes of the HPSG parse tree (bottom). This way, in addition to a widecoverage lexicon, we also obtain an HPSG treebank, which can be used as training data for disambiguation models. The disambiguation model of this parser is based on a maximum"
P07-1079,W06-1619,1,0.853739,"xical entries express subcategorization frames and predicate argument structures. Parsing proceeds by app(T |W ) = p(T |L, W )p(L|W ) plying schemas to lexical entries. In this example, ! X Y the Head-Complement Schema is applied to the lex1 = exp λ f (T ) p(lj |W ), i i ical entries of “tried” and “running”. We then obtain Z i j a phrasal structure for “tried running”. By repeatedly applying schemas to lexical/phrasal structures, where L = hl1 , . . . , ln i are lexical entries and 625 p(li |W ) is the supertagging probability, i.e., the probability of assignining the lexical entry li to wi (Ninomiya et al., 2006). The probability p(T |L, W ) is a maximum entropy model on HPSG parse trees, where Z is a normalization factor, and feature functions fi (T ) represent syntactic characteristics, such as head words, lengths of phrases, and applied schemas. Given the HPSG treebank as training data, the model parameters λi are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). 3 HPSG parsing with dependency constraints While a number of fairly straightforward models can be applied successfully to dependency parsing, designing and training HPSG parsing models has been regarded as"
P07-1079,C04-1010,0,0.410319,"dapting these models to a more complex and linguistically sophisticated formalism. In addition, improvements in dependency parsing accuracy are converted directly into improvements in HPSG parsing accuracy. From the point of view of dependency parsing, the application of HPSG rules to structures generated by a surface dependency model provides a principled and linguistically motivated way to identify deep syntactic phenomena, such as long-distance dependencies, raising and control. Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized T"
P07-1079,W06-2933,0,0.00677943,"Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Figure 1: HPSG parsing evaluate this framework empirically. Sections 5 and 6 discuss related work and conclusions. 2 2.1 Fast dependency parsing and wide-coverage HPSG parsing Data-driven dependency parsing Because we use dependency parsing as a step in deep parsing, it is important that we choose a parsing approach that is not only accurate, but also efficient. The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz, 2004) has been shown to offer state-of-the-art accuracy (Nivre et al., 2006) with high efficiency due to a greedy search strategy. Our approach is based on Nivre and Scholz’s approach, using support vector machines for classification of shift/reduce actions. 2.2 Wide-coverage HPSG parsing Figure 2: Extracting HPSG lexical entries from the Penn Treebank we finally obtain an HPSG parse tree that covers the entire sentence. In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). This parser has a widecoverage HPSG lexicon which is extracted from the Penn Treebank. Figure 2 illustrates their method for extraction of HPSG lexical entries. First, given a"
P07-1079,W05-1513,1,0.770179,"onverted into a CFG-style tree, simply by removing long-distance dependency links between nodes. A dependency tree is then extracted from the resulting lexicalized CFG-style tree, as is commonly done for converting constituent trees into dependency trees after the application of a headpercolation table (Collins, 1999). Once a dependency training corpus is available, it is used to train a dependency parser as described in section 2.1. This is done by training a classifier to determine parser actions based on local features that represent the current state of the parser (Nivre and Scholz, 2004; Sagae and Lavie, 2005). Training data for the classifier is obtained by applying the parsing algorithm over the training sentences (for which the correct dependency structures are known) and recording the appropriate parser actions that result in the formation of the correct dependency trees, coupled with the features that represent the state of the parser mentioned in section 2.1. An evaluation of the resulting dependency parser and its efficacy in aiding HPSG parsing is presented in section 4. 3.2 Parsing with dependency constraints Given a set of dependencies, the bottom-up process of HPSG parsing can be constra"
P07-1079,N06-2033,1,0.641306,"ser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). Using dependency constraints allows us to improve HPSG parsing accuracy simply by using an existing parser combination approach. As a first step, we train two additional parsers with the dependencies extracted from the HPSG Treebank. The first uses the same shiftreduce framework described in section 2.1, but it process the input from right to left (RL). This has been found to work well in previous work on depenˇ dency parser combination (Zeman and Zabokrtsk´ y, 2005; Sagae and Lavie, 2006). The second parser is MSTParser, the large-margin maximum spanning tree parser described in (McDonald et al., 2005)3 . We examine the use of two combination schemes: one using two parsers, and one using three parsers. The first combination approach is to keep only dependencies for which there is agreement between the two parsers. In other words, dependencies that are proposed by one parser but not the other are simply discarded. Using the left-to-right shift-reduce parser and MSTParser, we find that this results in very high precision of surface dependencies on the development data. In the se"
P07-1079,C88-2121,0,0.104986,"ral language using bilexical dependency relations (Eisner, 1996). Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar We begin by describing our dependency and (LFG) (Bresnan, 1982), Lexicalized Tree Adjoin- HPSG parsing approaches in section 2. In section ing Grammar (LTAG) (Schabes et al., 1988), Head- 3, we present our framework for HPSG parsing with driven Phrase Structure Grammar (HPSG) (Pollard shallow dependency constraints, and in section 4 we 624 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 624–631, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Figure 1: HPSG parsing evaluate this framework empirically. Sections 5 and 6 discuss related work and conclusions. 2 2.1 Fast dependency parsing and wide-coverage HPSG parsing Data-driven dependency parsing Because we use dependency parsing as a step"
P07-1079,W05-1518,0,0.0398449,"Missing"
P07-1079,J96-1002,0,\N,Missing
P07-1079,P02-1056,0,\N,Missing
P08-1006,P06-2006,0,0.0284334,"on, our system cannot be compared directly to the results reported by Erkan et al. (2007) and Katrenko and Adriaans (2006), while Sætre et al. (2007) presented better results than theirs in the same evaluation criterion. 5 Related Work Though the evaluation of syntactic parsers has been a major concern in the parsing community, and a couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation (Ringger et al., 2004; Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007; Miyao et al., 2007; Clegg and Shepherd, 2007; Pyysalo et al., 2007b; Pyysalo et al., 2007a; Sagae et al., 2008). Such evaluation requires gold standard data in an intermediate representation. However, it has been argued that the conversion of parsing results into an intermediate representation is difficult and far from perfect. The relationship between parsing accuracy and task accuracy has been obscure for many years. Quirk and Corston-Oliver (2006) investigated the impact of parsing accuracy on statistical MT. However, this work was only concerned with a single depe"
P08-1006,P04-1056,0,0.02889,"gher dependency accuracy when trained only with GENIA. We therefore only input GENIA as the training data for the retraining of dependency parsers. For the other parsers, we input the concatenation of WSJ and GENIA for the retraining, while the reranker of RERANK was not retrained due to its cost. Since the parsers other than NO-RERANK and RERANK require an external POS tagger, a WSJ-trained POS tagger is used with WSJtrained parsers, and geniatagger (Tsuruoka et al., 2005) is used with GENIA-retrained parsers. 4 Experiments 4.1 Experiment settings In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs. We use gold protein annotations given in the corpus. Multi-word protein names are concatenated and treated as single words. The accuracy is measured by abstract-wise 10-fold cross validation and the one-answer-per-occurrence criterion (Giuliano et al., 2006). A threshold for SVMs is moved to adjust the balance of precision and recall, and the maximum f-scores are reported for"
P08-1006,P05-1022,0,0.0663263,"parsing) using five different parse representations. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus,"
P08-1006,P04-1014,0,0.498927,"s show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often framework-specific and di"
P08-1006,P07-1032,0,0.0427625,"mpared directly to the results reported by Erkan et al. (2007) and Katrenko and Adriaans (2006), while Sætre et al. (2007) presented better results than theirs in the same evaluation criterion. 5 Related Work Though the evaluation of syntactic parsers has been a major concern in the parsing community, and a couple of works have recently presented the comparison of parsers based on different frameworks, their methods were based on the comparison of the parsing accuracy in terms of a certain intermediate parse representation (Ringger et al., 2004; Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007; Miyao et al., 2007; Clegg and Shepherd, 2007; Pyysalo et al., 2007b; Pyysalo et al., 2007a; Sagae et al., 2008). Such evaluation requires gold standard data in an intermediate representation. However, it has been argued that the conversion of parsing results into an intermediate representation is difficult and far from perfect. The relationship between parsing accuracy and task accuracy has been obscure for many years. Quirk and Corston-Oliver (2006) investigated the impact of parsing accuracy on statistical MT. However, this work was only concerned with a single dependency parser, and did n"
P08-1006,P02-1034,0,0.0398489,"ng classifiers (Katrenko and Adriaans, 2006; Erkan et al., 2007; Sætre et al., 2007). For the protein pair IL-8 and CXCR1 in Figure 4, a dependency parser outputs a dependency tree shown in Figure 1. From this dependency tree, we can extract a dependency path shown in Figure 5, which appears to be a strong clue in knowing that these proteins are mentioned as interacting. (dep_path (SBJ (ENTITY1 recognizes)) (rOBJ (recognizes ENTITY2))) Figure 6: Tree representation of a dependency path We follow the PPI extraction method of Sætre et al. (2007), which is based on SVMs with SubSet Tree Kernels (Collins and Duffy, 2002; Moschitti, 2006), while using different parsers and parse representations. Two types of features are incorporated in the classifier. The first is bag-of-words features, which are regarded as a strong baseline for IE systems. Lemmas of words before, between and after the pair of target proteins are included, and the linear kernel is used for these features. These features are commonly included in all of the models. Filtering by a stop-word list is not applied because this setting made the scores higher than Sætre et al. (2007)’s setting. The other type of feature is syntactic features. For de"
P08-1006,P97-1003,0,0.0850946,"ank. Penn Treebank-style phrase structure trees without function tags and empty nodes. This is the default output format for phrase structure parsers. We also create this representation by converting ENJU’s output by tree structure matching, although this conversion is not perfect because forms of PTB and ENJU’s output are not necessarily compatible. PTB Dependency trees of syntactic heads (Figure 8). This representation is obtained by converting PTB trees. We first determine lexical heads of nonterminal nodes by using Bikel’s implementation of Collins’ head detection algorithm9 (Bikel, 2004; Collins, 1997). We then convert lexicalized trees into dependencies between lexical heads. HD The Stanford dependency format (Figure 9). This format was originally proposed for extracting dependency relations useful for practical applications (de Marneffe et al., 2006). A program to convert PTB is attached to the Stanford parser. Although the concept looks similar to CoNLL, this representaSD 8 http://nlp.cs.lth.se/pennconverter/ http://www.cis.upenn.edu/˜dbikel/software. html 9 Figure 9: Stanford dependencies tion does not necessarily form a tree structure, and is designed to express more fine-grained relat"
P08-1006,de-marneffe-etal-2006-generating,0,0.148922,"Missing"
P08-1006,C96-1058,0,0.0205152,"for the sentence “IL-8 recognizes and activates CXCR1.” An advantage of dependency parsing is that dependency trees are a reasonable approximation of the semantics of sentences, and are readily usable in NLP applications. Furthermore, the efficiency of popular approaches to dependency parsing compare favorable with those of phrase structure parsing or deep parsing. While a number of approaches have been proposed for dependency parsing, this paper focuses on two typical methods. McDonald and Pereira (2006)’s dependency 1 parser, based on the Eisner algorithm for projective dependency parsing (Eisner, 1996) with the secondorder factorization. MST 1 http://sourceforge.net/projects/mstparser 47 Figure 2: Penn Treebank-style phrase structure tree Sagae and Tsujii (2007)’s dependency parser,2 based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005). KSDEP 2.2 Phrase structure parsing Owing largely to the Penn Treebank, the mainstream of data-driven parsing research has been dedicated to the phrase structure parsing. These parsers output Penn Treebank-style phrase structure trees, although function tags and empty categories are stri"
P08-1006,D07-1024,0,0.532677,"e for NLP researchers in choosing an appropriate parser for their purposes. In this paper, we present a comparative evaluation of syntactic parsers and their output representations based on different frameworks: dependency parsing, phrase structure parsing, and deep parsing. Our approach to parser evaluation is to measure accuracy improvement in the task of identifying protein-protein interaction (PPI) information in biomedical papers, by incorporating the output of different parsers as statistical features in a machine learning classifier (Yakushiji et al., 2005; Katrenko and Adriaans, 2006; Erkan et al., 2007; Sætre et al., 2007). PPI identification is a reasonable task for parser evaluation, because it is a typical information extraction (IE) application, and because recent studies have shown the effectiveness of syntactic parsing in this task. Since our evaluation method is applicable to any parser output, and is grounded in a real application, it allows for a fair comparison of syntactic parsers based on different frameworks. Parser evaluation in PPI extraction also illuminates domain portability. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) portion"
P08-1006,W01-0521,0,0.0556271,"parsing in this task. Since our evaluation method is applicable to any parser output, and is grounded in a real application, it allows for a fair comparison of syntactic parsers based on different frameworks. Parser evaluation in PPI extraction also illuminates domain portability. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) portion of the Penn Treebank, and high accuracy has been reported for WSJ text; however, these parsers rely on lexical information to attain high accuracy, and it has been criticized that these parsers may overfit to WSJ text (Gildea, 2001; 46 Proceedings of ACL-08: HLT, pages 46–54, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Klein and Manning, 2003). Another issue for discussion is the portability of training methods. When training data in the target domain is available, as is the case with the GENIA Treebank (Kim et al., 2003) for biomedical papers, a parser can be retrained to adapt to the target domain, and larger accuracy improvements are expected, if the training method is sufficiently general. We will examine these two aspects of domain portability by comparing the original parsers w"
P08-1006,E06-1051,0,0.19788,"used with GENIA-retrained parsers. 4 Experiments 4.1 Experiment settings In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs. We use gold protein annotations given in the corpus. Multi-word protein names are concatenated and treated as single words. The accuracy is measured by abstract-wise 10-fold cross validation and the one-answer-per-occurrence criterion (Giuliano et al., 2006). A threshold for SVMs is moved to adjust the balance of precision and recall, and the maximum f-scores are reported for each setting. 4.2 Comparison of accuracy improvements Tables 1 and 2 show the accuracy obtained by using the output of each parser in each parse representation. The row “baseline” indicates the accuracy obtained with bag-of-words features. Table 3 shows the time for parsing the entire AImed corpus, and Table 4 shows the time required for 10-fold cross validation with GENIA-retrained parsers. When using the original WSJ-trained parsers (Table 1), all parsers achieved almost t"
P08-1006,W07-2202,1,0.375843,"en used in parser evaluation and applications. PAS is a graph structure that represents syntactic/semantic relations among words (Figure 3). The concept is therefore similar to CoNLL dependencies, though PAS expresses deeper relations, and may include reentrant structures. In this work, we chose the two versions of the Enju parser (Miyao and Tsujii, 2008). The HPSG parser that consists of an HPSG grammar extracted from the Penn Treebank, and a maximum entropy model trained with an HPSG treebank derived from the Penn Treebank.7 ENJU The HPSG parser adapted to biomedical texts, by the method of Hara et al. (2007). Because this parser is trained with both WSJ and GENIA, we compare it parsers that are retrained with GENIA (see section 3.3). ENJU-GENIA 3 Evaluation Methodology In our approach to parser evaluation, we measure the accuracy of a PPI extraction system, in which 6 http://nlp.stanford.edu/software/lex-parser. shtml 7 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/ 48 the parser output is embedded as statistical features of a machine learning classifier. We run a classifier with features of every possible combination of a parser and a parse representation, by applying conversions between representat"
P08-1006,W07-2416,0,0.333541,"ntences, and are readily usable in NLP applications. Furthermore, the efficiency of popular approaches to dependency parsing compare favorable with those of phrase structure parsing or deep parsing. While a number of approaches have been proposed for dependency parsing, this paper focuses on two typical methods. McDonald and Pereira (2006)’s dependency 1 parser, based on the Eisner algorithm for projective dependency parsing (Eisner, 1996) with the secondorder factorization. MST 1 http://sourceforge.net/projects/mstparser 47 Figure 2: Penn Treebank-style phrase structure tree Sagae and Tsujii (2007)’s dependency parser,2 based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005). KSDEP 2.2 Phrase structure parsing Owing largely to the Penn Treebank, the mainstream of data-driven parsing research has been dedicated to the phrase structure parsing. These parsers output Penn Treebank-style phrase structure trees, although function tags and empty categories are stripped off (Figure 2). While most of the state-of-the-art parsers are based on probabilistic CFGs, the parameterization of the probabilistic model of each parser var"
P08-1006,N04-1013,0,0.601287,"uracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often f"
P08-1006,P03-1054,0,0.166071,"ructure parsing, or deep parsing) using five different parse representations. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of"
P08-1006,P05-1010,1,0.242557,"parser/ http://bllip.cs.brown.edu/resources.shtml 4 We set n = 50 in this paper. 5 http://nlp.cs.berkeley.edu/Main.html#Parsing This study demonstrates that IL-8 recognizes and activates CXCR1, CXCR2, and the Duffy antigen by distinct mechanisms. The molar ratio of serum retinol-binding protein (RBP) to transthyretin (TTR) is not useful to assess vitamin A status during infection in hospitalised children. Figure 3: Predicate argument structure timized automatically by assigning latent variables to each nonterminal node and estimating the parameters of the latent variables by the EM algorithm (Matsuzaki et al., 2005). Stanford’s unlexicalized parser (Klein and Manning, 2003).6 Unlike NO-RERANK, probabilities are not parameterized on lexical heads. Figure 4: Sentences including protein names SBJ OBJ ENTITY1(IL-8) −→ recognizes ←− ENTITY2(CXCR1) Figure 5: Dependency path STANFORD 2.3 Deep parsing Recent research developments have allowed for efficient and robust deep parsing of real-world texts (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). While deep parsers compute theory-specific syntactic/semantic structures, predicate argument structures (PAS) are often used in parser evaluation"
P08-1006,E06-1011,0,0.0491874,"inations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to"
P08-1006,J08-1002,1,0.858485,"f accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often framework-specific and differ from parser to parse"
P08-1006,E06-1015,0,0.0153065,"and Adriaans, 2006; Erkan et al., 2007; Sætre et al., 2007). For the protein pair IL-8 and CXCR1 in Figure 4, a dependency parser outputs a dependency tree shown in Figure 1. From this dependency tree, we can extract a dependency path shown in Figure 5, which appears to be a strong clue in knowing that these proteins are mentioned as interacting. (dep_path (SBJ (ENTITY1 recognizes)) (rOBJ (recognizes ENTITY2))) Figure 6: Tree representation of a dependency path We follow the PPI extraction method of Sætre et al. (2007), which is based on SVMs with SubSet Tree Kernels (Collins and Duffy, 2002; Moschitti, 2006), while using different parsers and parse representations. Two types of features are incorporated in the classifier. The first is bag-of-words features, which are regarded as a strong baseline for IE systems. Lemmas of words before, between and after the pair of target proteins are included, and the linear kernel is used for these features. These features are commonly included in all of the models. Filtering by a stop-word list is not applied because this setting made the scores higher than Sætre et al. (2007)’s setting. The other type of feature is syntactic features. For dependency-based par"
P08-1006,P05-1013,0,0.273658,"representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on"
P08-1006,N07-1051,0,0.0565978,"t parse representations. We run a PPI system with several combinations of parser and parse representation, and examine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treeban"
P08-1006,W07-1004,0,0.29972,"). This format was originally proposed for extracting dependency relations useful for practical applications (de Marneffe et al., 2006). A program to convert PTB is attached to the Stanford parser. Although the concept looks similar to CoNLL, this representaSD 8 http://nlp.cs.lth.se/pennconverter/ http://www.cis.upenn.edu/˜dbikel/software. html 9 Figure 9: Stanford dependencies tion does not necessarily form a tree structure, and is designed to express more fine-grained relations such as apposition. Research groups for biomedical NLP recently adopted this representation for corpus annotation (Pyysalo et al., 2007a) and parser evaluation (Clegg and Shepherd, 2007; Pyysalo et al., 2007b). Predicate-argument structures. This is the default output format for ENJU and ENJU-GENIA. PAS Although only CoNLL is available for dependency parsers, we can create four representations for the phrase structure parsers, and five for the deep parsers. Dotted arrows in Figure 7 indicate imperfect conversion, in which the conversion inherently introduces errors, and may decrease the accuracy. We should therefore take caution when comparing the results obtained by imperfect conversion. We also measure the accuracy obtained"
P08-1006,W06-1608,0,0.0654162,"n of the parsing accuracy in terms of a certain intermediate parse representation (Ringger et al., 2004; Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007; Miyao et al., 2007; Clegg and Shepherd, 2007; Pyysalo et al., 2007b; Pyysalo et al., 2007a; Sagae et al., 2008). Such evaluation requires gold standard data in an intermediate representation. However, it has been argued that the conversion of parsing results into an intermediate representation is difficult and far from perfect. The relationship between parsing accuracy and task accuracy has been obscure for many years. Quirk and Corston-Oliver (2006) investigated the impact of parsing accuracy on statistical MT. However, this work was only concerned with a single dependency parser, and did not focus on parsers based on different frameworks. 6 Conclusion and Future Work We have presented our attempts to evaluate syntactic parsers and their representations that are based on different frameworks; dependency parsing, phrase structure parsing, or deep parsing. The basic idea is to measure the accuracy improvements of the PPI extraction task by incorporating the parser output as statistical features of a machine learning classifier. Experiments"
P08-1006,ringger-etal-2004-using,0,0.102376,"owever, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, because parse representations are often framework-specific and differ from parser to parser (Ringger et al., 2004). The lack of such comparisons is a serious obstacle for NLP researchers in choosing an appropriate parser for their purposes. In this paper, we present a comparative evaluation of syntactic parsers and their output representations based on different frameworks: dependency parsing, phrase structure parsing, and deep parsing. Our approach to parser evaluation is to measure accuracy improvement in the task of identifying protein-protein interaction (PPI) information in biomedical papers, by incorporating the output of different parsers as statistical features in a machine learning classifier (Ya"
P08-1006,D07-1111,1,0.727784,"ine their impact on PPI identification accuracy. Our experiments show that the levels of accuracy obtained with these different parsers are similar, but that accuracy improvements vary when the parsers are retrained with domain-specific data. 1 Introduction Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). However, efforts to perform extensive comparisons of syntactic parsers based on different frameworks have been limited. The most popular method for parser comparison involves the direct measurement of the parser output accuracy in terms of metrics such as bracketing precision and recall, or dependency accuracy. This assumes the existence of a gold-standard test corpus, such as the Penn Treebank (Marcus et al., 1994). It is difficult to apply this method to compare parsers based on different frameworks, bec"
P08-1006,1993.iwpt-1.22,0,0.0459583,"e on par with each other, while parsing speed differs significantly. We also found that accuracy improvements vary when parsers are retrained with domainspecific data, indicating the importance of domain adaptation and the differences in the portability of parser training methods. Although we restricted ourselves to parsers trainable with Penn Treebank-style treebanks, our methodology can be applied to any English parsers. Candidates include RASP (Briscoe and Carroll, 53 2006), the C&C parser (Clark and Curran, 2004), the XLE parser (Kaplan et al., 2004), MINIPAR (Lin, 1998), and Link Parser (Sleator and Temperley, 1993; Pyysalo et al., 2006), but the domain adaptation of these parsers is not straightforward. It is also possible to evaluate unsupervised parsers, which is attractive since evaluation of such parsers with goldstandard data is extremely problematic. A major drawback of our methodology is that the evaluation is indirect and the results depend on a selected task and its settings. This indicates that different results might be obtained with other tasks. Hence, we cannot conclude the superiority of parsers/representations only with our results. In order to obtain general ideas on parser performance,"
P08-1006,A00-2018,0,\N,Missing
P08-1006,J93-2004,0,\N,Missing
P08-1006,J04-4004,0,\N,Missing
P10-1110,P89-1018,0,0.37637,"Missing"
P10-1110,A00-2018,0,0.284436,"implementation word 90.2 90.9 92.0 91.4 92.1 92.5 92.4 92.1 93.2 L Ja Ja − C Py C Ja C − time 0.12 0.15 − 0.11 0.04 0.49 0.21 − − comp. O(n2 ) O(n3 ) O(n4 ) O(n)‡ O(n) O(n5 ) O(n3 ) O(n2 )‡ O(n4 ) Table 3: Final test results on English (PTB). Our parser (in pure Python) has the highest accuracy among dependency parsers trained on the Treebank, and is also much faster than major parsers. † converted from constituency trees. C=C/C++, Py=Python, Ja=Java. Time is in seconds per sentence. Search spaces: ‡ linear; others exponential. (on a 3.2GHz Xeon CPU). Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower. Figure 8 shows the parse time in seconds for each test sentence. The observed time complexity of our DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly com"
P10-1110,P04-1015,0,0.840681,"ose dotproducts with the weight vector decide the best action (see Eqs. (1-3) in Fig. 1). 2.3 Beam Search and Early Update To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel. At each step we extend the states in the current beam by applying one of the three actions, and then choose the best b resulting states for the next step. Our dynamic programming algorithm also runs on top of beam search in practice. To train the model, we use the averaged perceptron algorithm (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the gold-standard action-sequence falls off the beam, with the rest of the sequence neglected.3 The intuition behind this strategy is that later mistakes are often caused by previous ones, and are irrelevant when the parser is on the wrong track. Dynamic programming turns out to be a great fit for early updating (see Section 4.3 for details). 3 Dynamic Programming (DP) 3.1 Merging Equivalent States The key observation for dynamic programming is to merge “equivalent states” in the same beam 3 As a special case, for the d"
P10-1110,W02-1001,0,0.299237,"ined feature instances, whose dotproducts with the weight vector decide the best action (see Eqs. (1-3) in Fig. 1). 2.3 Beam Search and Early Update To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel. At each step we extend the states in the current beam by applying one of the three actions, and then choose the best b resulting states for the next step. Our dynamic programming algorithm also runs on top of beam search in practice. To train the model, we use the averaged perceptron algorithm (Collins, 2002). Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the gold-standard action-sequence falls off the beam, with the rest of the sequence neglected.3 The intuition behind this strategy is that later mistakes are often caused by previous ones, and are irrelevant when the parser is on the wrong track. Dynamic programming turns out to be a great fit for early updating (see Section 4.3 for details). 3 Dynamic Programming (DP) 3.1 Merging Equivalent States The key observation for dynamic programming is to merge “equivalent states” in the same"
P10-1110,P81-1022,0,0.822739,"Way Marina del Rey, CA 90292 sagae@ict.usc.edu Abstract that runs in (almost) linear-time, yet searches over a huge space with dynamic programming? Theoretically, the answer is negative, as Lee (2002) shows that context-free parsing can be used to compute matrix multiplication, where sub-cubic algorithms are largely impractical. We instead propose a dynamic programming alogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice. The key idea is to merge equivalent stacks according to feature functions, inspired by Earley parsing (Earley, 1970; Stolcke, 1995) and generalized LR parsing (Tomita, 1991). However, our formalism is more flexible and our algorithm more practical. Specifically, we make the following contributions: Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on featu"
P10-1110,P99-1059,0,0.381041,"e inside cost of the current state p, and the combo cost. Note the prefix cost of q is irrelevant. The combo cost δ = ξ ′ + λ consists of shift cost ξ ′ of p and reduction cost λ of q. The cost in the non-DP shift-reduce algorithm (Fig. 1) is indeed a prefix cost, and the DP algorithm subsumes the non-DP one as a special case where no two states are equivalent. 3.5 Example: Edge-Factored Model As a concrete example, Figure 4 simulates an edge-factored model (Eisner, 1996; McDonald et al., 2005a) using shift-reduce with dynamic programming, which is similar to bilexical PCFG parsing using CKY (Eisner and Satta, 1999). Here the kernel feature function is e f (j, S) = (j, h(s1 ), h(s0 )) 5 Note that using inside cost v for ordering would be a bad idea, as it will always prefer shorter derivations like in best-first parsing. As in A* search, we need some estimate of “outside cost” to predict which states are more promising, and the prefix cost includes an exact cost for the left outside context, but no right outside context. 1081 ℓ : h , h i : (c, ) quality; in Sec. 4.3 we will retrain the model with DP and compare it against training with non-DP. ...j j<n ℓ + 1 : hh, ji : (c, 0) sh ′′ ′ ′ ′ : hh , h i : (c"
P10-1110,C96-1058,0,0.954766,"combo cost δ from the combination, while the resulting prefix cost c′ + v + δ is the sum of the prefix cost of the predictor state q, the inside cost of the current state p, and the combo cost. Note the prefix cost of q is irrelevant. The combo cost δ = ξ ′ + λ consists of shift cost ξ ′ of p and reduction cost λ of q. The cost in the non-DP shift-reduce algorithm (Fig. 1) is indeed a prefix cost, and the DP algorithm subsumes the non-DP one as a special case where no two states are equivalent. 3.5 Example: Edge-Factored Model As a concrete example, Figure 4 simulates an edge-factored model (Eisner, 1996; McDonald et al., 2005a) using shift-reduce with dynamic programming, which is similar to bilexical PCFG parsing using CKY (Eisner and Satta, 1999). Here the kernel feature function is e f (j, S) = (j, h(s1 ), h(s0 )) 5 Note that using inside cost v for ordering would be a bad idea, as it will always prefer shorter derivations like in best-first parsing. As in A* search, we need some estimate of “outside cost” to predict which states are more promising, and the prefix cost includes an exact cost for the left outside context, but no right outside context. 1081 ℓ : h , h i : (c, ) quality; in S"
P10-1110,W05-1506,1,0.790183,"k and Johnson, 2005; Huang, 2008).8 4.3 Perceptron Training and Early Updates Another interesting advantage of DP over non-DP is the faster training with perceptron, even when both parsers use the same beam width. This is due to the use of early updates (see Sec. 2.3), which happen much more often with DP, because a goldstandard state p is often merged with an equivalent (but incorrect) state that has a higher model score, which triggers update immediately. By contrast, in non-DP beam search, states such as p might still 8 DP’s k-best lists are extracted from the forest using the algorithm of Huang and Chiang (2005), rather than those in the final beam as in the non-DP case, because many derivations have been merged during dynamic programming. 1082 b=16 dependency accuracy DP non-DP 0 avg. model score b=64 93.1 93 92.9 92.8 92.7 92.6 92.5 92.4 92.3 92.2 0.05 0.1 0.15 0.2 0.25 0.3 0.35 b=16 b=64 DP non-DP 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 (a) search quality vs. time (full model) (b) parsing accuracy vs. time (full model) 2335 2330 2325 2320 2315 2310 2305 2300 2295 2290 93.5 93 92.5 92 91.5 91 90.5 full, DP 90 full, non-DP 89.5 edge-factor, DP 89 edge-factor, non-DP 88.5 2280 2300 2320 2340 2360 2380 2400"
P10-1110,D09-1127,1,0.86707,"ing. Shift-Reduce Parsing 2.1 Vanilla Shift-Reduce Shift-reduce parsing performs a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items at the end of the stack (Aho and Ullman, 1972). To adapt it to dependency parsing, we split the reduce action into two cases, rex and rey, depending on which one of the two items becomes the head after reduction. This procedure is known as “arc-standard” (Nivre, 2004), and has been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state hj, Si where S is a stack of trees s0 , s1 , ... where s0 is the top tree, and j is the 2 stack Figure 2: A trace of vanilla shift-reduce. After step (4), the parser branches off into (5a) or (5b). ℓ : hj, S|s1 |s0 i : c where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: 2 action sh sh rex sh rey sh There is another popular variant, “arc-eager” (Nivre, 2004; Zhang and Clark, 2008), which is more complicated and less similar to the classical"
P10-1110,P08-1067,1,0.84341,"Fig. 5d shows the (almost linear) correlation between dependency accuracy and search quality, confirming that better search yields better parsing. 4.2 Search Space, Forest, and Oracles DP achieves better search quality because it expores an exponentially large search space rather than only b trees allowed by the beam (see Fig. 6a). As a by-product, DP can output a forest encoding these exponentially many trees, out of which we can draw longer and better (in terms of oracle) kbest lists than those in the beam (see Fig. 6b). The forest itself has an oracle of 98.15 (as if k → ∞), computed a` la Huang (2008, Sec. 4.1). These candidate sets may be used for reranking (Charniak and Johnson, 2005; Huang, 2008).8 4.3 Perceptron Training and Early Updates Another interesting advantage of DP over non-DP is the faster training with perceptron, even when both parsers use the same beam width. This is due to the use of early updates (see Sec. 2.3), which happen much more often with DP, because a goldstandard state p is often merged with an equivalent (but incorrect) state that has a higher model score, which triggers update immediately. By contrast, in non-DP beam search, states such as p might still 8 DP’"
P10-1110,J98-4004,0,0.0478774,"window and can only extract bounded information on each tree, which is always the case in practice since we can not have infinite models. Monotonicity, on the other hand, says that features drawn from trees farther away from the top should not be more refined than from those closer to the top. This is also natural, since the information most relevant to the current decision is always around the stack top. For example, the kernel feature function in Eq. 5 is bounded and monotonic, since f2 is less refined than f1 and f0 . These two requirements are related to grammar refinement by annotation (Johnson, 1998), where annotations must be bounded and monotonic: for example, one cannot refine a grammar by only remembering the grandparent but not the parent symbol. The difference here is that the annotations are not vertical ((grand-)parent), but rather horizontal (left context). For instance, a context-free rule A → B C would become D A → D B B C for some D if there exists a rule E → αDAβ. This resembles the reduce step in Fig. 3. The very high-level idea of the proof is that boundedness is crucial for polynomial-time, while monotonicity is used for the optimal substructure property required by the co"
P10-1110,P08-1068,0,0.280845,"is in seconds per sentence. Search spaces: ‡ linear; others exponential. (on a 3.2GHz Xeon CPU). Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower. Figure 8 shows the parse time in seconds for each test sentence. The observed time complexity of our DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access to extra information like unlabeled data. Our technique is orthogonal to theirs, and combining these techniques could potentially lead to even better results. We also test our final parser on the Penn Chinese Treebank (CTB5). Following the set-up of Duan et al. (2007) and Zhang and Clark (2008), we split CTB5 into training (secs 001-815 and 1001- 1084 parsing time (secs) 1.4 Cha Berk MST DP 1.2 1 0.8 0.6 0.4 0.2 0 0 10"
P10-1110,P05-1012,0,0.949212,"ic programming which includes the dominant CKY algorithm, and greedy search which includes most incremental parsing methods such as shift-reduce.1 Both have pros and cons: the former performs an exact search (in cubic time) over an exponentially large space, while the latter is much faster (in linear-time) and is psycholinguistically motivated (Frazier and Rayner, 1982), but its greedy nature may suffer from severe search errors, as it only explores a tiny fraction of the whole space even with a beam. Can we combine the advantages of both approaches, that is, construct an incremental parser 1 McDonald et al. (2005b) is a notable exception: the MST algorithm is exact search but not dynamic programming. • theoretically, we show that for a large class of modern shift-reduce parsers, dynamic programming is in fact possible and runs in polynomial time as long as the feature functions are bounded and monotonic (which almost always holds in practice); • practically, dynamic programming is up to five times faster (with the same accuracy) as conventional beam-search on top of a stateof-the-art shift-reduce dependency parser; • as a by-product, dynamic programming can output a forest encoding exponentially many"
P10-1110,H05-1066,0,0.581495,"Missing"
P10-1110,J03-1006,0,0.0061058,"he concept of prefix cost from Stolcke (1995), originally developed for weighted Earley parsing. As shown in Fig. 3, the prefix cost c is the total cost of the best action sequence from the initial state to the end of state p, i.e., it includes both the inside cost v (for Viterbi inside derivation), and the cost of the (best) path leading towards the beginning of state p. We say that a state p with prefix cost c is better than a state p′ with prefix cost c′ , notated p ≺ p′ in Fig. 3, if c < c′ . We can also prove (by contradiction) that optimizing for prefix cost implies optimal inside cost (Nederhof, 2003, Sec. 4). 5 As shown in Fig. 3, when a state q with costs (c, v) is combined with a predictor state p with costs (c′ , v ′ ), the resulting state r will have costs (c′ + v + δ, v ′ + v + δ), where the inside cost is intuitively the combined inside costs plus an additional combo cost δ from the combination, while the resulting prefix cost c′ + v + δ is the sum of the prefix cost of the predictor state q, the inside cost of the current state p, and the combo cost. Note the prefix cost of q is irrelevant. The combo cost δ = ξ ′ + λ consists of shift cost ξ ′ of p and reduction cost λ of q. The c"
P10-1110,W04-0308,0,0.329304,"paper, though our formalism and algorithm can also be applied to phrase-structure parsing. Shift-Reduce Parsing 2.1 Vanilla Shift-Reduce Shift-reduce parsing performs a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items at the end of the stack (Aho and Ullman, 1972). To adapt it to dependency parsing, we split the reduce action into two cases, rex and rey, depending on which one of the two items becomes the head after reduction. This procedure is known as “arc-standard” (Nivre, 2004), and has been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state hj, Si where S is a stack of trees s0 , s1 , ... where s0 is the top tree, and j is the 2 stack Figure 2: A trace of vanilla shift-reduce. After step (4), the parser branches off into (5a) or (5b). ℓ : hj, S|s1 |s0 i : c where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: 2 action sh sh rex sh rey sh There is another popular variant, “arc-eager” (Nivr"
P10-1110,N07-1051,0,0.162627,"92.0 91.4 92.1 92.5 92.4 92.1 93.2 L Ja Ja − C Py C Ja C − time 0.12 0.15 − 0.11 0.04 0.49 0.21 − − comp. O(n2 ) O(n3 ) O(n4 ) O(n)‡ O(n) O(n5 ) O(n3 ) O(n2 )‡ O(n4 ) Table 3: Final test results on English (PTB). Our parser (in pure Python) has the highest accuracy among dependency parsers trained on the Treebank, and is also much faster than major parsers. † converted from constituency trees. C=C/C++, Py=Python, Ja=Java. Time is in seconds per sentence. Search spaces: ‡ linear; others exponential. (on a 3.2GHz Xeon CPU). Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower. Figure 8 shows the parse time in seconds for each test sentence. The observed time complexity of our DP parser is in fact linear compared to the superlinear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access"
P10-1110,N09-1073,0,0.060558,"Missing"
P10-1110,J95-2002,0,0.9297,"Rey, CA 90292 sagae@ict.usc.edu Abstract that runs in (almost) linear-time, yet searches over a huge space with dynamic programming? Theoretically, the answer is negative, as Lee (2002) shows that context-free parsing can be used to compute matrix multiplication, where sub-cubic algorithms are largely impractical. We instead propose a dynamic programming alogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice. The key idea is to merge equivalent stacks according to feature functions, inspired by Earley parsing (Earley, 1970; Stolcke, 1995) and generalized LR parsing (Tomita, 1991). However, our formalism is more flexible and our algorithm more practical. Specifically, we make the following contributions: Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values. Empir"
P10-1110,P88-1031,0,0.915394,"ǫi: (0, 0, ∅) state p: ℓ : h , j, sd ...s0 i: (c, , ) sh ℓ + 1 : hj, j + 1, sd−1 ...s0 , wj i : (c + ξ, 0, {p}) state p: rex goal j<n state q: : hk, i, s′d ...s′0 i: (c′ , v ′ , π ′ ) ℓ : hi, j, sd ...s0 i: ( , v, π) ℓ + 1 : hk, j, s′d ...s′1 , s′0 s0 i : (c′ + v + δ, v ′ + v + δ, π ′ ) x p∈π 2n − 1 : h0, n, sd ...s0 i: (c, c, {p0 }) where ξ = w · fsh (j, sd ...s0 ), and δ = ξ ′ + λ, with ξ ′ = w · fsh (i, s′d ...s′0 ) and λ = w · frex (j, sd ...s0 ). Figure 3: Deductive system for shift-reduce parsing with dynamic programming. The predictor state set π is an implicit graph-structured stack (Tomita, 1988) while the prefix cost c is inspired by Stolcke (1995). The rey case is similar, replacing s′0 xs0 with s′0 ys0 , and λ with ρ = w · frey (j, sd ...s0 ). Irrelevant information in a deduction step is marked as an underscore ( ) which means “can match anything”. tag of the next word q1 . Since the queue is static information to the parser (unlike the stack, which changes dynamically), we can use j to replace features from the queue. So in general we write e f (j, S) = (j, fd (sd ), . . . , f0 (s0 )) if the feature window looks at top d + 1 trees on stack, and where fi (si ) extracts kernel feat"
P10-1110,W03-3023,0,0.947319,"g et al. (2009) in Python (henceforth “non-DP”), and then extended it to do dynamic programing (henceforth “DP”). We evaluate their performances on the standard Penn Treebank (PTB) English dependency parsing task7 using the standard split: secs 02-21 for training, 22 for development, and 23 for testing. Both DP and non-DP parsers use the same feature templates in Table 1. For Secs. 4.1-4.2, we use a baseline model trained with non-DP for both DP and non-DP, so that we can do a side-by-side comparison of search 6 7 Or O(n2 ) with MST, but including non-projective trees. Using the head rules of Yamada and Matsumoto (2003). To compare parsing speed between DP and nonDP, we run each parser on the development set, varying the beam width b from 2 to 16 (DP) or 64 (non-DP). Fig. 5a shows the relationship between search quality (as measured by the average model score per sentence, higher the better) and speed (average parsing time per sentence), where DP with a beam width of b=16 achieves the same search quality with non-DP at b=64, while being 5 times faster. Fig. 5b shows a similar comparison for dependency accuracy. We also test with an edge-factored model (Sec. 3.5) using feature templates (1)-(3) in Tab. 1, whi"
P10-1110,D08-1059,0,0.904673,"been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state hj, Si where S is a stack of trees s0 , s1 , ... where s0 is the top tree, and j is the 2 stack Figure 2: A trace of vanilla shift-reduce. After step (4), the parser branches off into (5a) or (5b). ℓ : hj, S|s1 |s0 i : c where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: 2 action sh sh rex sh rey sh There is another popular variant, “arc-eager” (Nivre, 2004; Zhang and Clark, 2008), which is more complicated and less similar to the classical shift-reduce algorithm. Note that the shorthand notation txt′ denotes a new tree by “attaching tree t′ as the leftmost child of the root of tree t”. This procedure can be summarized as a deductive system in Figure 1. States are organized according to step ℓ, which denotes the number of actions accumulated. The parser runs in linear-time as there are exactly 2n−1 steps for a sentence of n words. As an example, consider the sentence “I saw Al with Joe” in Figure 2. At step (4), we face a shiftreduce conflict: either combine “saw” and"
P10-1110,P05-1022,0,\N,Missing
P11-2017,N09-2014,1,0.942178,"e}@ict.usc.edu Abstract Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines. 1 Introduction Natural language understanding (NLU) at the level of speech acts for conversational dialogue systems can be performed with high accuracy in limited domains using data-driven techniques (Bender et al., 2003; Sagae et al., 2009; Gandhe et al., 2008, for example), provided that enough training material is available. For most systems that implement novel conversational scenarios, however, enough examples of user utterances, which can be annotated as NLU training data, only become available once several users have interacted with the system. This situation is typically addressed by bootstrapping from a relatively small set of hand-authored utterances that perform key dialogue acts in the scenario or from utterances collected from wizard-of-oz or role-play exercises, and having NLU accuracy increase over time as more us"
Q16-1014,J92-4003,0,0.391663,"Missing"
Q16-1014,D14-1082,0,0.162446,"on of online structured prediction and beam search has made transition-based parsing competitive in accuracy (Zhang and Clark, 2008; Huang et al., 2012) while retaining linear time complexity, greedy inference with locally-trained classifiers is still widely used, and techniques for improving the performance of greedy parsing have been proposed recently (Choi and Palmer, 2011; Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Honnibal et al., 2013). Recent work on the application of neural network classification to drive greedy transition-based dependency parsing has achieved high accuracy (Chen and Manning, 2014), showing ∗ Both authors contributed equally to this paper. Kenji Sagae∗ USC Institute for Creative Technologies sagae@usc.edu how effective locally-trained neural network models are at predicting parser actions, while providing a straightforward way to improve parsing accuracy using word embeddings pre-trained using a large set of unlabeled data. We propose a novel approach for approximate structured inference for transition-based parsing that uses locally-trained neural networks that, unlike previous local classification approaches, produce scores suitable for global scoring. This is accompl"
Q16-1014,P11-2121,0,0.0946108,". 1 Introduction Transition-based parsing approaches based on local classification of parser actions (Nivre, 2008) remain attractive due to their simplicity, despite producing results slightly below the state-of-the-art. Although the application of online structured prediction and beam search has made transition-based parsing competitive in accuracy (Zhang and Clark, 2008; Huang et al., 2012) while retaining linear time complexity, greedy inference with locally-trained classifiers is still widely used, and techniques for improving the performance of greedy parsing have been proposed recently (Choi and Palmer, 2011; Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Honnibal et al., 2013). Recent work on the application of neural network classification to drive greedy transition-based dependency parsing has achieved high accuracy (Chen and Manning, 2014), showing ∗ Both authors contributed equally to this paper. Kenji Sagae∗ USC Institute for Creative Technologies sagae@usc.edu how effective locally-trained neural network models are at predicting parser actions, while providing a straightforward way to improve parsing accuracy using word embeddings pre-trained using a large set of unlabeled data. We pr"
Q16-1014,P04-1015,0,0.473571,"and allow for effective search is to use the structured perceptron (Collins, 2002). Unlike with local classifiers, weight updates are based on entire derivations, instead of individual states. However, because exact inference is too costly for transition-based parsing with a rich feature set, in practice parsers use beam search to perform approximate inference, and care must be taken to ensure the validity of weight updates (Huang et al., 2012). A widely used approach is to employ early updates, which stop parsing and perform weight updates once the desired structure is no longer in the beam (Collins and Roark, 2004). Transition-based dependency parsers based on the structured perceptron have reached high accuracy (Zhang and Nivre, 2011; Hatori et al., 2012), but these parsers remain in general less accurate than high-order graph-based parsers that model dependency graphs directly, instead of derivations (Zhang and McDonald, 2014; Martins et al., 2013). The drawback of these more accurate parsers is that they tend to be slower than transition-based parsers. duce conditional probabilities for each action given the features of the state, and score each state using the product of the probabilities of all act"
Q16-1014,W02-1001,0,0.291017,"state is in no way captured by the scores that will ultimately result in the overall score for the deriva184 Local Classification tion. In fact, from an incorrect parser state, more incorrect transitions may follow, due to a version of the label bias problem faced by MEMMs (Lafferty et al., 2001; McCallum et al., 2000). In Section 3, we will present our approach that significantly improves search with locally normalized models. 2.3 Structured Perceptron One effective way to create models that score parser transitions globally and allow for effective search is to use the structured perceptron (Collins, 2002). Unlike with local classifiers, weight updates are based on entire derivations, instead of individual states. However, because exact inference is too costly for transition-based parsing with a rich feature set, in practice parsers use beam search to perform approximate inference, and care must be taken to ensure the validity of weight updates (Huang et al., 2012). A widely used approach is to employ early updates, which stop parsing and perform weight updates once the desired structure is no longer in the beam (Collins and Roark, 2004). Transition-based dependency parsers based on the structu"
Q16-1014,W08-1301,0,0.0218734,"Missing"
Q16-1014,C12-1059,0,0.614803,"tion-based parsing approaches based on local classification of parser actions (Nivre, 2008) remain attractive due to their simplicity, despite producing results slightly below the state-of-the-art. Although the application of online structured prediction and beam search has made transition-based parsing competitive in accuracy (Zhang and Clark, 2008; Huang et al., 2012) while retaining linear time complexity, greedy inference with locally-trained classifiers is still widely used, and techniques for improving the performance of greedy parsing have been proposed recently (Choi and Palmer, 2011; Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Honnibal et al., 2013). Recent work on the application of neural network classification to drive greedy transition-based dependency parsing has achieved high accuracy (Chen and Manning, 2014), showing ∗ Both authors contributed equally to this paper. Kenji Sagae∗ USC Institute for Creative Technologies sagae@usc.edu how effective locally-trained neural network models are at predicting parser actions, while providing a straightforward way to improve parsing accuracy using word embeddings pre-trained using a large set of unlabeled data. We propose a novel approach for"
Q16-1014,Q13-1033,0,0.541109,"hes based on local classification of parser actions (Nivre, 2008) remain attractive due to their simplicity, despite producing results slightly below the state-of-the-art. Although the application of online structured prediction and beam search has made transition-based parsing competitive in accuracy (Zhang and Clark, 2008; Huang et al., 2012) while retaining linear time complexity, greedy inference with locally-trained classifiers is still widely used, and techniques for improving the performance of greedy parsing have been proposed recently (Choi and Palmer, 2011; Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Honnibal et al., 2013). Recent work on the application of neural network classification to drive greedy transition-based dependency parsing has achieved high accuracy (Chen and Manning, 2014), showing ∗ Both authors contributed equally to this paper. Kenji Sagae∗ USC Institute for Creative Technologies sagae@usc.edu how effective locally-trained neural network models are at predicting parser actions, while providing a straightforward way to improve parsing accuracy using word embeddings pre-trained using a large set of unlabeled data. We propose a novel approach for approximate structured in"
Q16-1014,P12-1110,0,0.0182031,"e derivations, instead of individual states. However, because exact inference is too costly for transition-based parsing with a rich feature set, in practice parsers use beam search to perform approximate inference, and care must be taken to ensure the validity of weight updates (Huang et al., 2012). A widely used approach is to employ early updates, which stop parsing and perform weight updates once the desired structure is no longer in the beam (Collins and Roark, 2004). Transition-based dependency parsers based on the structured perceptron have reached high accuracy (Zhang and Nivre, 2011; Hatori et al., 2012), but these parsers remain in general less accurate than high-order graph-based parsers that model dependency graphs directly, instead of derivations (Zhang and McDonald, 2014; Martins et al., 2013). The drawback of these more accurate parsers is that they tend to be slower than transition-based parsers. duce conditional probabilities for each action given the features of the state, and score each state using the product of the probabilities of all actions taken up to that state. However, they report that searching through the resulting space for the highest scoring parse does not consistently"
Q16-1014,W13-3518,0,0.153218,"ication of parser actions (Nivre, 2008) remain attractive due to their simplicity, despite producing results slightly below the state-of-the-art. Although the application of online structured prediction and beam search has made transition-based parsing competitive in accuracy (Zhang and Clark, 2008; Huang et al., 2012) while retaining linear time complexity, greedy inference with locally-trained classifiers is still widely used, and techniques for improving the performance of greedy parsing have been proposed recently (Choi and Palmer, 2011; Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Honnibal et al., 2013). Recent work on the application of neural network classification to drive greedy transition-based dependency parsing has achieved high accuracy (Chen and Manning, 2014), showing ∗ Both authors contributed equally to this paper. Kenji Sagae∗ USC Institute for Creative Technologies sagae@usc.edu how effective locally-trained neural network models are at predicting parser actions, while providing a straightforward way to improve parsing accuracy using word embeddings pre-trained using a large set of unlabeled data. We propose a novel approach for approximate structured inference for transition-b"
Q16-1014,P10-1110,1,0.247243,"a large set of unlabeled data. We propose a novel approach for approximate structured inference for transition-based parsing that uses locally-trained neural networks that, unlike previous local classification approaches, produce scores suitable for global scoring. This is accomplished with the introduction of error states in local training, which add information about incorrect derivation paths typically left out completely in locally-trained models. Our approach produces high accuracy for transition-based dependency parsing in English, surpassing parsers based on the structured perceptron (Huang and Sagae, 2010; Zhang and Nivre, 2011) by allowing seamless integration of pre-trained word embeddings, while requiring nearly none of the feature engineering typically associated with parsing with linear models. Trained without external resources or pre-trained embeddings, our neural network (NN) dependency parser outperforms the NN transition-based dependency parser from Chen and Manning (2014), which uses pre-trained word embeddings trained on external data and more features, thanks to improved search. Our experiments show that naive search produces very limited improvements in accuracy compared to greed"
Q16-1014,N12-1015,0,0.0348273,"ach that significantly improves search with locally normalized models. 2.3 Structured Perceptron One effective way to create models that score parser transitions globally and allow for effective search is to use the structured perceptron (Collins, 2002). Unlike with local classifiers, weight updates are based on entire derivations, instead of individual states. However, because exact inference is too costly for transition-based parsing with a rich feature set, in practice parsers use beam search to perform approximate inference, and care must be taken to ensure the validity of weight updates (Huang et al., 2012). A widely used approach is to employ early updates, which stop parsing and perform weight updates once the desired structure is no longer in the beam (Collins and Roark, 2004). Transition-based dependency parsers based on the structured perceptron have reached high accuracy (Zhang and Nivre, 2011; Hatori et al., 2012), but these parsers remain in general less accurate than high-order graph-based parsers that model dependency graphs directly, instead of derivations (Zhang and McDonald, 2014; Martins et al., 2013). The drawback of these more accurate parsers is that they tend to be slower than"
Q16-1014,W06-2930,0,0.0871299,"Missing"
Q16-1014,P08-1068,0,0.0372959,"Missing"
Q16-1014,J93-2004,0,0.0545446,"of h1 were 1536 for ErrSt–25– pre and ErrSt–25–rand, and either 1536, or 2048 for Local–14–pre, Local–14–rand, ErrSt–14–pre, and ErrSt–14–rand. For the best parser on Stanford and YM dependencies, (ErrSt–25–pre), we used a minibatch size of 256 and a initial learning rate of 0.25. For future work, we will explore a larger grid of learning rate, minibatch sizes, and dropout values. At parsing time, we pre-multiply the input embeddings, D and the position matrices, Cfi , which speeds up computation significantly. 5 Results In all experiments we use dependencies extracted from the Penn Treebank (Marcus et al., 1993) following the standard splits (WSJ sections 02 to 21 UAS 91.73 92.00 (+0.27) 91.77 92.61 (+0.84) 93.22 ����� Table 3: Unlabeled accuracy scores (UAS) on the development set with Stanford dependencies using transition-based models trained with pre-trained embeddings. ErrSt–25–pre uses best-first search with a priority queue of size limited to 100. System Local–14–rand (beam 1) Local–14–rand (beam 4) ErrSt–14–rand (beam 1) ErrSt–14–rand (beam 4) ErrSt–25–rand ������������������������ ����� ����� ����� ��� ����� ������������ ������������ ����� �� �� �� �� �� ��� ��������� ��� ��� ��� Figure 5: E"
Q16-1014,P13-2109,0,0.0816062,"Missing"
Q16-1014,C04-1010,0,0.259089,"Missing"
Q16-1014,J08-4003,0,0.881333,"es that mark incorrect derivations produces substantial accuracy improvements. 2 Background: Transition-Based Parsing Transition-based approaches are attractive in dependency parsing for their algorithmic simplicity and straightforward data-driven application. Using shift183 Transactions of the Association for Computational Linguistics, vol. 4, pp. 183–196, 2016. Action Editor: Marco Kuhlmann. Submission batch: 5/2015; Revision batch: 8/2015; Published 5/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. reduce algorithms, such as those pioneered by Nivre (2008), the task of finding a dependency structure becomes that of predicting each action in the derivation of desired structure. ing state. With global estimation of parameters for scoring parser actions, a beam search can produce more accurate results than greedy parsing by minimizing global loss (Zhang and Clark, 2008). 2.1 Arc-Standard Dependency Parsing 2.2 Our parsing models are based on a simple shiftreduce algorithm for dependency parsing known as the arc-standard dependency parsing algorithm (Nivre, 2008). An arc-standard dependency parser maintains one or more parser states T , each compos"
Q16-1014,D07-1111,1,0.875445,"o train local classifiers to predict actions for transition-based parsers is to run the parsing algorithm using a gold-standard sequence of actions (i.e. a sequence of actions that generates the gold-standard tree from a training set) and record the features corresponding to each parser state, where a parser state includes the parser’s stack, input buffer, and set of dependencies created so far. The features corresponding to a state are then associated with the gold-standard action that should be taken from that state, and this constitutes one training example for the local action classifier. Sagae and Tsujii (2007) propose using a maximum entropy classifier to proOur key contribution is a solution to this problem by introducing a notion of state quality in local action classification. This is done through the addition of a new error class to the local classification model. Unlike the other classes, the error class does not correspond to a parser action. In fact, the error class is not used at all during parsing, and serves to occupy probability mass, keeping it from the actual parser actions. Intuitively, the probability of the error class given the current state can be thought of as the probability tha"
Q16-1014,D13-1140,1,0.825641,"fference is that this provides a way for the parser to do better assuming that a mistake has already been made and is irrevocable, while our error states are designed to improve search, lowering the score of undesirable paths so a different path is chosen. Our greedy neural network parser is similar to Chen and Manning (2014), who are the first to show the benefits of using feed-forward neural network classifiers in greedy transition-based dependency parsing. Unlike us, they use a single hidden layer of cube activation functions, and more features. We follow the neural network architecture of Vaswani et al. (2013), using two hidden layers of rectified linear units. Chen and Manning (2014) use Adagrad (Duchi et al., 2011) and dropout for optimization, while we use stochastic gradient descent with dropout. Recent work by Weiss et al. (2015) produces the highest published accuracy for English dependency parsing with very similar neural network architectures and similar pre-training of word embeddings. The accuracy of the greedy version of their parser is substantially higher than that of our greedy parser, due at least in part to the use of more features. A more interesting difference between their approa"
Q16-1014,P15-1032,0,0.076468,"different path is chosen. Our greedy neural network parser is similar to Chen and Manning (2014), who are the first to show the benefits of using feed-forward neural network classifiers in greedy transition-based dependency parsing. Unlike us, they use a single hidden layer of cube activation functions, and more features. We follow the neural network architecture of Vaswani et al. (2013), using two hidden layers of rectified linear units. Chen and Manning (2014) use Adagrad (Duchi et al., 2011) and dropout for optimization, while we use stochastic gradient descent with dropout. Recent work by Weiss et al. (2015) produces the highest published accuracy for English dependency parsing with very similar neural network architectures and similar pre-training of word embeddings. The accuracy of the greedy version of their parser is substantially higher than that of our greedy parser, due at least in part to the use of more features. A more interesting difference between their approach and ours is in the way structured prediction is performed. While Weiss et al. add a structured perceptron layer to a network pretrained locally, we train only locally, but using error states. Both approaches are effective in p"
Q16-1014,W03-3023,0,0.338271,"e dependency tree output. Given a way to score parser actions instead of simply choosing one action to apply, a state score can be defined on the sequence of actions resulting in the state. Keeping track of multiple states with scores resulting from the application of different valid actions for a single state creates an exponential search space. Beam search can then be applied to search for a high scorInitial data-driven transition-based dependency parsing approaches employed locally-trained multiclass models to choose a parser action based on the parser state at each step in the derivation (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). In these models, classification is based on a set of features extracted from the current state of the parser, and creating training examples for the classifier requires only running the transition-based algorithm to reproduce the trees in a training treebank, while recording the features and actions at each step. A classifier is then trained with the actions as classes. While this simple procedure has allowed for training of dependency parsers using off-the-shelf classifier implementations, the resulting parsers are restricted to performing greedy search, considering"
Q16-1014,K15-1015,0,0.114372,"ne way to create models capable of global scoring, and therefore effective search, is to parse with the structured perceptron (Zhang and Clark, 2008), which we also discuss in Section 2. Instead of performing global weight updates, our approach relies on local classifiers, but adds information about incorrect derivation paths to approximate a notion of global loss. This gives us a simple way to train neural network models for predicting parser actions locally but still perform effective search. Our use of error states is conceptually related to the correctness probability estimate proposed by Yazdani and Henderson (2015), which is used only with each shift action of an arc-eager transitionbased parsing model. This correctness probability creates a measure of quality of derivations at the point of each shift, which allows a combination of local action scores and the correctness probability to be used with beam search. The beam is then determined only at each shift, while search paths produced by other actions are extended exhaustively. Our error states, in contrast, adjust the scores of every action, making the use of best-first search natural. Non-deterministic oracles for transition-based dependency parsing"
Q16-1014,D08-1059,0,0.670619,"for Computational Linguistics, vol. 4, pp. 183–196, 2016. Action Editor: Marco Kuhlmann. Submission batch: 5/2015; Revision batch: 8/2015; Published 5/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. reduce algorithms, such as those pioneered by Nivre (2008), the task of finding a dependency structure becomes that of predicting each action in the derivation of desired structure. ing state. With global estimation of parameters for scoring parser actions, a beam search can produce more accurate results than greedy parsing by minimizing global loss (Zhang and Clark, 2008). 2.1 Arc-Standard Dependency Parsing 2.2 Our parsing models are based on a simple shiftreduce algorithm for dependency parsing known as the arc-standard dependency parsing algorithm (Nivre, 2008). An arc-standard dependency parser maintains one or more parser states T , each composed of a stack S = [sm , ..., s1 , s0 ] (where the topmost item is s0 ) and input buffer W = [w0 , w1 , ..., wn ] (where the first element of the buffer is w0 ). In its initial state T0 , the stack is empty, and the input buffer contains each token in the input sentence with its part-of-speech tag. One of three actio"
Q16-1014,P14-2107,0,0.214609,"eam search to perform approximate inference, and care must be taken to ensure the validity of weight updates (Huang et al., 2012). A widely used approach is to employ early updates, which stop parsing and perform weight updates once the desired structure is no longer in the beam (Collins and Roark, 2004). Transition-based dependency parsers based on the structured perceptron have reached high accuracy (Zhang and Nivre, 2011; Hatori et al., 2012), but these parsers remain in general less accurate than high-order graph-based parsers that model dependency graphs directly, instead of derivations (Zhang and McDonald, 2014; Martins et al., 2013). The drawback of these more accurate parsers is that they tend to be slower than transition-based parsers. duce conditional probabilities for each action given the features of the state, and score each state using the product of the probabilities of all actions taken up to that state. However, they report that searching through the resulting space for the highest scoring parse does not consistently result in improved parser accuracy over a greedy policy (i.e. pursue only the highest scoring action at each state), suggesting that this strategy for scoring states is a poo"
Q16-1014,P11-2033,0,0.477885,"ed data. We propose a novel approach for approximate structured inference for transition-based parsing that uses locally-trained neural networks that, unlike previous local classification approaches, produce scores suitable for global scoring. This is accomplished with the introduction of error states in local training, which add information about incorrect derivation paths typically left out completely in locally-trained models. Our approach produces high accuracy for transition-based dependency parsing in English, surpassing parsers based on the structured perceptron (Huang and Sagae, 2010; Zhang and Nivre, 2011) by allowing seamless integration of pre-trained word embeddings, while requiring nearly none of the feature engineering typically associated with parsing with linear models. Trained without external resources or pre-trained embeddings, our neural network (NN) dependency parser outperforms the NN transition-based dependency parser from Chen and Manning (2014), which uses pre-trained word embeddings trained on external data and more features, thanks to improved search. Our experiments show that naive search produces very limited improvements in accuracy compared to greedy inference, while searc"
Q16-1014,D13-1071,0,0.408309,"awback of these more accurate parsers is that they tend to be slower than transition-based parsers. duce conditional probabilities for each action given the features of the state, and score each state using the product of the probabilities of all actions taken up to that state. However, they report that searching through the resulting space for the highest scoring parse does not consistently result in improved parser accuracy over a greedy policy (i.e. pursue only the highest scoring action at each state), suggesting that this strategy for scoring states is a poor choice. This is confirmed by Zhao et al. (2013), who report only a small improvement over greedy search despite using exact inference with this state scoring strategy. Parsing with Local Classifiers and Error States Because action probabilities are conditioned on the features of the current state alone and normalized locally, there is no reason to expect that the product of such probabilities along a derivation path up to a state, whether or not it is a final state, should reflect the overall quality of the state. Once an incorrect action is classified as more probable than the correct action in a given state Ti , the incorrect state Tj re"
Q16-1014,D07-1096,0,\N,Missing
S19-2017,N18-1091,0,0.0469275,"Missing"
S19-2017,P17-1104,0,0.325382,"licable across languages. SemEval 2019 Task 1 (Hershcovich et al., 2018b, 2019) focuses on semantic parsing of texts into graphs consisting of terminal nodes that represent words, non-terminal nodes that represent internal structure, and labeled edges representing relationships between nodes (e.g. participant, center, linker, adverbial, elaborator), according to the UCCA scheme. Annotated datasets are provided, and participants are evaluated in four settings: English with domain-specific data, English 2 Related work Leveraging parallels between UCCA and known approaches for syntactic parsing, Hershcovich et al. (2017) proposed TUPA, a customized transition-based parser with dense feature representation. Based on this model, Hershcovich et al. (2018a) used multitask learning effectively by training a UCCA model along with similar parsing tasks where more training data is available, such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and Universal Dependencies (UD) (Nivre et al., 2016). Due to 119 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 119–124 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics Fi"
S19-2017,P18-1035,0,0.0916935,"1 Introduction Semantic parsing aims to capture structural relationships between input strings and graph representations of sentence meaning, going beyond concerns of surface word order, phrases and relationships. The focus on meaning rather than surface relations often requires the use of reentrant nodes and discontinuous structures. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013) is designed to support semantic parsing with mappings between sentences and their corresponding meanings in a framework intended to be applicable across languages. SemEval 2019 Task 1 (Hershcovich et al., 2018b, 2019) focuses on semantic parsing of texts into graphs consisting of terminal nodes that represent words, non-terminal nodes that represent internal structure, and labeled edges representing relationships between nodes (e.g. participant, center, linker, adverbial, elaborator), according to the UCCA scheme. Annotated datasets are provided, and participants are evaluated in four settings: English with domain-specific data, English 2 Related work Leveraging parallels between UCCA and known approaches for syntactic parsing, Hershcovich et al. (2017) proposed TUPA, a customized transition-based"
S19-2017,S19-2001,0,0.0363345,"Missing"
S19-2017,Q16-1032,0,0.0605769,"Missing"
S19-2017,Q16-1023,0,0.225605,"with only development and test data, but no training data. Additionally, there are open and closed tracks, where the use of additional resources is and is not allowed, respectively. Our entry in the task is limited to the closed track and the first setting, domain-specific English using the Wiki corpus, where the relatively small dataset (4113 sentences for training, 514 for development, and 515 for testing) consists of annotated sentences from English Wikipedia. Our model follows the encoder-decoder architecture commonly used in state-of-the-art neural parsing models (Kitaev and Klein, 2018; Kiperwasser and Goldberg, 2016b; Cross and Huang, 2016; Chen and Manning, 2014). However, we propose a very simple decoder architecture that relies only on a recursive attention mechanism of the encoded latent representation. In other words, the decoder does not require state encoding and model-optimal inference whatsoever. Our novel model achieved a macro-averaged F1-score of 0.753 in labeled primary edges and 0.864 in unlabeled primary edge prediction on the test set. The results confirm the suitability of our proposed model to the semantic parsing task. We present a simple and accurate model for semantic parsing with UC"
S19-2017,P13-1023,0,0.262236,"ntences with a bidirectional-LSTM, and decode with selfattention to build a graph structure. Results show that our parser is simple and effective for semantic parsing with reentrancy and discontinuous structures. 1 Introduction Semantic parsing aims to capture structural relationships between input strings and graph representations of sentence meaning, going beyond concerns of surface word order, phrases and relationships. The focus on meaning rather than surface relations often requires the use of reentrant nodes and discontinuous structures. Universal Conceptual Cognitive Annotation (UCCA) (Abend and Rappoport, 2013) is designed to support semantic parsing with mappings between sentences and their corresponding meanings in a framework intended to be applicable across languages. SemEval 2019 Task 1 (Hershcovich et al., 2018b, 2019) focuses on semantic parsing of texts into graphs consisting of terminal nodes that represent words, non-terminal nodes that represent internal structure, and labeled edges representing relationships between nodes (e.g. participant, center, linker, adverbial, elaborator), according to the UCCA scheme. Annotated datasets are provided, and participants are evaluated in four setting"
S19-2017,P18-1249,0,0.110761,"ecific data, and French with only development and test data, but no training data. Additionally, there are open and closed tracks, where the use of additional resources is and is not allowed, respectively. Our entry in the task is limited to the closed track and the first setting, domain-specific English using the Wiki corpus, where the relatively small dataset (4113 sentences for training, 514 for development, and 515 for testing) consists of annotated sentences from English Wikipedia. Our model follows the encoder-decoder architecture commonly used in state-of-the-art neural parsing models (Kitaev and Klein, 2018; Kiperwasser and Goldberg, 2016b; Cross and Huang, 2016; Chen and Manning, 2014). However, we propose a very simple decoder architecture that relies only on a recursive attention mechanism of the encoded latent representation. In other words, the decoder does not require state encoding and model-optimal inference whatsoever. Our novel model achieved a macro-averaged F1-score of 0.753 in labeled primary edges and 0.864 in unlabeled primary edge prediction on the test set. The results confirm the suitability of our proposed model to the semantic parsing task. We present a simple and accurate mo"
S19-2017,D15-1166,0,0.0608307,"itional BiLSTM on the target span xi , xi+1 , ..., xj , similar to the recursive tree representations in (Socher et al., 2013; Kiperwasser and Goldberg, 2016a) but replaced the feed-forward network with an LSTM. In our experiments with the small dataset in the closed track of the English domain-specific track, this method did not result in improved performance. 3.3 Attention Mechanism For Decoding Our basic decoding model is inspired by the global attention mechanism used in machine translation. The attention averages the encoded state in each time step in the sequence with trainable weights (Luong et al., 2015). We set a maximum sequence length and calculate the attention weights (in probability) for the left boundary index of the span given the node representation vi,j (i ≤ j): hspan = M LP (vi,j ) plef t boundary = sof tmax(hspan ) between the indices and the actual words in each sentence. The model may cheat during training to attend to specific indices regardless of the actual words in these indices. Motivated by the success of biaffine attention(Dozat and Manning, 2016) and self-attention models (Vaswani et al., 2017), we replace the index attention decoder with a multiplication model where we"
S19-2017,L16-1262,0,0.0203639,"Missing"
S19-2017,D13-1170,0,0.010141,"ta available, we concatenate part-ofspeech tags embeddings to word embeddings in terminal nodes. In addition, because the connections between terminal nodes and non-terminal nodes often require identification of named enti1 2 120 https://fasttext.cc/ https://spacy.io/ Algorithm 1 Index-attention decoder 1: for recur num = 1 to max recur do 2: if i ≥ jl then 3: break 4: end if 5: hspan = M LP (vi,j ) 6: iattn = argmax sof tmax(hspan ) i 7: i = primary parent(viattn )l 8: end for with an additional BiLSTM on the target span xi , xi+1 , ..., xj , similar to the recursive tree representations in (Socher et al., 2013; Kiperwasser and Goldberg, 2016a) but replaced the feed-forward network with an LSTM. In our experiments with the small dataset in the closed track of the English domain-specific track, this method did not result in improved performance. 3.3 Attention Mechanism For Decoding Our basic decoding model is inspired by the global attention mechanism used in machine translation. The attention averages the encoded state in each time step in the sequence with trainable weights (Luong et al., 2015). We set a maximum sequence length and calculate the attention weights (in probability) for the left bound"
sagae-etal-2004-adding,W01-1816,1,\N,Missing
sagae-etal-2004-adding,W03-3019,1,\N,Missing
sagae-etal-2004-adding,A00-2018,0,\N,Missing
sagae-etal-2004-adding,N01-1006,0,\N,Missing
sagae-etal-2004-adding,P96-1025,0,\N,Missing
sagae-etal-2004-adding,P95-1037,0,\N,Missing
sagae-etal-2004-adding,rambow-etal-2002-dependency,0,\N,Missing
tateisi-etal-2008-genia,W03-2401,0,\N,Missing
tateisi-etal-2008-genia,de-marneffe-etal-2006-generating,0,\N,Missing
tateisi-etal-2008-genia,N04-1013,0,\N,Missing
tateisi-etal-2008-genia,J93-2004,0,\N,Missing
tateisi-etal-2008-genia,E03-1025,0,\N,Missing
tateisi-etal-2008-genia,W07-1004,0,\N,Missing
tateisi-etal-2008-genia,W07-2202,1,\N,Missing
tateisi-etal-2008-genia,W04-3111,0,\N,Missing
tateisi-etal-2008-genia,P06-4020,0,\N,Missing
tateisi-etal-2008-genia,P07-1032,0,\N,Missing
tateisi-etal-2008-genia,P06-2006,0,\N,Missing
tateisi-etal-2008-genia,ohta-etal-2006-linguistic,1,\N,Missing
W01-1816,A97-1051,0,0.0695592,"Missing"
W01-1816,W97-1310,0,0.030633,"n process. The NEGRA Corpus (Skut, 1997) is another example of human-machine collaboration in annotating a corpus with syntactic and grammatical function information. In fact, a number of syntactically annotated corpora (or treebanks) have been produced in recent years, with varying amounts of automation, but typically with human effort playing a major role in the annotation process. Although the study of syntax and grammar can be viewed as the major global concern in all treebanks, some collections address specific concerns in natural language research, from the study of anaphora resolution (McEnery et al., 1997), to parser evaluation (Carrol et al., 1999). For a collection of papers on different treebanks, annotation techniques, methodologies and foci, please see (Garside et al., 1997). Our work differs in focus from these other projects in that our primary goal is to provide information specific to the process of human language acquisition. Although this information could be used in the training of automatic learning systems, we also aim to make it suitable for researchers in various aspects of linguistics and child development to draw reliable conclusions on the human language learning process. Bec"
W01-1816,J93-2004,0,0.0261902,"Missing"
W01-1816,A97-1014,0,0.0872551,"Missing"
W01-1816,W00-0743,0,0.0542053,"Missing"
W01-1816,J95-4004,0,\N,Missing
W03-3019,P92-1024,0,0.0833088,"Missing"
W03-3019,A00-2031,0,0.0126436,"that this combination of the rulebased and data-driven systems outperforms either system in isolation. 4 Related Work Carroll and Briscoe (2002) present a wide-coverage parser that outputs grammatical relations, and discuss the trade-off between precision and recall of grammatical relations, as well as useful ways to manipulate such trade-off to achieve high precision at the expense of recall. This trade-off is also observed in our experiments. However, our angle on this issue focuses on the combination of systems with different precision/recall behavior, to achieve a higher combined F-score. Blaheta and Charniak (2000) discuss the assignment of Penn Treebank (Marcus et al., 1993) function tags to constituent structure trees. They use a statistical approach to assign tags (similar in many ways to grammatical relations) to parse tree nodes. Our corpus-based model also uses parse trees, but only to determine that a GR exists between two words. The work of Gildea and Palmer (2002) has shown that the use of constituent structure information is useful in determining predicateargument structure. While their work involved propositions of a more semantic nature, we believe their results to be applicable to the ident"
W03-3019,C02-1013,0,0.0247492,"d affect the overall performance of the Grammatical Relation Number of instances in test set Subject Object Adjunct Predicate nominal 20 14 8 3 Table 5: Number of instances of each GR in failed sentences Grammatical Relation Precision Recall F-score Subject Object Adjunct Predicate nominal 0.60 0.80 0.75 0.00 0.60 0.57 0.37 0.00 0.60 0.67 0.50 0.00 Table 6: Results using the data-driven system on failed sentences combined system. However, the results as they stand already show that this combination of the rulebased and data-driven systems outperforms either system in isolation. 4 Related Work Carroll and Briscoe (2002) present a wide-coverage parser that outputs grammatical relations, and discuss the trade-off between precision and recall of grammatical relations, as well as useful ways to manipulate such trade-off to achieve high precision at the expense of recall. This trade-off is also observed in our experiments. However, our angle on this issue focuses on the combination of systems with different precision/recall behavior, to achieve a higher combined F-score. Blaheta and Charniak (2000) discuss the assignment of Penn Treebank (Marcus et al., 1993) function tags to constituent structure trees. They use"
W03-3019,A00-2018,0,0.524935,"ontained in the words themselves is lost in the assignment of GR tags to part-of-speech tags alone, the output of this tagger is further refined by an error-driven transformation-based learning strategy that takes the actual words into account, implemented using the fnTBL toolkit (Ngai & Florian, 2001). While the above GR “tagger” can assign grammatical relation labels to words, we still have to determine the target of the directional link established by the grammatical relation indicated by the label. To accomplish this, the raw text input sentence was also parsed using a statistical parser (Charniak, 2000) trained on the Penn Treebank (Marcus et al., 1993), yielding an approximation of the skeletal constituent structure (parse tree) of the sentence. A slightly modified version of the “treebank constituent head table” originally designed by Magerman (1995) is then used to determine the heads of constituents in the parse tree. By stipulating that there is a directional link from every word in a constituent (except for the head) to the head of the constituent, and applying that notion to the entire parse-tree, we determine a set of unlabeled dependency links for the sentence. The combination of th"
W03-3019,P02-1031,0,0.0308609,"on at the expense of recall. This trade-off is also observed in our experiments. However, our angle on this issue focuses on the combination of systems with different precision/recall behavior, to achieve a higher combined F-score. Blaheta and Charniak (2000) discuss the assignment of Penn Treebank (Marcus et al., 1993) function tags to constituent structure trees. They use a statistical approach to assign tags (similar in many ways to grammatical relations) to parse tree nodes. Our corpus-based model also uses parse trees, but only to determine that a GR exists between two words. The work of Gildea and Palmer (2002) has shown that the use of constituent structure information is useful in determining predicateargument structure. While their work involved propositions of a more semantic nature, we believe their results to be applicable to the identification of grammatical relations. 5 Conclusions and Future Work We have presented a way to combine rule-based and data-driven NLP techniques in the extraction of grammatical relations. We have shown that starting with a rule-based system, we can use unlabeled data and a corpus-based system to improve recall (and F-score) of grammatical relations. While the expe"
W03-3019,P95-1037,0,0.106403,"ed using the fnTBL toolkit (Ngai & Florian, 2001). While the above GR “tagger” can assign grammatical relation labels to words, we still have to determine the target of the directional link established by the grammatical relation indicated by the label. To accomplish this, the raw text input sentence was also parsed using a statistical parser (Charniak, 2000) trained on the Penn Treebank (Marcus et al., 1993), yielding an approximation of the skeletal constituent structure (parse tree) of the sentence. A slightly modified version of the “treebank constituent head table” originally designed by Magerman (1995) is then used to determine the heads of constituents in the parse tree. By stipulating that there is a directional link from every word in a constituent (except for the head) to the head of the constituent, and applying that notion to the entire parse-tree, we determine a set of unlabeled dependency links for the sentence. The combination of the unlabeled links and the GR labels results in our target output of grammatical relations. 2.3 Identifying Grammatical Relations with Rule-Based and Data-Driven Methods Once we have established a corpus-based procedure that makes GR assignments to every"
W03-3019,J93-2004,0,0.0449404,"e assignment of GR tags to part-of-speech tags alone, the output of this tagger is further refined by an error-driven transformation-based learning strategy that takes the actual words into account, implemented using the fnTBL toolkit (Ngai & Florian, 2001). While the above GR “tagger” can assign grammatical relation labels to words, we still have to determine the target of the directional link established by the grammatical relation indicated by the label. To accomplish this, the raw text input sentence was also parsed using a statistical parser (Charniak, 2000) trained on the Penn Treebank (Marcus et al., 1993), yielding an approximation of the skeletal constituent structure (parse tree) of the sentence. A slightly modified version of the “treebank constituent head table” originally designed by Magerman (1995) is then used to determine the heads of constituents in the parse tree. By stipulating that there is a directional link from every word in a constituent (except for the head) to the head of the constituent, and applying that notion to the entire parse-tree, we determine a set of unlabeled dependency links for the sentence. The combination of the unlabeled links and the GR labels results in our"
W03-3019,W01-1816,1,0.761875,"Missing"
W05-1513,W00-0735,0,0.044751,"Missing"
W05-1513,A00-2018,0,0.869294,"Missing"
W05-1513,P97-1003,0,0.337194,"Missing"
W05-1513,P02-1031,0,0.0122831,"lowing parsing decisions made by a classifier. Despite their greedy nature, these parsers achieve high accuracy in determining dependencies. Although state-of-the-art statistical parsers (Collins, 1997; Charniak, 2000) are more accurate, the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast, light-weight parsing, or parsing of large amounts of data. However, dependency analyses lack important information contained in constituent structures. For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002). We present a parser that shares much of the simplicity and efficiency of the deterministic dependency parsers, but produces both dependency and constituent structures simultaneously. Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time. While it may seem that the larger search space of constituent trees (compared to the space of dependency trees) would make it unlikely that accurate parse trees could be built deterministically, we show that the precision and recall of constituents produced by our parser are close to"
W05-1513,W04-3203,0,0.165259,"Missing"
W05-1513,W04-3239,0,0.0472103,"Missing"
W05-1513,N01-1025,0,0.0966019,"Missing"
W05-1513,J98-4004,0,0.1162,"work we focus only on the processing that occurs once POS tagging is completed. In the sections that follow, we assume that the input to the parser is a sentence with corresponding POS tags for each word. 2 Parser Description Our parser employs a basic bottom-up shift-reduce parsing algorithm, requiring only a single pass over the input string. The algorithm considers only 126 trees with unary and binary branching. In order to use trees with arbitrary branching for training, or generating them with the parser, we employ an instance of the transformation/detransformation process described in (Johnson, 1998). In our case, the transformation step involves simply converting each production with n children (where n &gt; 2) into n – 1 binary productions. Trees must be lexicalized1, so that the newly created internal structure of constituents with previous branching of more than two contains only subtrees with the same lexical head as the original constituent. Additional nonterminal symbols introduced in this process are clearly marked. The transformed (or “binarized”) trees may then be used for training. Detransformation is applied to trees produced by the parser. This involves the removal of non-termin"
W05-1513,J93-2004,0,0.0355478,"Missing"
W05-1513,C04-1010,0,0.424712,"d of the new item is either the lexical head of its left child, or the lexical head of its right child. If S is empty, only a shift action is allowed. If W is empty, only a reduce action is allowed. If both S and W are non-empty, either shift or reduce actions are possible. Parsing terminates when W is empty and S contains only one item, and the single item in S is the parse tree for the input string. Because the parse tree is lexicalized, we also have a dependency structure for the sentence. In fact, the binary reduce actions are very similar to the reduce actions in the dependency parser of Nivre and Scholz (2004), but they are executed in a different order, so constituents can be built. If W is empty, and more than one item remain in S, and no further reduce actions take place, the input string is rejected. 127 2.2 Determining Actions with a Classifier A parser based on the algorithm described in the previous section faces two types of decisions to be made throughout the parsing process. The first type concerns whether to shift or reduce when both actions are possible, or whether to reduce or reject the input when only reduce actions are possible. The second type concerns what syntactic structures are"
W05-1513,W03-3023,0,0.143752,"Missing"
W05-1513,P01-1069,0,\N,Missing
W07-0604,J96-1002,0,0.00259386,"the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combined in a single structure). This parsing approach is very similar to the one used successfully by Nivre et al. (2006), but we use a maximum entropy classifier (Berger et al., 1996) to determine parser actions, which makes parsing extremely fast. In addition, our parsing approach performs a search over the space of possible parser actions, while Nivre et al.’s approach is deterministic. See Sagae and Tsujii (2007) for more information on the parser. Features used in classification to determine whether the parser takes a shift or a reduce action at any point during parsing are derived from the parser’s current configuration (contents of the stack and queue) at that point. The specific features used are:4 • Word and its POS tag: s(1), q(2), and q(1). • POS: s(3) and q(2)."
W07-0604,W06-2920,0,0.0359076,"arsing approach uses a best-first 28 probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie (2006). Because CHILDES syntactic annotations are represented as labeled dependencies, using a dependency parsing approach allows us to work with that representation directly. This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007)3 . Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combin"
W07-0604,A00-2018,0,0.284917,"ng punctuation) and 65,363 words. The average utterance length is 5.3 words (including punctuation) for adult utterances, 3.6 for child, 4.5 overall. The annotated Eve corpus is available at http://childes.psy.cmu. edu/data/Eng-USA/brown.zip. It was used for the Domain adaptation task at the CoNLL-2007 dependency parsing shared task (Nivre, 2007). 3 Parsing Although the CHILDES annotation scheme proposed by Sagae et al. (2004) has been used in practice for automatic parsing of child language transcripts (Sagae et al., 2004; Sagae et al., 2005), such work relied mainly on a statistical parser (Charniak, 2000) trained on the Wall Street Journal portion of the Penn Treebank, since a large enough corpus of annotated CHILDES data was not available to train a domain-specific parser. Having a corpus of 65,000 words of CHILDES data annotated with grammatical relations represented as labeled dependencies allows us to develop a parser tailored for the CHILDES domain. Our overall parsing approach uses a best-first 28 probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing a"
W07-0604,W06-2933,0,0.0264284,"used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combined in a single structure). This parsing approach is very similar to the one used successfully by Nivre et al. (2006), but we use a maximum entropy classifier (Berger et al., 1996) to determine parser actions, which makes parsing extremely fast. In addition, our parsing approach performs a search over the space of possible parser actions, while Nivre et al.’s approach is deterministic. See Sagae and Tsujii (2007) for more information on the parser. Features used in classification to determine whether the parser takes a shift or a reduce action at any point during parsing are derived from the parser’s current configuration (contents of the stack and queue) at that point. The specific features used are:4 • Wor"
W07-0604,P06-2089,1,0.365037,"reebank, since a large enough corpus of annotated CHILDES data was not available to train a domain-specific parser. Having a corpus of 65,000 words of CHILDES data annotated with grammatical relations represented as labeled dependencies allows us to develop a parser tailored for the CHILDES domain. Our overall parsing approach uses a best-first 28 probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie (2006). Because CHILDES syntactic annotations are represented as labeled dependencies, using a dependency parsing approach allows us to work with that representation directly. This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007)3 . Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic"
W07-0604,D07-1111,1,0.215691,"bilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie (2006). Because CHILDES syntactic annotations are represented as labeled dependencies, using a dependency parsing approach allows us to work with that representation directly. This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007)3 . Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combined in a single structure). This parsing a"
W07-0604,sagae-etal-2004-adding,1,0.867299,"Missing"
W07-0604,P05-1025,1,0.927916,"0 adult and 8,563 child. The utterances consist of 84,226 GRs (including punctuation) and 65,363 words. The average utterance length is 5.3 words (including punctuation) for adult utterances, 3.6 for child, 4.5 overall. The annotated Eve corpus is available at http://childes.psy.cmu. edu/data/Eng-USA/brown.zip. It was used for the Domain adaptation task at the CoNLL-2007 dependency parsing shared task (Nivre, 2007). 3 Parsing Although the CHILDES annotation scheme proposed by Sagae et al. (2004) has been used in practice for automatic parsing of child language transcripts (Sagae et al., 2004; Sagae et al., 2005), such work relied mainly on a statistical parser (Charniak, 2000) trained on the Wall Street Journal portion of the Penn Treebank, since a large enough corpus of annotated CHILDES data was not available to train a domain-specific parser. Having a corpus of 65,000 words of CHILDES data annotated with grammatical relations represented as labeled dependencies allows us to develop a parser tailored for the CHILDES domain. Our overall parsing approach uses a best-first 28 probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essenti"
W08-0504,J96-1002,0,0.0067758,"Missing"
W08-0504,D07-1024,0,0.016794,"OBJ This study demonstrates that IL-8 recognizes and activates CXCR1, CXCR2, and the Duffy antigen by distinct mechanisms. ROOT SBJ COORD CC ROOT IL-8 recognizes and activates CXCR1 The molar ratio of serum retinol-binding protein (RBP) to transthyretin (TTR) is not useful to assess vitamin A status during infection in hospitalized children. Figure 1: A dependency tree Figure 2: Example sentences with protein names SBJ is a positive example, and &lt;RBP, TTR&gt; is a negative example. Following recent work on using dependency parsing in systems that identify protein interactions in biomedical text (Erkan et al., 2007; Sætre et al., 2007; Katrenko and Adriaans, 2006), we have built a system for PPI extraction that uses dependency relations as features. As exemplified, for the protein pair IL-8 and CXCR1 in the first sentence of Figure 2, a dependency parser outputs a dependency tree shown in Figure 1. From this dependency tree, we can extract a dependency path between IL-8 and CXCR1 (Figure 3), which appears to be a strong clue in knowing that these proteins are mentioned as interacting. The system we use in this paper is similar to the one described in Sætre et al. (2007), except that it uses syntactic de"
W08-0504,W07-2202,1,0.851396,"of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994) during development and training. While this claim is supported by convincing evaluations that show that parsers trained on the WSJ Penn Treebank alone perform poorly on biomedical text in terms of accuracy of dependencies or bracketing of phrase structure, the benefits of using domainspecific data in terms of practical system performance have not been quantified. These expected benefits drive the development of domain-specific resources, such as the GENIA treebank (Tateisi et al., 2005), and parser domain adaption (Hara et al., 2007), which are of clear importance in parsing research, but of largely unconfirmed impact on practical systems. Quirk and Corston-Oliver (2006) examine a similar issue, the relationship between parser accuracy and overall system accuracy in syntaxinformed machine translation. Their research is similar to the work presented here, but they focused on the use of varying amounts of out-ofdomain training data for the parser, measuring how a translation system for technical text performed when its syntactic parser was trained with varying amounts of Wall Street Journal text. Our work, in contrast, inve"
W08-0504,W04-3111,0,0.0291568,"Missing"
W08-0504,I05-1006,0,0.120707,"Missing"
W08-0504,P08-1006,1,0.923319,"n section 2 we discuss our motivation and related efforts. Section 3 describes the system for identification of protein-protein interactions used in our experiments, and in section 4 describes the syntactic parser that provides the analyses for the PPI system, and the data used to train the parser. We describe our experiments, results and analysis in section 5, and conclude in section 6. 2 Motivation and related work While recent work has addressed questions relating to the use of different parsers or different types of syntactic representations in the PPI extraction task (Sætre et al., 2007, Miyao et al., 2008), little concrete evidence has been provided for potential benefits of improved parsers or additional resources for training syntactic parsers. In fact, although there is increasing interest in parser evaluation in the biomedical domain in terms of precision/recall of brackets and dependency accuracy (Clegg and Shepherd, 2007; Pyysalo et al., 2007; Sagae et al., 2008), the relationship between these evaluation metrics and the performance of practical information extraction systems remains unclear. In the parsing community, relatively small accuracy gains are often reported as success stories,"
W08-0504,W07-1004,0,0.0158531,"s in section 5, and conclude in section 6. 2 Motivation and related work While recent work has addressed questions relating to the use of different parsers or different types of syntactic representations in the PPI extraction task (Sætre et al., 2007, Miyao et al., 2008), little concrete evidence has been provided for potential benefits of improved parsers or additional resources for training syntactic parsers. In fact, although there is increasing interest in parser evaluation in the biomedical domain in terms of precision/recall of brackets and dependency accuracy (Clegg and Shepherd, 2007; Pyysalo et al., 2007; Sagae et al., 2008), the relationship between these evaluation metrics and the performance of practical information extraction systems remains unclear. In the parsing community, relatively small accuracy gains are often reported as success stories, but again, the 15 precise impact of such improvements on practical tasks in bioinformatics has not been established. One aspect of this issue is the question of domain portability and domain adaptation for parsers and other NLP modules. Clegg and Shepherd (2007) mention that available statistical parsers appear to overfit to the newswire domain, b"
W08-0504,W06-1608,0,0.0307019,"m is supported by convincing evaluations that show that parsers trained on the WSJ Penn Treebank alone perform poorly on biomedical text in terms of accuracy of dependencies or bracketing of phrase structure, the benefits of using domainspecific data in terms of practical system performance have not been quantified. These expected benefits drive the development of domain-specific resources, such as the GENIA treebank (Tateisi et al., 2005), and parser domain adaption (Hara et al., 2007), which are of clear importance in parsing research, but of largely unconfirmed impact on practical systems. Quirk and Corston-Oliver (2006) examine a similar issue, the relationship between parser accuracy and overall system accuracy in syntaxinformed machine translation. Their research is similar to the work presented here, but they focused on the use of varying amounts of out-ofdomain training data for the parser, measuring how a translation system for technical text performed when its syntactic parser was trained with varying amounts of Wall Street Journal text. Our work, in contrast, investigates the use of domain-specific training material in parsers for biomedical text, a domain where significant amounts of effort are alloc"
W08-0504,P06-2089,1,0.841027,"protein names split, tokenized, and annotated with proteins and PPIs. 4 A data-driven dependency parser for biomedical text The parser we used as component of our PPI extraction system was a shift-reduce dependency parser that uses maximum entropy models to determine the parser’s actions. Our overall parsing approach uses a best-first probabilistic shift-reduce algorithm, working left-to right to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the constituent parsing algorithm for probabilistic parsing with LR-like data-driven models described by Sagae and Lavie (2006). This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007). Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm and the features used to classify parser actions. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift acti"
W08-0504,I05-2038,1,0.931944,"the newswire domain, because of their extensive use of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994) during development and training. While this claim is supported by convincing evaluations that show that parsers trained on the WSJ Penn Treebank alone perform poorly on biomedical text in terms of accuracy of dependencies or bracketing of phrase structure, the benefits of using domainspecific data in terms of practical system performance have not been quantified. These expected benefits drive the development of domain-specific resources, such as the GENIA treebank (Tateisi et al., 2005), and parser domain adaption (Hara et al., 2007), which are of clear importance in parsing research, but of largely unconfirmed impact on practical systems. Quirk and Corston-Oliver (2006) examine a similar issue, the relationship between parser accuracy and overall system accuracy in syntaxinformed machine translation. Their research is similar to the work presented here, but they focused on the use of varying amounts of out-ofdomain training data for the parser, measuring how a translation system for technical text performed when its syntactic parser was trained with varying amounts of Wall"
W08-0504,D07-1096,0,\N,Missing
W09-3813,W06-2920,0,0.029325,"Missing"
W09-3813,W02-1001,0,0.0229705,"tuent structures (Sagae and Lavie, 2005), and more general de81 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 81–84, c Paris, October 2009. 2009 Association for Computational Linguistics 2 with very high accuracy (Gillick, 2009), we start by finding the dependency structure for each sentence. This includes part-of-speech (POS) tagging using a CRF tagger trained on the Wall Street Journal portion of the Penn Treebank, and transition-based dependency parsing using the shift-reduce arc-standard algorithm (Nivre, 2004) trained with the averaged perceptron (Collins, 2002). The dependency parser is also trained with the WSJ Penn Treebank, converted to dependencies using the head percolation rules of Yamada and Matsumoto (2003). Discourse segmentation is performed as a binary classification task on each word, where the decision is whether or not to insert an EDU boundary between the word and the next word. In a sentence of length n, containing the words w1, w2 … wn, we perform one classification per word, in order. For word wi, the binary choice is whether to insert an EDU boundary between wi and wi+1. The EDUs are then the words between EDU boundaries (assuming"
W09-3813,N09-2061,0,0.0150641,"ptimized at the level of individual shift-reduce actions, and can be used to drive parsers that produce competitive accuracy using greedy search strategies in linear time. Recent research in data-driven shift-reduce parsing has shown that the basic algorithms used for determining dependency trees (Nivre, 2004) can be extended to produce constituent structures (Sagae and Lavie, 2005), and more general de81 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 81–84, c Paris, October 2009. 2009 Association for Computational Linguistics 2 with very high accuracy (Gillick, 2009), we start by finding the dependency structure for each sentence. This includes part-of-speech (POS) tagging using a CRF tagger trained on the Wall Street Journal portion of the Penn Treebank, and transition-based dependency parsing using the shift-reduce arc-standard algorithm (Nivre, 2004) trained with the averaged perceptron (Collins, 2002). The dependency parser is also trained with the WSJ Penn Treebank, converted to dependencies using the head percolation rules of Yamada and Matsumoto (2003). Discourse segmentation is performed as a binary classification task on each word, where the deci"
W09-3813,W08-2122,0,0.0176532,"Missing"
W09-3813,P99-1047,0,0.769603,"Missing"
W09-3813,W03-3023,0,0.0555103,"pages 81–84, c Paris, October 2009. 2009 Association for Computational Linguistics 2 with very high accuracy (Gillick, 2009), we start by finding the dependency structure for each sentence. This includes part-of-speech (POS) tagging using a CRF tagger trained on the Wall Street Journal portion of the Penn Treebank, and transition-based dependency parsing using the shift-reduce arc-standard algorithm (Nivre, 2004) trained with the averaged perceptron (Collins, 2002). The dependency parser is also trained with the WSJ Penn Treebank, converted to dependencies using the head percolation rules of Yamada and Matsumoto (2003). Discourse segmentation is performed as a binary classification task on each word, where the decision is whether or not to insert an EDU boundary between the word and the next word. In a sentence of length n, containing the words w1, w2 … wn, we perform one classification per word, in order. For word wi, the binary choice is whether to insert an EDU boundary between wi and wi+1. The EDUs are then the words between EDU boundaries (assuming boundaries exist in the beginning and end of each sentence). The features used for classification are: the current word, its POS tag, its dependency label,"
W09-3813,A00-2018,0,\N,Missing
W09-3813,W04-0308,0,\N,Missing
W09-3813,W05-1513,1,\N,Missing
W09-3813,W01-1605,0,\N,Missing
W09-3813,J03-4003,0,\N,Missing
W09-3813,H05-1066,0,\N,Missing
W09-3813,C08-1095,1,\N,Missing
W09-3813,C04-1010,0,\N,Missing
W09-3813,N03-1030,0,\N,Missing
W09-3829,J99-2004,0,0.0986072,"clustering approaches that rely on lexical context (either linear or grammatical) to group words, resulting in a notion of word similarity that blurs syntactic and semantic characteristics of lexical items, we use unlexicalized syntactic context, so that words are clustered based only on their syntactic behavior. This way, we attempt to generate clusters that are more conceptually similar to part-of-speech tags or supertags 192 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 192–201, c Paris, October 2009. 2009 Association for Computational Linguistics (Bangalore and Joshi, 1999), but organized hierarchically to provide tagsets with varying levels of granularity. Our second contribution is a methodology for leveraging a high-accuracy parser to improve the accuracy of a parser that uses a different formalism (that represents different structural information), without the need to process the input with both parsers at run-time. In our experiments, we show that we can improve the accuracy of a fast dependency parser for predicate-argument structures by using a corpus which was previously automatically annotated using a highly accurate but considerably slower phrase-struc"
W09-3829,J92-4003,0,0.298969,"Missing"
W09-3829,A00-2018,0,0.467316,"c contexts. We then explore the use of these syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure. In our experiments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy. 1 Introduction Syntactic parsing of natural language has advanced greatly in recent years, in large part due to data-driven techniques (Collins, 1999; Charniak, 2000; Miyao and Tsujii, 2005; McDonald et al., 2005; Nivre et al., 2007) coupled with the availability of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such"
W09-3829,W02-1001,0,0.0318791,"dependencies. Figure 3 shows a predicateargument dependency structure following the annotation standard of the HPSG Treebank, where arrows point from head to modifier. We note that unlike in the widely known PropBank (Palmer et al., 2005) predicate-argument structures, argument labels start from ARG1 (not ARG0), and predicate-argument relationships are annotated for all words. One difference between in our implementation is that, instead of maximum entropy classification used by Sagae and Tsujii, we perform parser action classification using the averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which allows for the inclusion of all of Sagae and Tsujii’s features, in addition to a set of cluster-based features, while retaining fast training times. We now describe the parsing approach, starting with the dependency DAG parser that we use as a baseline, followed by how the syntactic cluster features were added to the baseline parser. 3.1 Arc-standard parsing for dependency DAGs Sagae and Tsujii (2008) describe two algorithms for dependency parsing with words that have multiple heads. Each corresponds to extensions of Nivre (2004)’s arc-standard and arc-eager algorithms for dependency ("
W09-3829,J02-3001,0,0.178137,"imilarity between words, and demonstrated its utility in improving the performance of a syntax-based Semantic Role Labeling system. The central idea behind their approach was that parse tree paths could be used as features for describing a word’s grammatical behavior. Figure 1: An example parse tree path from the verb ate to the argument NP He, represented as ↑VBD↑VP↑S↓NP. Parse tree paths are descriptions of tree transitions from a terminal (e.g. a verb) to a different node in a constituent parse tree of a sentence. Parse tree paths gained popularity in early Semantic Role Labeling research (Gildea and Jurafsky, 2002), where they were used as features describing the relationship between a verb and a particular semantic role label. For example, Figure 1 illustrates a parse tree path between a verb and a semantically related noun phrase. Gordon and Swanson viewed parse tree paths as features that could be used to describe the syntactic contexts of words in a corpus. In their approach, all of the possible parse tree paths that begin at a given word were identified in a large set of automatically generated constituent parse trees. The normalized frequency counts of unique parse tree paths were combined into a"
W09-3829,P07-1025,1,0.886707,"uted as the cosine distance between vector representations of the frequency of unique parse tree paths emanating from the word in a corpus of parse trees. In this research, we employ a novel encoding of syntactic parse tree paths that includes direction information and non-terminal node labels, but does not include lexical information or part-of-speech tags. Consequently, the resulting hierarchy groups words that appear in similar places in similar parse trees, regardless of its assigned part-of-speech tag. In this section we describe our approach in detail. 2.1 Parse tree path representation Gordon and Swanson (2007) first described a corpus-based method for calculating a measure of syntactic similarity between words, and demonstrated its utility in improving the performance of a syntax-based Semantic Role Labeling system. The central idea behind their approach was that parse tree paths could be used as features for describing a word’s grammatical behavior. Figure 1: An example parse tree path from the verb ate to the argument NP He, represented as ↑VBD↑VP↑S↓NP. Parse tree paths are descriptions of tree transitions from a terminal (e.g. a verb) to a different node in a constituent parse tree of a sentence"
W09-3829,D07-1097,0,0.0310715,"2007) coupled with the availability of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters were derived from a"
W09-3829,W08-2122,0,0.0353493,"stituent parsing model (McClosky et al., 2006), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (Koo et al., 2008), we use a large corpus of constituent trees (previously generated by an accurate constituent parser), which we use to produce syntactically derived clusters that are then used to improve a transition-based parser that outputs dependency graphs that reflect predicate-argument structure where words may be dependents of more than one parent. This type of representation is more general than dependency trees (Sagae and Tsujii, 2008; Henderson et al., 2008), and is suitable for representing both surface relations and long-distance dependencies (such as control, it-cleft and tough movement). The first contribution of this work is a novel approach for deriving syntactic word clusters from parsed text, grouping words by the general syntactic contexts where they appear, and not by n-gram word context (Brown et al., 1992) or by immediate dependency context (Lin, 1998). Unlike in clustering approaches that rely on lexical context (either linear or grammatical) to group words, resulting in a notion of word similarity that blurs syntactic and semantic c"
W09-3829,W99-0623,0,0.0509207,"Tsujii, 2005; McDonald et al., 2005; Nivre et al., 2007) coupled with the availability of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006),"
W09-3829,P08-1068,0,0.632514,"o combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (Koo et al., 2008), we use a large corpus of constituent trees (previously generated by an accurate constituent parser), which we use to produce synt"
W09-3829,P98-2127,0,0.368688,"edicate-argument structure where words may be dependents of more than one parent. This type of representation is more general than dependency trees (Sagae and Tsujii, 2008; Henderson et al., 2008), and is suitable for representing both surface relations and long-distance dependencies (such as control, it-cleft and tough movement). The first contribution of this work is a novel approach for deriving syntactic word clusters from parsed text, grouping words by the general syntactic contexts where they appear, and not by n-gram word context (Brown et al., 1992) or by immediate dependency context (Lin, 1998). Unlike in clustering approaches that rely on lexical context (either linear or grammatical) to group words, resulting in a notion of word similarity that blurs syntactic and semantic characteristics of lexical items, we use unlexicalized syntactic context, so that words are clustered based only on their syntactic behavior. This way, we attempt to generate clusters that are more conceptually similar to part-of-speech tags or supertags 192 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 192–201, c Paris, October 2009. 2009 Association for Computational Li"
W09-3829,J93-2004,0,0.0323845,"m a more general to a more finegrained level than that of parts-of-speech. As clusters become more fine-grained, they become more similar to supertags (Bangalore and Joshi, 1999). Clusters that represent more specific syntactic contexts can encode information about, for example, subcategorization. As these labels are derived empirically from a large corpus of syntactic parse trees, they accurately represent syntactic distinctions in real discourse at different granularities, in contrast to the single arbitrary granularity of theoretically derived part-ofspeech tags used in existing treebanks (Marcus et al., 1993). While it is sometimes useful to view types as having multiple part-of-speech tags at different levels of granularity (e.g. the 114 tags for the token “house”), it is often useful to select a single level of granularity to use across all tokens. For example, it is useful to know which one of the 114 cluster labels for “house” to use if exactly 100 part-of-speech distinctions are to be made among tokens in the set. These cluster labels can be identified by slicing the tree at the level for which there are exactly 100 branches, then using the label of the first branch point in each branch as th"
W09-3829,D08-1017,0,0.0529601,"veral recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (Koo"
W09-3829,N06-1020,0,0.12964,"ls can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (Koo et al., 2008), we use a large corpus of constituent trees (previously generated by an accurate constituent parser), which we"
W09-3829,P05-1012,0,0.0810729,"hese syntactic clusters in leveraging a large corpus of trees generated by a high-accuracy parser to improve the accuracy of another parser based on a different formalism for representing a different level of sentence structure. In our experiments, we use phrase-structure trees to produce syntactic word clusters that are used by a predicate-argument dependency parser, significantly improving its accuracy. 1 Introduction Syntactic parsing of natural language has advanced greatly in recent years, in large part due to data-driven techniques (Collins, 1999; Charniak, 2000; Miyao and Tsujii, 2005; McDonald et al., 2005; Nivre et al., 2007) coupled with the availability of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae an"
W09-3829,N04-1043,0,0.174365,"resentation of the cluster tree when it is sliced to produce exactly 60 clusters, 19 of which are individual types. For the other 41 clusters, we show only the most frequent word in the cluster and the number of additional words in the cluster. The scale line in the lower left of Figure 2 indicates the horizontal length of a calculated similarity between clusters of 0.1. 3 Transition-based dependency parsing with word clusters The clusters obtained with the approach described in section 2 provide sets of syntactic tags with varying levels of granularity. Previous work by Koo et al. (2008) and Miller et al. (2004) suggests that different levels of cluster granularity may be useful in natural language Figure 2: A hierarchical clustering of the top five thousand tokens in the BLLIP corpus, cut at 60 clusters. 195 Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. processing tasks with discriminative training. We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to be efficient and accurat"
W09-3829,W04-2407,0,0.0419924,"e Figure 2: A hierarchical clustering of the top five thousand tokens in the BLLIP corpus, cut at 60 clusters. 195 Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. processing tasks with discriminative training. We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to be efficient and accurate in dependency parsing of surface syntactic dependencies (Yamada and Matsumoto, 2003; Nivre et al., 2004; Hall et al., 2007) and predicate-argument parsing (Henderson et al., 2008; Sagae and Tsujii, 2008). Our experiments are based on an implementation of Sagae and Tsujii (2008)’s algorithm for basic shift-reduce parsing with multiple heads, which we use to identify predicate-argument dependencies extracted from the HPSG Treebank developed by Miyao et al. (2004). Using this data set allows for a comparison of our results with those obtained in previous work on datadriven HPSG predicate-argument analysis, while demonstrating the use of our clustering approach for cross-framework parser improvemen"
W09-3829,P05-1013,0,0.0622533,"w attach actions. This basic algorithm is only capable of producing labeled directed acyclic graphs where, if the nodes (which correspond to words) are placed on a left to right sequence on a horizontal line in the order in which the words appear in the input sentence, all arcs can be drawn above the nodes without crossing. This corresponds to the notion of projectivity that similarly limits the types of trees produced by Nivre’s algorithm. Just as in dependency parsing with tree structures, a way to effectively remove this limitation is the use of pseudo-projective transformations (Nivre and Nilsson, 2005), where arcs that cross have their heads moved towards the root and have their labels edited to reflect this change, often making it reversible. Once crossing arcs have been “lifted” so that no crossing arcs remain, the “projectivized” structures are used to train a parsing model. Projective structures produced by this model can be “deprojectivized” through the use of the edits in the arc labels, in an attempt to produce structures that conform to the scheme in the original data. Sagae and Tsujii also propose a simple arc reversal transform, which simply reverses the direction of a dependency"
W09-3829,P08-1108,0,0.0440514,"ity of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters were derived from a large corpus of plain text to improve a"
W09-3829,J05-1004,0,0.137811,"ork on datadriven HPSG predicate-argument analysis, while demonstrating the use of our clustering approach for cross-framework parser improvement, since the clusters were derived from syntactic trees in Penn Treebank format (as produced by the Charniak parser, without empty nodes, co-indexation or function tags), and used in the identification of HPSG Treebank predicate-argument dependencies. Figure 3 shows a predicateargument dependency structure following the annotation standard of the HPSG Treebank, where arrows point from head to modifier. We note that unlike in the widely known PropBank (Palmer et al., 2005) predicate-argument structures, argument labels start from ARG1 (not ARG0), and predicate-argument relationships are annotated for all words. One difference between in our implementation is that, instead of maximum entropy classification used by Sagae and Tsujii, we perform parser action classification using the averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which allows for the inclusion of all of Sagae and Tsujii’s features, in addition to a set of cluster-based features, while retaining fast training times. We now describe the parsing approach, starting with the dependency"
W09-3829,N06-2033,1,0.917966,"l., 2005; Nivre et al., 2007) coupled with the availability of large treebanks. Several recent efforts have started to look for ways to go beyond what individual annotated data sets and individual parser models can offer, looking to combine diverse parsing models, develop cross-framework interoperability and evaluation, and leverage the availability of large amounts of text available. Two research directions that have produced promising improvements on the accuracy of data-driven parsing are: (1) combining different parsers using ensemble techniques, such as voting (Henderson and Brill, 1999; Sagae and Lavie, 2006; Hall et al., 2007) and stacking (Nivre and McDonald, 2008; Martins et al., 2008), and (2) semi-supervised learning, where unlabeled data (plain text) is used in addition to a treebank (McClosky et al., 2006; Koo et al., 2008). In this paper we explore a new way to obtain improved parsing accuracy by using a large amount of unlabeled text and two parsers that use different ways of representing syntactic structure. In contrast to previous work where automatically generated constituent trees were used directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters"
W09-3829,C08-1095,1,0.901092,"directly to train a constituent parsing model (McClosky et al., 2006), or where word clusters were derived from a large corpus of plain text to improve a dependency parser (Koo et al., 2008), we use a large corpus of constituent trees (previously generated by an accurate constituent parser), which we use to produce syntactically derived clusters that are then used to improve a transition-based parser that outputs dependency graphs that reflect predicate-argument structure where words may be dependents of more than one parent. This type of representation is more general than dependency trees (Sagae and Tsujii, 2008; Henderson et al., 2008), and is suitable for representing both surface relations and long-distance dependencies (such as control, it-cleft and tough movement). The first contribution of this work is a novel approach for deriving syntactic word clusters from parsed text, grouping words by the general syntactic contexts where they appear, and not by n-gram word context (Brown et al., 1992) or by immediate dependency context (Lin, 1998). Unlike in clustering approaches that rely on lexical context (either linear or grammatical) to group words, resulting in a notion of word similarity that blurs"
W09-3829,W03-3023,0,0.354736,"be useful in natural language Figure 2: A hierarchical clustering of the top five thousand tokens in the BLLIP corpus, cut at 60 clusters. 195 Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. processing tasks with discriminative training. We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to be efficient and accurate in dependency parsing of surface syntactic dependencies (Yamada and Matsumoto, 2003; Nivre et al., 2004; Hall et al., 2007) and predicate-argument parsing (Henderson et al., 2008; Sagae and Tsujii, 2008). Our experiments are based on an implementation of Sagae and Tsujii (2008)’s algorithm for basic shift-reduce parsing with multiple heads, which we use to identify predicate-argument dependencies extracted from the HPSG Treebank developed by Miyao et al. (2004). Using this data set allows for a comparison of our results with those obtained in previous work on datadriven HPSG predicate-argument analysis, while demonstrating the use of our clustering approach for cross-framewo"
W09-3829,P05-1011,0,0.0390521,"Missing"
W09-3829,P81-1022,0,0.786435,"the cluster tree when it is sliced to produce exactly 60 clusters, 19 of which are individual types. For the other 41 clusters, we show only the most frequent word in the cluster and the number of additional words in the cluster. The scale line in the lower left of Figure 2 indicates the horizontal length of a calculated similarity between clusters of 0.1. 3 Transition-based dependency parsing with word clusters The clusters obtained with the approach described in section 2 provide sets of syntactic tags with varying levels of granularity. Previous work by Koo et al. (2008) and Miller et al. (2004) suggests that different levels of cluster granularity may be useful in natural language Figure 2: A hierarchical clustering of the top five thousand tokens in the BLLIP corpus, cut at 60 clusters. 195 Figure 3: Predicate-argument dependency structure following the HPSG Treebank standard. processing tasks with discriminative training. We add the syntactic clusters as features in a transition-based parser that uses a classifier to decide among shift/reduce parser actions based on the local context of the decision. This transition-based parsing approach has been found to be efficient and accurat"
W09-3829,W04-0308,0,\N,Missing
W09-3829,J03-4003,0,\N,Missing
W09-3829,P07-1079,1,\N,Missing
W09-3829,C98-2122,0,\N,Missing
W09-3829,D07-1096,0,\N,Missing
W09-3902,E09-1085,0,0.188525,"Missing"
W09-3902,J96-1002,0,0.0158956,"Missing"
W09-3902,P03-1070,0,\N,Missing
W09-3902,N09-2014,1,\N,Missing
W09-3902,hartholt-etal-2008-common,1,\N,Missing
W10-0906,P98-1013,0,0.406738,"eresting directions for future work. 1 Introduction The acquisition of open-domain knowledge in support of commonsense reasoning has long been a bottleneck within artificial intelligence. Such reasoning supports fundamental tasks such as textual entailment (Giampiccolo et al., 2008), automated question answering (Clark et al., 2008), and narrative comprehension (Graesser et al., 1994). These tasks, when conducted in open domains, require vast amounts of commonsense knowledge pertaining to states, events, and their causal and temporal relationships. Manually created resources such as FrameNet (Baker et al., 1998), WordNet (Fellbaum, 1998), and Cyc (Lenat, 1995) encode many aspects of commonsense knowledge; however, coverage of causal and temporal relationships remains low for many domains. Gordon and Swanson (2008) argued that the commonsense tasks of prediction, explanation, and imagination (collectively called envisionment) can 43 Andrew S. Gordon and Kenji Sagae Institute for Creative Technologies University of Southern California {gordon,sagae}@ict.usc.edu be supported by knowledge mined from a large corpus of personal stories written by Internet weblog authors.1 Gordon and Swanson (2008) identifi"
W10-0906,D09-1110,0,0.0469839,"Missing"
W10-0906,P05-1022,0,0.0561532,"ing personal stories. We addressed the second step - knowledge extraction by parsing the corpus using a Rhetorical Structure Theory (Carlson and Marcu, 2001) parser based on the one described by Sagae (2009). The parser performs joint syntactic and discourse dependency 3 The system (called SayAnything) is available at http://sayanything.ict.usc.edu parsing using a stack-based, shift-reduce algorithm with runtime that is linear in the input length. This lightweight approach is very efficient; however, it may not be quite as accurate as more complex, chartbased approaches (e.g., the approach of Charniak and Johnson (2005) for syntactic parsing). We trained the discourse parser over the causal and temporal relations contained in the RST corpus. Examples of these relations are shown below: (1) [cause Packages often get buried in the load] [result and are delivered late.] (2) [bef ore Three months after she arrived in L.A.] [af ter she spent $120 she didn’t have.] The RST corpus defines many fine-grained relations that capture causal and temporal properties. For example, the corpus differentiates between result and reason for causation and temporal-after and temporal-before for temporal order. In order to increas"
W10-0906,W08-2205,0,0.0118338,"r-time, joint syntax/discourse dependency parser for this purpose, and we show how the extracted discourse relations can be used to generate opendomain textual inferences. Our evaluations of the discourse parser and inference models show some success, but also identify a number of interesting directions for future work. 1 Introduction The acquisition of open-domain knowledge in support of commonsense reasoning has long been a bottleneck within artificial intelligence. Such reasoning supports fundamental tasks such as textual entailment (Giampiccolo et al., 2008), automated question answering (Clark et al., 2008), and narrative comprehension (Graesser et al., 1994). These tasks, when conducted in open domains, require vast amounts of commonsense knowledge pertaining to states, events, and their causal and temporal relationships. Manually created resources such as FrameNet (Baker et al., 1998), WordNet (Fellbaum, 1998), and Cyc (Lenat, 1995) encode many aspects of commonsense knowledge; however, coverage of causal and temporal relationships remains low for many domains. Gordon and Swanson (2008) argued that the commonsense tasks of prediction, explanation, and imagination (collectively called envisionm"
W10-0906,P06-1043,0,0.0240877,"he last two problems identified by Gordon and Swanson (2008): story analysis and envisioning with the analysis results. 5.1 Story analysis As in other NLP tasks, we observed significant performance degradation when moving from the training genre (newswire) to the testing genre (Internet weblog stories). Because our discourse parser relies heavily on lexical and syntactic features for classification, and because the distribution of the feature values varies widely between the two genres, the performance degradation is to be expected. Recent techniques in parser adaptation for the Brown corpus (McClosky et al., 2006) might be usefully applied to the weblog genre as well. Our supervised classification-based approach to discourse parsing could also be improved with additional training data. Causal and temporal relations are instantiated a combined 2,840 times in the RST corpus, with a large majority of these being causal. In contrast, the Penn Discourse TreeBank (Prasad et al., 2008) contains 7,448 training instances of causal relations and 2,763 training instances of temporal relations. This represents a significant increase in the amount of training data over the RST corpus. It would be informative to com"
W10-0906,P09-1095,0,0.0302769,"Missing"
W10-0906,W09-3813,1,0.822373,"Missing"
W10-0906,W03-0902,0,0.0297584,"cifically target the use of content that is collaboratively created by Internet users. Of particular relevance to the present work is the weblog corpus developed by Burton et al. (2009), which was used for the data challenge portion of the International Conference on Weblogs and Social Media (ICWSM). The ICWSM weblog corpus (referred to here as Spinn3r) is freely available and comprises tens of millions of weblog entries posted between August 1st, 2008 and October 1st, 2008. Gordon et al. (2009) describe an approach to knowledge extraction over the Spinn3r corpus using techniques described by Schubert and Tong (2003). In this approach, logical propositions (known as factoids) are constructed via approximate interpretation of syntactic analyses. As an example, the system identified a factoid glossed as “doors to a room may be opened”. Gordon et al. (2009) found that the extracted factoids cover roughly half of the factoids present in the corresponding Wikipedia2 articles. We used a subset of the Spinn3r corpus in our work, but focused on discourse analyses of entire texts instead of syntactic analyses of single sentences. Our goal was to extract general causal and temporal propositions instead of the fine-"
W10-0906,C98-1013,0,\N,Missing
W10-2606,A00-2018,0,0.148672,". Naturally, it may be that these conditions hold for some pairs of source and target domains but not others. In the next section, we present experiments that investigate whether simple selftraining is effective for one particular set of training (WSJ) and testing (Brown) corpora, which are widely used in parsing research for English. 1. A generative parser is trained using a treebank in a specific source domain. 3 2. The parser is used to generate parse trees from text in a target domain, different from the source domain. Domain adaptations experiments In our experiments we use primarily the Charniak (2000) parser. In a few specific experiments we also use the Charniak and Johnson (2005) reranker; such cases are noted explicitly and are not central to the paper, serving mostly for comparisons. We follow the three steps described in section 2.3. The manually labeled training corpus is the standard WSJ training sections of the Penn Treebank (sections 02 to 21). Sections 22 and 23 are used as in-domain development and testing sets, respectively. The outof-domain material is taken from the Brown portion of the Penn Treebank. We use the same Brown test set as McClosky et al. (2006b), every tenth sent"
W10-2606,P05-1022,0,0.123833,"urce and target domains but not others. In the next section, we present experiments that investigate whether simple selftraining is effective for one particular set of training (WSJ) and testing (Brown) corpora, which are widely used in parsing research for English. 1. A generative parser is trained using a treebank in a specific source domain. 3 2. The parser is used to generate parse trees from text in a target domain, different from the source domain. Domain adaptations experiments In our experiments we use primarily the Charniak (2000) parser. In a few specific experiments we also use the Charniak and Johnson (2005) reranker; such cases are noted explicitly and are not central to the paper, serving mostly for comparisons. We follow the three steps described in section 2.3. The manually labeled training corpus is the standard WSJ training sections of the Penn Treebank (sections 02 to 21). Sections 22 and 23 are used as in-domain development and testing sets, respectively. The outof-domain material is taken from the Brown portion of the Penn Treebank. We use the same Brown test set as McClosky et al. (2006b), every tenth sentence in the corpus. Another tenth of the corpus is used as a development set, and"
W10-2606,P08-1067,0,0.0427435,"Missing"
W10-2606,J93-2004,0,0.0351093,"Missing"
W10-2606,N06-1020,0,0.208623,"t on Semantic Role Labeling Kenji Sagae Institute for Creative Technologies University of Southern California Marina del Rey, CA 90292 sagae@ict.usc.edu We present experiments with a simple selftraining approach to semi-supervised parser domain adaptation that produce results that contradict the commonly held assumption that improved parser accuracy cannot be obtained by self-training a generative parser without reranking (Charniak, 1997; Steedman et al., 2003; McClosky et al., 2006b, 2008).1 We compare this simple self-training approach to the selftraining with reranking approach proposed by McClosky et al. (2006b), and show that although McClosky et al.’s approach produces better labeled bracketing precision and recall on out-ofdomain sentences, higher F-score on syntactic parses may not lead to an overall improvement in results obtained in NLP applications that include parsing, contrary to our expectations. This is evidenced by results obtained when different adaptation approaches are applied to a parser that serves as a component in a semantic role labeling (SRL) system. This is, to our knowledge, the first attempt to quantify the benefits of semisupervised parser domain adaptation in semantic role"
W10-2606,P06-1043,0,0.359379,"t on Semantic Role Labeling Kenji Sagae Institute for Creative Technologies University of Southern California Marina del Rey, CA 90292 sagae@ict.usc.edu We present experiments with a simple selftraining approach to semi-supervised parser domain adaptation that produce results that contradict the commonly held assumption that improved parser accuracy cannot be obtained by self-training a generative parser without reranking (Charniak, 1997; Steedman et al., 2003; McClosky et al., 2006b, 2008).1 We compare this simple self-training approach to the selftraining with reranking approach proposed by McClosky et al. (2006b), and show that although McClosky et al.’s approach produces better labeled bracketing precision and recall on out-ofdomain sentences, higher F-score on syntactic parses may not lead to an overall improvement in results obtained in NLP applications that include parsing, contrary to our expectations. This is evidenced by results obtained when different adaptation approaches are applied to a parser that serves as a component in a semantic role labeling (SRL) system. This is, to our knowledge, the first attempt to quantify the benefits of semisupervised parser domain adaptation in semantic role"
W10-2606,C08-1071,0,0.0860647,"Missing"
W10-2606,J05-1004,0,0.0218433,"Missing"
W10-2606,W05-0625,0,0.0267858,"Missing"
W10-2606,W05-0620,0,0.0577983,"Missing"
W10-2606,N03-1027,0,0.0362036,"roceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 37–44, c Uppsala, Sweden, 15 July 2010. 2010 Association for Computational Linguistics treebank), and a larger set of unlabeled data (plain text). Bacchiani and Roark (2003) obtained positive results in unsupervised domain adaptation of language models by using a speech recognition system with an out-of-domain language model to produce an automatically annotated training corpus that is used to adapt the language model using a maximum a posteriori (MAP) adaptation strategy. In subsequent work (Roark and Bacchiani, 2003), this MAP adaptation approach was applied to PCFG adaptation, where an out-ofdomain parser was used to annotate an in-domain corpus automatically with multiple candidate trees per sentence. A substantial improvement was achieved in out-of-domain parsing, although the obtained accuracy level was still far below that obtained with domain-specific training data. More recent work in unsupervised domain adaptation for state-of-the-art parsers has achieved accuracy levels on out-of-domain text that is comparable to that achieved with domainspecific training data (McClosky et al., 2006b). This is do"
W10-2606,D07-1111,1,0.615991,"t al. found that the discriminative model does not improve when retrained with its own output. 2.2 Self-training without reraking Although there have been instances of selftraining (or similar) approaches that produced improved parser accuracy without reranking, the success of these efforts are often attributed to other specific factors. Reichart and Rappoport (2007) obtained positive results in in-domain and out-of-domain scenarios with self-training without reranking, but under the constant condition that only a relatively small set of manually labeled data is used as the seed training set. Sagae and Tsujii (2007) improved the out-of-domain accuracy of a dependency parser trained on the entire WSJ training set (40k sentences) by using unlabeled data in the same domain as the out-of-domain test data (biomedical text). However, they used agreement between different parsers to estimate the quality of automatically generated training instances and selected only sentences with high estimated accuracy. Although the parser improves when trained with its own output, the training instances are selected through the use of a separate dependency parsing model. Self-training with reraking McClosky et al. (2006b) pr"
W10-2606,E03-1008,0,0.794753,"aptation for state-of-the-art parsers has achieved accuracy levels on out-of-domain text that is comparable to that achieved with domainspecific training data (McClosky et al., 2006b). This is done in a self-training setting, where a parser trained on a treebank (in a seed domain) is used to parse a large amount of unlabeled data in the target domain (assigning only one parse per sentence). The automatically parsed corpus is then used as additional training data for the parser. Although initial attempts to improve indomain parsing accuracy with self-training were unsuccessful (Charniak, 1997; Steedman et al., 2003), recent work has shown that self-training can work in specific conditions (McClosky et al., 2006b), and in particular it can be used to improve parsing accuracy on out-of-domain text (Reichart and Rappoport, 2007). 2.1 their approach. That work used text from the LA Times (taken from the North American News Corpus, or NANC), which is presumably more similar to the parser’s training material than to text in the Brown corpus, and resulted not only in an improvement of parser accuracy on out-ofdomain text (from the Brown corpus), but also in an improvement in accuracy on in-domain text (the stan"
W10-2606,W99-0623,0,\N,Missing
W10-2606,P07-1078,0,\N,Missing
W11-2006,H91-1060,0,0.0259323,"Missing"
W11-2006,J96-1002,0,0.12464,"Missing"
W11-2006,W10-4345,0,0.104337,"ples without annotation or formal modeling would require a seemingly insurmountable quantity of dialogues to serve as training data. We address this problem in a data collection framework with four main characteristics: (1) we sidestep the problem of learning natural language generation by using a fixed predefined set of utterances for the Amani character. This so-called “utterance selection” approach has been used in a number of dialogue systems (Zukerman and Marom, 2006; Sellberg and Jnsson, 2008; Kenny et al., 2007, for example) and often serves as a reasonable approximation to generation (Gandhe and Traum, 2010); (2) we collect dialogues from human participants who play the parts of Amani and the commander in a structured role play framework (Section 3.1); (3) we enrich the dialogues collected in the structured role play step with additional paraphrases for the utterances of the commander, in an attempt to deal with large variability of natural language input, even for a limited domain conversational dialogue scenario (Section 3.2); (4) we further augment the existing dialogue data by adding acceptable alternatives to the dialogue acts of the Amani role through the use of external referees (Section 3"
W11-2006,P02-1040,0,0.0806701,"Missing"
W11-2006,N10-1020,0,0.0143282,"of Southern California Playa Vista, CA 90094 {devault,leuski,sagae}@ict.usc.edu Abstract In this paper, we explore data collection and machine learning techniques that enable the implementation of domain-specific conversational dialogue policies through a relatively small data collection effort, and without any formal modeling. We present a case study, which serves to illustrate some of the possibilities in our framework. In contrast to recent work on data-driven dialogue policy learning that learns dialogue behavior from existing data sources (Gandhe and Traum, 2007; Jafarpour et al., 2009; Ritter et al., 2010), we address the task of authoring a dialogue policy from scratch with a specific purpose, task and scenario in mind. We examine the data collection, learning and evaluation steps. The contributions of this work include a data collection and enrichment framework without formal modeling, and the creation of dialogue policies from the collected data. We also propose a framework for evaluating learned policies. We show, for the scenario in our case study, that these techniques deliver promising levels of performance, and point to possible future developments in data-driven dialogue policy creatio"
W11-2006,sellberg-jonsson-2008-using,0,0.0143742,"al dialogue scenario such as the Amani scenario suggests that learning acceptable dialogue behavior from surface text examples without annotation or formal modeling would require a seemingly insurmountable quantity of dialogues to serve as training data. We address this problem in a data collection framework with four main characteristics: (1) we sidestep the problem of learning natural language generation by using a fixed predefined set of utterances for the Amani character. This so-called “utterance selection” approach has been used in a number of dialogue systems (Zukerman and Marom, 2006; Sellberg and Jnsson, 2008; Kenny et al., 2007, for example) and often serves as a reasonable approximation to generation (Gandhe and Traum, 2010); (2) we collect dialogues from human participants who play the parts of Amani and the commander in a structured role play framework (Section 3.1); (3) we enrich the dialogues collected in the structured role play step with additional paraphrases for the utterances of the commander, in an attempt to deal with large variability of natural language input, even for a limited domain conversational dialogue scenario (Section 3.2); (4) we further augment the existing dialogue data"
W12-1620,H05-1127,0,0.0329398,"user to say anything at any time, but have fairly flat dialogue policies, e.g., (Leuski et al., 2006). These systems can work well when the user is naturally in charge, such as in interviewing a character, but may not be suitable for situations in which a character is asking the user questions, or mixed initiative is desired. True mixed initiative is notoriously difficult for a manually constructed call-flow graph, in which the system might want to take different actions in response to similar stimuli, depending on local utilities. Reinforcement learning approaches (Williams and Young, 2007; English and Heeman, 2005) can be very useful at learning local policy optimizations, but they require large amounts of training data and a well-defined global reward structure, are difficult to apply to a large state-space and remove some of the control, which can be undesirable (Paek and Pieraccini, 2008). Our approach to this problem is a forward-looking reward seeking agent, similar to that described in (Liu and Schubert, 2010), though with support for complex dialogue interaction and its authoring. Authoring involves design of local subdialogue networks with pre-conditions and effects, and also qualitative reward"
W12-1620,W06-1303,1,0.738775,"s are only allowed at certain points in the dialogue. System initiative also usually makes it easier for a domain expert to design a dialogue policy that will behave as desired.1 Such systems can work well if the limited options available to the user are what the user wants to do, but can be problematic otherwise, especially if the user has a choice of whether or not to use the system. In particular, this approach may not be well suited to an application like SimCoach. At the other extreme, some systems allow the user to say anything at any time, but have fairly flat dialogue policies, e.g., (Leuski et al., 2006). These systems can work well when the user is naturally in charge, such as in interviewing a character, but may not be suitable for situations in which a character is asking the user questions, or mixed initiative is desired. True mixed initiative is notoriously difficult for a manually constructed call-flow graph, in which the system might want to take different actions in response to similar stimuli, depending on local utilities. Reinforcement learning approaches (Williams and Young, 2007; English and Heeman, 2005) can be very useful at learning local policy optimizations, but they require"
W12-1620,2005.sigdial-1.1,0,0.0448207,". Authoring involves design of local subdialogue networks with pre-conditions and effects, and also qualitative reward categories (goals), which can be instantiated with specific reward values. The dialogue manager, called FLoReS, can locally optimize policy decisions, by calculating the highest overall expected reward for the best sequence of subdialogues from a given point. Within a subdialogue, authors can craft the specific structure of interaction. Briefly, the main modules that form FLoReS are: • The information state, a propositional knowl1 Simple structures, such as a call flow graph (Pieraccini and Huerta, 2005) and branching narrative for interactive games (Tavinor, 2009) will suffice for authoring. 138 edge base that keeps track of the current state of the conversation. The information state supports missing or unknown information by allowing atomic formulas to have 3 possible values: true, false and null. • A set of inference rules that allows the system to add new knowledge to its information state, based on logical reasoning. Forward inference facilitates policy authoring by providing a mechanism to specify information state updates that are independent of the specific dialogue context.2 • An ev"
W13-4061,W12-1620,1,0.833814,"thors and varying team sizes to create flexible interactions by automating many editing workflows while limiting complexity and hiding architectural concerns. Finished characters can be published directly to web servers, enabling highly interactive applications. 1 Introduction To support the creation of a virtual guide system called SimCoach (Rizzo et al, 2011) designed to help military service personnel and their families understand behavioral healthcare issues and learn about support resources, a core virtual human architecture that included a new dialogue management approach was developed (Morbini et al., 2012b). SimCoach is an embodied, conversational virtual human guide delivered via the web and is supported by a flexible information state dialogue manager called FLoReS designed to support mixed initiative dialogue with conversational systems. Morbini et al. (2012a) provide a detailed description of the dialogue manager. Although FLoReS supports a wide variety of virtual human character behaviors, these must be specified in dialogue policies that must be authored manually. Initially, authoring for this dialogue manager required coding of policies using a custom programming language. Therefore sig"
W13-4064,aggarwal-etal-2012-twins,1,0.766178,"d new hires, who acted as test subjects. This dataset has 4K audio files each annotated with one of the 117 different NLU semantic classes. answering characters, but unlike SGTs Blackwell and Star, the response is a whole dialogue sequence, potentially involving interchange from both characters, rather than a single character turn. There are two types of users for the Twins: demonstrators, who are museum staff members, using head-mounted microphones, and museum visitors, who use a Shure 522 table-top mounted microphone (Traum et al., 2012). More on analysis of the museum data can be found in (Aggarwal et al., 2012). We also investigated speech recognition and NLU performance in this domain in Morbini et al. (2012). This dataset contains 14K audio files each annotated with one of the 168 possible response sequences. The division in training development and test is the same used in Morbini et al. (2012) (10K for training, the rest equally divided between development and test). Amani (Artstein et al., 2009b; Artstein et al., 2011) is an advanced question-answering character used as a prototype for systems meant to train soldiers to perform tactical questioning. The users are in between real users and test"
W13-4064,W11-2037,1,0.821297,"nstructed from known parts using a generative approach. A second issue is that even though we can cast the problem as multi-class classification, classification accuracy is not always the most appropriate metric of NLU quality. For question-answering characters, getting an appropriate and relevant reply is more important than picking the exact reply selected by a human domain designer or annotator: there might be multiple good answers, or even the best available answer might not be very good. For that reason, the question-answering characters allow an “off-topic” answer and Errorreturn plots (Artstein, 2011) might be necessary to choose an optimal threshold. For the SASO-EN system, slot-filler metrics such as precision, recall, and f-score are more appropriate than frame accu400 conventional ASR scale and use word accuracy, so that higher numbers signify better performance on both scales.13 Figure 1 shows the results obtained in the 3 dialogue systems by the various ASR systems. The figures plot ASR performance against NLU performance; NLU results on manual transcriptions are included for comparison. There are too few data points for the correlations between ASR and NLU performance to be signific"
W13-4064,2007.sigdial-1.23,0,0.036582,"derstand or react well to, even when an alternative formulation is known to work. ASR performance as well. One important aspect is the broad physical differences among speakers, such as male vs female, adult vs child (e.g. Bell and Gustafson, 2003), or language proficiency/accent, that will have implications for the acoustics of what is said, and ASR results. Other aspects of users have implications for what will be said, and how successful the interface may be, overall. Many (e.g. Hassel and Hagen, 2006; Jokinen and Kanto, 2004) have looked at the differences between novice and expert users. Ai et al. (2007a) also points out a difference between real users and recruited subjects. Real users also come in many different flavors, depending on their purposes. E.g. are they interacting with the system for fun, to do a specific task that they need to get done, to learn something (specific or general), or with some other purpose in mind? We considered the following classes of users, ordered from easiest to hardest to get to acceptable performance and robustness levels: 3.2 Types of Dialogue System Genres Dialogue Genres can be distinguished along many lines, e.g. the number and relationship of particip"
W13-4064,N03-1001,0,0.0442742,"Missing"
W13-4064,robinson-etal-2008-ask,1,0.689243,"ring character, with no internal semantic representation and the primary NLU task merged with Dialogue management as selecting the best response. The original users were ICT demonstrators. However, there were also some experiments with recruited participants (Leuski et al., 2006a; Leuski et al., 2006b). Later SGT Blackwell became a part of the “best design in America” triennial at the Cooper-Hewitt Museum in New York City, and the data set here is from visitors to the museum, who are mostly casual users, but range from expert to red-team. Users spoke into a mounted directional microphone (see Robinson et al., 2008 for more details). Slot-filling Probably the most common type of dialogue system (at least in the research community) is slot-filling. Here the dialogue is fairly structured, with an initial greeting phase, then one or more tasks, which all start with the user selecting the task, and the system taking over initiative to “fill” and possibly confirm the needed slots, before retrieving some information from a database, or performing a simple service.11 This genre also requires a semantic representation, at least of the slots and acceptable values. Generally, the set of possible values is large e"
W13-4064,P04-1012,0,0.0133144,"ak” the system, or show it as not-competent, and may try to do things the system can’t understand or react well to, even when an alternative formulation is known to work. ASR performance as well. One important aspect is the broad physical differences among speakers, such as male vs female, adult vs child (e.g. Bell and Gustafson, 2003), or language proficiency/accent, that will have implications for the acoustics of what is said, and ASR results. Other aspects of users have implications for what will be said, and how successful the interface may be, overall. Many (e.g. Hassel and Hagen, 2006; Jokinen and Kanto, 2004) have looked at the differences between novice and expert users. Ai et al. (2007a) also points out a difference between real users and recruited subjects. Real users also come in many different flavors, depending on their purposes. E.g. are they interacting with the system for fun, to do a specific task that they need to get done, to learn something (specific or general), or with some other purpose in mind? We considered the following classes of users, ordered from easiest to hardest to get to acceptable performance and robustness levels: 3.2 Types of Dialogue System Genres Dialogue Genres can"
W13-4064,W06-1303,1,0.719162,"s on the size of the training and development sets may be found in Yao et al. (2010), here we report only the numbers relevant to the Twins domain and to the NLU analysis, which are not in Yao et al. (2010). SGT Blackwell was created as a virtual human technology demonstration for the 2004 Army Science Conference. This is a question-answering character, with no internal semantic representation and the primary NLU task merged with Dialogue management as selecting the best response. The original users were ICT demonstrators. However, there were also some experiments with recruited participants (Leuski et al., 2006a; Leuski et al., 2006b). Later SGT Blackwell became a part of the “best design in America” triennial at the Cooper-Hewitt Museum in New York City, and the data set here is from visitors to the museum, who are mostly casual users, but range from expert to red-team. Users spoke into a mounted directional microphone (see Robinson et al., 2008 for more details). Slot-filling Probably the most common type of dialogue system (at least in the research community) is slot-filling. Here the dialogue is fairly structured, with an initial greeting phase, then one or more tasks, which all start with the u"
W13-4064,yao-etal-2010-practical,1,0.963582,"du Abstract While this increased choice of quality recognizers is of great benefit to dialogue system developers, it also creates a dilemma – which recognizer to use? Unfortunately, the answer is not simple – it depends on a number of issues, including the type of dialogue domain, availability and amount of training data, availability of internet connectivity for the runtime system, and speed of response needed. In this paper we assess several freely available speech recognition engines, and examine their suitability and performance in several dialogue systems. Here we extend the work done in Yao et al. (2010) focusing in particular on cloud based freely available ASR systems. We include 2 local ASRs for reference, one of which was also used in the earlier work for easy comparison. We present an analysis of several publicly available automatic speech recognizers (ASRs) in terms of their suitability for use in different types of dialogue systems. We focus in particular on cloud based ASRs that recently have become available to the community. We include features of ASR systems and desiderata and requirements for different dialogue systems, taking into account the dialogue genre, type of user, and oth"
W14-4309,J96-1002,0,0.044855,"Missing"
W14-4309,N09-2014,1,0.890463,"Missing"
W14-4309,N13-1129,0,0.0113405,"cted to surpass the initial model M0 . In general, we replace the running NLU model Mr whenever we have a better performing Mi model. This straightforward process can be used to obtain increasingly more accurate language understanding, at the cost of data annotation in the form of labelling utterances with categories that are defined according to the needs of the specific system and the specific domain. The categories may be based on dialogue acts, e.g. (Core and Allen, 1997; Bunt et al., 2010), user information needs, e.g. (Moreira et al., 2011), or stand in for entire semantic frames, e.g. (DeVault and Traum, 2013). The technical nature of the task of categorizing utterances in schemes such as these usually means that substantial time is required of an expert annotator. 2.1 Annotation as a human intelligence task Although the task of annotating NLU training data involves assigning categories with technical defi1 For every time i and j with i < j it holds that Ti ⊆ Tj . 70 System Hello and welcome. ... User Are you married? System Yeah, forty-four years this September. I can’t believe Linda’s stuck with me this long. System Okay, let’s get to it. Why are you here today? User I’ve been having trouble slee"
W14-4309,W10-4302,0,0.0174061,"in conversational dialogue systems. Experiments using a webaccessible conversational character that interacts with a varied user population show that a dramatic improvement in natural language understanding and a substantial reduction in expert annotation effort can be achieved by leveraging non-expert annotation. 1 Introduction Robust Natural Language Understanding (NLU) remains a challenge in conversational dialogue systems that allow arbitrary natural language input from users. Although data-driven approaches are now commonly used to address the NLU problem as one of classification, e.g. (Heintze et al., 2010; Leuski and Traum, 2010; Moreira et al., 2011), where input utterances are mapped automatically into system-specific categories, the dependence of such approaches on training data annotated with semantic classes or dialogue acts creates a chicken and egg problem: user utterances are needed to create the annotated training data necessary for NLU by classification, but these cannot be collected without a working system that users can interact with. Common solutions to this problem include the use of Wizard-of-Oz data collection, where a human expert manually provides the functionality of data-d"
W14-4309,bunt-etal-2010-towards,0,\N,Missing
W14-5908,W13-4032,1,0.893149,"eviews). The following is the hypothesis that we specifically tested with our experiments: Hypothesis 1: Verbal behavior, as captured by lexical usage, is indicative of persuasiveness in online social multimedia content, irrespective of whether the opinion expressed is positive or negative. Paraverbal behaviors indicative of hesitation can constitute important information for predicting persuasiveness. For instance, a speaker’s stuttering or breaking his/her speech with filled pauses (such as um and uh) has influence on how other people perceive his/her persuasiveness. Although previous work (DeVault et al, 2013) suggests paraverbal behavior may be indicative of depression, another work on emotion prediction however, (Devillers et al., 2006) raised questions about its predictive power when compared to using standard cues derived from lexical usage. This leads us to our second hypothesis on paraverbal behaviors in the context of predicting persuasiveness: Hypothesis 2: Paraverbal behaviors related to hesitation are indicative of persuasiveness in online social multimedia content. Past research highlights the importance of the knowledge of the affective state of a document towards its perceived persuasi"
W14-5908,W02-1011,0,0.0147775,"Missing"
W15-1513,P14-1023,0,0.0473059,"end to overcome drift in the other. However, a precise accounting of the variations in this behavior is one area where further work is required. In particular, exploring the impact of variations in the weights of this linear combination (easily done simply by weighting one of the vectors prior to concatenation) is an obvious first step. Additionally, it will be interesting to explore the combination of more than two vectors, effectively defining a new semantic space over those bases. Generally, in the continuing discussion about the relative merits of count-based and prediction-based methods (Baroni et al., 2014), the present work suggests that there may not be a need to choose. By combining both methods through simple compositional functions, we show that it is possible to combine the benefits of both models in a single hybrid representation. Given the extensive work put into the development of distributed representations and the known variations in relative strengths and weaknesses, the benefit of these simple combination schemes is intriguing. We plan to explore the effects of vector combination in downstream tasks. This work provides several key initial pieces. The first is an existence proof that"
W15-1513,P14-1130,0,0.0164907,"antic vector representations. Introduction Distributed vector representations allow words to be represented in a continuous space. By learning these representations using unsupervised methods over large corpora, these models capture key distributional aspects of word function and meaning. In particular, such representations provide a valuable response to issues of data sparsity by providing simple similarity measures between terms. Whether used indirectly in terms of those similarity measures (e.g. for smoothing in language models) or directly as features to a model for tasks such as parsing (Lei et al., 2014), these representations have proved increasingly valuable to a variety of NLP tasks (Bengio et al., 2013). Given these benefits, a number of approaches have been explored for generating these representations beginning with early work in connectionist Recent work has shown that relationships in these models (such as gender differences or pluralization) are often linear (Mikolov et al., 2013b). Drawing on this, we explore composition through linear combinations of these representational spaces. In particular, we explore combinations of a popular neural network method (Word2Vec) (Mikolov et al.,"
W15-1513,P14-2050,0,0.104137,"consider cooccurrences at the document level, often making use of weighting factors such as tf-idf. Techniques such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990) make use of this approach, building a word-document co-occurrence matrix for an entire corpus. Other methods make use of a sliding window of words in a corpus, considering only words which occur within a certain distance of the target. The skip gram and continuous bag of words methods in Word2Vec are examples of this approach. Rather than a fixed width sampling space, other approaches make use of either sentence level (Levy and Goldberg, 2014a) (critical when using sentence-level parsing information) or paragraph windows, making use of the structure of the text itself to determine the window size. Yet another class of methods makes use of randomly initialized word values with updates based on the local context. Techniques such as as BEAGLE (Jones and Mewhort, 2007) capture ordering information through the use of circular convolutions while DVRS makes use of piecewise vector multiplications over random projections. As we make particular use of DVRS and Word2Vec, it is worth looking at the methods used by each in more detail. 2.1 DV"
W15-1513,W14-1618,0,0.18788,"consider cooccurrences at the document level, often making use of weighting factors such as tf-idf. Techniques such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990) make use of this approach, building a word-document co-occurrence matrix for an entire corpus. Other methods make use of a sliding window of words in a corpus, considering only words which occur within a certain distance of the target. The skip gram and continuous bag of words methods in Word2Vec are examples of this approach. Rather than a fixed width sampling space, other approaches make use of either sentence level (Levy and Goldberg, 2014a) (critical when using sentence-level parsing information) or paragraph windows, making use of the structure of the text itself to determine the window size. Yet another class of methods makes use of randomly initialized word values with updates based on the local context. Techniques such as as BEAGLE (Jones and Mewhort, 2007) capture ordering information through the use of circular convolutions while DVRS makes use of piecewise vector multiplications over random projections. As we make particular use of DVRS and Word2Vec, it is worth looking at the methods used by each in more detail. 2.1 DV"
W15-1513,N13-1090,0,0.0774772,"ding simple similarity measures between terms. Whether used indirectly in terms of those similarity measures (e.g. for smoothing in language models) or directly as features to a model for tasks such as parsing (Lei et al., 2014), these representations have proved increasingly valuable to a variety of NLP tasks (Bengio et al., 2013). Given these benefits, a number of approaches have been explored for generating these representations beginning with early work in connectionist Recent work has shown that relationships in these models (such as gender differences or pluralization) are often linear (Mikolov et al., 2013b). Drawing on this, we explore composition through linear combinations of these representational spaces. In particular, we explore combinations of a popular neural network method (Word2Vec) (Mikolov et al., 2013a) with Distributed Vector Representations in Sigma (DVRS) (Ustun et al., 2014), a method based on prior work in holographic representation (Jones and Mewhort, 2007). We demonstrate that various methods of composing these vectors can produce hybrid representations which perform significantly better than either method in isolation. This leap in performance is particularly pronounced whe"
yao-etal-2010-practical,W06-1303,1,\N,Missing
yao-etal-2010-practical,burger-etal-2006-competitive,0,\N,Missing
yao-etal-2010-practical,robinson-etal-2008-ask,1,\N,Missing
