2020.acl-main.528,N13-1006,0,0.112491,"Missing"
2020.acl-main.528,W06-0130,0,0.562999,"ls Yang et al., 2016 Yang et al., 2016∗† Che et al., 2013∗ Wang et al., 2013∗ Word-based (LSTM) + char + bichar Word-based (LSTM) + char + bichar Char-based (LSTM) + bichar + softword + ExSoftword + bichar + ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT P 65.59 72.98 77.71 76.43 76.66 78.62 72.84 73.36 68.79 74.36 69.90 73.80 76.35 76.40 77.28 77.13 76.01 81.99 83.41 R 71.84 80.15 72.51 72.32 63.60 73.13 59.72 70.12 60.35 69.43 66.46 71.05 71.56 72.60 74.07 75.22 79.96 81.65 82.21 Models Chen et al., 2006 Zhang et al. 2006∗ Zhou et al. 2013 Lu et al. 2016 Dong et al. 2016 Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT F1 68.57 76.40 75.02 74.32 69.52 75.77 65.63 71.70 64.30 71.89 68.13 72.40 73.88 74.45 75.64 76.16 77.93 81.82 82.81 Models Peng and Dredze, 2015 Peng and Dredze, 2016∗ He and Sun, 2017a He and Sun, 2017b∗ Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLe"
2020.acl-main.528,P15-1017,0,0.059364,"Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-ofthe-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT. 1 1 Introduction Named Entity Recognition (NER) is concerned with the identification of named entities, such as persons, locations, and organizations, in unstructured text. NER plays an important role in many downstream tasks, including knowledge base construction (Riedel et al., 2013), information retrieval (Chen et al., 2015), and question answering (Diefenbach et al., 2018). In languages where words are naturally separated (e.g., English), NER has been conventionally formulated as a sequence ∗ Equal contribution. The source code of this paper is publicly available at https://github.com/v-mipeng/ LexiconAugmentedNER. 1 labeling problem, and the state-of-the-art results have been achieved using neural-network-based models (Huang et al., 2015; Chiu and Nichols, 2016; Liu et al., 2018). Compared with NER in English, Chinese NER is more difficult since sentences in Chinese are not naturally segmented. Thus, a common p"
2020.acl-main.528,Q16-1026,0,0.127253,"unstructured text. NER plays an important role in many downstream tasks, including knowledge base construction (Riedel et al., 2013), information retrieval (Chen et al., 2015), and question answering (Diefenbach et al., 2018). In languages where words are naturally separated (e.g., English), NER has been conventionally formulated as a sequence ∗ Equal contribution. The source code of this paper is publicly available at https://github.com/v-mipeng/ LexiconAugmentedNER. 1 labeling problem, and the state-of-the-art results have been achieved using neural-network-based models (Huang et al., 2015; Chiu and Nichols, 2016; Liu et al., 2018). Compared with NER in English, Chinese NER is more difficult since sentences in Chinese are not naturally segmented. Thus, a common practice for Chinese NER is to first perform word segmentation using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to perf"
2020.acl-main.528,N19-1423,0,0.0703552,"Missing"
2020.acl-main.528,P19-1141,0,0.280767,"on using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this consideration, Zhang and Yang, (2018) proposed Lattice-LSTM for incorporating word lexicons into the character-based NER model. Moreover, rather than heuristically choosing a word for the character when it matches multiple words in the lexicon, the authors proposed to preserve all words that match the character, leaving the subsequent NER model to determine which word to apply. To realize this idea, they introduced an elaborate modification to the sequence modeling layer of the"
2020.acl-main.528,D19-1096,1,0.748725,"Missing"
2020.acl-main.528,E17-2113,0,0.452886,"on. The source code of this paper is publicly available at https://github.com/v-mipeng/ LexiconAugmentedNER. 1 labeling problem, and the state-of-the-art results have been achieved using neural-network-based models (Huang et al., 2015; Chiu and Nichols, 2016; Liu et al., 2018). Compared with NER in English, Chinese NER is more difficult since sentences in Chinese are not naturally segmented. Thus, a common practice for Chinese NER is to first perform word segmentation using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this considerati"
2020.acl-main.528,I08-4022,0,0.050302,"nese are not naturally segmented. Thus, a common practice for Chinese NER is to first perform word segmentation using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this consideration, Zhang and Yang, (2018) proposed Lattice-LSTM for incorporating word lexicons into the character-based NER model. Moreover, rather than heuristically choosing a word for the character when it matches multiple words in the lexicon, the authors proposed to preserve all words that match the character, leaving the subsequent NER model to determine which word to"
2020.acl-main.528,W06-0115,0,0.621713,"highest conditional probability given the input sequence s: y ∗ =y p(y|s; θ), (14) which can be efficiently solved using the Viterbi algorithm (Forney, 1973). 4 4.1 Experiments Experiment Setup Most experimental settings in this work followed the protocols of Lattice-LSTM (Zhang and Yang, 2018), including tested datasets, compared baselines, evaluation metrics (P, R, F1), and so on. To make this work self-completed, we concisely illustrate some primary settings of this work. Datasets The methods were evaluated on four Chinese NER datasets, including OntoNotes (Weischedel et al., 2011), MSRA (Levow, 2006), Weibo NER (Peng 5955 OntoNotes 1× 2.23× 2.56× 2.77× 6.15× 6.08× 2.74× MSRA 1× 1.57× 2.55× 2.32× 5.78× 5.95× 2.33× Weibo 1× 2.41× 4.45× 2.84× 6.10× 5.91× 2.85× Resume 1× 1.44× 3.12× 2.38× 6.13× 6.45× 2.32× 175 Table 2: Inference speed (average sentences per second, the larger the better) of our method with LSTM layer compared with Lattice-LSTM, LR-CNN and BERT. and Dredze, 2015; He and Sun, 2017a), and Resume NER (Zhang and Yang, 2018). OntoNotes and MSRA are from the newswire domain, where gold-standard segmentation is available for training data. For OntoNotes, gold segmentation is also ava"
2020.acl-main.528,li-etal-2014-comparison,0,0.270291,"Missing"
2020.acl-main.528,N19-1247,0,0.295037,"ice for Chinese NER is to first perform word segmentation using an existing CWS system and then apply a word-level sequence labeling model to the segmented sentence (Yang et al., 2016; He and Sun, 2017b). However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this consideration, Zhang and Yang, (2018) proposed Lattice-LSTM for incorporating word lexicons into the character-based NER model. Moreover, rather than heuristically choosing a word for the character when it matches multiple words in the lexicon, the authors proposed to preserve all words that match the character, leaving the subsequent NER model to determine which word to apply. To realize this idea, they introduced an elab"
2020.acl-main.528,L16-1138,0,0.210153,"013∗ Wang et al., 2013∗ Word-based (LSTM) + char + bichar Word-based (LSTM) + char + bichar Char-based (LSTM) + bichar + softword + ExSoftword + bichar + ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT P 65.59 72.98 77.71 76.43 76.66 78.62 72.84 73.36 68.79 74.36 69.90 73.80 76.35 76.40 77.28 77.13 76.01 81.99 83.41 R 71.84 80.15 72.51 72.32 63.60 73.13 59.72 70.12 60.35 69.43 66.46 71.05 71.56 72.60 74.07 75.22 79.96 81.65 82.21 Models Chen et al., 2006 Zhang et al. 2006∗ Zhou et al. 2013 Lu et al. 2016 Dong et al. 2016 Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT F1 68.57 76.40 75.02 74.32 69.52 75.77 65.63 71.70 64.30 71.89 68.13 72.40 73.88 74.45 75.64 76.16 77.93 81.82 82.81 Models Peng and Dredze, 2015 Peng and Dredze, 2016∗ He and Sun, 2017a He and Sun, 2017b∗ Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagge"
2020.acl-main.528,D15-1064,0,0.353453,"Missing"
2020.acl-main.528,W06-0126,0,0.660456,"16 Yang et al., 2016∗† Che et al., 2013∗ Wang et al., 2013∗ Word-based (LSTM) + char + bichar Word-based (LSTM) + char + bichar Char-based (LSTM) + bichar + softword + ExSoftword + bichar + ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT P 65.59 72.98 77.71 76.43 76.66 78.62 72.84 73.36 68.79 74.36 69.90 73.80 76.35 76.40 77.28 77.13 76.01 81.99 83.41 R 71.84 80.15 72.51 72.32 63.60 73.13 59.72 70.12 60.35 69.43 66.46 71.05 71.56 72.60 74.07 75.22 79.96 81.65 82.21 Models Chen et al., 2006 Zhang et al. 2006∗ Zhou et al. 2013 Lu et al. 2016 Dong et al. 2016 Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftLexicon (LSTM) + bichar BERT-Tagger BERT + LSTM + CRF SoftLexicon (LSTM) + BERT F1 68.57 76.40 75.02 74.32 69.52 75.77 65.63 71.70 64.30 71.89 68.13 72.40 73.88 74.45 75.64 76.16 77.93 81.82 82.81 Models Peng and Dredze, 2015 Peng and Dredze, 2016∗ He and Sun, 2017a He and Sun, 2017b∗ Char-based (LSTM) + bichar+softword + ExSoftword + bichar+ExSoftword Lattice-LSTM LR-CNN (Gui et al., 2019) SoftLexicon (LSTM) SoftL"
2020.acl-main.528,P18-1144,0,0.753284,"However, it is inevitable that the CWS system will incorrectly segment query sentences. This will result in errors in the detection of entity boundary and the prediction of entity category in NER. Therefore, some approaches resort to performing Chinese NER directly at the character level, which has been empirically proven to be effective (He and Wang, 2008; Liu et al., 2010; Li et al., 2014; Liu et al., 2019; Sui et al., 2019; Gui et al., 2019b; Ding et al., 2019). A drawback of the purely character-based NER method is that the word information is not fully exploited. With this consideration, Zhang and Yang, (2018) proposed Lattice-LSTM for incorporating word lexicons into the character-based NER model. Moreover, rather than heuristically choosing a word for the character when it matches multiple words in the lexicon, the authors proposed to preserve all words that match the character, leaving the subsequent NER model to determine which word to apply. To realize this idea, they introduced an elaborate modification to the sequence modeling layer of the LSTM-CRF model (Huang et al., 2015). Experimental studies on four Chinese NER datasets have verified the effectiveness of Lattice-LSTM. However, the model"
2020.acl-main.528,I08-4017,0,0.0546155,"al., 2018). We performed experiments on four public Chinese NER datasets. The experimental results show that when implementing the sequence modeling layer with a single-layer Bi-LSTM, our method achieves considerable improvements over the state-of-theart methods in both inference speed and sequence labeling performance. 2 Background In this section, we introduce several previous works that influenced our work, including the Softword technique and Lattice-LSTM. 2.1 Softword Feature The Softword technique was originally used for incorporating word segmentation information into downstream tasks (Zhao and Kit, 2008; Peng and Dredze, 2016). It augments the character representation with the embedding of its corresponding segmentation label: xcj ← [xcj ; eseg (seg(cj ))]. (1) Here, seg(cj ) ∈ Yseg denotes the segmentation label of the character cj predicted by the word segmentor, eseg denotes the segmentation label embedding lookup table, and typically Yseg = {B, M, E, S}. However, gold segmentation is not provided in most datasets, and segmentation results obtained by a segmenter can be incorrect. Therefore, segmentation errors will inevitably be introduced through this approach. 2.2 Lattice-LSTM Lattice-"
2020.acl-main.528,P16-2025,0,0.595518,"rmed experiments on four public Chinese NER datasets. The experimental results show that when implementing the sequence modeling layer with a single-layer Bi-LSTM, our method achieves considerable improvements over the state-of-theart methods in both inference speed and sequence labeling performance. 2 Background In this section, we introduce several previous works that influenced our work, including the Softword technique and Lattice-LSTM. 2.1 Softword Feature The Softword technique was originally used for incorporating word segmentation information into downstream tasks (Zhao and Kit, 2008; Peng and Dredze, 2016). It augments the character representation with the embedding of its corresponding segmentation label: xcj ← [xcj ; eseg (seg(cj ))]. (1) Here, seg(cj ) ∈ Yseg denotes the segmentation label of the character cj predicted by the word segmentor, eseg denotes the segmentation label embedding lookup table, and typically Yseg = {B, M, E, S}. However, gold segmentation is not provided in most datasets, and segmentation results obtained by a segmenter can be incorrect. Therefore, segmentation errors will inevitably be introduced through this approach. 2.2 Lattice-LSTM Lattice-LSTM designs to incorpor"
2020.acl-main.528,N13-1008,0,0.0185885,"ation. Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-ofthe-art methods, along with a better performance. The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT. 1 1 Introduction Named Entity Recognition (NER) is concerned with the identification of named entities, such as persons, locations, and organizations, in unstructured text. NER plays an important role in many downstream tasks, including knowledge base construction (Riedel et al., 2013), information retrieval (Chen et al., 2015), and question answering (Diefenbach et al., 2018). In languages where words are naturally separated (e.g., English), NER has been conventionally formulated as a sequence ∗ Equal contribution. The source code of this paper is publicly available at https://github.com/v-mipeng/ LexiconAugmentedNER. 1 labeling problem, and the state-of-the-art results have been achieved using neural-network-based models (Huang et al., 2015; Chiu and Nichols, 2016; Liu et al., 2018). Compared with NER in English, Chinese NER is more difficult since sentences in Chinese ar"
2020.acl-main.528,D19-1396,0,0.83911,"Missing"
2020.coling-main.204,W05-0909,0,0.0914806,"nt for sentence generation . VST: This is the baseline version of our model without using topic information as guidance. TAVST w/o IU: This is our proposed TAVST method without IU module, which only equipped with initial topic description generator. TAVST: This is our full model. TAVST (MLE) is trained using MLE loss, while TAVST (RL) is trained via RL loss. 3.4 Automatic Evaluation Results We evaluate our model on two generation tasks i.e., story generation and topic description generation, in terms of four automatic metrics: BLEU (Papineni et al., 2002), ROUGE-L (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Story Generation The overall experimental results are shown in Table 1. TAVST (MLE) outperforms all of the baseline models trained with MLE. This confirms the effectivness of topic information for generating better stories. Noticeably, compared with the RL-based models, our TAVST (MLE) has already achieved a competitive performance and outperforms other RL models (i.e., AREL and HSRL) in terms of METEOR and BLEU@[2, 3, 4] metrics. After equipped with RL, our TAVST (RL) model is able to further improve the performance, outperforming the two RL models in term"
2020.coling-main.204,D13-1128,0,0.0232593,". 3.7 Case Study Figure 4 shows an example of the ground-truth story and stories generated automatically by different models. The words in red, blue and yellow color represent the topic, subject, and emotion, respectively. Our model shows promising results according to topic consistency, which further confirms that our model can extract appropriate topic which serves as the guidance of generating a topic-consistent story. 2257 4 Related work This paper is related to the fields of image captioning, visual storytelling and multi-task learning. Image Captioning In early works (Yang et al., 2011; Elliott and Keller, 2013), image captioning is treated as a ranking problem, which is based on retrieval models to identify similar captions from the database. Later, the end-to-end frameworks based on the CNN and RNN are adopted by researchers (Xu et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals et al., 2017; Dai et al., 2017). Such works focus on the literal description of image content, while the generated texts is limited in a single sentence. Visual Storytelling Visual storytelling is the task of generating a narrative paragraph for an image stream. Huang et al. (2016) introduce the first dataset (VIST) for visu"
2020.coling-main.204,N16-1147,0,0.520831,"g (Antol et al., 2015; Yu et al., 2017b; Singh et al., 2019), aiming at generating a short sentence or a phrase conditioned on certain visual information. With the development of deep learning and reinforcement learning models, recent years witness promising improvement of these tasks for single-image-to-single-sentence generation. Visual storytelling moves one step further, extending the input and output dimension to a sequence of images and a sequence of sentences. It requires the model to understand the main idea of an image stream and generate coherent sentences. Most of existing methods (Huang et al., 2016; Liu et al., 2017; Yu et al., 2017a; Wang et al., 2018a; Wang et al., 2020) for visual storytelling extend approaches of image captioning without considering topic information of the image sequence, which causes the problem of generating semantically incoherent content. An example of visual storytelling can be seen in Figure 1. An image stream with five images about a car accident is presented accompanied with two stories. One is constructed by a human annotator and the other is produced by an automatic storytelling approach. There are two problems with the machine generated story. First, the"
2020.coling-main.204,P18-1240,0,0.103348,"he decoder produces the a hidden state hti . Once the last topic hidden state t hM is obtained, we concatenate all topic hidden states ht = [ht1 , ..., htM ], M &gt; 1 as the topic memory, which are fed into the story generation module. 2.3 Initial Story Generator with Co-attention Network The initial story generator is responsible for generating the story with the guidance of the topic description constructed by the initial topic description generator. Co-attention Encoding In order to combine both visual information and topic information for story generation, we adopt a co-attention mechanism (Jing et al., 2018) for context information encoding. Specifically, given visual context vectors hv and topic vectors ht , the affinity matrix C is calculated by T C = tanh(ht Wb hv ) (2) where Wb is the weight parameter. After calculating this matrix, we compute attentions weights over the visual context vectors and the topic vectors via the following operations: H v = tanh(Wv hv + (Wt ht )C) H t = tanh(Wt ht + (Wv hv )C T ) T av = softmax(whv Hv) (3) T at = softmax(wht H t) T , w T are the weight parameters. Based on the attention weights, the visual and where Wv , Wt , whv ht semantic attentions are calculate"
2020.coling-main.204,P04-1077,0,0.0985818,"dance to the lower level agent for sentence generation . VST: This is the baseline version of our model without using topic information as guidance. TAVST w/o IU: This is our proposed TAVST method without IU module, which only equipped with initial topic description generator. TAVST: This is our full model. TAVST (MLE) is trained using MLE loss, while TAVST (RL) is trained via RL loss. 3.4 Automatic Evaluation Results We evaluate our model on two generation tasks i.e., story generation and topic description generation, in terms of four automatic metrics: BLEU (Papineni et al., 2002), ROUGE-L (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Story Generation The overall experimental results are shown in Table 1. TAVST (MLE) outperforms all of the baseline models trained with MLE. This confirms the effectivness of topic information for generating better stories. Noticeably, compared with the RL-based models, our TAVST (MLE) has already achieved a competitive performance and outperforms other RL models (i.e., AREL and HSRL) in terms of METEOR and BLEU@[2, 3, 4] metrics. After equipped with RL, our TAVST (RL) model is able to further improve the performance, outp"
2020.coling-main.204,P02-1040,0,0.108952,"concept for each image as the guidance to the lower level agent for sentence generation . VST: This is the baseline version of our model without using topic information as guidance. TAVST w/o IU: This is our proposed TAVST method without IU module, which only equipped with initial topic description generator. TAVST: This is our full model. TAVST (MLE) is trained using MLE loss, while TAVST (RL) is trained via RL loss. 3.4 Automatic Evaluation Results We evaluate our model on two generation tasks i.e., story generation and topic description generation, in terms of four automatic metrics: BLEU (Papineni et al., 2002), ROUGE-L (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Story Generation The overall experimental results are shown in Table 1. TAVST (MLE) outperforms all of the baseline models trained with MLE. This confirms the effectivness of topic information for generating better stories. Noticeably, compared with the RL-based models, our TAVST (MLE) has already achieved a competitive performance and outperforms other RL models (i.e., AREL and HSRL) in terms of METEOR and BLEU@[2, 3, 4] metrics. After equipped with RL, our TAVST (RL) model is able to further"
2020.coling-main.204,P18-1083,0,0.0760091,"019), aiming at generating a short sentence or a phrase conditioned on certain visual information. With the development of deep learning and reinforcement learning models, recent years witness promising improvement of these tasks for single-image-to-single-sentence generation. Visual storytelling moves one step further, extending the input and output dimension to a sequence of images and a sequence of sentences. It requires the model to understand the main idea of an image stream and generate coherent sentences. Most of existing methods (Huang et al., 2016; Liu et al., 2017; Yu et al., 2017a; Wang et al., 2018a; Wang et al., 2020) for visual storytelling extend approaches of image captioning without considering topic information of the image sequence, which causes the problem of generating semantically incoherent content. An example of visual storytelling can be seen in Figure 1. An image stream with five images about a car accident is presented accompanied with two stories. One is constructed by a human annotator and the other is produced by an automatic storytelling approach. There are two problems with the machine generated story. First, the sentiment expressed in the text is inappropriate. In f"
2020.coling-main.204,P19-1240,0,0.128394,"o BLEU@N all the time (Vedantam et al., 2015; Wang et al., 2018a). During the test stage, we generate the stories by performing a beam-search with a beam size of 3. 3.3 Models for Comparison We compare our proposed methods with several baselines for visual storytelling as follows: seq2seq (Huang et al., 2016): It generates caption for each single model via classic sequence-tosequence model and concatenate all captions to form the final story. h-attn-rank (Yu et al., 2017a): On top of the classic sequence-to-sequence model, it adds an additional RNN to select photos for story generation. HPSR (Wang et al., 2019a): It introduces an additional RNN stacked on the RNN-based photo encoder to detect the scene change. Information from both RNNs are fed into an RNN for story generation. 2254 Methods MLE seq2seq (Huang et al., 2016) h-attn-rank (Yu et al., 2017a) HPSR (Wang et al., 2019a) VST (MLE) TAVST w/o IU (MLE) TAVST (MLE) RL AREL (Wang et al., 2018b) HSRL (Huang et al., 2019) TAVST w/o IU (RL) TAVST (RL) B-1 B-2 B-3 B-4 R-L C M − − 61.9 62.3 63.1 63.6 − − 37.8 38.0 38.6 39.3 − 21.0 21.5 21.8 22.9 23.4 3.5 − 12.2 12.7 14.0 14.2 − 29.5 31.2 29.7 29.7 30.3 6.8 7.5 8.0 7.8 8.5 8.7 31.4 34.1 34.4 34.3 35.1"
2020.coling-main.204,H05-1044,0,0.0348739,"hoose which story is better in terms of a certain factor. Results are shown in Table 4. Our model performs better than the other two models in terms of relevance and topic consistency. The advantage of topic consistency is more promising. This proves that the topic description generator can help the story generation agent construct a more consistent story. 3.6 Further Analysis on Topic Consistency We further evaluate the quality of the generated story in terms of topic consistency from the perspective of sentiment. Specifically, we employ a lexicon-based approach using a subjectivity lexicon (Wilson et al., 2005). We count the number of sentiment words in each sentence for the polarity evaluation. The score will be 1,0,-1 if a sentence is positive, neutral and negative, respectively. Based on the score for each sentence, two qualitative experiments are designed to measure the in-story sentiment consistency and topic-story sentiment consistency. In-story Sentiment Consistency We argue that the sentiment of sentences in a story should be consistent given the album is related to a certain topic. For each story, we obtain a vector with 5 sentiment scores in correspondence to 5 sentences. We then calculate"
2020.coling-main.204,D18-1397,0,0.0158067,"ing text generation model via introducing automatic metrics (e.g., METEOR) to guide the training process (Wang et al., 2018b). We also explore the RL-based approach to train our generator. The reinforcement learning (RL) loss can be written as: ∗ 2 Linit (9) story(rl) (θ1 ) = −Ey∼pθ1 (r(y; y ) − b) where r is a sentence-level metric for the sampled sentence y and the ground-truth y ∗ ; b is the baseline which can be an arbitrary function but a linear layer in our experiments for simply. To stabilize the RL training process, a simple way is to linearly combine MLE and RL objectives as follows (Wu et al., 2018): init init Linit (10) story(com) = αLstory(rl) + (1 − α)Lstory(mle) where hyper-parameter α is employed to control the trade-off between MLE and RL objectives. init In the initial stage, a combined loss function of Linit story(com) and Ltopic is computed through: init Linit = λ1 Linit story(com) + (1 − λ1 )Ltopic(mle) (11) where hyper-parameter λ1 is employed to balance these losses. 2.4 Iterative Updating Module Considering that the generated story would also be helpful for the generation of topic description, we design an iterative updating module for the two agents to interact with each ot"
2020.coling-main.204,D17-1101,0,0.241617,"earn them simultaneously via iterative updating mechanism. We validate our approach on VIST dataset, where quantitative results, ablations, and human evaluation demonstrate our method’s good ability in generating stories with higher quality compared to state-of-the-art methods. 1 Introduction Image-to-text generation is an important topic in artificial intelligence (AI) which connects computer vision (CV) and natural language processing (NLP). Popular tasks include image captioning (Karpathy and Fei-Fei, 2015; Ren et al., 2017; Vinyals et al., 2017) and question answering (Antol et al., 2015; Yu et al., 2017b; Singh et al., 2019), aiming at generating a short sentence or a phrase conditioned on certain visual information. With the development of deep learning and reinforcement learning models, recent years witness promising improvement of these tasks for single-image-to-single-sentence generation. Visual storytelling moves one step further, extending the input and output dimension to a sequence of images and a sequence of sentences. It requires the model to understand the main idea of an image stream and generate coherent sentences. Most of existing methods (Huang et al., 2016; Liu et al., 2017;"
2020.coling-main.561,K16-1002,0,0.045755,"Missing"
2020.coling-main.561,C18-1288,0,0.203055,"ses the factuality of the message. Besides, disagreement and query towards descriptive statements are able to trigger drastic discussion and result in validity modification. Although some researchers explore propagation structure of rumor proliferation (Ma et al., 2017; Kumar and Carley, 2019), they typically rely on rough aggregation of locally successional messages. Moreover, the evolution of message interaction depicts the global characteristic of rumor cascades which improves the performance of verification. Figure 1 (b) illustrates the intuition using statistics drawn from PHEME dataset (Kochkina et al., 2018). It can be seen that denial tweets with supportive parent posts appear frequently in false rumors especially in an early stage, while unverified rumors constantly stimulate queries behind positive messages along with time. As rumor cascade evolves, with more dialogue context and auxiliary evidence, assessing the message credibility comprehensively becomes possible. ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6377 Proceedings of the 28th International Conference on Co"
2020.coling-main.561,P19-1498,0,0.211651,"pened during the message diffusion which is deemed to be important for the identification of rumors. Figure 1 (a) shows a rumor cascade which is identified as false for devilishly suspect Ray Radley’s role in the appalling Sydney siege. As can be seen, denial to false rumor tends to evoke affirmative replies which further confuses the factuality of the message. Besides, disagreement and query towards descriptive statements are able to trigger drastic discussion and result in validity modification. Although some researchers explore propagation structure of rumor proliferation (Ma et al., 2017; Kumar and Carley, 2019), they typically rely on rough aggregation of locally successional messages. Moreover, the evolution of message interaction depicts the global characteristic of rumor cascades which improves the performance of verification. Figure 1 (b) illustrates the intuition using statistics drawn from PHEME dataset (Kochkina et al., 2018). It can be seen that denial tweets with supportive parent posts appear frequently in false rumors especially in an early stage, while unverified rumors constantly stimulate queries behind positive messages along with time. As rumor cascade evolves, with more dialogue con"
2020.coling-main.561,P17-1066,0,0.154828,"interactions happened during the message diffusion which is deemed to be important for the identification of rumors. Figure 1 (a) shows a rumor cascade which is identified as false for devilishly suspect Ray Radley’s role in the appalling Sydney siege. As can be seen, denial to false rumor tends to evoke affirmative replies which further confuses the factuality of the message. Besides, disagreement and query towards descriptive statements are able to trigger drastic discussion and result in validity modification. Although some researchers explore propagation structure of rumor proliferation (Ma et al., 2017; Kumar and Carley, 2019), they typically rely on rough aggregation of locally successional messages. Moreover, the evolution of message interaction depicts the global characteristic of rumor cascades which improves the performance of verification. Figure 1 (b) illustrates the intuition using statistics drawn from PHEME dataset (Kochkina et al., 2018). It can be seen that denial tweets with supportive parent posts appear frequently in false rumors especially in an early stage, while unverified rumors constantly stimulate queries behind positive messages along with time. As rumor cascade evolve"
2020.coling-main.561,D14-1162,0,0.085117,"nfirm the effectiveness of message interaction. Among these two tasks, verification is labeled on cascade-level while stance belongs to tweet-level annotations. PHEME is undoubtedly suitable for our exploration of message interaction as it is constructed by a large amount of conversational threads in which participants tend to launch discussion other than judge on the source tweet. 4.2 Preprocessing and Training Details We preprocess each tweet by the NLTK toolkit (Bird et al., 2009) and follow a procedure of removing url and @, tokenizing, lemmatizing, and removing all the stop words. Glove (Pennington et al., 2014) word embeddings with dimension of 300 are adopted without being fine-tuned. As for training process, we 6382 perform leave-one-event-out (LOEO) cross validation (Kochkina et al., 2018). Although it suffers a lot to handle problems such as evil-balanced instances for each event and semantic inconsistency between events, LOEO is much more representative of real world and has been adopted by latest researches (Kumar and Carley, 2019; Wei et al., 2019). Hyperparameters performing best in development set are fixed and recorded. The network is trained with back propagation using the Adagrad update"
2020.coling-main.561,D19-1485,0,0.334562,"at are ambiguous at the time of posting, then explore how users share and discuss rumors and finally assess their veracity as true, false or unverified. This can be represented as a pipeline of sub-tasks, including rumor detection, stance classification and rumor verification (Zubiaga et al., 2018a). Identifying and debunking rumors automatically has been extensively studied in the past few years. State-of-the-art approaches construct sequential representations following a chronological order and then utilize temporal features to capture dynamic signals (Zubiaga et al., 2016; Ma et al., 2016; Wei et al., 2019). Although the source content stays invariable, time-series modeling successfully locates modifiers who might import evidence to correct misinformation or stir up enmity to discredit truth (Zhang et al., 2013). These models generate promising results, however, they ignore local interactions happened during the message diffusion which is deemed to be important for the identification of rumors. Figure 1 (a) shows a rumor cascade which is identified as false for devilishly suspect Ray Radley’s role in the appalling Sydney siege. As can be seen, denial to false rumor tends to evoke affirmative rep"
2020.coling-main.561,P18-1101,0,0.10894,"poses the discrete variational autoencoders (DVAEs) which assume that the corresponding prior distribution over the latent space is characterized by independent categorical distributions. Especially for text mining, discrete variables are adaptive to holistic properties of text and much more friendly for interpreting categories of natural language such as style, topic and high-level syntactic features. For instance, in neural dialog generation, DVAE is able to learn underlying dialogue intentions that can be interpreted as actions guiding the generation of machine responses (Wen et al., 2017; Zhao et al., 2018). In this paper, we learn discrete latent variables between inherited post pairs and incorporate them with textual information to model message interaction. 3 Proposed Model Resolution of rumor cascades can be formulated as a supervised classification problem. Given a treestuctured T WITTER cascade C which corresponds to a root tweet r0 and its relevant responsive tweets {r1 , r2 , ..., rT }, the goal is to recognize the stance of each tweet Yis as support, comment, deny or query, as well as determine the class of the cascade Yv as true, false or unverified. From our dataset, for each tweet ri"
2020.coling-main.561,N19-1163,0,0.0121094,"ssages, dynamic time series structure (Ma et al., 2015) and tree model using propagation pattern (Ma et al., 2017) is effective of depicting global difference between rumor and non-rumor claims. To avoid the effort and bias of feature engineering, methods based on deep neural networks are massively applied and have demonstrated great efficacy of discovering data representation automatically. Ma 6378 et al. (2016) employ recurrent neural networks (RNNs) to capture dynamic temporal signals. Yu et al. (2017) use convolutional neural networks (CNNs) to flexibly extract evidential posts. Recently, Zhou et al. (2019) integrate reinforcement learning to select the minimum number of posts required for early rumor detection. Ma et al. (2019) generate less indicative semantic representation via generative adversarial networks to gain better generalization for rumor detection. Besides, since rumor resolution is a coherent process, researchers also combine detection and stance classification with verification under the framework of multi-task learning (Ma et al., 2018; Kochkina et al., 2018; Kumar and Carley, 2019; Wei et al., 2019). In summary, deep learning approaches for rumor resolution involves three criti"
2020.emnlp-main.181,P19-1285,0,0.181006,"han some threshold value Γ, then the draft label yi∗ has a high probability of being wrong. Hence, we utilize a novel two-stream self-attention model to refine those uncertain labels using long-term label dependencies and word-label interactions. 3.2 Two-Stream Self-Attention for Label Refinement Given the draft labels and corresponding epistemic uncertainties, we seek the help of label dependencies and word-label interactions to refine the uncertain labels. In order to refine the draft labels in parallel, we use the Transformer (Vaswani et al., 2017) incorporating relative position encoding (Dai et al., 2019) to model the words and draft labels. In the standard Transformer, the attention score incorporating absolute position encoding between query qi and key vector kj can be decomposed as &gt; &gt; &gt; &gt; Aabs i,j = Exi Wq Wk Exj + Exi Wq Wk Uj &gt; &gt; &gt; + U&gt; i Wq Wk Exj + Ui Wq Wk Uj , (6) where U ∈ RLmax ×d provides a set of positional encodings. The ith row Ui corresponds to the ith absolute position and Lmax prescribes the maximum possible length to be modeled. The relative position between labels is very important for modeling the label dependencies. Inspired by Dai et al. (2019), we modify the Eq.6 using"
2020.emnlp-main.181,Q16-1026,0,0.21736,"his paper can be summarized as follows: 1) we propose the use of Bayesian neural networks to estimate 1 We slightly modified the code using Bayesian neural networks. 2.1 Related Work and Background Sequence Labeling Traditional sequence labeling models use statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Cuong et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models could achieve competitive performances without massive handcrafted feature engineering (Chiu and Nichols, 2016; Santos and Zadrozny, 2014). In recent years, modeling label dependencies has been the other focus of sequence labeling tasks, such as using a CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016), and introducing label embeddings to manage longer ranges of dependencies (Vaswani et al., 2016; Zhang et al., 2018; Cui and Zhang, 2019). Our work is an extension of label embedding methods, which applies label dependencies and word-label interactions to only refine the labels with high probabilities of being incorrect. The probability"
2020.emnlp-main.181,W02-1001,0,0.534138,"Missing"
2020.emnlp-main.181,D19-1422,0,0.359939,"ojhYt75BSmjl47k=&quot;&gt;AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVZIq6LLoRnBTwT6gLZJMp3VsXiYTsRZX/oBb/THxD/QvvDOmoBbRCUnOnHvPmbn3upEnEmlZrzljZnZufiG/WFhaXlldK65vNJIwjRmvs9AL45brJNwTAa9LIT3eimLu+K7Hm+7wWMWbNzxORBicy1HEu74zCERfMEcS1ehI4fPkoliyypZe5jSwM1BCtmph8QUd9BCCIYUPjgCSsAcHCT1t2LAQEdfFmLiYkNBxjnsUSJtSFqcMh9ghfQe0a2dsQHvlmWg1o1M8emNSmtghTUh5MWF1mqnjqXZW7G/eY+2p7jaiv5t5+cRKXBL7l26S+V+dqkWij0Ndg6CaIs2o6ljmkuquqJubX6qS5BARp3CP4jFhppWTPptak+jaVW8dHX/TmYpVe5blpnhXt6QB2z/HOQ0albK9V66c7ZeqR9mo89jCNnZpngeo4gQ11Mn7Co94wrNxalwbt8bdZ6qRyzSb+LaMhw/935G5&lt;/latexit&gt; ... Figure 1: Schematic of label refinement framework (Cui and Zhang, 2019). The goal is refining the label of “Arab” using contextual labels and words, while the refinement of other correct labels may be negatively impacted by incorrect draft labels. Linguistic sequence labeling is one of the fundamental tasks in natural language processing. It has the goal of predicting a linguistic label for each word, including part-of-speech (POS) tagging, text chunking, and named entity recognition (NER). Benefiting from representation learning, neural network-based approaches can achieve state-ofthe-art performance without massive handcrafted feature engineering (Ma and Hovy,"
2020.emnlp-main.181,N19-1423,0,0.206605,"words, while the refinement of other correct labels may be negatively impacted by incorrect draft labels. Linguistic sequence labeling is one of the fundamental tasks in natural language processing. It has the goal of predicting a linguistic label for each word, including part-of-speech (POS) tagging, text chunking, and named entity recognition (NER). Benefiting from representation learning, neural network-based approaches can achieve state-ofthe-art performance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. X Refinement Input Introduction ∗ I-LOC creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However, CRF only captures the neighboring label dependencies and must rely"
2020.emnlp-main.181,P06-1141,0,0.191903,"Missing"
2020.emnlp-main.181,N16-1030,0,0.824531,"goal is refining the label of “Arab” using contextual labels and words, while the refinement of other correct labels may be negatively impacted by incorrect draft labels. Linguistic sequence labeling is one of the fundamental tasks in natural language processing. It has the goal of predicting a linguistic label for each word, including part-of-speech (POS) tagging, text chunking, and named entity recognition (NER). Benefiting from representation learning, neural network-based approaches can achieve state-ofthe-art performance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. X Refinement Input Introduction ∗ I-LOC creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However,"
2020.emnlp-main.181,D15-1104,0,0.0311838,"s on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also significantly accelerated the inference process. The main contributions of this paper can be summarized as follows: 1) we propose the use of Bayesian neural networks to estimate 1 We slightly modified the code using Bayesian neural networks. 2.1 Related Work and Background Sequence Labeling Traditional sequence labeling models use statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Cuong et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models could achieve competitive performances without massive handcrafted feature engineering (Chiu and Nichols, 2016; Santos and Zadrozny, 2014). In recent years, modeling label dependencies has been the other focus of sequence labeling tasks, such as using a CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016), and introducing label embeddings to manage longer ranges of dependencies (Vaswani et al., 2016; Zhang et al., 2018; Cui and Z"
2020.emnlp-main.181,P16-1101,0,0.395958,"Zhang, 2019). The goal is refining the label of “Arab” using contextual labels and words, while the refinement of other correct labels may be negatively impacted by incorrect draft labels. Linguistic sequence labeling is one of the fundamental tasks in natural language processing. It has the goal of predicting a linguistic label for each word, including part-of-speech (POS) tagging, text chunking, and named entity recognition (NER). Benefiting from representation learning, neural network-based approaches can achieve state-ofthe-art performance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. X Refinement Input Introduction ∗ I-LOC creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labe"
2020.emnlp-main.181,J93-2004,0,0.0688971,"n is collected from Reuters Corpus. The dataset divide name entities into four different types: persons, locations, organizations, and miscellaneous entities. We use the BIOES tag scheme instead of standard BIO2, which is the same as Ma and Hovy (2016). OntoNotes 5.0. English NER dataset OntoNotes 5.0 (Weischedel et al., 2013) is a large corpus consists of various genres, such as newswire, broadcast, and telephone speech. Named entities are labeled in eleven types and values are specifically divided into seven types, like DATE, TIME, ORDINAL. WSJ. Wall Street Journal portion of Penn Treebank (Marcus et al., 1993), which contains 45 types of part-of-speech tags. We adopts standard splits following previous works (Collins, 2002; Manning, 2320 Dataset CoNLL2003 OntoNotes WSJ #Train #Dev #Test class 204,567 1,088,503 912,344 51,578 147,724 131,768 46,666 152,728 129,654 17 73 45 Models Chiu and Nichols (2016) Strubell et al. (2017) Liu et al. (2018) Chen et al. (2019) BiLSTM-CRF (Ma and Hovy, 2016) BiLSTM-Softmax (Yang et al., 2018) BiLSTM-Seq2seq (Zhang et al., 2018) Rel-Transformer (Dai et al., 2019) BiLSTM-LAN (Cui and Zhang, 2019) BiLSTM-UANet (M = 8) Table 2: Statistics of CoNLL2003, OntoNotes and WS"
2020.emnlp-main.181,W14-1609,0,0.0163721,"a faster prediction. Experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also significantly accelerated the inference process. The main contributions of this paper can be summarized as follows: 1) we propose the use of Bayesian neural networks to estimate 1 We slightly modified the code using Bayesian neural networks. 2.1 Related Work and Background Sequence Labeling Traditional sequence labeling models use statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Cuong et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models could achieve competitive performances without massive handcrafted feature engineering (Chiu and Nichols, 2016; Santos and Zadrozny, 2014). In recent years, modeling label dependencies has been the other focus of sequence labeling tasks, such as using a CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016), and introducing label embeddings to manage longer ranges of dependencies (Vaswani et a"
2020.emnlp-main.181,N18-1202,0,0.0266918,"Missing"
2020.emnlp-main.181,K19-1058,0,0.463518,"Missing"
2020.emnlp-main.181,D17-1283,0,0.303028,"label of “Arab” using contextual labels and words, while the refinement of other correct labels may be negatively impacted by incorrect draft labels. Linguistic sequence labeling is one of the fundamental tasks in natural language processing. It has the goal of predicting a linguistic label for each word, including part-of-speech (POS) tagging, text chunking, and named entity recognition (NER). Benefiting from representation learning, neural network-based approaches can achieve state-ofthe-art performance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. X Refinement Input Introduction ∗ I-LOC creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However, CRF only captures the n"
2020.emnlp-main.181,N16-1027,0,0.150284,"been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However, CRF only captures the neighboring label dependencies and must rely on inefficient Viterbi decoding. Many of the recent methods try to introduce label embeddings to manage longer ranges of dependencies, such as two-stage label refinement (Krishnan and Manning, 2006; Cui and Zhang, 2019) and seq2seq (Vaswani et al., 2016; Zhang et al., 2018) frameworks. In particular, Cui and Zhang (2019) introduced a hierarchically-refined representation of marginal label distributions, which predicts a sequence of draft labels in advance and then uses the word-label interactions to refine them. Although these methods can model longer label dependencies, they are vulnerable to error propagation: if a label is mistakenly predicted during inference, the error will be propagated and the other labels conditioned on this one will be impacted (Bengio et al., 2015). As shown in Figure 1, the label attention network (LAN) (Cui and 2"
2020.emnlp-main.181,D18-1279,0,0.0456029,"M = 8) Table 2: Statistics of CoNLL2003, OntoNotes and WSJ datasets, where # represents the number of tokens in datasets. The class number of NER datasets is counted under BIOES tag scheme. 2011), selecting section 0-18 for training, section 19-21 for validation and section 22-24 for test. 4.2 In this work, we mainly focus on improving decoding efficiency and enhancing label dependencies. Thus, we make comparisons with the classic methods that have different decoding layers, such as Softmax, CRF, and LAN frameworks. We also compare some recent competitive methods, such as Transformer, IntNet (Xin et al., 2018), and BERT (Devlin et al., 2019). BiLSTM-Softmax. This baseline uses bidirectional LSTM to reprensent a sequence. The BiLSTM concatenates the forward hidden state → − ← − h i and backward hidden state h i to form an → − ← − integral representation hi = [ h i ; h i ]. Finally, sentence representation H = {hi , · · · , hn } is fed to softmax layer for predicting. BiLSTM-CRF. A CRF layer is used on top of the hidden vectors H (Ma and Hovy, 2016). The CRF can model bigram interactions between two successive labels (Lample et al., 2016) instead of making independent labeling decisions for each outp"
2020.emnlp-main.181,C18-1327,0,0.121374,"Missing"
2020.emnlp-main.181,P18-4013,0,0.0156567,"ayers of multihead transformer for WSJ and CoNLL2003 and 3 for OntoNotes dataset to refine the label. The number of heads is chosen from {5, 7, 9}, and the dimension of each head is chosen from {80, 120, 160} via grid search. We use SGD as the optimizer for variational LSTM and Adam (Kingma and Ba, 2014) for transformer. Learning rates are set to 0.015 for SGD on CoNLL2003 and Ontonotes datasets and 0.2 on WSJ dataset. The learning rates for Adam are set to 0.0001 for all datasets. F1 score and accuracy are used for NER and POS tagging, respectively. All experiments are implemented in NCRF++ (Yang and Zhang, 2018) and conducted using a GeForce GTX 1080Ti with 11GB memory. More details are shown in our codes3 . 5 Results and Analysis In this section, we present the experimental results of the proposed and baseline models. We show that the proposed method not only achieves better performance but also has a significant speed advantage. Since our contribution is mainly focused on the label decoding layer, the proposed model can also be combined with the latest pretrained model to further improve performance. Rel-Transformer. This baseline model adopts self-attention mechanism with relative position represe"
2020.emnlp-main.181,P18-2038,0,0.0166228,"named entity recognition (NER). Benefiting from representation learning, neural network-based approaches can achieve state-ofthe-art performance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. X Refinement Input Introduction ∗ I-LOC creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However, CRF only captures the neighboring label dependencies and must rely on inefficient Viterbi decoding. Many of the recent methods try to introduce label embeddings to manage longer ranges of dependencies, such as two-stage label refinement (Krishnan and Manning, 2006; Cui and Zhang, 2019) and seq2seq (Vaswani et al., 2016; Zhang et al., 2018) frameworks. In particular, Cui and Zhang ("
2020.emnlp-main.181,P15-1109,0,0.0678095,"rformance without massive handcrafted feature engineering (Ma and Hovy, 2016; Lample et al., 2016; Strubell et al., 2017; Peters et al., 2018; Devlin et al., 2019). Although the use of representation learning to obtain better text representation is very successful, Both authors contributed equally. X Refinement Input Introduction ∗ I-LOC creating better models for label dependencies has always been the focus of sequence labeling tasks (Collobert et al., 2011; Ye and Ling, 2018; Zhang et al., 2018). Among them, the CRF layer integrated with neural encoders to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016) has become ubiquitous in sequence labeling tasks. However, CRF only captures the neighboring label dependencies and must rely on inefficient Viterbi decoding. Many of the recent methods try to introduce label embeddings to manage longer ranges of dependencies, such as two-stage label refinement (Krishnan and Manning, 2006; Cui and Zhang, 2019) and seq2seq (Vaswani et al., 2016; Zhang et al., 2018) frameworks. In particular, Cui and Zhang (2019) introduced a hierarchically-refined representation of marginal label distributions, which predicts a sequence of draft labels in a"
2020.emnlp-main.292,D18-1316,0,0.0189004,"ust too difficult to alter in our current generation framework. It needs more advanced models such as text style transfer (Shen et al., 2017; Jin et al., 2019b). 7 Related Work Robustness in NLP Robustness in NLP has attracted extensive attention in recent works (Hsieh et al., 2019; Li et al., 2016). As a popular method to probe the robustness of models, adversarial text generation becomes an emerging research field in NLP. Techniques include adding extraneous text to the input (Jia and Liang, 2016), character-level noise (Belinkov and Bisk, 2018; Ebrahimi et al., 2018), and word replacement (Alzantot et al., 2018; Jin et al., 2019a). Using the adversarial generation techniques, new adversarial test sets are proposed for several tasks such as paraphrasing (Zhang et al., 2019b) and entailment (Glockner et al., 2018; McCoy et al., 2019). Aspect-Based Sentiment Analysis ABSA has emerged as an active research area recently. Early works hand-craft sentiment lexicons and syntactic features for rule-based classifiers (Vo and Zhang, 2015; Kiritchenko et al., 2014). Recent neural network-based models use architectures such as LSTM (Tang et al., 2016a), CNN (Xue and Li, 2018), Attention mechanisms (Wang et al.,"
2020.emnlp-main.292,N19-1423,0,0.0480086,"aspect and word embeddings of each token. GatedCNN: Xue and Li (2018) use a Gated Convolutional Neural Networks (CNN) that applies a Tanh-ReLU gating mechanism to the CNNencoded text with aspect embeddings. MemNet: Tang et al. (2016b) use memory networks to store the sentence as external memory and calculate the attention with the target aspect. GCN: Aspect-specific Graph Convolutional Networks (GCN) (Zhang et al., 2019a) first applies 3598 GCN over the syntax tree of the sentence and then imposes an aspect-specific masking layer on its top. BERT: Xu et al. (2019a) uses a BERT-based baseline (Devlin et al., 2019) and takes as input the concatenation of the aspect term and the sentence. BERT-PT: Xu et al. (2019a) post-train BERT on other review datasets such as Amazon laptop reviews (He and McAuley, 2016) and Yelp Dataset Challenge reviews, and finetune on ABSA tasks. CapsBERT: (Jiang et al., 2019) encode the sentence and the aspect term with BERT, and then feed it into Capsule Networks to predict the polarity. BERT-Sent: For more in-depth analysis, we also implement a sentence classification baseline. BERT-Sent takes as input sentences without aspect information, and predicts the “global” sentiment. W"
2020.emnlp-main.292,P14-2009,0,0.160287,"Missing"
2020.emnlp-main.292,P18-2006,0,0.0197411,"se. For example, “a 2-hour wait” is negative bust too difficult to alter in our current generation framework. It needs more advanced models such as text style transfer (Shen et al., 2017; Jin et al., 2019b). 7 Related Work Robustness in NLP Robustness in NLP has attracted extensive attention in recent works (Hsieh et al., 2019; Li et al., 2016). As a popular method to probe the robustness of models, adversarial text generation becomes an emerging research field in NLP. Techniques include adding extraneous text to the input (Jia and Liang, 2016), character-level noise (Belinkov and Bisk, 2018; Ebrahimi et al., 2018), and word replacement (Alzantot et al., 2018; Jin et al., 2019a). Using the adversarial generation techniques, new adversarial test sets are proposed for several tasks such as paraphrasing (Zhang et al., 2019b) and entailment (Glockner et al., 2018; McCoy et al., 2019). Aspect-Based Sentiment Analysis ABSA has emerged as an active research area recently. Early works hand-craft sentiment lexicons and syntactic features for rule-based classifiers (Vo and Zhang, 2015; Kiritchenko et al., 2014). Recent neural network-based models use architectures such as LSTM (Tang et al., 2016a), CNN (Xue and L"
2020.emnlp-main.292,N19-1259,0,0.307938,"s, including adversarial training detailed in Section 5.4. 2 Data Generation As shown in Table 1, we aim to build a systematic method to generate all possible aspect-related alternations, in order to remove the confounding factors in the existing ABSA data. In the following, we will introduce three different ways to disentangle the sentiment of the target aspect from sentiments of non-target aspects. 2.1 R EV T GT The first strategy is to generate sentences that reverse the original sentiment of the target aspect. The word spans of each aspect’s sentiment of SemEval 2014 data are provided by (Fan et al., 2019a). We design two methods to reverse the sentiment, and one additional step of conjunction adjustment on top of the two methods to polish the resulting sentence. 3595 Strategy Example It’s light and easy to transport. Flip Opinion → It’s heavy and difficult to transport. The menu changes seasonally. Add Negation → The menu does not change seasonally. Adjust The food is good, and the decor is nice. Conjunctions → The food is good, but the decor is nasty. Table 2: Three strategies and examples of R EV T GT. Strategy Original sentence & sentiment R EV N ON Flip same-sentiment non-target aspects ("
2020.emnlp-main.292,P18-2103,0,0.0216594,"s in NLP has attracted extensive attention in recent works (Hsieh et al., 2019; Li et al., 2016). As a popular method to probe the robustness of models, adversarial text generation becomes an emerging research field in NLP. Techniques include adding extraneous text to the input (Jia and Liang, 2016), character-level noise (Belinkov and Bisk, 2018; Ebrahimi et al., 2018), and word replacement (Alzantot et al., 2018; Jin et al., 2019a). Using the adversarial generation techniques, new adversarial test sets are proposed for several tasks such as paraphrasing (Zhang et al., 2019b) and entailment (Glockner et al., 2018; McCoy et al., 2019). Aspect-Based Sentiment Analysis ABSA has emerged as an active research area recently. Early works hand-craft sentiment lexicons and syntactic features for rule-based classifiers (Vo and Zhang, 2015; Kiritchenko et al., 2014). Recent neural network-based models use architectures such as LSTM (Tang et al., 2016a), CNN (Xue and Li, 2018), Attention mechanisms (Wang et al., 2016), Capsule Network (Jiang et al., 2019), and the pretrained model BERT (Xu et al., 2019a). Similar to the motivation in our paper, some work shows preliminary speculation that the current ABSA dataset"
2020.emnlp-main.292,P19-1147,0,0.0350996,"Missing"
2020.emnlp-main.292,P16-1002,0,0.0208959,"stances with complicated sentiment expressions which rely on commonsense. For example, “a 2-hour wait” is negative bust too difficult to alter in our current generation framework. It needs more advanced models such as text style transfer (Shen et al., 2017; Jin et al., 2019b). 7 Related Work Robustness in NLP Robustness in NLP has attracted extensive attention in recent works (Hsieh et al., 2019; Li et al., 2016). As a popular method to probe the robustness of models, adversarial text generation becomes an emerging research field in NLP. Techniques include adding extraneous text to the input (Jia and Liang, 2016), character-level noise (Belinkov and Bisk, 2018; Ebrahimi et al., 2018), and word replacement (Alzantot et al., 2018; Jin et al., 2019a). Using the adversarial generation techniques, new adversarial test sets are proposed for several tasks such as paraphrasing (Zhang et al., 2019b) and entailment (Glockner et al., 2018; McCoy et al., 2019). Aspect-Based Sentiment Analysis ABSA has emerged as an active research area recently. Early works hand-craft sentiment lexicons and syntactic features for rule-based classifiers (Vo and Zhang, 2015; Kiritchenko et al., 2014). Recent neural network-based mo"
2020.emnlp-main.292,D19-1654,0,0.0384818,"mory and calculate the attention with the target aspect. GCN: Aspect-specific Graph Convolutional Networks (GCN) (Zhang et al., 2019a) first applies 3598 GCN over the syntax tree of the sentence and then imposes an aspect-specific masking layer on its top. BERT: Xu et al. (2019a) uses a BERT-based baseline (Devlin et al., 2019) and takes as input the concatenation of the aspect term and the sentence. BERT-PT: Xu et al. (2019a) post-train BERT on other review datasets such as Amazon laptop reviews (He and McAuley, 2016) and Yelp Dataset Challenge reviews, and finetune on ABSA tasks. CapsBERT: (Jiang et al., 2019) encode the sentence and the aspect term with BERT, and then feed it into Capsule Networks to predict the polarity. BERT-Sent: For more in-depth analysis, we also implement a sentence classification baseline. BERT-Sent takes as input sentences without aspect information, and predicts the “global” sentiment. We use it because if other ABSA models fails to pay attention to aspects, they will degenerate to a sentence classifier. If so, they will resemble BERTSent, which performs well on original tests and badly on ARTS. So BERT-Sent is a reference to check degenerated aspect-level models. 4.2 Imp"
2020.emnlp-main.292,D19-1306,1,0.885032,"Missing"
2020.emnlp-main.292,P18-1110,0,0.0579983,"Missing"
2020.emnlp-main.292,S14-2076,0,0.0322958,"adding extraneous text to the input (Jia and Liang, 2016), character-level noise (Belinkov and Bisk, 2018; Ebrahimi et al., 2018), and word replacement (Alzantot et al., 2018; Jin et al., 2019a). Using the adversarial generation techniques, new adversarial test sets are proposed for several tasks such as paraphrasing (Zhang et al., 2019b) and entailment (Glockner et al., 2018; McCoy et al., 2019). Aspect-Based Sentiment Analysis ABSA has emerged as an active research area recently. Early works hand-craft sentiment lexicons and syntactic features for rule-based classifiers (Vo and Zhang, 2015; Kiritchenko et al., 2014). Recent neural network-based models use architectures such as LSTM (Tang et al., 2016a), CNN (Xue and Li, 2018), Attention mechanisms (Wang et al., 2016), Capsule Network (Jiang et al., 2019), and the pretrained model BERT (Xu et al., 2019a). Similar to the motivation in our paper, some work shows preliminary speculation that the current ABSA datasets might be downgraded to sentence-level sentiment classification (Xu et al., 2019b). 8 Conclusion In this paper, we proposed a simple but effective mechanism to generate test instances to probe the aspect robustness of the models. We enhanced the"
2020.emnlp-main.292,P19-1334,0,0.0310923,"Missing"
2020.emnlp-main.292,S14-2004,0,0.0585091,"e sentiments are different from the target aspect’s, and then append these to the end of the original example. For example, “Great food A DD D IFF and best of all GREAT beer!” −−−−−→ “Great food and best of all GREAT beer, but management is less than accommodating, music is too heavy, and service is severely slow.” In this way, A DD D IFF enables the advanced testing of whether the model will be confused when there are more irrelevant aspects with opposite sentiments. 3 ARTS Dataset 3.1 Overview Our source data is the most5 widely used ABSA dataset, SemEval 2014 Laptop and Restaurant Reviews (Pontiki et al., 2014).6 We follow (Wang et al., 2016; Ma et al., 2017; Xu et al., 2019a) to remove instances with conflicting polarity and only keep positive, negative, and neutral labels. We use the train-dev split as in (Xu et al., 2019a). The resulting Laptop dataset has 2,163 training, 150 4 The full AspectSet is available on our GitHub. We surveyed deep learning-based ABSA papers from 2015 to 2020 at top conferences (ACL, EMNLP, NAACL, NeurIPS, ICLR, ICML, AAAI, IJCAI). Among the 63 ABSA papers, 50 use SemEval 2014 Laptop and Restaurant, which is the top 1 widely used dataset. 6 http://alt.qcri.org/semeval201"
2020.emnlp-main.292,D16-1021,0,0.207321,"et has an increasing number of all labels, and especially balances the ratio of positive-to-negative labels from the original 2.66 to 1.5 on Laptop, and from 3.71 to 1.77 on Restaurant. For the aspect-related challenge in the test set, the new test set, first of all, has a larger number of aspects per sentence than the original. Our test set also features the higher disentanglement of the target aspect from the non-target aspects that have Models For a comprehensive overview of the ABSA field, we conduct extensive experiments on models with a variety of neural network architectures. TD-LSTM: (Tang et al., 2016a) uses two Long Short-Term Memory Networks (LSTM) to encode the preceding and following contexts of the target aspect (inclusive) and concatenate the last hidden states of the two LSTMs to make the sentiment classification. AttLSTM: Wang et al. (2016) apply an Attention-based LSTM on the concatenatation of the aspect and word embeddings of each token. GatedCNN: Xue and Li (2018) use a Gated Convolutional Neural Networks (CNN) that applies a Tanh-ReLU gating mechanism to the CNNencoded text with aspect embeddings. MemNet: Tang et al. (2016b) use memory networks to store the sentence as externa"
2020.emnlp-main.292,D16-1058,0,0.555148,"target aspect’s, and then append these to the end of the original example. For example, “Great food A DD D IFF and best of all GREAT beer!” −−−−−→ “Great food and best of all GREAT beer, but management is less than accommodating, music is too heavy, and service is severely slow.” In this way, A DD D IFF enables the advanced testing of whether the model will be confused when there are more irrelevant aspects with opposite sentiments. 3 ARTS Dataset 3.1 Overview Our source data is the most5 widely used ABSA dataset, SemEval 2014 Laptop and Restaurant Reviews (Pontiki et al., 2014).6 We follow (Wang et al., 2016; Ma et al., 2017; Xu et al., 2019a) to remove instances with conflicting polarity and only keep positive, negative, and neutral labels. We use the train-dev split as in (Xu et al., 2019a). The resulting Laptop dataset has 2,163 training, 150 4 The full AspectSet is available on our GitHub. We surveyed deep learning-based ABSA papers from 2015 to 2020 at top conferences (ACL, EMNLP, NAACL, NeurIPS, ICLR, ICML, AAAI, IJCAI). Among the 63 ABSA papers, 50 use SemEval 2014 Laptop and Restaurant, which is the top 1 widely used dataset. 6 http://alt.qcri.org/semeval2014/ task4/ 5 Restaurant 1,120 3,"
2020.emnlp-main.292,N19-1242,0,0.0762749,"Missing"
2020.emnlp-main.292,P18-1234,0,0.01456,"nt of the target aspect from the non-target aspects that have Models For a comprehensive overview of the ABSA field, we conduct extensive experiments on models with a variety of neural network architectures. TD-LSTM: (Tang et al., 2016a) uses two Long Short-Term Memory Networks (LSTM) to encode the preceding and following contexts of the target aspect (inclusive) and concatenate the last hidden states of the two LSTMs to make the sentiment classification. AttLSTM: Wang et al. (2016) apply an Attention-based LSTM on the concatenatation of the aspect and word embeddings of each token. GatedCNN: Xue and Li (2018) use a Gated Convolutional Neural Networks (CNN) that applies a Tanh-ReLU gating mechanism to the CNNencoded text with aspect embeddings. MemNet: Tang et al. (2016b) use memory networks to store the sentence as external memory and calculate the attention with the target aspect. GCN: Aspect-specific Graph Convolutional Networks (GCN) (Zhang et al., 2019a) first applies 3598 GCN over the syntax tree of the sentence and then imposes an aspect-specific masking layer on its top. BERT: Xu et al. (2019a) uses a BERT-based baseline (Devlin et al., 2019) and takes as input the concatenation of the aspe"
2020.emnlp-main.292,D19-1464,0,0.224295,"Missing"
2020.emnlp-main.292,N19-1131,0,0.0432846,"Missing"
2020.emnlp-main.579,D19-1241,0,0.550869,"Missing"
2020.emnlp-main.579,Q18-1012,0,0.0587451,"football and generates “N2” based on the nearest node “N4”. Our proposed method can capture long-distance features and therefore generate correct results. 4 Related Work Solving math word problems has long been a challenging task (Fletcher, 1985; Bakman, 2007; Roy and Roth, 2016) and has attracted the attention of many researchers. Some methods on math word problem solving incorporate extra features by manually crafting fine-grained templates or defining math concepts. Huang et al. (2017) formulated finegrained templates and aligned numbers in math word problems to those candidate templates. Roy and Roth (2018) developed declarative rules to transform math concepts into expressions. These methods require manually formulated features and may be difficult to apply to math word problems in different domains. Recently, many studies have used deep learning methods to incorporate external knowledge from the knowledge base into many NLP tasks, such as dialogue systems (Zhong et al., 2019) and reading comprehension (Wang and Jiang, 2019; Qiu et al., 2019). These methods extend knowledge triples into natural language sequences or build multi-hop inference graphs based on relationships in the knowledge base,"
2020.emnlp-main.579,D17-1084,0,0.070361,"wo nodes [-, N4] of the expression tree. GTS does not realize that the current subexpression tree indicates the price of each football and generates “N2” based on the nearest node “N4”. Our proposed method can capture long-distance features and therefore generate correct results. 4 Related Work Solving math word problems has long been a challenging task (Fletcher, 1985; Bakman, 2007; Roy and Roth, 2016) and has attracted the attention of many researchers. Some methods on math word problem solving incorporate extra features by manually crafting fine-grained templates or defining math concepts. Huang et al. (2017) formulated finegrained templates and aligned numbers in math word problems to those candidate templates. Roy and Roth (2018) developed declarative rules to transform math concepts into expressions. These methods require manually formulated features and may be difficult to apply to math word problems in different domains. Recently, many studies have used deep learning methods to incorporate external knowledge from the knowledge base into many NLP tasks, such as dialogue systems (Zhong et al., 2019) and reading comprehension (Wang and Jiang, 2019; Qiu et al., 2019). These methods extend knowled"
2020.emnlp-main.579,P19-1219,0,0.0221675,"fine-grained templates or defining math concepts. Huang et al. (2017) formulated finegrained templates and aligned numbers in math word problems to those candidate templates. Roy and Roth (2018) developed declarative rules to transform math concepts into expressions. These methods require manually formulated features and may be difficult to apply to math word problems in different domains. Recently, many studies have used deep learning methods to incorporate external knowledge from the knowledge base into many NLP tasks, such as dialogue systems (Zhong et al., 2019) and reading comprehension (Wang and Jiang, 2019; Qiu et al., 2019). These methods extend knowledge triples into natural language sequences or build multi-hop inference graphs based on relationships in the knowledge base, and have achieved promising results. In this paper, we model the entities in the problem and their categories as entity graphs and use graph attention to generate knowledge-aware problem representations. Seq2Seq neural networks (Sutskever et al., 2014) have achieved promising results on math word problem solving. For instance, Wang et al. (2017) used a Seq2Seq model to generate math expressions. Wang et al. (2018b) incorpo"
2020.emnlp-main.579,D18-1132,0,0.463877,"nsion (Wang and Jiang, 2019; Qiu et al., 2019). These methods extend knowledge triples into natural language sequences or build multi-hop inference graphs based on relationships in the knowledge base, and have achieved promising results. In this paper, we model the entities in the problem and their categories as entity graphs and use graph attention to generate knowledge-aware problem representations. Seq2Seq neural networks (Sutskever et al., 2014) have achieved promising results on math word problem solving. For instance, Wang et al. (2017) used a Seq2Seq model to generate math expressions. Wang et al. (2018b) incorporated reinforcement learning into the model to construct a math expression step by step. Zou and Lu (2019) used a data-driven approach to semantically parsing text into math expressions. Recently, tree-structured decoder was used to further improve the seq2seq framework. Xie and Sun (2019) propose a seq2tree model to generate expression tree in a goal-driven manner based on the parent node and left sibling tree of each node. Liu et al. (2019) propose a tree-structured decoding method with an auxiliary stack that generates the abstract syntax tree of the equation in a top-down manner."
2020.emnlp-main.579,D17-1088,0,0.302848,"ee, a model can capture information between long-distance nodes. Math word problem solving has attracted increasing attention, and many math word problem solving systems have been developed. Early statistical learning methods (Feigenbaum et al., 1963; Fletcher, 1985; Bakman, 2007; Roy and Roth, 2016) extracted templates or features from problems and generated corresponding math expressions based on these templates or features. These methods require a large number of manually formulated features or can only be applied to small application problems in small areas. In recent years, many methods (Wang et al., 2017, 2018b; Xie and Sun, 2019) have been developed that apply neural networks to analyze math word problems, with Corresponding author. orange Expression tree: 1 step / 4 Introduction ∗ fruit promising results. These methods use end-to-end models to directly generate the corresponding math expressions from the problem text. Although previous methods have achieved promising results, several problems remain that need to be addressed: 1) Background knowledge and common sense should be incorporated. For example, as shown in Figure 1, both apples and oranges are fruit. Humans are naturally aware of th"
2020.emnlp-main.579,D19-1016,0,0.0616293,"Missing"
2020.emnlp-main.579,D19-1536,0,0.0798034,"es or build multi-hop inference graphs based on relationships in the knowledge base, and have achieved promising results. In this paper, we model the entities in the problem and their categories as entity graphs and use graph attention to generate knowledge-aware problem representations. Seq2Seq neural networks (Sutskever et al., 2014) have achieved promising results on math word problem solving. For instance, Wang et al. (2017) used a Seq2Seq model to generate math expressions. Wang et al. (2018b) incorporated reinforcement learning into the model to construct a math expression step by step. Zou and Lu (2019) used a data-driven approach to semantically parsing text into math expressions. Recently, tree-structured decoder was used to further improve the seq2seq framework. Xie and Sun (2019) propose a seq2tree model to generate expression tree in a goal-driven manner based on the parent node and left sibling tree of each node. Liu et al. (2019) propose a tree-structured decoding method with an auxiliary stack that generates the abstract syntax tree of the equation in a top-down manner. In this paper, we generated the pre-order math expression tree based on parent node state of each node and recursiv"
2020.emnlp-main.729,N10-1086,0,0.532672,"ted ones (GTQs). Phrases underlined are the answers to ground-truth questions. (b) Knowledge graph constructed based on the input text shown in top sub-figure. Two colored ellipsoid are two query paths related to two ground truth questions in sub-figure (a) respectively. Nodes in green are covered by ground-truth questions. Figure 1: A sample paragraph from SQuAD with machine generated questions (Zhou et al., 2017) (a), ground truth questions (a) and corresponding knowledge graph (b). Introduction Question Generation (QG) from text aims to automatically construct questions from textual input (Heilman and Smith, 2010). It receives increasing attentions from research communities recently, due to its broad applications in scenarios of dialogue system and educational reading comprehension (Piwek et al., 2007; Duan et al., 2017). It can also help to augment the question set to enhance the performance of question answering systems. ∗ Corresponding author Our code is available at https://github.com/ WangsyGit/PathQG. 1 Current QG systems mainly follow the sequenceto-sequence structure with an encoder for modeling the textual input and a decoder for text generation (Du et al., 2017). These neural-based models hav"
2020.emnlp-main.729,W05-0909,0,0.0144766,"eration. - PathQG is our proposed generation framework consisting of a query representation learner and a query-based question generator. PathQG-V is the variational version of PathQG with an additional posterior query learner. - NQG++ is an oracle model that is aware of all path information contained in the target question and encode them via BIO scheme. It can be treated as the upper bound of NQG+ (pl). We present this result for reference. 5.4 Automatic Evaluation Results For the automatic evaluation, we utlize some widely adopted metrics including BLEU 1-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGEL (Lin and Hovy, 2003). Besides, we also compare results in the semantic content level by using a metric named SPICE (Anderson et al., 2016). It evaluates the similarity of scene graphs generated from candidate and reference questions. Evaluation results on both whole and complex datasets are shown in the Table 2 and 3. We have several findings: - PathQG-V outperforms other models in terms of all metrics on both datasets by a considerable margin. This indicates the effectiveness of our 9071 variational inference framework for modeling the query path for better question generation. -"
2020.emnlp-main.729,N18-1165,0,0.0203742,"tuitively, we divide the task of question generation from a query path into two steps, namely, query representation learning and query-based question generation. We formulate the former step as a sequence labeling problem for identifying the involved facts to form a query. For query-based question generation, an RNN-based generator is used to generate the question word by word. We first train the two modules jointly in an end-to-end fashion (PathQG in Section 3). In order to further enforce the interaction between theses two modules, we employ a variational framework to train the two modules (Chen et al., 2018; Zhang et al., 2018) that treats query representation learning as an inference process from the query path taking the generated question as the target (PathQG-V in Section 4). For model evaluation, we build the experimental environment on top of the benchmark dataset SQuAD (Rajpurkar et al., 2016). In specific, we automatically construct the KG for each piece of input text, and pair ground-truth questions with corresponding query paths from the KG. Experimental results show that our generation model outperforms other state-of-the-art QG models, especially when the questions are more complicat"
2020.emnlp-main.729,D19-1317,0,0.214464,"et al., 2018a,b), has attracted increasing attention in recent years. Most previous studies on textual question generation are rule-based and transform a declarative sentence into an interrogative sentence according to hand-crafted patterns (Heilman and Smith, 2010; Heilman, 2011). With the advance of neural network, Du et al. (2017) propose to apply a seq2seq structure with attention for automatic question generation. As follow-up, Zhou et al. (2017); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (2019) explore to use answer-relevant context to guide question generation. Besides, some studies (Wang et al., 2017; Tang et al., 2017; Wang et al., 2019) take question generation as a subtask, and jointly learn it with other tasks, such as question answering and phrase extraction, which also help to alleviate the uncertainty and improve the generation performance. Another stream of research for question generation is from KG to question. Reddy et al. (2017); Elsahar et al. (2018) explore to generate questions from a single KG triple using text as context information. It is close to our setting, bu"
2020.emnlp-main.729,P17-1123,0,0.422642,"ns from textual input (Heilman and Smith, 2010). It receives increasing attentions from research communities recently, due to its broad applications in scenarios of dialogue system and educational reading comprehension (Piwek et al., 2007; Duan et al., 2017). It can also help to augment the question set to enhance the performance of question answering systems. ∗ Corresponding author Our code is available at https://github.com/ WangsyGit/PathQG. 1 Current QG systems mainly follow the sequenceto-sequence structure with an encoder for modeling the textual input and a decoder for text generation (Du et al., 2017). These neural-based models have shown promising performance, however, they suffer from generating irrelevant and uninformative questions. Figure 1a presents two sample questions generated by a nueral QG model. Q2 contains irrelevant information “Everton Fc”. Although Q1 is correct, it is a safe play without mentioning any 9066 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 9066–9075, c November 16–20, 2020. 2020 Association for Computational Linguistics specific information in the input text. One possible reason causing the problem is that curren"
2020.emnlp-main.729,N03-1020,0,0.165225,"Missing"
2020.emnlp-main.729,D17-1090,0,0.0396691,"ground truth questions in sub-figure (a) respectively. Nodes in green are covered by ground-truth questions. Figure 1: A sample paragraph from SQuAD with machine generated questions (Zhou et al., 2017) (a), ground truth questions (a) and corresponding knowledge graph (b). Introduction Question Generation (QG) from text aims to automatically construct questions from textual input (Heilman and Smith, 2010). It receives increasing attentions from research communities recently, due to its broad applications in scenarios of dialogue system and educational reading comprehension (Piwek et al., 2007; Duan et al., 2017). It can also help to augment the question set to enhance the performance of question answering systems. ∗ Corresponding author Our code is available at https://github.com/ WangsyGit/PathQG. 1 Current QG systems mainly follow the sequenceto-sequence structure with an encoder for modeling the textual input and a decoder for text generation (Du et al., 2017). These neural-based models have shown promising performance, however, they suffer from generating irrelevant and uninformative questions. Figure 1a presents two sample questions generated by a nueral QG model. Q2 contains irrelevant informat"
2020.emnlp-main.729,N18-1020,0,0.0182037,"; Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (2019) explore to use answer-relevant context to guide question generation. Besides, some studies (Wang et al., 2017; Tang et al., 2017; Wang et al., 2019) take question generation as a subtask, and jointly learn it with other tasks, such as question answering and phrase extraction, which also help to alleviate the uncertainty and improve the generation performance. Another stream of research for question generation is from KG to question. Reddy et al. (2017); Elsahar et al. (2018) explore to generate questions from a single KG triple using text as context information. It is close to our setting, but we are different in two aspects. First, we propose to form a query path consisting of multiple triples for question generation instead of a single triple. Second, the context we process is where the extracted triples from. This setting is more natural and different from using retrieved text as context as they did. 7 Conclusion and Future Work In this paper, we propose to model facts in the input text as knowledge graph for question generation. We present a novel task of gen"
2020.emnlp-main.729,C18-1150,1,0.710092,"o cases of input texts, paths, answers and questions generated by human, NQG+ and PathQG-V. Phrases underlined are irrelevant to the input text. model PathQG-V is more informative and specific, which consists of information “plymouth” and “late 18th”. In sample 2, our generated question is consistent to the input text while the one from NQG+ contains irrelevant phrase “swazi economy”. 6 Related Work Question Generation, aiming at generating questions from a range of inputs, such as raw text (Heilman and Smith, 2010), structured data (Serban et al., 2016) and images (Mostafazadeh et al., 2016; Fan et al., 2018a,b), has attracted increasing attention in recent years. Most previous studies on textual question generation are rule-based and transform a declarative sentence into an interrogative sentence according to hand-crafted patterns (Heilman and Smith, 2010; Heilman, 2011). With the advance of neural network, Du et al. (2017) propose to apply a seq2seq structure with attention for automatic question generation. As follow-up, Zhou et al. (2017); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (20"
2020.emnlp-main.729,P16-1170,0,0.0988213,"estion by 9072 Figure 6: Two cases of input texts, paths, answers and questions generated by human, NQG+ and PathQG-V. Phrases underlined are irrelevant to the input text. model PathQG-V is more informative and specific, which consists of information “plymouth” and “late 18th”. In sample 2, our generated question is consistent to the input text while the one from NQG+ contains irrelevant phrase “swazi economy”. 6 Related Work Question Generation, aiming at generating questions from a range of inputs, such as raw text (Heilman and Smith, 2010), structured data (Serban et al., 2016) and images (Mostafazadeh et al., 2016; Fan et al., 2018a,b), has attracted increasing attention in recent years. Most previous studies on textual question generation are rule-based and transform a declarative sentence into an interrogative sentence according to hand-crafted patterns (Heilman and Smith, 2010; Heilman, 2011). With the advance of neural network, Du et al. (2017) propose to apply a seq2seq structure with attention for automatic question generation. As follow-up, Zhou et al. (2017); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018)"
2020.emnlp-main.729,P02-1040,0,0.109663,"sing BIO scheme for question generation. - PathQG is our proposed generation framework consisting of a query representation learner and a query-based question generator. PathQG-V is the variational version of PathQG with an additional posterior query learner. - NQG++ is an oracle model that is aware of all path information contained in the target question and encode them via BIO scheme. It can be treated as the upper bound of NQG+ (pl). We present this result for reference. 5.4 Automatic Evaluation Results For the automatic evaluation, we utlize some widely adopted metrics including BLEU 1-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGEL (Lin and Hovy, 2003). Besides, we also compare results in the semantic content level by using a metric named SPICE (Anderson et al., 2016). It evaluates the similarity of scene graphs generated from candidate and reference questions. Evaluation results on both whole and complex datasets are shown in the Table 2 and 3. We have several findings: - PathQG-V outperforms other models in terms of all metrics on both datasets by a considerable margin. This indicates the effectiveness of our 9071 variational inference framework for modeling the query path"
2020.emnlp-main.729,D14-1162,0,0.0838358,"esponding query paths from the KG for ground truth questions. In practice, a path can be determined by a start node and an end node. We thus use answer entity of the question as the start node and use the entity identified in the question as the end node. If the question contains multiple entities, we take the one farthest to the start node in the KG as the end node. We ignore the edge directions to simplify the modeling of query path. 5.2 Implementation Details We construct different vocabularies for input texts and questions respectively by keeping words which appear more than twice. Glove (Pennington et al., 2014) is used to initialize word embedding with dimension 300 and the embedding for BIO tag is randomly initialized of size 20. The size of hidden units in LSTM cell in all encoders is 300 while the size of the generation decoder is 1200. The hyperparameters to balance weights of losses are chosen as λ = 0.5 and β = 0.1. We evaluate our model on validation set to choose parameters. During test 2 The constructed KGs and complex question index can be downloaded from https://www.disc.fudan.edu. cn/data/fudan_pathqg_data.zip. 9070 Model NQG+ AFPA ASs2s NQG+ (pl) PathQG PathQG-V NQG++ BLEU 1 49.89 50.05"
2020.emnlp-main.729,D16-1264,0,0.0492678,"stion generation, an RNN-based generator is used to generate the question word by word. We first train the two modules jointly in an end-to-end fashion (PathQG in Section 3). In order to further enforce the interaction between theses two modules, we employ a variational framework to train the two modules (Chen et al., 2018; Zhang et al., 2018) that treats query representation learning as an inference process from the query path taking the generated question as the target (PathQG-V in Section 4). For model evaluation, we build the experimental environment on top of the benchmark dataset SQuAD (Rajpurkar et al., 2016). In specific, we automatically construct the KG for each piece of input text, and pair ground-truth questions with corresponding query paths from the KG. Experimental results show that our generation model outperforms other state-of-the-art QG models, especially when the questions are more complicated. Human evaluation also proves the effectiveness of our model in terms of both relevance and informativeness. 2 Task Definition We first introduce some notations in our task: - x = (x1 , ..., xn ): an input text with n tokens, where xi is the ith token; - G: a knowledge graph constructed from x,"
2020.emnlp-main.729,E17-1036,0,0.0170504,"7); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (2019) explore to use answer-relevant context to guide question generation. Besides, some studies (Wang et al., 2017; Tang et al., 2017; Wang et al., 2019) take question generation as a subtask, and jointly learn it with other tasks, such as question answering and phrase extraction, which also help to alleviate the uncertainty and improve the generation performance. Another stream of research for question generation is from KG to question. Reddy et al. (2017); Elsahar et al. (2018) explore to generate questions from a single KG triple using text as context information. It is close to our setting, but we are different in two aspects. First, we propose to form a query path consisting of multiple triples for question generation instead of a single triple. Second, the context we process is where the extracted triples from. This setting is more natural and different from using retrieved text as context as they did. 7 Conclusion and Future Work In this paper, we propose to model facts in the input text as knowledge graph for question generation. We pres"
2020.emnlp-main.729,W15-2812,0,0.0136549,"tructed by other methods. Experiments Experimental Dataset Our experiments are conducted on SQuAD (Rajpurkar et al., 2016) consisting of 61,623 sentences. Each sentence is annotated with several questions together with their answers extracted from the input text. We build our experimental dataset on top of SQuAD. We construct knowledge graph for each sentence automatically and identify query paths for ground truth questions for evaluation. The resulted dataset consists of 89,976 tuples (input sentence x, query path s, ground truth question y). KG construction We employ the scene graph parser (Schuster et al., 2015) for KG construction from a textual description. It identifies entities and their relationships from a text and build a scene graph. It turns out that the generated scene graph usually misses some key information in the text, thus we employ the part-of-speech tagger to extract verb phrases between entities to further enrich relationship labels. The extended scene graph is used as the knowledge graph for the input text. The average quantities of entities and facts in each KG are dataset train valid test complex whole 12,828 68,704 1,895 10,313 1,855 10,959 len. of ques. 14.7 13.3 Table 1: Stati"
2020.emnlp-main.729,P16-1056,0,0.040077,"Missing"
2020.emnlp-main.729,N18-2090,0,0.09269,"zadeh et al., 2016; Fan et al., 2018a,b), has attracted increasing attention in recent years. Most previous studies on textual question generation are rule-based and transform a declarative sentence into an interrogative sentence according to hand-crafted patterns (Heilman and Smith, 2010; Heilman, 2011). With the advance of neural network, Du et al. (2017) propose to apply a seq2seq structure with attention for automatic question generation. As follow-up, Zhou et al. (2017); Sun et al. (2018); Kim et al. (2019) propose to utilize the answers to decrease the generation uncertainty. Meanwhile, Song et al. (2018) and Li et al. (2019) explore to use answer-relevant context to guide question generation. Besides, some studies (Wang et al., 2017; Tang et al., 2017; Wang et al., 2019) take question generation as a subtask, and jointly learn it with other tasks, such as question answering and phrase extraction, which also help to alleviate the uncertainty and improve the generation performance. Another stream of research for question generation is from KG to question. Reddy et al. (2017); Elsahar et al. (2018) explore to generate questions from a single KG triple using text as context information. It is clo"
2020.findings-emnlp.60,D19-5709,0,0.0274029,"Missing"
2020.findings-emnlp.60,W03-2201,0,0.00780148,"pes. We formulate such a task as a partially supervised learning problem and accordingly propose an effective algorithm to solve the problem. Comprehensive experimental studies on several public NER datasets validate the effectiveness of our method. 1 Introduction Named Entity Recognition (NER) is a type of information extraction task that seeks to identify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 678–688 c November 1"
2020.findings-emnlp.60,M98-1001,0,0.380345,"osed, like the bidirectional long short-term memory network (LSTM) plus a CRF layer (Huang et al., 2015), the convolutional neural network (CNN) plus a CRF layer, the combination of LSTM and CNN (Chiu and Nichols, 2016), and the BERT based LSTM+CRF model (Jiang et al., 2019; Hakala and Pyysalo, 2019). 4 4.1 Experiments Datasets Following the experimental setting of the most related work (Peng et al., 2019), we performed the experiments on the four public NER datasets, including Conll03 (en) in English (Tjong Kim Sang and De Meulder, 2003), CoNLL02 (sp) (Sang and Erik, 2002) in Spanish, MUC-7 (Chinchor, 1998), Twitter (Zhang et al., 2018) in English, and OntoNotes4.0 (Weischedel et al., 2011) in Chinese. For the former four datasets, we treated the location (LOC) and person (PER) types as the newly introduced entity types in the target task, and treated the rest entity types as the predefined entity types in the source task. While for OntoNotes4.0, we treated the GPE (countries, cities, states) and location (non-GPE locations, mountain ranges, bodies of water) types as the newly introduced entity types in the target task, which are all classified as the location type in the source task. Table 3 sh"
2020.findings-emnlp.60,Q16-1026,0,0.228132,"al., 2006; Gerner et al., 2010; Liu et al., 2015). They do not require annotated training data but heavily rely on background knowledge (rules) and lexicon resources. They work well when the lexicon is exhaustive, but fail when the lexicon is incomplete. Precision is generally high for these systems, but recall is often low due to incomplete lexicons. Current state-of-the-art NER systems are mainly based on annotated data and machine learning approaches. The lexicons introduced in some of these systems are mainly for extracting some external features (Liu et al., 2015; Agerri and Rigau, 2016; Chiu and Nichols, 2016). This field has been previously dominated by the graph πi ← |Dit |/|Dt |, i = 1, · · · , ns , since class i data is fully labeled in Dt . For estimating πns +j and πn0 s +j , j = 1, · · · , nt + 1, we apply an iteration strategy. In particular, we first initialized πns +j and πn0 s +j for j ≤ nt 0 by by |Dnt s +j |/|Dt |, and initialize πK and πK |Dut |/|Dt |and 1, respectively. Based on this, we train the classifier f and then re-estimate πns +j and πn0 s +j using the trained classifier as follows: πns +j ← πn0 s +j (10) i j=1 1 X f (w)[ns + j], |Dt | w∈Dt 1 X ← t f (w)[ns + j], |Du | t Labe"
2020.findings-emnlp.60,N19-1423,0,0.0638126,"the labeled data of a new entity type ej , we use its corresponding entity lexicon Lj to scan words of class K in Ds and find out some confident words of the entity type to construct labeled data Dnt s +j of class ns + j. This process applies nt times to obtain the labeled data of the nt new entity types. The rest words of class K in Ds not being selected by the lexicons form the unlabeled data set Dut ⊆ Dt in the target task, which contains 2.5 Model Architecture For a sentence s = [w1 , · · · , wl ] with l words, we first get the contextualized representations of words using the BERT model (Devlin et al., 2019): h1 , · · · , hl = BERT(w1 , · · · , wl ). 680 (1) Algorithm 1 Data Labeling using the Lexicons 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: fully supervised learning loss, which is defined as follows: nX s +nt Lsup = πi Lii . (3) Input: entity lexicons Lj , for j = 1, · · · , nt with Lj ∩ Lk = ∅ if j 6= k, a word sequence s = {w1 , · · · , wn } ∈ class K in Ds , and the maximum mention length lw Result: the partially labeled dataset Dt Initialize: i ← 1 while i ≤ n do for k ∈ [lw , · · · , 0] do b ← f alse for j ∈ [1, · · · , nt ] do if {wi , · · · , wi+k } ∈ Lj then assign {wi"
2020.findings-emnlp.60,N16-1030,0,0.0181556,"ype of information extraction task that seeks to identify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 678–688 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Predict PER O O ORG O O JOB JOB JOB Ts , Tt ns nt ei , i ≤ ns ens +j Lj Ds , Dt Dit Dut πi πn0 s +j L ps = LPER + LORG + Lu Partially Supervised Learning Partial Label PER U U Source Label PER O O ORG U U U U JOB O O Data labeling usin"
2020.findings-emnlp.60,P05-1045,0,0.206195,"75.02 78.39 82.22 76.24 79.39 82.00 76.45 79.38 GPE LOC Overall 64.37 25.03 61.02 65.73 25.92 61.81 68.21 35.29 65.15 79.66 43.65 77.93 44.42 23.17 40.33 68.17 33.28 65.79 68.15 34.33 66.22 72.37 36.40 70.66 71.27 37.22 69.78 Table 4: Testing chunk-level F1 on the target task. The four label-based methods are fully supervised and trained on the fully re-annotated data of the source task. While the five lexicon-based methods train the model using only the existing labels of the source task and entity lexicons of the new entity types. The best performance in each group is marked in a boldface. Finkel et al., 2005), the bi-directional long shortterm memory network with the CRF layer BiLSTM+CRF or not BiLSTM (Huang et al., 2015), and the BERT based model (Devlin et al., 2019) described in the “Model architecture” section. These supervised models were trained on the fully re-annotated Ds according to the data labeling criteria of the target task. names, respectively. We refer you to the referred work for more information about the lexicons. Here, we address that it can only label a small part of the mentions of the person and location types using the lexicons. 4.3 Compared Methods In the following, we ref"
2020.findings-emnlp.60,D19-1096,1,0.887387,"Missing"
2020.findings-emnlp.60,W02-2019,0,0.0444266,"studies on several public NER datasets validate the effectiveness of our method. 1 Introduction Named Entity Recognition (NER) is a type of information extraction task that seeks to identify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 678–688 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Predict PER O O ORG O O JOB JOB JOB Ts , Tt ns nt ei , i ≤ ns ens +j Lj Ds , Dt Dit Dut πi πn0"
2020.findings-emnlp.60,W03-0430,0,0.22851,"eral public NER datasets validate the effectiveness of our method. 1 Introduction Named Entity Recognition (NER) is a type of information extraction task that seeks to identify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 678–688 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Predict PER O O ORG O O JOB JOB JOB Ts , Tt ns nt ei , i ≤ ns ens +j Lj Ds , Dt Dit Dut πi πn0 s +j L ps = LPER + LORG"
2020.findings-emnlp.60,O03-4002,0,0.0596991,"B (beginning), I (internal), or E (end), the words labeled by the lexicons belong to. 2.3 2.4 Method Overview Obtain the Partially Labeled Data using the Entity Lexicons In this section, we detail the construction of the partially labeled dataset Dt for the target task. As illustrated before, the labeled data Dit , i ≤ ns of class i can be easily obtained from Ds according to the data labeling of Ts . Thus, in the following, we focus on obtaining Dnt s +j , j = 1, · · · , nt and Dut using the entity lexicons. Following the idea of (Peng et al., 2019), we apply the maximum matching algorithm (Xue, 2003) to obtain words that match with the lexicon Lj and belong to class K in Ds to construct Dnt s +j . As summarized in algo. 1, this algorithm is a greedy search routine that walks through a sequence of class K words trying to find the longest string that matches with an entry of the lexicons. Note that in algo. 1, lw is intuitively set to 4, and the “for” loop is broken in step 12 because a mention must not occur in multiple lexicons, which is guaranteed by Lj ∩ Lk = ∅ if j 6= k. Based on the above label assignment mechanism, we train a (ns + nt )-class classifier to perform the target task, Tt"
2020.findings-emnlp.60,P18-1144,0,0.0165176,"ify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 678–688 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Predict PER O O ORG O O JOB JOB JOB Ts , Tt ns nt ei , i ≤ ns ens +j Lj Ds , Dt Dit Dut πi πn0 s +j L ps = LPER + LORG + Lu Partially Supervised Learning Partial Label PER U U Source Label PER O O ORG U U U U JOB O O Data labeling using the Job Title lexicon Input Bobick works at O O Googl"
2020.findings-emnlp.60,P19-1231,1,0.235364,"ity mention, which means that we cannot distinguish the type, B (beginning), I (internal), or E (end), the words labeled by the lexicons belong to. 2.3 2.4 Method Overview Obtain the Partially Labeled Data using the Entity Lexicons In this section, we detail the construction of the partially labeled dataset Dt for the target task. As illustrated before, the labeled data Dit , i ≤ ns of class i can be easily obtained from Ds according to the data labeling of Ts . Thus, in the following, we focus on obtaining Dnt s +j , j = 1, · · · , nt and Dut using the entity lexicons. Following the idea of (Peng et al., 2019), we apply the maximum matching algorithm (Xue, 2003) to obtain words that match with the lexicon Lj and belong to class K in Ds to construct Dnt s +j . As summarized in algo. 1, this algorithm is a greedy search routine that walks through a sequence of class K words trying to find the longest string that matches with an entry of the lexicons. Note that in algo. 1, lw is intuitively set to 4, and the “for” loop is broken in step 12 because a mention must not occur in multiple lexicons, which is guaranteed by Lj ∩ Lk = ∅ if j 6= k. Based on the above label assignment mechanism, we train a (ns +"
2020.findings-emnlp.60,P02-1060,0,0.530733,"Missing"
2020.findings-emnlp.60,N13-1008,0,0.0459908,"led data and entity lexicons of the newly introduced entity types. We formulate such a task as a partially supervised learning problem and accordingly propose an effective algorithm to solve the problem. Comprehensive experimental studies on several public NER datasets validate the effectiveness of our method. 1 Introduction Named Entity Recognition (NER) is a type of information extraction task that seeks to identify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association f"
2020.findings-emnlp.60,W02-2024,0,0.149758,"ious neural architectures have been proposed, like the bidirectional long short-term memory network (LSTM) plus a CRF layer (Huang et al., 2015), the convolutional neural network (CNN) plus a CRF layer, the combination of LSTM and CNN (Chiu and Nichols, 2016), and the BERT based LSTM+CRF model (Jiang et al., 2019; Hakala and Pyysalo, 2019). 4 4.1 Experiments Datasets Following the experimental setting of the most related work (Peng et al., 2019), we performed the experiments on the four public NER datasets, including Conll03 (en) in English (Tjong Kim Sang and De Meulder, 2003), CoNLL02 (sp) (Sang and Erik, 2002) in Spanish, MUC-7 (Chinchor, 1998), Twitter (Zhang et al., 2018) in English, and OntoNotes4.0 (Weischedel et al., 2011) in Chinese. For the former four datasets, we treated the location (LOC) and person (PER) types as the newly introduced entity types in the target task, and treated the rest entity types as the predefined entity types in the source task. While for OntoNotes4.0, we treated the GPE (countries, cities, states) and location (non-GPE locations, mountain ranges, bodies of water) types as the newly introduced entity types in the target task, which are all classified as the location"
2020.findings-emnlp.60,W04-1221,0,0.0830411,"s validate the effectiveness of our method. 1 Introduction Named Entity Recognition (NER) is a type of information extraction task that seeks to identify entity names from unstructured text and categorize them into a predefined list of types. It plays an important role in many downstream tasks such as knowledge base construction (Riedel et al., 2013; Shen et al., 2012), machine translation (Babych and Hartley, 2003), and search (Zhu et al., 2005), etc. In this field, the supervised methods, ranging from the conventional graph models (McCallum et al., 2000; Malouf, 2002; McCallum and Li, 2003; Settles, 2004) to the dominant deep neural methods (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang and Yang, 2018; Jiang et al., 2019; Gui et al., 2019), have achieved great success. However, these supervised methods usually require large scale labeled data to 678 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 678–688 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Predict PER O O ORG O O JOB JOB JOB Ts , Tt ns nt ei , i ≤ ns ens +j Lj Ds , Dt Dit Dut πi πn0 s +j L ps = LPER + LORG + Lu Partially"
2020.findings-emnlp.60,W03-0419,0,0.385436,"Missing"
2021.acl-demo.41,D18-2029,0,0.0379272,"Missing"
2021.acl-demo.41,N19-1423,0,0.207492,"re rated on a 1-5 scale (5 denotes the best). in a general way, TextFlint supports generating massive and comprehensive transformed samples with just one command. By default, TextFlint performs all single transformations on the original dataset to form the corresponding transformed datasets, and the performance of the target models is tested on these datasets. The evaluation report provides a comparative view of model performance on datasets before and after certain types of transformations, which supports model weakness analyses and guides particular improvements. For example, take BERT base(Devlin et al., 2019) as the target model to verify its robustness on the CONLL2003 dataset(Tjong Kim Sang and De Meulder, 2003), whose robustness report is shown in Figure 5. The performance of BERT base decreases significantly in some morphology transformations, such as OCR, Keyboard, Typos, and Spelling Error. To combat these errors of input texts and improve the robustness of the model, we suggest that placing a word correction model(Pruthi et al., 2019) before BERT would be beneficial. 0.8 Case 2: Customized Evaluation For users who want to test model performance on specific aspects, they demand a customized"
2021.acl-demo.41,D18-1380,0,0.107351,"Missing"
2021.acl-demo.41,2021.naacl-demos.6,0,0.061782,"Missing"
2021.acl-demo.41,D14-1181,0,0.00437317,"Missing"
2021.acl-demo.41,2020.emnlp-main.500,1,0.864395,"like to teach kids in the kindergarten. The storm destroyed many houses in the village. ✘ Figure 1: Examples of three main generation functions. The transformation example is from ABSA (Aspectbased Sentiment Analysis) task, where the italic bold RevTgt (short for reverse target) denotes task-specific transformations, and the bold Typos denotes universal transformation. Introduction The detection of model robustness has been attracting increasing attention in recent years, given that deep neural networks (DNNs) of high accuracy can still be vulnerable to carefully crafted adversarial examples (Li et al., 2020), distribution shift (Miller et al., 2020), data transformation (Xing et al., 2020), and shortcut learning (Geirhos et al., 2020). Existing approaches to textual robustness evaluation focus on slightly modifying the input data, which maintains the original meaning and results in a different prediction. However, these methods often concentrate on either universal or task-specific generalization capabilities, which is difficult to comprehensively evaluate. In response to the shortcomings of recent works, we introduce TextFlint, a unified, multilingual, and analyzable robustness evaluation toolki"
2021.acl-demo.41,P18-1087,0,0.0272118,"Missing"
2021.acl-demo.41,P02-1040,0,0.116092,"are implemented based on TextAttack (Morris et al., 2020). Validator It is crucial to verify the quality of the samples generated by Transformation and AttackRecipe. TextFlint provides several metrics to evaluate the quality of the generated text, including (1) language model perplexity calculated based on the GPT2 model (Radford et al., 2019), (2) word replacement ratio in generated text compared with its original text, (3) edit distance between original text and generated text, (4) semantic 349 similarity calculated based on Universal Sentence Encoder (Cer et al., 2018), and (5) BLEU score (Papineni et al., 2002). 2.3 Reporter Layer Generation Layer yields three types of adversarial samples and verifies the robustness of the target model. Based on the evaluation results from Generation Layer, Report Layer aims to provide users with a standard analysis report from syntax, morphology, pragmatics, and paradigmatic relation aspects. The running process of Report Layer can be regarded as a pipeline from Analyzer to ReportGenerator. 3 Figure 4: Screenshot of TextFlint’s web interface running Ocr transformation for ABSA task. Usage Using TextFlint to verify the robustness of a specific model is as simple as"
2021.acl-demo.41,P19-1561,0,0.0287018,"Missing"
2021.acl-demo.41,2020.acl-main.442,0,0.114738,"methods equipped with pre-training LMs showed better performance than other models on the task-specific transformations, e.g., AddDiff , where the accuracy score of BERT-Aspect dropped from 90.32 to merely 81.58. All the evaluation results and comprehensive robustness analysis are available at textflint.io. 5 Related Work Our work is related to many existing open-source tools and works in different areas. Robustness Evaluation Many tools include evaluation methods for robustness, including NLPAug (Ma, 2019), Errudite (Wu et al., 2019), AllenNLP Interpret (Wallace et al., 2019), and Checklist (Ribeiro et al., 2020), which are only applicable to limited parts of robustness evaluations, while TextFlint supports comprehensive evaluation methods, e.g., subpopulation, adversarial attacks, transformations, and so on. Besides the common transformation methods like synonym substitution and typos, various task-specific transformations are tailored for each of the 12 NLP tasks, which is peculiar to TextFlint. Moreover, we are the first to provide linguistic support for the transformations, the designs for which were inspired by linguistics and have been proved plausible and readable by human annotators. Several t"
2021.acl-demo.41,C16-1311,0,0.0414402,"Missing"
2021.acl-demo.41,D16-1021,0,0.0913955,"Missing"
2021.acl-demo.41,D19-3002,0,0.0392234,"Missing"
2021.acl-demo.41,D16-1058,0,0.0373143,"Missing"
2021.acl-demo.41,P19-1073,0,0.133542,"underlying patterns about model robustness. As for the ABSA task (Table 2), methods equipped with pre-training LMs showed better performance than other models on the task-specific transformations, e.g., AddDiff , where the accuracy score of BERT-Aspect dropped from 90.32 to merely 81.58. All the evaluation results and comprehensive robustness analysis are available at textflint.io. 5 Related Work Our work is related to many existing open-source tools and works in different areas. Robustness Evaluation Many tools include evaluation methods for robustness, including NLPAug (Ma, 2019), Errudite (Wu et al., 2019), AllenNLP Interpret (Wallace et al., 2019), and Checklist (Ribeiro et al., 2020), which are only applicable to limited parts of robustness evaluations, while TextFlint supports comprehensive evaluation methods, e.g., subpopulation, adversarial attacks, transformations, and so on. Besides the common transformation methods like synonym substitution and typos, various task-specific transformations are tailored for each of the 12 NLP tasks, which is peculiar to TextFlint. Moreover, we are the first to provide linguistic support for the transformations, the designs for which were inspired by lingu"
2021.acl-demo.41,2020.emnlp-main.292,1,0.893206,"Missing"
2021.acl-long.354,P19-1208,0,0.350893,"Missing"
2021.acl-long.354,D18-1439,0,0.711058,"Missing"
2021.acl-long.354,N19-1292,0,0.0565039,"s (Hulth, 2003; Nguyen and Kan, 2007). Other extractive approaches utilize neural-based sequence labeling methods (Zhang et al., 2016; Gollapalli et al., 2017). 2.2 Keyphrase Generation Compared to extractive approaches, generative ones have the ability to consider the absent keyphrase prediction. Meng et al. (2017) proposed a generative model CopyRNN, which employs a encoder-decoder framework (Sutskever et al., 2014) with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016). Many works are proposed based on the CopyRNN architecture (Chen et al., 2018; Zhao and Zhang, 2019; Chen et al., 2019b,a). In previous CopyRNN based works, each source text corresponds to a single target keyphrase. Thus, the model needs beam search during inference to overgenerate multiple keyphrases, which cannot determine the dynamic number of keyphrases and consider the inter-relation among keyphrases. 1 https://github.com/jiacheng-ye/kg_ one2set 4599 Set of Present Keyphrase deep learning Set of Absent Keyphrase ? topic model K-step Target Assignment ? neural network model ... ? ? ... ... topic vector quantization deep learning Transformer Encoder neural model ... neural model BOS + neural + Transformer"
2021.acl-long.354,2020.acl-main.103,0,0.348323,"Missing"
2021.acl-long.354,P16-1154,0,0.267944,"2S EQ and O NE 2S ET training paradigm. For the O NE 2S EQ training paradigm, although the predictions are correct in each keyphrase, they will still be considered wrong due to the shift in keyphrase order, and the model will receive a large penalty. Keyphrase generation (KG) aims to generate of a set of keyphrases that expresses the high-level semantic meaning of a document. These keyphrases can be further categorized into present keyphrases that appear in the document and absent keyphrases that do not. Meng et al. (2017) proposed a sequence-to-sequence (Seq2Seq) model with a copy mechanism (Gu et al., 2016) to predict both present and absent keyphrases. However, the model needs beam search during inference to overgenerate multiple keyphrases, which cannot determine the dynamic number of keyphrases. To Corresponding authors. topic model &lt;eos&gt; deep learning &lt;eos&gt; Introduction ∗ deep learning &lt;sep&gt; address this, Yuan et al. (2020) proposed the O NE 2S EQ training paradigm where each source text corresponds to a sequence of keyphrases that are concatenated with a delimiter hsepi and a terminator heosi. As keyphrases must be ordered before being concatenated, Yuan et al. (2020) sorted the present key"
2021.acl-long.354,W03-1028,0,0.614941,"ion model that can generate a set of diverse keyphrases in parallel and a dynamic target assignment mechanism to solve the intractable training problem under the O NE 2S ET paradigm; (3) our method consistently outperforms all the state-of-the-art methods and greatly reduces the duplication ratio. Our codes are publicly available at Github1 . 2 2.1 Related Work Keyphrase Extraction Existing approaches for keyphrase prediction can be broadly divided into extraction and generation methods. Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed (Hulth, 2003; Mihalcea and Tarau, 2004; Nguyen and Kan, 2007; Wan and Xiao, 2008). First, they extract a large set of candidate phrases by hand-crafted rules (Mihalcea and Tarau, 2004; Medelyan et al., 2009; Liu et al., 2011). Then, these candidates are scored and reranked based on unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised methods (Hulth, 2003; Nguyen and Kan, 2007). Other extractive approaches utilize neural-based sequence labeling methods (Zhang et al., 2016; Gollapalli et al., 2017). 2.2 Keyphrase Generation Compared to extractive approaches, generative ones have"
2021.acl-long.354,S10-1004,0,0.0317827,"using abs Y and predictions from C2 . Thus, we can modify the final loss in Equal 4 as follows: n L(θ) = −( N/2 |y | X X π ˆ pre (n) log pt (ytn ) n=1 t=1 + N X (5) n |y | X π ˆ abs (n) log pt (ytn )). n=N/2+1 t=1 In practice, we down-weight the log-probability term when ytn = ∅ by scale factors λpre and λabs for present keyphrase set and absent keyphrase set to account for the class imbalance. 4 4.1 Experimental Setup Datasets We conduct our experiments on five scientific article datasets, including Inspec (Hulth, 2003), NUS (Nguyen and Kan, 2007), Krapivin (Krapivin et al., 2009), SemEval (Kim et al., 2010) and KP20k (Meng et al., 2017). Each sample from these datasets consists of a title, an abstract, and some keyphrases. Following previous works (Meng et al., 2017; Chen et al., 2019b,a; Yuan et al., 2020), we concatenate the title and abstract as a source document. We use the largest dataset (i.e., KP20k) to train all the models. After preprocessing (i.e., lowercasing, replacing all the digits with the symbol hdigiti and removing the duplicated data), the final KP20k dataset contains 509,818 samples for training, 20,000 for validation, and 20,000 for testing. The dataset statistics are shown i"
2021.acl-long.354,P17-1054,0,0.470392,"igure 1: An example of ground-truth keyphrases (upper) and predictions (lower) under O NE 2S EQ and O NE 2S ET training paradigm. For the O NE 2S EQ training paradigm, although the predictions are correct in each keyphrase, they will still be considered wrong due to the shift in keyphrase order, and the model will receive a large penalty. Keyphrase generation (KG) aims to generate of a set of keyphrases that expresses the high-level semantic meaning of a document. These keyphrases can be further categorized into present keyphrases that appear in the document and absent keyphrases that do not. Meng et al. (2017) proposed a sequence-to-sequence (Seq2Seq) model with a copy mechanism (Gu et al., 2016) to predict both present and absent keyphrases. However, the model needs beam search during inference to overgenerate multiple keyphrases, which cannot determine the dynamic number of keyphrases. To Corresponding authors. topic model &lt;eos&gt; deep learning &lt;eos&gt; Introduction ∗ deep learning &lt;sep&gt; address this, Yuan et al. (2020) proposed the O NE 2S EQ training paradigm where each source text corresponds to a sequence of keyphrases that are concatenated with a delimiter hsepi and a terminator heosi. As keyphra"
2021.acl-long.354,W04-3252,0,0.39019,"t can generate a set of diverse keyphrases in parallel and a dynamic target assignment mechanism to solve the intractable training problem under the O NE 2S ET paradigm; (3) our method consistently outperforms all the state-of-the-art methods and greatly reduces the duplication ratio. Our codes are publicly available at Github1 . 2 2.1 Related Work Keyphrase Extraction Existing approaches for keyphrase prediction can be broadly divided into extraction and generation methods. Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed (Hulth, 2003; Mihalcea and Tarau, 2004; Nguyen and Kan, 2007; Wan and Xiao, 2008). First, they extract a large set of candidate phrases by hand-crafted rules (Mihalcea and Tarau, 2004; Medelyan et al., 2009; Liu et al., 2011). Then, these candidates are scored and reranked based on unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised methods (Hulth, 2003; Nguyen and Kan, 2007). Other extractive approaches utilize neural-based sequence labeling methods (Zhang et al., 2016; Gollapalli et al., 2017). 2.2 Keyphrase Generation Compared to extractive approaches, generative ones have the ability to consider t"
2021.acl-long.354,P17-1099,0,0.0164705,"cument after removing all the ∅ tokens from the N predictions. Formally, the decoder input at time step t for control code n is defined as follows: n dnt = ew + ept + cn , yt−1 (1) n , ep where ew is the embedding of word yt−1 n t yt−1 is the t-th sinusoid positional embedding as in (Vaswani et al., 2017) and cn is the n-th learned control code embedding. The decoder outputs the predictive distribution pnt , which is used to get the next word ytn . As some keyphrases contain words that do not exist in the predefined vocabulary but appear in the input document, we also employ a copy mechanism (See et al., 2017), which is generally adopted for many previous KG works (Meng et al., 2017; Chan et al., 2019; Chen et al., 2020; Yuan et al., 2020). larger than the number of ground-truth keyphrases, we consider the ground-truth keyphrases also as a set of size N padded with ∅. Note that the bipartite matching enforces permutation-invariance, and guarantees that each target element has a unique match. Thus, it reduces the duplication ratio of predictions. Specifically, as shown in Figure 2, both the fifth and eighth control code predict the same keyphrase “neural model”, but one of them is assigned with ∅. T"
2021.acl-long.354,2020.emnlp-main.645,0,0.0431275,"conditions. A K-step target assignment mechanism is used during training, where we first predict K words for each code, and then find an optimal allocation among the predictions and targets. In the figure, N = 8 and K = 2 are used. To this end, Yuan et al. (2020) proposed an O NE 2S EQ training paradigm where each source text corresponds to a sequence of concatenated keyphrases. Thus, the model can capture the contextual information between the keyphrases as well as determines the dynamic number of keyphrases for different source texts. The recent works (Chan et al., 2019; Chen et al., 2020; Swaminathan et al., 2020) mostly follow the O NE 2S EQ training paradigm. Chan et al. (2019) proposed an RL-based fine-tuning method using F1 and Recall metrics as rewards. Swaminathan et al. (2020) proposed an RL-based fine-tuning method using a discriminator to produce rewards. All the above models need to be trained or pre-trained under the O NE 2S EQ paradigm. As keyphrases must be ordered before concatenating and keyphrases are inherently an unordered set, the model can be trained with wrong signal. Our O NE 2S ET training paradigm aims to solve this problem. 3 Methodology This paper proposes a new training parad"
2021.acl-long.354,W11-0316,0,0.032953,"tly outperforms all the state-of-the-art methods and greatly reduces the duplication ratio. Our codes are publicly available at Github1 . 2 2.1 Related Work Keyphrase Extraction Existing approaches for keyphrase prediction can be broadly divided into extraction and generation methods. Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed (Hulth, 2003; Mihalcea and Tarau, 2004; Nguyen and Kan, 2007; Wan and Xiao, 2008). First, they extract a large set of candidate phrases by hand-crafted rules (Mihalcea and Tarau, 2004; Medelyan et al., 2009; Liu et al., 2011). Then, these candidates are scored and reranked based on unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised methods (Hulth, 2003; Nguyen and Kan, 2007). Other extractive approaches utilize neural-based sequence labeling methods (Zhang et al., 2016; Gollapalli et al., 2017). 2.2 Keyphrase Generation Compared to extractive approaches, generative ones have the ability to consider the absent keyphrase prediction. Meng et al. (2017) proposed a generative model CopyRNN, which employs a encoder-decoder framework (Sutskever et al., 2014) with attention (Bahdanau et al.,"
2021.acl-long.354,D09-1137,0,0.0155742,"3) our method consistently outperforms all the state-of-the-art methods and greatly reduces the duplication ratio. Our codes are publicly available at Github1 . 2 2.1 Related Work Keyphrase Extraction Existing approaches for keyphrase prediction can be broadly divided into extraction and generation methods. Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed (Hulth, 2003; Mihalcea and Tarau, 2004; Nguyen and Kan, 2007; Wan and Xiao, 2008). First, they extract a large set of candidate phrases by hand-crafted rules (Mihalcea and Tarau, 2004; Medelyan et al., 2009; Liu et al., 2011). Then, these candidates are scored and reranked based on unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised methods (Hulth, 2003; Nguyen and Kan, 2007). Other extractive approaches utilize neural-based sequence labeling methods (Zhang et al., 2016; Gollapalli et al., 2017). 2.2 Keyphrase Generation Compared to extractive approaches, generative ones have the ability to consider the absent keyphrase prediction. Meng et al. (2017) proposed a generative model CopyRNN, which employs a encoder-decoder framework (Sutskever et al., 2014) with attentio"
2021.acl-long.354,2020.acl-main.710,0,0.11346,"resses the high-level semantic meaning of a document. These keyphrases can be further categorized into present keyphrases that appear in the document and absent keyphrases that do not. Meng et al. (2017) proposed a sequence-to-sequence (Seq2Seq) model with a copy mechanism (Gu et al., 2016) to predict both present and absent keyphrases. However, the model needs beam search during inference to overgenerate multiple keyphrases, which cannot determine the dynamic number of keyphrases. To Corresponding authors. topic model &lt;eos&gt; deep learning &lt;eos&gt; Introduction ∗ deep learning &lt;sep&gt; address this, Yuan et al. (2020) proposed the O NE 2S EQ training paradigm where each source text corresponds to a sequence of keyphrases that are concatenated with a delimiter hsepi and a terminator heosi. As keyphrases must be ordered before being concatenated, Yuan et al. (2020) sorted the present keyphrases by their order of the first occurrence in the source text and appended the absent keyphrases to the end. During inference, the decoding process terminates when generating heosi, and the final keyphrase predictions are obtained after splitting the sequence by hsepi. Thus, a model trained with O NE 2S EQ paradigm can ge"
2021.acl-long.354,D16-1080,1,0.897734,"ethods. Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed (Hulth, 2003; Mihalcea and Tarau, 2004; Nguyen and Kan, 2007; Wan and Xiao, 2008). First, they extract a large set of candidate phrases by hand-crafted rules (Mihalcea and Tarau, 2004; Medelyan et al., 2009; Liu et al., 2011). Then, these candidates are scored and reranked based on unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised methods (Hulth, 2003; Nguyen and Kan, 2007). Other extractive approaches utilize neural-based sequence labeling methods (Zhang et al., 2016; Gollapalli et al., 2017). 2.2 Keyphrase Generation Compared to extractive approaches, generative ones have the ability to consider the absent keyphrase prediction. Meng et al. (2017) proposed a generative model CopyRNN, which employs a encoder-decoder framework (Sutskever et al., 2014) with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016). Many works are proposed based on the CopyRNN architecture (Chen et al., 2018; Zhao and Zhang, 2019; Chen et al., 2019b,a). In previous CopyRNN based works, each source text corresponds to a single target keyphrase. Thus, the model ne"
2021.acl-long.354,P19-1515,0,0.225059,") or supervised methods (Hulth, 2003; Nguyen and Kan, 2007). Other extractive approaches utilize neural-based sequence labeling methods (Zhang et al., 2016; Gollapalli et al., 2017). 2.2 Keyphrase Generation Compared to extractive approaches, generative ones have the ability to consider the absent keyphrase prediction. Meng et al. (2017) proposed a generative model CopyRNN, which employs a encoder-decoder framework (Sutskever et al., 2014) with attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016). Many works are proposed based on the CopyRNN architecture (Chen et al., 2018; Zhao and Zhang, 2019; Chen et al., 2019b,a). In previous CopyRNN based works, each source text corresponds to a single target keyphrase. Thus, the model needs beam search during inference to overgenerate multiple keyphrases, which cannot determine the dynamic number of keyphrases and consider the inter-relation among keyphrases. 1 https://github.com/jiacheng-ye/kg_ one2set 4599 Set of Present Keyphrase deep learning Set of Absent Keyphrase ? topic model K-step Target Assignment ? neural network model ... ? ? ... ... topic vector quantization deep learning Transformer Encoder neural model ... neural model BOS + ne"
2021.acl-long.455,P19-1635,0,0.0200443,"l. (2020b) connects each 5866 number in the problem with nearby nouns to enrich the problem representations. However, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties prediction mechani"
2021.acl-long.455,C10-3014,0,0.00817697,"est set, respectively. We report answer accuracy as the main evaluation metrics of the math word problem solving task. 3.2 Implementation Details In this paper, we truncate the problem to a max sequence length of 150, and the expression to a max sequence length of 50. We select 4,000 words that appear most frequently in the training set of each dataset as the vocabulary, and replace the remaining words with a special token UNK. We initialize the word embedding with the pretrained 300-dimension word vectors3 . The problem encoder used two external knowledge bases: Cilin (Mei, 1985) and Hownet (Dong et al., 2010). The number of heads T in GAT is 8. The hidden size is 512 and the batch size is 64. We use the Adam optimizer (Kingma and Ba, 2014) to optimize the models an the learning rate is 0.001. We compute the final loss function with β1 , β2 , β3 of 0.5. Dropout (Srivastava et al., 2014) is set to 0.5. Models are trained in 80 epoches for the Math23K dataset and 50 epoches for the Ape210K dataset. During testing, the beam size is set to 5. Once all internal nodes in the expression tree have two child nodes, the decoder stops generating the next word. The hyper-parameters are tuned on the valid set."
2021.acl-long.455,2020.findings-emnlp.262,0,0.0207101,"eplace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties prediction mechanism. 5 Conclusion In this study, we proposed a novel approach called NumS2T, that better captures numerical value information and utilizes numerical properties. In this model, we use a digi"
2021.acl-long.455,P18-1039,0,0.0158951,"te-of-the-art models. 1 1 Figure 1: Example of a math word problem. The same problem with different numerical values may correspond to different math expressions. Without numerical value information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate a pre-order traversal"
2021.acl-long.455,P19-1619,0,0.0121074,"h word problem solving tasks (Ling et al., 2017; Wang et al., 2017b, 2018a). To better utilize expression structure information, recent studies have used Seq2Tree models (Liu et al., 2019; Zhang et al., 2020a). Xie and Sun (2019) proposed a tree structured decoder that uses a goal-driven approach to generate expression trees. Wu et al. (2020) proposed a knowledge-aware Seq2Tree model with a state aggregation mechanism that incorporates common-sense knowledge from external knowledge bases. Recently, several methods have attempted to use the contextual information of the numbers in the problem. Li et al. (2019) propose a group attention mechanism to extract quantity-related features and quantitypair features. Zhang et al. (2020b) connects each 5866 number in the problem with nearby nouns to enrich the problem representations. However, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations"
2021.acl-long.455,P17-1015,0,0.107963,"e than existing state-of-the-art models. 1 1 Figure 1: Example of a math word problem. The same problem with different numerical values may correspond to different math expressions. Without numerical value information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate"
2021.acl-long.455,D19-1241,0,0.0365,"Missing"
2021.acl-long.455,P16-1202,0,0.0198685,"ieves better performance than existing state-of-the-art models. 1 1 Figure 1: Example of a math word problem. The same problem with different numerical values may correspond to different math expressions. Without numerical value information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary"
2021.acl-long.455,P19-1329,0,0.0250924,"eatures. Zhang et al. (2020b) connects each 5866 number in the problem with nearby nouns to enrich the problem representations. However, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties"
2021.acl-long.455,D15-1202,0,0.0210073,"e that our model achieves better performance than existing state-of-the-art models. 1 1 Figure 1: Example of a math word problem. The same problem with different numerical values may correspond to different math expressions. Without numerical value information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target ex"
2021.acl-long.455,P18-1196,0,0.0571618,"Missing"
2021.acl-long.455,D19-1534,0,0.018856,"each 5866 number in the problem with nearby nouns to enrich the problem representations. However, these methods rarely take numerical values into consideration. They replace all the numbers in the problems with number symbols and ignore the vital information provided by the numerical values in math word problem solving. As such, these methods will incorrectly generates the same expression for problems with different numerical values. Numerical Value Representations: Some recent studies have explored the numerical value representations in language models (Naik et al., 2019; Chen et al., 2019; Wallace et al., 2019). Spithourakis and Riedel (2018) investigated several of the strategies used for language models for their possible application to model numerals. Gong et al. (2020) proposed the use of contextual numerical value representations to enhance neural content planning by helping models to understand data values. To incorporate numerical value information into math word solving tasks, we use a digit-todigit numerical value encoder to obtain the numberaware problem representations. To further utilize the numerical properties, we propose a numerical properties prediction mechanism. 5 Conclusion In thi"
2021.acl-long.455,D18-1132,0,0.0325612,"Missing"
2021.acl-long.455,P17-1018,0,0.225624,"lue information, the model can hardly determine which expression is correct. Introduction Taking a math word problem as input, the math word problem solving task aims to generate a corresponding solvable expression and answer. With the advancements in natural language processing, math word problem solving has received growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate a pre-order traversal sequence of this expression tree based on the parent and sibling nodes of each node. Although promising results have been achieved, previous methods rarely take numerical values in"
2021.acl-long.455,D17-1088,0,0.0519152,"Missing"
2021.acl-long.455,2020.emnlp-main.579,1,0.866795,"growing attention in recent years (Roy and Roth, 2015; Mitra and Baral, 2016; Ling et al., 2017; Huang et al., 2018). Many methods have been proposed that use sequence-to-sequence (seq2seq) models with an attention mechanism (Bahdanau et al., 2014) for math word problem solving (Wang et al., 2017b, 2018b, 2019). To better utilize expression structure information, some methods use sequenceto-tree (seq2tree) models to generate expressions ∗ Corresponding author. Code is available at qinzhuowu/NumS2T/ 1 https://github.com/ and have achieved promising results (Liu et al., 2019; Xie and Sun, 2019; Wu et al., 2020). These methods convert the target expression into a binary tree, and generate a pre-order traversal sequence of this expression tree based on the parent and sibling nodes of each node. Although promising results have been achieved, previous methods rarely take numerical values into consideration, despite the fact that in math word problem solving, numerical values provide vital information. As an infinite number of numerals can appear in math word problems, it is impossible to list them all in the vocabulary. Previous methods replace all the numbers in the problems with number symbols (e.g.,"
2021.acl-long.455,2020.acl-main.362,0,0.0258773,"nerates the sub-expression “80-52”. However, this problem is about the fares that have already been sold rather than how many tickets are left. With numerical properties, NumS2T is able to realize that 80 is not related to the target expression and should not appear in the generated result. 4 Related Work Math Word Problem Solving: In recent years, Seq2Seq (Sutskever et al., 2014) has been widely used in math word problem solving tasks (Ling et al., 2017; Wang et al., 2017b, 2018a). To better utilize expression structure information, recent studies have used Seq2Tree models (Liu et al., 2019; Zhang et al., 2020a). Xie and Sun (2019) proposed a tree structured decoder that uses a goal-driven approach to generate expression trees. Wu et al. (2020) proposed a knowledge-aware Seq2Tree model with a state aggregation mechanism that incorporates common-sense knowledge from external knowledge bases. Recently, several methods have attempted to use the contextual information of the numbers in the problem. Li et al. (2019) propose a group attention mechanism to extract quantity-related features and quantitypair features. Zhang et al. (2020b) connects each 5866 number in the problem with nearby nouns to enrich"
2021.acl-long.484,P19-1134,0,0.0222896,"Missing"
2021.acl-long.484,N19-1310,0,0.0168086,"Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classification scores on bag labels. Jia et al. (2019) builds an initial training set and further select confident instances based on selected patterns. The difference between the proposed work and previous works is that we do not rely on bag-level labels for sentence selecting. Furthermore, w"
2021.acl-long.484,N19-1184,0,0.0277828,"Missing"
2021.acl-long.484,D19-1031,0,0.0237491,"et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classification scores on bag labels. Jia et al. (2019) builds an initial training set and further select confident instances based on selected patterns. The difference between the proposed work and previous works is that we do not rely on bag-level labels for sentence selecting. Furthermore, we leverage NT to dynamically separate the noisy data"
2021.acl-long.484,D19-1039,0,0.0151829,"018c); Li et al. (2020); Hu et al. (2019a); Ye and Ling (2019); Yuan et al. (2019a); Zhu et al. (2019); Yuan et al. (2019b); Wu et al. (2017)), the selection strategies such as RL or adversarial training to remove noisy sentences from the bag (Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classification scores on bag labels. Jia et al."
2021.acl-long.484,N19-1423,0,0.0209143,"Missing"
2021.acl-long.484,D18-1247,0,0.140547,", 2019; Yuan et al., 2019a,b); 2) the hard de-noise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these bag-level approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentencelevel relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentence-level relation extraction to identify the relation between the answer and the entity in the question. Therefore, several studies (Jia et al. (2019); Feng et al. (2018)) have made efforts on sentence-level (or instance-level) 6201 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6201–6213 August 1–6, 2021. ©2021 Association for Computational Linguistics distant RE, empirically verifying the deficiency of bag-level methods on sentence-level evaluation. However, the instance selection approaches of these methods depend on rewards(Feng et al., 2018) or frequent patterns(Jia et al., 2019) determined by bag-level labels, which contain much nois"
2021.acl-long.484,P11-1055,0,0.0732522,"nt improvement over previous methods in terms of both RE performance and de-noise effect. 1 https://github.com/rtmaww/SENT 2 2.1 Related Work Distant Supervision for RE Supervised relation extraction (RE) has been constrained by the lack of large-scale labeled data. Therefore, distant supervision (DS) is introduced by Mintz et al. (2009), which employs existing knowledge bases (KBs) as source of supervision instead of annotated text. Riedel et al. (2010) relaxes the DS assumption to the express-at-least-once assumption. As a result, multi-instance learning is introduced (Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012)) for this task, where the training and evaluating process are performed in bag-level, with potential noisy sentences existing in each bag. Most following studies in distant RE adopt this paradigm, aiming to decrease the impact of noisy sentences in each bag. These studies include the attention-based methods to attend to useful information ( Lin et al. (2016); Han et al. (2018c); Li et al. (2020); Hu et al. (2019a); Ye and Ling (2019); Yuan et al. (2019a); Zhu et al. (2019); Yuan et al. (2019b); Wu et al. (2017)), the selection strategies such as RL or adversarial train"
2021.acl-long.484,D19-1395,0,0.140457,"lignment between a database and plain text. Such annotation paradigm results in an inevitable noise problem, which is alleviated by previous studies using multiinstance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classified into two categories: 1) the soft de-noise methods that leverage soft weights to differentiate the influence of each sentence (Lin et al., 2016; Han et al., 2018c; Li et al., 2020; Hu et al., 2019a; Ye and Ling, 2019; Yuan et al., 2019a,b); 2) the hard de-noise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these bag-level approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentencelevel relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentence-level relation extraction to identify the relation between the answer and the entity in the question. Therefore, several studies (Jia e"
2021.acl-long.484,C18-1036,0,0.027544,"ase the impact of noisy sentences in each bag. These studies include the attention-based methods to attend to useful information ( Lin et al. (2016); Han et al. (2018c); Li et al. (2020); Hu et al. (2019a); Ye and Ling (2019); Yuan et al. (2019a); Zhu et al. (2019); Yuan et al. (2019b); Wu et al. (2017)), the selection strategies such as RL or adversarial training to remove noisy sentences from the bag (Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Fen"
2021.acl-long.484,P17-1004,0,0.0126913,"tion ( Lin et al. (2016); Han et al. (2018c); Li et al. (2020); Hu et al. (2019a); Ye and Ling (2019); Yuan et al. (2019a); Zhu et al. (2019); Yuan et al. (2019b); Wu et al. (2017)), the selection strategies such as RL or adversarial training to remove noisy sentences from the bag (Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classif"
2021.acl-long.484,P16-1200,0,0.0704369,"s proposed to gather training data through automatic alignment between a database and plain text. Such annotation paradigm results in an inevitable noise problem, which is alleviated by previous studies using multiinstance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classified into two categories: 1) the soft de-noise methods that leverage soft weights to differentiate the influence of each sentence (Lin et al., 2016; Han et al., 2018c; Li et al., 2020; Hu et al., 2019a; Ye and Ling, 2019; Yuan et al., 2019a,b); 2) the hard de-noise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these bag-level approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentencelevel relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentence-level relation extraction to identify the relation between the answer and the enti"
2021.acl-long.484,D17-1189,0,0.0203554,"uan et al. (2019b); Wu et al. (2017)), the selection strategies such as RL or adversarial training to remove noisy sentences from the bag (Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classification scores on bag labels. Jia et al. (2019) builds an initial training set and further select confident instances based on selected patterns"
2021.acl-long.484,P09-1113,0,0.249095,"s noisy information. Furthermore, the model trained with NT is able to separate the noisy data from the training data. Based on NT, we propose a sentence-level framework, SENT, for distant relation extraction. SENT not only filters the noisy data to construct a cleaner dataset, but also performs a relabeling process to transform the noisy data into useful training data, thus further benefiting the model’s performance. Experimental results show the significant improvement of the proposed method over previous methods on sentence-level evaluation and de-noise effect. 1 Which label ? supervision (Mintz et al., 2009) is proposed to gather training data through automatic alignment between a database and plain text. Such annotation paradigm results in an inevitable noise problem, which is alleviated by previous studies using multiinstance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classified into two categories: 1) the soft de-noise methods that leverage soft weights to differentiate the influence of each sentence"
2021.acl-long.484,P18-1199,0,0.226852,"sing multiinstance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classified into two categories: 1) the soft de-noise methods that leverage soft weights to differentiate the influence of each sentence (Lin et al., 2016; Han et al., 2018c; Li et al., 2020; Hu et al., 2019a; Ye and Ling, 2019; Yuan et al., 2019a,b); 2) the hard de-noise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these bag-level approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentencelevel relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentence-level relation extraction to identify the relation between the answer and the entity in the question. Therefore, several studies (Jia et al. (2019); Feng et al. (2018)) have made efforts on sentence-level (or instance-level) 6201 Proceedings of the 59th Annual Meeting of the Associatio"
2021.acl-long.484,N16-1103,0,0.0326703,"Missing"
2021.acl-long.484,C18-1099,0,0.013895,"2016); Han et al. (2018c); Li et al. (2020); Hu et al. (2019a); Ye and Ling (2019); Yuan et al. (2019a); Zhu et al. (2019); Yuan et al. (2019b); Wu et al. (2017)), the selection strategies such as RL or adversarial training to remove noisy sentences from the bag (Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classification scores on ba"
2021.acl-long.484,P10-1013,0,0.0635427,"of noise exist in bag-level labels: 1) Multi-label noise: the exact label (“place of birth” or “employee of”) for each sentence is unclear; 2) Wrong-label noise: the third sentence inside the bag actually expresses “live in” which is not included in the bag labels. Relation extraction (RE), which aims to extract the relation between entity pairs from unstructured text, is a fundamental task in natural language processing. The extracted relation facts can benefit various downstream applications, e.g., knowledge graph completion (Bordes et al., 2013; Wang et al., 2014), information extraction (Wu and Weld, 2010) and question answering (Yao and Van Durme, 2014; Fader et al., 2014). A significant challenge for relation extraction is the lack of large-scale labeled data. Thus, distant Corresponding authors.  Place_of_birth Obama Lived_in (unincluded label) Introduction ∗ ? ? Distant supervision for relation extraction provides uniform bag labels for each sentence inside the bag, while accurate sentence labels are important for downstream applications that need the exact relation type. Directly using bag labels for sentence-level training will introduce much noise, thus severely degrading performance. I"
2021.acl-long.484,D17-1187,0,0.0429042,"Missing"
2021.acl-long.484,P14-1090,0,0.0510755,"Missing"
2021.acl-long.484,N19-1288,0,0.0619595,"database and plain text. Such annotation paradigm results in an inevitable noise problem, which is alleviated by previous studies using multiinstance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classified into two categories: 1) the soft de-noise methods that leverage soft weights to differentiate the influence of each sentence (Lin et al., 2016; Han et al., 2018c; Li et al., 2020; Hu et al., 2019a; Ye and Ling, 2019; Yuan et al., 2019a,b); 2) the hard de-noise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these bag-level approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentencelevel relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentence-level relation extraction to identify the relation between the answer and the entity in the question. Therefore, several studies (Jia et al. (2019); Feng e"
2021.acl-long.484,D15-1203,0,0.463709,"previous studies using multiinstance learning (MIL). In MIL, the training and testing processes are performed at the bag level, where a bag contains noisy sentences mentioning the same entity pair but possibly not describing the same relation. Studies using MIL can be broadly classified into two categories: 1) the soft de-noise methods that leverage soft weights to differentiate the influence of each sentence (Lin et al., 2016; Han et al., 2018c; Li et al., 2020; Hu et al., 2019a; Ye and Ling, 2019; Yuan et al., 2019a,b); 2) the hard de-noise methods that remove noisy sentences from the bag (Zeng et al., 2015; Qin et al., 2018; Han et al., 2018a; Shang, 2019). However, these bag-level approaches fail to map each sentence inside bags with explicit sentence labels. This problem limits the application of RE in some downstream tasks that require sentencelevel relation type, e.g., Yao and Van Durme (2014) and Xu et al. (2016) use sentence-level relation extraction to identify the relation between the answer and the entity in the question. Therefore, several studies (Jia et al. (2019); Feng et al. (2018)) have made efforts on sentence-level (or instance-level) 6201 Proceedings of the 59th Annual Meeting"
2021.acl-long.484,C14-1220,0,0.129393,"Missing"
2021.acl-long.484,N19-1306,0,0.0227406,"Missing"
2021.acl-long.484,Y15-1009,0,0.0673948,"Missing"
2021.acl-long.484,D17-1004,0,0.0603697,"Missing"
2021.acl-long.484,P19-1137,0,0.0120284,"training to remove noisy sentences from the bag (Zeng et al. (2015); Shang (2019); Qin et al. (2018); Han et al. (2018a)) and the incorporation with extra information such as KGs, multi-lingual corpora or other information (Ji et al. (2017); Lei et al. (2018); Vashishth et al. (2018); Han et al. (2018b); Zhang et al. (2019); Qu et al. (2019); Verga et al. (2016); Lin et al. (2017); Wang et al. (2018); Deng and Sun (2019); Beltagy et al. (2019)). Other approaches include soft-label strategy for denoising (Liu et al. (2017)), leveraging pre-trained LM (Alt et al. (2019)), pattern-based method (Zheng et al. (2019)), structured learning method (Bai and Ritter (2019)) and so forth (Luo et al. (2017); Chen et al. (2019)). In this work, we focus on sentence-level relation extraction. Several previous studies also perform Distant RE on sentence-level. Feng et al. (2018) proposes a reinforcement learning framework for sentence selecting, where the reward is given by the classification scores on bag labels. Jia et al. (2019) builds an initial training set and further select confident instances based on selected patterns. The difference between the proposed work and previous works is that we do not rely on bag"
2021.emnlp-main.169,N19-1423,0,0.0103871,"Missing"
2021.emnlp-main.169,2020.emnlp-main.57,0,0.016772,"plies the nonautoregressive mechanism, which can explicitly introduce multiple concepts simultaneously to responses to enhance coherence and diversity. 2.2 Non-Autoregressive Generation Compared with traditional sequential generators that conditions each output word on previously generated outputs, non-autoregressive (non-AR) generation avoids this property to speed up decoding efficiency and has recently attracted much attention (Gu et al., 2018, 2019; Ma et al., 2019; Stern et al., 2019). Another relevant line of research is refinement-based generation (Lee et al., 2018; Kasai et al., 2020; Hua and Wang, 2020; Tan et al., 2021), which gradually improves generation quality by iterative refinement on the draft instead of one-pass generation. For dialogue systems, there 2 Related Work has been prior works that attempt to improve the traditional autoregressive generation. Mou et al. 2.1 Open-Domain Dialogue Generation (2016) explores the way of generating words to Neural seq2seq models (Sutskever et al., 2014) both directions, but it is still in an autoregressive have achieved remarkable success in dialogue manner. Song et al. (2020) introduces a three1 https://github.com/RowitZou/CG-nAR stage refinem"
2021.emnlp-main.169,D18-1149,0,0.0546509,"Missing"
2021.emnlp-main.169,N16-1014,0,0.0342926,"cepts Recently, due to the rapid advancements in nat- into responses in an implicit manner, which cannot ural language generation (NLG) techniques, data- guarantee the appearance of a concept in a response. driven approaches have attracted lots of research Compared with dialogue concepts, a large proporinterest and have achieved impressive progress in tion of chit-chat words are common and usually producing fluent dialogue responses (Shang et al., have a high word frequency and are relatively over2015; Vinyals and Le, 2015; Serban et al., 2016; optimized in language models (Gong et al., 2018; Li et al., 2016). However, such seq2seq models Khassanov et al., 2019). Consequently, conventend to degenerate generic or off-topic responses tional seq2seq generators are more &quot;familiar&quot; with (Tang et al., 2019; Welleck et al., 2020). An these generic words than those requiring concept effective way to address this issue is to leverage management, which prevents introducing certain external knowledge (Zhou et al., 2018a,b) or concepts to the response with sequential decoding topic information (Xing et al., 2017), which are (either greedily or with beam search) (Mou et al., integrated as additional semantic r"
2021.emnlp-main.169,W04-1013,0,0.0708432,"Missing"
2021.emnlp-main.169,D19-1437,0,0.017355,"le concepts. By contrast, we focus on how to effectively integrate multiple extracted concepts into dialogue responses. The proposed CG-nAR applies the nonautoregressive mechanism, which can explicitly introduce multiple concepts simultaneously to responses to enhance coherence and diversity. 2.2 Non-Autoregressive Generation Compared with traditional sequential generators that conditions each output word on previously generated outputs, non-autoregressive (non-AR) generation avoids this property to speed up decoding efficiency and has recently attracted much attention (Gu et al., 2018, 2019; Ma et al., 2019; Stern et al., 2019). Another relevant line of research is refinement-based generation (Lee et al., 2018; Kasai et al., 2020; Hua and Wang, 2020; Tan et al., 2021), which gradually improves generation quality by iterative refinement on the draft instead of one-pass generation. For dialogue systems, there 2 Related Work has been prior works that attempt to improve the traditional autoregressive generation. Mou et al. 2.1 Open-Domain Dialogue Generation (2016) explores the way of generating words to Neural seq2seq models (Sutskever et al., 2014) both directions, but it is still in an autoregres"
2021.emnlp-main.169,P19-1081,0,0.0224288,"quality and decoding efficiency. systems (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Xing et al., 2017), but they prefer to produce generic and off-topic responses (Tang et al., 2019; Welleck et al., 2020). Dozens of works have attempted to incorporate external knowledge into dialogue systems to improve informativeness and diversity (Zhou et al., 2018a; Zhang et al., 2018; Dinan et al., 2019; Ren et al., 2020). Beyond the progress on response quality, a couple of works focus on goal planning or concept transition for a controllable and coherent dialogue (Yao et al., 2018; Moon et al., 2019; Wu et al., 2019; Xu et al., 2020a,b; Wu et al., 2020; Zhang et al., 2020). Most of these works mainly explore how to effectively leverage external knowledge graphs and extract concepts from them. Nevertheless, they generally introduce concepts into the response implicitly with gated controlling or copy mechanism, which cannot ensure the success of concept integration because seq2seq models prefer generic words. Some works (Mou et al., 2016; Xu et al., 2020a) try to produce concept words first and generate the remaining words to both directions to complete a response, but they cannot handle t"
2021.emnlp-main.169,C16-1316,0,0.175629,"Although promising results have been obtained information, e.g., action, entity, and emotion (see by equipping dialogue models with external knowl- Figure 1). Unfortunately, most existing methods 2215 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2215–2226 c November 7–11, 2021. 2021 Association for Computational Linguistics can only retrieve one concept for each utterance (Tang et al., 2019; Qin et al., 2020). Another line of approaches attempt to explicitly integrate concepts into responses and generate the remaining words in both directions (Mou et al., 2016; Xu et al., 2020a), but they also fail to deal with multiple concepts. In this paper, we devise a concept-guided nonautoregressive model (CG-nAR) to facilitate dialogue coherence by explicitly introducing multiple concepts into dialogue responses. Specifically, following Xu et al. (2020a), a concept graph is constructed based on the dialogue data, where the vertices represent concepts, and edges represent concept transitions between utterances. Based on the concept graph, we introduce a novel multiconcept planning module that learns to manage concept transitions in a dialogue flow. It recurre"
2021.emnlp-main.169,P02-1040,0,0.109419,"8. At inference time, the multiconcept extractor produces concepts greedily, and the maximum number of allowed concepts Nmax was set to 5. For the Insertion Transformer, we used the configuration that achieved the best results reported in Stern et al. (2019). The whole model was trained for 100,000 steps with 8,000 warm-up steps on a 3090 GPU. Checkpoints were saved and evaluated on the validation set every 2,000 steps. Checkpoints with the top performance were finally evaluated on the test set to report final results. 5 5.1 Results and Analysis Automatic Evaluation We adopt widely used BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) to measure the relevance between the generation and the ground-truth. We report averaged BLEU scores with 4-grams at most 4 The original CCM uses an external knowledge graph. and ROUGE-1/L (RG-1/L) F-scores. To measure Here we adapt it to our constructed concept graph for a fair comparison. The same strategy is applied to ConceptFlow. the diversity of generated responses, we report 2220 Model Persona-Chat CG-nAR vs. ReCoSa CG-nAR vs. Seq2BF CG-nAR vs. CCM CG-nAR vs. ConceptFlow Weibo CG-nAR vs. ReCoSa CG-nAR vs. Seq2BF CG-nAR vs. CCM CG-nAR vs. ConceptFlow App. Inf. 72.5"
2021.emnlp-main.169,P15-1152,0,0.0753454,"Missing"
2021.emnlp-main.169,2020.acl-main.516,0,0.0197974,"s refinement-based generation (Lee et al., 2018; Kasai et al., 2020; Hua and Wang, 2020; Tan et al., 2021), which gradually improves generation quality by iterative refinement on the draft instead of one-pass generation. For dialogue systems, there 2 Related Work has been prior works that attempt to improve the traditional autoregressive generation. Mou et al. 2.1 Open-Domain Dialogue Generation (2016) explores the way of generating words to Neural seq2seq models (Sutskever et al., 2014) both directions, but it is still in an autoregressive have achieved remarkable success in dialogue manner. Song et al. (2020) introduces a three1 https://github.com/RowitZou/CG-nAR stage refinement strategy for improving persona 2216 I have little time for hobbies due to my work . <end> hospital doctor Do you like playing games for relaxing ? Step 3: Graph Attention <bos> I&apos;m a doctor working in a hospital <eos> Insertion Transformer You are always busy . What&apos;s your job ? Step 2: doctor Concept Flow Encoder relax work busy job Hierarchical Dialogue Encoder job Insertion Transformer Concept Graph Step 1: <bos> doctor hospital <eos> doctor Query <start> doctor Initialize hospital Concept Decoder You are always busy ."
2021.emnlp-main.169,2021.naacl-main.341,0,0.0193117,"ressive mechanism, which can explicitly introduce multiple concepts simultaneously to responses to enhance coherence and diversity. 2.2 Non-Autoregressive Generation Compared with traditional sequential generators that conditions each output word on previously generated outputs, non-autoregressive (non-AR) generation avoids this property to speed up decoding efficiency and has recently attracted much attention (Gu et al., 2018, 2019; Ma et al., 2019; Stern et al., 2019). Another relevant line of research is refinement-based generation (Lee et al., 2018; Kasai et al., 2020; Hua and Wang, 2020; Tan et al., 2021), which gradually improves generation quality by iterative refinement on the draft instead of one-pass generation. For dialogue systems, there 2 Related Work has been prior works that attempt to improve the traditional autoregressive generation. Mou et al. 2.1 Open-Domain Dialogue Generation (2016) explores the way of generating words to Neural seq2seq models (Sutskever et al., 2014) both directions, but it is still in an autoregressive have achieved remarkable success in dialogue manner. Song et al. (2020) introduces a three1 https://github.com/RowitZou/CG-nAR stage refinement strategy for im"
2021.emnlp-main.169,P19-1369,0,0.113825,"lar dialogue with concept transitions, where each utterance is composed of multiple associated concepts to convey diverse information. edge, the development of dialogue discourse still has its own challenge: human dialogue generally evolves around a number of concepts that might frequently shift in a dialogue flow (Zhang et al., 2020). The lack of concept management strategies might lead to incoherent dialogue due to the loosely connected concepts. To address this problem, recent studies have combined concept planning with response generation to form a more coherent and controllable dialogue (Wu et al., 2019; Xu et al., 2020a,b; Wu et al., 2020; Zhang et al., 2020). Creating a &quot;human-like&quot; dialogue system is one of the important goals of artificial intelligence. Most of these approaches incorporate concepts Recently, due to the rapid advancements in nat- into responses in an implicit manner, which cannot ural language generation (NLG) techniques, data- guarantee the appearance of a concept in a response. driven approaches have attracted lots of research Compared with dialogue concepts, a large proporinterest and have achieved impressive progress in tion of chit-chat words are common and usually p"
2021.emnlp-main.169,2020.acl-main.166,0,0.462664,"concept transitions, where each utterance is composed of multiple associated concepts to convey diverse information. edge, the development of dialogue discourse still has its own challenge: human dialogue generally evolves around a number of concepts that might frequently shift in a dialogue flow (Zhang et al., 2020). The lack of concept management strategies might lead to incoherent dialogue due to the loosely connected concepts. To address this problem, recent studies have combined concept planning with response generation to form a more coherent and controllable dialogue (Wu et al., 2019; Xu et al., 2020a,b; Wu et al., 2020; Zhang et al., 2020). Creating a &quot;human-like&quot; dialogue system is one of the important goals of artificial intelligence. Most of these approaches incorporate concepts Recently, due to the rapid advancements in nat- into responses in an implicit manner, which cannot ural language generation (NLG) techniques, data- guarantee the appearance of a concept in a response. driven approaches have attracted lots of research Compared with dialogue concepts, a large proporinterest and have achieved impressive progress in tion of chit-chat words are common and usually producing fluent d"
2021.emnlp-main.169,P19-1362,0,0.0227618,"the two dialogue datasets along with the constructed graphs is shown in Table 1. 4.2 Comparison Methods We compare CG-nAR with two groups of baselines: general seq2seq models and concept-guided systems. General seq2seq models produce responses conditioned on the dialogue messages without concept planning, including: Seq2seq+Att (Sutskever et al., 2014), a standard RNN model with attention mechanism; Transformer (Vaswani et al., 2017), a seq2seq model with a multi-head attention mechanism; HRED (Serban et al., 2016), a hierarchical encoder-decoder framework to model context utterances; ReCoSa (Zhang et al., 2019), a state-of-the-art model using the self-attention mechanism to measure the relevance of response and context. Concept-guided dialogue systems leverage concept information to control response generation, including: Seq2BF (Mou et al., 2016), a non-left-to-right generation model that explicitly incorporates a keyword into the response; CCM (Zhou et al., 2018a), a model that uses the graph attention mechanism to choose graph entities4 , and introduces them into response implicitly by a copy mechanism; ConceptFlow (Zhang et al., 2020), a state-of-the-art model that grounds each dialogue in the c"
2021.emnlp-main.169,2020.acl-main.184,0,0.490979,"evaluations with substantially faster inference speed. 1 Introduction What are your hobbies ? I like going shopping and watching tv . Same. I like to sit on my couch and watch anime . That’s cool. I&apos;m a big fan of japanese anime. I really like hearing its music . Figure 1: An exemplar dialogue with concept transitions, where each utterance is composed of multiple associated concepts to convey diverse information. edge, the development of dialogue discourse still has its own challenge: human dialogue generally evolves around a number of concepts that might frequently shift in a dialogue flow (Zhang et al., 2020). The lack of concept management strategies might lead to incoherent dialogue due to the loosely connected concepts. To address this problem, recent studies have combined concept planning with response generation to form a more coherent and controllable dialogue (Wu et al., 2019; Xu et al., 2020a,b; Wu et al., 2020; Zhang et al., 2020). Creating a &quot;human-like&quot; dialogue system is one of the important goals of artificial intelligence. Most of these approaches incorporate concepts Recently, due to the rapid advancements in nat- into responses in an implicit manner, which cannot ural language gene"
2021.emnlp-main.169,P18-1205,0,0.0635518,"Missing"
2021.emnlp-main.169,D18-1076,0,0.176313,"cing fluent dialogue responses (Shang et al., have a high word frequency and are relatively over2015; Vinyals and Le, 2015; Serban et al., 2016; optimized in language models (Gong et al., 2018; Li et al., 2016). However, such seq2seq models Khassanov et al., 2019). Consequently, conventend to degenerate generic or off-topic responses tional seq2seq generators are more &quot;familiar&quot; with (Tang et al., 2019; Welleck et al., 2020). An these generic words than those requiring concept effective way to address this issue is to leverage management, which prevents introducing certain external knowledge (Zhou et al., 2018a,b) or concepts to the response with sequential decoding topic information (Xing et al., 2017), which are (either greedily or with beam search) (Mou et al., integrated as additional semantic representations to 2016). Moreover, speakers naturally associate improve dialogue informativeness. multiple concepts to proactively convey diverse Although promising results have been obtained information, e.g., action, entity, and emotion (see by equipping dialogue models with external knowl- Figure 1). Unfortunately, most existing methods 2215 Proceedings of the 2021 Conference on Empirical Methods in N"
2021.emnlp-main.17,P14-1038,0,0.223441,"ns. Conventionally, Named Entity Recognition (NER) and Relation Extraction (RE) are performed in a pipelined manner (Zelenko et al., 2002; Chan and Roth, 2011). These approaches are flawed in that they do not consider the intimate connection between NER and RE. Also, error propagation is another drawback of pipeline methods. In order to conquer these issues, joint extracting entity and relation is proposed and demonstrates stronger performance on both tasks. In early work, joint methods mainly rely on elaborate feature engineering to establish interaction between NER and RE (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014). Recently, end-to-end neural network has shown to be successful in extracting relational triples (Zeng et al., 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Shen et al., 2021) and has since become the mainstream of joint 1 Introduction entity and relation extraction. According to their differences in encoding taskJoint entity and relation extraction intend to specific features, most of the existing methods simultaneously extract entity and relation facts can be divided into two categories: sequential in the given text to form relational triples as encoding and pa"
2021.emnlp-main.17,P19-1129,0,0.0348847,"Missing"
2021.emnlp-main.17,D18-1360,0,0.0536971,"Missing"
2021.emnlp-main.17,P16-1105,0,0.050881,"Missing"
2021.emnlp-main.17,D14-1200,0,0.170678,"y, Named Entity Recognition (NER) and Relation Extraction (RE) are performed in a pipelined manner (Zelenko et al., 2002; Chan and Roth, 2011). These approaches are flawed in that they do not consider the intimate connection between NER and RE. Also, error propagation is another drawback of pipeline methods. In order to conquer these issues, joint extracting entity and relation is proposed and demonstrates stronger performance on both tasks. In early work, joint methods mainly rely on elaborate feature engineering to establish interaction between NER and RE (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014). Recently, end-to-end neural network has shown to be successful in extracting relational triples (Zeng et al., 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Shen et al., 2021) and has since become the mainstream of joint 1 Introduction entity and relation extraction. According to their differences in encoding taskJoint entity and relation extraction intend to specific features, most of the existing methods simultaneously extract entity and relation facts can be divided into two categories: sequential in the given text to form relational triples as encoding and parallel encoding. In sequ"
2021.emnlp-main.17,N13-1008,0,0.0406422,"of joint 1 Introduction entity and relation extraction. According to their differences in encoding taskJoint entity and relation extraction intend to specific features, most of the existing methods simultaneously extract entity and relation facts can be divided into two categories: sequential in the given text to form relational triples as encoding and parallel encoding. In sequential (s, r, o). The extracted information provides a encoding, task-specific features are generated supplement to many studies, such as knowledge sequentially, which means features extracted first graph construction (Riedel et al., 2013), question are not affected by those that are extracted later. ∗ Corresponding author. Zeng et al. (2018) and Wei et al. (2020) are 185 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 185–197 c November 7–11, 2021. 2021 Association for Computational Linguistics typical examples of this category. Their methods 2. We conduct extensive experiments on six extract features for different tasks in a predefined datasets. The main results show that our order. In parallel encoding, task-specific features method is superior to other baseline apare generated i"
2021.emnlp-main.17,2021.acl-demo.41,1,0.598002,"Ori → Aug Decline 82.9→64.2 18.7 87.4→79.0 8.4 88.7→74.6 14.1 84.7→51.5 33.2 84.6→81.3 3.3 89.0→80.4 8.6 SwapLonger Average Ori → Aug Decline Decline 82.9→67.7 15.2 16.6 87.4→82.1 5.3 11.6 88.7→78.5 10.2 14.6 84.7→31.1 53.6 28.1 84.6→73.1 11.5 11.9 89.0→84.3 4.7 5.1 Table 4: Robustness test of NER against input perturbation in ACE05, baseline results and test files are copied from https://www.textflint.io/ 6.2 Robustness Test on Named Entity Recognition We use robustness test to evaluate our model under adverse circumstances. In this case, we use the domain transformation methods of NER from (Wang et al., 2021). The compared baselines are all relation-free models, including BiLSTMCRF (Huang et al., 2015), BERT (Devlin et al., 2019), TENER (Yan et al., 2019) and FlairEmbeddings (Akbik et al., 2019). Descriptions of the transformation methods can be found in Appendix D From table 4, we observe that our model is mostly more resilient against input perturbations compared to other baselines, especially in the category of CrossCategory, which is probably attributed to the fact that relation signals used in our training impose type constraints on entities, thus inference of entity types is less affected by"
2021.emnlp-main.17,2020.emnlp-main.132,0,0.156794,"nd Sasaki, 2014) to handle each relation separately. specific ones, are formed through concerted efforts Task interaction modeling, however, has not of entity and relation gates, allowing for interaction been well handled by most of the previous between the formation of entity and relation features determined by these partitions. Second, work. In some of the previous approaches, Task interaction is achieved with entity and relation the shared partition, which represents information prediction sharing the same features (Tran and useful to both task, is equally accessible to the Kavuluru, 2019; Wang et al., 2020b). This could formation of both task-specific features, ensuring be problematic as information about entity and balanced two-way interaction. The contributions relation could sometimes be contradictory. Also, of our work are summarized below: 1. We propose partition filter network, a frame- as models that use sequential encoding (Bekoulis work designed specifically for joint encoding. et al., 2018b; Eberts and Ulges, 2019; Wei et al., This method is capable of encoding task- 2020) or parallel encoding (Fu et al., 2019) lack specific features and guarantees proper two- proper two-way interacti"
2021.emnlp-main.17,2020.coling-main.138,0,0.0424825,"Missing"
2021.emnlp-main.17,2020.acl-main.136,0,0.494739,"xtraction intend to specific features, most of the existing methods simultaneously extract entity and relation facts can be divided into two categories: sequential in the given text to form relational triples as encoding and parallel encoding. In sequential (s, r, o). The extracted information provides a encoding, task-specific features are generated supplement to many studies, such as knowledge sequentially, which means features extracted first graph construction (Riedel et al., 2013), question are not affected by those that are extracted later. ∗ Corresponding author. Zeng et al. (2018) and Wei et al. (2020) are 185 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 185–197 c November 7–11, 2021. 2021 Association for Computational Linguistics typical examples of this category. Their methods 2. We conduct extensive experiments on six extract features for different tasks in a predefined datasets. The main results show that our order. In parallel encoding, task-specific features method is superior to other baseline apare generated independently using shared input. proaches, and the ablation study provides inCompared with sequential encoding, models build si"
2021.emnlp-main.17,C10-2160,0,0.0484055,"two task partitions. Conventionally, Named Entity Recognition (NER) and Relation Extraction (RE) are performed in a pipelined manner (Zelenko et al., 2002; Chan and Roth, 2011). These approaches are flawed in that they do not consider the intimate connection between NER and RE. Also, error propagation is another drawback of pipeline methods. In order to conquer these issues, joint extracting entity and relation is proposed and demonstrates stronger performance on both tasks. In early work, joint methods mainly rely on elaborate feature engineering to establish interaction between NER and RE (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014). Recently, end-to-end neural network has shown to be successful in extracting relational triples (Zeng et al., 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Shen et al., 2021) and has since become the mainstream of joint 1 Introduction entity and relation extraction. According to their differences in encoding taskJoint entity and relation extraction intend to specific features, most of the existing methods simultaneously extract entity and relation facts can be divided into two categories: sequential in the given text to form relational triples a"
2021.emnlp-main.17,W02-1010,0,0.0646134,"entity prediction in a non-negligible way. The source code can be found at https://github.com/ Coopercoppers/PFN. answering (Diefenbach et al., 2018) and text summarization (Gupta and Lehal, 2010). ✂ Abstract NER-Specific Cell State NER RE Partition Figure 1: Partition process of cell neurons. Entity and relation gate are used to divide neurons into taskrelated and task-unrelated ones. Neurons relating to both tasks form the shared partition while the rest form two task partitions. Conventionally, Named Entity Recognition (NER) and Relation Extraction (RE) are performed in a pipelined manner (Zelenko et al., 2002; Chan and Roth, 2011). These approaches are flawed in that they do not consider the intimate connection between NER and RE. Also, error propagation is another drawback of pipeline methods. In order to conquer these issues, joint extracting entity and relation is proposed and demonstrates stronger performance on both tasks. In early work, joint methods mainly rely on elaborate feature engineering to establish interaction between NER and RE (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014). Recently, end-to-end neural network has shown to be successful in extracting relational triples"
2021.emnlp-main.17,C14-1220,0,0.0530811,"Chan and Roth, 2011). These approaches are flawed in that they do not consider the intimate connection between NER and RE. Also, error propagation is another drawback of pipeline methods. In order to conquer these issues, joint extracting entity and relation is proposed and demonstrates stronger performance on both tasks. In early work, joint methods mainly rely on elaborate feature engineering to establish interaction between NER and RE (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014). Recently, end-to-end neural network has shown to be successful in extracting relational triples (Zeng et al., 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Shen et al., 2021) and has since become the mainstream of joint 1 Introduction entity and relation extraction. According to their differences in encoding taskJoint entity and relation extraction intend to specific features, most of the existing methods simultaneously extract entity and relation facts can be divided into two categories: sequential in the given text to form relational triples as encoding and parallel encoding. In sequential (s, r, o). The extracted information provides a encoding, task-specific features are generated supplement to"
2021.emnlp-main.201,P17-1171,0,0.0518809,"Missing"
2021.emnlp-main.201,2020.acl-main.69,0,0.0481323,"Ss2s (Kim et al., 2019): proposes an answer separated Seq2Seq model by replacing the answer in the input sequence with some specific words. To the Point Context (Li et al., 2019): extracts answer-relevant relations in the sentence and encodes both sentence and relations to capture answer-focused representations. QG-pg (Jia et al., 2020): leverages the paraphrase information to the QG model. Graph2seq +RL+ BERT (Chen et al., 2020): is a BERT enhanced Graph2seq QG model with reinforcement learning. QQP & QAP with BERT (Zhang and Bansal, 2019): combines the QG task and QA task with BERT. Syn-QG (Dhole and Manning, 2020): is a rulebased QG model that uses the PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content. It is a SOTA model in sentence-level QG task. Recurrent BERT (Chan and Fan, 2020): employs the pre-trained BERT language model to tackle question generation tasks. It is also a SOTA model in sentence-level QG task. We evaluate the performance of our models using BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004), which are widely used in previous QG works. 3.3 Implementation Details We fix the 300-dim GloVe vectors for the most frequent 70,000 words in"
2021.emnlp-main.201,P17-1123,0,0.0162922,"ucture man and Smith, 2010) focused on the rule-based information and copied words in generated words approaches that rely on heuristic rules or hand- at each decoding step. In addition, we proposed the crafted templates, with low generalizability and relational graph encoder to capture the dependency scalability. Recent works adopted the attention- relations information to improve the performance. based sequence-to-sequence neural model for QG For the sentence-level QG task on SQuAD and tasks, taking answer sentence as input and output MARCO dataset, our method outperforms existthe question (Du et al., 2017), which proved to be ing methods by a significant margin and achieves better than rule-based methods. (Zhou et al., 2018) the new state-of-the-art results. Future directions proposed the feature-enriched encoder to encode include investigating more effective ways of utilizthe input sentence by concatenating word embed- ing previous generation information and exploitding with lexical features as the encoder input, and ing Graph2Seq models with GNN-based decoder answer position are to inform the model to locate for question generation from structured data like the answer. To generate a question"
2021.emnlp-main.201,N10-1086,0,0.0240664,"wer to a passage lies at the heart of this task. tive sentence, QG has many useful applications: (1) (Song et al., 2018) leveraged multi-perspective it improves the question answering task (Chen et al., matching methods and (Sun et al., 2018) proposed 2017) by providing more training data (Tang et al., a position-aware model to put more emphasis on answer-surrounded context words. (Zhao et al., 2017; Yuan et al., 2017); (2) it generates practice 2018) aggregated the paragraph-level context to exercises and assessments for educational purposes provide sufficient information for question genera(Heilman and Smith, 2010); and (3) it helps dialog tion. (Chen et al., 2020; Liu et al., 2019) employed systems to kick-start and continue a conversation the Graph2Seq architecture to capture the informawith human users (Mostafazadeh et al., 2016). In tion in a passage. this study, we focus on sentence-level QG tasks. Conventional QG methods (Mostow and Chen, Most models with state-of-the-art performance 2009; Heilman and Smith, 2010; Dhole and Man- model the previously generated text at each dening, 2020) rely on heuristic rules or hand-crafted coding step. However, they ignore (1) the rich templates as it suffers a"
2021.emnlp-main.201,2020.acl-main.545,0,0.0795905,"pectively. Answer-focused Position-aware model (Sun et al., 2018): generates an accurate interrogative word and focuses on important context words. s2sa-at-mp-gsa (Zhao et al., 2018): employs a gated attention encoder and a maxout pointer decoder to deal with long text inputs. ASs2s (Kim et al., 2019): proposes an answer separated Seq2Seq model by replacing the answer in the input sequence with some specific words. To the Point Context (Li et al., 2019): extracts answer-relevant relations in the sentence and encodes both sentence and relations to capture answer-focused representations. QG-pg (Jia et al., 2020): leverages the paraphrase information to the QG model. Graph2seq +RL+ BERT (Chen et al., 2020): is a BERT enhanced Graph2seq QG model with reinforcement learning. QQP & QAP with BERT (Zhang and Bansal, 2019): combines the QG task and QA task with BERT. Syn-QG (Dhole and Manning, 2020): is a rulebased QG model that uses the PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content. It is a SOTA model in sentence-level QG task. Recurrent BERT (Chan and Fan, 2020): employs the pre-trained BERT language model to tackle question generation tasks. It is als"
2021.emnlp-main.201,D19-1317,0,0.0499325,"in the passages to construct sentence-level dataset. That contains 4,6109, 4539 and 4539 sentence-question-answer triples for training, validation and test respectively. Answer-focused Position-aware model (Sun et al., 2018): generates an accurate interrogative word and focuses on important context words. s2sa-at-mp-gsa (Zhao et al., 2018): employs a gated attention encoder and a maxout pointer decoder to deal with long text inputs. ASs2s (Kim et al., 2019): proposes an answer separated Seq2Seq model by replacing the answer in the input sequence with some specific words. To the Point Context (Li et al., 2019): extracts answer-relevant relations in the sentence and encodes both sentence and relations to capture answer-focused representations. QG-pg (Jia et al., 2020): leverages the paraphrase information to the QG model. Graph2seq +RL+ BERT (Chen et al., 2020): is a BERT enhanced Graph2seq QG model with reinforcement learning. QQP & QAP with BERT (Zhang and Bansal, 2019): combines the QG task and QA task with BERT. Syn-QG (Dhole and Manning, 2020): is a rulebased QG model that uses the PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content. It is a SOTA"
2021.emnlp-main.201,P16-1170,0,0.021628,"and (Sun et al., 2018) proposed 2017) by providing more training data (Tang et al., a position-aware model to put more emphasis on answer-surrounded context words. (Zhao et al., 2017; Yuan et al., 2017); (2) it generates practice 2018) aggregated the paragraph-level context to exercises and assessments for educational purposes provide sufficient information for question genera(Heilman and Smith, 2010); and (3) it helps dialog tion. (Chen et al., 2020; Liu et al., 2019) employed systems to kick-start and continue a conversation the Graph2Seq architecture to capture the informawith human users (Mostafazadeh et al., 2016). In tion in a passage. this study, we focus on sentence-level QG tasks. Conventional QG methods (Mostow and Chen, Most models with state-of-the-art performance 2009; Heilman and Smith, 2010; Dhole and Man- model the previously generated text at each dening, 2020) rely on heuristic rules or hand-crafted coding step. However, they ignore (1) the rich templates as it suffers a significant lack of ques- structure information hidden in generated words tion, sticking to a few simple and reliable syntactic (2) and the impact of copied words on the passage. transformation patterns. Recently, neural-b"
2021.emnlp-main.201,P02-1040,0,0.110059,"is a BERT enhanced Graph2seq QG model with reinforcement learning. QQP & QAP with BERT (Zhang and Bansal, 2019): combines the QG task and QA task with BERT. Syn-QG (Dhole and Manning, 2020): is a rulebased QG model that uses the PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content. It is a SOTA model in sentence-level QG task. Recurrent BERT (Chan and Fan, 2020): employs the pre-trained BERT language model to tackle question generation tasks. It is also a SOTA model in sentence-level QG task. We evaluate the performance of our models using BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004), which are widely used in previous QG works. 3.3 Implementation Details We fix the 300-dim GloVe vectors for the most frequent 70,000 words in the training set. We compute the 1024-dim BERT embeddings on the fly for each word in text using a trainable weighted sum of all BERT layer outputs. The embedding 3.2 Baseline Methods and Metrics sizes of the case, answer, copy, POS , and NER For fair comparison, we report the following recent tags are set of 3, 3, 3, 12 and 8, respectively. We set works on sentence-level QG dataset: the hidden state size of BiLSTM to 150 so tha"
2021.emnlp-main.201,2020.findings-emnlp.217,0,0.0122507,"the question can be answered by the given answer. The rating score is set to [0, 5]. The evaluation results are shown in Table 3. Our model receives higher scores on all three metrics, indicating that our generated questions have higher quality in different aspects. Models Fluency Relavancy Answerability Table 1 shows the experimental results of the baseline 3.81 3.69 3.74 SQuAD sentence-level dateset. For fair compariour model 4.24 4.33 4.26 son, we report the results on sentence-level dataset ground-truth 4.89 4.38 4.75 excludes paragraph-level results (Dong et al., 2019; Bao et al., 2020; Qi et al., 2020). Table 3: Human evaluation results. In terms of BLEU-4 regarded as the main evaluation metric for text generation, our model yields the 4.3 Ablation Study best results, with 20.33. We achieve state-of-the-art results on SQuAD for sentence-level QG. As shown in Table 4, we perform an ablation study to systematically assess the impact of different We perform experiments on MARCO and achieve the state-of-the-art results as shown in Ta- model components (BERT, relational embedding, IGND) on the SQuAD test-set. ble 2. SQuAD and MARCO are built in different ways. The questions in SQuAD are generate"
2021.emnlp-main.201,D16-1264,0,0.0387479,"network to produce the vocabulary distriresent the answer word contain the answer tag at bution Pvocab . 2576 G The final probability distribution is the combination of the two modes: P (w) = pg Pvocab + (1 − pg )Pcopy (27) where pg is computed from the context vector ct , decoder hidden states st and the decoder input wt : pg = σ(Wg (ct + st + wt )) (28) where Wg is a trainable weighted matrix and σ is a sigmoid function. We train our model by the negative log likelihood for the target sequence y: T 1X L= logP (˜ yt = yt ) T (29) t=1 3 3.1 Experimental Settings Dataset 3.1.1 SQuAD The SQuAD (Rajpurkar et al., 2016) dataset contains 536 Wikipedia articles and more than 100K questions from the articles of crowd-workers. Answers are provided to the questions, which are spans of tokens in the articles. We use the sentencelevel data shared by (Zhou et al., 2018) 2 and there are 86,635, 8,965 and 8,964 triples correspondingly. 3.1.2 MARCO MS MARCO datasets (Nguyen et al., 2016) 3 contains 100,000 queries with corresponding answers and passages. All questions are sampled from real anonymized user queries and context passages are extracted from real web documents. We picked a subset of MS MARCO data where answe"
2021.emnlp-main.201,N18-2090,0,0.210569,"ords are in blue and the copied words are in purple. The model can copy the right word develop with a high certainty (0.97 score). datasets and employing the encoder-decoder framework. Most of the existing works are based on the sequence-to-sequence (Seq2Seq) network, incorpo1 Introduction rating the attention mechanism and copy mode, applied by (Zhou et al., 2018). Intuitively, connecting Automatic Question Generation (QG) is the task of generating question-answer pairs from a declara- an answer to a passage lies at the heart of this task. tive sentence, QG has many useful applications: (1) (Song et al., 2018) leveraged multi-perspective it improves the question answering task (Chen et al., matching methods and (Sun et al., 2018) proposed 2017) by providing more training data (Tang et al., a position-aware model to put more emphasis on answer-surrounded context words. (Zhao et al., 2017; Yuan et al., 2017); (2) it generates practice 2018) aggregated the paragraph-level context to exercises and assessments for educational purposes provide sufficient information for question genera(Heilman and Smith, 2010); and (3) it helps dialog tion. (Chen et al., 2020; Liu et al., 2019) employed systems to kick-s"
2021.emnlp-main.201,D18-1427,0,0.189033,"score). datasets and employing the encoder-decoder framework. Most of the existing works are based on the sequence-to-sequence (Seq2Seq) network, incorpo1 Introduction rating the attention mechanism and copy mode, applied by (Zhou et al., 2018). Intuitively, connecting Automatic Question Generation (QG) is the task of generating question-answer pairs from a declara- an answer to a passage lies at the heart of this task. tive sentence, QG has many useful applications: (1) (Song et al., 2018) leveraged multi-perspective it improves the question answering task (Chen et al., matching methods and (Sun et al., 2018) proposed 2017) by providing more training data (Tang et al., a position-aware model to put more emphasis on answer-surrounded context words. (Zhao et al., 2017; Yuan et al., 2017); (2) it generates practice 2018) aggregated the paragraph-level context to exercises and assessments for educational purposes provide sufficient information for question genera(Heilman and Smith, 2010); and (3) it helps dialog tion. (Chen et al., 2020; Liu et al., 2019) employed systems to kick-start and continue a conversation the Graph2Seq architecture to capture the informawith human users (Mostafazadeh et al., 2"
2021.emnlp-main.201,D17-1090,0,0.0406796,"Missing"
2021.emnlp-main.201,D19-1253,0,0.0630535,"tention encoder and a maxout pointer decoder to deal with long text inputs. ASs2s (Kim et al., 2019): proposes an answer separated Seq2Seq model by replacing the answer in the input sequence with some specific words. To the Point Context (Li et al., 2019): extracts answer-relevant relations in the sentence and encodes both sentence and relations to capture answer-focused representations. QG-pg (Jia et al., 2020): leverages the paraphrase information to the QG model. Graph2seq +RL+ BERT (Chen et al., 2020): is a BERT enhanced Graph2seq QG model with reinforcement learning. QQP & QAP with BERT (Zhang and Bansal, 2019): combines the QG task and QA task with BERT. Syn-QG (Dhole and Manning, 2020): is a rulebased QG model that uses the PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content. It is a SOTA model in sentence-level QG task. Recurrent BERT (Chan and Fan, 2020): employs the pre-trained BERT language model to tackle question generation tasks. It is also a SOTA model in sentence-level QG task. We evaluate the performance of our models using BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004), which are widely used in previous QG works. 3.3 Implementation D"
2021.emnlp-main.201,D18-1424,0,0.229912,"}, and a target answer is a sequence of word tokens X a = {xa1 , xa2 , ..., xaL }. The natural question generation task generates the best natural language question consisting of a sequence of word tokens Yˆ = {y1 , y2 , ..., yT } and maximizing the conditional likelihood arg maxY P (Y |X p , X a ). Here N, L, and T are the lengths of the passage, the answer, and the question, respectively. We focus on the problem set based on a set of passages, answers, and questions triples. We learn the connection between them. Existing QG approaches (Zhou et al., 2018; Sun et al., 2018; Song et al., 2018; Zhao et al., 2018; Chen et al., 2020) have the same assumption. 2.2 Graph2Seq Model with Iterative Graph Network-based Decoder Compared to RNNs, GNNs can efficiently use the rich hidden text structure information such as syntactic information. In addition, they can model the global relations among the sequence words to improve the representations. We construct a directed • We design an Iterative Graph Network-based Decoder (IGND) to capture the structure in- and weighted text graph G based on dependency tree. In a passage graph, each passage word is formation in generation and model the copied treated as a nod"
2021.emnlp-main.22,D17-1047,0,0.14349,"espectively. We adopt Adam (Kingma and Ba, 2015) with warm-up to optimize our models with learning rate 1e−3 for Transformer encoder and 5e−5 for BERT. The pre-trained models are fine-tuned by aspect-aware fine-tuning with 5e−5 learning rate. The hyper-parameters are set as α = β = 1 for combining objectives in SCAPT, and τ = 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning BERT for Sentence Pair Classification, in which t"
2021.emnlp-main.22,P19-1052,0,0.0157185,"5 learning rate. The hyper-parameters are set as α = β = 1 for combining objectives in SCAPT, and τ = 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning BERT for Sentence Pair Classification, in which the input review is transformed to “[CLS] + context + [SEP] + aspect + [SEP]” and fed into BERT for classification (Song et al., 2019). 250 Method Attention GNN Knowledge Enhanced Ours ATAE-LSTM (Wang et al., 2016a) IAN (Ma et al"
2021.emnlp-main.22,D14-1125,0,0.0711583,"Missing"
2021.emnlp-main.22,2021.naacl-main.146,0,0.0153807,", 2018) was further proposed to identify corresponding sentiment expression for aspects. Recent efforts (He et al., 2018; Tang et al., 2020) used syntax information from dependency trees to enhance attention-based models. A lot of works (Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020) make use of graph neural networks to incorporate tree-structured syntactic information and capture aspect-related information in text. Another line in ABSA concentrated on utilizing external corpus and pre-trained knowledge to enhance semantic awareness of models (Xu et al., 2019; Rietzler et al., 2020; Dai et al., 2021). Contrastive Representation Learning Our work adopts contrastive method in representation learning to acquire discriminating instance representations. Recent work on contrastive representation learning of instances usually based on estimating representation similarities on similar and dissimilar pairs, which are usually composed in a self-supervised manner (Chen et al., 2020; He et al., 2020). Specially, Khosla et al. (2020) illustrated a supervised contrastive method to build positive pairs between instances with same class label, and put their representations together. In this work, our mod"
2021.emnlp-main.22,D19-1006,0,0.0221846,"ng scheme. Aspect-Aware Fine-tuning Our proposed models are fine-tuned on ABSA benchmarks by aspect-aware fine-tuning, to fully leverage their ability of sentiment identification. They also learn to capture aspect-related sentiment information during fine-tuning. Specifically, given a sentence xab = ab {w1 , . . . , wa , . . . wn } in ABSA dataset D , and wa is one of the aspects occurring in xab . In fine-tuning, models predict aspect-level sentiment orientation yaab according to aspect-based ¯ ab and sentiment representation representation h a sab . Aspect-based Representation The research (Ethayarajh, 2019) on pre-trained contextualized word representation has demonstrated that it can capture context information related to the word. Thus, in spite of using laborious methods to embed the aspect information, we extract aspect-based ¯ ab by collecting final hidden representation h a ¯ ab states that correspond to wa . In fine-tuning, h a would focus on aspect-related words in context, which we believe would enhance the perception of aspect-specific opinion words and bring the model with a good view of explicit sentiment. Specifically, let Ia be the token index in aspect xa , we average the hidden s"
2021.emnlp-main.22,D18-1380,0,0.0309841,"Missing"
2021.emnlp-main.22,N19-1259,0,0.0476879,"Missing"
2021.emnlp-main.22,C18-1096,0,0.0888715,"Aspect-level sentiment analysis (ABSA) is a fine- contain implicit sentiment among Restaurant and grained variant aiming to identify the sentiment Laptop datasets. However, most of the previous polarity of one or more mentioned aspects in methods generally pay little attention on modeling product reviews. Recent studies tackle the task implicit sentiment expressions. This motivates us by either employing attention mechanisms (Wang to better solve the task of ABSA by capturing et al., 2016b; Ma et al., 2017) or incorporating implicit sentiment in an advanced way. syntax-aware graph structures (He et al., 2018; To equip current models with the ability to Tang et al., 2020; Zhang et al., 2019; Sun et al., capture implicit sentiment, inadequate ABSA 2019; Wang et al., 2020). Both methodologies aim datasets are the main challenge. With only a to capture the corresponding sentiment expression few thousand labeled data, models could hardly towards a particular aspect, which is usually an recognize comprehensive patterns of sentiment opinion word that explicitly expresses sentiment expressions, and are unable to capture enough polarity. For instance, given the review on a commonsense knowledge, which is"
2021.emnlp-main.22,D19-1654,0,0.21183,"sidering the sentiment annotations of retrieved corpora are noisy, supervised contrastive learning enhances noise immunity of the pretraining process. Also, SCAPT contains review reconstruction and masked aspect predication objectives. The former requires representation encoding review context besides sentiment polarity, and the latter adds the model’s ability to capture the sentiment target. Overall, the pre-training process captures both implicit and explicit sentiment orientation towards aspects in reviews. Experimental evaluations conducted on SemEval-2014 (Pontiki et al., 2014) and MAMS (Jiang et al., 2019) datasets show that proposed SCAPT outperforms baseline models by a large margin. The results on partitioned datasets demonstrate the effectiveness of both implicit sentiment expression and explicit sentiment expression. Moreover, the ablation study verifies that SCAPT efficiently learns implicit sentiment expression on the external noisy corpora. Codes and datasets are publicly available1 . The contributions of this work include: • We reveal that ABSA was only marginally tackled by previous studies since they paid little attention to implicit sentiment. • We propose Supervised Contrastive Pre"
2021.emnlp-main.22,2020.acl-main.703,0,0.0512512,"Missing"
2021.emnlp-main.22,S14-2004,0,0.316571,"ent labels are pushed apart. Considering the sentiment annotations of retrieved corpora are noisy, supervised contrastive learning enhances noise immunity of the pretraining process. Also, SCAPT contains review reconstruction and masked aspect predication objectives. The former requires representation encoding review context besides sentiment polarity, and the latter adds the model’s ability to capture the sentiment target. Overall, the pre-training process captures both implicit and explicit sentiment orientation towards aspects in reviews. Experimental evaluations conducted on SemEval-2014 (Pontiki et al., 2014) and MAMS (Jiang et al., 2019) datasets show that proposed SCAPT outperforms baseline models by a large margin. The results on partitioned datasets demonstrate the effectiveness of both implicit sentiment expression and explicit sentiment expression. Moreover, the ablation study verifies that SCAPT efficiently learns implicit sentiment expression on the external noisy corpora. Codes and datasets are publicly available1 . The contributions of this work include: • We reveal that ABSA was only marginally tackled by previous studies since they paid little attention to implicit sentiment. • We prop"
2021.emnlp-main.22,2020.lrec-1.607,0,0.135445,"tive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning BERT for Sentence Pair Classification, in which the input review is transformed to “[CLS] + context + [SEP] + aspect + [SEP]” and fed into BERT for classification (Song et al., 2019). 250 Method Attention GNN Knowledge Enhanced Ours ATAE-LSTM (Wang et al., 2016a) IAN (Ma et al., 2017) RAM (Chen et al., 2017) MGAN (Fan et al., 2018) ASGCN (Zhang et al., 2019) BiGCN (Zhang and Qian, 2020) CDT (Sun et al.,"
2021.emnlp-main.22,S15-2077,0,0.0243514,"2017). We denote the retrieved review corpus used in SCAPT as D = {x1 , x2 , . . . , xn } including n sentences. The i-th sentence xi is labeled with yi . For each input sentence xi , following Devlin et al. (2019), we format the input sentence as Ii = [CLS] + xi + [SEP] to feed into the model. The output vector of [CLS] token encodes the ¯ i: sentence representation h ¯ i , · · · = TransEnc(Ii ) h (1) Implicit Sentiment As sentiment that can only be inferred within the context of reviews, many researches address the presence of implicit sentiment in sentiment analysis. Toprak et al. (2010); Russo et al. (2015) proposed similar terminologies (as implicit polarity or polar facts), and provided corpora containing implicit sentiment. Deng and Wiebe (2014) detected implicit sentiment via inference over explicit sentiment expressions and so-called 1 https://github.com/Tribleave/SCAPT-ABSA 3 Methodology In this section, we introduce the pre-training and fine-tuning scheme of our models. In pre-training, we introduce Supervised ContrAstive Pre-Training (SCAPT) for ABSA, which learns the polarity of sentiment expressions by leveraging retrieved review corpus. In fine-tuning, aspect-aware finetuning is adopt"
2021.emnlp-main.22,D19-1569,0,0.076785,"5e−5 for BERT. The pre-trained models are fine-tuned by aspect-aware fine-tuning with 5e−5 learning rate. The hyper-parameters are set as α = β = 1 for combining objectives in SCAPT, and τ = 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning BERT for Sentence Pair Classification, in which the input review is transformed to “[CLS] + context + [SEP] + aspect + [SEP]” and fed into BERT for classification (Song et al., 2019). 2"
2021.emnlp-main.22,D16-1021,0,0.0157601,"ons can models pre-trained with SCAPT are more robust be found in BERTAsp. The visualization also for aspect-level perturbations, which attribute to shows that BERTAsp+SCAPT tightly clusters the better modeling for sentiment and context the representations of both implicit and explicit information with the enhancement of in-domain sentiment expressions. sentiment knowledge. 253 6 Related Work Neural Network Methods for ABSA The early neural network methods (Wang et al., 2016b; Ma et al., 2017) in ABSA employed various of attention mechanisms to identify aspect-related context. Memory Network (Tang et al., 2016; Chen et al., 2017; Wang et al., 2018) was further proposed to identify corresponding sentiment expression for aspects. Recent efforts (He et al., 2018; Tang et al., 2020) used syntax information from dependency trees to enhance attention-based models. A lot of works (Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020) make use of graph neural networks to incorporate tree-structured syntactic information and capture aspect-related information in text. Another line in ABSA concentrated on utilizing external corpus and pre-trained knowledge to enhance semantic awareness of models (Xu et al"
2021.emnlp-main.22,2020.acl-main.588,0,0.103833,"licit sentiment among Restaurant and grained variant aiming to identify the sentiment Laptop datasets. However, most of the previous polarity of one or more mentioned aspects in methods generally pay little attention on modeling product reviews. Recent studies tackle the task implicit sentiment expressions. This motivates us by either employing attention mechanisms (Wang to better solve the task of ABSA by capturing et al., 2016b; Ma et al., 2017) or incorporating implicit sentiment in an advanced way. syntax-aware graph structures (He et al., 2018; To equip current models with the ability to Tang et al., 2020; Zhang et al., 2019; Sun et al., capture implicit sentiment, inadequate ABSA 2019; Wang et al., 2020). Both methodologies aim datasets are the main challenge. With only a to capture the corresponding sentiment expression few thousand labeled data, models could hardly towards a particular aspect, which is usually an recognize comprehensive patterns of sentiment opinion word that explicitly expresses sentiment expressions, and are unable to capture enough polarity. For instance, given the review on a commonsense knowledge, which is required in restaurant “Great food but the service is dreadful”"
2021.emnlp-main.22,2021.acl-demo.41,1,0.717486,"Ori → New Decline 67.55→9.87 -57.68 72.41→19.91 -52.50 77.12→25.86 -51.46 77.59→50.94 -26.65 78.53→53.29 -25.24 76.80→52.52 -24.28 82.76→76.13 -6.63 Table 6: Model performance on aspect robustness test sets. We compare the model accuracy on the original and new test sets, and the decline of prediction on new examples are reported. 5.6 Aspect Robustness We analyze the robustness of our proposed models on aspect robustness test sets. Aspect robustness of ABSA was first emphasized and tested in Xing et al. (2020) by applying several perturbations on reviews from Restaurant and Laptop. TextFlint (Wang et al., 2021) extended these transformations by introducing transformations from various linguistic perspectives. The test sets are designed to probe whether models could distinguish the sentiment of the target aspect from the non-target aspects and unrelated information. 5.5 Hidden Sentiment Representations Table 6 lists the performance of tested modFor better understanding the behavior of our els, in which the robustness of our proposed proposed methods, we further perform a visual- models is convincingly proved. Comparing to ization of the sentiment representation using t-SNE obvious performance drop in"
2021.emnlp-main.22,2020.acl-main.374,0,0.0192792,"ked input token at k-th position, its contextualized hidden representation hik is fed into a softmax layer to predict the original word: P map (k) = softmax(Wo hik ) (6) Specific to the above equation, hik is the output of Transformer encoder at k-th position, Wo is a trainable parameter matrix, and P map (k) indicates the predict probability of the original word at k-th position. The masked aspect prediction loss is an accumulation of log-likelihood on predictions of each masked position: Lmap = i X − log P map (k) (7) xik =wMASK Different from MLM (Devlin et al., 2019) or sentiment masking (Tian et al., 2020), masked aspect prediction focuses more on modeling aspectrelated context information in aspect-based representations, which complements the other pretraining objectives and purposefully benefits our fine-tuning scheme. Aspect-Aware Fine-tuning Our proposed models are fine-tuned on ABSA benchmarks by aspect-aware fine-tuning, to fully leverage their ability of sentiment identification. They also learn to capture aspect-related sentiment information during fine-tuning. Specifically, given a sentence xab = ab {w1 , . . . , wa , . . . wn } in ABSA dataset D , and wa is one of the aspects occurrin"
2021.emnlp-main.22,P10-1059,0,0.0455267,"coder (Vaswani et al., 2017). We denote the retrieved review corpus used in SCAPT as D = {x1 , x2 , . . . , xn } including n sentences. The i-th sentence xi is labeled with yi . For each input sentence xi , following Devlin et al. (2019), we format the input sentence as Ii = [CLS] + xi + [SEP] to feed into the model. The output vector of [CLS] token encodes the ¯ i: sentence representation h ¯ i , · · · = TransEnc(Ii ) h (1) Implicit Sentiment As sentiment that can only be inferred within the context of reviews, many researches address the presence of implicit sentiment in sentiment analysis. Toprak et al. (2010); Russo et al. (2015) proposed similar terminologies (as implicit polarity or polar facts), and provided corpora containing implicit sentiment. Deng and Wiebe (2014) detected implicit sentiment via inference over explicit sentiment expressions and so-called 1 https://github.com/Tribleave/SCAPT-ABSA 3 Methodology In this section, we introduce the pre-training and fine-tuning scheme of our models. In pre-training, we introduce Supervised ContrAstive Pre-Training (SCAPT) for ABSA, which learns the polarity of sentiment expressions by leveraging retrieved review corpus. In fine-tuning, aspect-awar"
2021.emnlp-main.22,2020.acl-main.295,0,0.109745,". However, most of the previous polarity of one or more mentioned aspects in methods generally pay little attention on modeling product reviews. Recent studies tackle the task implicit sentiment expressions. This motivates us by either employing attention mechanisms (Wang to better solve the task of ABSA by capturing et al., 2016b; Ma et al., 2017) or incorporating implicit sentiment in an advanced way. syntax-aware graph structures (He et al., 2018; To equip current models with the ability to Tang et al., 2020; Zhang et al., 2019; Sun et al., capture implicit sentiment, inadequate ABSA 2019; Wang et al., 2020). Both methodologies aim datasets are the main challenge. With only a to capture the corresponding sentiment expression few thousand labeled data, models could hardly towards a particular aspect, which is usually an recognize comprehensive patterns of sentiment opinion word that explicitly expresses sentiment expressions, and are unable to capture enough polarity. For instance, given the review on a commonsense knowledge, which is required in restaurant “Great food but the service is dreadful”, sentiment identification. It reveals that external current models attempt to find “great” for aspect"
2021.emnlp-main.22,P18-1088,0,0.0177396,"are more robust be found in BERTAsp. The visualization also for aspect-level perturbations, which attribute to shows that BERTAsp+SCAPT tightly clusters the better modeling for sentiment and context the representations of both implicit and explicit information with the enhancement of in-domain sentiment expressions. sentiment knowledge. 253 6 Related Work Neural Network Methods for ABSA The early neural network methods (Wang et al., 2016b; Ma et al., 2017) in ABSA employed various of attention mechanisms to identify aspect-related context. Memory Network (Tang et al., 2016; Chen et al., 2017; Wang et al., 2018) was further proposed to identify corresponding sentiment expression for aspects. Recent efforts (He et al., 2018; Tang et al., 2020) used syntax information from dependency trees to enhance attention-based models. A lot of works (Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020) make use of graph neural networks to incorporate tree-structured syntactic information and capture aspect-related information in text. Another line in ABSA concentrated on utilizing external corpus and pre-trained knowledge to enhance semantic awareness of models (Xu et al., 2019; Rietzler et al., 2020; Dai et"
2021.emnlp-main.22,D16-1058,0,0.261289,"nsformer encoder and BERT takes 80 and 8 epochs respectively. We adopt Adam (Kingma and Ba, 2015) with warm-up to optimize our models with learning rate 1e−3 for Transformer encoder and 5e−5 for BERT. The pre-trained models are fine-tuned by aspect-aware fine-tuning with 5e−5 learning rate. The hyper-parameters are set as α = β = 1 for combining objectives in SCAPT, and τ = 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning"
2021.emnlp-main.22,2020.emnlp-main.292,1,0.84704,"Missing"
2021.emnlp-main.22,N19-1242,0,0.101511,"= 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning BERT for Sentence Pair Classification, in which the input review is transformed to “[CLS] + context + [SEP] + aspect + [SEP]” and fed into BERT for classification (Song et al., 2019). 250 Method Attention GNN Knowledge Enhanced Ours ATAE-LSTM (Wang et al., 2016a) IAN (Ma et al., 2017) RAM (Chen et al., 2017) MGAN (Fan et al., 2018) ASGCN (Zhang et al., 2019) BiGCN (Zhang"
2021.emnlp-main.22,D19-1464,0,0.0267047,"Missing"
2021.emnlp-main.22,2020.emnlp-main.286,0,0.0167801,"for Transformer encoder and 5e−5 for BERT. The pre-trained models are fine-tuned by aspect-aware fine-tuning with 5e−5 learning rate. The hyper-parameters are set as α = β = 1 for combining objectives in SCAPT, and τ = 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning BERT for Sentence Pair Classification, in which the input review is transformed to “[CLS] + context + [SEP] + aspect + [SEP]” and fed into BERT for classificatio"
2021.emnlp-main.22,D16-1059,0,0.162078,"nsformer encoder and BERT takes 80 and 8 epochs respectively. We adopt Adam (Kingma and Ba, 2015) with warm-up to optimize our models with learning rate 1e−3 for Transformer encoder and 5e−5 for BERT. The pre-trained models are fine-tuned by aspect-aware fine-tuning with 5e−5 learning rate. The hyper-parameters are set as α = β = 1 for combining objectives in SCAPT, and τ = 0.07 in supervised contrastive learning. Baselines We compare the proposed models with baselines from different perspectives to comprehensively evaluate the performance of our approach: • Attention-based models: ATAE-LSTM (Wang et al., 2016b), IAN (Ma et al., 2017), RAM (Chen et al., 2017), and MGAN (Fan et al., 2018). • Graph neural networks: ASGCN (Zhang et al., 2019), BiGCN (Zhang and Qian, 2020), CDT (Sun et al., 2019), and RGAT (Wang et al., 2020). • Knowledge-enhanced methods: TransCap (Chen and Qian, 2019), BERT-SPC 3 (Devlin et al., 2019), CapsNet+BERT (Jiang et al., 2019), BERT-PT (Xu et al., 2019), BERT-ADA (Rietzler et al., 2020), and R-GAT+BERT (Wang et al., 2020). For better analyze the effect of SCAPT and aspectaware fine-tuning, we further propose the following variants as baselines: 3 BERT-SPC denotes fine-tuning"
2021.emnlp-main.7,N19-1423,0,0.01868,"thods LightConv (Wu et al., 2019) and DynamicConv (Wu et al., 2019); Methods based on graph neural networks, including D-HGN (Feng et al., 2020) and TGDGA (Zhao et al., 2020); A seq2seq model BERT+TRF (Liu and Lapata, 2019) that is equipped with pretrained LMs. 4.3 Implementation Details At the pretraining stage, we mix up the datasets from multiple sources and keep dialogues, short texts, and news summaries in a percentage of 1:1:1. The total data points are around 15M. Since DAMS consists of Transformer encoders and decoders, it can be easily combined with pretrained LMs. Here, we use BERT (Devlin et al., 2019) as the utterance/sentence encoder TFθed and use a separate optimization strategy (Liu and Lapata, 2019) to alleviate the mismatch between BERT and other randomly initialized parameters. We apply Adam (Kingma and Ba, 2015) (β1 =0.9, β2 =0.999) with learning rate 1e3 for BERT and 1e-2 for other parameters. All transformer blocks except BERT have 6 layers, 8 heads, 768 hidden units, and the hidden size for all feed-forward layers is 2048. Loss coefficient α is selected from {0.01, 0.05, 0.1, 0.5} to control adversarial signals, and we empirically find that α = 0.1 achieves the best performance o"
2021.emnlp-main.7,2020.findings-emnlp.335,0,0.173751,"summarization in low-resource settings, where only limited or even no training examples are available. Recently, domain adaptation approaches with large-scale pretraining have attracted much attention in low-resource summarization (Wang et al., 2019; Yang et al., 2020; Zhang et al., 2020). A similar strategy is used in dialogues, whereby external summary data from other domains, e.g., the CNN/Dailymail news dataset (Hermann et al., 2015), are introduced for model pretraining prior to the final fine-tuning on low-resource dialogue summaries. Recent works (Gliwa et al., 2019; Zhu et al., 2020; Joshi et al., 2020) have also reported the effectiveness of pretrained summarizers for different kinds of dialogue scenarios, such as chat logs and medical conversations. However, dialogue summary data has several inherent and significant differences from conventional articles in terms of text styles and summary structures. (i) Dialogues generally contain multiple participants who have distinct characteristics. (ii) Rather than the formal expressions found in news documents, dialogues often comprise utterances with informal or ungrammatical phrases. (iii) The structure of a dialogue summary, including length and"
2021.emnlp-main.7,W19-4103,0,0.0240618,"xperimental Settings Datasets Following the latest works (Zhao et al., 2020; Feng et al., 2020), we evaluate our method on two public dialogue summary datasets SAMSum 84 (Gliwa et al., 2019) and ADSC3 (Misra et al., 2015). Statistics of the dialogue datasets is shown in Table 1. SAMSum originally contains 14k training examples. To simulate a low-resource scenario, we start from using the full training data, and gradually reduce the number of training examples by halving the training set. For multi-source pretraining, we use the following datasets. Dialogues. We use Reddit Conversation Corpus (Dziri et al., 2019)4 for the pretaining of dialogue modeling. It contains about 15M context-response pairs for training, where each dialogue context consists of 3.5 utterances on average. Short Texts. We choose MSCOCO (Lin et al., 2014) and BookCorpus (Zhu et al., 2015) to pretrain the summary language model. MSCOCO is a standard benchmark dataset for the image caption generation task, which contains over 120K images and 600K captions describing the prominent object/action in an image. Here, we only use captions to train the generator. BookCorpus is a large-scale corpus containing 11,038 free books from the Inte"
2021.emnlp-main.7,2020.acl-main.703,0,0.0250869,"main-invariant features and encouraging the summarizer to only focus on content rather than domain-specific attributes. Split # of dial. Avg. words Avg. turns Ref. length SAMSum Train Dev. Test 14,732 818 819 120.26 117.46 122.71 11.13 10.72 11.24 22.81 22.80 22.47 ADSC All 45 370.44 7.51 101.99 Table 1: Statistics of dialogue summary datasets. 3.4 Discussion of the Encoder-Decoder Connection Strategy The encoder-decoder cross attention for encoding the context information is widely used in transformer-based architectures. Large-scale pretraining models for the summarization task, e.g., BART (Lewis et al., 2020), generally exploit token-level attention to integrate the document context. In this work, we have tried keeping the traditional token-level cross attention in the proposed architecture to directly connect the dialogue encoder and the summary decoder. However, we find that it is difficult to disentangle the encoder and the decoder for separate pretraining. It is also hard to add adversarial critics to token-level representations involved in the cross attention to learn domain-invariant features. Considering the above limitations, we use an embedding concatenation strategy in the dialogue decod"
2021.emnlp-main.7,W04-1013,0,0.0323144,"ble 2: Results of ROUGE-1/2/L on the SAMSum corpus. +News means whether the approach exploits external news summary data or not. Model RG-1 RG-2 RG-L PGNet 28.95 5.34 22.41 Transformer 27.13 5.30 20.59 FastRL Enhanced 30.00 4.87 22.27 BERT+TRF* 28.13 4.63 27.17 DAMS (w/o pretrain) 28.17 5.11 27.09 DAMS* 31.29 5.53 30.14 Table 3: Results of zero-shot testing on ADSC. Models marked with * use external news summary data. 5.1 Informativeness Automatic Evaluation Table 2 and Table 3 show the results of automatic evaluation on the SAMSum and ADSC dataset. We evaluate summary quality using ROUGE F1 (Lin, 2004), including the unigram and bigram overlap (ROUGE-1, ROUGE-2) between system outputs and gold summaries, and the longest common subsequence (ROUGE-L). Some results are from the reported scores in previous literatures (Gliwa et al., 2019; Feng et al., 2020; Zhao et al., 2020). In Table 2, all baseline methods are categorized into two groups. The first group includes models that are directly trained on the SAMSum corpus, and methods in the second group benefit from external news summary data6 . DAMS with full training data outperforms all baseline methods and is significantly different from BERT"
2021.emnlp-main.7,D19-5409,0,0.12045,",xwhu20,tgui,qz}@fudan.edu.cn Abstract summarization in low-resource settings, where only limited or even no training examples are available. Recently, domain adaptation approaches with large-scale pretraining have attracted much attention in low-resource summarization (Wang et al., 2019; Yang et al., 2020; Zhang et al., 2020). A similar strategy is used in dialogues, whereby external summary data from other domains, e.g., the CNN/Dailymail news dataset (Hermann et al., 2015), are introduced for model pretraining prior to the final fine-tuning on low-resource dialogue summaries. Recent works (Gliwa et al., 2019; Zhu et al., 2020; Joshi et al., 2020) have also reported the effectiveness of pretrained summarizers for different kinds of dialogue scenarios, such as chat logs and medical conversations. However, dialogue summary data has several inherent and significant differences from conventional articles in terms of text styles and summary structures. (i) Dialogues generally contain multiple participants who have distinct characteristics. (ii) Rather than the formal expressions found in news documents, dialogues often comprise utterances with informal or ungrammatical phrases. (iii) The structure of a"
2021.emnlp-main.7,N18-1065,0,0.0423014,"Missing"
2021.emnlp-main.7,C04-1110,0,0.284521,"Missing"
2021.emnlp-main.7,D19-1387,0,0.0178476,"ion Corpus. CNN/DailyMail (Hermann et al., 2015), Gigaword (Rush et al., 2015), and NewsRoom (Grusky et al., 2018) are used as our external summary datasets for joint pretraining. All the three datasets are news articles or headlines with summaries from various news publications. We combine these datasets and the total training set consists of 5.6M samples. 4.2 refines them; Convolution-based methods LightConv (Wu et al., 2019) and DynamicConv (Wu et al., 2019); Methods based on graph neural networks, including D-HGN (Feng et al., 2020) and TGDGA (Zhao et al., 2020); A seq2seq model BERT+TRF (Liu and Lapata, 2019) that is equipped with pretrained LMs. 4.3 Implementation Details At the pretraining stage, we mix up the datasets from multiple sources and keep dialogues, short texts, and news summaries in a percentage of 1:1:1. The total data points are around 15M. Since DAMS consists of Transformer encoders and decoders, it can be easily combined with pretrained LMs. Here, we use BERT (Devlin et al., 2019) as the utterance/sentence encoder TFθed and use a separate optimization strategy (Liu and Lapata, 2019) to alleviate the mismatch between BERT and other randomly initialized parameters. We apply Adam (K"
2021.emnlp-main.7,W17-4513,0,0.0173869,"ogue summary dataset is very expensive and labor-intensive, which makes the traditional methods hard to apply in real-world applications, especially when only limited or even no training signals are available. In this work, we explore dialogue summarization in a low-resource setting, and leverage external large-scale corpora to facilitate the task, which is applicable to most dialogue scenarios. 2.2 Since texts and their summaries across diverse domains might share similarities and benefit from each other, domain adaptation for text summarization has attracted much research interest recently (Hua and Wang, 2017; Wang et al., 2019; Zhang et al., 2020; Yang et al., 2020; Yu et al., 2021). Most existing works perform pretraining on largescale out-of-domain datasets and then adapt to the in-domain summary data. For dialogue summarization, although it is more ideal to perform adaptation from a source dialogue domain to a target dialogue domain (Sandu et al., 2010; Wang and Cardie, 2013), unfortunately, the inadequacy of dialogue summary data makes it infeasible to directly train a large summarization model on the source data in an end-to-end manner. Recently, a couple of works have leveraged large-scale"
2021.emnlp-main.7,N15-1046,0,0.13426,"e combined and pretrained on external summary data to go through an integral process of summarization. The above pretraining processes from the three sources are performed simultaneously. By this means, DAMS exploits large-scale non-summary data in the same domain to narrow the gap between pretraining and finetuning. Additionally, adversarial critics are used to capture the features shared between dialogues and general documents, and to learn to perform domain-agnostic summarization. We conducted experiments on two public dialogue summary datasets, namely SAMSum (Gliwa et al., 2019) and ADSC (Misra et al., 2015). Pretraining was conducted on datasets from multiple sources, including dialogue corpora, dailylife short text corpora, and text summarization datasets from the news domain. The experimental results show that with only limited training data of dialogue summaries, our approach achieved competitive performance and showed a promising ability for generalizing different dialogue scenarios. Our codes and datasets are publicly available1 . In summary, our contributions are three-fold: 1) We explore the task of dialogue summarization in a low-resource setting with the usage of external multi-source c"
2021.emnlp-main.7,D08-1081,0,0.0311549,"odel is then pretrained on the out-of-domain summary data using adversarial critics, aiming to facilitate domain-agnostic summarization. The experimental results on two public datasets show that with only limited training data, our approach achieves competitive performance and generalizes well in different dialogue scenarios. 1 Introduction With the explosion in the quantity of dialogue data from the Internet and daily life, there is growing interest in automatic dialogue summarization for various scenarios and applications, such as email threads, meetings, customer service, and online chats (Murray and Carenini, 2008; Shang et al., 2018; Liu et al., 2019; Zou et al., 2021a,b). Unfortunately, creating large-scale dialogue datasets with annotated summaries is costly and labor-intensive, which makes it difficult to build and train large summarization models using adequate supervision signals, especially in a new domain. Hence, it is necessary to develop models for dialogue ∗ Corresponding authors. 80 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 80–91 c November 7–11, 2021. 2021 Association for Computational Linguistics scale unannotated dialogues to learn the"
2021.emnlp-main.7,D18-1206,0,0.104653,"main nonsummary data, we explore the domain-agnostic summarization. It is supported by a multi-source pretraining paradigm with adversarial learning, where the encoder and the decoder are separately pretrained on the in-domain non-summary data and combinedly pretrained on the out-of-domain summary data, aiming to narrow the gap between pretraining and fine-tuning. Related Work 2.1 Dialogue Summarization Dialogue summarization is a challenging and valuable task that receives much attention in recent years. Different from studies on conventional documents like news or reviews (See et al., 2017; Narayan et al., 2018; Chu and Liu, 2019), dialogue summarization is investigated in multi-party interactions such as mail threads (Rambow et al., 2004), meetings (Gillick et al., 2009; Shang et al., 2018; Zhong et al., 2021), telephone conversation records (Zechner, 2001; Gurevych and Strube, 1 Domain Adaptation for Summarization https://github.com/RowitZou/DAMS 81 Recon. Loss Recon. Loss NLL Loss Dialogue Dialogue Summary Adv. Loss ? Short Text News Dialogue Decoder Summary Decoder Mix Up Dialogue Utter. Representations News Sent. Representations Context Encoder News Sent. Representations Dialogue Encoder Mix Up"
2021.emnlp-main.7,N04-4027,0,0.30122,"Missing"
2021.emnlp-main.7,P13-1137,0,0.0765477,"Missing"
2021.emnlp-main.7,D15-1044,0,0.342084,", 2015) to pretrain the summary language model. MSCOCO is a standard benchmark dataset for the image caption generation task, which contains over 120K images and 600K captions describing the prominent object/action in an image. Here, we only use captions to train the generator. BookCorpus is a large-scale corpus containing 11,038 free books from the Internet. We randomly truncate long documents into text pieces as training samples5 . Each sample contains 1.5 sentences on average and we collect about 5M samples for training. Summarization Corpus. CNN/DailyMail (Hermann et al., 2015), Gigaword (Rush et al., 2015), and NewsRoom (Grusky et al., 2018) are used as our external summary datasets for joint pretraining. All the three datasets are news articles or headlines with summaries from various news publications. We combine these datasets and the total training set consists of 5.6M samples. 4.2 refines them; Convolution-based methods LightConv (Wu et al., 2019) and DynamicConv (Wu et al., 2019); Methods based on graph neural networks, including D-HGN (Feng et al., 2020) and TGDGA (Zhao et al., 2020); A seq2seq model BERT+TRF (Liu and Lapata, 2019) that is equipped with pretrained LMs. 4.3 Implementation"
2021.emnlp-main.7,W10-2603,0,0.016219,"ch is applicable to most dialogue scenarios. 2.2 Since texts and their summaries across diverse domains might share similarities and benefit from each other, domain adaptation for text summarization has attracted much research interest recently (Hua and Wang, 2017; Wang et al., 2019; Zhang et al., 2020; Yang et al., 2020; Yu et al., 2021). Most existing works perform pretraining on largescale out-of-domain datasets and then adapt to the in-domain summary data. For dialogue summarization, although it is more ideal to perform adaptation from a source dialogue domain to a target dialogue domain (Sandu et al., 2010; Wang and Cardie, 2013), unfortunately, the inadequacy of dialogue summary data makes it infeasible to directly train a large summarization model on the source data in an end-to-end manner. Recently, a couple of works have leveraged large-scale summary data that is more distinct from the dialogue domain, e.g., the news domain, to facilitate dialogue summarization (Gliwa et al., 2019; Zhu et al., 2020; Joshi et al., 2020). However, the huge gap between dialogues and general articles is barely noticed. Yu et al. (2021) conducted pretraining on the news summary data and the dialogue non-summary"
2021.emnlp-main.7,2020.findings-emnlp.168,0,0.18472,"urce Pretraining Yicheng Zou1,2 , Bolin Zhu2 , Xingwu Hu2 , Tao Gui1∗ , Qi Zhang2∗ 1 Institute of Modern Languages and Linguistics, Fudan University 2 Shanghai Key Laboratory of Intelligent Information Processing, Fudan University 2 School of Computer Science, Fudan University Shanghai, China {yczou18,blzhu20,xwhu20,tgui,qz}@fudan.edu.cn Abstract summarization in low-resource settings, where only limited or even no training examples are available. Recently, domain adaptation approaches with large-scale pretraining have attracted much attention in low-resource summarization (Wang et al., 2019; Yang et al., 2020; Zhang et al., 2020). A similar strategy is used in dialogues, whereby external summary data from other domains, e.g., the CNN/Dailymail news dataset (Hermann et al., 2015), are introduced for model pretraining prior to the final fine-tuning on low-resource dialogue summaries. Recent works (Gliwa et al., 2019; Zhu et al., 2020; Joshi et al., 2020) have also reported the effectiveness of pretrained summarizers for different kinds of dialogue scenarios, such as chat logs and medical conversations. However, dialogue summary data has several inherent and significant differences from conventional"
2021.emnlp-main.7,P17-1099,0,0.313569,"data and the in-domain nonsummary data, we explore the domain-agnostic summarization. It is supported by a multi-source pretraining paradigm with adversarial learning, where the encoder and the decoder are separately pretrained on the in-domain non-summary data and combinedly pretrained on the out-of-domain summary data, aiming to narrow the gap between pretraining and fine-tuning. Related Work 2.1 Dialogue Summarization Dialogue summarization is a challenging and valuable task that receives much attention in recent years. Different from studies on conventional documents like news or reviews (See et al., 2017; Narayan et al., 2018; Chu and Liu, 2019), dialogue summarization is investigated in multi-party interactions such as mail threads (Rambow et al., 2004), meetings (Gillick et al., 2009; Shang et al., 2018; Zhong et al., 2021), telephone conversation records (Zechner, 2001; Gurevych and Strube, 1 Domain Adaptation for Summarization https://github.com/RowitZou/DAMS 81 Recon. Loss Recon. Loss NLL Loss Dialogue Dialogue Summary Adv. Loss ? Short Text News Dialogue Decoder Summary Decoder Mix Up Dialogue Utter. Representations News Sent. Representations Context Encoder News Sent. Representations D"
2021.emnlp-main.7,2021.naacl-main.471,0,0.0577581,"Missing"
2021.emnlp-main.7,P19-1499,0,0.0485263,"Missing"
2021.emnlp-main.7,2020.coling-main.39,0,0.533133,"ce and showed a promising ability for generalizing different dialogue scenarios. Our codes and datasets are publicly available1 . In summary, our contributions are three-fold: 1) We explore the task of dialogue summarization in a low-resource setting with the usage of external multi-source corpora. 2) A novel pretraining strategy is designed to bridge the gap between out-of-domain pretraining and in-domain finetuning for domain-agnostic summarization. 3) Comprehensive studies on two datasets show the effectiveness of our method in various aspects. 2 2004), and daily chats (Gliwa et al., 2019; Zhao et al., 2020). Most of these approaches share a similar prerequisite: a decent labeled training dataset with annotated summaries. Nevertheless, creating a large-scale dialogue summary dataset is very expensive and labor-intensive, which makes the traditional methods hard to apply in real-world applications, especially when only limited or even no training signals are available. In this work, we explore dialogue summarization in a low-resource setting, and leverage external large-scale corpora to facilitate the task, which is applicable to most dialogue scenarios. 2.2 Since texts and their summaries across"
2021.emnlp-main.7,2020.findings-emnlp.19,0,0.145216,"an.edu.cn Abstract summarization in low-resource settings, where only limited or even no training examples are available. Recently, domain adaptation approaches with large-scale pretraining have attracted much attention in low-resource summarization (Wang et al., 2019; Yang et al., 2020; Zhang et al., 2020). A similar strategy is used in dialogues, whereby external summary data from other domains, e.g., the CNN/Dailymail news dataset (Hermann et al., 2015), are introduced for model pretraining prior to the final fine-tuning on low-resource dialogue summaries. Recent works (Gliwa et al., 2019; Zhu et al., 2020; Joshi et al., 2020) have also reported the effectiveness of pretrained summarizers for different kinds of dialogue scenarios, such as chat logs and medical conversations. However, dialogue summary data has several inherent and significant differences from conventional articles in terms of text styles and summary structures. (i) Dialogues generally contain multiple participants who have distinct characteristics. (ii) Rather than the formal expressions found in news documents, dialogues often comprise utterances with informal or ungrammatical phrases. (iii) The structure of a dialogue summary,"
2021.emnlp-main.765,W19-4828,0,0.0123967,"vel relations in unlabeled data. Different from them, we propose a relation-oriented method explicitly clustering data based on relational information. Knowledge in High-Dimensional Vector. Pretrained static and contextual word representations can provide valuable prior knowledge for constructing relational representations (Soares et al., 2019; Elsahar et al., 2017). Peters et al. (2018) showed that different neural architectures (e.g., LSTM, CNN, and Transformers) can hierarchically structure linguistic information that varies with network depth. Recently, many studies (Jawahar et al., 2019; Clark et al., 2019; Goldberg, 2019) have shown that such hierarchy also exists in pretraining models like BERT. These results suggest that high-dimensional embeddings, independent of model architecture, learn much about the structure of language. Directly clustering on these highdimensional embeddings should hardly produce 9708 STEP 2: Optimize Clustering and Generate Pseudo Labels for Novel Relation Relational Centroids Non-linear Decoder Non-linear Mapping Relational Semantic Space Optimized Relational Semantic Space backprop Entity Pair Encoder (BERT) [CLS] Bill Gates was born in Seattle in 1995 Classifier f"
2021.emnlp-main.765,N19-1423,0,0.0124841,"Missing"
2021.emnlp-main.765,D11-1142,0,0.0533551,"stances S2 and S3 express founded relation while S1 expresses CEO relation, the distance between S1 and S2 is still smaller than that between S2 and S3 . This is because there may be more similar surface information (e.g. word overlapping) or syntactic structure between S1 and S2 , thus the derived clusters cannot explicitly align with relations. in extracting new emerging relational types from open-domain corpora. The approaches used to handle open relations roughly fall into one of two groups. The first group is open information extraction (OpenIE) (Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), which directly extracts related phrases as representations of different relational types. However, if not properly canonicalized, the extracted relational facts can be redundant and ambiguous. The second group 1 Introduction is unsupervised relation discovery (Yao et al., Relation extraction (RE), a crucial basic task 2011; Shinyama and Sekine, 2006; Simon et al., in the field of information extraction, is of the 2019). In this type of research, much attention utmost practical interest to various fields including has been focused on unsupervised clustering-based web search (Xiong et al., 201"
2021.emnlp-main.765,2020.emnlp-main.299,0,0.189321,"esearch, much attention utmost practical interest to various fields including has been focused on unsupervised clustering-based web search (Xiong et al., 2017), knowledge base RE methods, which cluster and recognize relations completion (Bordes et al., 2013), and question from high-dimensional representations (Elsahar answering (Yu et al., 2017). However, conventional et al., 2017). Recently, the self-supervised signals RE paradigms such as supervision and distant in pretrained language model are further exploited supervision are generally designed for pre-defined for clustering optimization (Hu et al., 2020). relations, which cannot deal with new emerging However, many studies show that highrelations in the real world. dimensional embeddings can encode complex Under this background, open relation extraction linguistic information such as morphological (OpenRE) has been widely studied for its use (Peters et al., 2018), local syntactic (Hewitt and * Corresponding authors. Manning, 2019), and longer range semantic 9707 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9707–9718 c November 7–11, 2021. 2021 Association for Computational Linguistics informati"
2021.emnlp-main.765,P19-1356,0,0.052826,"lations, which cannot deal with new emerging However, many studies show that highrelations in the real world. dimensional embeddings can encode complex Under this background, open relation extraction linguistic information such as morphological (OpenRE) has been widely studied for its use (Peters et al., 2018), local syntactic (Hewitt and * Corresponding authors. Manning, 2019), and longer range semantic 9707 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9707–9718 c November 7–11, 2021. 2021 Association for Computational Linguistics information (Jawahar et al., 2019). Consequently, the distance of representation is not completely consistent with relational semantic similarity. Although Hu et al. (2020) use self-supervised signals to optimize clustering, there is still no guarantee that the learned clusters will explicitly align with the desired relational semantic classes (Xing et al., 2002). As shown in Figure 1, we use the method proposed by Hu et al. (2020) to get the instance representations. Although both instances S2 and S3 express the founded relation, the euclidean distance between them is larger than that between S1 and S2 , which express differe"
2021.emnlp-main.765,Q16-1017,0,0.0227251,"X 1080Ti with 11GB memory and table 1 shows our best hyper-parameter settings. To evaluate the effectiveness of our method, we select the following SOTA OpenRE models for comparison. Note that the first four methods are unsupervised and RSN as well as RSN-BERT leverages labeled data of predefined relations. HAC with Re-weighted Word Embeddings (RW-HAC) (Elsahar et al., 2017). RW-HAC is a feature clustering method for OpenRE. The model contructs relational feature based on the weighted word embeddings as well as entity types. Discrete-state Variational Autoencoder (VAE) 5 Results and Analysis (Marcheggiani and Titov, 2016). VAE is a reconstruction-based method for OpenRE. The In this section, we present the experimental results model is optimized by reconstructing entities from of our model on two real-world datasets to demonpairing entities and predicted relations. strate the effectiveness of our method. We also Entity Based URE (Etype+) (Tran et al., 2020). provide additional experimental results on hyperEtype+ is a simple and effective method relying parameter analysis and relation representation only on entity types. The same link predictor as in visualization in appendix A and B. 9712 Dataset FewRel TACRED"
2021.emnlp-main.765,D18-1179,0,0.0769902,"(Elsahar answering (Yu et al., 2017). However, conventional et al., 2017). Recently, the self-supervised signals RE paradigms such as supervision and distant in pretrained language model are further exploited supervision are generally designed for pre-defined for clustering optimization (Hu et al., 2020). relations, which cannot deal with new emerging However, many studies show that highrelations in the real world. dimensional embeddings can encode complex Under this background, open relation extraction linguistic information such as morphological (OpenRE) has been widely studied for its use (Peters et al., 2018), local syntactic (Hewitt and * Corresponding authors. Manning, 2019), and longer range semantic 9707 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9707–9718 c November 7–11, 2021. 2021 Association for Computational Linguistics information (Jawahar et al., 2019). Consequently, the distance of representation is not completely consistent with relational semantic similarity. Although Hu et al. (2020) use self-supervised signals to optimize clustering, there is still no guarantee that the learned clusters will explicitly align with the desired relati"
2021.emnlp-main.765,N06-1039,0,0.0801104,". in extracting new emerging relational types from open-domain corpora. The approaches used to handle open relations roughly fall into one of two groups. The first group is open information extraction (OpenIE) (Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), which directly extracts related phrases as representations of different relational types. However, if not properly canonicalized, the extracted relational facts can be redundant and ambiguous. The second group 1 Introduction is unsupervised relation discovery (Yao et al., Relation extraction (RE), a crucial basic task 2011; Shinyama and Sekine, 2006; Simon et al., in the field of information extraction, is of the 2019). In this type of research, much attention utmost practical interest to various fields including has been focused on unsupervised clustering-based web search (Xiong et al., 2017), knowledge base RE methods, which cluster and recognize relations completion (Bordes et al., 2013), and question from high-dimensional representations (Elsahar answering (Yu et al., 2017). However, conventional et al., 2017). Recently, the self-supervised signals RE paradigms such as supervision and distant in pretrained language model are further"
2021.emnlp-main.765,P19-1133,0,0.0431535,"Missing"
2021.emnlp-main.765,P19-1279,0,0.01917,"al., 2020) and there is still no guarantee that the learned clusters will align with the relational semantic classes (Xing et al., 2002). Wu et al. (2019) proposed the relation similarity metrics from labeled data, and then transfers the relational knowledge to identify novel relations in unlabeled data. Different from them, we propose a relation-oriented method explicitly clustering data based on relational information. Knowledge in High-Dimensional Vector. Pretrained static and contextual word representations can provide valuable prior knowledge for constructing relational representations (Soares et al., 2019; Elsahar et al., 2017). Peters et al. (2018) showed that different neural architectures (e.g., LSTM, CNN, and Transformers) can hierarchically structure linguistic information that varies with network depth. Recently, many studies (Jawahar et al., 2019; Clark et al., 2019; Goldberg, 2019) have shown that such hierarchy also exists in pretraining models like BERT. These results suggest that high-dimensional embeddings, independent of model architecture, learn much about the structure of language. Directly clustering on these highdimensional embeddings should hardly produce 9708 STEP 2: Optimiz"
2021.emnlp-main.765,D11-1135,0,0.107936,"Missing"
2021.emnlp-main.765,N07-4013,0,0.0904686,"1: Although both instances S2 and S3 express founded relation while S1 expresses CEO relation, the distance between S1 and S2 is still smaller than that between S2 and S3 . This is because there may be more similar surface information (e.g. word overlapping) or syntactic structure between S1 and S2 , thus the derived clusters cannot explicitly align with relations. in extracting new emerging relational types from open-domain corpora. The approaches used to handle open relations roughly fall into one of two groups. The first group is open information extraction (OpenIE) (Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), which directly extracts related phrases as representations of different relational types. However, if not properly canonicalized, the extracted relational facts can be redundant and ambiguous. The second group 1 Introduction is unsupervised relation discovery (Yao et al., Relation extraction (RE), a crucial basic task 2011; Shinyama and Sekine, 2006; Simon et al., in the field of information extraction, is of the 2019). In this type of research, much attention utmost practical interest to various fields including has been focused on unsupervised clustering-based web sear"
2021.emnlp-main.765,2021.eacl-main.251,0,0.037917,"train the classifier and refine entity pair representation h to encode more contextual relation information. Since it’s difficult to keep the order of clusters consistent in multiple clustering, instead of using standard cross entropy loss, we propose to use the pairwise similarities for novel relation learning. where r is a hyperparameter that denotes the output layer of BERT. s and e represent start and end position of the corresponding entity respectively. ⊕ denotes the concatenation operator. This structure of entity pair representation encoder has been widely used in previous RE methods (Wang et al., 2021; Hu et al., 2020). 3.3 where cr denotes the centroids of relation r. The center loss Lcenter seems reasonable, but problematic. A global optimal solution to minimize Lcenter is g(hi ) = 0, which is far from being desired. This motivates us to incorporate a reconstruction term to prevent the semantic space from collapsing. Specifically„ a decoding network d(·) is used to map the representation h0i back to the original representation hi .Thus, we can derive the following loss function: Relation-Oriented Clustering Module In order to make the distance between representation accurately reflect th"
2021.emnlp-main.765,D19-1021,0,0.371799,"RE methods is attracting lots of attentions. Elsahar et al. (2017) proposed to extract and cluster open relations by re-weighting word embeddings and using the types of named entities as additional features. Hu et al. (2020) proposed to exploit weak, self-supervised signals in pretrained language model for adaptive clustering on contextualized relational features. However, the self-supervised signals are sensitive to the initial representation (Gansbeke et al., 2020) and there is still no guarantee that the learned clusters will align with the relational semantic classes (Xing et al., 2002). Wu et al. (2019) proposed the relation similarity metrics from labeled data, and then transfers the relational knowledge to identify novel relations in unlabeled data. Different from them, we propose a relation-oriented method explicitly clustering data based on relational information. Knowledge in High-Dimensional Vector. Pretrained static and contextual word representations can provide valuable prior knowledge for constructing relational representations (Soares et al., 2019; Elsahar et al., 2017). Peters et al. (2018) showed that different neural architectures (e.g., LSTM, CNN, and Transformers) can hierarc"
2021.emnlp-main.765,2020.acl-main.669,0,0.0179995,") (Elsahar et al., 2017). RW-HAC is a feature clustering method for OpenRE. The model contructs relational feature based on the weighted word embeddings as well as entity types. Discrete-state Variational Autoencoder (VAE) 5 Results and Analysis (Marcheggiani and Titov, 2016). VAE is a reconstruction-based method for OpenRE. The In this section, we present the experimental results model is optimized by reconstructing entities from of our model on two real-world datasets to demonpairing entities and predicted relations. strate the effectiveness of our method. We also Entity Based URE (Etype+) (Tran et al., 2020). provide additional experimental results on hyperEtype+ is a simple and effective method relying parameter analysis and relation representation only on entity types. The same link predictor as in visualization in appendix A and B. 9712 Dataset FewRel TACRED Method VAE(Marcheggiani and Titov, 2016) RW-HAC(Elsahar et al., 2017) EType+(Tran et al., 2020) SelfORE(Hu et al., 2020) RSN(Wu et al., 2019) RSN-BERT RoCORE VAE(Marcheggiani and Titov, 2016) RW-HAC(Elsahar et al., 2017) EType+(Tran et al., 2020) SelfORE(Hu et al., 2020) RSN(Wu et al., 2019) RSN-BERT RoCORE Prec. B3 Rec. F1 Hom. 0.309 0.25"
2021.emnlp-main.765,D17-1004,0,0.062867,"Missing"
2021.findings-acl.203,2020.acl-main.632,0,0.0998607,"Missing"
2021.findings-acl.203,2020.acl-main.371,0,0.0529097,"Missing"
2021.findings-acl.203,2020.emnlp-main.3,0,0.0695418,"Missing"
2021.findings-acl.203,2020.emnlp-main.569,0,0.205178,"responding author. logical context like student essays, public speeches, etc., where only one participant is involved. Online forums such as idebate1 and changemyview2 , enable people to exchange opinions on some specific topics freely. The user generated dataset of interactive arguments also motivates another line of research for argumentation in dialogical context (Asterhan and Schwarz, 2007). Initial researches in this filed focused on analyzing the ChangeMyView data (Tan et al., 2016; Wei et al., 2016) to summarize the key factors of persuasive arguments. Furthermore, Ji et al. (2019) and Cheng et al. (2020) propose the task of identifying and extracting interactive arguments. Ji et al. (2019) formulate this task as a problem of sentence pair scoring and computes the textual similarity 1 https://idebate.org/ https://www.reddit.com/r/ changemyview/ 2 2310 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2310–2319 August 1–6, 2021. ©2021 Association for Computational Linguistics between the two arguments as the result. Such task is then further applied to other fields such as legal domain. For instance, Yuan et al. (2021) organize a challenge aimed to identify the i"
2021.findings-acl.203,D14-1125,0,0.0706235,"Missing"
2021.findings-acl.203,N19-1423,0,0.00667796,"represent the weight vector and the bias respectively. After obtaining the matching score for each argument pair, we treat the task as a sentence pair ranking problem, and use MarginRankingLoss for training: 2313 L= 4 X i=1 max(0, γ − S(q, r+ ) + S(q, ri− )), (12) where S(q, r+ ) refers to the matching score of the positive argument pair while S(q, ri− ) refers to the matching score of the i-th negative argument pair, and γ is the margin hyperparameter. 4 Experiments In this section, we will introduce the dataset, the evaluation metrics, comparative models and experiment results. 4.1 - BERT (Devlin et al., 2019): This method finetunes the pre-trained BERT model for sentencepair classification. Note that this model is not only a baseline model but also a sub-module of our proposed model. Experiment Setup Experimental Dataset We use the dataset constructed in (Ji et al., 2019) for evaluation. The authors find that in the ChangeMyView dataset (Tan et al., 2016), there exist replies that quote sentences from the original post. They extract all these quotation-reply pair q, r from posts in ChangeMyView dataset (Tan et al., 2016). For every interactive argument pair, they randomly sample four negative repl"
2021.findings-acl.203,2020.acl-main.287,0,0.0359314,"Missing"
2021.findings-acl.203,P14-1145,0,0.0580711,"Missing"
2021.findings-acl.203,2020.emnlp-main.716,0,0.0131216,"ath-level). Experiment results indicate that our model achieves state-of-the-art performance in the benchmark dataset. Further analysis demonstrates the effectiveness of our model for enforcing knowledge reasoning through paths in the knowledge graph. 1 Figure 1: Two instances of interactive argument pairs, the related concepts are colored same, and the corresponding knowledge is visualised in the right side. Introduction Argumentation Mining aims at analyzing the semantic and logical structure of argumentative texts. Existing research covers argument structure prediction (Morio et al., 2020; Li et al., 2020), persuasiveness evaluation (Al Khatib et al., 2020; El Baff et al., 2020) and argument summarization (BarHaim et al., 2020b,a). Most of them focus on mono∗ Corresponding author. logical context like student essays, public speeches, etc., where only one participant is involved. Online forums such as idebate1 and changemyview2 , enable people to exchange opinions on some specific topics freely. The user generated dataset of interactive arguments also motivates another line of research for argumentation in dialogical context (Asterhan and Schwarz, 2007). Initial researches in this filed focused"
2021.findings-acl.203,D19-1282,0,0.0112976,"annotation stage to obtain an automatically generated knowledge graph. Leveraging external knowledge in NLU Our work also lies in the general context of using external knowledge to encode sentences and paragraphs. Yang and Mitchell (2017) are among the first researches that retrieve the related entities in the external knowledge base and merge them into an LSTM encoder. Afterward, Weissenborn et al. (2017), Mihaylov and Frank (2018) and Zhang et al. (2020) mainly follows the main idea of the work to incorporate external word-level lexical knowledge to enhance the sentence embedding. Moreover, Lin et al. (2019) propose a knowledge-aware network(KagNet) that utilizes that graph knowledge from ConceptNet to answer the commonsense questions. Compared with these methods, our work utilizes conceptual information from dialogical argumentation lexicons and conducts a reasoning process resembling human beings, which is then encoded by a path transformer, and finally aligned with the semantic information through a hierarchical attention mechanism. 2317 7 Conclusion and Future Work We propose a framework that imitates human’s reasoning process in debating. Practically, we first construct a dialogical argument"
2021.findings-acl.203,P18-1076,0,0.0136484,"wledge graph. Our work obtains inspiration from the construction of Al-Khatib’s knowledge graph, but adapting their method to the dialogical debating forum settings, and removing the human annotation stage to obtain an automatically generated knowledge graph. Leveraging external knowledge in NLU Our work also lies in the general context of using external knowledge to encode sentences and paragraphs. Yang and Mitchell (2017) are among the first researches that retrieve the related entities in the external knowledge base and merge them into an LSTM encoder. Afterward, Weissenborn et al. (2017), Mihaylov and Frank (2018) and Zhang et al. (2020) mainly follows the main idea of the work to incorporate external word-level lexical knowledge to enhance the sentence embedding. Moreover, Lin et al. (2019) propose a knowledge-aware network(KagNet) that utilizes that graph knowledge from ConceptNet to answer the commonsense questions. Compared with these methods, our work utilizes conceptual information from dialogical argumentation lexicons and conducts a reasoning process resembling human beings, which is then encoded by a path transformer, and finally aligned with the semantic information through a hierarchical att"
2021.findings-acl.203,2020.acl-main.298,0,0.0323231,"h entity-level and path-level). Experiment results indicate that our model achieves state-of-the-art performance in the benchmark dataset. Further analysis demonstrates the effectiveness of our model for enforcing knowledge reasoning through paths in the knowledge graph. 1 Figure 1: Two instances of interactive argument pairs, the related concepts are colored same, and the corresponding knowledge is visualised in the right side. Introduction Argumentation Mining aims at analyzing the semantic and logical structure of argumentative texts. Existing research covers argument structure prediction (Morio et al., 2020; Li et al., 2020), persuasiveness evaluation (Al Khatib et al., 2020; El Baff et al., 2020) and argument summarization (BarHaim et al., 2020b,a). Most of them focus on mono∗ Corresponding author. logical context like student essays, public speeches, etc., where only one participant is involved. Online forums such as idebate1 and changemyview2 , enable people to exchange opinions on some specific topics freely. The user generated dataset of interactive arguments also motivates another line of research for argumentation in dialogical context (Asterhan and Schwarz, 2007). Initial researches in t"
2021.findings-acl.203,P16-1030,0,0.0612931,"Missing"
2021.findings-acl.203,P16-2032,1,0.817327,"l Baff et al., 2020) and argument summarization (BarHaim et al., 2020b,a). Most of them focus on mono∗ Corresponding author. logical context like student essays, public speeches, etc., where only one participant is involved. Online forums such as idebate1 and changemyview2 , enable people to exchange opinions on some specific topics freely. The user generated dataset of interactive arguments also motivates another line of research for argumentation in dialogical context (Asterhan and Schwarz, 2007). Initial researches in this filed focused on analyzing the ChangeMyView data (Tan et al., 2016; Wei et al., 2016) to summarize the key factors of persuasive arguments. Furthermore, Ji et al. (2019) and Cheng et al. (2020) propose the task of identifying and extracting interactive arguments. Ji et al. (2019) formulate this task as a problem of sentence pair scoring and computes the textual similarity 1 https://idebate.org/ https://www.reddit.com/r/ changemyview/ 2 2310 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2310–2319 August 1–6, 2021. ©2021 Association for Computational Linguistics between the two arguments as the result. Such task is then further applied to othe"
2021.findings-acl.203,P17-1132,0,0.019062,"such discussion; Khatib et al. (2020) constructs a monological argumentation graph by extracting knowledge from Debatepedia.org and use human annotation to further improve the quality of their knowledge graph. Our work obtains inspiration from the construction of Al-Khatib’s knowledge graph, but adapting their method to the dialogical debating forum settings, and removing the human annotation stage to obtain an automatically generated knowledge graph. Leveraging external knowledge in NLU Our work also lies in the general context of using external knowledge to encode sentences and paragraphs. Yang and Mitchell (2017) are among the first researches that retrieve the related entities in the external knowledge base and merge them into an LSTM encoder. Afterward, Weissenborn et al. (2017), Mihaylov and Frank (2018) and Zhang et al. (2020) mainly follows the main idea of the work to incorporate external word-level lexical knowledge to enhance the sentence embedding. Moreover, Lin et al. (2019) propose a knowledge-aware network(KagNet) that utilizes that graph knowledge from ConceptNet to answer the commonsense questions. Compared with these methods, our work utilizes conceptual information from dialogical argu"
2021.findings-acl.203,2020.acl-main.291,0,0.0277117,"inspiration from the construction of Al-Khatib’s knowledge graph, but adapting their method to the dialogical debating forum settings, and removing the human annotation stage to obtain an automatically generated knowledge graph. Leveraging external knowledge in NLU Our work also lies in the general context of using external knowledge to encode sentences and paragraphs. Yang and Mitchell (2017) are among the first researches that retrieve the related entities in the external knowledge base and merge them into an LSTM encoder. Afterward, Weissenborn et al. (2017), Mihaylov and Frank (2018) and Zhang et al. (2020) mainly follows the main idea of the work to incorporate external word-level lexical knowledge to enhance the sentence embedding. Moreover, Lin et al. (2019) propose a knowledge-aware network(KagNet) that utilizes that graph knowledge from ConceptNet to answer the commonsense questions. Compared with these methods, our work utilizes conceptual information from dialogical argumentation lexicons and conducts a reasoning process resembling human beings, which is then encoded by a path transformer, and finally aligned with the semantic information through a hierarchical attention mechanism. 2317 7"
2021.findings-acl.203,N19-1421,0,0.0226791,".3), which are finally fed into a multi-layer perceptron (MLP) to calculate the final matching score (§3.4). 3.1 Sentence Encoding As for the quotation and reply arguments, it is critical to use the semantic information implied in the texts. Various works have already proved the outstanding performance of pre-trained models in semantic modeling. In our work, we use the BERT model to generate the encoding s for the given argument pair by simply creating a sentence that takes the form of ”[CLS] q [SEP] r [SEP]” and taking the embedding for the ”[CLS]” token, just as suggested by previous works (Talmor et al., 2019). 3.2 Concept Encoding For entities in the argumentation knowledge graph, we need to obtain the representation for each node. We use the BERT model with average pooling to get the initial representation for each entity. Then we encode the conceptual information in both entity-level and path-level with graph networks to enforce the background knowledge modeling and reasoning. 3.2.1 Entity Level Representation To utilize the structural information entailed in the knowledge graph, we apply a 2-layer Graph Convolutional Network (GCN) to it. Here we adopt GCN as it has proved to be both effective a"
2021.findings-emnlp.127,2020.findings-emnlp.211,0,0.0792033,"Missing"
2021.findings-emnlp.127,C10-3014,0,0.0327931,"Missing"
2021.findings-emnlp.127,P16-1014,0,0.0704491,"Missing"
2021.findings-emnlp.127,P18-1039,0,0.0159639,"ategory neighbor same . category If a box of fruit weighs 24 kilograms , how many boxes of pears are less than apples ? Expression: ( 360 / 24 ) – ( 240 / 24 ) – – / 240 / / 24 360 24 360 / 24 240 24 pear boxes apple boxes 240/24 360/24 - / 240 24 / 360 34 (a) Graph2Tree - / 360 24 / 240 24 (b) EEH-G2T Figure 1: An example of a math word problem. The top part of the figure shows the different types of edges connected to the word “pear” in the graph. The bottom part of the figure shows the expressions generated by Graph2Tree (Zhang et al., 2020b) and EEH-G2T. Previous works (Wang et al., 2017; Huang et al., 2018; Wang et al., 2019) used sequenceto-sequence (seq2seq) methods with an attention mechanism (Bahdanau et al., 2014) to generate math expression sequences from math word prob1 Introduction lems. To capture the structural information of math Math word problem solving is an important natural expressions, many works (Liu et al., 2019; Xie language processing (NLP) task that has recently and Sun, 2019; Zhang et al., 2020a) treat math been attracting increasing research interests. Math expressions as binary trees and propose several word problems are narrative text that describe sequence-to-tree (se"
2021.findings-emnlp.127,N16-1136,0,0.0684508,"Missing"
2021.findings-emnlp.127,2020.findings-emnlp.255,0,0.0329546,"Missing"
2021.findings-emnlp.127,P17-1015,0,0.0683501,"Missing"
2021.findings-emnlp.127,D19-1241,0,0.0403052,"Missing"
2021.findings-emnlp.127,P14-5010,0,0.00261207,"nal knowledge bases. An illustrative example is shown in Figure 3. Specifically, given a math word problem X, its dependency tree, and word category information, our model constructs a graph according to the following steps. Edge-labeled Graph 2.2.1 Graph Construction This section introduces how to construct an edgelabeled graph that contains both the local relations between nodes within a sentence and the longrange relations between nodes across sentences. Our model extracts these relations from the problem’s dependency tree and external knowledge base. We use the Stanford Corenlp toolkit 2 (Manning et al., 2014) to parse each math word problem into a dependency tree. The toolkit analyzes the grammatical structure of a sentence and establishes relationships between “head” words and words which modify those heads. In addition, inspired by Wu et al. (2020), we collected word 2 https://stanfordnlp.github.io/CoreNLP/ 1475 • Self node & Neighbor: We define each word xi in the problem X as a node. Each word node xi is connected to its adjacent word nodes (xi−1 , xi+1 ) in the problem. These edges are labeled as “neighbor”. Also, to incorporate the node’s own information into the problem representations, we"
2021.findings-emnlp.127,D14-1162,0,0.090807,"L=− T X log P(yt |y&lt;t ,X). (9) t=1 During the inference, we use beam search to generate final expression. At time step t, if yt is an operator, the current node is an internal node, and the model continues to generate its child nodes. If yt is a number, it represents a leaf node with no child node. Once the children of all the internal nodes have been generated, the generated expression sequence Y= {y1 , y2 , . . . , yT } is transformed into an expression tree, and the decoding process is terminated. 3.1 We used Pytorch for our implementation 3 . We used 300-dimensional Glove word embeddings (Pennington et al., 2014). The hidden size is 512. The batch size is 64. The number of heads M in problem-level aggregation is 8. The number K of split attention vectors is 2. We set the learning rate of the Adam optimizer (Kingma and Ba, 2014) to 0.001, and the dropout is 0.5. During training, it took 120 epochs to train the model. During decoding, we used a beam search with a beam size of 5. We used the same parameter settings for both Math23K and MAWPS datasets. The hyper-parameters are tuned on the valid set. Training We train the model with the cross-entropy loss, defined as: 3 Implementation Details Experiments"
2021.findings-emnlp.127,P19-1423,0,0.0373921,"Missing"
2021.findings-emnlp.127,2020.acl-main.362,0,0.190558,"tate-of-the-art methods. 1 240 kilograms of category neighbor pears category neighbor same . category If a box of fruit weighs 24 kilograms , how many boxes of pears are less than apples ? Expression: ( 360 / 24 ) – ( 240 / 24 ) – – / 240 / / 24 360 24 360 / 24 240 24 pear boxes apple boxes 240/24 360/24 - / 240 24 / 360 34 (a) Graph2Tree - / 360 24 / 240 24 (b) EEH-G2T Figure 1: An example of a math word problem. The top part of the figure shows the different types of edges connected to the word “pear” in the graph. The bottom part of the figure shows the expressions generated by Graph2Tree (Zhang et al., 2020b) and EEH-G2T. Previous works (Wang et al., 2017; Huang et al., 2018; Wang et al., 2019) used sequenceto-sequence (seq2seq) methods with an attention mechanism (Bahdanau et al., 2014) to generate math expression sequences from math word prob1 Introduction lems. To capture the structural information of math Math word problem solving is an important natural expressions, many works (Liu et al., 2019; Xie language processing (NLP) task that has recently and Sun, 2019; Zhang et al., 2020a) treat math been attracting increasing research interests. Math expressions as binary trees and propose severa"
2021.findings-emnlp.127,D18-1132,0,0.015132,"014) to 0.001, and the dropout is 0.5. During training, it took 120 epochs to train the model. During decoding, we used a beam search with a beam size of 5. We used the same parameter settings for both Math23K and MAWPS datasets. The hyper-parameters are tuned on the valid set. Training We train the model with the cross-entropy loss, defined as: 3 Implementation Details Experiments Datasets 3.3 Baselines We compare the performance of our model with the following baselines: DNS (Wang et al., 2017) is a seq2seq model that consists of a two-layer GRU encoder and a two-layer LSTM decoder. MathEN (Wang et al., 2018) is a seq2seq model with a bidirectional LSTM encoder and an attention mechanism. Recu-RNN (Wang et al., 2019) uses recursive neural networks on the predicted tree structure templates. Tree-Dec (Liu et al., 2019) is a seq2tree model with a tree-structured decoder, which generates each node based on its parent and sibling node. GTS (Xie and Sun, 2019) is a seq2tree model that generates expression trees in a goal-driven manner. It generates each node based on its parent node and its left sibling subtree embedding. KA-S2T (Wu et al., 2020) is a graphto-tree model with commonsense knowledge from t"
2021.findings-emnlp.127,D18-1244,0,0.0407093,"Missing"
2021.findings-emnlp.127,D17-1088,0,0.0639743,"Missing"
2021.findings-emnlp.127,2020.emnlp-main.579,1,0.902406,"raph Construction This section introduces how to construct an edgelabeled graph that contains both the local relations between nodes within a sentence and the longrange relations between nodes across sentences. Our model extracts these relations from the problem’s dependency tree and external knowledge base. We use the Stanford Corenlp toolkit 2 (Manning et al., 2014) to parse each math word problem into a dependency tree. The toolkit analyzes the grammatical structure of a sentence and establishes relationships between “head” words and words which modify those heads. In addition, inspired by Wu et al. (2020), we collected word 2 https://stanfordnlp.github.io/CoreNLP/ 1475 • Self node & Neighbor: We define each word xi in the problem X as a node. Each word node xi is connected to its adjacent word nodes (xi−1 , xi+1 ) in the problem. These edges are labeled as “neighbor”. Also, to incorporate the node’s own information into the problem representations, we connect each node to itself and label the edge as “self node”. • Dependency (edges within sentences): The dependency tree is a structured representation that contains various grammatical relationships between word pairs. Following Zhang et al. (2"
2021.findings-emnlp.45,I11-1130,0,0.0686167,"Missing"
2021.findings-emnlp.45,P16-1154,0,0.550006,"ummarization (Wang and Cardie, 2013). Keyphrase Generation (KG) is a classical task In recent years, end to end neural models have for capturing the central idea from a given been widely-used in generating both present and document. Based on Seq2Seq models, the previous reinforcement learning framework on absent keyphrases. Meng et al. (2017) introKG tasks utilizes the evaluation metrics to furduced CopyRNN, which consists of an attentional ther improve the well-trained neural models. encoder-decoder model (Luong et al., 2015) and a However, these KG evaluation metrics such as copy mechanism (Gu et al., 2016). After that, relF1 @5 and F1 @M are only aware of the exact evant works are mainly based on the sequence-tocorrectness of predictions on phrase-level and sequence framework (Yuan et al., 2020; Chen et al., ignore the semantic similarities between simi2018, 2019). Meanwhile, F1 @5 (Meng et al., 2017) lar predictions and targets, which inhibits the and F1 @M (Yuan et al., 2020) are used for evalmodel from learning deep linguistic patterns. In response to this problem, we propose a new uating the model prediction. F1 @5 computes the fine-grained evaluation metric to improve the F1 score with the"
2021.findings-emnlp.45,P19-1208,0,0.121533,"a new uating the model prediction. F1 @5 computes the fine-grained evaluation metric to improve the F1 score with the first five predicted phrases (if the RL framework, which considers different grannumber of phrases is less than five, it will randomly ularities: token-level F1 score, edit distance, append until there are five phrases). F1 @M comduplication, and prediction quantities. On the pares all keyphrases (variable number) predicted whole, the new framework includes two reby the model with the ground truth to compute ward functions: the fine-grained evaluation an F1 score. Furthermore, Chan et al. (2019) utiscore and the vanilla F1 score. This framework helps the model identifying some partial match lize the evaluation scores as the reward function to phrases which can be further optimized as the further optimize the neural model throughout the exact match ones. Experiments on KG benchreinforcement learning (RL) approach. marks show that our proposed training frameHowever, the traditional F1 -like metrics are on work outperforms the previous RL training phrase-level, which can hardly recognize some parframeworks among all evaluation scores. In tial match predictions. For example, supposing ad"
2021.findings-emnlp.45,P06-1068,0,0.0284967,"s a partial match phrase receives the same reward as an exact mismatch phrase. In order to help to recognize these partial match phrases during the training stage, we propose a two-stage RL training method. In the first stage, we use our new metric (F G score or F B score) as a reward to train the model. Then we apply the vanilla RL (using F1 score) training as the second training stage. The whole RL training technique is similar to Chan et al. (2019), while we re-write the reward function. 4 4.1 Experiment Dataset We evaluate our model on three public scientific KG dataset, including Inspec (Hulth and Megyesi, 2006), Krapivin (Krapivin et al., 2009), KP20k (Meng et al., 2017). Each case from these datasets consists of the title, abstract, and a set of scoreLBi = max{BERT(pi , yj )}. (8) yj keyphrases. Following the previous work (Chen et al., 2020), we concatenate the title and abstract where i ∈ [1, P] and j ∈ [1, Y]. Finally, we also as input document, and use the set of keyphrases as put scoreLBi into Algorithm 1 to compute finally labels. The same as the previous works above, we BERT-based score (also called F B score). use the largest dataset, KP20k, to train the model, 501 Model catSeq(Yuan et al.,"
2021.findings-emnlp.45,D18-1439,0,0.269014,"ot. In score, which can also be used in our two-stage RL recent years, end to end neural model has been framework (the red edges in Figure 1). widely-used in generating both present and absent Comparing with the F1 score, our F G score keyphrases. Meng et al. (2017) introduced Copyhas two main advantages: (1) F G score can rec- RNN, which consists of an attentional encoderognize some partial match predictions, which can decoder model (Luong et al., 2015) and a copy 498 mechanism (Gu et al., 2016). After that, relevant works are mainly based on the sequence-tosequence framework. More recently, Chen et al. (2018) leverages the coverage (Tu et al., 2016) mechanism to incorporate the correlation among keyphrases, Chen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only ne"
2021.findings-emnlp.45,2020.acl-main.103,0,0.154604,"with the F1 score, our F G score keyphrases. Meng et al. (2017) introduced Copyhas two main advantages: (1) F G score can rec- RNN, which consists of an attentional encoderognize some partial match predictions, which can decoder model (Luong et al., 2015) and a copy 498 mechanism (Gu et al., 2016). After that, relevant works are mainly based on the sequence-tosequence framework. More recently, Chen et al. (2018) leverages the coverage (Tu et al., 2016) mechanism to incorporate the correlation among keyphrases, Chen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only need to generate some short keyphrases, which means ngram-based metrics (e.g., ROUGE (Lin, 2004), BLEU (Papineni et al., 2002)) may not suitable for evaluations. Therefore, F1 @5 (Meng et al., 2017) and F"
2021.findings-emnlp.45,D14-1179,0,0.00768898,"Missing"
2021.findings-emnlp.45,N19-1423,0,0.0993397,"lack edges show the previous RL ysis to show the effectively of our proposed process and the blue edges show our proposed twoF G metric. stage RL process. Our two-stage RL can be divided into two parts: (1) First, we set F G score as the 2 Related Work adaptive RL reward and use RL technique to train the model; (2) Second, we use F1 score as the re- In this section, we briefly introduce keyphrase genward, which is the same as Chan et al. (2019). Fur- eration models and evaluation metrics. thermore, in order to make F G score smoothly, we 2.1 Keyphrase Generation Models carefully train a BERT (Devlin et al., 2019) model to expand the original F G score from discrete to In KG task, keyphrases can be categorized into two continuous numbers (the green line in Figure 1). types: present and absent, depending on whether This BERT scorer can predict a continuous F G it can be found in the source document or not. In score, which can also be used in our two-stage RL recent years, end to end neural model has been framework (the red edges in Figure 1). widely-used in generating both present and absent Comparing with the F1 score, our F G score keyphrases. Meng et al. (2017) introduced Copyhas two main advantages:"
2021.findings-emnlp.45,D15-1166,0,0.537658,"lustering (Hulth and Megyesi, 2006), and Aiming to generate a set of keyphrases, text summarization (Wang and Cardie, 2013). Keyphrase Generation (KG) is a classical task In recent years, end to end neural models have for capturing the central idea from a given been widely-used in generating both present and document. Based on Seq2Seq models, the previous reinforcement learning framework on absent keyphrases. Meng et al. (2017) introKG tasks utilizes the evaluation metrics to furduced CopyRNN, which consists of an attentional ther improve the well-trained neural models. encoder-decoder model (Luong et al., 2015) and a However, these KG evaluation metrics such as copy mechanism (Gu et al., 2016). After that, relF1 @5 and F1 @M are only aware of the exact evant works are mainly based on the sequence-tocorrectness of predictions on phrase-level and sequence framework (Yuan et al., 2020; Chen et al., ignore the semantic similarities between simi2018, 2019). Meanwhile, F1 @5 (Meng et al., 2017) lar predictions and targets, which inhibits the and F1 @M (Yuan et al., 2020) are used for evalmodel from learning deep linguistic patterns. In response to this problem, we propose a new uating the model prediction"
2021.findings-emnlp.45,P17-1054,0,0.208551,"chool of Computer Science, Fudan University Songhu Road 2005, Shanghai, China {ycluo18,ygxu18,yejc19,xpqiu,qz}@fudan.edu.cn Abstract mining (Wilson et al., 2005; Berend, 2011), document clustering (Hulth and Megyesi, 2006), and Aiming to generate a set of keyphrases, text summarization (Wang and Cardie, 2013). Keyphrase Generation (KG) is a classical task In recent years, end to end neural models have for capturing the central idea from a given been widely-used in generating both present and document. Based on Seq2Seq models, the previous reinforcement learning framework on absent keyphrases. Meng et al. (2017) introKG tasks utilizes the evaluation metrics to furduced CopyRNN, which consists of an attentional ther improve the well-trained neural models. encoder-decoder model (Luong et al., 2015) and a However, these KG evaluation metrics such as copy mechanism (Gu et al., 2016). After that, relF1 @5 and F1 @M are only aware of the exact evant works are mainly based on the sequence-tocorrectness of predictions on phrase-level and sequence framework (Yuan et al., 2020; Chen et al., ignore the semantic similarities between simi2018, 2019). Meanwhile, F1 @5 (Meng et al., 2017) lar predictions and target"
2021.findings-emnlp.45,N18-1158,0,0.027417,"[SEP] is the same as the vanilla BERT (Devlin et al., 2019). In the training stage, scoreLi score is used as the supervised target. After get the BERT scorer, we can easily evaluate the similarity of two keyphrase. Similar to the Eq (7), for a instance (x, Y, P), we compute BERT-based score list scoreLB as follow: 3.5 Reinforcement Learning In this section, we will briefly describe our proposed two-stage reinforcement learning method. 3.5.1 Vanilla RL Training Reinforcement learning has been widely applied to text generation tasks, such as machine translation (Wu et al., 2018), summarization (Narayan et al., 2018), because it can train the model towards a non-differentiable reward. Chan et al. (2019) incorporate reinforce algorithm to optimize the Seq2Seq model with an adaptive reward function. They formulate keyphrase generation as follow. At the time step t = 1, . . . , T , the agent produces an action (token) ybt sampled from the policy (language model) P (b yt |b y&lt;t ), where yb&lt;t represent the sequence generated before step t. After generated t-th tokens, the environment sbt will gives a reward rt (b y&lt;=t , Y) to the agent and updates the next step with a new state sbt+1 = (b y&lt;=t , x, Y). We repe"
2021.findings-emnlp.45,2020.acl-main.710,0,0.0994243,"Missing"
2021.findings-emnlp.45,P02-1040,0,0.10916,"hen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only need to generate some short keyphrases, which means ngram-based metrics (e.g., ROUGE (Lin, 2004), BLEU (Papineni et al., 2002)) may not suitable for evaluations. Therefore, F1 @5 (Meng et al., 2017) and F1 @M (Yuan et al., 2020) are used to evaluate the keyphrases which is predicted by models. This evaluation score is also used as an adaptive reward to improve the performance through reinforcement learning approach (Chan et al., 2019). 3 3.1 Methodology Problem Definition In this section, we will briefly define the keyphrase generation problem. Given a source document x, the objective is to predict a set of keyphrases P = {p1 , p2 , . . . , p|P |} to maximum match the ground-truth keyphrases Y = {y1 , y2 , . . . , y|"
2021.findings-emnlp.45,P16-1008,0,0.0164964,"two-stage RL recent years, end to end neural model has been framework (the red edges in Figure 1). widely-used in generating both present and absent Comparing with the F1 score, our F G score keyphrases. Meng et al. (2017) introduced Copyhas two main advantages: (1) F G score can rec- RNN, which consists of an attentional encoderognize some partial match predictions, which can decoder model (Luong et al., 2015) and a copy 498 mechanism (Gu et al., 2016). After that, relevant works are mainly based on the sequence-tosequence framework. More recently, Chen et al. (2018) leverages the coverage (Tu et al., 2016) mechanism to incorporate the correlation among keyphrases, Chen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only need to generate some short keyphrases, whi"
2021.findings-emnlp.45,P13-1137,0,0.063625,"Missing"
2021.findings-emnlp.45,2021.naacl-main.455,0,0.0221629,"e some partial match predictions, which can decoder model (Luong et al., 2015) and a copy 498 mechanism (Gu et al., 2016). After that, relevant works are mainly based on the sequence-tosequence framework. More recently, Chen et al. (2018) leverages the coverage (Tu et al., 2016) mechanism to incorporate the correlation among keyphrases, Chen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only need to generate some short keyphrases, which means ngram-based metrics (e.g., ROUGE (Lin, 2004), BLEU (Papineni et al., 2002)) may not suitable for evaluations. Therefore, F1 @5 (Meng et al., 2017) and F1 @M (Yuan et al., 2020) are used to evaluate the keyphrases which is predicted by models. This evaluation score is also used as an adaptive reward to improve the performance thr"
2021.findings-emnlp.45,P19-1515,0,0.0116015,"mechanism (Gu et al., 2016). After that, relevant works are mainly based on the sequence-tosequence framework. More recently, Chen et al. (2018) leverages the coverage (Tu et al., 2016) mechanism to incorporate the correlation among keyphrases, Chen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only need to generate some short keyphrases, which means ngram-based metrics (e.g., ROUGE (Lin, 2004), BLEU (Papineni et al., 2002)) may not suitable for evaluations. Therefore, F1 @5 (Meng et al., 2017) and F1 @M (Yuan et al., 2020) are used to evaluate the keyphrases which is predicted by models. This evaluation score is also used as an adaptive reward to improve the performance through reinforcement learning approach (Chan et al., 2019). 3 3.1 Methodology Problem Definition I"
2021.findings-emnlp.45,H05-1044,0,0.0220272,"Missing"
2021.findings-emnlp.45,D18-1397,0,0.0216897,"for BERT scorer, where [CLS] and [SEP] is the same as the vanilla BERT (Devlin et al., 2019). In the training stage, scoreLi score is used as the supervised target. After get the BERT scorer, we can easily evaluate the similarity of two keyphrase. Similar to the Eq (7), for a instance (x, Y, P), we compute BERT-based score list scoreLB as follow: 3.5 Reinforcement Learning In this section, we will briefly describe our proposed two-stage reinforcement learning method. 3.5.1 Vanilla RL Training Reinforcement learning has been widely applied to text generation tasks, such as machine translation (Wu et al., 2018), summarization (Narayan et al., 2018), because it can train the model towards a non-differentiable reward. Chan et al. (2019) incorporate reinforce algorithm to optimize the Seq2Seq model with an adaptive reward function. They formulate keyphrase generation as follow. At the time step t = 1, . . . , T , the agent produces an action (token) ybt sampled from the policy (language model) P (b yt |b y&lt;t ), where yb&lt;t represent the sequence generated before step t. After generated t-th tokens, the environment sbt will gives a reward rt (b y&lt;=t , Y) to the agent and updates the next step with a new"
2021.findings-emnlp.45,2021.acl-long.354,1,0.781165,"of an attentional encoderognize some partial match predictions, which can decoder model (Luong et al., 2015) and a copy 498 mechanism (Gu et al., 2016). After that, relevant works are mainly based on the sequence-tosequence framework. More recently, Chen et al. (2018) leverages the coverage (Tu et al., 2016) mechanism to incorporate the correlation among keyphrases, Chen et al. (2019) enrich the generating stage by utilizing title information, and Chen et al. (2020) proposed hierarchical decoding for better generating keyphrases. In addition, there are some works focus on keyphrase diversity (Ye et al., 2021), selections (Zhao et al., 2021), different module structure (Xu et al., 2021), or linguistic constraints (Zhao and Zhang, 2019). 2.2 Keyphrase Generation Metrics Different to other generation tasks that need to generate long sequences, KG task only need to generate some short keyphrases, which means ngram-based metrics (e.g., ROUGE (Lin, 2004), BLEU (Papineni et al., 2002)) may not suitable for evaluations. Therefore, F1 @5 (Meng et al., 2017) and F1 @M (Yuan et al., 2020) are used to evaluate the keyphrases which is predicted by models. This evaluation score is also used as an adaptive rewar"
2021.naacl-main.115,D14-1162,0,0.081962,"as achieved larger improvement by average, which can be further enhanced by introducing contextualized pre-trained models (e.g. BERT). 3) Incorporating larger-context information with some aggregators also can lead to performance drop on some datasets (e.g, using graph aggregaSettings and Hyper-parameters We adopt CNN-LSTM-CRF as a prototype and augment it with larger-context information by four categories of aggregators: bow, seq, graph, and cPre-seq. We use Word2Vec (Mikolov et al., 2013) (trained on simplified Chinese Wikipedia dump) as noncontextualized embeddings for CWS task, and GloVe (Pennington et al., 2014) for NER, Chunk, and POS tasks. The window size (the number of sentence) k of larger-context aggregators will be explored with a range of k = {1, 2, 3, 4, 5, 6, 10} for seq, bow, and cPre-seq. We chose the best performance that the larger-context aggregator achieved with window 1466 4 The settings of window size k are listed in the appendix. CWS Emb. Agg. NER CITYU NCC SXU PKU CN03 92.26 +0.42 -0.61 +0.34 94.94 +0.03 -0.02 +0.18 94.35 +0.04 +0.33 +0.08 90.46 -0.39 +1.47 -0.14 Chunk POS BC BN MZ WB NW TC 75.38 +1.66 +0.17 +0.65 86.89 +0.32 +0.42 -0.50 85.42 +1.51 -0.16 +1.49 62.09 +3.49 +4.84 +"
2021.naacl-main.115,N18-1202,0,0.0996018,"apid development of deep neural models has and Hovy, 2016; Lample et al., 2016) (RNNs) or shown impressive performances on sequence tag- graph topology by graph neural networks (Kipf ging tasks that aim to assign labels to each token and Welling, 2016; Schlichtkrull et al., 2018). of an input sequence (Sang and De Meulder, 2003; Understanding the discrepancies of these aggreLample et al., 2016; Ma and Hovy, 2016). More gators can help us reach a more generalized conrecently, the use of unsupervised pre-trained modclusion about the effectiveness of larger-context els (Akbik et al., 2018, 2019; Peters et al., 2018; training. To this end, we study larger-context aggreDevlin et al., 2018) (especially contextualized vergators with three different structural priors (defined sion) has driven state-of-the-art performance to a in Sec. 3.2) and comprehensively evaluate their ∗ Corresponding author efficacy. 1463 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1463–1475 June 6–11, 2021. ©2021 Association for Computational Linguistics Q2: Can the larger-context training easily play to its strengths with the help"
2021.naacl-main.115,W03-0419,0,0.170672,"Missing"
2021.naacl-main.115,D17-1283,0,0.0391961,"Missing"
2021.naacl-main.115,D19-1585,0,0.0360579,"Missing"
2021.naacl-main.115,C18-1327,0,0.0153608,"haddar and Langlais, 2018) that utilize different domains of this dataset, which also paves the way for our fine-grained analysis. Chinese Word Segmentation (CWS) We use four mainstream datasets from SIGHAN2005 and SIGHAN2008, in which CITYU is traditional Chinese, while PKU, NCC, and SXU are simplified ones. Chunking (Chunk) CoNLL-2000 (CN00) is a benchmark dataset for text chunking. Part-of-Speech (POS) We use the Penn Treebank (PTB) III dataset for POS tagging.2 2.3 Neural Tagging Models Despite the emergence of a bunch of architectural explorations (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2018; Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018) for sequence tagging, two general frameworks can be summarized: (i) cEnc-wEnc-CRF consists of the wordlevel encoder, sentence-level encoder, and CRF 2 It’s hard to cover all datasets for all tasks. For Chunk and POS tasks, we adopt the two most popular benchmark datasets. 1464 layer (Lafferty et al., 2001); (ii) ContPre-MLP is composed of a contextualized pre-trained layer, followed by an MLP or CRF layer. In this paper, we take both frameworks as study objects for our three research questions first, 3 and instantiate them as two"
2021.naacl-main.115,2021.eacl-main.324,1,0.680233,"Missing"
2021.naacl-main.115,2020.acl-main.306,0,0.0200424,"Missing"
2021.naacl-main.431,P16-2085,0,0.0264799,"tation consists of two posts from change my view, a sub-forum of Reddit.com. Different types of underlines are used to highlight the interactive argument pairs. Introduction Arguments play a central role in decision making on social issues. Striving to automatically understand human arguments, computational argumentation becomes a growing field in natural language processing. It can be analyzed at two levels — monological argumentation and dialogical argumentation. Existing research on monological argumentation covers argument structure prediction (Stab and Gurevych, 2014), claims generation (Bilu and Slonim, 2016), essay scoring (Taghipour and Ng, 2016), etc. Recently, dialogical argumentation becomes an active topic. In the process of dialogical arguments, participants exchange arguments on a given topic (Asterhan and Schwarz, 2007; Hunter, 2013). With the popularity of online debating forums, large volume of dialogical arguments are daily formed, concerning wide range of topics. A social media dialogical argumentation example from ChangeMyView subreddit is shown in Figure 1. There we show two ∗ *Corresponding author posts holding opposite stances over the same topic. One is the original post and the"
2021.naacl-main.431,P16-1150,0,0.0235027,"ise ranking problem. The performance of different models are shown in Table 4. Note that we use the original CMV dataset and follow the previous setup in Tan et al. (2016); Ji et al. (2018). We find that our model outperforms the state-ofthe-art method (Ji et al., 2018) by a large margin, which indicates that our learned representation can well help downstream tasks. 6 Related Work In this section, we will introduce two major areas related to our work, which are dialogical argumentation and argument representation learning. 6.1 Dialogical Argumentation ing the quality of persuasive arguments (Habernal and Gurevych, 2016). Gottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performanc"
2021.naacl-main.431,W18-5211,0,0.0272452,"s reply. As can be seen, opinions from both sides are voiced with multiple arguments and the reply post B is organized in-line with post A’s arguments. Here we define an interactive argument pair formed with two arguments from both sides (with the same underline), which focuses on the same perspective of the discussion topic. The automatic identification of these pairs will be a fundamental step towards the understanding of dialogical argumentative structure. Moreover, it can benefit downstream tasks, such as debate summarization (Sanchan et al., 2017) and logical chain extraction in debates (Botschen et al., 2018). However, it is non-trivial to extract the interactive argument pairs holding opposite stances. Back to the example. Given argument b1 with only four words contained, it is difficult, without richer contextual information, to understand why it has interactive relationship with a1. In addition, without modeling the debating focuses of arguments, it is likely for models to wrongly predict that b2 has interactive relationship with a4 for sharing more words. Motivated by these observations, we pro5467 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computat"
2021.naacl-main.431,N16-1162,0,0.0465159,"Missing"
2021.naacl-main.431,K16-1002,0,0.0151145,"ention mechanism to learn sentence represening the relationship between arguments (Wang and tations (Wang et al., 2017a) and their relations with Cardie, 2014; Persing and Ng, 2017) and evaluat- others (Wang et al., 2017b). Our task is inherently 5474 different from theirs because our target arguments naturally occur in the complex interaction context of dialogues, which requires additional efforts for understanding the discourse structure therein. 6.2 Argument Representation Learning the encoder to encode a particular argument and then using the decoder to decode words in adjacent arguments. Bowman et al. (2016) introduce variational autoencoders to incorporate distributed latent representations of entire arguments. In addition, Hill et al. (2016) propose the FastSent model, using bag-of-words of arguments to predict the adjacent arguments. Logeswaran and Lee (2018) propose the Quick Thoughts to exploit the closeness of adjacent arguments. They formulate the argument representation learning as a classification problem. Argument representation learning for natural language has been studied widely in the past few years. Previous work discuss prior approaches to learning argument representations from la"
2021.naacl-main.431,J96-2004,0,0.875242,"Missing"
2021.naacl-main.431,D17-1070,0,0.0181352,"and Blunsom (2013) explore a language-specific contexts of arguments and induce latent representaencoder applied to each argument and represent tions via discrete variational autoencoders. Experithe argument by the mean vector of the words mental results on the dataset show that our model involved. They consider minimizing the inner significantly outperforms the competitive baselines. product between paired arguments in different Further analyses reveal why our model yields sulanguages as the training objective and do not perior performance and prove the usefulness of rely on word alignments. Conneau et al. (2017) discrete argument representations. propose a model called InferSent, which is used The future work will be carried out in two direcas the baseline as it served as the inspiration for tions. First, we will study the usage of our model the inclusion of the SNLI task in the multitask for applying to other dialogical argumentation remodel. They prove that NLI is an effective task lated tasks, such as debate summarization. Second, for pre-training and transfer learning in obtaining we will utilize neural topic model for learning disgeneric argument representations. They train crete argument repres"
2021.naacl-main.431,K17-1017,0,0.0196911,"the probability distribution of zi over K categories, which contains salient features of the argument on varying aspects. Therefore, we obtain the discrete argument representation by the posterior distribution of discrete latent variables z. R= M X Wei q(zi |x) (7) i=1 3.2 Argumentative Context Modeling Here, we introduce contextual information of the quotation and the reply to help identify the interactive argument pairs. The argumentative context contains a list of arguments. Following previous setting in Ji et al. (2018), we consider each sentence as an argument in the context. Inspired by Dong et al. (2017), we employ a hierarchical architecture to obtain argumentative context representations. Argument-level CNN. Given an argument and their embedding forms {e1 , e2 , ..., en }, we employ a convolution layer to incorporate the context information on word level. si = f (Ws · [ei : ei+ws−1 ] + bs ) where Ws and bs are weight matrix and bias vector. ws is the window size in the convolution layer and si is the feature representation. Then, we conduct an attention pooling operation over all the words to get argument embedding vectors. mi = tanh(Wm · si + bm ) eWu ·mi ui = P eWu ·mj (9) (10) j a= X ui"
2021.naacl-main.431,D13-1191,0,0.0328623,"rmance of different models are shown in Table 4. Note that we use the original CMV dataset and follow the previous setup in Tan et al. (2016); Ji et al. (2018). We find that our model outperforms the state-ofthe-art method (Ji et al., 2018) by a large margin, which indicates that our learned representation can well help downstream tasks. 6 Related Work In this section, we will introduce two major areas related to our work, which are dialogical argumentation and argument representation learning. 6.1 Dialogical Argumentation ing the quality of persuasive arguments (Habernal and Gurevych, 2016). Gottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014)"
2021.naacl-main.431,C18-1314,1,0.788169,"do you believe those aspects to be? ... Figure 2: An example illustrating the formation process of a quotation-reply pair in CMV. extract interactive argument pairs with the relation of quotation-reply. In general, the content of posts 2 Task Definition and Dataset Collection in CMV is informal, making it difficult to parse an In this section, we first define our task of inter- argument in a finer-grain with premise, conclusion and other components. Therefore, following previactive argument pair identification, followed by a description of how we collect the data for this task. ous setting in Ji et al. (2018), we treat each sentence as an argument. Specifically, we only consider the quotation containing one argument and view the 2.1 Task Definition first sentence after the quotation as the reply. We Given a argument q from the original post, a candi- treat the quotation-reply pairs extracted as posidate set of replies consisting of one positive reply tive samples and randomly select four replies from r+ , several negative replies r1− ∼ ru− , and their cor- other posts that are also related to the original post responding argumentative contexts, our goal is to to pair with the quotation as negative"
2021.naacl-main.431,P18-1040,0,0.0257888,"structure in texts. Recently, the dialogical argumen- teractions between two arguments in debate. Howtation has become an active topic. ever, there is limited research on the interactions Dialogical argumentation refers to a series of between posts. In this work, we propose a novel interactive arguments related to a given topic, in- task of identifying interactive argument pairs from volving argument retraction, view exchange, and argumentative posts to further understand the inso on. Existing research covers discourse struc- teractions between posts. Our work is also related ture prediction (Liu et al., 2018), dialog summa- with some similar tasks, such as question answering rization (Hsueh and Moore, 2007), etc. There are and sentence alignment. They focus on the design several attempts to address tasks related to analyz- of attention mechanism to learn sentence represening the relationship between arguments (Wang and tations (Wang et al., 2017a) and their relations with Cardie, 2014; Persing and Ng, 2017) and evaluat- others (Wang et al., 2017b). Our task is inherently 5474 different from theirs because our target arguments naturally occur in the complex interaction context of dialogues, which r"
2021.naacl-main.431,P11-1035,0,0.0212355,"find that our model outperforms the state-ofthe-art method (Ji et al., 2018) by a large margin, which indicates that our learned representation can well help downstream tasks. 6 Related Work In this section, we will introduce two major areas related to our work, which are dialogical argumentation and argument representation learning. 6.1 Dialogical Argumentation ing the quality of persuasive arguments (Habernal and Gurevych, 2016). Gottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014) create a dispute corpus from Wikipedia and use a sentiment analysis to predict the dispute label of arguments. Wei et al. (2016) collect a dataset from CMV an"
2021.naacl-main.431,D14-1162,0,0.0856575,"Missing"
2021.naacl-main.431,sanchan-etal-2017-automatic,0,0.0177199,"s over the same topic. One is the original post and the other is reply. As can be seen, opinions from both sides are voiced with multiple arguments and the reply post B is organized in-line with post A’s arguments. Here we define an interactive argument pair formed with two arguments from both sides (with the same underline), which focuses on the same perspective of the discussion topic. The automatic identification of these pairs will be a fundamental step towards the understanding of dialogical argumentative structure. Moreover, it can benefit downstream tasks, such as debate summarization (Sanchan et al., 2017) and logical chain extraction in debates (Botschen et al., 2018). However, it is non-trivial to extract the interactive argument pairs holding opposite stances. Back to the example. Given argument b1 with only four words contained, it is difficult, without richer contextual information, to understand why it has interactive relationship with a1. In addition, without modeling the debating focuses of arguments, it is likely for models to wrongly predict that b2 has interactive relationship with a4 for sharing more words. Motivated by these observations, we pro5467 Proceedings of the 2021 Conferen"
2021.naacl-main.431,P15-1012,0,0.0276404,"ion can well help downstream tasks. 6 Related Work In this section, we will introduce two major areas related to our work, which are dialogical argumentation and argument representation learning. 6.1 Dialogical Argumentation ing the quality of persuasive arguments (Habernal and Gurevych, 2016). Gottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014) create a dispute corpus from Wikipedia and use a sentiment analysis to predict the dispute label of arguments. Wei et al. (2016) collect a dataset from CMV and analyze the correlation between disputing quality and disputation behaviors. analyze the disputation action in the online debate. Given an or"
2021.naacl-main.431,D14-1006,0,0.0284413,"1 Figure 1: An example of dialogical argumentation consists of two posts from change my view, a sub-forum of Reddit.com. Different types of underlines are used to highlight the interactive argument pairs. Introduction Arguments play a central role in decision making on social issues. Striving to automatically understand human arguments, computational argumentation becomes a growing field in natural language processing. It can be analyzed at two levels — monological argumentation and dialogical argumentation. Existing research on monological argumentation covers argument structure prediction (Stab and Gurevych, 2014), claims generation (Bilu and Slonim, 2016), essay scoring (Taghipour and Ng, 2016), etc. Recently, dialogical argumentation becomes an active topic. In the process of dialogical arguments, participants exchange arguments on a given topic (Asterhan and Schwarz, 2007; Hunter, 2013). With the popularity of online debating forums, large volume of dialogical arguments are daily formed, concerning wide range of topics. A social media dialogical argumentation example from ChangeMyView subreddit is shown in Figure 1. There we show two ∗ *Corresponding author posts holding opposite stances over the sa"
2021.naacl-main.431,D16-1193,0,0.0203612,"e my view, a sub-forum of Reddit.com. Different types of underlines are used to highlight the interactive argument pairs. Introduction Arguments play a central role in decision making on social issues. Striving to automatically understand human arguments, computational argumentation becomes a growing field in natural language processing. It can be analyzed at two levels — monological argumentation and dialogical argumentation. Existing research on monological argumentation covers argument structure prediction (Stab and Gurevych, 2014), claims generation (Bilu and Slonim, 2016), essay scoring (Taghipour and Ng, 2016), etc. Recently, dialogical argumentation becomes an active topic. In the process of dialogical arguments, participants exchange arguments on a given topic (Asterhan and Schwarz, 2007; Hunter, 2013). With the popularity of online debating forums, large volume of dialogical arguments are daily formed, concerning wide range of topics. A social media dialogical argumentation example from ChangeMyView subreddit is shown in Figure 1. There we show two ∗ *Corresponding author posts holding opposite stances over the same topic. One is the original post and the other is reply. As can be seen, opinions"
2021.naacl-main.431,P14-2113,0,0.0260124,"ottipati et al. (2013) use sentiment lexicons as a preprocessing step and propose a probabilistic graphical model to predict stance of arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014) create a dispute corpus from Wikipedia and use a sentiment analysis to predict the dispute label of arguments. Wei et al. (2016) collect a dataset from CMV and analyze the correlation between disputing quality and disputation behaviors. analyze the disputation action in the online debate. Given an original argument and an argument disputing it, they aims to evaluate the quality of a disputing comment based on the original argument and the discussed topic. Habernal and Gurevych (2016) crowdsource the UKPConvArg1 corpus to study what makes an informal social media argument convincing. They crow"
2021.naacl-main.431,S18-1073,0,0.0409519,"Missing"
2021.naacl-main.431,P17-1018,0,0.0168005,"identifying interactive argument pairs from volving argument retraction, view exchange, and argumentative posts to further understand the inso on. Existing research covers discourse struc- teractions between posts. Our work is also related ture prediction (Liu et al., 2018), dialog summa- with some similar tasks, such as question answering rization (Hsueh and Moore, 2007), etc. There are and sentence alignment. They focus on the design several attempts to address tasks related to analyz- of attention mechanism to learn sentence represening the relationship between arguments (Wang and tations (Wang et al., 2017a) and their relations with Cardie, 2014; Persing and Ng, 2017) and evaluat- others (Wang et al., 2017b). Our task is inherently 5474 different from theirs because our target arguments naturally occur in the complex interaction context of dialogues, which requires additional efforts for understanding the discourse structure therein. 6.2 Argument Representation Learning the encoder to encode a particular argument and then using the decoder to decode words in adjacent arguments. Bowman et al. (2016) introduce variational autoencoders to incorporate distributed latent representations of entire ar"
2021.naacl-main.431,W16-2820,1,0.80987,"arguments in their dataset. Park et al. (2011) design several argumentation-motivated features to finish the debate stance classification in Korean newswire discourse. Sridhar et al. (2015) consider the joint stance classification of arguments and relations among them and find a multi-level model will work better. for a combination of post-level and authorlevel collective modeling of both stance and disagreement could bring further improvements in performance. Wang and Cardie (2014) create a dispute corpus from Wikipedia and use a sentiment analysis to predict the dispute label of arguments. Wei et al. (2016) collect a dataset from CMV and analyze the correlation between disputing quality and disputation behaviors. analyze the disputation action in the online debate. Given an original argument and an argument disputing it, they aims to evaluate the quality of a disputing comment based on the original argument and the discussed topic. Habernal and Gurevych (2016) crowdsource the UKPConvArg1 corpus to study what makes an informal social media argument convincing. They crowdsource the UKPConvArg1 corpus and use SVM and bidirectional LSTM to experiment on their annotated datasets. Tan et al. (2016) pa"
2021.naacl-main.431,P17-1190,0,0.038311,"Missing"
2021.naacl-main.431,D17-1026,0,0.0153416,"ork discuss prior approaches to learning argument representations from labelled and unlabelled data. There have been attempts to use laPrevious work focuses on learning continuous arbeled/structured data to learn argument repgument representations with no interpretability. In resentations. Wieting et al. (2016) and Wieting this work, we study the discrete argument represenand Gimpel (2017) introduce a large sentential tations, capturing varying aspects in argumentation paraphrase dataset and use paraphrase data to languages. learn an encoder that maps synonymous phrases to similar embeddings. Wieting et al. (2017) explore the use of machine translation to obtain more 7 Conclusion and Future Work paraphrase data via back-translation of bilingual argument pairs for learning paraphrastic embed- In this paper, we propose a novel task of interactive dings. They show how neural backtranslation argument pair identification from two posts with could be used to generate paraphrases. Hermann opposite stances on a certain topic. We examine and Blunsom (2013) explore a language-specific contexts of arguments and induce latent representaencoder applied to each argument and represent tions via discrete variational a"
2021.naacl-main.431,P18-1101,0,0.0232342,"the two WOF (Ji et al., 2018), the state-of-the-art model to loss terms. The first loss term is defined on the evaluate the persuasiveness of argumentative comDVAE and cross entropy loss is defined as the re- ments, which is tailored to fit our task. In addition, construction loss. We apply the regularization on we compare with some ablations to study the contriKL cost term to solve posterior collapse issue. Due bution from our components. Here we first consider to the space limitation, we leave out the derivation M ATCHrnn , which uses BiGRU to learn argument details and refer the readers to Zhao et al. (2018). representations and explore the match of arguments LDV AE = Eq(z|x) [log p(x|z)]−KL(q(z|x)||p(z)) without modeling the context therein. Then we (19) compare with other ablations that adopt varying The second loss term is defined on the argument argument context modeling methods. Here we conmatching. We formalize this issue as a ranking task sider BiGRU (henceforth M ATCHrnn +Cb ), which 5471 3.4 Joint Learning P@1 MRR 28.36* 28.70* 51.66* 52.03* 31.26* 52.97* 56.04* 73.03* 0.60 0.59 0.58 0.57 0 51.52* 55.98* 57.46* 58.27‡ 58.61‡ 61.17 70.57* 73.20* 73.72* 74.16* 74.66‡ 76.16 Table 2: The per"
2021.semeval-1.183,N19-1213,0,0.0197408,"pages 1283–1288 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics on generating high-quality pseudo labels for target domain samples before or during training phase and do not involve self-supervised pre-training. In the natural language processing filed, pretraining is popular. We train language models on huge corpora and fine-tune the pre-trained architectures (Devlin et al., 2018; Liu et al., 2019; Zhang et al., 2019; Yang et al., 2019) in downstream tasks, achieving state-of-the-art results on most NLP tasks. Prior studies (Radford et al., 2018; Chronopoulou et al., 2019; Gururangan et al., 2020; Lee et al., 2020) has further shown the potential of domain-adaptive and task-adaptive pre-training. 3 Method We approach the task of source-free domain adaptation for negation detection as a problem of learning with pseudo labels. To generate high-quality pseudo labels, we use mean self-entropy as metric to search appropriate probability threshold, which is inspired by (Li et al., 2020). Besides, in order to learn more negation semantic knowledge from target domain, we propose negation-aware pre-training to incorporate negation knowledge by self-supervised training."
2021.semeval-1.183,P19-1139,0,0.0152667,"d for image classification task. These methods mostly focus 1283 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 1283–1288 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics on generating high-quality pseudo labels for target domain samples before or during training phase and do not involve self-supervised pre-training. In the natural language processing filed, pretraining is popular. We train language models on huge corpora and fine-tune the pre-trained architectures (Devlin et al., 2018; Liu et al., 2019; Zhang et al., 2019; Yang et al., 2019) in downstream tasks, achieving state-of-the-art results on most NLP tasks. Prior studies (Radford et al., 2018; Chronopoulou et al., 2019; Gururangan et al., 2020; Lee et al., 2020) has further shown the potential of domain-adaptive and task-adaptive pre-training. 3 Method We approach the task of source-free domain adaptation for negation detection as a problem of learning with pseudo labels. To generate high-quality pseudo labels, we use mean self-entropy as metric to search appropriate probability threshold, which is inspired by (Li et al., 2020). Besides, in order to le"
2021.semeval-1.183,2021.ccl-1.108,0,0.0926825,"Missing"
2021.semeval-1.183,W02-0109,0,0.270677,"re added to perform masked language modeling and negation cue prediction. During pre-training, the learning rate is set as 0.00001, and batch size is set to be 64. We conduct pre-training in 3 epochs. 5.2 Pseudo Label Training Stage We assume that a sample including no negation cue is definitely non-negated. Thus, we assign these sample without negation cues non-negated label. These samples will not be included in the training phase. Since the provided test samples are extracted directly from NOTEEVENTS.csv file, the format of each sample is messy, we conduct sentence split with NLTK toolkit (Loper and Bird, 2002) for each sample and only keep the sentence with target mention. Then confidence threshold search is conducted to generate high-quality labels for the rest of test data. Finally, the confidence threshold for negated and non-negated samples are selected as 0.983 and 0.999 respectively. In other words, if a test sample assigned negated label, the probability to be negated generated by the source model should be higher than 0.983. Similarly, if a test sample assigned non-negated label, the probability to be nonnegated should be higher than 0.999. During training stage, we use the source model pro"
2021.semeval-1.183,D16-1078,0,0.0290714,"tion from clinical text. The test dataset used is based on MIMICIII version 1.4 (Johnson et al., 2016), which is a large, freely-available english database comprising de-identified health-related data. Related Work Traditional negation detection method are mostly rule-based. These methods (Chapman et al., 2001; Sanchez-Graillet and Poesio, 2007; Huang and Lowe, 2007; Sohn et al., 2012) used regular expression algorithm, dependency parsing and grammatical parsing to perform negation cue detection and scope resolution. Recent years, deep learning has been applied to negation detection task. In (Qian et al., 2016), Convolutional Neural Network was used to recognize negation scope in the sentence. (Lazib et al., 2019) and (Gautam et al., 2018) leveraged recurrent neural network variants to perform negation scope resolution and achieved better performance with BiLSTM, which further indicates the potential in deep learning-based methods. Joint model to detect negation cues and targets simultaneously had been studied by Bhatia et al. (2019). More recently, popular transformer-based model (Khandelwal and Sawant, 2019) had also been used to perform negation detection. Due to data privacy and data transmissio"
C10-1102,W09-1210,0,0.127431,"al applications, decoding speed is very important. Modern structured learning technique adopts template based method to generate millions of features. Such as shallow parsing (Sha and Pereira, 2003), named entity recognition (Kazama and Torisawa, ), dependency parsing (McDonald et al., 2005), etc. The problem arises when the number of templates increases, more features generated, making the extraction step time consuming. Especially for maximum spanning tree (MST) dependency parsing, since feature extraction requires quadratic time even using a first order model. According to Bohnet’s report (Bohnet, 2009), a fast feature extraction beside of a fast parsing algorithm is important for the parsing and training speed. He takes 3 measures for a 40X speedup, despite the same inference algorithm. One important measure is to store the feature vectors in file to skip feature extraction, otherwise it will be the bottleneck. Now we quickly review the feature extraction stage of structured learning. Typically, it consists of 2 steps. First, features represented by strings are generated using templates. Then a feature indexing structure searches feature indexes to get corresponding feature weights. Figure"
C10-1102,P09-1087,0,0.0429088,"Missing"
C10-1102,P08-2060,0,0.14195,"f words) is much larger than standard double array Trie, which has only 256 children, i.e, range of a byte. Therefore, inserting a 2D Trie node is more strict, yielding sparser double arrays. 5.3 Comparison against state-of-the-art Recent works on dependency parsing speedup mainly focus on inference, such as expected linear time non-projective dependency parsing (Nivre, 2009), integer linear programming (ILP) for higher order non-projective parsing (Martins et al., 2009). They achieve 0.632 seconds per sentence over several languages. On the other hand, Goldberg and Elhadad proposed splitSVM (Goldberg and Elhadad, 2008) for fast low-degree polynomial kernel classifiers, and applied it to transition based parsing (Nivre, 2003). They achieve 53 sentences per second parsing speed on English corpus, which is faster than our results, since transition based parsing is linear time, while for graph based method, complexity of feature extraction is quadratic. Xavier Llu´ıs et al. (Llu´ıs et al., 2009) achieve 8.07 seconds per sentence speed on CoNLL09 (Hajiˇc et al., 2009) Chinese Tree Bank test data with a second order graphic model. Bernd Bohnet (Bohnet, 2009) also uses second order model, and achieves 610 minutes"
C10-1102,P09-1040,0,0.0279857,"set, yielding 13.4 sentences per second parsing speed, about 4.3 times faster. Space requirement of 2D Trie is about 2.1 times as binary search, and 1.7 times as Trie. One possible reason is that column number of 2D Trie (e.g. size of words) is much larger than standard double array Trie, which has only 256 children, i.e, range of a byte. Therefore, inserting a 2D Trie node is more strict, yielding sparser double arrays. 5.3 Comparison against state-of-the-art Recent works on dependency parsing speedup mainly focus on inference, such as expected linear time non-projective dependency parsing (Nivre, 2009), integer linear programming (ILP) for higher order non-projective parsing (Martins et al., 2009). They achieve 0.632 seconds per sentence over several languages. On the other hand, Goldberg and Elhadad proposed splitSVM (Goldberg and Elhadad, 2008) for fast low-degree polynomial kernel classifiers, and applied it to transition based parsing (Nivre, 2003). They achieve 53 sentences per second parsing speed on English corpus, which is faster than our results, since transition based parsing is linear time, while for graph based method, complexity of feature extraction is quadratic. Xavier Llu´ıs"
C10-1102,N03-1028,0,0.0397539,"r than traditional Trie structure, making parsing speed 4.3 times faster. 1 Feature: lucky/ADJ Feature Retrieval Parse Tree Index: 3228~3233 Build lattice, inference etc. Figure 1: Flow chart of dependency parsing. p0 .word, p0 .pos denotes the word and POS tag of parent node respectively. Indexes correspond to the features conjoined with dependency types, e.g., lucky/ADJ/OBJ, lucky/ADJ/NMOD, etc. Introduction In practical applications, decoding speed is very important. Modern structured learning technique adopts template based method to generate millions of features. Such as shallow parsing (Sha and Pereira, 2003), named entity recognition (Kazama and Torisawa, ), dependency parsing (McDonald et al., 2005), etc. The problem arises when the number of templates increases, more features generated, making the extraction step time consuming. Especially for maximum spanning tree (MST) dependency parsing, since feature extraction requires quadratic time even using a first order model. According to Bohnet’s report (Bohnet, 2009), a fast feature extraction beside of a fast parsing algorithm is important for the parsing and training speed. He takes 3 measures for a 40X speedup, despite the same inference algorit"
C10-1102,D07-1033,0,0.0582839,"Missing"
C10-1102,W09-1212,0,0.032854,"Missing"
C10-1102,P09-1039,0,0.131367,"quirement of 2D Trie is about 2.1 times as binary search, and 1.7 times as Trie. One possible reason is that column number of 2D Trie (e.g. size of words) is much larger than standard double array Trie, which has only 256 children, i.e, range of a byte. Therefore, inserting a 2D Trie node is more strict, yielding sparser double arrays. 5.3 Comparison against state-of-the-art Recent works on dependency parsing speedup mainly focus on inference, such as expected linear time non-projective dependency parsing (Nivre, 2009), integer linear programming (ILP) for higher order non-projective parsing (Martins et al., 2009). They achieve 0.632 seconds per sentence over several languages. On the other hand, Goldberg and Elhadad proposed splitSVM (Goldberg and Elhadad, 2008) for fast low-degree polynomial kernel classifiers, and applied it to transition based parsing (Nivre, 2003). They achieve 53 sentences per second parsing speed on English corpus, which is faster than our results, since transition based parsing is linear time, while for graph based method, complexity of feature extraction is quadratic. Xavier Llu´ıs et al. (Llu´ıs et al., 2009) achieve 8.07 seconds per sentence speed on CoNLL09 (Hajiˇc et al.,"
C10-1102,P05-1012,0,0.0376088,"Feature Retrieval Parse Tree Index: 3228~3233 Build lattice, inference etc. Figure 1: Flow chart of dependency parsing. p0 .word, p0 .pos denotes the word and POS tag of parent node respectively. Indexes correspond to the features conjoined with dependency types, e.g., lucky/ADJ/OBJ, lucky/ADJ/NMOD, etc. Introduction In practical applications, decoding speed is very important. Modern structured learning technique adopts template based method to generate millions of features. Such as shallow parsing (Sha and Pereira, 2003), named entity recognition (Kazama and Torisawa, ), dependency parsing (McDonald et al., 2005), etc. The problem arises when the number of templates increases, more features generated, making the extraction step time consuming. Especially for maximum spanning tree (MST) dependency parsing, since feature extraction requires quadratic time even using a first order model. According to Bohnet’s report (Bohnet, 2009), a fast feature extraction beside of a fast parsing algorithm is important for the parsing and training speed. He takes 3 measures for a 40X speedup, despite the same inference algorithm. One important measure is to store the feature vectors in file to skip feature extraction,"
C10-1102,W03-3017,0,0.0209086,"rting a 2D Trie node is more strict, yielding sparser double arrays. 5.3 Comparison against state-of-the-art Recent works on dependency parsing speedup mainly focus on inference, such as expected linear time non-projective dependency parsing (Nivre, 2009), integer linear programming (ILP) for higher order non-projective parsing (Martins et al., 2009). They achieve 0.632 seconds per sentence over several languages. On the other hand, Goldberg and Elhadad proposed splitSVM (Goldberg and Elhadad, 2008) for fast low-degree polynomial kernel classifiers, and applied it to transition based parsing (Nivre, 2003). They achieve 53 sentences per second parsing speed on English corpus, which is faster than our results, since transition based parsing is linear time, while for graph based method, complexity of feature extraction is quadratic. Xavier Llu´ıs et al. (Llu´ıs et al., 2009) achieve 8.07 seconds per sentence speed on CoNLL09 (Hajiˇc et al., 2009) Chinese Tree Bank test data with a second order graphic model. Bernd Bohnet (Bohnet, 2009) also uses second order model, and achieves 610 minutes on CoNLL09 English data (2399 sentences, 15.3 second per sentence). Although direct comparison of parsing ti"
C10-1102,W09-1201,0,\N,Missing
C12-2027,P09-1082,0,0.0137345,"to as a vocabulary gap problem. Tweet At the WWDC conference 2012, Apple introduces its new operating system release-Lion. Annotated tags Apple Inc, WWDC, MAC OS Lion Table 1: An example of a tweet with annotated hashtags. Tweet˖At the WWDC conference 2012, Apple introduces its new opera!ng system release-Lion. Word alignment Tags˖ Apple Inc, WWDC, MAC OS Lion Figure 1: The basic idea of word alignment method for suggesting hashtags. To solve the vocabulary gap problem, most researchers applied a statistic machine translation model to learn the word alignment probabilities(Zhou et al., 2011; Bernhard and Gurevych, 2009). Liu et al. (2011) proposed a simple word alignment method to suggest tags for book reviews and online bibliographies. In this work, tags are trigged by the important words of the resource. Figure 1 shows the basic idea of using word alignment method for tag suggestion. Due to the open access in microblogs, topics tend to be more diverse in microblogs than in formal documents. However, all the existing models did not take into account any contextual information in modeling word translation probabilities. Beyond word-level, contextual-level topical information can help word-alignment choice be"
C12-2027,J93-2003,0,0.0269332,"B t,w,c have a potential size of W T K, assuming the vocabulary sizes for words, hashtags and topics are W , T and K. The data sparsity poses a more serious problem in estimating B t,w,c than the topic-free word alignment case. To reduce the data sparsity problem, we introduce the remedy in our model. We can employ a linear interpolation with topic-free word alignment probability to avoid data sparseness: B ∗t,w,c = λB t,w,c + (1 − λ)P(t|w), where P(t|w) is topic-free word alignment probability from the word w and the hashtag t, λ is tradeoff of two probabilities. Here we explore IBM model-1 (Brown et al., 1993), which is a widely used word alignment model, to obtain P(t|w). 3.3 Tag recommendation using Topic-specific translation probabilities 3.3.1 Topic identification Suppose given an unlabeled dataset W∗ = {w d∗ }Ud=1 with U tweets, where the dth tweet w d∗ = Ld ∗ Ld {w dn }n=1 consists of L d words. zd∗ = {zd∗ n }n=1 denotes topics of words in dth tweet and Z∗ = ∗ U {zd }d=1 . we first identify topics for each tweet using the standard LDA model. The collapsed Gibbs sampling is also applied for inference. After the topics of each word become stable, we can estimate the distribution of topic choice"
C12-2027,C10-2028,0,0.159363,": Posters, pages 265–274, COLING 2012, Mumbai, December 2012. 265 1 Introduction Hashtags, which are usually prefixed with the symbol # in microblogging services, represent the relevance of a tweet to a particular group, or a particular topic (Kwak et al., 2010). Popularity of hashtags grows concurrently with the rise and popularity of microblogging services. Many microblog posts contain a wide variety of user-defined hashtags. It has been proven to be useful for many applications, including microblog retrieval (Efron, 2010), query expansion (A.Bandyopadhyay et al., 2011), sentiment analysis (Davidov et al., 2010; Wang et al., 2011), and many other applications. However, not all posts are marked with hashtags. How to automatically generate or recommend hashtags has become an important research topic. The task of hashtag recommendation is to automatically generate hashtags for a given tweet. It is similar to the task of keyphrase extraction, but it has several different aspects. Keyphrases are defined as a short list of phrases to capture the main topics of a given document (Turney, 2000). Keyphrases are usually extracted from the given document. However, hashtags indicate where a tweet is about a part"
C12-2027,W03-1028,0,0.151163,"Missing"
C12-2027,D11-1146,0,0.419161,"m. Tweet At the WWDC conference 2012, Apple introduces its new operating system release-Lion. Annotated tags Apple Inc, WWDC, MAC OS Lion Table 1: An example of a tweet with annotated hashtags. Tweet˖At the WWDC conference 2012, Apple introduces its new opera!ng system release-Lion. Word alignment Tags˖ Apple Inc, WWDC, MAC OS Lion Figure 1: The basic idea of word alignment method for suggesting hashtags. To solve the vocabulary gap problem, most researchers applied a statistic machine translation model to learn the word alignment probabilities(Zhou et al., 2011; Bernhard and Gurevych, 2009). Liu et al. (2011) proposed a simple word alignment method to suggest tags for book reviews and online bibliographies. In this work, tags are trigged by the important words of the resource. Figure 1 shows the basic idea of using word alignment method for tag suggestion. Due to the open access in microblogs, topics tend to be more diverse in microblogs than in formal documents. However, all the existing models did not take into account any contextual information in modeling word translation probabilities. Beyond word-level, contextual-level topical information can help word-alignment choice because sometimes tra"
C12-2027,P11-1066,0,0.00966198,"is usually refered to as a vocabulary gap problem. Tweet At the WWDC conference 2012, Apple introduces its new operating system release-Lion. Annotated tags Apple Inc, WWDC, MAC OS Lion Table 1: An example of a tweet with annotated hashtags. Tweet˖At the WWDC conference 2012, Apple introduces its new opera!ng system release-Lion. Word alignment Tags˖ Apple Inc, WWDC, MAC OS Lion Figure 1: The basic idea of word alignment method for suggesting hashtags. To solve the vocabulary gap problem, most researchers applied a statistic machine translation model to learn the word alignment probabilities(Zhou et al., 2011; Bernhard and Gurevych, 2009). Liu et al. (2011) proposed a simple word alignment method to suggest tags for book reviews and online bibliographies. In this work, tags are trigged by the important words of the resource. Figure 1 shows the basic idea of using word alignment method for tag suggestion. Due to the open access in microblogs, topics tend to be more diverse in microblogs than in formal documents. However, all the existing models did not take into account any contextual information in modeling word translation probabilities. Beyond word-level, contextual-level topical information can"
C12-2027,W04-3252,0,\N,Missing
C14-1021,J93-2003,0,0.0649756,"the microblogs. where Nz,w For the probability alignment ϕ between hashtag and word, the potential size is W · V · K. The data sparsity poses a more serious problem in estimating ϕ than the topic-free word alignment case. To remedy the problem, we use interpolation smoothing technique for ϕ. In this paper, we emplogy smoothing as follows: ϕ∗h,w,z = γϕh,w,z + (1 − γ)P (h|w), (4) where ϕ∗h,w,z is the smoothed topical alignment probabilities, ϕh,w,z is the original topical alignment probabilities. P (h|w) is topic-free word alignment probability. Here we obtain P (h|w) by exploring IBM model-1 (Brown et al., 1993). γ is trade-off of two probabilities ranging from 0.0 to 1.0. When γ = 0.0, ϕ∗h,w,z will be reduce to topic-free word alignment probability; and when γ = 1.0, there will be no smoothing in ϕ∗h,w,z . For the word itself there are no smoothing, because it is a pseudo-count. 3.3 Hashtag Extraction We perform hashtag extraction as follows. Suppose given an unlabeled dataset, we perform Gibbs Sampling to iteratively estimate the topic and determine topic/background words for each microblog. The process is the same as described in Section 3.2. After the hidden variables of topic/background words an"
C14-1021,C10-2028,0,0.0381562,"Missing"
C14-1021,P11-1016,0,0.0268478,"Missing"
C14-1021,C12-1105,0,0.444475,"egg design Leo Taurus like saint sunday employees together film yourself life husband loyalty incentive street children parent Aries Venus Apple happyness like charges Pluto Horoscope iphone pumpkin 0 2012-04 2012-06 2012-08 2012-10 2012-12 2013-02 2013-04 Figure 1: An example of the topics of retweets in each month. Each colored stripe represents a topic, whose height is the number of words assigned to the topic. For each topic, the top words of this topic in each month are placed on the stripe. Motivated by the methods proposed to handle the vocabulary gap problem for keyphrase extraction (Liu et al., 2012) and hashtag suggestion (Ding et al., 2013), in this work, we also assume that the hashtags and textual content in a microblog are parallel descriptions of the same thing in different languages. To model the document themes, in this paper, we adopt the topical translation model to facilitate the translation process. Topic-specific word triggers are used to bridge the gap between the words and hashtags. Since existing topical translation models can only recommend hashtags learned from the training data, we also incorporate an extraction process into the model. This work makes three main contrib"
C14-1021,J03-1002,0,0.038208,"uly assigned” among “hashtags manually assigned”. F1-score is the harmonic mean of precision and recall. We do 500 iterations of Gibbs sampling to train the model. For optimize the hyperparmeters of the proposed method and alternative methods, we use 5-fold cross-validation in the training data to do it. The number of topics is set to 70. The other settings of hyperparameters are as follows: α = 50/K, β w = 0.1, β h = 0.1, λ = 0.01, and δ = 0.01. The smoothing factor γ in Eq.(3) is set to 0.6. For estimating the translation probability without topical information, we use GIZA++ 1.07 to do it (Och and Ney, 2003). For baselines, we compare the proposed model with the following alternative models. • TWTM: Topical word trigger model (TWTM) was proposed by Liu et al. for keyphrase extraction using only textual information (Liu et al., 2012). We implemented the model and used it to achieve the task. • TTM: Ding et al. (2013) proposed the topical translation model (TTM) for hash tag extraction. We implemented and extended their method for evaluating it on the corpus constructed in this work. 4.3 Experimental Results Table 2 shows the comparisons of the proposed method with the state-of-the-art methods on t"
C14-1021,P11-1039,0,\N,Missing
C14-1065,P11-1016,0,0.116116,"Missing"
C14-1065,W11-2213,0,0.0277192,"is fundamental problems in many NLP applications. The task aims to distinguish the real world relevant of a given name with the same surface in context. WePS36 (Amig´o et al., 2010) and RepLab 20137 (Amig´o et al., 2013) evaluation campaigns have also addressed the problem from the perspective of disambiguation organization names in microblogs. Pedersen et al. (2006) proposed an unsupervised method for name discrimination. Yerva et al. (2010) used support vector machines (SVM) classiﬁer with various external resources, such as WordNet, metadata proﬁle, category proﬁle, Google set, and so on. Kozareva and Ravi (2011) proposed to use latent dirichlet allocation to incorporate topical information. Zhang et al. (2012) proposed to use adaptive method for this task. However, most of these methods focused on the text with predeﬁned surface words. The documents which do not contain organization names or person names can not be well processed by these methods. To bridge the vocabulary gap between content and hashtags, Liu et al. (2012b) proposed to use translation model to handle it. They modeled the tag suggestion task as a translation process from 6 7 http://nlp.uned.es/weps/weps-3 http://www.limosine-project.e"
C14-1065,D12-1123,0,0.0140633,". Yerva et al. (2010) used support vector machines (SVM) classiﬁer with various external resources, such as WordNet, metadata proﬁle, category proﬁle, Google set, and so on. Kozareva and Ravi (2011) proposed to use latent dirichlet allocation to incorporate topical information. Zhang et al. (2012) proposed to use adaptive method for this task. However, most of these methods focused on the text with predeﬁned surface words. The documents which do not contain organization names or person names can not be well processed by these methods. To bridge the vocabulary gap between content and hashtags, Liu et al. (2012b) proposed to use translation model to handle it. They modeled the tag suggestion task as a translation process from 6 7 http://nlp.uned.es/weps/weps-3 http://www.limosine-project.eu/events/replab2013 694 Table 4: The inﬂuence of the smoothing parameter σ of the propose method. σ Precision Recall F1 0.0 0.2 0.4 0.6 0.8 1.0 0.471 0.490 0.495 0.511 0.522 0.519 0.432 0.449 0.454 0.468 0.479 0.476 0.451 0.469 0.474 0.489 0.500 0.496 0.7 0.6 F1-Score 0.5 0.4 0.3 0.2 0.1 0 1 NB 2 3 SVM 4 5 IBM1 6 TTM 7 8 9 Our w/o BG 10 Our Figure 2: Evaluation results of NB, SVM, IBM1, TTM, and our method on the d"
C14-1065,C12-1105,0,0.0295422,". Yerva et al. (2010) used support vector machines (SVM) classiﬁer with various external resources, such as WordNet, metadata proﬁle, category proﬁle, Google set, and so on. Kozareva and Ravi (2011) proposed to use latent dirichlet allocation to incorporate topical information. Zhang et al. (2012) proposed to use adaptive method for this task. However, most of these methods focused on the text with predeﬁned surface words. The documents which do not contain organization names or person names can not be well processed by these methods. To bridge the vocabulary gap between content and hashtags, Liu et al. (2012b) proposed to use translation model to handle it. They modeled the tag suggestion task as a translation process from 6 7 http://nlp.uned.es/weps/weps-3 http://www.limosine-project.eu/events/replab2013 694 Table 4: The inﬂuence of the smoothing parameter σ of the propose method. σ Precision Recall F1 0.0 0.2 0.4 0.6 0.8 1.0 0.471 0.490 0.495 0.511 0.522 0.519 0.432 0.449 0.454 0.468 0.479 0.476 0.451 0.469 0.474 0.489 0.500 0.496 0.7 0.6 F1-Score 0.5 0.4 0.3 0.2 0.1 0 1 NB 2 3 SVM 4 5 IBM1 6 TTM 7 8 9 Our w/o BG 10 Our Figure 2: Evaluation results of NB, SVM, IBM1, TTM, and our method on the d"
C14-1065,P13-1172,0,0.0781602,"Missing"
C14-1065,H05-1043,0,0.168454,"Missing"
C14-1065,J11-1002,0,0.0957434,"Missing"
C14-1065,D09-1159,1,0.889039,"Missing"
C14-1065,C10-2167,0,0.0545261,"Missing"
C14-1065,Y12-1025,0,0.166176,"ceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 688–697, Dublin, Ireland, August 23-29 2014. The WePS-32 (Amig´o et al., 2010) and RepLab 20133 (Amig´o et al., 2013) evaluation campaigns also addressed the problem from the perspective of the disambiguation of company names in microblogs. Microblogs that contain company names at a lexical level are classiﬁed based on whether it refers to the company or not. Various approaches have been proposed to address the task with different methods (Pedersen et al., 2006; Yerva et al., 2010; Zhang et al., 2012; Spina et al., 2012; Spina et al., 2013). However, the microblogs that do not contain company names cannot be correctly processed using these methods. From analyzing the data, we observe that a variety of microblog posts belong to this type. They only contain products names, slang terms, and other related company content. To achieve this task, in this paper, we propose the use of a translation based model to identify the targets of microblogs. We assume that the microblog posts and targets describe the same topic using different languages. Hence, the target identiﬁcation problem can be regard"
C14-1065,H05-2017,0,\N,Missing
C16-1090,D11-1146,0,0.0276258,"ariety of circumstances. Hashtagged words that become very popular are often trending topics. Various works have also shown that hashtags can provide valuable information about different problems such as twitter spammer detection (Benevenuto et al., 2010), popularity prediction (Tsur and Rappoport, 2012), and sentiment analysis (Wang et al., 2011). With the increasing requirements, the hashtag recommendation task has received considerable attention in recent years. Discriminative models have been proposed from different aspects using various kinds of features and models (Heymann et al., 2008; Liu et al., 2011), collaborative filtering (Kywe et al., 2012), generative models (Ding et al., 2013; Godin et al., 2013; She and Chen, 2014), and convolutional neural networks (CNN) (Gong and Zhang, 2016). Some of the previous works treated this task as a multiclass classification problem and used word-level features and exquisitely designed patterns to perform the task. Numerous existing studies utilized the word trigger assumption (Liu et al., 2011; Ding et al., 2013) and introduced topical machine translation models to achieve the task. Due to the advantages of deep neural networks and the effectiveness of"
C16-1090,C14-1021,1,\N,Missing
C18-1074,baccianella-etal-2010-sentiwordnet,0,0.278007,"s on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods. 1 Introduction Sentiment analysis has the goal of analyzing people’s sentiments or opinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentiment classification tasks, sentiment words such as great and terrible tend to play a more critical role than other words in texts (Liu, 2010). One attribute of words annotated in"
C18-1074,D16-1171,0,0.520128,", in this work, we propose a novel lexicon-based supervised attention model (LBSA), which allows a recurrent neural network to focus on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods. 1 Introduction Sentiment analysis has the goal of analyzing people’s sentiments or opinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentiment classification tasks, sentiment word"
C18-1074,D16-1057,0,0.0571043,"Missing"
C18-1074,C16-1291,0,0.0151058,"erage number of sentences in a document, while Words/sent denotes the average length of a sentence. 2.3 Jointly Supervised Classification and Attention The final document vector d is used to conduct sentiment classification, which requires the probability of labeling a document with sentiment polarity c, c ∈ [1, C], where C represents the total number of classes. The probability is computed by a softmax function: p = sof tmax(Wc d + bc ). (11) To jointly supervise the classification and attention, we introduce a soft constraint method that is employed by other tasks with supervised attention (Liu et al., 2016). Specifically, it is defined as follows: L=− C X c=1 gc log(pc ) + µw · T X i=1 4(a?i , ai ) + µs · 4(a? , a), (12) where the classification loss function is cross entropy. gc ∈ RC denotes the ground truth of the sentiment classification, presented by a one-hot vector. pc ∈ RC is the predicted probability for each class. a?i and a? are gold attention vectors for the word and sentence levels, respectively. ai and a are learned attention vectors for the word and sentence levels, respectively. 4 is a loss function that indicates the disagreement between two vectors. Because the learned attention"
C18-1074,D17-1048,0,0.333232,"mechanism with such sentiment information. Yang et al. (2016) and Chen et al. (2016) proposed a hierarchical RNN model to learn attention weights based on the local context using an unsupervised method. However, their method may induce much noise and suffers from a lack of interpretability because it tends to capture some domain-specific words (Mudinas et al., 2012) instead of real sentiment information. For example, in movie reviews, the name of a movie with a good reputation tends to be regarded as positive words, and is thereby assigned higher weights, which does not work in other domains. Long et al. (2017) incorporated the reading time of human beings into the attention mechanism, but their method also has much noise because during the reading process, people tend to spend more time on intricate contents (Goodman, 1988) than on real sentiment words like good and bad. Other methods employed external information such as users and products to guide attention weights (Ma et al., 2017; Chen et al., 2016), which can boost the classification performance by a large margin. However, most of the time, we have no access to such external information. In this paper, we propose a novel Lexicon-Based Supervis"
C18-1074,I17-1064,0,0.078577,"sentiment information. For example, in movie reviews, the name of a movie with a good reputation tends to be regarded as positive words, and is thereby assigned higher weights, which does not work in other domains. Long et al. (2017) incorporated the reading time of human beings into the attention mechanism, but their method also has much noise because during the reading process, people tend to spend more time on intricate contents (Goodman, 1988) than on real sentiment words like good and bad. Other methods employed external information such as users and products to guide attention weights (Ma et al., 2017; Chen et al., 2016), which can boost the classification performance by a large margin. However, most of the time, we have no access to such external information. In this paper, we propose a novel Lexicon-Based Supervised Attention model (LBSA) that combines sentiment lexicons and an attention mechanism to better extract sentiment information and form This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 868 Proceedings of the 27th International Conference on Computational Linguistics, pages 868–877 S"
C18-1074,D13-1170,0,0.308042,"Net3.0 (SWN) (Baccianella et al., 2010). The original scores in SWN are the probabilities of positive, negative, or neutral polarities for words. We take the maximum sum of the positive and negative scores as the sentiment degree for different part-of-speech tags. The second part is extracted from MPQA (Wilson et al., 2005), which tags polarity ratings for sentiment words. To a word whose polarity is positive or negative, we assign 1 as its sentiment degree. Otherwise, the word’s sentiment degree is 0. The third part consists of the leaf nodes of the Stanford Sentiment Treebank (SST) dataset (Socher et al., 2013). The sentiment scores of 1 ˜ http://ir.hit.edu.cn/dytang/paper/acl2015/dataset.7z 872 SST are in the range of [0, 1]. We scale it to [-1, 1] and keep the absolute value as the sentiment degree. The last part is the sentiment lexicon of the Hownet Knowledge Database (HKD)2 , which contains a list of positive or negative words. We conduct the pre-processing in the same way as for MPQA. Finally, we combine the four parts and average the sentiment degree for words appearing in two or more lexicon parts. All of the neutral words in different parts are included in our lexicon because of the ignoran"
C18-1074,C14-1018,0,0.266272,"eve the above target, in this work, we propose a novel lexicon-based supervised attention model (LBSA), which allows a recurrent neural network to focus on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods. 1 Introduction Sentiment analysis has the goal of analyzing people’s sentiments or opinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentiment classification"
C18-1074,P14-1146,0,0.505478,"eve the above target, in this work, we propose a novel lexicon-based supervised attention model (LBSA), which allows a recurrent neural network to focus on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods. 1 Introduction Sentiment analysis has the goal of analyzing people’s sentiments or opinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentiment classification"
C18-1074,P15-1098,0,0.512148,"as the metric: X 4(a? , a) = − a?i log(ai ). (13) i µw and µs are the coefficients for attention loss functions, which can balance the preference between classification and attention disagreements, to alleviate the overfitting problem. 3 Experimental Settings In this section, we introduce the experimental settings in detail, including the datasets and lexicons, hyper-parameter settings, and baseline models. 3.1 Datasets and Lexicons We conduct experiments to evaluate the effectiveness of our method on three document-level review datasets: IMDB, Yelp 2013 and Yelp 2014, which are developed by Tang et al. (2015). We split the datasets into training, development and testing sets in the proportion of 8:1:1, using pre-processing in the same way as Tang et al. (2015). Table 1 summarizes the statistics of the datasets. All of these datasets can be publicly accessed1 . The sentiment lexicon contains four parts. During the process of constructing our lexicon, we only use the sentiment scores for unigrams. The first part comes from SentimentWordNet3.0 (SWN) (Baccianella et al., 2010). The original scores in SWN are the probabilities of positive, negative, or neutral polarities for words. We take the maximum"
C18-1074,D16-1169,0,0.536361,"pinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentiment classification tasks, sentiment words such as great and terrible tend to play a more critical role than other words in texts (Liu, 2010). One attribute of words annotated in sentiment lexicons is their sentiment strength, which is intuitively associated with a word’s contribution to the sentiment representation of a sentence. It is similar to the basic idea of the attention mechanism that not all words have the same importance. However, very few studies have focused on a method that combines an attention mechanism with such sentiment information. Yang et al."
C18-1074,P02-1053,0,0.0548251,"lysis. To achieve the above target, in this work, we propose a novel lexicon-based supervised attention model (LBSA), which allows a recurrent neural network to focus on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods. 1 Introduction Sentiment analysis has the goal of analyzing people’s sentiments or opinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentim"
C18-1074,H05-1044,0,0.714174,"eural network to focus on the sentiment content, thus generating sentiment-informative representations. Compared with general attention models, our model has better interpretability and less noise. Experimental results on three large-scale sentiment classification datasets showed that the proposed method outperforms previous methods. 1 Introduction Sentiment analysis has the goal of analyzing people’s sentiments or opinions and has been well explored (Turney, 2002; Tang et al., 2014b; Chen et al., 2016). In order to improve sentiment analysis results, large sentiment lexicons have been built (Wilson et al., 2005; Baccianella et al., 2010; Tang et al., 2014a). A sentiment lexicon is a set of words such as excellent, terrible and ordinary, each of which is assigned a fixed positive or negative score that presents its sentiment polarity and strength (Tang et al., 2014a). Such information can serve as sentiment-informative features and significantly boost the classification performance (Agarwal et al., 2013; Teng et al., 2016; Qian et al., 2016). In sentiment classification tasks, sentiment words such as great and terrible tend to play a more critical role than other words in texts (Liu, 2010). One attri"
C18-1074,D16-1172,1,0.908819,"ssifier. Paragraph + Vector (Le and Mikolov, 2014) learns distributed representations of a document for classification. RNTN + RNN (Socher et al., 2013) represents sentences with the Recursive Neural Tensor Network (RNTN) and feeds them into a recurrent neural networks (RNN) to obtain document representations. UPNN (Tang et al., 2015) uses a text preference matrix and vector for each user and product as extra information to train a CNN sentiment classifier. UPNN(noUP) only uses CNN without considering user and product information. The models in Group 2 are based on sentiment lexicons. BiLSTM (Xu et al., 2016) is a bidirectional LSTM baseline with a simple average pooling layer. AveLex naively averages sentiment scores of words in the document to measure the overall sentiment polarities according to our sentiment lexicon. BiLSTM + Lex (Teng et al., 2016) takes the local context into consideration, which leverages a bidirectional LSTM to capture context information and calculates the weighted sum of the sentiment scores. The two lexicon-based methods only work on the word level because sentences do not have a gold sentiment score. For a fair comparison, BiLSTM + LBSA is our model, which removes the"
C18-1074,N16-1174,0,0.444225,"et al., 2016; Qian et al., 2016). In sentiment classification tasks, sentiment words such as great and terrible tend to play a more critical role than other words in texts (Liu, 2010). One attribute of words annotated in sentiment lexicons is their sentiment strength, which is intuitively associated with a word’s contribution to the sentiment representation of a sentence. It is similar to the basic idea of the attention mechanism that not all words have the same importance. However, very few studies have focused on a method that combines an attention mechanism with such sentiment information. Yang et al. (2016) and Chen et al. (2016) proposed a hierarchical RNN model to learn attention weights based on the local context using an unsupervised method. However, their method may induce much noise and suffers from a lack of interpretability because it tends to capture some domain-specific words (Mudinas et al., 2012) instead of real sentiment information. For example, in movie reviews, the name of a movie with a good reputation tends to be regarded as positive words, and is thereby assigned higher weights, which does not work in other domains. Long et al. (2017) incorporated the reading time of human bei"
C18-1074,D16-1024,0,0.0241342,"an attention mechanism, which was proven to be effective in our experiments. Recently, the attention mechanism has been widely studied in sentiment classification. Yang et al. (2016) proposed two attention-based bidirectional GRUs to enforce the neural networks to attend to 875 the related part of a sentence or document. Apart from local contexts, Chen et al. (2016) incorporated extra information such as users and products in review datasets into a hierarchical attention mechanism. Ma et al. (2017) cascaded multiway of user and product information to enhance the effects of different aspects. Zhou et al. (2016) proposed a LSTM network based on an attention mechanism for cross-lingual sentiment classification at the document level. The model consists of two attention-based hierarchical LSTMs for bilingual representation. Long et al. (2017) took cognition into consideration, leveraging extra resources including the reading time of human beings. They proposed a mutimodel that learns to predict the reading time to construct the cognitive attention. The main difference between our method and others is that we supervise the attention weights using sentiment information. As a result, it is able to capture"
C18-1314,N16-1165,0,0.0352315,"Missing"
C18-1314,D16-1053,0,0.0182811,"lignment matrix based on two inputs, which can model complex interactions between the two inputs. Xiong et al. (2016) present a co-attention encoder to focus on relevant parts of the representations of the question and document and use a dynamic pointing decoder to locate the answer. Cui et al. (2016) propose a two-way attention mechanism to encode the passage and question mutually and induce attended attention for final answer predictions. Self-attention mechanism is an attention mechanism aiming at aligning the sequence with itself, which has been successfully used in a variety of tasks. In Cheng et al. (2016), both encoder and decoder are modeled as LSTMs with self-attention for extractive summarization of documents. In Lin et al. (2017), the authors conduct a self-attention over the hidden states of a BiLSTM to extract the sentence embedding. Instead of sentence vector, they use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. In this work, we employ a co-attention mechanism to capture the interactions between the original post and the reply on argument level. What’s more, we use a self-attention mechanism to obtain the argument r"
C18-1314,W14-4012,0,0.0128942,"Missing"
C18-1314,K17-1017,0,0.0774691,"Matrix Alignment Matrix Post to reply argument attention + Weights GRU GRU GRU u OP Argument Vector Original Post Softmax r1R r1O P uOP GRU r r ... r Co-Attention Network Attention Post argument to reply argument attention R m OP 2 n {U*i }i=1 Co-Attention Network r ... R 2 r1O P Softmax Aggregation Network r X feat O* Attention Pooling R 1 r1R Original Post Reply ... r2R rmR Reply Figure 2: Overall architecture of the proposed model. The left part is the main framework of this work. The right part is the detailed structure of the co-attention network. 2.1 Argument Representation Inspired by Dong et al. (2017), we employ a hierarchical architecture to obtain two different representations for each single argument. For simplicity, we consider each sentence as an argument. Representation based on internal words given an argument with words w1 , w2 , ..., wT , we first map each word to a dense vector obtaining x1 , x2 , ..., xT correspondingly. We then employ a convolution layer to incorporate the context information on word level. zi = f (Wz · [xi : xi+hw −1 ] + bz ) (1) where Wz and bz are weight matrix and bias vector. hw is the window size in the convolution layer and zi is the feature representati"
C18-1314,C14-1089,0,0.0293406,"results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a numeric score, considering features of content, grammar, d"
C18-1314,D16-1129,0,0.350129,"nt, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First, viewpoints of the original poster and repliers"
C18-1314,P16-1150,0,0.458692,"nt, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First, viewpoints of the original poster and repliers"
C18-1314,P15-1107,0,0.0352229,"xt similarity. The interactions among argument pairs are ignored. In this work, we evaluate the quality of debate comments through the interactions among them on argument level. 5.2 Attention mechanism Attention mechanism allows models to focus on specific parts of inputs at each step of a task. Moreover, attention mechanism has been proved to be significantly effective in natural language processing tasks. Co-attention mechanism has recently attracted lots of research interest in the fields of machine translation (Bahdanau et al., 2014), question answering (Wu et al., 2017), text generation (Li et al., 2015), etc. It is computed as an alignment matrix based on two inputs, which can model complex interactions between the two inputs. Xiong et al. (2016) present a co-attention encoder to focus on relevant parts of the representations of the question and document and use a dynamic pointing decoder to locate the answer. Cui et al. (2016) propose a two-way attention mechanism to encode the passage and question mutually and induce attended attention for final answer predictions. Self-attention mechanism is an attention mechanism aiming at aligning the sequence with itself, which has been successfully us"
C18-1314,D15-1110,0,0.0767786,"network to capture the interactions between participants on argument level. Experimental results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as"
C18-1314,D14-1162,0,0.0817283,"as a ranking task and utilize a pairwise hinge loss for training. Given a triple (OP, R+ , R− ), where R+ and R− respectively denote the positive and the negative reply for OP . The loss function is defined in Equation 18. L = max(0, 1 − S(OP, R+ ) + S(OP, R− )) (18) where S(OP, R+ ) and S(OP, R− ) are the corresponding persuasiveness scores. The model is trained by stochastic gradient descent on 105 epochs, and evaluated on the development set at every epoch to select the best model. Dropout (Srivastava et al., 2014) has proved to be an effective method and is used in our work. We use Glove (Pennington et al., 2014) word embeddings, which are 50-dimension word vectors trained with a crawled large corpus with 840 billion tokens. Embeddings for words not present are randomly initialized with sampled numbers from a uniform distribution [0.25,0.25]. We set initial learning rate to 0.1, batch size to 20, filter sizes to 5, filter numbers to 100 and the hidden unit of BiGRU to 200. Early stopping was used with a patience of 15 epochs. We implemented our model using TensorFlow. The model converged in 23 hours on an NVIDIA Titan X machine. 3707 Original post Positive reply Negative reply Avew 10 10 10 Training S"
C18-1314,D14-1006,0,0.0506881,"eractions between participants on argument level. Experimental results on a publicly available dataset show that the proposed model significantly outperforms some state-of-the-art methods for persuasiveness evaluation. Further analysis reveals that attention weights computed in our model are able to extract interactive argument pairs from the original post and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a n"
C18-1314,D16-1193,0,0.0383443,"tructure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a numeric score, considering features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality"
C18-1314,E17-1017,0,0.0698165,"and the reply. 1 Introduction Computational argumentation is a growing field in natural language processing. Existing research covers argument unit detection (Al-Khatib et al., 2016), argument structure prediction (Peldszus and Stede, 2015; Stab and Gurevych, 2014), argumentation scheme classification (Feng et al., 2014), etc. Recently, the automatic assessment of argumentation quality has started gaining attention. It can be analyzed at two levels, namely monological argumentation and dialogical argumentation. Monological argumentation refers to a composition of arguments on a certain issue (Wachsmuth et al., 2017). A typical example of quality evaluation for monological argumentation is automated essay scoring, which aims to process argumentative essays without human interference (Taghipour and Ng, 2016). It takes an essay as the input and outputs a numeric score, considering features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on ("
C18-1314,Q17-1016,0,0.22711,"all of the attention representations in a linear function to get the integrated information. The detail is illustrated in Equation 12. n m U = f (U 1 , U 2 , U 3 , {riOP }i=1 , {rjR }j=1 ) (12) where f is a simple linear function, U 3 is a matrix that is tiled n times by u3 . 2.3 Aggregation Network After acquiring the local alignment representation by the co-attention network, we employ a filtration gate to hold the interactive information. Then, we fuse the interactive information via a bi-directional GRU and compute the persuasiveness score. Filtration Gate We utilize the filtration gate (Wang et al., 2017b) to hold the information that helps to understand the argument-level interactions between the original post and the reply. The formulas are in Equation 13 and 14. gt = sigmoid(Wg U + b) (13) U ∗ = gt U (14) We fuse the interactive information reserved by the filtration gate via a bi-directional GRU. The calculation is described in Equation 15. Ot = BiGRU (Ot−1 , Ut∗ ) (15) Then, we use an attention pooling operation over the whole hidden states of this BiGRU to summarize the interactive features into a dense vector O∗ . Scoring Tay et al. (2017) prove that adding some manual features such as"
C18-1314,P17-1018,0,0.170286,"all of the attention representations in a linear function to get the integrated information. The detail is illustrated in Equation 12. n m U = f (U 1 , U 2 , U 3 , {riOP }i=1 , {rjR }j=1 ) (12) where f is a simple linear function, U 3 is a matrix that is tiled n times by u3 . 2.3 Aggregation Network After acquiring the local alignment representation by the co-attention network, we employ a filtration gate to hold the interactive information. Then, we fuse the interactive information via a bi-directional GRU and compute the persuasiveness score. Filtration Gate We utilize the filtration gate (Wang et al., 2017b) to hold the information that helps to understand the argument-level interactions between the original post and the reply. The formulas are in Equation 13 and 14. gt = sigmoid(Wg U + b) (13) U ∗ = gt U (14) We fuse the interactive information reserved by the filtration gate via a bi-directional GRU. The calculation is described in Equation 15. Ot = BiGRU (Ot−1 , Ut∗ ) (15) Then, we use an attention pooling operation over the whole hidden states of this BiGRU to summarize the interactive features into a dense vector O∗ . Scoring Tay et al. (2017) prove that adding some manual features such as"
C18-1314,P16-2032,1,0.941255,"score, considering features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First,"
C18-1314,W16-2820,1,0.734769,"features of content, grammar, discourse structure and lexical richness (Burstein et al., 2013). Most of the efforts are made on the exploration for better document representation. Dialogical argumentation refers to a series of interactive arguments related to a given topic, involving argument retraction, view exchange, and so on (Besnard et al., 2014). With the popularity of online debating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a). Although some similarity features of content between the reply and the original post are used to evaluate the quality of the given reply, they are computed on word level without considering the exchange of opinions on the basis of arguments. An example of dialogical argumentation is shown in Figure 1. There are three posts, one is original and the other two are replies. The repliers post to change the view of the original poster. And the positive reply is deemed to be more persuasive than the negative reply. We have three observations. First, viewpoints of the"
D09-1159,H05-1045,0,0.0102432,"Missing"
D09-1159,W06-1651,0,0.0530414,"work on extracting opinion units including: opinion holder, subject, aspect and evaluation. Subject and aspect belong to product features, while evaluation is the opinion expression in our work. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which combines contextual clues and statistical clues. Their experimental results showed that the model using contextual clues improved the performance. However since the contextual information in a domain is specific, the model got by their approach can not easily converted to other domains. Choi et al. (2006) used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction. They identified expressions of opinions, sources of opinions and the linking relation that exists between them. The sources of opinions denote to the person or entity that holds the opinion. Another area related to our work is opinion expressions identification (Wilson et al., 2005a; Breck et al., 2007). They worked on identifying the words and phrases that express opinions in text. According to Wiebe et al. (2005), there are two types of opinion exp"
D09-1159,P04-1054,0,0.108616,"an that could be represented by a feature extraction-based approach, we define a new tree kernel over phrase dependency trees and incorporate this kernel within an SVM to extract relations between opinion expressions and product features. The potential relation set consists of the all combinations between candidate product features and candidate opinion expressions in a sentence. Given a phrase dependency parsing tree, we choose the subtree rooted at the lowest common parent(LCP) of opinion expression and product feature to represent the relation. Dependency tree kernels has been proposed by (Culotta and Sorensen, 2004). Their kernel is defined on lexical dependency tree by the convolution of similarities between all possible subtrees. However, if the convolution containing too many irrelevant subtrees, over-fitting may occur and decreases the performance of the classifier. In phrase dependency tree, local words in a same phrase are compacted, therefore it provides a way to treat “local dependencies” and “global dependencies” differently (Fig. 3). As a consequence, these two kinds of dependencies will not disturb each other in measuring similarity. Later experiments prove the validity of this statement. Phra"
D09-1159,C04-1200,0,0.59445,"lyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements 1. I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2) . 2. This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4) . The phrases underlined are the product features, marked with square brackets are opinion expressions. Product features and opinion expressions with identical superscript compose a relation. For the first sentence, an opinion relation exists between “the Canon SD500” and “"
D09-1159,D07-1114,0,0.71835,"product review. In practice, for a certain domain of product reviews, a language model is build on easily acquired unlabeled data. Each candidate NP or VP chunk in the output of shallow parser is scored by the model, and cut off if its score is less than a threshold. Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phra"
D09-1159,W02-1011,0,0.023691,"s. Retrieving this information and analyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements 1. I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2) . 2. This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4) . The phrases underlined are the product features, marked with square brackets are opinion expressions. Product features and opinion expressions with identical superscript compose a relation. For the first sentence, an opinion relation"
D09-1159,H05-1043,0,0.956926,"Missing"
D09-1159,C08-1101,0,0.0140112,"d tracking customer opinions. Retrieving this information and analyzing this content are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements 1. I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2) . 2. This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4) . The phrases underlined are the product features, marked with square brackets are opinion expressions. Product features and opinion expressions with identical superscript compose a relation. For the first sentence,"
D09-1159,P05-1017,0,0.272784,"are impossible tasks if they were to be manually done. However, advances in machine learning and natural language processing present us with a unique opportunity to automate the decoding of consumers’ opinions from online reviews. Previous works on mining opinions can be divided into two directions: sentiment classification and sentiment related information extraction. The former is a task of identifying positive and negative sentiments from a text which can be a passage, a sentence, a phrase and even a word (Somasundaran et al., 2008; Pang et al., 2002; Dave et al., 2003; Kim and Hovy, 2004; Takamura et al., 2005). The latter focuses on extracting the elements composing a sentiment text. The elements 1. I highly [recommend](1) the Canon SD500(1) to anybody looking for a compact camera that can take [good](2) pictures(2) . 2. This camera takes [amazing](3) image qualities(3) and its size(4) [cannot be beat](4) . The phrases underlined are the product features, marked with square brackets are opinion expressions. Product features and opinion expressions with identical superscript compose a relation. For the first sentence, an opinion relation exists between “the Canon SD500” and “recommend”, but not betw"
D09-1159,H05-2018,0,0.103904,"old. Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phrase dependency parsing tree. 2.3 Relation Extraction This section describes our method on extracting relations between opinion expressions and product features using phrase dependency tree. Manually built patterns were used in previous works which have an obvio"
D09-1159,H05-1044,0,0.194139,"old. Opinion expressions are spans of text that express a comment or attitude of the opinion holder, which are usually evaluative or subjective phrases. We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashi et al. (2007). They collected 5,550 opinion expressions from various sources . The coverage of the dictionary is high in multiple domains. Motivated by those observations, we use a dictionary which contains 8221 opinion expressions to select candidates (Wilson et al., 2005b). An assumption we use to filter candidate opinion expressions is that opinion expressions tend to appear closely with product features, which is also used to extract product features by Hu and Liu (2004). In our experiments, the tree distance between product feature and opinion expression in a relation should be less than 5 in the phrase dependency parsing tree. 2.3 Relation Extraction This section describes our method on extracting relations between opinion expressions and product features using phrase dependency tree. Manually built patterns were used in previous works which have an obvio"
D09-1159,H01-1014,0,0.00472748,"roblem for dependency grammar. Fig. 2(a) shows the dependency representation of an example sentence. The root of the sentence is “enjoyed”. There are seven pairs of dependency relationships, depicted by seven arcs from heads to dependents. 2.1.2 Phrase Dependency Parsing Currently, the mainstream of dependency parsing is conducted on lexical elements: relations are built between single words. A major information loss of this word level dependency tree compared with constituent tree is that it doesn’t explicitly provide local structures and syntactic categories (i.e. NP, VP labels) of phrases (Xia and Palmer, 2001). On the other hand, dependency tree provides connections between distant words, which are useful in extracting long distance relations. Therefore, compromising between the two, we extend the dependency tree node with phrases. That implies a noun phrase “Cannon SD500 PowerShot” can be a dependent that modifies a verb phrase head “really enjoy using” with relation type “dobj”. The feasibility behind is that a phrase is a syntactic unit regardless of the length or syntactic category (Santorini and Kroch, 2007), and it is acceptable to substitute a single word by a phrase with same syntactic cate"
D09-1159,H05-2017,0,\N,Missing
D10-1019,D08-1070,0,0.0257223,"modular approach. While, the key disadvantage of such method is that errors propagate between stages, significantly affecting the quality of the final results. To cope with this problem, Shi and Wang (2007) proposed a reranking framework in which N-best segment candidates generated in the first stage are passed to the tagging model, and the final output is the one with the highest overall segmentation and tagging probability score. The main drawback of this method is that the interaction between tagging and segmentation is restricted by the number of candidate segmentation outputs. Razvan C. Bunescu (2008) presented an improved pipeline model in which upstream subtask outputs are regarded as hidden variables, together with their probabilities are used as probabilistic features in the downstream subtasks. One shortcoming of this method is that calculation of marginal probabilities of features may be inefficient and some approximations are required for fast computation. Another disadvantage of these two methods is that they employ separate training and the segmentation model could not take advantages of tagging information in the training procedure. On the other hand, joint learning and decoding"
D10-1019,P08-1102,0,0.157897,"Missing"
D10-1019,I08-4010,0,0.125398,"4, supervised learning is fundamental. We believe that combination of our method and semi-supervised learning will achieve further improvement. 4.2 Chinese word segmentation and POS tagging Our second experiment is the Chinese word segmentation and POS tagging task. To facilitate comparison, we focus only on the closed test, which means that the system is trained only with a designated training corpus, any extra knowledge is not allowed, including Chinese and Arabic numbers, letters and so on. We use the Chinese Treebank (CTB) POS corpus from the Fourth International SIGHAN Bakeoff data sets (Jin and Chen, 2008). The training data consist of 23444 sentences, 642246 Chinese words, 1.05M Chinese characters and testing data consist of 2079 sentences, 59955 Chinese words, 0.1M Chinese characters. We compare our hybrid CRFs with pipeline and candidate reranking methods (Shi and Wang, 2007) 192 Table 4: Comparison with other systems on shallow parsing task Method F1 Cross-Product CRFs Hybrid CRFs SVM combination (Kudo and Matsumoto, 2001) Voted Perceptrons (Carreras and Marquez, 2003) ETL (Milidiu et al., 2008) (Wu et al., 2006) 93.88 94.31 93.91 HySOL (Suzuki et al., 2007) ASO-semi (Ando and Zhang, 2005)"
D10-1019,D07-1033,0,0.0239896,"Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187–195, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics ever, DCRFs do not guarantee non-violation of hardconstraints that nodes within the same segment get a single consistent tagging label. Another drawback of DCRFs is that exact inference is generally time consuming, some approximations are required to make it tractable. Recently, perceptron based learning framework has been well studied for incorporating node level and segment level features together (Kazama and Torisawa, 2007; Zhang and Clark, 2008). The main shortcoming is that exact inference is intractable for those dynamically generated segment level features, so candidate based searching algorithm is used for approximation. On the other hand, Jiang (2008) proposed a cascaded linear model which has a two layer structure, the inside-layer model uses node level features to generate candidates with their weights as inputs of the outside layer model which captures non-local features. As pipeline models, error propagation problem exists for such method. In this paper, we present a novel graph structure that exploit"
D10-1019,N01-1025,0,0.163696,"Missing"
D10-1019,P08-1074,0,0.026393,"Missing"
D10-1019,W04-3236,0,0.0233341,"abilistic features in the downstream subtasks. One shortcoming of this method is that calculation of marginal probabilities of features may be inefficient and some approximations are required for fast computation. Another disadvantage of these two methods is that they employ separate training and the segmentation model could not take advantages of tagging information in the training procedure. On the other hand, joint learning and decoding using cross-product of segmentation states and tagging states does not suffer from error propagation problem and achieves higher accuracy on both subtasks (Ng and Low, 2004). However, two problems arises due to the large state space, one is that the amount of parameters increases rapidly, which is apt to overfit on the training corpus, the other is that the inference by dynamic programming could be inefficient. Sutton (2004) proposed Dynamic Conditional Random Fields (DCRFs) to perform joint training/decoding of subtasks using much fewer parameters than the cross-product approach. How187 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187–195, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computat"
D10-1019,P08-1076,0,0.0303963,"Missing"
D10-1019,D07-1083,0,0.0280086,"Missing"
D10-1019,P08-1101,0,0.0604991,"ference on Empirical Methods in Natural Language Processing, pages 187–195, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics ever, DCRFs do not guarantee non-violation of hardconstraints that nodes within the same segment get a single consistent tagging label. Another drawback of DCRFs is that exact inference is generally time consuming, some approximations are required to make it tractable. Recently, perceptron based learning framework has been well studied for incorporating node level and segment level features together (Kazama and Torisawa, 2007; Zhang and Clark, 2008). The main shortcoming is that exact inference is intractable for those dynamically generated segment level features, so candidate based searching algorithm is used for approximation. On the other hand, Jiang (2008) proposed a cascaded linear model which has a two layer structure, the inside-layer model uses node level features to generate candidates with their weights as inputs of the outside layer model which captures non-local features. As pipeline models, error propagation problem exists for such method. In this paper, we present a novel graph structure that exploits joint training and dec"
D10-1019,I08-4017,0,0.189923,"Missing"
D10-1019,P05-1001,0,\N,Missing
D11-1123,P09-1079,0,0.0857641,"Missing"
D11-1123,N07-1030,0,0.0131716,"rative sentences and proposed learning approaches to identify them. Sentiment analysis of conditional sentences were studied by Narayanan et al. (2009). They aimed 1340 to determine whether opinions expressed on different topics in a conditional sentence are positive, negative or neutral. They analyzed the conditional sentences in both linguistic and computitional perspectives and used learning method to do it. They followed the feature-based sentiment analysis model (Hu and Liu, 2004), which also use flat frames to represent evaluations. Integer linear programming was used in many NLP tasks (Denis and Baldridge, 2007), for its power in both expressing and approximating various inference problems, especially in parsing (Riedel and Clarke, 2006; Martins et al., 2009). Martins etc. (2009) also applied ILP with flow formulation for maximum spanning tree, besides, they also handled dependency parse trees involving high order features(sibling, grandparent), and with projective constraint. 6 Conclusions This paper introduces a representation method for opinions in online reviews. Inspections on corpus show that the information ignored in previous sentiment representation can cause incorrect or incomplete mining r"
D11-1123,P10-1041,0,0.0347343,"Missing"
D11-1123,P06-2063,0,0.0289828,"Missing"
D11-1123,D07-1114,0,0.155196,"Results on vertices extraction with 10 folder cross validation. We use two criterion: 1) the vertex is correct if it is exactly same as ground truth(“E”), 2) the vertex is correct if it overlaps with ground truth(“O”). 5 Related Work Opinion mining has recently received considerable attentions. Large amount of work has been done on sentimental classification in different levels and sentiment related information extraction. Researches on different types of sentences such as comparative sentences (Jindal and Liu, 2006) and conditional sentences (Narayanan et al., 2009) have also been proposed. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. They used slots to represent evaluations, converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual and statistical clues. Jindal and Liu (2006) studied the problem of identifying comparative sentences. They analyzed different types of comparative sentences and proposed learning approaches to identify them. Sentiment analysis of conditional sentences were studied by Narayanan et al. (2009). They aimed 1340 to det"
D11-1123,P09-1039,0,0.115006,"connects to more than one opinion expression rarely occur comparing with those vertices which have a single parent. An explaination for this sparseness is that opinions in online reviews always concentrate in local context and have local semantic connections. 3.2.2 ILP Formulation Based on the property 3, we divide the inference algorithm into two steps: i) constructing G’s spanning tree (arborescence) with property 1 and 2; ii) finding additional non-tree edges as a post processing task. The first step is close to the works on ILP formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009). In the second step, we use a heuristic method which greedily adds non-tree edges. A similar approximation method is also used in (Mcdonald and Pereira, 2006) for acyclic dependency graphs. Step 1. Find MST. Following the multicommodity flow formulation of maximum spanning tree (MST) problem in (Magnanti and Wolsey, 1994), the ILP for MST is: ∑ max. yij · score(xi , xj ) (3) i,j ∑ s.t. ∑ i i,j (4) yij = |V |− 1 fiju − ∑ ∑ k u f0k u fjk = δju ,1 ≤ u, j ≤ |V |(5) = 1, k fiju fiju (6) 1 ≤ u ≤ |V | ≤ yij , 1 ≤ u, j ≤ |V |, ≥ 0, 1 ≤ u, j ≤ |V |, yij ∈ { 0, 1}, 0 ≤ i ≤ |V | (7) 0 ≤ i ≤ |V | (8) 0 ≤"
D11-1123,E06-1011,0,0.0461743,"is that opinions in online reviews always concentrate in local context and have local semantic connections. 3.2.2 ILP Formulation Based on the property 3, we divide the inference algorithm into two steps: i) constructing G’s spanning tree (arborescence) with property 1 and 2; ii) finding additional non-tree edges as a post processing task. The first step is close to the works on ILP formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009). In the second step, we use a heuristic method which greedily adds non-tree edges. A similar approximation method is also used in (Mcdonald and Pereira, 2006) for acyclic dependency graphs. Step 1. Find MST. Following the multicommodity flow formulation of maximum spanning tree (MST) problem in (Magnanti and Wolsey, 1994), the ILP for MST is: ∑ max. yij · score(xi , xj ) (3) i,j ∑ s.t. ∑ i i,j (4) yij = |V |− 1 fiju − ∑ ∑ k u f0k u fjk = δju ,1 ≤ u, j ≤ |V |(5) = 1, k fiju fiju (6) 1 ≤ u ≤ |V | ≤ yij , 1 ≤ u, j ≤ |V |, ≥ 0, 1 ≤ u, j ≤ |V |, yij ∈ { 0, 1}, 0 ≤ i ≤ |V | (7) 0 ≤ i ≤ |V | (8) 0 ≤ i, j ≤ |V |. (9) In this formulation, yij is an edge indicator variable that (xi , xj ) is a spanning tree edge when yij = 1, (xi , xj ) is a non-tree edge wh"
D11-1123,D09-1019,0,0.0509771,"5.1 47.9 57.2 60.2 F 50.3 52.1 63.7 65.6 Table 7: Results on vertices extraction with 10 folder cross validation. We use two criterion: 1) the vertex is correct if it is exactly same as ground truth(“E”), 2) the vertex is correct if it overlaps with ground truth(“O”). 5 Related Work Opinion mining has recently received considerable attentions. Large amount of work has been done on sentimental classification in different levels and sentiment related information extraction. Researches on different types of sentences such as comparative sentences (Jindal and Liu, 2006) and conditional sentences (Narayanan et al., 2009) have also been proposed. Kobayashi et al. (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation. They used slots to represent evaluations, converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual and statistical clues. Jindal and Liu (2006) studied the problem of identifying comparative sentences. They analyzed different types of comparative sentences and proposed learning approaches to identify them. Sentiment analysis of conditional sentences were studied b"
D11-1123,W02-1011,0,0.0140594,"Missing"
D11-1123,W06-1616,0,0.0622205,"the cases that a modifier connects to more than one opinion expression rarely occur comparing with those vertices which have a single parent. An explaination for this sparseness is that opinions in online reviews always concentrate in local context and have local semantic connections. 3.2.2 ILP Formulation Based on the property 3, we divide the inference algorithm into two steps: i) constructing G’s spanning tree (arborescence) with property 1 and 2; ii) finding additional non-tree edges as a post processing task. The first step is close to the works on ILP formulations of dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009). In the second step, we use a heuristic method which greedily adds non-tree edges. A similar approximation method is also used in (Mcdonald and Pereira, 2006) for acyclic dependency graphs. Step 1. Find MST. Following the multicommodity flow formulation of maximum spanning tree (MST) problem in (Magnanti and Wolsey, 1994), the ILP for MST is: ∑ max. yij · score(xi , xj ) (3) i,j ∑ s.t. ∑ i i,j (4) yij = |V |− 1 fiju − ∑ ∑ k u f0k u fjk = δju ,1 ≤ u, j ≤ |V |(5) = 1, k fiju fiju (6) 1 ≤ u ≤ |V | ≤ yij , 1 ≤ u, j ≤ |V |, ≥ 0, 1 ≤ u, j ≤ |V |, yij ∈ { 0, 1}, 0 ≤ i ≤ |V | ("
D11-1123,W03-0404,0,0.0879412,"Missing"
D11-1123,C08-1101,0,0.0508096,"Missing"
D11-1123,P05-1017,0,0.0851631,"Missing"
D11-1123,D09-1159,1,0.419347,"Missing"
D13-1097,baccianella-etal-2010-sentiwordnet,0,0.0955458,"Missing"
D13-1097,C10-1021,0,0.014323,"t analysis model (Hu and Liu, 2004), which also use flat frames to represent evaluations. Since the cross sentences relations are considered in this work, the discourse-level relation extraction methods are also related to ours. Marcu and Echihabi (2002) proposed to use an unsupervised approach to recognizing discourse relations. Lin et al.(2009) analyzed the impacts of features extracted from contextual information, constituent parse trees, dependency parse trees, and word pairs. Asher et al.(2009) studied discourse segments containing opinion expressions from the perspective of linguistics. Chen et al. (2010) introduced a multi-label model to detect emotion causes. They developed two sets of linguistic features for this task base on linguistic cues. Zirn et al. (2011) proposed to use MLN framework to capture the context information in analysing (sub-)sentences. The most similar work to ours was proposed by Somasundaran et al.(2009). They proposed to use iterative classification algorithm to capture discourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-leve"
D13-1097,P09-1079,0,0.0441758,"Missing"
D13-1097,P12-1105,0,0.0280042,"Missing"
D13-1097,P12-1007,0,0.0182653,"mented several state7 • CRF-Subj: We follow the method proposed by Zhao et al. (2008), which regard the subjectivity of all clauses throughout a paragraph as a sequential flow of sentiments and use CRFs to model it. The feature sets are similar as the local formulas for MLN including words, POS tags, dependency relations, and opinion lexicon. an explanation of the opinion sentence. click of-the-art methods for comparison. http://code.google.com/p/thebeast 952 • SVM-Rel: We also use LibSVM (Chang and Lin, 2011) to classify the relations between clauses. Following the configurations reported by Feng and Hirst (2012), we use linear kernel and probability estimation to model it. 4.3 Results Table 3 shows the comparisons of the proposed method with the state-of-the-art systems on subjectivity classification and explanatory relation extraction. From the results, we can observe that recursive autoencoders based subjectivity classification method achieves slightly better performance than our method and conditional random fields based method. The performances of the proposed method are similar as CRFs’. We think that the main reason is that only lexical features are used in MLN models for subjective classificat"
D13-1097,P10-1041,0,0.051271,"Missing"
D13-1097,C04-1200,0,0.0806372,"cts of clause distance and sentence distance are not as significant as the other features. 5 Related Work Our work relates to three research areas: sentiment analysis/opinion mining, discourse-level relation extraction, and Markov logic networks. Along with the increasing requirement, subjectivity classification has recently received considerable attention from both the industry and researchers. A variety of approaches and methods have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of rela"
D13-1097,D07-1114,0,0.147274,"an be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Ric"
D13-1097,D09-1036,0,0.447174,". (c4) (c3) (c7) C5 C2 The S90 is a true pocket camera. It is very compact. also top notch. grip. C1 Both cameras are quite different but truly (c6) (c5) C6 C3 The build quality is It feels solid and it is easy to It is so small and convenient, (c8) C7 C4 you C8 will find that you will always carry it with you. (b) Directed Graph Representation (a) Example Review Figure 1: Directed graph representation of a sample document. and fits more than enough stuff. Many sentences, which express explanatory relation, do not contain any connectives (e.g. “because”, “the reason is”, and so on). Lin et al.(2009) generalized four challenges (include ambiguity, inference, context, and world knowledge) to automated implicit discourse relation recognition. In this task, we also need to address those challenges. From the these examples, we can observe that extracting explanatory relations from product reviews is a challenging task. Both linguistic and global constraints should be carefully studied. 3 The Proposed Approach In this section, we present our method for jointly classifying the subjectivity of text segments and extracting explanatory relations. Firstly, we briefly describe the framework of Marko"
D13-1097,P02-1047,0,0.106981,"Missing"
D13-1097,P12-1060,0,0.0144076,"tion, to the best of our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, w"
D13-1097,P07-1123,0,0.0303499,"have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based method which used both contextual and statistical clues. Analysis of some special types of sentences were also introduced in recent years. Jindal and Liu (2006) studied the problem of identifying comparative sentences. They analyzed different types of comparative sentences and proposed learning approaches to identify them. Conditional sentences were studied b"
D13-1097,D09-1019,0,0.0486385,"Missing"
D13-1097,W02-1011,0,0.0150022,"ail descriptions are used to explain the reflection problem of the camera screen. Although, explanations provide valuable information, to the best of our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major resea"
D13-1097,P09-2004,0,0.154187,"g both generative and discriminative approaches (Richardson and Domingos, 2006; Singla and Domingos, 2006). There are also several MLN learning packages available online such as thebeast2 , Tuffy3 , PyMLNs4 , Alchemy5 , and so on. 2 http://code.google.com/p/thebeast http://hazy.cs.wisc.edu/hazy/tuffy/ 4 http://www9-old.in.tum.de/people/jain/mlns/ 5 http://alchemy.cs.washington.edu/ 3 Describing the attributes of words subjLexicon(w) The word w belongs to the subjective lexicon (Baccianella et al., 2010). relationLexicon(w) The word w belongs to the lexicon of explanation relation connectives (Pitler and Nenkova, 2009). Describing the attributes of the clause ci word(i, w) The clause ci has word w. f irstW ord(i, w) The first word of clause ci is word w. pos(i, w, t) The POS tag of word w is t in clause ci . dep(i, h, m) Word m and h are governor and dependent of a dependency relation in clause ci . Describing the attributes of relations between clause ci and clause cj clauseDistance(i, j, m) Distance between clause ci and clause cj in clauses is m. sentenceDistance(i, j, n) Distance between clause ci and clause cj in sentences is n. Table 1: Descriptions of observed predicates. 3.2 Clause Identification We"
D13-1097,D08-1068,0,0.22553,"ational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a lot from this framework. To evaluate the proposed method, we crawled a large number of product reviews and constructed a labeled corpus through Amazon’s Mechanical Turk. Two tasks were deployed for labeling th"
D13-1097,W08-2125,0,0.022149,"algorithm to capture discourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which combines firstorder logic and probabilistic graphical models. In 954 recent years, MLN has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint unsupervised coreference resolution. Yoshikawa et al. (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. Andrzejewski et al. (2011) introduced a framework for incorporating general domain knowledge, w"
D13-1097,W03-0404,0,0.154829,"Missing"
D13-1097,D11-1014,0,0.0581746,"life is something I come to expect from this line of camera. ()Subjective ()Objective I have the camera set to shut off the sensor after about 30 seconds ()Subjective ()Objective Task2: Help us check whether a sentence is The opinion sentence (red one) is extracted from product reviews and express opinion towards some attributes/parts of a product. Please help us check whether the following blue sentences describe a set of facts which clarifies the causes, reason, and consequences of the opinion given in the opinion sentence. &quot;yes&quot; if there is an explanation relation between them, • RAE-Subj: Socher et al. (2011) proposed to use recursive autoencoders for sentence-level predication of sentiment label distributions. To compare with it, we also reimplement their method without any hand designed lexicon. &quot;no&quot; otherwise. • PDTB-Rel: For discourse relation extraction, we use “PDTB-Styled End-to-End Discourse Parser” (Lin et al., 2010) to extract discourse level relations as baseline. Since it is a general discourse relations identification algorithms, “Cause”, “Pragmatic Cause”, “Instantiation”, and “Restatement” relation types are treated as explanatory relation in this work. The battery life is something"
D13-1097,W09-3210,0,0.224164,"Missing"
D13-1097,D12-1114,0,0.0968763,"earch directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a lot from this framework. To evaluate the proposed method, we crawled a large number of product reviews and constructed a labeled corpus through Amazon’s Mechanical Turk. Two tasks were deployed for labeling the corpus. We compared the proposed method with state-ofthe-art methods"
D13-1097,P05-1017,0,0.0292598,"reflection problem of the camera screen. Although, explanations provide valuable information, to the best of our knowledge, there is no existing work that deals with explanation extraction for opinions in discourse level. We think that if explanatory relations can be automatically identified from reviews, sentiment analysis applications may benefit from it. Existing opinion mining approaches mainly focus on subjective text. They try to determine the subjectivity and polarity of fragments of documents (e.g. a paragraph, a sentence, a phrase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment a"
D13-1097,W01-0708,0,0.393394,"Missing"
D13-1097,H05-1044,0,0.0318563,"tence distance are not as significant as the other features. 5 Related Work Our work relates to three research areas: sentiment analysis/opinion mining, discourse-level relation extraction, and Markov logic networks. Along with the increasing requirement, subjectivity classification has recently received considerable attention from both the industry and researchers. A variety of approaches and methods have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and propo"
D13-1097,D11-1123,1,0.874054,"Missing"
D13-1097,P13-1161,0,0.0301006,"rase and a word) (Pang et al., 2002; Riloff et al., 2003; Takamura et al., 2005; Mihalcea et al., 2007; Dasgupta and Ng, ; Hassan and Radev, 2010; Meng et al., 2012; Dragut et al., 2012). Fine-grained methods were also introduced to extract opinion holder, opinion expression, opinion target, and other opinion elements (Kobayashi et al., 2007; Wu et al., reviews-guidelines 946 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946–957, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2011; Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et"
D13-1097,P09-1046,0,0.175952,"Xu et al., 2013; Yang and Cardie, 2013). Major research directions and challenges of sentiment analysis can also be found in surveys (Pang and Lee, 2008; Liu, 2012). In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (MLN) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a lot from this framework. To evaluate the proposed method, we crawled a large number of product reviews and constructed a labeled corpus through Amazon’s Mechanical Turk. Two tasks were deployed for labeling the corpus. We compared th"
D13-1097,D08-1013,0,0.0246806,"cant as the other features. 5 Related Work Our work relates to three research areas: sentiment analysis/opinion mining, discourse-level relation extraction, and Markov logic networks. Along with the increasing requirement, subjectivity classification has recently received considerable attention from both the industry and researchers. A variety of approaches and methods have been proposed for this task from different aspects. Among them, a number of approaches focus on classifying sentiments of text in different levels (e.g. words (Kim and Hovy, 2004), phrases (Wilson et al., 2005), sentences (Zhao et al., 2008), documents (Pang et al., 2002) and so on.), and detecting the overall polarity of them. Another research direction tries to convert the sentiment analysis task into entity identification and relation extraction. Hu and Liu (2004) proposed to use a set of methods to produce feature-based summary of a large number of customer reviews. Kobayashi et al. (2007) assumed that evaluative opinions could be structured as a frame which is composed by opinion holder, subject, aspect, and evaluation. They converted the task to two kinds of relation extraction tasks and proposed a machine learning-based me"
D13-1097,I11-1038,0,0.0226505,"course-level relation extraction methods are also related to ours. Marcu and Echihabi (2002) proposed to use an unsupervised approach to recognizing discourse relations. Lin et al.(2009) analyzed the impacts of features extracted from contextual information, constituent parse trees, dependency parse trees, and word pairs. Asher et al.(2009) studied discourse segments containing opinion expressions from the perspective of linguistics. Chen et al. (2010) introduced a multi-label model to detect emotion causes. They developed two sets of linguistic features for this task base on linguistic cues. Zirn et al. (2011) proposed to use MLN framework to capture the context information in analysing (sub-)sentences. The most similar work to ours was proposed by Somasundaran et al.(2009). They proposed to use iterative classification algorithm to capture discourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which comb"
D13-1097,prasad-etal-2008-penn,0,\N,Missing
D15-1046,P11-1016,0,0.0360938,"Missing"
D15-1046,J93-2003,0,0.0542942,"βh Mw,2 k Mw,(.) + 2β h (12) The potential size of the probability alignment ϕ1 between hashtag and word is W · V · K. The data sparsity may pose a more serious problem in estimating ϕ1 than the topic-free word alignment case. We use interpolation smoothing technique for ϕ1 . In this paper, we employ smoothing as follows: 1 ϕ1∗ h,k,w = γϕh,k,w + (1 − γ)P (h|w), (13) where ϕ1∗ h,k,w is the smoothed topical alignment probabilities, ϕ1h,k,w is the original topical alignment probabilities, P (h|w) is topic-free word alignment probability. In this work, we obtain P (h|w) by exploring IBM model-1 (Brown et al., 1993). γ is trade-off of two probabilities ranging from 0 to 1. When γ = 0, ϕ1∗ h,k,w reduces to topicfree word alignment probability, and when γ = 1, there will be no smoothing in ϕ1∗ h,k,w . ¯ w¬d , y, β w ) = p(wd |β w ) = p(wd |zd = k, Z (9) pe(wd |φk¯ )h(φk¯ )dφk¯ , φk¯ Q where pe(wd |φk¯ ) = 1≤n≤Nd ,yd =1 p(wdn |φk¯ ). n We can calculate the probabilities of generating hashtags from two situations as follows: p(hd |z, wd , y, x, β h ) =  k,¬d Mw +β h Q Md P  dn ,hdm    m=1 n∈Ned M k,¬d +β h V = k + βh Mw,h 2.2.3 Hashtag Recommendation Suppose given an unlabeled dataset, we firstly wd ,(."
D15-1046,C10-2028,0,0.0282227,"Missing"
D15-1046,D11-1146,0,0.175799,"ere obtained by starting from a set of seed users and their follower/followee relations. We extract the microblogs posted with hashtags between Jan. 2012 and July 2013. Finally, 1,118,792 microblogs posted are selected for this work. The unique number of hashtags in the corpus is 305,227. We randomly select 100K as training data, 10K as development data, and 10K as test set. The hashtags marked in the original microblogs are considered as the golden standards. 3.2 • Translation model (IBM-1): IBM model 1 is directly applied to obtain the alignment probability between the word and the hashtag (Liu et al., 2011). • Topical translation model (TTM): Ding et al. (2013) proposed the TTM for hashtag extraction. We implemented and extended their method for evaluating on the corpus constructed in this work. The number of topics in TTM is set to 20, and α is set to 50/K. The hyperparameters used in TTM are also selected based on the development data set. 3.3 Table 1 shows the comparisons of the proposed method with the state-of-the-art methods on the constructed evaluation dataset. “CNHR” denotes the method proposed in this paper. “NHR1” is a degenerate variation of CNHR, in which we consider all the hashtag"
D15-1046,C12-1105,0,0.224987,"corresponding microblog. While, the aim of hashtag #BREAKING in the example 2 is used as a label of the microblog. The different uses greatly 401 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 401–410, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. impact the strategy of hashtag recommendation. However, there has been relatively few studies which take this issue into consideration. In this paper, we propose a novel nonparametric Bayesian method to perform this problem. Inspired by the methods proposed by Liu et al. (2012), we assume that the hashtags and textual content in the corresponding microblog are parallel descriptions of the same thing in different languages. We adapt a translation model with topic distribution to achieve this task. Because of the ability of Dirichlet Process Mixture Models (DPMM) (Antoniak and others, 1974; Ferguson, 1983) to handle an unbounded number of topics, the proposed method is extended from them. Based on the different uses of hashtags, we incorporate the type of hashtag into the DPMM as a hidden variable. The main contributions of this work can be summarized as follows: A pe"
D15-1046,J03-1002,0,0.0434618,"w Nk dn +β (.) Nk +W β assigned” among “hashtags manually assigned”. F1 is the harmonic mean of precision and recall. We do 500 iterations of Gibbs sampling to train the model. For optimizing the hyperparmeters of the proposed method and alternative methods, we use development data set to do it. In this work, the scale parameter α is set to Gamma(5, 0.5). The other settings of hyperparameters are as follows: β w = 0.1, β h = 0.1, η = 0.01, and σ = 0.01. The smoothing factor γ in Eq.(13) is set to 0.8. For estimating the translation probability without topical information, we use GIZA++ 1.07 (Och and Ney, 2003) to do it. Since hashtag recommendation task can also be modeled as a classification problem, we compare the proposed model with the following alternative methods: w and Nk dn is a count of words wdn that are assigned to topic k in the corpus. And p(k) = N Nk+α is regarded as a prior (.) for topic distribution, where Z is the normalized factor. With topic distribution χ and topic-specific word alignment table ϕ∗ , we can rank hashtags for the microblog d in the unlabeled data through the following equation: p(hdm |wd , χd , ϕ∗ ) ∝ Nd X C K X X p(zd |χd ) · p(wdn |wd ) · p(xdm ) zd =1 n=1 x=1 ·"
D16-1069,O97-4005,0,0.254164,"he main reason that the dictionary may bring too much conflict. From the results of CRF and RNN, we can see that similar to the Chinese word segmentation task, methods using character dense representations can usually achieve better performance than character based methods. 4 Related Work Although dictionary can be manually constructed, it is a time-consuming work. Moreover, these manually constructed dictionaries are usually updated only occasionally. It would take months before it could be updated. Hence, automatic dictionary construction methods have also been investigated in recent years. Chang and Su (1997) proposed an unsupervised iterative approach for extracting out-of-vocabulary words from Chinese text corpora. Khoo (Khoo et al., 2002) introduced a method based on stepwise logistic regression to identify two-and three-character words in Chinese text. Jin and Wong (2002) incorporated local statistical information, global statistical information and contextual constraints to identify Chinese words. For collecting Thai unknown words, Haruechaiyasak et al. (2006) proposes a collaborative framework for achieving the task based on Web pages over the Internet. Except these unsupervised methods, the"
D16-1069,I13-1037,1,0.897472,"Missing"
D16-1069,P14-1062,0,0.014964,"haracters existing in the training data. We used LIBSVM to implement (Chang and Lin, 2011). Conditional Random Fields (CRFs) were proposed by Lafferty et al. (2001) to model sequence labeling tasks. According to the description given in §2.2, an NLP task can be converted into a sequence labeling problem. Hence, we used CRF to model characters as basic features and several combination templates of them. Compared to SVM, CRF takes both richer features and the labeling sequence into consideration. CRF++ 0.588 was used to do the experiments. Dynamic Convolutional Neural Network (DCNN), defined by Kalchbrenner et al. (2014), is used to model sentence semantics. The proposed method can handle input sequences of varying length, so we adopted their method by using the embeddings of characters as input. The toolkit we used in this work is provided by the authors9 . Recursive Autoencoder (RAE) (Socher et al., 2011), is a machine learning framework for representing variable sized words with a fixed length vector. In this work, we used greedy unsupervised RAE for modeling sequences of Chinese characters. The toolkit was provided by the authors 10 . Then, SVM was used to do the binary classification based on the generat"
D16-1069,P12-1055,0,0.0186041,"s the basic classification unit and are classified one by one. In these methods, dictionaries play important effect in constructing features and avoiding meaningless outputs. Various previous works have demonstrated the significant positive effectiveness of the external dictionary (Zhang et al., 2010). However, because these external dictionaries are usually static and preconstructed, one of the main drawbacks of these methods is that the words which are not included in the dictionaries cannot be well processed. This issue has also been mentioned by numerous previous works (Peng et al., 2004; Liu et al., 2012). Hence, understanding how Chinese words are constructed can benefit a variety of Chinese NLP tasks to avoid meaningless output. For example, to generate the abbreviation for a named entity, we can use a binary classifier to determine whether a character should be removed or retained. Both “国 航” and “中国国航” are appropriate abbreviations for “中 国 国 际 航 空 公 司(Air China)”. However “国 航 司” is not a Chinese word and cannot be understood by humans. 721 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 721–730, c Austin, Texas, November 1-5, 2016. 2016 Assoc"
D16-1069,W13-3512,0,0.0773684,"Missing"
D16-1069,C04-1081,0,0.0848875,"cters are treated as the basic classification unit and are classified one by one. In these methods, dictionaries play important effect in constructing features and avoiding meaningless outputs. Various previous works have demonstrated the significant positive effectiveness of the external dictionary (Zhang et al., 2010). However, because these external dictionaries are usually static and preconstructed, one of the main drawbacks of these methods is that the words which are not included in the dictionaries cannot be well processed. This issue has also been mentioned by numerous previous works (Peng et al., 2004; Liu et al., 2012). Hence, understanding how Chinese words are constructed can benefit a variety of Chinese NLP tasks to avoid meaningless output. For example, to generate the abbreviation for a named entity, we can use a binary classifier to determine whether a character should be removed or retained. Both “国 航” and “中国国航” are appropriate abbreviations for “中 国 国 际 航 空 公 司(Air China)”. However “国 航 司” is not a Chinese word and cannot be understood by humans. 721 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 721–730, c Austin, Texas, November 1-"
D16-1069,P03-1050,0,0.0396788,"r words in Chinese text. Jin and Wong (2002) incorporated local statistical information, global statistical information and contextual constraints to identify Chinese words. For collecting Thai unknown words, Haruechaiyasak et al. (2006) proposes a collaborative framework for achieving the task based on Web pages over the Internet. Except these unsupervised methods, there have been other approaches requiring additional information or selective input. Yarowsky and Wicentowski (2000) proposed to use labeled corpus to train a supervised method for transforming pasttense in English. Rogati et al. (2003) introduced a stemming model based on statistical machine 728 translation for Arabic. They used a parallel corpus to train the model. Luong et al. (2013) studied the problem of word representations for rare and complex words. They proposed to combine recursive neural networks and neural language models to build representations for morphologically complex words from their morphemes. Since English is usually considered limited in terms of morphology, their method can handle unseen words, whose representations could be constructed from vectors of known morphemes. However, most of the existing Chi"
D16-1069,D11-1014,0,0.125641,"Missing"
D16-1069,H05-1054,0,0.0457712,"for Chinese Named Entities Using Recurrent Neural Network with Dynamic Dictionary Qi Zhang, Jin Qian, Ya Guo, Yaqian Zhou, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University Shanghai, P.R. China {qz, jqian12, yguo13, zhouyaqian, xjhuang}@fudan.edu.cn Abstract word segmenter produces erroneous output, the quality of these methods will be degraded as a direct result. Moreover, since the word segmenter may split the targets into two individual words, many methods adopted character-based methodologies, such as methods for named entity recognition (Wu et al., 2005), aspect-based opinion mining (Xu et al., 2014), and so on. Chinese named entities occur frequently in formal and informal environments. Various approaches have been formalized the problem as a sequence labelling task and utilize a character-based methodology, in which character is treated as the basic classification unit. One of the main drawbacks of these methods is that some of the generated abbreviations may not follow the conventional wisdom of Chinese. To address this problem, we propose a novel neural network architecture to perform task. It combines recurrent neural network (RNN) with"
D16-1069,D09-1159,1,0.849835,"Missing"
D16-1069,P14-1032,0,0.018155,"ral Network with Dynamic Dictionary Qi Zhang, Jin Qian, Ya Guo, Yaqian Zhou, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University Shanghai, P.R. China {qz, jqian12, yguo13, zhouyaqian, xjhuang}@fudan.edu.cn Abstract word segmenter produces erroneous output, the quality of these methods will be degraded as a direct result. Moreover, since the word segmenter may split the targets into two individual words, many methods adopted character-based methodologies, such as methods for named entity recognition (Wu et al., 2005), aspect-based opinion mining (Xu et al., 2014), and so on. Chinese named entities occur frequently in formal and informal environments. Various approaches have been formalized the problem as a sequence labelling task and utilize a character-based methodology, in which character is treated as the basic classification unit. One of the main drawbacks of these methods is that some of the generated abbreviations may not follow the conventional wisdom of Chinese. To address this problem, we propose a novel neural network architecture to perform task. It combines recurrent neural network (RNN) with an architecture determining whether a given seq"
D16-1069,N09-2069,0,0.45292,"Missing"
D16-1069,P00-1027,0,0.0213643,"from Chinese text corpora. Khoo (Khoo et al., 2002) introduced a method based on stepwise logistic regression to identify two-and three-character words in Chinese text. Jin and Wong (2002) incorporated local statistical information, global statistical information and contextual constraints to identify Chinese words. For collecting Thai unknown words, Haruechaiyasak et al. (2006) proposes a collaborative framework for achieving the task based on Web pages over the Internet. Except these unsupervised methods, there have been other approaches requiring additional information or selective input. Yarowsky and Wicentowski (2000) proposed to use labeled corpus to train a supervised method for transforming pasttense in English. Rogati et al. (2003) introduced a stemming model based on statistical machine 728 translation for Arabic. They used a parallel corpus to train the model. Luong et al. (2013) studied the problem of word representations for rare and complex words. They proposed to combine recursive neural networks and neural language models to build representations for morphologically complex words from their morphemes. Since English is usually considered limited in terms of morphology, their method can handle uns"
D16-1069,C10-2167,0,0.0882137,"Missing"
D16-1080,C10-2042,0,0.036687,"n (Medelyan and Witten, 2006). In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem. A well-known approach is the Term Frequency Inverse Document Frequency (TF-IDF) (Sparck Jones, 1972; Zhang et al., 2007; Lee and Kim, 2008). Measures like term frequencies (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009), inverse document frequencies, topic proportions, etc. and knowledge of specific domain are applied to rank terms in documents which are aggregated to score the phrases. The ranking based on tf-idf has been shown to work well in practice (Hasan and Ng, 2010). Mihalcea et al. proposed the TextRank, which constructs keyphrases using the PageRank values obtained on a graph based ranking model for graphs extracted from texts (Mihalcea and Tarau, 2004). Liu et al. proposed to extract keyphrases by adopting a clustering-based approach, which ensures that the document is semantically covered by these keyphrases (Liu et al., 2009). Ali Mehri et al. put 843 forward a method for ranking the words in texts, which can also be used to classify the correlation range between word-type occurrences in a text, by using non-extensive statistical mechanics (Mehri an"
D16-1080,W03-1028,0,0.772505,"Missing"
D16-1080,N09-1060,0,0.0787219,"l. (2004) applied Bayesian decision theory for keyword extraction. Medelyan and Witten extended the KEA to KEA++, which uses semantic information on terms and phrases extracted from a domain specific thesaurus, thus enhances automatic keyphrase extraction (Medelyan and Witten, 2006). In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem. A well-known approach is the Term Frequency Inverse Document Frequency (TF-IDF) (Sparck Jones, 1972; Zhang et al., 2007; Lee and Kim, 2008). Measures like term frequencies (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009), inverse document frequencies, topic proportions, etc. and knowledge of specific domain are applied to rank terms in documents which are aggregated to score the phrases. The ranking based on tf-idf has been shown to work well in practice (Hasan and Ng, 2010). Mihalcea et al. proposed the TextRank, which constructs keyphrases using the PageRank values obtained on a graph based ranking model for graphs extracted from texts (Mihalcea and Tarau, 2004). Liu et al. proposed to extract keyphrases by adopting a clustering-based approach, which ensures that the document is semantically covered by thes"
D16-1080,D09-1027,0,0.416574,"ment frequencies, topic proportions, etc. and knowledge of specific domain are applied to rank terms in documents which are aggregated to score the phrases. The ranking based on tf-idf has been shown to work well in practice (Hasan and Ng, 2010). Mihalcea et al. proposed the TextRank, which constructs keyphrases using the PageRank values obtained on a graph based ranking model for graphs extracted from texts (Mihalcea and Tarau, 2004). Liu et al. proposed to extract keyphrases by adopting a clustering-based approach, which ensures that the document is semantically covered by these keyphrases (Liu et al., 2009). Ali Mehri et al. put 843 forward a method for ranking the words in texts, which can also be used to classify the correlation range between word-type occurrences in a text, by using non-extensive statistical mechanics (Mehri and Darooneh, 2011). Recurrent neural networks(RNNs) (Elman, 1990) has been applied to many sequential prediction tasks, which is an important class of naturally deep architecture. In NLP, RNNs deal with a sentence as a sequence of tokens and have been successfully applied to various tasks like spoken language understanding (Mesnil et al., 2013) and language modeling (Mik"
D16-1080,P15-2105,0,0.133585,"Missing"
D16-1080,C08-2021,0,0.0494406,"Missing"
D16-1080,W15-3606,0,0.057334,"Missing"
D16-1080,N13-1026,0,0.0598647,"tify candidate phrases from the text. Tang et al. (2004) applied Bayesian decision theory for keyword extraction. Medelyan and Witten extended the KEA to KEA++, which uses semantic information on terms and phrases extracted from a domain specific thesaurus, thus enhances automatic keyphrase extraction (Medelyan and Witten, 2006). In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem. A well-known approach is the Term Frequency Inverse Document Frequency (TF-IDF) (Sparck Jones, 1972; Zhang et al., 2007; Lee and Kim, 2008). Measures like term frequencies (Wu and Giles, 2013; Rennie and Jaakkola, 2005; Kireyev, 2009), inverse document frequencies, topic proportions, etc. and knowledge of specific domain are applied to rank terms in documents which are aggregated to score the phrases. The ranking based on tf-idf has been shown to work well in practice (Hasan and Ng, 2010). Mihalcea et al. proposed the TextRank, which constructs keyphrases using the PageRank values obtained on a graph based ranking model for graphs extracted from texts (Mihalcea and Tarau, 2004). Liu et al. proposed to extract keyphrases by adopting a clustering-based approach, which ensures that t"
D16-1080,W04-3252,0,\N,Missing
D16-1080,P11-1039,0,\N,Missing
D17-1256,R13-1026,0,0.453604,"tics of the datasets used in this work are listed in Table 1. 2415 • Stanford POS Tagger: Stanford POS Tagger is a widely used tool for newswire domains (Toutanova et al., 2003). In this work, we train it using two different sets, the WSJ (sections 0-18) and a WSJ, IRC, and Twitter mixed corpus. We use StanfordWSJ and Stanford-MIX to represent them, respectively. • T-POS: T-Pos (Ritter et al., 2011) adopts the Conditional Random Fields and clustering algorithm to perform the task. It was trained from a mixture of hand-annotated tweets and existing POS-labeled data. • GATE Tagger: GATE tagger (Derczynski et al., 2013) is based on vote-constrained bootstrapping with unlabeled data. It combines cases where available taggers use different tagsets. • ARK Tagger: ARK tagger (Owoputi et al., 2013) is a system that reports the best accuracy on the RIT dataset. It uses unsupervised word clustering and a variety of lexical features. • bi-LSTM: Bidirectional Long Short-Term Memory (LSTM) networks have been widely used in a variety of sequence labeling tasks (Graves and Schmidhuber, 2005). In this work, we evaluate it at character level, word level, and combining them together. bi-LSTM (word level) uses one layer of"
D17-1256,P11-2008,0,0.0699662,"Missing"
D17-1256,N13-1097,0,0.0251818,"structured data from social media provides valuable information for a variety of applications such as stock prediction (Bollen et al., 2011), public health analysis (Wilson and Brownstein, 2009; Paul and Dredze, 2011), real-time event detection (Sakaki et al., 2010), and so on. The quality of these applications is highly impacted by the performance of natural language processing tasks. ∗ Corresponding author. u PRP Part-of-speech (POS) tagging is one of the most important natural language processing tasks. It has also been widely used in the social media analysis systems (Ritter et al., 2012; Lamb et al., 2013; Kiritchenko et al., 2014). Most stateof-the-art POS tagging approaches are based on supervised methods. Hence, they usually require a large amount of annotated data to train models. Many datasets have been constructed for POS tagging task. Because newswire articles are carefully edited, benchmarks usually use them for annotation (Marcus et al., 1993). However, usergenerated contents on social media are usually informal and contain many nonstandard lexical items. Moreover, the difference in domains between training data and evaluation data may heavily impact the performance of approaches base"
D17-1256,P15-1107,0,0.0393995,"Missing"
D17-1256,P16-1101,0,0.00689095,"tagging is an important preprocessing step and can provide valuable information for various natural language processing tasks. In recent years, deep learning algorithms have been successfully used for POS tagging. A number of approaches have been proposed and have achieved some progress. Santos and Guimaraes (2015) proposed using a character-based convolutional neural network to perform the POS tagging problem. Bi-LSTMs with word, character or unicode byte embedding were also introduced to achieve the POS tagging and named entity recognition tasks (Plank et al., 2016; Chiu and Nichols, 2015; Ma and Hovy, 2016). In this work, we study the problem from a domain adaption perspective. Inspired by these works, we also propose to use character-level methods to handle out-ofvocabulary words and bi-LSTMs to model the sequence relations. Adversarial networks were successfully used for image generation (Goodfellow et al., 2014; Dosovitskiy et al., 2015; Denton et al., 2015), domain adaption (Tzeng et al., 2014; Ganin et al., 2016), and semi-supervised learning (Denton et al., 2016). The key idea of adversarial networks for domain adaption is to construct invariant features by optimizing the feature extractor"
D17-1256,J93-2004,0,0.0607874,"al discriminator. In addition, we hypothesize that domain-specific features of target domain should be preserved in some degree. Hence, the proposed method adopts a sequence-to-sequence autoencoder to perform this task. Experimental results on three different datasets show that our method achieves better performance than state-of-the-art methods. 1 was talkin bout another time . nd i dnt VBD VBG IN DT NN . CC PRP VBP see u either ! VB PRP RB . Figure 1: An example of tagged Tweet, which contains nonstandard orthography, emoticon, and abbreviation. The tagset is defined similar as that of PTB (Marcus et al., 1993). Introduction During the last decade, social media have become extremely popular, on which billions of usergenerated contents are posted every day. Many users have been writing about their thoughts and lives on the go. The massive unstructured data from social media provides valuable information for a variety of applications such as stock prediction (Bollen et al., 2011), public health analysis (Wilson and Brownstein, 2009; Paul and Dredze, 2011), real-time event detection (Sakaki et al., 2010), and so on. The quality of these applications is highly impacted by the performance of natural lang"
D17-1256,N13-1039,0,0.0473268,"Missing"
D17-1256,Q17-1036,0,0.0580385,"Missing"
D17-1256,P16-2067,0,0.0287739,"ure knowledge. 5 Related Work Part-of-Speech tagging is an important preprocessing step and can provide valuable information for various natural language processing tasks. In recent years, deep learning algorithms have been successfully used for POS tagging. A number of approaches have been proposed and have achieved some progress. Santos and Guimaraes (2015) proposed using a character-based convolutional neural network to perform the POS tagging problem. Bi-LSTMs with word, character or unicode byte embedding were also introduced to achieve the POS tagging and named entity recognition tasks (Plank et al., 2016; Chiu and Nichols, 2015; Ma and Hovy, 2016). In this work, we study the problem from a domain adaption perspective. Inspired by these works, we also propose to use character-level methods to handle out-ofvocabulary words and bi-LSTMs to model the sequence relations. Adversarial networks were successfully used for image generation (Goodfellow et al., 2014; Dosovitskiy et al., 2015; Denton et al., 2015), domain adaption (Tzeng et al., 2014; Ganin et al., 2016), and semi-supervised learning (Denton et al., 2016). The key idea of adversarial networks for domain adaption is to construct invariant"
D17-1256,D11-1141,0,0.264053,"Missing"
D17-1256,W15-3904,0,0.0247453,"for proper nouns recognition, which fires on names from many sources, such as Freebase lists of celebrities, the Moby Words list of US Locations, proper names from Mark Kantrowitz’s name corpus and so on. So, our model is also competitive when lacking of manual feature knowledge. 5 Related Work Part-of-Speech tagging is an important preprocessing step and can provide valuable information for various natural language processing tasks. In recent years, deep learning algorithms have been successfully used for POS tagging. A number of approaches have been proposed and have achieved some progress. Santos and Guimaraes (2015) proposed using a character-based convolutional neural network to perform the POS tagging problem. Bi-LSTMs with word, character or unicode byte embedding were also introduced to achieve the POS tagging and named entity recognition tasks (Plank et al., 2016; Chiu and Nichols, 2015; Ma and Hovy, 2016). In this work, we study the problem from a domain adaption perspective. Inspired by these works, we also propose to use character-level methods to handle out-ofvocabulary words and bi-LSTMs to model the sequence relations. Adversarial networks were successfully used for image generation (Goodfello"
D17-1256,N03-1033,0,0.0622392,"g POS tagging approaches, we compare the proposed method with other approaches on three benchmarks: RIT-Twitter (Ritter et al., 2011), NPSCHAT (Forsyth, 2007), and ARKTwitter (Gimpel et al., 2011). Unlabeled in-domain data. For training the adversarial network, we need to use a dataset that has large scale unlabeled tweets. Hence, in this work, we construct large scale unlabeled data (UNL), from Twitter using its API. The detailed data statistics of the datasets used in this work are listed in Table 1. 2415 • Stanford POS Tagger: Stanford POS Tagger is a widely used tool for newswire domains (Toutanova et al., 2003). In this work, we train it using two different sets, the WSJ (sections 0-18) and a WSJ, IRC, and Twitter mixed corpus. We use StanfordWSJ and Stanford-MIX to represent them, respectively. • T-POS: T-Pos (Ritter et al., 2011) adopts the Conditional Random Fields and clustering algorithm to perform the task. It was trained from a mixture of hand-annotated tweets and existing POS-labeled data. • GATE Tagger: GATE tagger (Derczynski et al., 2013) is based on vote-constrained bootstrapping with unlabeled data. It combines cases where available taggers use different tagsets. • ARK Tagger: ARK tagge"
D18-1275,R13-1026,0,0.0167791,"tter NPSChat ARK-Twitter #Train 10652 40497 26594 #Dev 2242 - #Test 2291 4500 7707 Table 1: The statistics of the datasets used in our experiments, where # represents the number of tokens in datasets. where zi is the one-hot vector of the POS tagging label corresponding to xi . zˆi is the output of the top softmax layer: zˆi = sof tmax(M LP (hi )). 3 Experimental Setup In this section, we will first detail the datasets we used. Then, we will describe several baseline methods, including a number of classic taggers and a series of deep learning sequence labeling methods. 3.1 Datasets Following (Derczynski et al., 2013), we use RIT-Twitter (Ritter et al., 2011) as our main dataset. The RIT-Twitter was split into training, development and evaluation sets (RIT-Train, RITDev, RIT-Test). The splitting method was shown in (Derczynski et al., 2013). In order to verify the validity of our model, we also tested it on two more datasets, NPSChat (Forsythand and Martell, 2007), and ARK-Twitter (Gimpel et al., 2011) using standard splits. The tag-sets of the RIT-Twitter and NPSChat are PTB-like, while that of the ARK-Twitter is specific. In order to use WSJ labeled data in experiments on ARKTwitter, we performed the map"
D18-1275,J93-2004,0,0.0606697,"-Twitter and NPSChat are PTB-like, while that of the ARK-Twitter is specific. In order to use WSJ labeled data in experiments on ARKTwitter, we performed the mapping from PTB tagsets to ARK tag-sets, according to the PTB POS Tagging Guidelines (Santorini, 1990) and ARK Guidelines1 . The mapping proceeded from fine to coarse. For pretraining the word embedding, we constructed a dataset containing 30 million tweets, from Twitter using its API. We introduced a newswire dataset containing 1173K tokens as the written language dataset, namely the Wall Street Journal (WSJ) from the Penn TreeBank v3 (Marcus et al., 1993). During training, we mixed each of RIT-Twitter, NPSChat and ARKTwitter with WSJ into three kinds of training data. The detailed data statistics of the above datasets used in this work are listed in Table 1. i 2543 1 http://www.ark.cs.cmu.edu/TweetNLP/ 3.2 3.3 Competitor Methods We applied several classic and state-of-the-art methods for comparison. In addition, we used a series of deep learning sequence labeling methods as baselines for comparison, as follows: Stanford POS Tagger is a widely used partof-speech taggers described in (Toutanova et al., 2003). It demonstrates the broad use of fea"
D18-1275,N13-1039,0,0.425412,"Missing"
D18-1275,D11-1141,0,0.21874,"594 #Dev 2242 - #Test 2291 4500 7707 Table 1: The statistics of the datasets used in our experiments, where # represents the number of tokens in datasets. where zi is the one-hot vector of the POS tagging label corresponding to xi . zˆi is the output of the top softmax layer: zˆi = sof tmax(M LP (hi )). 3 Experimental Setup In this section, we will first detail the datasets we used. Then, we will describe several baseline methods, including a number of classic taggers and a series of deep learning sequence labeling methods. 3.1 Datasets Following (Derczynski et al., 2013), we use RIT-Twitter (Ritter et al., 2011) as our main dataset. The RIT-Twitter was split into training, development and evaluation sets (RIT-Train, RITDev, RIT-Test). The splitting method was shown in (Derczynski et al., 2013). In order to verify the validity of our model, we also tested it on two more datasets, NPSChat (Forsythand and Martell, 2007), and ARK-Twitter (Gimpel et al., 2011) using standard splits. The tag-sets of the RIT-Twitter and NPSChat are PTB-like, while that of the ARK-Twitter is specific. In order to use WSJ labeled data in experiments on ARKTwitter, we performed the mapping from PTB tagsets to ARK tag-sets, acc"
D18-1275,N03-1033,0,0.0360337,"Missing"
D19-1096,N13-1006,0,0.385532,"rent domains and the baseline methods applied for comparison. We also detail the hyperparameter configuration of the proposed model. Our codes and datasets can be found at https: //github.com/RowitZou/LGN. 4.1 Data We conducted experiments on four Chinese NER datasets. (1) OntoNotes 4.0 (Weischedel et al., 2011): OntoNotes is a manually annotated multilingual corpus in the news domain that contains various text annotations, including Chinese named entity labels. Gold-standard segmentation is available. We only use Chinese documents (about 16k sentences) and process the data in the same way as Che et al. (2013). (2) MSRA (Levow, 2006): MSRA is also a dataset in the news domain and contains three types of named entities: LOC, PER, and ORG. Chinese word segmentation is available in the training set but not in the test set. (3) Weibo NER (Peng and Dredze, 2015): It consists of annotated NER messages drawn from the social media Sina Weibo2 . The corpus contains PER, ORG, GPE, and LOC for both named entity and nominal mention. (4) Resume NER (Zhang and Yang, 2018): It is composed of resumes collected from Sina Finance3 and is annotated with 8 types of named entities. Both Weibo and Resume datasets do not"
D19-1096,W06-0130,0,0.804173,"Missing"
D19-1096,D15-1141,1,0.763206,"Missing"
D19-1096,P15-1017,0,0.443298,"equence 河流 River Figure 1: Example of word character lattice with partial input. Because of the characteristic of chain structure, RNN-based methods must predict the label “度” using only previous partial sequences “印度 (India)”, which may suffer from word ambiguities without global sentence semantics. The task of named entity recognition (NER) involves determining entity boundaries and recognizing categories of named entities, which is a fundamental task in the field of natural language processing (NLP). NER plays an important role in many downstream NLP tasks, including information retrieval (Chen et al., 2015b), relation extraction (Bunescu and Mooney, 2005), question answering systems (Diefenbach et al., 2018), and other applications. Compared with English NER, Chinese named entities are more difficult to identify due to their uncertain boundaries, complex composition, and NE definitions within the nest (Duan and Zheng, 2011). One intuitive way to alleviate word boundary problems is to first perform word segmentation Equal contribution. M-LOC ? Yin Introduction ∗ B-LOC E-GPE B-GPE and then apply word sequence labeling (Yang et al., 2016; He and Sun, 2017). However, the rare gold-standard segmenta"
D19-1096,N19-1423,0,0.052104,"Missing"
D19-1096,li-etal-2014-comparison,0,0.377738,"Missing"
D19-1096,L16-1138,0,0.203827,"Missing"
D19-1096,D15-1104,0,0.0258868,"vely aggregating mechanism; 3) several experimental results demonstrate the effectiveness of the proposed method in different aspects. 2 2.1 Related Work Chinese NER with Lexicon. Some previous Chinese NER researches have compared word-based and character-based methods (Li et al., 2014) and show that due to the limited performance of the current Chinese word segmentation, character-based name taggers can outperform their word-based counterparts (He and Wang, 2008; Liu et al., 2010). Lexicon features have been widely used to better leverage word information for Chinese NER (Huang et al., 2015; Luo et al., 2015; Gui et al., 2019). Especially, Zhang and Yang (2018) proposed a lattice LSTM to model characters and potential words simultaneously. However, their lattice LSTM used a concatenation of independently trained left-toright and right-to-left LSTM to represent features, which was also limited (Devlin et al., 2018). In this work, we propose a novel character-based method that treats the named entities as a node classification task. The proposed method can utilize global information (both the left and the right context) (Dong et al., 2019) to tackle word ambiguities. 2.2 Graph Neural Networks on Te"
D19-1096,I08-4022,0,0.0863472,"on task; 2) the proposed model can capture global context information and local compositions to tackle Chinese word ambiguity problems through recursively aggregating mechanism; 3) several experimental results demonstrate the effectiveness of the proposed method in different aspects. 2 2.1 Related Work Chinese NER with Lexicon. Some previous Chinese NER researches have compared word-based and character-based methods (Li et al., 2014) and show that due to the limited performance of the current Chinese word segmentation, character-based name taggers can outperform their word-based counterparts (He and Wang, 2008; Liu et al., 2010). Lexicon features have been widely used to better leverage word information for Chinese NER (Huang et al., 2015; Luo et al., 2015; Gui et al., 2019). Especially, Zhang and Yang (2018) proposed a lattice LSTM to model characters and potential words simultaneously. However, their lattice LSTM used a concatenation of independently trained left-toright and right-to-left LSTM to represent features, which was also limited (Devlin et al., 2018). In this work, we propose a novel character-based method that treats the named entities as a node classification task. The proposed method"
D19-1096,D14-1181,0,0.00888242,"Missing"
D19-1096,W06-0115,0,0.819159,"methods applied for comparison. We also detail the hyperparameter configuration of the proposed model. Our codes and datasets can be found at https: //github.com/RowitZou/LGN. 4.1 Data We conducted experiments on four Chinese NER datasets. (1) OntoNotes 4.0 (Weischedel et al., 2011): OntoNotes is a manually annotated multilingual corpus in the news domain that contains various text annotations, including Chinese named entity labels. Gold-standard segmentation is available. We only use Chinese documents (about 16k sentences) and process the data in the same way as Che et al. (2013). (2) MSRA (Levow, 2006): MSRA is also a dataset in the news domain and contains three types of named entities: LOC, PER, and ORG. Chinese word segmentation is available in the training set but not in the test set. (3) Weibo NER (Peng and Dredze, 2015): It consists of annotated NER messages drawn from the social media Sina Weibo2 . The corpus contains PER, ORG, GPE, and LOC for both named entity and nominal mention. (4) Resume NER (Zhang and Yang, 2018): It is composed of resumes collected from Sina Finance3 and is annotated with 8 types of named entities. Both Weibo and Resume datasets do not contain the gold-standa"
D19-1096,W14-1609,0,0.0468043,"nest (Duan and Zheng, 2011). One intuitive way to alleviate word boundary problems is to first perform word segmentation Equal contribution. M-LOC ? Yin Introduction ∗ B-LOC E-GPE B-GPE and then apply word sequence labeling (Yang et al., 2016; He and Sun, 2017). However, the rare gold-standard segmentation in NER datasets and incorrectly segmented entity boundaries both negatively impact the identification of named entities (Peng and Dredze, 2015; He and Sun, 2016). Hence, character-level Chinese NER using lexicon features to better leverage word information has attracted research attention (Passos et al., 2014; Zhang and Yang, 2018). In particular, Zhang and Yang (2018) introduced a variant of a long short-term memory network (latticestructured LSTM) that encodes all potential words matching a sentence to exploit explicit word information, achieving state-of-the-art results. However, these methods are usually based on RNN or CRF to sequentially encode a sentence, while the underlying structure of language is not strictly sequential (Shen et al., 2019). As a result, these models would encounter serious word ambiguity problems (Mich et al., 2000). Especially in Chinese texts, the recognition of named"
D19-1096,D15-1064,0,0.488836,"ns. Compared with English NER, Chinese named entities are more difficult to identify due to their uncertain boundaries, complex composition, and NE definitions within the nest (Duan and Zheng, 2011). One intuitive way to alleviate word boundary problems is to first perform word segmentation Equal contribution. M-LOC ? Yin Introduction ∗ B-LOC E-GPE B-GPE and then apply word sequence labeling (Yang et al., 2016; He and Sun, 2017). However, the rare gold-standard segmentation in NER datasets and incorrectly segmented entity boundaries both negatively impact the identification of named entities (Peng and Dredze, 2015; He and Sun, 2016). Hence, character-level Chinese NER using lexicon features to better leverage word information has attracted research attention (Passos et al., 2014; Zhang and Yang, 2018). In particular, Zhang and Yang (2018) introduced a variant of a long short-term memory network (latticestructured LSTM) that encodes all potential words matching a sentence to exploit explicit word information, achieving state-of-the-art results. However, these methods are usually based on RNN or CRF to sequentially encode a sentence, while the underlying structure of language is not strictly sequential ("
D19-1096,W06-0126,0,0.781484,"Missing"
D19-1096,P18-1030,0,0.0335756,"Missing"
D19-1096,P18-1144,0,0.484748,", 2011). One intuitive way to alleviate word boundary problems is to first perform word segmentation Equal contribution. M-LOC ? Yin Introduction ∗ B-LOC E-GPE B-GPE and then apply word sequence labeling (Yang et al., 2016; He and Sun, 2017). However, the rare gold-standard segmentation in NER datasets and incorrectly segmented entity boundaries both negatively impact the identification of named entities (Peng and Dredze, 2015; He and Sun, 2016). Hence, character-level Chinese NER using lexicon features to better leverage word information has attracted research attention (Passos et al., 2014; Zhang and Yang, 2018). In particular, Zhang and Yang (2018) introduced a variant of a long short-term memory network (latticestructured LSTM) that encodes all potential words matching a sentence to exploit explicit word information, achieving state-of-the-art results. However, these methods are usually based on RNN or CRF to sequentially encode a sentence, while the underlying structure of language is not strictly sequential (Shen et al., 2019). As a result, these models would encounter serious word ambiguity problems (Mich et al., 2000). Especially in Chinese texts, the recognition of named entities with overlapp"
D19-1096,D18-1244,0,0.383117,"er, their lattice LSTM used a concatenation of independently trained left-toright and right-to-left LSTM to represent features, which was also limited (Devlin et al., 2018). In this work, we propose a novel character-based method that treats the named entities as a node classification task. The proposed method can utilize global information (both the left and the right context) (Dong et al., 2019) to tackle word ambiguities. 2.2 Graph Neural Networks on Texts Graph neural networks have been successfully applied to several text classification tasks (Veliˇckovi´c et al., 2017; Yao et al., 2018; Zhang et al., 2018b). Peng et al. (2018) proposed a GCNbased deep learning model for text classification. Zhang et al. (2018c) proposed using the dependency parse trees to construct a graph for relation extraction. Recently, multi-head attention mechanisms (Vaswani et al., 2017) have been widely used by graph neural networks during the fusion process (Zhang et al., 2018a; Lee et al., 2018), which can aggregate graph information by assigning different weights to neighboring nodes or associated edges. Given a set of vectors H ∈ ˆ ∈ R1×d , and a set of Rn×d , a query vector q trainable parameters W, this mechanism"
D19-1096,W03-1728,0,0.122763,"hese methods are based on character sequences. We applied the bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kim, 2014) as classic baseline methods. Character-level methods + bichar + softword: Character bigrams are useful for capturing adjacent features and representing characters. We concatenated bigram embeddings with character embeddings to better leverage the bigram information. In addition, we added the segmentation information by incorporating segmentation label embeddings into the character representation. The BMES scheme is used for representing the word segmentation (Xue and Shen, 2003). Word-level methods: For the datasets with gold segmentation, we directly employed wordlevel NER methods to evaluate the performance, which are denoted as Gold seg. Otherwise, we first used open source segmentation toolkit6 to automatically segment the datasets. Then wordlevel NER methods are applied, which are denoted as Auto seg. The bi-directional LSTM and CNN are also applied as baselines. Word-level methods + char + bichar: For characters in the subsequence wb,e , we first used a bi-directional LSTM to learn their hidden states and bigram states. We then augmented the wordlevel methods w"
D19-1271,D15-1075,0,0.670997,"relationships between the two sentences include entailment (if the premise is true, then the hypothesis must be true), contradiction (if the premise is true, then the hypothesis must be false), and neutral (neither entailment nor contradiction). As a core task, conventional approaches have studied various aspects of the inference prob∗ † Equal contribution. Alphabetical order of the last name. Corresponding author. lem (MacCartney and Manning, 2008; Heilman and Smith, 2010). Thanks to the release of the largest publicly available corpus - the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), neural network-based models have also been successfully used for this task (Parikh et al., 2016; Chen et al., 2016; Tay et al., 2018; Duan et al., 2018). These methods typically treat the premise sentence and the hypothesis sentence equally, learn an alignment of sub-phrases in both sentences symmetrically and in parallel, and fuse local information for making a global decision at the sentence level. They all frame the inference problem as a semantic matching task and ignore the reasoning process. However, different from a simple semantic matching task, reasoning should be asynchronous and f"
D19-1271,P16-1139,0,0.0218827,"d features such as syntactic information, n-gram overlapping and so on (Bowman et al., 2015; Heilman and Smith, 2010). Benefiting from the development of deep learning and the availability of large-scale annotated datasets (Bowman et al., 2015), neural networkbased models have also been successfully used for this task. And two categories of neural networkbased models have been developed for this problem. The first set of models is sentence encodingbased and aims to find vector representation for each sentence and classifies the relation by using the concatenation of two vector representation (Bowman et al., 2016; Nangia et al., 2017; Mou et al., 2015). However, this kind of framework ignores the interaction between two sentences. The other set of models uses the cross-sentence feature or inter-sentence attention from one sentence to another, and is hence referred to as a matching-aggregation framework. Parikh et al. (2016) use attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. Chen et al. (2016) propose a state-of-the-art model for the natural language inference (NLI) task. It is a sequential model that incorporates the chain LS"
D19-1271,P18-1224,0,0.5016,"mbedding, and syntactical features as the sentence representation. The word embedding is obtained by mapping token to high dimensional vector space by pre-trained word vector (300D Glove 840B), and the word embedding is updated during training. Character-level embedding could alleviate out-of-vocabulary (OOV) problems and capture helpful morphological information. As in (Kim et al., 2016; Lee et al., 2016), we filter the character embedding with 1D convolution kernel. The character convolutional feature maps are then max pooled over the time dimension for each token to obtain a vector. As in (Chen et al., 2018), the syntactical features consist of one-hot part-of-speech (POS) tagging feature and binary exact match (EM) feature. For one sentence, the EM value is activated if the same word is found in the other sentence. Next, ADIN adopts bidirectional Long ShortTerm Memory network (Bi-LSTM) (Graves and Schmidhuber, 2005) to model the internal temporal interaction on both directions of the sentences. Consider a premise sentence p and a hypothesis sentence q, we have got their multi-level features representation. Suppose the length of p and q are m and n respectively. These multi-level features represe"
D19-1271,N10-1145,0,0.254214,"intelligence. The goal of NLI is to predict whether a premise sentence can infer another hypothesis sentence. As illustrated in Table 1, logical relationships between the two sentences include entailment (if the premise is true, then the hypothesis must be true), contradiction (if the premise is true, then the hypothesis must be false), and neutral (neither entailment nor contradiction). As a core task, conventional approaches have studied various aspects of the inference prob∗ † Equal contribution. Alphabetical order of the last name. Corresponding author. lem (MacCartney and Manning, 2008; Heilman and Smith, 2010). Thanks to the release of the largest publicly available corpus - the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), neural network-based models have also been successfully used for this task (Parikh et al., 2016; Chen et al., 2016; Tay et al., 2018; Duan et al., 2018). These methods typically treat the premise sentence and the hypothesis sentence equally, learn an alignment of sub-phrases in both sentences symmetrically and in parallel, and fuse local information for making a global decision at the sentence level. They all frame the inference problem as a semantic m"
D19-1271,P16-1160,0,0.020495,"lov et al., 2013), and fasttext (Joulin et al., 2016). It can also incorporate more syntactical and lexical information into the feature vector. For ADIN, we use a concatenation of word embedding, character embedding, and syntactical features as the sentence representation. The word embedding is obtained by mapping token to high dimensional vector space by pre-trained word vector (300D Glove 840B), and the word embedding is updated during training. Character-level embedding could alleviate out-of-vocabulary (OOV) problems and capture helpful morphological information. As in (Kim et al., 2016; Lee et al., 2016), we filter the character embedding with 1D convolution kernel. The character convolutional feature maps are then max pooled over the time dimension for each token to obtain a vector. As in (Chen et al., 2018), the syntactical features consist of one-hot part-of-speech (POS) tagging feature and binary exact match (EM) feature. For one sentence, the EM value is activated if the same word is found in the other sentence. Next, ADIN adopts bidirectional Long ShortTerm Memory network (Bi-LSTM) (Graves and Schmidhuber, 2005) to model the internal temporal interaction on both directions of the senten"
D19-1271,C08-1066,0,0.0462228,"e understanding and artificial intelligence. The goal of NLI is to predict whether a premise sentence can infer another hypothesis sentence. As illustrated in Table 1, logical relationships between the two sentences include entailment (if the premise is true, then the hypothesis must be true), contradiction (if the premise is true, then the hypothesis must be false), and neutral (neither entailment nor contradiction). As a core task, conventional approaches have studied various aspects of the inference prob∗ † Equal contribution. Alphabetical order of the last name. Corresponding author. lem (MacCartney and Manning, 2008; Heilman and Smith, 2010). Thanks to the release of the largest publicly available corpus - the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), neural network-based models have also been successfully used for this task (Parikh et al., 2016; Chen et al., 2016; Tay et al., 2018; Duan et al., 2018). These methods typically treat the premise sentence and the hypothesis sentence equally, learn an alignment of sub-phrases in both sentences symmetrically and in parallel, and fuse local information for making a global decision at the sentence level. They all frame the inferen"
D19-1271,W17-5301,0,0.0162782,"ntactic information, n-gram overlapping and so on (Bowman et al., 2015; Heilman and Smith, 2010). Benefiting from the development of deep learning and the availability of large-scale annotated datasets (Bowman et al., 2015), neural networkbased models have also been successfully used for this task. And two categories of neural networkbased models have been developed for this problem. The first set of models is sentence encodingbased and aims to find vector representation for each sentence and classifies the relation by using the concatenation of two vector representation (Bowman et al., 2016; Nangia et al., 2017; Mou et al., 2015). However, this kind of framework ignores the interaction between two sentences. The other set of models uses the cross-sentence feature or inter-sentence attention from one sentence to another, and is hence referred to as a matching-aggregation framework. Parikh et al. (2016) use attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. Chen et al. (2016) propose a state-of-the-art model for the natural language inference (NLI) task. It is a sequential model that incorporates the chain LSTM and the tree LSTM"
D19-1271,D16-1244,0,0.164459,"Missing"
D19-1271,D14-1162,0,0.0816097,"or sentence b: ˜ b = (h ˜b , h ˜b , ..., h ˜bn ), H 1 2 ˆ b = LayerN orm(H ˜ b ), H (6) (7) Where LayerN orm(.) is layer normalization (Ba ˆ b is a 2D-tensor that has et al., 2016). The result H the same shape as Hb , and we refer to the whole inferential module as: The information representation layer converts each word or phrase in the sentences into a vector representation and constructs the representation matrix for the sentences. We combine the multi-level features as the sentence representation. Each token is represented as a vector by using the pre-trained word embedding such as GloVe (Pennington et al., 2014), word2Vec (Mikolov et al., 2013), and fasttext (Joulin et al., 2016). It can also incorporate more syntactical and lexical information into the feature vector. For ADIN, we use a concatenation of word embedding, character embedding, and syntactical features as the sentence representation. The word embedding is obtained by mapping token to high dimensional vector space by pre-trained word vector (300D Glove 840B), and the word embedding is updated during training. Character-level embedding could alleviate out-of-vocabulary (OOV) problems and capture helpful morphological information. As in (Ki"
D19-1271,D18-1185,0,0.522206,"Missing"
D19-5716,P18-1047,0,0.107257,"ion methodology and pipeline method which firstly do named entity recognition (NER) and then do relation extraction on the results of NER. (Zheng et al., 2017) proposes a novel tagging schema (NTS) that encodes relation type in the NER tag to recognize the named entity and extract the relation between them jointly. This methodology has a fatal flaw that it can not handle relation facts that share the same entity and this phenomenon is common in BB task. (Bekoulis et al., 2018) proposes a multihead selection layer (MHS) to model the relation of each entity pair which is similar to our method. (Zeng et al., 2018) proposes a sequence to sequence model with copy mechanism (Copy RE). However, all above the previous work has been done on a large-scale general dataset. While the Bacteria Biotope rel+ner task only bases on a domain-specific and comparatively small dataset. Under this background, we adapt a recently widely used transfer learning framework, BERT(Devlin et al., 2018), and pre-train it on large-scale corpus using two novel unsupervised prediction tasks to mitigate the problem of insufficient data. Introduction Information extraction aims to recognize the entities and classify the relations betw"
D19-5716,P17-1113,0,0.168209,"the relation extraction layer and mention recognition layer designed by us on the top of BERT to extract mentions and relations simultaneously. The evaluation results show that our method achieves the best performance on all metrics (including slot error rate, precision and recall) in the Bacteria Biotope rel+ner subtask. 1 Some previous work has been done in handling such an information extraction problem, including some joint entity and relation extraction methodology and pipeline method which firstly do named entity recognition (NER) and then do relation extraction on the results of NER. (Zheng et al., 2017) proposes a novel tagging schema (NTS) that encodes relation type in the NER tag to recognize the named entity and extract the relation between them jointly. This methodology has a fatal flaw that it can not handle relation facts that share the same entity and this phenomenon is common in BB task. (Bekoulis et al., 2018) proposes a multihead selection layer (MHS) to model the relation of each entity pair which is similar to our method. (Zeng et al., 2018) proposes a sequence to sequence model with copy mechanism (Copy RE). However, all above the previous work has been done on a large-scale gen"
E17-1097,N15-1146,0,0.0240204,". We aim to make all algorithms simple, fast and scalable for large-scale corpus. Our system is tested on Amazon review data which contains 15 different domains and 33 million reviews. The output database contains 72.5 million pairs of opinion relations. Extensive experiments have been conducted on various aspects of the algorithm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annot"
E17-1097,P15-1061,0,0.0341312,"orithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; Wang et al.,"
E17-1097,C08-1031,0,0.0342543,"ception”, “unit”), and (“touch your heart”, “The Passion of The Christ”). Extracting opinion bearing relations is usually the first step towards fine-gained analysis of opinion in texts, and plays an important role in other sentiment related applications (e.g., sentiment summarization). The goal of this paper is to extract opinion relations from open domain largescale opinion bearing texts. Previous works on fine-grained opinion information extraction have achieved notable success on many aspects: various domains were examined (Pontiki et al., 2015), different types of relations were studied (Ganapathibhotla and Liu, 2008; Narayanan et al., 2009; Wu et al., 2011), and both supervised and unsupervised (patternbased) algorithms were applied. But we have observed some difficulties when trying to use existing methods. The pattern based methods (both lexical patterns and syntactic patterns) are simple, fast, and scalable on large-scale datasets. However, the robustness of patterns is usually questionable in practice. For example, syntactic patterns are sensitive to errors in parse trees, which are common in user generated contents. Lexical patterns either have limited coverage (e.g., fixed set of patterns (Riloff a"
E17-1097,J13-3002,0,0.0219279,"hich contains 15 different domains and 33 million reviews. The output database contains 72.5 million pairs of opinion relations. Extensive experiments have been conducted on various aspects of the algorithm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootst"
E17-1097,klinger-cimiano-2014-usage,0,0.156827,"figurations We extract opinion relations on a subset of Amazon product review corpus provided by (McAuley et al., 2015), which contains 15 domains and 33 million reviews. The statistics of extracted relations are in Table 2. For quantitative evaluation, we select four domains (Cell Phones, Movie and TV, Food, Pet Supplies) for detailed analyses. We manually label all correct opinion relations in 1000 sentences, and select 200 sentences as the development set, the rest 800 as the test set 3 . Furthermore, to compare with previous supervised methods, we also conduct experiments on USAGE corpus (Klinger and Cimiano, 2014) which annotates 4481 opinion relations for 8 products. We use NLTK (Bird et al., 2009) for sentence splitting and word segmentation, Stanford parser 4 for getting POS tags, phrase chunks and dependency trees, and scikit-learn toolkit (Pedregosa et al., 2011) and TensorFlow 5 for machine learning algorithms. The general purpose opinion lexicon is from (Wilson et al., 2005). 4.2 Main Results Table 4 shows results on four domains. The methods for comparison are: • Adjacent is a simple baseline system from (Hu and Liu, 2004). It first identifies words in the general purpose opinion lexicon, then"
E17-1097,D07-1114,0,0.0320979,"on Amazon review data which contains 15 different domains and 33 million reviews. The output database contains 72.5 million pairs of opinion relations. Extensive experiments have been conducted on various aspects of the algorithm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Appr"
E17-1097,C10-1074,0,0.0363762,"thm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models"
E17-1097,D15-1278,0,0.0212734,"tantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu"
E17-1097,S14-2004,0,0.183539,"Missing"
E17-1097,P14-1030,0,0.0171181,"t candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and"
E17-1097,S15-2082,0,0.0608991,"Missing"
E17-1097,P09-1113,0,0.724846,"rdie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network struc"
E17-1097,P16-1105,0,0.0208472,"or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; Wang et al., 2015). Instead of locating the opinion expressions, aspect-based opinion mining di"
E17-1097,P12-1036,0,0.0202368,"d joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models"
E17-1097,D09-1019,0,0.0283461,"your heart”, “The Passion of The Christ”). Extracting opinion bearing relations is usually the first step towards fine-gained analysis of opinion in texts, and plays an important role in other sentiment related applications (e.g., sentiment summarization). The goal of this paper is to extract opinion relations from open domain largescale opinion bearing texts. Previous works on fine-grained opinion information extraction have achieved notable success on many aspects: various domains were examined (Pontiki et al., 2015), different types of relations were studied (Ganapathibhotla and Liu, 2008; Narayanan et al., 2009; Wu et al., 2011), and both supervised and unsupervised (patternbased) algorithms were applied. But we have observed some difficulties when trying to use existing methods. The pattern based methods (both lexical patterns and syntactic patterns) are simple, fast, and scalable on large-scale datasets. However, the robustness of patterns is usually questionable in practice. For example, syntactic patterns are sensitive to errors in parse trees, which are common in user generated contents. Lexical patterns either have limited coverage (e.g., fixed set of patterns (Riloff and Wiebe, 2003)), or har"
E17-1097,J11-1002,0,0.140584,"Missing"
E17-1097,W03-1014,0,0.19536,"iu, 2008; Narayanan et al., 2009; Wu et al., 2011), and both supervised and unsupervised (patternbased) algorithms were applied. But we have observed some difficulties when trying to use existing methods. The pattern based methods (both lexical patterns and syntactic patterns) are simple, fast, and scalable on large-scale datasets. However, the robustness of patterns is usually questionable in practice. For example, syntactic patterns are sensitive to errors in parse trees, which are common in user generated contents. Lexical patterns either have limited coverage (e.g., fixed set of patterns (Riloff and Wiebe, 2003)), or hard-tocontrol noise (e.g., bootstrapping approaches (Qiu et al., 2011)). On the other hand, supervised models can achieve better performances than patterns on manually labeled datasets, but it is often difficult to obtain large number of annotations for the relation extraction task, and the trained models are 1033 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1033–1043, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics also limited to specified domains. Thus, we s"
E17-1097,P08-1036,0,0.0501098,"Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wan"
E17-1097,N16-1065,0,0.0589031,"Missing"
E17-1097,P15-1060,0,0.041169,"Missing"
E17-1097,H05-1044,0,0.0959387,"nion relations in 1000 sentences, and select 200 sentences as the development set, the rest 800 as the test set 3 . Furthermore, to compare with previous supervised methods, we also conduct experiments on USAGE corpus (Klinger and Cimiano, 2014) which annotates 4481 opinion relations for 8 products. We use NLTK (Bird et al., 2009) for sentence splitting and word segmentation, Stanford parser 4 for getting POS tags, phrase chunks and dependency trees, and scikit-learn toolkit (Pedregosa et al., 2011) and TensorFlow 5 for machine learning algorithms. The general purpose opinion lexicon is from (Wilson et al., 2005). 4.2 Main Results Table 4 shows results on four domains. The methods for comparison are: • Adjacent is a simple baseline system from (Hu and Liu, 2004). It first identifies words in the general purpose opinion lexicon, then finds the nearest noun or verb phrase to them as their opinion targets. 3 https://github.com/AntNLP/OpinionRelationCorpus http://nlp.stanford.edu/software/lex-parser.shtml 5 https://www.tensorflow.org/ Domain #Reviews #Sents #Relations Cell Phones Movie and TV Food Pet Supplies Automotive Digital Music Beauty Toys and Games Instruments Office Products Patio Baby Clothing S"
E17-1097,D09-1159,1,0.780249,"cts of the algorithm, and the performances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabili"
E17-1097,D11-1123,1,0.742611,"n of The Christ”). Extracting opinion bearing relations is usually the first step towards fine-gained analysis of opinion in texts, and plays an important role in other sentiment related applications (e.g., sentiment summarization). The goal of this paper is to extract opinion relations from open domain largescale opinion bearing texts. Previous works on fine-grained opinion information extraction have achieved notable success on many aspects: various domains were examined (Pontiki et al., 2015), different types of relations were studied (Ganapathibhotla and Liu, 2008; Narayanan et al., 2009; Wu et al., 2011), and both supervised and unsupervised (patternbased) algorithms were applied. But we have observed some difficulties when trying to use existing methods. The pattern based methods (both lexical patterns and syntactic patterns) are simple, fast, and scalable on large-scale datasets. However, the robustness of patterns is usually questionable in practice. For example, syntactic patterns are sensitive to errors in parse trees, which are common in user generated contents. Lexical patterns either have limited coverage (e.g., fixed set of patterns (Riloff and Wiebe, 2003)), or hard-tocontrol noise"
E17-1097,P13-1173,0,0.0194606,"hich first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural"
E17-1097,D15-1203,0,0.027498,"al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation class"
E17-1097,D10-1006,0,0.0325526,"gated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; Wang et al., 2015). Instead of locating the opinion expressions, aspect-based opinion mining directly analyzes polarities of different opinion targets. The targets are usually constrained to be some predefined set. Shared tasks (SemEval2014, SemEval2015) have been held on the task, and various systems are proposed and evaluated (Pontiki et al., 2014; Pontiki et al., 2015). Comparing with aspect-based opinion mining, we will extract opinion expressions which are more informative, and we won’t constrain opinion target types which helps us to handle open domain texts. 3 The App"
E17-1097,D15-1062,0,0.181134,"babilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which hel"
E17-1097,D15-1206,0,0.170705,"babilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use relations from WordNet or knowledge bases as distant supervision. Since we don’t have similar resources for opinion relation extraction, we use patterns to generate relations. Neural network classifiers are popular for relation extraction recently. Many of them focus on fully supervised settings, recurrent neural networks (RNN) and convolutional neural networks (CNN) (Vu et al., 2016; Zeng et al., 2015; Xu et al., 2015a; Xu et al., 2015b; Zhang and Wang, 2015), sequence models and tree models 1034 are investigated (Li et al., 2015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which hel"
E17-1097,D12-1122,0,0.0219912,"ormances of the proposed unsupervised models are even competitive with previous supervised models. 2 Related Works Opinion relation extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 20"
E17-1097,P13-1161,0,0.0209364,"tion extraction is an important task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et a"
E17-1097,Q14-1039,0,0.0194001,"mportant task for fine-grained sentiment analysis. If human annotations are provided (e.g. MPQA corpus (Deng and Wiebe, 2015)), we could formulate the task into a supervised relation extraction problem as (Kobayashi et al., 2007; Johansson and Moschitti, 2013). Two types of models have been applied: pipeline models which first extract candidates of opinion expressions and targets then identify correct relations (Wu et al., 2009; Li et al., 2010; Yang and Cardie, 2012), and joint models which extract opinion expressions, targets and relations using a unified joint model (Yang and Cardie, 2013; Yang and Cardie, 2014). One consideration of applying supervised methods is their dependencies on the domains and human annotations. Semi-supervised and unsupervised models are also applied for extracting opinion relations. Approaches include rule-based bootstrapping (Qiu et al., 2011), graph propagation algorithms (Xu et al., 2013; Liu et al., 2014; Brody and Elhadad, 2010), integer programming (Lu et al., 2011), and probabilistic topic models (Titov and McDonald, 2008; Mukherjee and Liu, 2012). Our model is inspired by previous distantly supervised algorithms (Snow et al., 2004; Mintz et al., 2009). They use rela"
E17-1097,D11-1013,0,0.0190978,"015; dos Santos et al., 2015). One similar network structure to our model is proposed in (Miwa and Bansal, 2016). They jointly extract entities and relations using two LSTM models. Another recent work (Jebbara and Cimiano, 2016) uses stacked RNNs and CNNs for aspect and opinion detection. Different from models there, we will learn representations for different lexical and syntactic features explicitly. Our formulation follows the features in traditional relation classifiers, which helps to interpret the learned vectors. A closely related task is aspect-based opinion mining (Zhao et al., 2010; Yu et al., 2011; Wang et al., 2015). Instead of locating the opinion expressions, aspect-based opinion mining directly analyzes polarities of different opinion targets. The targets are usually constrained to be some predefined set. Shared tasks (SemEval2014, SemEval2015) have been held on the task, and various systems are proposed and evaluated (Pontiki et al., 2014; Pontiki et al., 2015). Comparing with aspect-based opinion mining, we will extract opinion expressions which are more informative, and we won’t constrain opinion target types which helps us to handle open domain texts. 3 The Approach Given an in"
E17-1097,N10-1122,0,\N,Missing
H05-2013,W02-0216,1,0.842027,"pages 24–25, Vancouver, October 2005. any sense of the parsed structure. The NLU module also supports dynamic updates of the knowledge base. The CSLI DM module mediates and manages interaction. It uses the dialogue-move approach to maintain dialogue context, which is then used to interpret incoming utterances (including fragments and revisions), resolve NPs, construct salient responses, track issues, etc. Dialogue states can also be used to bias SR expectation and improve SR performance, as has been performed in previous applications of the DM. Detailed descriptions of the DM can be found in [Lemon et al 2002; Mirkovic & Cavedon 2005]. The Knowledge Manager (KM) controls access to knowledge base sources (such as domain knowledge and device information) and their updates. Domain knowledge is structured according to domain-dependent ontologies. The current KM makes use of OWL, a W3C standard, to represent the ontological relationships between domain entities. Protégé (http://protege.stanford.edu), a domain-independent ontology tool, is used to maintain the ontology offline. In a typical interaction, the DM converts a user’s query into a semantic frame (i.e. a set of semantic constraints) and sends t"
I11-1019,W03-1028,0,0.367895,"pproach on a manually labeled corpus. Experimental results demonstrate that our approach achieves better performances compared with the state-of-the-art methods. 1 Introduction Keyphrase extraction is a long studied topic in natural language processing. A keyphrase, which consists a word or a group of words, is defined as a precise and concise expression of one or more documents. It has been widely used in various applications such as summarization, clustering, categorizing, browsing, and so on. In recent years, keyphrase extraction has received much attention (Witten et al., 1999; Zha, 2002; Hulth, 2003; Tomokiyo and Hurst, 2003; Chen et al., 2005; Medelyan et al., 2009; Liu et al., 2009). Keyphrases are usually manually chosen by 165 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 165–173, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2 Related Work that the keyphrases of news should satisfy the following properties: As mentioned in the previous section, most of current studies on keyphrase extraction can be roughly divided into two categories: supervised and unsupervised approaches. Unsupervised approaches usually select general sets"
I11-1019,W03-1805,0,0.865611,"manually labeled corpus. Experimental results demonstrate that our approach achieves better performances compared with the state-of-the-art methods. 1 Introduction Keyphrase extraction is a long studied topic in natural language processing. A keyphrase, which consists a word or a group of words, is defined as a precise and concise expression of one or more documents. It has been widely used in various applications such as summarization, clustering, categorizing, browsing, and so on. In recent years, keyphrase extraction has received much attention (Witten et al., 1999; Zha, 2002; Hulth, 2003; Tomokiyo and Hurst, 2003; Chen et al., 2005; Medelyan et al., 2009; Liu et al., 2009). Keyphrases are usually manually chosen by 165 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 165–173, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2 Related Work that the keyphrases of news should satisfy the following properties: As mentioned in the previous section, most of current studies on keyphrase extraction can be roughly divided into two categories: supervised and unsupervised approaches. Unsupervised approaches usually select general sets of candidates and use a ra"
I11-1019,D09-1027,0,0.730556,"pproach achieves better performances compared with the state-of-the-art methods. 1 Introduction Keyphrase extraction is a long studied topic in natural language processing. A keyphrase, which consists a word or a group of words, is defined as a precise and concise expression of one or more documents. It has been widely used in various applications such as summarization, clustering, categorizing, browsing, and so on. In recent years, keyphrase extraction has received much attention (Witten et al., 1999; Zha, 2002; Hulth, 2003; Tomokiyo and Hurst, 2003; Chen et al., 2005; Medelyan et al., 2009; Liu et al., 2009). Keyphrases are usually manually chosen by 165 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 165–173, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2 Related Work that the keyphrases of news should satisfy the following properties: As mentioned in the previous section, most of current studies on keyphrase extraction can be roughly divided into two categories: supervised and unsupervised approaches. Unsupervised approaches usually select general sets of candidates and use a ranking step to select the most important candidates. For examp"
I11-1019,P09-1039,0,0.075709,"Missing"
I11-1019,P10-1058,0,0.0295274,"integer linear programming (Alevras and Padberg, 2001) can be used to incorporate both local features and non-local features, which are difficult to handle with traditional algorithms, it has received much attention in various NLP problems in recent years. Roth and Yih (2005) extended CRF models by applying inference procedure based on ILP to naturally and efficiently support general constraint structures. They applied their model on semantic role labeling (SRL) task. Martin et al. (2009) formulated the problem of nonprojective dependency parsing as a polynomial-sized integer linear program. Woodsend and Lapata (2010) presented a joint content selection and compression model for singledocument summarization using an integer linear programming formulation. 3 maximize cT x subject to 0 ≤ xi ≤ 1 3.1 Objective Function x∈Z With thePBIP formulation, objective function cT x = k ck xk denotes the expected informative scores over all the words of a solution x. Maximizing the expected scores biases the words with highest ci values as keyphrases. Various features can be considered as the values c. In this work, we use two basic features TF·IDF and locality. They have also been widely used in existing keyphrase extra"
I11-1019,D09-1137,0,0.0507347,"demonstrate that our approach achieves better performances compared with the state-of-the-art methods. 1 Introduction Keyphrase extraction is a long studied topic in natural language processing. A keyphrase, which consists a word or a group of words, is defined as a precise and concise expression of one or more documents. It has been widely used in various applications such as summarization, clustering, categorizing, browsing, and so on. In recent years, keyphrase extraction has received much attention (Witten et al., 1999; Zha, 2002; Hulth, 2003; Tomokiyo and Hurst, 2003; Chen et al., 2005; Medelyan et al., 2009; Liu et al., 2009). Keyphrases are usually manually chosen by 165 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 165–173, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2 Related Work that the keyphrases of news should satisfy the following properties: As mentioned in the previous section, most of current studies on keyphrase extraction can be roughly divided into two categories: supervised and unsupervised approaches. Unsupervised approaches usually select general sets of candidates and use a ranking step to select the most important ca"
I11-1019,W04-3252,0,\N,Missing
I11-1112,P06-2042,0,0.0339718,"Missing"
I13-1014,P11-1032,0,0.0321282,"order to deceive readers. This kind of spammers are even more hazardous, since they are neither easily ignored nor identifiable by a human reader. Google Confucius CQA system also reported that best answer spammers may generate amounts of fake best answers, which could have a non-trivial impact on the quality of machine learning model (Si et al., 2010). With the increasing requirements, spammer detection has received considerable attentions, including e-mails(L.Gomes et al., 2007; C.Wu et al., 2005), web spammer (Cheng et al., 2011), review spammer (Lim et al., 2010; N.Jindal and B.Liu, 2008; ott et al., 2011), social media spammer (Zhu et al., 2012; Bosma et al., 2012; Wang, 2010). However, little work has been done about spammers on CQA sites. Filling this need is a challenging task. The existing approaches of spam detection can be roughly into two directions. The first direction usually relied on costly human-labeled training data for building spam classifiers based on textual features (Y.Liu et al., 2008; Y.Xie et al., Abstract As the popularity of Community Question Answering(CQA) increases, spamming activities also picked up in numbers and variety. On CQA sites, spammers often pretend to ask"
I13-1037,P06-2005,0,0.128411,"ocedure to identify formal-informal relations informal phrases in web corpora (Li and Yarowsky, 2008a) . They used search engine to extract contextual instances of the given an informal phrase, and ranked the candidate relation pairs using conditional log-linear model. Xie et al. (2011) proposed to extract Chinese abbreviations and their corresponding definitions based on anchor texts. They constructed a weighted URL-AnchorText bipartite graph from anchor texts and applied co-frequency based measures to quantify the relatedness between two anchor texts. Related Work For lexical normalisation, Aw et al. (2006) treated the lexical normalisation problem as a translation problem from the informal language to the formal English language and adapted a phrase-based method to do it. Han and Baldwin (2011) proposed a supervised method to detect ill-formed words and used morphophonemic similarity to generate correction candidates. Liu et al. (2012) proposed to use a broad coverage lexical normalization method consisting three key components enhanced letter transformation, visual priming, and string/phonetic similarity. Han et al. (2012) introduced a dictionary based method and an automatic normalisation-dic"
I13-1037,W04-1102,0,0.0386556,"this work, we only have one hidden predicate, drop, the global formulas incorporate correlations among different ground atoms of the drop predicate. We propose to use global formulas to force the abbreviations to contain at least 2 characters and to make sure that at least one character is deleted. The following formulas are implemented: 3.2 MLN for Abbreviation Generation In this work, we convert the abbreviation generation problem as a labeling task for every characters in entities. Predicate drop(i) indicates that the character at position i is omitted in the abbreviation. Previous works (Chang and Lai, 2004; Yang et al., 2009) show that Chinese named entities can be further segmented into words. Words also provide important information for abbreviation generation. Hence, in this work, we also segment named entities into words and propose an observed predict to connect words and characters. |character(i, c) ∧ drop(i) |all i |> 1 |character(i, c) ∧ ¬drop(i) |all i |> 2 Another constraint is that for the characters in some particular words should by dropped or kept simultaneously. So we add two formulas to model this: 3.2.1 Local Formulas character(i, c1) ∧ cwM ap(i, j) ∧ drop(i) ∧ The local formul"
I13-1037,D12-1039,0,0.0392449,"Missing"
I13-1037,D08-1068,0,0.0232957,"ogic, these hard logic formulas are softened and can be violated with some penalty (the weight of formula) in MLN. We use M to represent a MLN and {(ϕi , wi )} to represent formula ϕi and its weight wi . These weighted formulas define a probability distribution over sets of possible worlds. Let y denote a possible world, the p(y) is defined as follows (Richardson and Domingos, 2006):   ∑ ∑ 1 p(y) = exp  wi fcϕi (y) , Z nϕ Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint unsupervised coreference resolution. Yoshikawa et al. (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. Andrzejewski et al. (2011) introduced a framework for incorporating general domain knowledge, which is represented by First-Order Logic (FOL) rules, into LDA inference to produce topics shaped by both the data and the rules. (ϕi ,wi )∈M c∈C i where each c is a binding of free variable in ϕi to constraints; fcϕi (y) is a binary feature function that 322 also incorporate position in"
I13-1037,P11-1016,0,0.0590315,"Missing"
I13-1037,W08-2125,0,0.0190399,"he corresponding word. Table 2: Descriptions of observed predicates. 3 The Proposed Approach 2.2 Markov Logic Networks In this section, firstly, we briefly describe the Markov Logic Networks framework. Then, we present the first-order logic formulas including local formulas and global formulas we used in this work. Richardson and Domingos (2006) proposed Markov Logic Networks (MLN), which combines first-order logic and probabilistic graphical models. MLN framework has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). 3.1 Markov Logic Networks A MLN consists of a set of logic formulas that describe first-order knowledge base. Each formula consists of a set of first-order predicates, logical connectors and variables. Different with firstorder logic, these hard logic formulas are softened and can be violated with some penalty (the weight of formula) in MLN. We use M to represent a MLN and {(ϕi , wi )} to represent formula ϕi and its weight wi . These weighted formulas define a probability distribution over sets of po"
I13-1037,D08-1108,0,0.12808,"e a few of simple patterns to extract abbreviations from documents and search engine snippets with high precision as training data. Experimental results show that the proposed methods achieve better performance than state-of-the-art methods and can efficiently process large volumes of data. The remainder of the paper is organized as follows: In section 2, we review a number of related works and the state-of-the-art approaches in related areas. Section 3 presents the proposed method. Experimental results in test collections and analyses are shown in section 4. Section 5 concludes this paper. 2 Li and Yarowsky (2008b) proposed an unsupervised method extracting the relation between a full-form phrase and its abbreviation from monolingual corpora. They used data co-occurrence intuition to identify relations between abbreviation and full names. They also improved a statistical machine translation by incorporating the extracted relations into the baseline translation system. Based on the data co-occurrence phenomena, they introduced a bootstrapping procedure to identify formal-informal relations informal phrases in web corpora (Li and Yarowsky, 2008a) . They used search engine to extract contextual instances"
I13-1037,P08-1049,0,0.139412,"e a few of simple patterns to extract abbreviations from documents and search engine snippets with high precision as training data. Experimental results show that the proposed methods achieve better performance than state-of-the-art methods and can efficiently process large volumes of data. The remainder of the paper is organized as follows: In section 2, we review a number of related works and the state-of-the-art approaches in related areas. Section 3 presents the proposed method. Experimental results in test collections and analyses are shown in section 4. Section 5 concludes this paper. 2 Li and Yarowsky (2008b) proposed an unsupervised method extracting the relation between a full-form phrase and its abbreviation from monolingual corpora. They used data co-occurrence intuition to identify relations between abbreviation and full names. They also improved a statistical machine translation by incorporating the extracted relations into the baseline translation system. Based on the data co-occurrence phenomena, they introduced a bootstrapping procedure to identify formal-informal relations informal phrases in web corpora (Li and Yarowsky, 2008a) . They used search engine to extract contextual instances"
I13-1037,N09-2069,0,0.652514,"ities. Table 1 shows several examples of entities and their corresponding abbreviations. A few of approaches have been done on this task. Li and Yarowsky (Li and Yarowsky, 2008b) introduced an unsupervised method used to extract phrases and their abbreviation pair using parallel dataset and monolingual corpora. Xie et al. (2011) proposed to use weighted bipartite graph to extract definition and corresponding abbreviation pairs from anchor texts. Since these methods rely heavily on lexical/phonetic similarity, substitution of characters and portion may not be correctly identified through them. Yang et al. (2009) studied the Chinese entity name abbreviation problem. They formulated the abbreviation task as a sequence labeling problem and used the conditional random fields (CRFs) to model it. However the long distance and global constraint can not be easily modeled thorough CRFs. Normalizing named entity abbreviations to their standard forms is an important preprocessing task for question answering, entity retrieval, event detection, microblog processing, and many other applications. Along with the quick expansion of microblogs, this task has received more and more attentions in recent years. In this p"
I13-1037,P12-1109,0,0.035327,"Missing"
I13-1037,P09-1046,0,0.0221652,"2: Descriptions of observed predicates. 3 The Proposed Approach 2.2 Markov Logic Networks In this section, firstly, we briefly describe the Markov Logic Networks framework. Then, we present the first-order logic formulas including local formulas and global formulas we used in this work. Richardson and Domingos (2006) proposed Markov Logic Networks (MLN), which combines first-order logic and probabilistic graphical models. MLN framework has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Jiang et al., 2012; Huang et al., 2012). 3.1 Markov Logic Networks A MLN consists of a set of logic formulas that describe first-order knowledge base. Each formula consists of a set of first-order predicates, logical connectors and variables. Different with firstorder logic, these hard logic formulas are softened and can be violated with some penalty (the weight of formula) in MLN. We use M to represent a MLN and {(ϕi , wi )} to represent formula ϕi and its weight wi . These weighted formulas define a probability distribution over sets of possible worlds. Let y den"
I13-1037,I08-2127,0,0.181752,"Named entity normalization, abbreviation generation, and lexical normalization are related to this task. These problems have been recognized as important problems for various languages. Since different languages have their own peculiarities, many approaches have been proposed to handle variants of words (Aw et al., 2006; Liu et al., 2012; Han et al., 2012) and named entities (Yang et al., 2009; Xie et al., 2011; Li and Yarowsky, 2008b). Chang and Teng (2006) introduced an HMMbased single character recovery model to extract character level abbreviation pairs for textual corpus. Okazaki et al. (2008) also used discriminative approach for this task. They formalized the abbreviation recognition task as a binary classification problem and used Support Vector Machines to model it. Yang et al. (2012) also treated the abbreviation generation problem as a labeling task and used Conditional Random Fields (CRFs) to do it. They also proposed to re-rank candidates by a In this paper, we focused on named entity abbreviation generation problem and treated the problem as a labeling task. Due to the flexibilities of Markov Logic Networks on capturing local and global linguistic feature, we adopted it to"
I13-1037,P11-1038,0,\N,Missing
I13-1063,P11-1011,0,0.0611681,"tuents (i.e the“restaurant” is head search terms, “the best Italian ” and “near seattle washington” are modifier), then it would be able to route the query to a specialized search module (in this case restaurant search) and return the most relevant and essential answers rather than results that merely contain all these keywords. In no small part, the success of such approach relies on robust understanding of query intent. Most previous works in this area focus on query tagging problem, i.e., assigning semantic labels to query terms (Li et al., 2009; Manshadi and Li, 2009; Sarkas et al., 2010; Bendersky et al., 2011). Indeed, with the label information, a search engine is able to provide users with more relevant results. But previous works have not considered the issue for understanding the semantic intent of NL queries and their methods are not suitable for interpreting the semantic intent of this kind of complex queries. In this work, in order to enable search engines to understand natural language query, we focus on the problem of mapping NL queries from a particular search engine like Google maps, Bing maps etc, to their semantic intents representation. A key contribution of this work is that we forma"
I13-1063,J98-4004,0,0.085137,"occurs in the tree y. The example shown in Figure 2 clearly depicts the way features are mapped from a tree structure intent representation of an NL query. n 4 SVM1 s 1 CX : |{z} min kwk2 + ξi 2 n w i=1 s.t. ∀i, ξi ≥0 1 − ξi ∀i, ∀y∈Y yi : hw, δψi (y)i≥ 4(yi , y) (4) (5) The second approach to include loss function is to rescale the margin as a special case of the Hamming loss. The margin constraints in this setting take the following form: ∀i, ∀y∈Y yi : hw, δψi (y)i≥4(yi , y) − ξi 4.2.3 Loss function Typically, the correctness of a predicted parse tree is measured by its F1 score (see e.g. (Johnson, 1998)), the harmonic mean of precision of recall as calculated based on the overlap of nodes between the trees. In this work, we follow this loss function and introduce the standard zero-one classification loss as a baseline measure method. Let z and zi be two parse tree outputs and |z| and |zi |be the number of brackets in z and zi , re(6) This set of constraints yields an optimization probm lem, namely SVM4 . 1 The algorithm to solve the maximum-margin problem in structured learning problem is presented in detail in (Tsochantaridis et al., 2004). And 556 ; Algorithm 1 Algorithm of SSVM learning f"
I13-1063,P10-1136,0,0.0247245,"Business. In particular, Li et al. leverage clickthrough data and a database to automatically derive training data for learning a CRF-based tagger. Manshadi and Li develop a hybrid, generative grammar model for a similar task. Sarkas et al. consider an annotation as a mapping of a query to a table of structured data and attributes of this table, while Bendersky et al. mark up queries with annotations such as partof-speech tags, capitalization, and segmentation. There are relatively little published work on understanding the semantic intent of natural language query. Manshadi and Li (2009) and Li (2010) consider the semantic structure of queries. In particular, Li (2010) defines the semantic structure of noun-phrase queries as intent heads (attributes) coupled with some number of intent modifiers (attribute values), e.g., the query [alice in wonderland 2010 cast] is comprised of an intent head cast and two intent modifiers alice in wonderland and 2010. Our approach differs from the earlier work in that we investigate the natural language query intent understanding problem, and build a hierarchical representation for it. 2.2 Semantic Parsing For the purpose of enabling search engines to under"
I13-1063,P09-1097,0,\N,Missing
N19-1290,C14-1220,0,0.0201895,"sion. An expressed-at-least-once assumption is employed in (Mintz et al., 2009): if two entities participated in a relation, at least one instance in the bag might express that relation. Many follow-up studies adopt this assumption and choose a most credible instance to represent the bag. (Lin et al., 2016; Ji et al., 2017) employs the attention mechanism to put different attention weight on each sentence in one bag and assume each sentence is related to the relation but have a different correlation. Another key issue for relation extraction is how to model the instance and extract features, (Zeng et al., 2014, 2015; Zhou et al., 2016) adopts deep Figure 2: Overall Framework neural network including CNN and RNN, these methods perform better than conventional featurebased methods. Reinforcement learning has been widely used in data selection and natural language processing. (Feng et al., 2018) adopts REINFORCE in instance selection for distant supervision which is the basis of our work. Posterior regularization (Ganchev, 2010) is a framework to handle the problem that a variety of tasks and domains require the creation of large problem-specific annotated data. This framework incorporates external pr"
N19-1290,P16-2034,0,0.29305,"st-once assumption is employed in (Mintz et al., 2009): if two entities participated in a relation, at least one instance in the bag might express that relation. Many follow-up studies adopt this assumption and choose a most credible instance to represent the bag. (Lin et al., 2016; Ji et al., 2017) employs the attention mechanism to put different attention weight on each sentence in one bag and assume each sentence is related to the relation but have a different correlation. Another key issue for relation extraction is how to model the instance and extract features, (Zeng et al., 2014, 2015; Zhou et al., 2016) adopts deep Figure 2: Overall Framework neural network including CNN and RNN, these methods perform better than conventional featurebased methods. Reinforcement learning has been widely used in data selection and natural language processing. (Feng et al., 2018) adopts REINFORCE in instance selection for distant supervision which is the basis of our work. Posterior regularization (Ganchev, 2010) is a framework to handle the problem that a variety of tasks and domains require the creation of large problem-specific annotated data. This framework incorporates external problem-specific information"
N19-1290,P16-1200,0,0.0164938,"arge amount of annotated data (Bach and Badaskar, 2007). Distant supervision is proposed to alleviate this problem by aligning plain text with Freebase. However, distant supervision inevitably suffers from the wrong label problem. Some previous research has been done in handling noisy data in distant supervision. An expressed-at-least-once assumption is employed in (Mintz et al., 2009): if two entities participated in a relation, at least one instance in the bag might express that relation. Many follow-up studies adopt this assumption and choose a most credible instance to represent the bag. (Lin et al., 2016; Ji et al., 2017) employs the attention mechanism to put different attention weight on each sentence in one bag and assume each sentence is related to the relation but have a different correlation. Another key issue for relation extraction is how to model the instance and extract features, (Zeng et al., 2014, 2015; Zhou et al., 2016) adopts deep Figure 2: Overall Framework neural network including CNN and RNN, these methods perform better than conventional featurebased methods. Reinforcement learning has been widely used in data selection and natural language processing. (Feng et al., 2018) a"
N19-1290,P09-1113,0,0.512698,"domain-specific rules in instance selection using REINFORCE. As the experiment results show, this method remarkably improves the performance of the relation classifier trained on cleaned distant supervision dataset as well as the efficiency of the REINFORCE training. 1 Introduction Relation extraction is a fundamental work in natural language processing. Detecting and classifying the relation between entity pairs from the unstructured document, it can support many other tasks such as question answering. While relation extraction requires lots of labeled data and make methods labor intensive, (Mintz et al., 2009) proposes distant supervision (DS), a widely used automatic annotating way. In distant supervision, knowledge base (KB) , such as Freebase, is aligned with nature documents. In this way, the sentences which contain an entity pair in KB all express the exact relation that the entity pair has in KB. We usually call the set of instances that contain the same entity pair a bag. In this way, the training instances can be divided into N bags B = {B 1 , B 2 , ..., B N }. Each bag B k are corresponding to an unique entity pair ∗ Corresponding author E k = (ek1 , ek2 ) and contains a sequence of instan"
N19-1290,D15-1203,0,0.0622836,"Missing"
N19-1290,D17-1005,1,\N,Missing
P06-1071,J96-1002,0,0.0456675,"e effectiveness of the PFS with different feature spaces. Section 5 concludes the paper. 2 Background Before presenting the PFS algorithm, we first give a brief review of the conditional maximum entropy modeling, its training process, and the SGC algorithm. This is to provide the background and motivation for our PFS algorithm. 2.1 Conditional Maximum Entropy Model The goal of CME is to find the most uniform conditional distribution of y given observation x, p (y x ) , subject to constraints specified by a set of features f i (x, y ) , where features typically take the value of either 0 or 1 (Berger et al., 1996). More precisely, we want to maximize H ( p) = − ∑ ~ p (x ) p ( y x ) log( p ( y x )) (1) x, y given the constraints: ~ E ( fi ) = E ( fi ) where ~ E ( fi ) = (2) ∑ ~p (x, y ) fi (x, y ) x, y is the empirical expected feature count from the training data and E ( fi ) = ∑ ~p (x ) p(y x ) fi (x, y ) x, y is the feature expectation from the conditional model p ( y x ) . This results in the following exponential model: p(y x ) = ⎛ ⎞ 1 exp⎜ ∑ λ j f j (x, y )⎟ ⎜ ⎟ Z (x ) ⎝ j ⎠ terms of features. For example, a feature can be whether the word in the current position is a verb, or the word is a partic"
P06-1071,N01-1016,0,0.638433,"ted on top of the Selective Gain Computation (SGC) algorithm (Zhou et al., 2003), which offers fast training and high quality models. Theoretically, the new algorithm is able to explore an unlimited amount of features. Because of the improved capability of the CME algorithm, we are able to consider many new features and feature combinations during model construction. To demonstrate the effectiveness of our new algorithm, we conducted a number of experiments on the task of identifying edit regions, a practical task in spoken language processing. Based on the convention from Shriberg (1994) and Charniak and Johnson (2001), a disfluent spoken utterance is divided into three parts: the reparandum, the part that is repaired; the inter561 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 561–568, c Sydney, July 2006. 2006 Association for Computational Linguistics regnum, which can be filler words or empty; and the repair/repeat, the part that replaces or repeats the reparandum. The first two parts combined are called an edit or edit region. An example is shown below: It is, you know, reparandum interregnum this is a tough problem. repair 2.2 In"
P06-1071,P05-1022,0,0.0156186,"aints: ~ E ( fi ) = E ( fi ) where ~ E ( fi ) = (2) ∑ ~p (x, y ) fi (x, y ) x, y is the empirical expected feature count from the training data and E ( fi ) = ∑ ~p (x ) p(y x ) fi (x, y ) x, y is the feature expectation from the conditional model p ( y x ) . This results in the following exponential model: p(y x ) = ⎛ ⎞ 1 exp⎜ ∑ λ j f j (x, y )⎟ ⎜ ⎟ Z (x ) ⎝ j ⎠ terms of features. For example, a feature can be whether the word in the current position is a verb, or the word is a particular lexical item. A feature can also be about a particular syntactic subtree, or a dependency relation (e.g., Charniak and Johnson, 2005). (3) where λj is the weight corresponding to the feature fj, and Z(x) is a normalization factor. A variety of different phenomena, including lexical, structural, and semantic aspects, in natural language processing tasks can be expressed in Selective Gain Computation Algorithm In real world applications, the number of possible features can be in the millions or beyond. Including all the features in a model may lead to data over-fitting, as well as poor efficiency and memory overflow. Good feature selection algorithms are required to produce efficient and high quality models. This leads to a g"
P06-1071,P02-1002,0,0.0142193,"er when more and more features are added to the model, and the gains do not get unexpectively bigger or smaller as the model grows. Furthermore, the experiments in Zhou et al. (2003) show no significant advantage for looking ahead beyond the first element in the feature list. The SGC algorithm runs hundreds to thousands of times faster than the original IFS algorithm without degrading classification performance. We used this algorithm for it enables us to find high quality CME models quickly. The original SGC algorithm uses a technique proposed by Darroch and Ratcliff (1972) and elaborated by Goodman (2002): when considering a feature fi, the algorithm only modifies those un-normalized conditional probabilities: exp ∑ j λ j f j (x, y ) ( of the feature groups and new features are selected from each of them. In other words, the feature space splitting and subspace merging are performed mainly on the feature-to-instance mapping tables. This is a key step that leads to this very efficient PFS algorithm. At the beginning of each round for feature selection, a uniform prior distribution is always assumed for the new CME model. A more precise description of the PFS algorithm is given in Table 1, and i"
P06-1071,P04-1005,0,0.24637,"compare with the state-oft-art, we use the University of Washington re-segmented Switchboard corpus, described in Kahn et al. (2005). In this corpus, the Switchboard sentences were segmented into V5-style sentence-like units (SUs) (LDC, 2004). The resulting sentences fit more closely with the boundaries that can be detected through automatic procedures (e.g., Liu et al., 2005). Because the edit region identification results on the original Switchboard are not directly comparable with the results on the newly segmented data, the state-of-art results reported by Charniak and Johnson (2001) and Johnson and Charniak (2004) are repeated on this new corpus by Kahn et al. (2005). The re-segmented UW Switchboard corpus is labeled with a simplified subset of the ToBI prosodic system (Ostendorf et al., 2001). The three simplified labels in the subset are p, 1 and 4, where p refers to a general class of disfluent boundaries (e.g., word fragments, abruptly shortened words, and hesitation); 4 refers to break level 4, which describes a boundary that has a boundary tone and phrase-final lengthening; 1 Among the original 18 variables, two variables, Pf and Tf are not used in our experiments, because they are mostly covered"
P06-1071,H05-1030,0,0.236564,"itional 29 variables from Zhang and Weng (2005), 11 hierarchical POS tag variables, and 8 prosody variables (labels and their confidence scores). Furthermore, we explore 377 combinations of these 62 variables, which include 40 combinations from Zhang and Weng (2005). The complete list of the variables is given in Table 2, and the combinations used in the experiments are given in Table 3. One additional note is that some features are obtained after the rough copy procedure is performed, where we used the same procedure as the one by Zhang and Weng (2005). For a fair comparison with the work by Kahn et al. (2005), word fragment information is retained. 4.2 The Re-segmented Switchboard Data In order to include prosodic features and be able to compare with the state-oft-art, we use the University of Washington re-segmented Switchboard corpus, described in Kahn et al. (2005). In this corpus, the Switchboard sentences were segmented into V5-style sentence-like units (SUs) (LDC, 2004). The resulting sentences fit more closely with the boundaries that can be detected through automatic procedures (e.g., Liu et al., 2005). Because the edit region identification results on the original Switchboard are not dire"
P06-1071,W00-0729,0,0.0543167,"Missing"
P06-1071,A97-1004,0,0.0533994,"Missing"
P06-1071,W04-3223,0,0.014811,"t phenomena, including lexical, structural, and semantic aspects, in natural language processing tasks can be expressed in Selective Gain Computation Algorithm In real world applications, the number of possible features can be in the millions or beyond. Including all the features in a model may lead to data over-fitting, as well as poor efficiency and memory overflow. Good feature selection algorithms are required to produce efficient and high quality models. This leads to a good amount of work in this area (Ratnaparkhi et al., 1994; Berger et al., 1996; Pietra et al, 1997; Zhou et al., 2003; Riezler and Vasserman, 2004) In the most basic approach, such as Ratnaparkhi et al. (1994) and Berger et al. (1996), training starts with a uniform distribution over all values of y and an empty feature set. For each candidate feature in a predefined feature space, it computes the likelihood gain achieved by including the feature in the model. The feature that maximizes the gain is selected and added to the current model. This process is repeated until the gain from the best candidate feature only gives marginal improvement. The process is very slow, because it has to re-compute the gain for every feature at each selecti"
P06-1071,W05-1519,1,0.869793,"the development data. The results of our experiments on the test data are summarized in Table 4. The first three lines show that the TAG-based approach is outperformed by the new CME baseline (line 3) using all the features in Zhang and Weng (2005). However, the improvement from 2 PFS is not applied to the boosting algorithm at this time because it would require significant changes to the available algorithm. 565 number of features Feature Space Codes Results on test data Precision Recall F-Value TAG-based result on UW-SWBD reported in Kahn et al. (2005) 78.20 CME with all the variables from Zhang and Weng (2005) 2412382 89.42 71.22 79.29 CME with all the variables from Zhang and Weng (2005) + post 2412382 87.15 73.78 79.91 +HTag +HTagComb +WTComb +RCComb 17116957 90.44 72.53 80.50 +HTag +HTagComb +WTComb +RCComb +PL0 … PL3 17116981 88.69 74.01 80.69 +HTag +HTagComb +WTComb +RCComb +PComb: without cut 20445375 89.43 73.78 80.86 +HTag +HTagComb +WTComb +RCComb +PComb: cut2 19294583 88.95 74.66 81.18 +HTag +HTagComb +WTComb +RCComb +PComb: cut2 +Gau 19294583 90.37 74.40 81.61 +HTag +HTagComb +WTComb +RCComb +PComb: cut2 +post 19294583 86.88 77.29 81.80 +HTag +HTagComb +WTComb +RCComb +PComb: cut2 +Gau +"
P06-1071,W03-1020,1,0.885812,"ver, like many other statistical modeling algorithms, such as boosting (Schapire and Singer, 1999) and support vector machine (Vapnik 1995), the algorithm is limited by the size of the defined feature space. Past results show that larger feature spaces tend to give better results. However, finding a way to include an unlimited amount of features is still an open research problem. In this paper, we propose a novel progressive feature selection (PFS) algorithm that addresses the feature space size limitation. The algorithm is implemented on top of the Selective Gain Computation (SGC) algorithm (Zhou et al., 2003), which offers fast training and high quality models. Theoretically, the new algorithm is able to explore an unlimited amount of features. Because of the improved capability of the CME algorithm, we are able to consider many new features and feature combinations during model construction. To demonstrate the effectiveness of our new algorithm, we conducted a number of experiments on the task of identifying edit regions, a practical task in spoken language processing. Based on the convention from Shriberg (1994) and Charniak and Johnson (2001), a disfluent spoken utterance is divided into three"
P06-1071,W02-2018,0,0.102641,"Missing"
P06-1071,H94-1048,0,0.0286118,"esponding to the feature fj, and Z(x) is a normalization factor. A variety of different phenomena, including lexical, structural, and semantic aspects, in natural language processing tasks can be expressed in Selective Gain Computation Algorithm In real world applications, the number of possible features can be in the millions or beyond. Including all the features in a model may lead to data over-fitting, as well as poor efficiency and memory overflow. Good feature selection algorithms are required to produce efficient and high quality models. This leads to a good amount of work in this area (Ratnaparkhi et al., 1994; Berger et al., 1996; Pietra et al, 1997; Zhou et al., 2003; Riezler and Vasserman, 2004) In the most basic approach, such as Ratnaparkhi et al. (1994) and Berger et al. (1996), training starts with a uniform distribution over all values of y and an empty feature set. For each candidate feature in a predefined feature space, it computes the likelihood gain achieved by including the feature in the model. The feature that maximizes the gain is selected and added to the current model. This process is repeated until the gain from the best candidate feature only gives marginal improvement. The pro"
P13-4009,W09-1201,0,0.0699738,"ped mainly for English and not optimized for Chinese. In order to customize an optimized system for Chinese language process, we implement an open source toolkit, FudanNLP5 , which is written in Java. Since most of the state-of-theart methods for NLP are based on statistical learning, the whole framework of our toolkit is established around statistics-based methods, supplemented by some rule-based methods. Therefore, the quality of training data is crucial for our toolkit. However, we ﬁnd that there are some drawbacks in currently most commonly used corpora, such as CTB (Xia, 2000) and CoNLL (Hajič et al., 2009) corpora. For example, in CTB corpus, the set of POS tags is relative small and some categories are derived from the perspective of English grammar. And in CoNLL corpus, the head words are often interrogative particles and punctuations, which are unidiomatic in Chinese. These drawbacks bring more challenges to further analyses, such as information extraction and semantic understanding. Therefore, we ﬁrst construct a corpus with a modiﬁed guideline, which is more in accordance with the common understanding for Chinese grammar. In addition to the basic Chinese NLP tasks The growing need for Chin"
P13-4009,W04-3236,0,0.0704609,": 5 TIME: 7 → → PER LOC → 1 → 1980 8. CS:COO means the coordinate complex sentence. Table 1: Example of the output representation of our toolkit location, organization and other proper name. Conversely, we merge the “VC” and “VE” into “VV” since there is no link verb in Chinese. Finally, we use a tag set with 39 categories in total. Since a POS tag is assigned to each word, not to each character, Chinese POS tagging has two ways: pipeline method or joint method. Currently, the joint method is more popular and eﬀective because it uses more ﬂexible features and can reduce the error propagation (Ng and Low, 2004). In our system, we implement both methods for POS tagging. Besides, we also use some knowledge to improve the performance, such as Chinese surname and the common suﬃxes of the names of locations and organizations. 2.3.3 tively more important, we use language models to capture the internal structures. Second, we merge the continuous NEs with some rulebased strategies. For example, we combine the continuous words “人民/NN 大会堂/NN” into “ 人民大会堂/LOC”. 2.3.4 Dependency parsing Our syntactic parser is currently a dependency parser, which is implemented with the shift-reduce deterministic algorithm bas"
P13-4009,C04-1081,0,0.0463487,"s are trained on our developed corpus. We also develop a visualization module to displaying the output. Table 1 shows the output representation of our toolkit. (4) 2.3.1 Chinese Word Segmentation Diﬀerent from English, Chinese sentences are written in a continuous sequence of characters without explicit delimiters such as the blank space. Since the meanings of most Chinese characters are not complete, words are the basic syntactic and semantic units. Therefore, it is indispensable step to segment the sentence into words in Chinese language processing. We use character-based sequence labeling (Peng et al., 2004) to ﬁnd the boundaries of words. Besides the carefully chosen features, we also use the meaning of character drawn from HowNet(Dong and Dong, 2006), which improves the performance greatly. Since unknown words detection is still one of main challenges of Chinese word segmentation. We implement a constrained Viterbi algorithm to allow users to add their own word dictionary. 2.2.1 Training In the training stage, we use the passiveaggressive algorithm to learn the model parameters. Passive-aggressive (PA) algorithm (Crammer et al., 2006) was proposed for normal multi-class classiﬁcation and can be"
P13-4009,W03-3023,0,0.0336701,"implement both methods for POS tagging. Besides, we also use some knowledge to improve the performance, such as Chinese surname and the common suﬃxes of the names of locations and organizations. 2.3.3 tively more important, we use language models to capture the internal structures. Second, we merge the continuous NEs with some rulebased strategies. For example, we combine the continuous words “人民/NN 大会堂/NN” into “ 人民大会堂/LOC”. 2.3.4 Dependency parsing Our syntactic parser is currently a dependency parser, which is implemented with the shift-reduce deterministic algorithm based on the work in (Yamada and Matsumoto, 2003). The syntactic structure of Chinese is more complex than that of English, and semantic meaning is more dominant than syntax in Chinese sentences. So we select the dependency parser to avoid the minutiae in syntactic constituents and wish to pay more attention to the subsequent semantic analysis. Since the structure of the Chinese language is quite different from that of English, we use more eﬀective features according to the characteristics of Chinese sentences. The common used corpus for Chinese dependency parsing is CoNLL corpus (Hajič et al., 2009). However, there are some illogical cases"
P16-1163,D15-1262,0,0.816312,"on becomes much more challenging when such connectives are missing. In fact, such implicit discourse relations Intuitively, (good, wrong) and (good, ruined), seem to be the most informative word pairs, and it is likely that they will trigger a contrast relation. Therefore, we can see that another main disadvantage of using word pairs is the lack of contextual information, and using n-gram pairs will again suffer from data sparsity problem. Recently, the distributed word representations (Bengio et al., 2006; Mikolov et al., 2013) have shown an advantage when dealing with data sparsity problem (Braud and Denis, 2015), and many deep learning based models are generating substantial interests in text semantic matching and have achieved some significant progresses (Hu 1726 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1726–1735, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Psyllium's not a good crop. You get a rain at the wrong time and the crop is ruined. Bidirectional LSTM Pooling Layer MLP f Gated Relevance Network Bilinear Tensor Single Layer Network Figure 1: The processing framework of the proposed approach. et al.,"
P16-1163,Q15-1024,0,0.374564,"for testing and the other sections for validation (Prasad et al., 2008). For comparison with the previous work (Pitler et al., 2009; Zhou et al., 2010; Park 1729 Table 1: The unbalanced sample distribution of PDTB. Relation Comparison Contingency Expansion Temporal Train 1894 3281 6792 665 Dev 401 628 1253 93 Table 2: Hyperparameters for our model in the experiment. Word Embedding size Initial learning rate Minibatch size Pooling Size Number of tensor slice Test 146 276 556 68 segments representation. The rest of the method is the same as Word-NTN. and Cardie, 2012; Rutherford and Xue, 2014; Ji and Eisenstein, 2015), we train four binary classifiers to identify each of the top level relations, the EntRel relations are merged with Expansion relations. For each classifier, we use an equal number of positive and negative samples as training data, because each of the relations except Expansion is infrequent (Pitler et al., 2009) as what shows in Table 1. The negative samples were chosen randomly from training sections 2-20. 3.2 3.2.1 • Word+GRN: We use the gated relevance network proposed in this paper to capture the semantic interaction scores between every word embedding pair of the two text segments. The"
P16-1163,P13-2013,0,0.594359,"ng@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only word pairs. Consider the following sentence pair with a casual relation as an example: Word pairs, which are one of the most easily accessible features between two text segments, have been proven to be very useful for detecting the discourse relations"
P16-1163,W12-1614,0,0.186442,"an implicit marker. Since explicit relations are easy to identify (Pitler et al., 2008), existing methods achieved good performance on the relations with explicit maker. In recent years, researchers mainly focused on implicit relations. For easily comparing with other methods, in this work, we also use PDTB as the training and testing corpus. As we mentioned above, various approaches have been proposed to do the task. Pitler et al. (2009) proposed to train four binary classifiers using word pairs as well as other rich linguistic features to automatically identify the top-level PDTB relations. Park and Cardie (2012) achieved a higher performance by optimizing the feature set. McKeown and Biran (2013) aims at solving the data sparsity problem, and they extended the work of Pitler et al. (2009) by aggregating word pairs. Rutherford and Xue (2014) used Brown clusters and coreferential patterns as new features and improved the baseline a lot. Braud and Denis (2015) compared different word representations for implicit relation classification. The word pairs feature have been studied by all of the work above, showing its importance on discourse relation. We follow their work, and incorporate word embedding to"
P16-1163,P09-1077,0,0.173917,"e with Gated Relevance Network Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only w"
P16-1163,prasad-etal-2008-penn,0,0.92201,"ur model is trained end to end by BackPropagation and Adagrad. The main contribution of this paper can be summarized as follows: • We use word embeddings to replace the original words in the text segments to overcome data sparsity problem. In order to preserve the contextual information, we further encode the text segment to its positional representation through a recurrent neural network. • To deal with the semantic gap problem, we adopt a gated relevance network to capture the semantic interaction between the intermediate representations of the text segments. • Experimental results on PDTB (Prasad et al., 2008) show that the proposed method can achieve better performance in recognizing discourse level relations in all of the relations than the previous methods. 2 The Proposed Method The architecture of our proposed method is shown in figure 1. In the following of this section, we will illustrate the details of the proposed framework. 2.1 Embedding Layer To model the sentences with neural model, we firstly need to transform the one-hot representation of word into the distributed representation. All words of two text segments X and Y will be mapped into low dimensional vector representations, which ar"
P16-1163,E14-1068,0,0.467992,"e Network Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only word pairs. Consider the fol"
P16-1163,N03-1030,0,0.090673,"on via a Deep Architecture with Gated Relevance Network Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held betw"
P16-1163,P10-1040,0,0.0338307,"d+NTN: We use the neural tensor defined in (8) to capture the semantic interaction scores between every word embedding pair, the rest of the method is the same as our proposed method. • LSTM+NTN: We use two single LSTM to generate the positional text segments representation. The rest of the method is the same as Word-NTN. • BLSTM+NTN: We use two single bidirectional LSTM to generate the positional text nw = 50 ρ = 0.01 m = 32 (p, q) = (3, 3) r=2 3.2.2 Parameter Setting For the initialization of the word embeddings used in our model, we use the 50-dimensional pre-trained embeddings provided by Turian et al. (2010), and the embeddings are fixed during training. We only preserve the top 10,000 words according to its frequency of occurrence in the training data, all the text segments are padded to have the same length of 50, the intermediate representations of LSTM are also set to 50. The other parameters are initialized by randomly sampling from uniform distribution in [-0.1,0.1]. For other hyperparameters of our proposed model, we take those hyperparameters that achieved best performance on the development set, and keep the same parameters for other competitors. The final hyper-parameters are show in Ta"
P16-1163,K15-2001,0,0.0985963,"ation could help to determine which part of the two sentence should be focused when identifying their relation. 4 Related Work Discourse relations, which link clauses in text, are used to represent the overall text structure. Many downstream NLP tasks such as text summarization, question answering, and textual entailment can benefit from the task. Along with the increasing requirement, many works have been constructed to automatically identify these relations from different aspects (Pitler et al., 2008; Pitler et al., 2009; Zhou et al., 2010; McKeown and Biran, 2013; Rutherford and Xue, 2014; Xue et al., 2015). For training and comparing the performance of different methods, the Penn Discourse Treebank (PDTB) 2.0, which is large annotated discourse corpuses, were released in 2008 (Prasad et al., 2008). The annotation methodology of it follows the lexically grounded, predicate-argument approach. In PDTB, the discourse relations were predefined by Webber (2004). PDTB-styled discourse relations hold in only a local contextual window, and these relations are organized hierarchically. Also, every relation in PDTB has either an explicit or an implicit marker. Since explicit relations are easy to identify"
P16-1163,C10-2172,0,0.697668,"τ for parameter θτ,i . 3 3.1 Experiment Dataset The dataset we used in this work is Penn Discourse Treebank 2.0 (Prasad et al., 2008), which is one of the largest available annotated corpora of discourse relations. It contains 40,600 relations, which are manually annotated from the same 2,312 Wall Street Journal (WSJ) articles as the Penn Treebank. We follow the recommended section partition of PDTB 2.0, which is to use sections 2-20 for training, sections 21-22 for testing and the other sections for validation (Prasad et al., 2008). For comparison with the previous work (Pitler et al., 2009; Zhou et al., 2010; Park 1729 Table 1: The unbalanced sample distribution of PDTB. Relation Comparison Contingency Expansion Temporal Train 1894 3281 6792 665 Dev 401 628 1253 93 Table 2: Hyperparameters for our model in the experiment. Word Embedding size Initial learning rate Minibatch size Pooling Size Number of tensor slice Test 146 276 556 68 segments representation. The rest of the method is the same as Word-NTN. and Cardie, 2012; Rutherford and Xue, 2014; Ji and Eisenstein, 2015), we train four binary classifiers to identify each of the top level relations, the EntRel relations are merged with Expansion"
P16-1163,miltsakaki-etal-2004-penn,0,\N,Missing
P16-1163,C08-2022,0,\N,Missing
P18-1233,P07-1056,0,0.20628,"the target labels are used. The entire procedure is described in Algorithm 1. 3.4 i=1 lt X (7) where α, γ, and λ are weights that control the interaction of the loss terms. L(θ) means that loss, L, is optimized on the parameters θ during training. And Lc denotes the classification loss on the domain invariant representation, which 4 lt 1X −Yti log Ft (Yti |Et (Lit )) lt (10) i=1 Experiment 4.1 Dataset Domain adaptation for sentiment classification has been widely studied in the NLP community. The major experiments were performed on the benchmark made of reviews of Amazon products gathered by Blitzer et al. (2007). This data set1 contains Amazon product reviews from four different domains: Books, DVD, Electronics, and Kitchen appliances from Amazon.com. Each review was originally associated with a rating of 15 stars. For simplicity, we are only concerned with whether or not a review is positive (higher than 3 stars) or negative (3 stars or lower). Reviews are encoded in 5,000 dimensional tf-idf feature vectors of bag-of-words unigrams and bigrams. From this data, we constructed 12 cross-domain binary classification tasks. Each domain adaptation task consists of 2,000 labeled source examples, 2509 1 htt"
P19-1231,W03-2201,0,0.0333383,"naries and makes our method generalize well with quite simple dictionaries. Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method. We have published the source code at https:// github.com/v-mipeng/LexiconNER. 1 Joe Figure 1: Data labeling example for person names using our constructed dictionary. Named Entity Recognition (NER) is concerned with identifying named entities, such as person, location, product and organization names in unstructured text. It is a fundamental component in many natural language processing tasks such as machine translation (Babych and Hartley, 2003), knowledge base construction (Riedel et al., 2013; Shen et al., 2012), automatic question answering (Bordes et al., 2015), search (Zhu et al., 2005), etc. In this field, supervised methods, ranging from the typical graph models (Zhou and Su, 2002; McCallum et al., 2000; McCallum and Li, 2003; Settles, 2004) to current popular neural-networkbased models (Chiu and Nichols, 2016; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang Equal contribution. Anna Bobick was managed by weight legend Joe Frazier Introduction ∗ David and Yang, 2018), have achieved great success. However, these supe"
P19-1231,Q16-1026,0,0.0814987,"rned with identifying named entities, such as person, location, product and organization names in unstructured text. It is a fundamental component in many natural language processing tasks such as machine translation (Babych and Hartley, 2003), knowledge base construction (Riedel et al., 2013; Shen et al., 2012), automatic question answering (Bordes et al., 2015), search (Zhu et al., 2005), etc. In this field, supervised methods, ranging from the typical graph models (Zhou and Su, 2002; McCallum et al., 2000; McCallum and Li, 2003; Settles, 2004) to current popular neural-networkbased models (Chiu and Nichols, 2016; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang Equal contribution. Anna Bobick was managed by weight legend Joe Frazier Introduction ∗ David and Yang, 2018), have achieved great success. However, these supervised methods often require large scale fine-grained annotations (label every word of a sentence) to generalize well. This makes it hard to apply them to label-few domains, e.g., bio/medical domains (Del˙eger et al., 2016). In this work, we explore the way to perform NER using only unlabeled data and named entity dictionaries, which are relatively easier to obtain compared wi"
P19-1231,W16-3002,0,0.0272937,"Missing"
P19-1231,P05-1045,0,0.0546107,"Missing"
P19-1231,D14-1181,0,0.00326128,"summarized in Alg. 1. In our experiments, we intuitively set the context size k = 4. 3.4 Build PU Learning Classifier In this work, we use a neural-network-based architecture to implement the classifier f , and this architecture is shared by different entity types. Word Representation. Context-independent word representation consists of three part of features, i.e., the character sequence representation ec (w), the word embedding ew (w), and some human designed features on the word-face eh (w). For the character-level representation ec (w) of w, we use the one-layer convolution network model (Kim, 2014) on its character sequence {c1 , c2 , · · · , cm } ∈ Vc , where Vc is the character vocabulary. Each character c is represented using where Wc denotes a character embedding lookup table. The one-layer convolution network is then applied to {v(c1 ), v(c2 ), · · · , v(cm )} to obtain ec (w). For the word-level representation ew (w) of w, we introduce an unique dense vector for w, which is initialized with Stanford’s GloVe word embeddings1 (Pennington et al., 2014) and finetuned during model training. For the human designed features eh (w) of w, we introduce a set of binary feature indicators. Th"
P19-1231,N16-1030,0,0.0223614,"med entities, such as person, location, product and organization names in unstructured text. It is a fundamental component in many natural language processing tasks such as machine translation (Babych and Hartley, 2003), knowledge base construction (Riedel et al., 2013; Shen et al., 2012), automatic question answering (Bordes et al., 2015), search (Zhu et al., 2005), etc. In this field, supervised methods, ranging from the typical graph models (Zhou and Su, 2002; McCallum et al., 2000; McCallum and Li, 2003; Settles, 2004) to current popular neural-networkbased models (Chiu and Nichols, 2016; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang Equal contribution. Anna Bobick was managed by weight legend Joe Frazier Introduction ∗ David and Yang, 2018), have achieved great success. However, these supervised methods often require large scale fine-grained annotations (label every word of a sentence) to generalize well. This makes it hard to apply them to label-few domains, e.g., bio/medical domains (Del˙eger et al., 2016). In this work, we explore the way to perform NER using only unlabeled data and named entity dictionaries, which are relatively easier to obtain compared with labeled data. A na"
P19-1231,W02-2024,0,0.17193,"s ) and recall. (Huang et al., 2015) a neural-network-based method as the BiLSTM baseline, but additionally introducing a CRF layer. 4.2 PER Datasets CoNLL (en). CoNLL2003 NER Shared Task Dataset in English (Tjong Kim Sang and De Meulder, 2003) collected from Reuters News. It is annotated by four types: PER, LOC, ORG, and MISC. We used the official split training set for model training, and testb for testing in our experiments, which contains 203K and 46K tokens, respectively. In addition, there are about 456k additional unlabeled tokens. CoNLL (sp). CoNLL2002 Spanish NER Shared Task Dataset (Sang and Erik, 2002) collected from Spanish EFE News Agency. It is also annotated by PER, LOC, ORG, and MISC types. The training and test data sets contain 273k and 53k lines, respectively. MUC. Message Understanding Conference 7 released by Chinchor (1998) for NER. It has about 190K tokens in the training set and 64K tokens in the testing set. For the sake of homogeneity, we perform entity detection on PER, LOC, and ORG in this study. Twitter. Twitter is a dataset collected from Twitter and released by Zhang et al. (2018). It contains 4,000 tweets for training and 3,257 tweets for testing. Every tweet contains b"
P19-1231,W03-0419,0,0.608779,"Missing"
P19-1231,W03-0430,0,0.294989,"erson names using our constructed dictionary. Named Entity Recognition (NER) is concerned with identifying named entities, such as person, location, product and organization names in unstructured text. It is a fundamental component in many natural language processing tasks such as machine translation (Babych and Hartley, 2003), knowledge base construction (Riedel et al., 2013; Shen et al., 2012), automatic question answering (Bordes et al., 2015), search (Zhu et al., 2005), etc. In this field, supervised methods, ranging from the typical graph models (Zhou and Su, 2002; McCallum et al., 2000; McCallum and Li, 2003; Settles, 2004) to current popular neural-networkbased models (Chiu and Nichols, 2016; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang Equal contribution. Anna Bobick was managed by weight legend Joe Frazier Introduction ∗ David and Yang, 2018), have achieved great success. However, these supervised methods often require large scale fine-grained annotations (label every word of a sentence) to generalize well. This makes it hard to apply them to label-few domains, e.g., bio/medical domains (Del˙eger et al., 2016). In this work, we explore the way to perform NER using only unlabeled"
P19-1231,D14-1162,0,0.0793453,"uman designed features on the word-face eh (w). For the character-level representation ec (w) of w, we use the one-layer convolution network model (Kim, 2014) on its character sequence {c1 , c2 , · · · , cm } ∈ Vc , where Vc is the character vocabulary. Each character c is represented using where Wc denotes a character embedding lookup table. The one-layer convolution network is then applied to {v(c1 ), v(c2 ), · · · , v(cm )} to obtain ec (w). For the word-level representation ew (w) of w, we introduce an unique dense vector for w, which is initialized with Stanford’s GloVe word embeddings1 (Pennington et al., 2014) and finetuned during model training. For the human designed features eh (w) of w, we introduce a set of binary feature indicators. These indicators are designed on options proposed by Collobert et al. (2011): allCaps, upperInitial, lowercase, mixedCaps, noinfo. If any feature is activated, its corresponding indicator is set to 1, otherwise 0. This way, it can keep the capitalization information erased during lookup of the word embedding. The final word presentation independent to its context e(w) ∈ Rkw of w, is obtained by concatenating these three part of features: e(w) = [ec (w) ⊕ ew (w) ⊕"
P19-1231,N13-1008,0,0.0463058,"imple dictionaries. Empirical studies on four public NER datasets demonstrate the effectiveness of our proposed method. We have published the source code at https:// github.com/v-mipeng/LexiconNER. 1 Joe Figure 1: Data labeling example for person names using our constructed dictionary. Named Entity Recognition (NER) is concerned with identifying named entities, such as person, location, product and organization names in unstructured text. It is a fundamental component in many natural language processing tasks such as machine translation (Babych and Hartley, 2003), knowledge base construction (Riedel et al., 2013; Shen et al., 2012), automatic question answering (Bordes et al., 2015), search (Zhu et al., 2005), etc. In this field, supervised methods, ranging from the typical graph models (Zhou and Su, 2002; McCallum et al., 2000; McCallum and Li, 2003; Settles, 2004) to current popular neural-networkbased models (Chiu and Nichols, 2016; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang Equal contribution. Anna Bobick was managed by weight legend Joe Frazier Introduction ∗ David and Yang, 2018), have achieved great success. However, these supervised methods often require large scale fine-grai"
P19-1231,C18-1183,0,0.108261,"ge scale fine-grained annotations (label every word of a sentence) to generalize well. This makes it hard to apply them to label-few domains, e.g., bio/medical domains (Del˙eger et al., 2016). In this work, we explore the way to perform NER using only unlabeled data and named entity dictionaries, which are relatively easier to obtain compared with labeled data. A natural practice to perform the task is to scan through the query text using the dictionary and treat terms matched with a list of entries of the dictionary as the entities (Nadeau et al., 2006; Gerner et al., 2010; Liu et al., 2015; Yang et al., 2018). However, this practice requires very high quality named entity dictionaries that cover most of entities, otherwise it will fail with poor performance. As shown in Figure 1, the constructed dictionary of person names only labels one entity within the query text, which contains two entities “Bobick” and “Joe Frazier”, and it only labels one word “Joe” out of the two-word entity “Joe Frazier”. To address this problem, an intuitive solution is to further perform supervised or semi-supervised learning using the dictionary labeled data. However, since it does not guarantee that the dictionary cove"
P19-1231,P18-1144,0,0.0850307,"Missing"
P19-1231,P02-1060,0,0.407134,"Joe Figure 1: Data labeling example for person names using our constructed dictionary. Named Entity Recognition (NER) is concerned with identifying named entities, such as person, location, product and organization names in unstructured text. It is a fundamental component in many natural language processing tasks such as machine translation (Babych and Hartley, 2003), knowledge base construction (Riedel et al., 2013; Shen et al., 2012), automatic question answering (Bordes et al., 2015), search (Zhu et al., 2005), etc. In this field, supervised methods, ranging from the typical graph models (Zhou and Su, 2002; McCallum et al., 2000; McCallum and Li, 2003; Settles, 2004) to current popular neural-networkbased models (Chiu and Nichols, 2016; Lample et al., 2016; Gridach, 2017; Liu et al., 2018; Zhang Equal contribution. Anna Bobick was managed by weight legend Joe Frazier Introduction ∗ David and Yang, 2018), have achieved great success. However, these supervised methods often require large scale fine-grained annotations (label every word of a sentence) to generalize well. This makes it hard to apply them to label-few domains, e.g., bio/medical domains (Del˙eger et al., 2016). In this work, we explo"
P19-1231,W04-1221,0,\N,Missing
P19-1231,O03-4002,0,\N,Missing
W05-1519,P94-1041,0,0.0406545,"lative error reduction in F-score over the latest best result where punctuation is excluded from the training and testing data [Johnson and Charniak 2004]. 1 Introduction Repairs, hesitations, and restarts are common in spoken language, and understanding spoken language requires accurate methods for identifying such disfluent phenomena. Processing speech repairs properly poses a challenge to spoken dialog systems. Early work in this field is primarily based on small and proprietary corpora, which makes the comparison of the proposed methods difficult [Young and Matessa 1991, Bear et al. 1992, Heeman & Allen 1994]. Because of the availability In this paper we describe our effort towards the task of edited region identification with the intention of parsing disfluent sentences in the Switchboard corpus. A clear benefit of having accurate edited regions for parsing has been demonstrated by a concurrent effort on parsing conversational speech [Kahn et al 2005]. Since different machine learning methods provide similar performances on many NLP tasks, in this paper, we focus our attention on exploring feature spaces and selecting good features for identifying edited regions. We start by analyzing the distri"
W05-1519,P04-1005,0,0.487441,"the distributions of the edited regions and their components in the targeted corpus. We then design several feature spaces to cover the disfluent regions in the training data. In addition, we also explore new feature spaces of a part-of-speech hierarchy and extend candidate pools in the experiments. These steps result in a significant improvement in F-score over the earlier best result reported in [Charniak and Johnson 2001], where punctuation is included in both the training and testing data of the Switchboard corpus, and a significant error reduction in F-score over the latest best result [Johnson and Charniak 2004], where punctuation is ignored in both the training and testing data of the Switchboard corpus. 179 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 179–185, c Vancouver, October 2005. 2005 Association for Computational Linguistics In this paper, we follow the definition of [Shriberg 1994] and others for speech repairs: A speech repair is divided into three parts: the reparandum, the part that is repaired; the interregnum, the part that can be either empty or fillers; and the repair/repeat, the part that replaces or repeats the reparandum. The definition c"
W05-1519,N04-4032,0,0.025191,"Missing"
W05-1519,P92-1008,0,0.18668,"Missing"
W05-1519,N01-1016,0,0.356034,"s provide similar performances on many NLP tasks, in this paper, we focus our attention on exploring feature spaces and selecting good features for identifying edited regions. We start by analyzing the distributions of the edited regions and their components in the targeted corpus. We then design several feature spaces to cover the disfluent regions in the training data. In addition, we also explore new feature spaces of a part-of-speech hierarchy and extend candidate pools in the experiments. These steps result in a significant improvement in F-score over the earlier best result reported in [Charniak and Johnson 2001], where punctuation is included in both the training and testing data of the Switchboard corpus, and a significant error reduction in F-score over the latest best result [Johnson and Charniak 2004], where punctuation is ignored in both the training and testing data of the Switchboard corpus. 179 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 179–185, c Vancouver, October 2005. 2005 Association for Computational Linguistics In this paper, we follow the definition of [Shriberg 1994] and others for speech repairs: A speech repair is divided into three parts"
W05-1519,W02-1007,0,0.0269027,"Missing"
W05-1519,H05-1030,0,\N,Missing
