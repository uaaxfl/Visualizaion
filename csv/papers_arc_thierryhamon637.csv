2020.jeptalnrecital-taln.21,Pr{\\'e}dire le niveau de langue d{'}apprenants d{'}anglais (Predict the language level for {E}nglish learners),2020,-1,-1,2,0,5649,natalia grabar,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"L{'}apprentissage de la deuxi{\`e}me langue (L2) est un processus progressif dans lequel l{'}apprenant am{\'e}liore sa ma{\^\i}trise au fur et {\`a} mesure de l{'}apprentissage. L{'}analyse de productions d{'}apprenants int{\'e}resse les chercheurs et les enseignants car cela permet d{'}avoir une meilleure id{\'e}e des difficult{\'e}s et les facilit{\'e}s d{'}apprentissage et de faire des programmes didactiques plus adapt{\'e}s. Cela peut {\'e}galement donner des indications sur les difficult{\'e}s cognitives {\`a} ma{\^\i}triser les notions grammaticales abstraites dans une nouvelle langue. Nous proposons de travailler sur un corpus de productions langagi{\`e}res d{'}apprenants d{'}anglais provenant de diff{\'e}rents pays et donc ayant diff{\'e}rentes langues maternelles (L1). Notre objectif consiste {\`a} cat{\'e}goriser ces productions langagi{\`e}res selon six niveaux de langue (A1, A2, B1, B2, C1, C2). Nous utilisons diff{\'e}rents ensembles de descripteurs, y compris les verbes et expressions modaux. Nous obtenons des r{\'e}sultats int{\'e}ressants pour cette cat{\'e}gorisation multiclasse, ce qui indique qu{'}il existe des diff{\'e}rences linguistiques inh{\'e}rentes entre les diff{\'e}rents niveaux."
2020.jeptalnrecital-taln.32,Identification des probl{\\`e}mes d{'}annotation pour l{'}extraction de relations (Identification of annotation problem for the relation extraction),2020,-1,-1,2,1,18598,tsanta randriatsitohaina,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"L{'}annotation d{'}un corpus est une t{\^a}che difficile et laborieuse, notamment sur des textes de sp{\'e}cialit{\'e} comme les textes biom{\'e}dicaux. Ainsi, dans un contexte comme l{'}extraction des interactions alimentm{\'e}dicament (FDI), l{'}annotation du corpus POMELO a {\'e}t{\'e} r{\'e}alis{\'e}e par un seul annotateur et pr{\'e}sente des risques d{'}erreur. Dans cet article, nous proposons d{'}identifier ces probl{\`e}mes d{'}annotation en utilisant un corpus Silver Standard (CSS) que nous {\'e}tablissons {\`a} partir d{'}un vote majoritaire parmi les annotations propos{\'e}es par des mod{\`e}les entra{\^\i}n{\'e}s sur un domaine similaire (interaction m{\'e}dicamentm{\'e}dicament {--} DDI) et l{'}annotation manuelle {\`a} {\'e}valuer. Les r{\'e}sultats obtenus montrent que l{'}annotation dans POMELO est consid{\'e}rablement {\'e}loign{\'e}e du CSS. L{'}analyse des erreurs permet d{'}en identifier les principales causes et de proposer des solutions pour corriger l{'}annotation existante."
2020.jeptalnrecital-taln.33,Simplification automatique de texte dans un contexte de faibles ressources (Automatic Text Simplification : Approaching the Problem in Low Resource Settings for {F}rench),2020,-1,-1,5,0,2848,sadaf rauf,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"La simplification de textes a {\'e}merg{\'e} comme un sous-domaine actif du traitement automatique des langues, du fait des probl{\`e}mes pratiques et th{\'e}oriques qu{'}elle permet d{'}aborder, ainsi que de ses nombreuses applications pratiques. Des corpus de simplification sont n{\'e}cessaires pour entrainer des syst{\`e}mes de simplification automatique ; ces ressources sont toutefois rares et n{'}existent que pour un petit nombre de langues. Nous montrons ici que dans un contexte o{\`u} les ressources pour la simplification sont rares, il reste n{\'e}anmoins possible de construire des syst{\`e}mes de simplification, en ayant recours {\`a} des corpus synth{\'e}tiques, par exemple obtenus par traduction automatique, et nous {\'e}valuons diverses mani{\`e}res de les constituer."
2020.jeptalnrecital-deft.1,Pr{\\'e}sentation de la campagne d{'}{\\'e}valuation {DEFT} 2020 : similarit{\\'e} textuelle en domaine ouvert et extraction d{'}information pr{\\'e}cise dans des cas cliniques (Presentation of the {DEFT} 2020 Challenge : open domain textual similarity and precise information extraction from clinical cases ),2020,-1,-1,4,0,5648,remi cardon,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Atelier D{\\'E}fi Fouille de Textes",0,"L{'}{\'e}dition 2020 du d{\'e}fi fouille de texte (DEFT) a propos{\'e} deux t{\^a}ches autour de la similarit{\'e} textuelle et une t{\^a}che d{'}extraction d{'}information. La premi{\`e}re t{\^a}che vise {\`a} identifier le degr{\'e} de similarit{\'e} entre paires de phrases sur une {\'e}chelle de 0 (le moins similaire) {\`a} 5 (le plus similaire). Les r{\'e}sultats varient de 0,65 {\`a} 0,82 d{'}EDRM. La deuxi{\`e}me t{\^a}che consiste {\`a} d{\'e}terminer la phrase la plus proche d{'}une phrase source parmi trois phrases cibles fournies, avec des r{\'e}sultats tr{\`e}s {\'e}lev{\'e}s, variant de 0,94 {\`a} 0,99 de pr{\'e}cision. Ces deux t{\^a}ches reposent sur un corpus du domaine g{\'e}n{\'e}ral et de sant{\'e}. La troisi{\`e}me t{\^a}che propose d{'}extraire dix cat{\'e}gories d{'}informations du domaine m{\'e}dical depuis le corpus de cas cliniques de DEFT 2019. Les r{\'e}sultats varient de 0,07 {\`a} 0,66 de F-mesure globale pour la sous-t{\^a}che des pathologies et signes ou sympt{\^o}mes, et de 0,14 {\`a} 0,76 pour la sous-t{\^a}che sur huit cat{\'e}gories m{\'e}dicales. Les m{\'e}thodes utilis{\'e}es reposent sur des CRF et des r{\'e}seaux de neurones."
W19-5011,{RNN} Embeddings for Identifying Difficult to Understand Medical Words,2019,0,1,4,0,23936,hanna pylieva,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"Patients and their families often require a better understanding of medical information provided by doctors. We currently address this issue by improving the identification of difficult to understand medical words. We introduce novel embeddings received from RNN - FrnnMUTE (French RNN Medical Understandability Text Embeddings) which allow to reach up to 87.0 F1 score in identification of difficult words. We also note that adding pre-trained FastText word embeddings to the feature set substantially improves the performance of the model which classifies words according to their difficulty. We study the generalizability of different models through three cross-validation scenarios which allow testing classifiers in real-world conditions: understanding of medical words by new users, and classification of new unseen words by the automatic models. The RNN - FrnnMUTE embeddings and the categorization code are being made available for the research."
W19-5013,Query selection methods for automated corpora construction with a use case in food-drug interactions,2019,0,0,5,0,17236,georgeta bordea,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"In this paper, we address the problem of automatically constructing a relevant corpus of scientific articles about food-drug interactions. There is a growing number of scientific publications that describe food-drug interactions but currently building a high-coverage corpus that can be used for information extraction purposes is not trivial. We investigate several methods for automating the query selection process using an expert-curated corpus of food-drug interactions. Our experiments show that index term features along with a decision tree classifier are the best approach for this task and that feature selection approaches and in particular gain ratio outperform frequency-based methods for query selection."
W19-5029,Clinical Case Reports for {NLP},2019,0,1,4,0,5675,cyril grouin,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"Textual data are useful for accessing expert information. Yet, since the texts are representative of distinct language uses, it is necessary to build specific corpora in order to be able to design suitable NLP tools. In some domains, such as medical domain, it may be complicated to access the representative textual data and their semantic annotations, while there exists a real need for providing efficient tools and methods. Our paper presents a corpus of clinical cases written in French, and their semantic annotations. Thus, we manually annotated a set of 717 files into four general categories (age, gender, outcome, and origin) for a total number of 2,835 annotations. The values of age, gender, and outcome are normalized. A subset with 70 files has been additionally manually annotated into 27 categories for a total number of 5,198 annotations."
2019.jeptalnrecital-tia.2,Identification des cat{\\'e}gories de relations aliment-m{\\'e}dicament (Identification of categories of food-drug relations),2019,-1,-1,2,1,18598,tsanta randriatsitohaina,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Terminologie et Intelligence Artificielle (atelier TALN-RECITAL {\\textbackslash}{\\&} IC),0,"Les interactions aliment-m{\'e}dicament se produisent lorsque des aliments et des m{\'e}dicaments pris ensembles provoquent un effet inattendu. Leur reconnaissance automatique dans les textes peut {\^e}tre consid{\'e}r{\'e}e comme une t{\^a}che d{'}extraction de relation {\`a} l{'}aide de m{\'e}thodes de classification. Toutefois, {\'e}tant donn{\'e} que ces interactions sont d{\'e}crites de mani{\`e}re tr{\`e}s fine, nous sommes confront{\'e}s au manque de donn{\'e}es et au manque d{'}exemples par type de relation. Pour r{\'e}soudre ce probl{\`e}me, nous proposons une approche efficace pour regrouper des relations partageant une repr{\'e}sentation similaire en groupes et r{\'e}duire le manque d{'}exemples. Notre approche am{\'e}liore les performances de la classification des FDI. Enfin, nous contrastons une m{\'e}thode de regroupement intuitive bas{\'e}e sur la d{\'e}finition des types de relation et un apprentissage non supervis{\'e} bas{\'e} sur les instances de chaque type de relation."
2019.jeptalnrecital-long.5,Corpus annot{\\'e} de cas cliniques en fran{\\c{c}}ais (Annotated corpus with clinical cases in {F}rench),2019,-1,-1,3,0,5649,natalia grabar,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume I : Articles longs,0,"Les corpus textuels sont utiles pour diverses applications de traitement automatique des langues (TAL) en fournissant les donn{\'e}es n{\'e}cessaires pour leur cr{\'e}ation, adaptation ou {\'e}valuation. Cependant, dans certains domaines comme le domaine m{\'e}dical, l{'}acc{\`e}s aux donn{\'e}es est rendu compliqu{\'e}, voire impossible, pour des raisons de confidentialit{\'e} et d{'}{\'e}thique. Il existe n{\'e}anmoins de r{\'e}els besoins en corpus cliniques pour l{'}enseignement et la recherche. Pour r{\'e}pondre {\`a} ce d{\'e}fi, nous pr{\'e}sentons dans cet article le corpus CAS contenant des cas cliniques de patients, r{\'e}els ou fictifs, que nous avons compil{\'e}s. Ces cas cliniques en fran{\c{c}}ais couvrent plusieurs sp{\'e}cialit{\'e}s m{\'e}dicales et focalisent donc sur diff{\'e}rentes situations cliniques. Actuellement, le corpus contient 4 300 cas (environ 1,5M d{'}occurrences de mots). Il est accompagn{\'e} d{'}informations (discussions des cas cliniques, mots-cl{\'e}s, etc.) et d{'}annotations que nous avons effectu{\'e}es au regard des besoins de la recherche en TAL dans ce domaine. Nous pr{\'e}sentons {\'e}galement les r{\'e}sultats de premi{\`e}res exp{\'e}riences de recherche et d{'}extraction d{'}information qui ont {\'e}t{\'e} effectu{\'e}es avec ce corpus annot{\'e}. Ces exp{\'e}riences peuvent fournir une baseline {\`a} d{'}autres chercheurs souhaitant travailler avec les donn{\'e}es."
2019.jeptalnrecital-deft.1,Recherche et extraction d{'}information dans des cas cliniques. Pr{\\'e}sentation de la campagne d{'}{\\'e}valuation {DEFT} 2019 (Information Retrieval and Information Extraction from Clinical Cases),2019,-1,-1,3,0,5649,natalia grabar,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. D{\\'e}fi Fouille de Textes (atelier TALN-RECITAL),0,"Cet article pr{\'e}sente la campagne d{'}{\'e}valuation DEFT 2019 sur l{'}analyse de textes cliniques r{\'e}dig{\'e}s en fran{\c{c}}ais. Le corpus se compose de cas cliniques publi{\'e}s et discut{\'e}s dans des articles scientifiques, et index{\'e}s par des mots-cl{\'e}s. Nous proposons trois t{\^a}ches ind{\'e}pendantes : l{'}indexation des cas cliniques et discussions, {\'e}valu{\'e}e prioritairement par la MAP (mean average precision), l{'}appariement entre cas cliniques et discussions, {\'e}valu{\'e} au moyen d{'}une pr{\'e}cision, et l{'}extraction d{'}information parmi quatre cat{\'e}gories ({\^a}ge, genre, origine de la consultation, issue), {\'e}valu{\'e}e en termes de rappel, pr{\'e}cision et F-mesure. Nous pr{\'e}sentons les r{\'e}sultats obtenus par les participants sur chaque t{\^a}che."
2019.jeptalnrecital-court.8,De l{'}extraction des interactions m{\\'e}dicament-m{\\'e}dicament vers les interactions aliment-m{\\'e}dicament {\\`a} partir de textes biom{\\'e}dicaux: Adaptation de domaine (From the extraction of drug-drug interactions to the food-drug interactions in biomedical texts : domain adaptation),2019,-1,-1,2,1,18598,tsanta randriatsitohaina,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"Les interactions aliments-m{\'e}dicaments (FDI) se produisent lorsque des aliments et des m{\'e}dicaments sont pris simultan{\'e}ment et provoquent un effet inattendu. Nous consid{\'e}rons l{'}extraction de ces interactions dans les textes comme une t{\^a}che d{'}extraction de relation pouvant {\^e}tre r{\'e}solue par des m{\'e}thodes de classification. Toutefois, {\'e}tant donn{\'e} que ces interactions sont d{\'e}crites de mani{\`e}re tr{\`e}s fine, nous sommes confront{\'e}s au manque de donn{\'e}es et au manque d{'}exemples par type de relation. Pour r{\'e}soudre ce probl{\`e}me, nous proposons d{'}appliquer une adaptation de domaine {\`a} partir des interactions m{\'e}dicament-m{\'e}dicament (DDI) qui est une t{\^a}che similaire, afin d{'}{\'e}tablir une correspondance entre les types de relations et d{'}{\'e}tiqueter les instances FDI selon les types DDI. Notre approche confirme une coh{\'e}rence entre les 2 domaines et fournit une base pour la sp{\'e}cification des relations et la pr{\'e}-annotation de nouvelles donn{\'e}es. Les performances des mod{\`e}les de classification appuie {\'e}galement l{'}efficacit{\'e} de l{'}adaptation de domaine sur notre t{\^a}che."
2018.jeptalnrecital-court.27,D{\\'e}tection des couples de termes translitt{\\'e}r{\\'e}s {\\`a} partir d{'}un corpus parall{\\`e}le anglais-arabe (),2018,-1,-1,2,1,30997,wafa neifar,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,
grabar-hamon-2017-understanding,Understanding of unknown medical words,2017,0,1,2,0,5649,natalia grabar,Proceedings of the Biomedical {NLP} Workshop associated with {RANLP} 2017,0,"We assume that unknown words with internal structure (affixed words or compounds) can provide speakers with linguistic cues as for their meaning, and thus help their decoding and understanding. To verify this hypothesis, we propose to work with a set of French medical words. These words are annotated by five annotators. Then, two kinds of analysis are performed: analysis of the evolution of understandable and non-understandable words (globally and according to some suffixes) and analysis of clusters created with unsupervised algorithms on basis of linguistic and extra-linguistic features of the studied words. Our results suggest that, according to linguistic sensitivity of annotators, technical words can be decoded and become understandable. As for the clusters, some of them distinguish between understandable and non-understandable words. Resources built in this work will be made freely available for the research purposes."
hamon-etal-2017-pomelo,{POMELO}: {M}edline corpus with manually annotated food-drug interactions,2017,0,0,1,1,18582,thierry hamon,Proceedings of the Biomedical {NLP} Workshop associated with {RANLP} 2017,0,"When patients take more than one medication, they may be at risk of drug interactions, which means that a given drug can cause unexpected effects when taken in combination with other drugs. Similar effects may occur when drugs are taken together with some food or beverages. For instance, grapefruit has interactions with several drugs, because its active ingredients inhibit enzymes involved in the drugs metabolism and can then cause an excessive dosage of these drugs. Yet, information on food/drug interactions is poorly researched. The current research is mainly provided by the medical domain and a very tentative work is provided by computer sciences and NLP domains. One factor that motivates the research is related to the availability of the annotated corpora and the reference data. The purpose of our work is to describe the rationale and approach for creation and annotation of scientific corpus with information on food/drug interactions. This corpus contains 639 MEDLINE citations (titles and abstracts), corresponding to 5,752 sentences. It is manually annotated by two experts. The corpus is named POMELO. This annotated corpus will be made available for the research purposes."
2017.jeptalnrecital-long.14,Analyse et {\\'e}volution de la compr{\\'e}hension de termes techniques (Analysis and Evolution of Understanding of Technical Terms),2017,-1,-1,2,0,5649,natalia grabar,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 - Articles longs,0,"Nous faisons l{'}hypoth{\`e}se que les mots techniques inconnus dot{\'e}s d{'}une structure interne (mots affix{\'e}s ou compos{\'e}s) peuvent fournir des indices linguistiques {\`a} un locuteur, ce qui peut l{'}aider {\`a} analyser et {\`a} comprendre ces mots. Afin de tester notre hypoth{\`e}se, nous proposons de travailler sur un ensemble de mots techniques provenant du domaine m{\'e}dical. Un grand ensemble de mots techniques est annot{\'e} par cinq annotateurs. Nous effectuons deux types d{'}analyses : l{'}analyse de l{'}{\'e}volution des mots compr{\'e}hensibles et incompr{\'e}hensibles (de mani{\`e}re g{\'e}n{\'e}rale et en fonction de certains suffixes) et l{'}analyse des clusters avec ces mots cr{\'e}{\'e}s par apprentissage non-supervis{\'e}, sur la base des descripteurs linguistiques et extra-linguistiques. Nos r{\'e}sultats indiquent que, selon la sensibilit{\'e} linguistique des annotateurs, les mots techniques peuvent devenir d{\'e}codables et compr{\'e}hensibles. Quant aux clusters, le contenu de certains refl{\`e}te la difficult{\'e} des mots qui les composent et montre {\'e}galement la progression des annotateurs dans leur compr{\'e}hension. La ressource construite est disponible pour la recherche : http://natalia.grabar.free.fr/rated-lexicon.html."
L16-1420,A Large Rated Lexicon with {F}rench Medical Words,2016,22,1,2,0,5649,natalia grabar,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Patients are often exposed to medical terms, such as anosognosia, myelodysplastic, or hepatojejunostomy, that can be semantically complex and hardly understandable by non-experts in medicine. Hence, it is important to assess which words are potentially non-understandable and require further explanations. The purpose of our work is to build specific lexicon in which the words are rated according to whether they are understandable or non-understandable. We propose to work with medical words in French such as provided by an international medical terminology. The terms are segmented in single words and then each word is manually processed by three annotators. The objective is to assign each word into one of the three categories: I can understand, I am not sure, I cannot understand. The annotators do not have medical training nor they present specific medical problems. They are supposed to represent an average patient. The inter-annotator agreement is then computed. The content of the categories is analyzed. Possible applications in which this lexicon can be helpful are proposed and discussed. The rated lexicon is freely available for the research purposes. It is accessible online at http://natalia.grabar.perso.sfr.fr/rated-lexicon.html"
2016.jeptalnrecital-poster.20,Impact de l{'}agglutination dans l{'}extraction de termes en arabe standard moderne (Adaptation of a term extractor to the {M}odern {S}tandard {A}rabic language),2016,-1,-1,2,1,30997,wafa neifar,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"Nous pr{\'e}sentons, dans cet article, une adaptation {\`a} l{'}arabe standard moderne d{'}un extracteur de termes pour le fran{\c{c}}ais et l{'}anglais. L{'}adaptation a d{'}abord consist{\'e} {\`a} d{\'e}crire le processus d{'}extraction des termes de mani{\`e}re similaire {\`a} celui d{\'e}fini pour l{'}anglais et le fran{\c{c}}ais en prenant en compte certains particularit{\'e}s morpho-syntaxiques de la langue arabe. Puis, nous avons consid{\'e}r{\'e} le ph{\'e}nom{\`e}ne de l{'}agglutination de la langue arabe. L{'}{\'e}valuation a {\'e}t{\'e} r{\'e}alis{\'e}e sur un corpus de textes m{\'e}dicaux. Les r{\'e}sultats montrent que parmi 400 termes candidats maximaux analys{\'e}s, 288 sont jug{\'e}s corrects par rapport au domaine (72,1{\%}). Les erreurs d{'}extraction sont dues {\`a} l{'}{\'e}tiquetage morpho-syntaxique et {\`a} la non-voyellation des textes mais aussi {\`a} des ph{\'e}nom{\`e}nes d{'}agglutination."
2016.jeptalnrecital-demo.11,Interface Web pour l{'}annotation morpho-syntaxique de textes (Web interface for the morpho-syntactic annotation of texts),2016,-1,-1,1,1,18582,thierry hamon,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 5 : D{\\'e}monstrations,0,"Nous pr{\'e}sentons une interface Web pour la visualisation etl{'}annotation de textes avec des {\'e}tiquettes morphosyntaxiques etdes lemmes. Celle-ci est actuellement utilis{\'e}e pour annoter destextes ukrainiens avec le jeu d{'}{\'e}tiquettes Multext-East.Les utilisateurs peuvent rapidement visualiser les annotationsassoci{\'e}es aux mots d{'}un texte, modifier les annotationsexistantes ou en ajouter de nouvelles. Les annotations peuvent charg{\'e}eset export{\'e}es en XML au format TEI, mais aussi sous forme tabul{\'e}e.Des scripts de conversion de format et de chargement dans une basede donn{\'e}es sont {\'e}galement mis {\`a} disposition."
2015.jeptalnrecital-long.16,Extraction automatique de paraphrases grand public pour les termes m{\\'e}dicaux,2015,-1,-1,2,0,5649,natalia grabar,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous sommes tous concern{\'e}s par notre {\'e}tat de sant{\'e} et restons sensibles aux informations de sant{\'e} disponibles dans la soci{\'e}t{\'e} moderne {\`a} travers par exemple les r{\'e}sultats des recherches scientifiques, les m{\'e}dias sociaux de sant{\'e}, les documents cliniques, les {\'e}missions de t{\'e}l{\'e} et de radio ou les nouvelles. Cependant, il est commun de rencontrer dans le domaine m{\'e}dical des termes tr{\`e}s sp{\'e}cifiques (e.g., bl{\'e}pharospasme, alexitymie, appendicectomie), qui restent difficiles {\`a} comprendre par les non sp{\'e}cialistes. Nous proposons une m{\'e}thode automatique qui vise l{'}acquisition de paraphrases pour les termes m{\'e}dicaux, qui soient plus faciles {\`a} comprendre que les termes originaux. La m{\'e}thode est bas{\'e}e sur l{'}analyse morphologique des termes, l{'}analyse syntaxique et la fouille de textes non sp{\'e}cialis{\'e}s. L{'}analyse et l{'}{\'e}valuation des r{\'e}sultats indiquent que de telles paraphrases peuvent {\^e}tre trouv{\'e}es dans les documents non sp{\'e}cialis{\'e}s et pr{\'e}sentent une compr{\'e}hension plus facile. En fonction des param{\`e}tres de la m{\'e}thode, la pr{\'e}cision varie entre 86 et 55{\%}. Ce type de ressources est utile pour plusieurs applications de TAL (e.g., recherche d{'}information grand public, lisibilit{\'e} et simplification de textes, syst{\`e}mes de question-r{\'e}ponses)."
W14-6301,Automatic Analysis of Scientific and Literary Texts. Presentation and Results of the {DEFT}2014 Text Mining Challenge (Analyse automatique de textes litt{\\'e}raires et scientifiques : pr{\\'e}sentation et r{\\'e}sultats du d{\\'e}fi fouille de texte {DEFT}2014) [in {F}rench],2014,-1,-1,1,1,18582,thierry hamon,TALN-RECITAL 2014 Workshop DEFT 2014 : D{\\'E}fi Fouille de Textes (DEFT 2014 Workshop: Text Mining Challenge),0,None
W14-4801,Generalising and Normalising Distributional Contexts to Reduce Data Sparsity: Application to Medical Corpora,2014,-1,-1,2,0,38354,amandine perinet,Proceedings of the 4th International Workshop on Computational Terminology (Computerm),0,None
W14-4812,Unsupervised Method for the Acquisition of General Language Paraphrases for Medical Compounds,2014,-1,-1,2,0,5649,natalia grabar,Proceedings of the 4th International Workshop on Computational Terminology (Computerm),0,None
W14-1202,Automatic diagnosis of understanding of medical words,2014,42,2,2,0,5649,natalia grabar,Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations ({PITR}),0,"Within the medical field, very specialized terms are commonly used, while their understanding by laymen is not always successful. We propose to study the understandability of medical words by laymen. Three annotators are involved in the creation of the reference data used for training and testing. The features of the words may be linguistic (i.e., number of characters, syllables, number of morphological bases and affixes) and extra-linguistic (i.e., their presence in a reference lexicon, frequency on a search engine). The automatic categorization results show between 0.806 and 0.947 F-measure values. It appears that several features and their combinations are relevant for the analysis of understandability (i.e., syntactic categories, presence in reference lexica, frequency on the general search engine, final substring)."
W14-1114,Reducing {VSM} data sparseness by generalizing contexts: application to health text mining,2014,29,0,2,0,38354,amandine perinet,Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi),0,"Vector Space Models are limited with low frequency words due to few available contexts and data sparseness. To tackle this problem, we generalize contexts by integrating semantic relations acquired with linguistic approaches. We use three methods that acquire hypernymy relations on a EHR corpus. Context Generalization obtains the best results when performed with hypernyms, the quality of the relations being more important than the quantity."
W14-1116,Tuning {H}eidel{T}ime for identifying time expressions in clinical texts in {E}nglish and {F}rench,2014,18,5,1,1,18582,thierry hamon,Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi),0,"We present work on tuning the Heideltime system for identifying time expressions in clinical texts in English and French languages. The main amount of the method is related to the enrichment and adaptation of linguistic resources to identify Timex3 clinical expressions and to normalize them. The test of the adapted versions have been done on the i2b2/VA 2012 corpus for English and a collection of clinical texts for French, which have been annotated for the purpose of this study. We achieve a 0.8500 F-measure on the recognition and normalization of temporal expressions in English, and up to 0.9431 in French. Future work will allow to improve and consolidate the results."
F14-1021,Reducing data sparsity by generalising distributional contexts: application to specialised texts (R{\\'e}duction de la dispersion des donn{\\'e}es par g{\\'e}n{\\'e}ralisation des contextes distributionnels : application aux textes de sp{\\'e}cialit{\\'e}) [in {F}rench],2014,-1,-1,2,0,38354,amandine perinet,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
W13-2013,{B}io{NLP} Shared Task 2013: Supporting Resources,2013,25,3,3,0,4231,pontus stenetorp,Proceedings of the {B}io{NLP} Shared Task 2013 Workshop,0,"This paper describes the technical contribution of the supporting resources provided for the BioNLP Shared Task 2013. Following the tradition of the previous two BioNLP Shared Task events, the task organisers and several external groups sought to make system development easier for the task participants by providing automatically generated analyses using a variety of automated tools. Providing analyses created by different tools that address the same task also enables extrinsic evaluation of the tools through the evaluation of their contributions to the event extraction task. Such evaluation can improve understanding of the applicability and benefits of specific tools and representations. The supporting resources described in this paper will continue to be publicly available from the shared task homepage http://2013.bionlp-st.org/"
F13-1005,Grouping of terms based on linguistic and semantic regularities in a cross-lingual context (Groupement de termes bas{\\'e} sur des r{\\'e}gularit{\\'e}s linguistiques et s{\\'e}mantiques dans un contexte cross-langue) [in {F}rench],2013,-1,-1,2,0,41761,marie dupuch,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
W12-2403,Semantic distance and terminology structuring methods for the detection of semantically close terms,2012,19,2,3,0,41761,marie dupuch,{B}io{NLP}: Proceedings of the 2012 Workshop on Biomedical Natural Language Processing,0,"The identification of semantically similar linguistic expressions despite their formal difference is an important task within NLP applications (information retrieval and extraction, terminology structuring...) We propose to detect the semantic relatedness between biomedical terms from the pharmacovigilance area. Two approaches are exploited: semantic distance within structured resources and terminology structuring methods applied to a raw list of terms. We compare these methods and study their complementarity. The results are evaluated against the reference pharmacovigilance data and manually by an expert."
W12-2413,Combining Compositionality and Pagerank for the Identification of Semantic Relations between Biomedical Words,2012,25,0,1,1,18582,thierry hamon,{B}io{NLP}: Proceedings of the 2012 Workshop on Biomedical Natural Language Processing,0,"The acquisition of semantic resources and relations is an important task for several applications, such as query expansion, information retrieval and extraction, machine translation. However, their validity should also be computed and indicated, especially for automatic systems and applications. We exploit the compositionality based methods for the acquisition of synonymy relations and of indicators of these synonyms. We then apply pagerank-derived algorithm to the obtained semantic graph in order to filter out the acquired synonyms. Evaluation performed with two independent experts indicates that the quality of synonyms is systematically improved by 10 to 15% after their filtering."
W12-1103,Acquisition terminologique pour identifier les mots-cl{\\'e}s d{'}articles scientifiques (Terminological acquisition for identifying keywords of scientific articles) [in {F}rench],2012,0,0,1,1,18582,thierry hamon,"JEP-TALN-RECITAL 2012, Workshop DEFT 2012: D{\\'E}fi Fouille de Textes (DEFT 2012 Workshop: Text Mining Challenge)",0,None
W09-1311,Exploring Graph Structure for Detection of Reliability Zones within Synonym Resources: Experiment with the Gene Ontology,2009,18,1,1,1,18582,thierry hamon,Proceedings of the {B}io{NLP} 2009 Workshop,0,"Computing the semantic similarity between terms relies on existence and usage of semantic resources. However, these resources, often composed of equivalent units, or synonyms, must be first analyzed and weighted in order to define within them the reliability zones where the semantic cohesiveness is stronger. We propose an original method for acquisition of elementary synonyms based on exploitation of structured terminologies, analysis of syntactic structure of complex (multi-unit) terms and their compositionality. The acquired synonyms are then profiled thanks to endogenous lexical and linguistic indicators (other types of relations, lexical inclusions, productivity), which are automatically inferred within the same terminologies. Additionally, synonymy relations are observed within graph, and its structure is analyzed. Particularly, we explore the usefulness of the graph theory notions such as connected component, clique, density, bridge, articulation vertex, and centrality of vertices."
2009.jeptalnrecital-court.31,Profilage s{\\'e}mantique endog{\\`e}ne des relations de synonymie au sein de Gene Ontology,2009,-1,-1,1,1,18582,thierry hamon,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Le calcul de la similarit{\'e} s{\'e}mantique entre les termes repose sur l{'}existence et l{'}utilisation de ressources s{\'e}mantiques. Cependant de telles ressources, qui proposent des {\'e}quivalences entre entit{\'e}s, souvent des relations de synonymie, doivent elles-m{\^e}mes {\^e}tre d{'}abord analys{\'e}es afin de d{\'e}finir des zones de fiabilit{\'e} o{\`u} la similarit{\'e} s{\'e}mantique est plus forte. Nous proposons une m{\'e}thode d{'}acquisition de synonymes {\'e}l{\'e}mentaires gr{\^a}ce {\`a} l{'}exploitation des terminologies structur{\'e}es au travers l{'}analyse de la structure syntaxique des termes complexes et de leur compositionnalit{\'e}. Les synonymes acquis sont ensuite profil{\'e}s gr{\^a}ce aux indicateurs endog{\`e}nes inf{\'e}r{\'e}s automatiquement {\`a} partir de ces m{\^e}mes terminologies (d{'}autres types de relations, inclusions lexicales, productivit{\'e}, forme des composantes connexes). Dans le domaine biom{\'e}dical, il existe de nombreuses terminologies structur{\'e}es qui peuvent {\^e}tre exploit{\'e}es pour la constitution de ressources s{\'e}mantiques. Le travail pr{\'e}sent{\'e} ici exploite une de ces terminologies, Gene Ontology."
2007.jeptalnrecital-poster.10,{OGMIOS} : une plate-forme d{'}annotation linguistique de collection de documents issus du Web,2007,-1,-1,1,1,18582,thierry hamon,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"L{'}un des objectifs du projet ALVIS est d{'}int{\'e}grer des informations linguistiques dans des moteurs de recherche sp{\'e}cialis{\'e}s. Dans ce contexte, nous avons con{\c{c}}u une plate-forme d{'}enrichissement linguistique de documents issus du Web, OGMIOS, exploitant des outils de TAL existants. Les documents peuvent {\^e}tre en fran{\c{c}}ais ou en anglais. Cette architecture est distribu{\'e}e, afin de r{\'e}pondre aux contraintes li{\'e}es aux traitements de gros volumes de textes, et adaptable, pour permettre l{'}analyse de sous-langages. La plate-forme est d{\'e}velopp{\'e}e en Perl et disponible sous forme de modules CPAN. C{'}est une structure modulaire dans lequel il est possible d{'}int{\'e}grer de nouvelles ressources ou de nouveaux outils de TAL. On peut ainsi d{\'e}finir des configuration diff{\'e}rentes pour diff{\'e}rents domaines et types de collections. Cette plateforme robuste permet d{'}analyser en masse des donn{\'e}es issus du web qui sont par essence tr{\`e}s h{\'e}t{\'e}rog{\`e}nes. Nous avons {\'e}valu{\'e} les performances de la plateforme sur plusieurs collections de documents. En distribuant les traitements sur vingt machines, une collection de 55 329 documents du domaine de la biologie (106 millions de mots) a {\'e}t{\'e} annot{\'e}e en 35 heures tandis qu{'}une collection de 48 422 d{\'e}p{\^e}ches relatives aux moteurs de recherche (14 millions de mots) a {\'e}t{\'e} annot{\'e}e en 3 heures et 15 minutes."
2005.jeptalnrecital-long.30,Comment mesurer la couverture d{'}une ressource terminologique pour un corpus ?,2005,-1,-1,3,0,51253,goritsa ninova,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,Cet article propose une d{\'e}finition formelle de la notion de couverture lexicale. Celleci repose sur un ensemble de quatre m{\'e}triques qui donnent une vue globale de l{'}ad{\'e}quation d{'}une ressource lexicale {\`a} un corpus et permettent ainsi de guider le choix d{'}une ressource en fonction d{'}un corpus donn{\'e}. Les m{\'e}triques propos{\'e}es sont test{\'e}es dans le contexte de l{'}analyse de corpus sp{\'e}cialis{\'e}s en g{\'e}nomique : 5 terminologies diff{\'e}rentes sont confront{\'e}es {\`a} 4 corpus. La combinaison des valeurs obtenues permet de discerner diff{\'e}rents types de relations entre ressources et corpus.
W04-1207,Event-Based Information Extraction for the Biomedical Domain: the Caderige Project,2004,19,26,5,0,44285,erick alphonse,Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications ({NLPBA}/{B}io{NLP}),0,"This paper gives an overview of the Caderige project. This project involves teams from different areas (biology, machine learning, natural language processing) in order to develop highlevel analysis tools for extracting structured information from biological bibliographical databases, especially Medline. The paper gives an overview of the approach and compares it to the state of the art."
hamon-hu-2002-evaluate,How to evaluate necessary cooperative systems of terminology building?,2002,19,0,1,1,18582,thierry hamon,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Terminology building cannot be considered as a full automated process but rather as a cooperative task between terminological tools and terminologists. Identifying terms in a technical domain is a matter of word usage and expert agreement. We point out the problem of the evaluation of such tools: their quality and their contribution to the terminology building is difficult to estimate and cannot be fully evaluated with usual precision and recall measures. We aim at evaluating more globally their technical aspects and their usability. We give a non-exhaustive list of the features of such evaluation. Then, we apply them on four terminological systems."
2001.jeptalnrecital-long.19,Exploitation de l{'}expertise humaine dans un processus de constitution de terminologie,2001,-1,-1,1,1,18582,thierry hamon,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Le processus de construction de terminologie ne peut {\^e}tre enti{\`e}rement automatis{\'e}. Les m{\'e}thodes et des outils de la terminologie computationnelle permettent de prendre en charge une partie de la t{\^a}che, mais l{'}expertise humaine garde une place pr{\'e}pond{\'e}rant. Le d{\'e}fi pour les outils terminologiques est de d{\'e}grossir les t{\^a}ches qui sont soit trop longues soit trop complexes pour l{'}utilisateur tout en permettant {\`a} ce dernier d{'}int{\'e}grer ses propres connaissances sp{\'e}cialis{\'e}es et en lui laissant le contr{\^o}le sur la terminologie {\`a} construire. Nous montrons ici comment le r{\^o}le de cette expertise est pris en compte dans SynoTerm, l{'}outil d{'}acquisition de relation de synonymie entre termes que nous avons d Ìevelopp{\'e}."
P98-1082,A Step towards the Detection of Semantic Variants of Terms in Technical Documents,1998,13,30,1,1,18582,thierry hamon,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"This paper reports the results of a preliminary experiment on the detection of semantic variants of terms in a French technical document. The general goal of our work is to help the structuration of terminologies. Two kinds of semantic variants can be found in traditional terminologies: strict synonymy links and fuzzier relations like see-also. We have designed three rules which exploit general dictionary information to infer synonymy relations between complex candidate terms. The results have been examined by a human terminologist. The expert has judged that half of the overall pairs of terms are relevant for the semantic variation. He validated an important part of the detected links as synonymy. Moreover, it appeared that numerous errors are due to few mis-interpreted links: they could be eliminated by few exception rules."
C98-1079,A step towards the detection of semantic variants of terms in technical documents,1998,13,30,1,1,18582,thierry hamon,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"This paper reports the results of a preliminary experiment on the detection of semantic variants of terms in a French technical document. The general goal of our work is to help the structuration of terminologies. Two kinds of semantic variants can be found in traditional terminologies: strict synonymy links and fuzzier relations like see-also. We have designed three rules which exploit general dictionary information to infer synonymy relations between complex candidate terms. The results have been examined by a human terminologist. The expert has judged that half of the overall pairs of terms are relevant for the semantic variation. He validated an important part of the detected links as synonymy. Moreover, it appeared that numerous errors are due to few mis-interpreted links: they could be eliminated by few exception rules."
