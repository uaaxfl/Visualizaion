2011.iwslt-evaluation.16,2011.iwslt-evaluation.15,1,\N,Missing
2011.iwslt-evaluation.16,H05-1026,1,\N,Missing
2020.acl-demos.31,W12-1814,0,0.210613,"industrial contexts such as Amazon Alexa, Apple Siri, Microsoft Cortana, Google Duplex, XiaoIce (Zhou et al., 2018) and Furhat2 , as well as academia such as MuMMER (Foster et al., 2016) and Alana (Curry et al., 2018). However, open-source toolkits and frameworks for developing such systems are rare, especially for developing multi-modal systems comprised of speech, text, and vision. Most of the existing toolkits are designed for developing dialog systems focused only on core dialog components, with or without the option to access external speech processing services (Bohus and Rudnicky, 2009; Baumann and Schlangen, 2012; Lison and Kennington, 2016; Ultes et al., 2017; Ortega et al., 2019; Lee et al., 2019). To the best of our knowledge, there are only two * All authors contributed equally. Link to open-source code: https://github.com/ DigitalPhonetics/adviser 2 https://docs.furhat.io 1 toolkits, proposed in (Foster et al., 2016) and (Bohus et al., 2017), that support developing dialog agents using multi-modal processing and social signals (Wagner et al., 2013). Both provide a decent platform for building systems, however, to the best of our knowledge, the former is not open-source, and the latter is based on"
2020.acl-demos.31,N12-1010,0,0.0329781,"ral, sad}, (b) arousal levels {low, medium, high}, and (c) valence levels {negative, neutral, positive}. The models are trained on the IEMOCAP dataset (Busso et al., 2008). The output of the emotion recognition module consists of three predictions per user turn, which can then be used by the user state tracker (see section 3.4). For future releases, we plan to incorporate multiple training datasets as well as visual features. Engagement Level Prediction User engagement is closely related to states such as boredom and level of interest, with implications for user satisfaction and task success (Forbes-Riley et al., 2012; Schuller et al., 2009). In ADVISER, we assume that eye activity serves as an indicator of various mental states (Schuller et al., 2009; Niu et al., 2018) and implement a gaze tracker that monitors the user’s direction of focus via webcam. Using OpenFace 2.2.0, a toolkit for facial behavior analysis (Baltrusaitis et al., 2018), we extract the features gaze angle x and gaze angle y, which capture left-right and up-down eye movement, for each frame and compute the deviation from the central point of the screen. If the deviation exceeds a certain threshold for a certain number of seconds, the us"
2020.acl-demos.31,P16-4012,0,0.0795616,"mazon Alexa, Apple Siri, Microsoft Cortana, Google Duplex, XiaoIce (Zhou et al., 2018) and Furhat2 , as well as academia such as MuMMER (Foster et al., 2016) and Alana (Curry et al., 2018). However, open-source toolkits and frameworks for developing such systems are rare, especially for developing multi-modal systems comprised of speech, text, and vision. Most of the existing toolkits are designed for developing dialog systems focused only on core dialog components, with or without the option to access external speech processing services (Bohus and Rudnicky, 2009; Baumann and Schlangen, 2012; Lison and Kennington, 2016; Ultes et al., 2017; Ortega et al., 2019; Lee et al., 2019). To the best of our knowledge, there are only two * All authors contributed equally. Link to open-source code: https://github.com/ DigitalPhonetics/adviser 2 https://docs.furhat.io 1 toolkits, proposed in (Foster et al., 2016) and (Bohus et al., 2017), that support developing dialog agents using multi-modal processing and social signals (Wagner et al., 2013). Both provide a decent platform for building systems, however, to the best of our knowledge, the former is not open-source, and the latter is based on the .NET platform, which co"
2020.acl-demos.31,P19-3016,1,0.831063,"Missing"
2020.acl-demos.31,N07-2038,0,0.0747895,"nd the system has a better understanding of user intent. User State Tracking (UST): Similar to the BST, the UST tracks the history of the user’s state over the course of a dialog, with one entry per turn. In the current implementation, the user state consists of the user’s engagement level, valence, arousal, and emotion category (details in section 3.1). 3.5 User Simulator To support automatic evaluation and to train the RL policy, we provide a user simulator service outputting at the user acts level. As we are concerned with task-oriented dialogs here, the user simulator has an agenda-based (Schatzmann et al., 2007) architecture and is randomly assigned a goal at the beginning of the dialog. Each turn, it then works to first respond to the system utterance, and then after to fulfill its own goal. When the system utterance also works toward fulfilling the user goal, the RL policy is rewarded by achieving a shorter total dialog turn count (Ortega et al., 2019). Dialog Policies Policies To determine the correct system action, we provide three types of policy services: a handcrafted and a reinforcement learning policy for finding entities from a database (Ortega et al., 2019), as well as a handcrafted policy"
2020.acl-demos.31,P17-4013,0,0.0405033,"Missing"
2020.acl-demos.31,W19-5908,1,0.878184,"Missing"
2020.acl-demos.31,2020.cl-1.2,0,\N,Missing
2020.acl-main.134,W11-2832,0,0.0387203,"an Problem (TSP), and use a biaffine attention model to calculate the edge costs. We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding. 1 Introduction Surface realization is the task of generating a sentence from a syntactic or semantic representation. In several shared tasks (Belz et al., 2011; Mille et al., 2018, 2019), the input representations are unordered dependency trees. The state-of-the-art system (Yu et al., 2019a) in the Surface Realization Shared Task 2019 (SR’19) takes a pipeline approach, where the first step is linearization, namely ordering the tokens in the dependency tree. They use a Tree-LSTM to encode each token with awareness of the whole tree, then apply the divide-andconquer strategy to split the full tree into subtrees and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The"
2020.acl-main.134,D12-1085,1,0.791573,"Missing"
2020.acl-main.134,C10-1012,0,0.31125,"input representations are unordered dependency trees. The state-of-the-art system (Yu et al., 2019a) in the Surface Realization Shared Task 2019 (SR’19) takes a pipeline approach, where the first step is linearization, namely ordering the tokens in the dependency tree. They use a Tree-LSTM to encode each token with awareness of the whole tree, then apply the divide-andconquer strategy to split the full tree into subtrees and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The general strategy is adapted from Bohnet et al. (2010). In this work, we tackle linearization decoding in a different way, by casting it as a Traveling Salesman Problem (TSP). Knight (1999) first formulated the word ordering of the target language in word-based machine translation as a TSP, where the words are the nodes to traverse, and the log probabilities of the bigrams are the edge costs. Several works have followed this formulation. Among others, Zaslavskiy et al. (2009) formulate the word ordering in phrase-based machine translation as a TSP, and show that it achieves better performance and speed than beam search decoding with the same bigr"
2020.acl-main.134,P81-1022,0,0.697299,"Missing"
2020.acl-main.134,P01-1030,0,0.37145,"e word ordering in phrase-based machine translation as a TSP, and show that it achieves better performance and speed than beam search decoding with the same bigram language model. Horvat and Byrne (2014) explore higher-order n-gram language models for TSP-based word ordering, which transforms into a much larger TSP graph. All of the aforementioned works operate on a bag of words without syntax, which is a TSP graph of non-trivial size with little information about the internal structure. Much effort has been put into incorporating more powerful decoding algorithms such as Integer Programming (Germann et al., 2001) and Dynamic Programming (Tillmann and Ney, 2003). Our work differs from the previous work on TSP-based word ordering in several aspects. (1) Linearization is a special case of word ordering with syntax, where we can use a tree-structured encoder to provide better representation of the tokens. (2) We adopt the divide-and-conquer strategy to break down the full tree into subtrees and order each subtree separately, which is faster and more reliable with an approximate decoder. (3) We apply deep biaffine attention (Dozat and Manning, 2016), which has yielded great improvements in dependency parsi"
2020.acl-main.134,E14-3010,0,0.0162235,"tackle linearization decoding in a different way, by casting it as a Traveling Salesman Problem (TSP). Knight (1999) first formulated the word ordering of the target language in word-based machine translation as a TSP, where the words are the nodes to traverse, and the log probabilities of the bigrams are the edge costs. Several works have followed this formulation. Among others, Zaslavskiy et al. (2009) formulate the word ordering in phrase-based machine translation as a TSP, and show that it achieves better performance and speed than beam search decoding with the same bigram language model. Horvat and Byrne (2014) explore higher-order n-gram language models for TSP-based word ordering, which transforms into a much larger TSP graph. All of the aforementioned works operate on a bag of words without syntax, which is a TSP graph of non-trivial size with little information about the internal structure. Much effort has been put into incorporating more powerful decoding algorithms such as Integer Programming (Germann et al., 2001) and Dynamic Programming (Tillmann and Ney, 2003). Our work differs from the previous work on TSP-based word ordering in several aspects. (1) Linearization is a special case of word"
2020.acl-main.134,Q16-1023,0,0.482967,"matrix by a margin: X X ( max(0, 1 + si,j 0 − si,j ) L= (i,j)∈z + j 0 6=j X max(0, 1 + si0 ,j − si,j )) This objective aims to maximizing the score of each correct bigram (i, j) in both directions, essentially log P (j|i) and log P (i|j), where the cells in the same row corresponds to all possible tokens following i, and the cells in the column corresponds to all possible tokens preceding j. The objective is greedy in the sense that it updates more than “necessary” to decode the correct path. We contrast it to the structured loss in most graph-based dependency parsers (McDonald et al., 2005; Kiperwasser and Goldberg, 2016), which updates the scores of the correct path z against the highest scoring incorrect path z 0 : (6) L0 = max(0, 1+ max 0 z 6=z Finally, we turn the score matrix into a nonnegative cost matrix for the TSP solver: C = max (S) − S (7) Our model is inspired by the biaffine dependency parser of Dozat and Manning (2016), but stands in contrast in many aspects. They use a bidirectional LSTM to encode the sequential information of the tokens, and the biaffine attention itself does not (8) i0 6=i X (i0 ,j 0 )∈z 0 si0 ,j 0 − X si,j ) (i,j)∈z (9) The greedy objective for the TSP has two main advantages"
2020.acl-main.134,J99-4005,0,0.652196,"19 (SR’19) takes a pipeline approach, where the first step is linearization, namely ordering the tokens in the dependency tree. They use a Tree-LSTM to encode each token with awareness of the whole tree, then apply the divide-andconquer strategy to split the full tree into subtrees and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The general strategy is adapted from Bohnet et al. (2010). In this work, we tackle linearization decoding in a different way, by casting it as a Traveling Salesman Problem (TSP). Knight (1999) first formulated the word ordering of the target language in word-based machine translation as a TSP, where the words are the nodes to traverse, and the log probabilities of the bigrams are the edge costs. Several works have followed this formulation. Among others, Zaslavskiy et al. (2009) formulate the word ordering in phrase-based machine translation as a TSP, and show that it achieves better performance and speed than beam search decoding with the same bigram language model. Horvat and Byrne (2014) explore higher-order n-gram language models for TSP-based word ordering, which transforms in"
2020.acl-main.134,H05-1066,0,0.422268,"Missing"
2020.acl-main.134,W18-3601,0,0.0548761,"nd use a biaffine attention model to calculate the edge costs. We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding. 1 Introduction Surface realization is the task of generating a sentence from a syntactic or semantic representation. In several shared tasks (Belz et al., 2011; Mille et al., 2018, 2019), the input representations are unordered dependency trees. The state-of-the-art system (Yu et al., 2019a) in the Surface Realization Shared Task 2019 (SR’19) takes a pipeline approach, where the first step is linearization, namely ordering the tokens in the dependency tree. They use a Tree-LSTM to encode each token with awareness of the whole tree, then apply the divide-andconquer strategy to split the full tree into subtrees and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The general strategy is"
2020.acl-main.134,D19-6301,0,0.312681,"Missing"
2020.acl-main.134,P09-1040,0,0.5824,"nstead of any other bigram in the same row or column. 2.5 Table 1: The shift-swap transition system. Generating Non-Projective Trees 1 If we directly linearize the full tree, the output is naturally unrestricted, i.e., possibly non-projective. However, when we linearize each subtree separately in order to reduce the search space, as in the proposed method, the reconstructed output is restricted to be projective (Bohnet et al., 2010). To relax the projectivity restriction, we design a transition system to reorder projective trees into non-projective trees as a post-processing step, inspired by Nivre (2009) but working in the opposite way. It is essentially a reduced version of their transition system, removing the attachment transitions and keeping only swap and shift. In the transition system (as shown in Table 1), a configuration consists of a stack σ, which is initially empty, and a buffer β, which initially holds all input tokens. The shift transition moves the front of the buffer to the top of the stack, and the swap transition moves the top of the stack back to the second place in the buffer. When all tokens are moved from the buffer to the stack, the procedure terminates. To prevent the"
2020.acl-main.134,L16-1262,0,0.0276742,"Missing"
2020.acl-main.134,P05-1013,0,0.108146,"sitions (cf. Vinyals et al. (2015) for the discussion on encoding a set with an LSTM). In contrast, when we only use this sys1454 tem to reorder a linearized projective tree as postprocessing, where input sequence is meaningful and consistent, it is much easier to learn. Using the swap system as a post-processing step stands in contrast to Bohnet et al. (2012), where they pre-process the tree by lifting the arcs so that the correct word order could form a projective tree. These two approaches draw inspiration from the non-projective parsing in Nivre (2009) and the pseudo-projective parsing in Nivre and Nilsson (2005) respectively. We argue that our post-processing approach is more convenient since there is no need to change the syntactic annotation in the original tree, and it is much easier to evaluate the effectiveness of the sorting model. 2.6 Relative Word Order Constraints In the SR’19 dataset, some relative word order information is given, which indicates e.g. the order of the conjuncts in the coordination. Since the order in a coordination is generally arbitrary (at least syntactically), it will thus introduce randomness in the single reference evaluation. We believe that using such information lea"
2020.acl-main.134,P02-1040,0,0.107753,"Missing"
2020.acl-main.134,N16-1058,0,0.0583907,"Baselines We use the datasets from the Surface Realization 2019 Shared Task (Mille et al., 2019) in our experiments, which includes 11 languages in 20 treebanks from the Universal Dependencies (Nivre et al., 2016). We experiment on the shallow track, i.e., all tokens in the output are present in the input tree. We only report the BLEU score (Papineni et al., 2002) as the evaluation metric, since we mostly evaluate on the lemma level, where the metrics involving word forms are irrelevant. As baselines for the final evaluation, we use several available linearizers by Bohnet et al. (2010) (B10), Puduppully et al. (2016) (P16) and Yu et al. (2019a) (Y19). B10, P16 and our linearizer all use the same inflection and contraction models, trained with the same hyperparameters as in Y19, and we compare to the reported shared task results of Y19. 3.2 Main Results Table 2 shows the performance of different linearizers, where beam is the baseline beam-search linearizer as in Yu et al. (2019b) with default hyperparameters, full is the TSP decoder on the full tree level, sub is the TSP decoder on the subtree level, and +swap is sub post-processed with reordering. We test the decoders under two conditions: without word o"
2020.acl-main.134,J03-1005,0,0.173486,"tion as a TSP, and show that it achieves better performance and speed than beam search decoding with the same bigram language model. Horvat and Byrne (2014) explore higher-order n-gram language models for TSP-based word ordering, which transforms into a much larger TSP graph. All of the aforementioned works operate on a bag of words without syntax, which is a TSP graph of non-trivial size with little information about the internal structure. Much effort has been put into incorporating more powerful decoding algorithms such as Integer Programming (Germann et al., 2001) and Dynamic Programming (Tillmann and Ney, 2003). Our work differs from the previous work on TSP-based word ordering in several aspects. (1) Linearization is a special case of word ordering with syntax, where we can use a tree-structured encoder to provide better representation of the tokens. (2) We adopt the divide-and-conquer strategy to break down the full tree into subtrees and order each subtree separately, which is faster and more reliable with an approximate decoder. (3) We apply deep biaffine attention (Dozat and Manning, 2016), which has yielded great improvements in dependency parsing, and reinterpret it as a bigram language model"
2020.acl-main.134,D19-6306,1,0.820137,"ach subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding. 1 Introduction Surface realization is the task of generating a sentence from a syntactic or semantic representation. In several shared tasks (Belz et al., 2011; Mille et al., 2018, 2019), the input representations are unordered dependency trees. The state-of-the-art system (Yu et al., 2019a) in the Surface Realization Shared Task 2019 (SR’19) takes a pipeline approach, where the first step is linearization, namely ordering the tokens in the dependency tree. They use a Tree-LSTM to encode each token with awareness of the whole tree, then apply the divide-andconquer strategy to split the full tree into subtrees and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The general strategy is adapted from Bohnet et al. (2010). In this work, we tackle linearization decoding in a different way, by castin"
2020.acl-main.134,W19-8636,1,0.70168,"ach subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding. 1 Introduction Surface realization is the task of generating a sentence from a syntactic or semantic representation. In several shared tasks (Belz et al., 2011; Mille et al., 2018, 2019), the input representations are unordered dependency trees. The state-of-the-art system (Yu et al., 2019a) in the Surface Realization Shared Task 2019 (SR’19) takes a pipeline approach, where the first step is linearization, namely ordering the tokens in the dependency tree. They use a Tree-LSTM to encode each token with awareness of the whole tree, then apply the divide-andconquer strategy to split the full tree into subtrees and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The general strategy is adapted from Bohnet et al. (2010). In this work, we tackle linearization decoding in a different way, by castin"
2020.acl-main.134,P09-1038,0,0.0336679,"es and find the optimal order for each subtree using beam search. Finally, the linearized subtrees are combined into a full projective tree. The general strategy is adapted from Bohnet et al. (2010). In this work, we tackle linearization decoding in a different way, by casting it as a Traveling Salesman Problem (TSP). Knight (1999) first formulated the word ordering of the target language in word-based machine translation as a TSP, where the words are the nodes to traverse, and the log probabilities of the bigrams are the edge costs. Several works have followed this formulation. Among others, Zaslavskiy et al. (2009) formulate the word ordering in phrase-based machine translation as a TSP, and show that it achieves better performance and speed than beam search decoding with the same bigram language model. Horvat and Byrne (2014) explore higher-order n-gram language models for TSP-based word ordering, which transforms into a much larger TSP graph. All of the aforementioned works operate on a bag of words without syntax, which is a TSP graph of non-trivial size with little information about the internal structure. Much effort has been put into incorporating more powerful decoding algorithms such as Integer"
2020.coling-main.100,2020.osact-1.2,0,0.0469019,"Missing"
2020.coling-main.100,D19-1539,0,0.021762,"r experimental results on the GLUE data set show an advantage in model performance by maximizing the approximate knowledge gain of the model when querying from the pool of unlabeled data. Finally, we demonstrate and analyze the benefits of freezing layers of the language model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings. 1 Introduction Pre-trained language models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019). These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence (Radford, 2018). Then, transfer learning (Pan and Yang, 2010; Ruder et al., 2019; Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al., 2019; Sun et al., 2019b). Devlin et al. (2019) introduced the “Bidirectional Encoder Representations from Transformers” (BERT), a pre-trained language model based on the Trans"
2020.coling-main.100,P19-1279,0,0.0277269,"a masked language model objective where the goal is to predict randomly masked words from their context (Taylor, 1953). The fact is, BERT has achieved state of the art results on the “General Language Understanding Evaluation” (GLUE) benchmark (Wang et al., 2018) by only training a single, task-specific layer at the output and fine-tuning the base model for each task. Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis (Sun et al., 2019a; Xu et al., 2019; Li et al., 2019), relation extraction (Baldini Soares et al., 2019; Huang et al., 2019b) and word sense disambiguation (Huang et al., 2019a; Hadiwinoto et al., 2019; Huang et al., 2019a), as well as its adaptability to languages other than English (Martin et al., 2020; Antoun et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1158 Proceedings of the 28th International Conference on Computational Linguistics, pages 1158–1171 Barcelona, Spain (Online), December 8-13, 2020 2020; Agerri et al., 2020). However, the fine-tuning data set often contains thousand"
2020.coling-main.100,N04-4028,0,0.0117152,"g for NLP tasks using deep neural networks. Zhang et al. (2017) explored pool-based active learning for text classification using a model similar to our setting. However, they used word-level embeddings (Mikolov et al., 2013) and focus on representation learning, querying pool points that are expected to maximize the gradient changes in the embedding layer. Shen et al. (2017) used active learning for named entity recognition tasks. They proposed a acquisition strategy named Maximum Normalized Log-Probability which is a normalized form of the Constrained Forward-Backward confidence estimation (Culotta and McCallum, 2004; Culotta and McCallum, 2005). Using this strategy, they achieved on-par performance in comparison to a model using the BALD acquisition function and MC Dropout without needing multiple forward passes. However, this approach is not suitable for any arbitrary model architecture but requires conditional random fields (CRFs) for the approximation of model uncertainty. 7 Conclusion In this paper, we evaluated the performance of a pre-trained Transformer model - BERT - in an active learning scenario for text classification in low-resource settings. We showed that using Monte-Carlo Dropout in the cl"
2020.coling-main.100,N19-1423,0,0.460839,"e natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019). These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence (Radford, 2018). Then, transfer learning (Pan and Yang, 2010; Ruder et al., 2019; Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al., 2019; Sun et al., 2019b). Devlin et al. (2019) introduced the “Bidirectional Encoder Representations from Transformers” (BERT), a pre-trained language model based on the Transformer architecture (Vaswani et al., 2017). BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context (Taylor, 1953). The fact is, BERT has achieved state of the art results on the “General Language Understanding Evaluation” (GLUE) benchmark (Wang et al., 2018) by only training a single, task-specific layer at the output and fine-tu"
2020.coling-main.100,P17-2093,0,0.0202983,"ges other than English (Martin et al., 2020; Antoun et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1158 Proceedings of the 28th International Conference on Computational Linguistics, pages 1158–1171 Barcelona, Spain (Online), December 8-13, 2020 2020; Agerri et al., 2020). However, the fine-tuning data set often contains thousands of labeled data points. This plethora of training data is often not available in real world scenarios (Tan and Zhang, 2008; Wan, 2008; Salameh et al., 2015; Fang and Cohn, 2017). In this paper, we focus on the low-resource setting with less than 1,000 training data points. Our research attempts to answer the question if pool-based active learning can be used to increase the performance of a text classifier based on a Transformer architecture such as BERT. That leads to the next question: How can layer freezing techniques (Yosinski et al., 2014; Howard and Ruder, 2018; Peters et al., 2019), i.e. reducing the parameter space, impact model training convergence with fewer data points? To answer these questions, we explore the use of recently introduced Bayesian approxima"
2020.coling-main.100,D19-1533,0,0.0231679,"r context (Taylor, 1953). The fact is, BERT has achieved state of the art results on the “General Language Understanding Evaluation” (GLUE) benchmark (Wang et al., 2018) by only training a single, task-specific layer at the output and fine-tuning the base model for each task. Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis (Sun et al., 2019a; Xu et al., 2019; Li et al., 2019), relation extraction (Baldini Soares et al., 2019; Huang et al., 2019b) and word sense disambiguation (Huang et al., 2019a; Hadiwinoto et al., 2019; Huang et al., 2019a), as well as its adaptability to languages other than English (Martin et al., 2020; Antoun et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1158 Proceedings of the 28th International Conference on Computational Linguistics, pages 1158–1171 Barcelona, Spain (Online), December 8-13, 2020 2020; Agerri et al., 2020). However, the fine-tuning data set often contains thousands of labeled data points. This plethora of training data is often not available in real world scen"
2020.coling-main.100,P18-1031,0,0.358212,"ng new data constant. Our experimental results on the GLUE data set show an advantage in model performance by maximizing the approximate knowledge gain of the model when querying from the pool of unlabeled data. Finally, we demonstrate and analyze the benefits of freezing layers of the language model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings. 1 Introduction Pre-trained language models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019). These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence (Radford, 2018). Then, transfer learning (Pan and Yang, 2010; Ruder et al., 2019; Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al., 2019; Sun et al., 2019b). Devlin et al. (2019) introduced the “Bidirectional Encoder Representations from Transformers” (BERT), a pre-trained language mo"
2020.coling-main.100,D19-1355,0,0.0228213,"el objective where the goal is to predict randomly masked words from their context (Taylor, 1953). The fact is, BERT has achieved state of the art results on the “General Language Understanding Evaluation” (GLUE) benchmark (Wang et al., 2018) by only training a single, task-specific layer at the output and fine-tuning the base model for each task. Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis (Sun et al., 2019a; Xu et al., 2019; Li et al., 2019), relation extraction (Baldini Soares et al., 2019; Huang et al., 2019b) and word sense disambiguation (Huang et al., 2019a; Hadiwinoto et al., 2019; Huang et al., 2019a), as well as its adaptability to languages other than English (Martin et al., 2020; Antoun et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1158 Proceedings of the 28th International Conference on Computational Linguistics, pages 1158–1171 Barcelona, Spain (Online), December 8-13, 2020 2020; Agerri et al., 2020). However, the fine-tuning data set often contains thousands of labeled data po"
2020.coling-main.100,D14-1181,0,0.00475794,"Embeddings CNN Decoder BERT Transformer At the ... Tokens SentencePiece Encoder end of FFNN Decoder Rue ... ... ... ... Dropout operation at the output of the layer Position Embeddings MC-Dropout [cls] normal Dropout Layer 1 Layer 2 Layer 3 Layer 12 Figure 1: High-level overview of the architecture used in our experiments including a depiction of the simple FFNN decoder used by Devlin et al. (2019) highlighting where our model differs from the original experiment setup. 1159 In contrast, we use a more complex classification architecture based on a convolutional neural network (CNN) following Kim (2014)1 . All hidden states in the last layer of the BERT model are arranged in a 2-dimensional matrix. Then, convolutional filters of height (3, 4, 5) and length corresponding to the hidden state size (786) are shifted over the input to calculate 64 1-dimensional feature maps per filter size. These feature maps are then batch-normalized (Ioffe and Szegedy, 2015), dropout regularized and global max-pooled before they are concatenated and fed into 2 fully-connected layers, each of which applies another dropout operation on its input 2 . 2.2 How to Select Data When labeled training data is sparse, but"
2020.coling-main.100,W18-6325,0,0.0150329,"more likely to be sampled during the acquisition phase. The model at this point in training thus seems to already have learned to perform its task confidently on the simpler examples and can thus concentrate more on the non-trivial data-points. 6 Related Work Low-resource NLP Previous work in low-resource NLP tasks includes feature-engineering (Tan and Zhang, 2008) which requires a recurring effort when adapting to a new data set. Another approach is to transfer knowledge across domains to increase the amount of data that is available for training (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Yang et al., 2017; Gupta et al., 2018). One of these approaches relied on adversarial training (Goodfellow et al., 2014) to learn a domain adaptive classifier (Ganin et al., 2016) in another domain or language where training data was plentiful while ensuring that the model generalizes to the low-resource domain (Chen et al., 2018). However, these approaches have not used a pre-trained generic language model, but perform pre-training for each task individually. Adapting pre-trained models The effectiveness of transfer learning in low-resource settings was previously demonstrated for machine t"
2020.coling-main.100,D19-5505,0,0.0190194,"s pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context (Taylor, 1953). The fact is, BERT has achieved state of the art results on the “General Language Understanding Evaluation” (GLUE) benchmark (Wang et al., 2018) by only training a single, task-specific layer at the output and fine-tuning the base model for each task. Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis (Sun et al., 2019a; Xu et al., 2019; Li et al., 2019), relation extraction (Baldini Soares et al., 2019; Huang et al., 2019b) and word sense disambiguation (Huang et al., 2019a; Hadiwinoto et al., 2019; Huang et al., 2019a), as well as its adaptability to languages other than English (Martin et al., 2020; Antoun et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1158 Proceedings of the 28th International Conference on Computational Linguistics, pages 1158–1171 Barcelona, Spain (Online), December 8-13, 2020 2020; Agerri et al., 2020). However"
2020.coling-main.100,2020.lrec-1.172,0,0.0208012,"age model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings. 1 Introduction Pre-trained language models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019). These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence (Radford, 2018). Then, transfer learning (Pan and Yang, 2010; Ruder et al., 2019; Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al., 2019; Sun et al., 2019b). Devlin et al. (2019) introduced the “Bidirectional Encoder Representations from Transformers” (BERT), a pre-trained language model based on the Transformer architecture (Vaswani et al., 2017). BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context (Taylor, 1953). The fact is,"
2020.coling-main.100,I17-2050,0,0.0196237,"r BALD score and thus are more likely to be sampled during the acquisition phase. The model at this point in training thus seems to already have learned to perform its task confidently on the simpler examples and can thus concentrate more on the non-trivial data-points. 6 Related Work Low-resource NLP Previous work in low-resource NLP tasks includes feature-engineering (Tan and Zhang, 2008) which requires a recurring effort when adapting to a new data set. Another approach is to transfer knowledge across domains to increase the amount of data that is available for training (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Yang et al., 2017; Gupta et al., 2018). One of these approaches relied on adversarial training (Goodfellow et al., 2014) to learn a domain adaptive classifier (Ganin et al., 2016) in another domain or language where training data was plentiful while ensuring that the model generalizes to the low-resource domain (Chen et al., 2018). However, these approaches have not used a pre-trained generic language model, but perform pre-training for each task individually. Adapting pre-trained models The effectiveness of transfer learning in low-resource settings was previously dem"
2020.coling-main.100,W19-4302,0,0.187784,"often contains thousands of labeled data points. This plethora of training data is often not available in real world scenarios (Tan and Zhang, 2008; Wan, 2008; Salameh et al., 2015; Fang and Cohn, 2017). In this paper, we focus on the low-resource setting with less than 1,000 training data points. Our research attempts to answer the question if pool-based active learning can be used to increase the performance of a text classifier based on a Transformer architecture such as BERT. That leads to the next question: How can layer freezing techniques (Yosinski et al., 2014; Howard and Ruder, 2018; Peters et al., 2019), i.e. reducing the parameter space, impact model training convergence with fewer data points? To answer these questions, we explore the use of recently introduced Bayesian approximations of model uncertainty (Gal and Ghahramani, 2016) for data selection that potentially leads to faster convergence during fine-tuning by only introducing new data points that maximize the knowledge gain of the model. To the best of our knowledge, the work presented in this paper is the first demonstration of combining modern transfer learning using pre-trained Transformer-based language model such as the BERT mo"
2020.coling-main.100,P19-1054,0,0.0248896,"models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019). These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence (Radford, 2018). Then, transfer learning (Pan and Yang, 2010; Ruder et al., 2019; Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al., 2019; Sun et al., 2019b). Devlin et al. (2019) introduced the “Bidirectional Encoder Representations from Transformers” (BERT), a pre-trained language model based on the Transformer architecture (Vaswani et al., 2017). BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context (Taylor, 1953). The fact is, BERT has achieved state of the art results on the “General Language Understanding Evaluation” (GLUE) benchmark (Wang et al., 2018) by only training a single, tas"
2020.coling-main.100,N19-5004,0,0.0237217,"layers of the language model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings. 1 Introduction Pre-trained language models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019). These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence (Radford, 2018). Then, transfer learning (Pan and Yang, 2010; Ruder et al., 2019; Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al., 2019; Sun et al., 2019b). Devlin et al. (2019) introduced the “Bidirectional Encoder Representations from Transformers” (BERT), a pre-trained language model based on the Transformer architecture (Vaswani et al., 2017). BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context (Taylor"
2020.coling-main.100,N15-1078,0,0.0119607,"adaptability to languages other than English (Martin et al., 2020; Antoun et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1158 Proceedings of the 28th International Conference on Computational Linguistics, pages 1158–1171 Barcelona, Spain (Online), December 8-13, 2020 2020; Agerri et al., 2020). However, the fine-tuning data set often contains thousands of labeled data points. This plethora of training data is often not available in real world scenarios (Tan and Zhang, 2008; Wan, 2008; Salameh et al., 2015; Fang and Cohn, 2017). In this paper, we focus on the low-resource setting with less than 1,000 training data points. Our research attempts to answer the question if pool-based active learning can be used to increase the performance of a text classifier based on a Transformer architecture such as BERT. That leads to the next question: How can layer freezing techniques (Yosinski et al., 2014; Howard and Ruder, 2018; Peters et al., 2019), i.e. reducing the parameter space, impact model training convergence with fewer data points? To answer these questions, we explore the use of recently introdu"
2020.coling-main.100,W17-2630,0,0.02277,"contain more transferable features. However, none of the work has considered the training set size as a dependent parameter as our experiments presented in this paper. Active learning in NLP There is some prior work regarding active learning for NLP tasks using deep neural networks. Zhang et al. (2017) explored pool-based active learning for text classification using a model similar to our setting. However, they used word-level embeddings (Mikolov et al., 2013) and focus on representation learning, querying pool points that are expected to maximize the gradient changes in the embedding layer. Shen et al. (2017) used active learning for named entity recognition tasks. They proposed a acquisition strategy named Maximum Normalized Log-Probability which is a normalized form of the Constrained Forward-Backward confidence estimation (Culotta and McCallum, 2004; Culotta and McCallum, 2005). Using this strategy, they achieved on-par performance in comparison to a model using the BALD acquisition function and MC Dropout without needing multiple forward passes. However, this approach is not suitable for any arbitrary model architecture but requires conditional random fields (CRFs) for the approximation of mod"
2020.coling-main.100,N19-1035,0,0.0205704,"great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015; Radford, 2018; Howard and Ruder, 2018; Baevski et al., 2019; Dong et al., 2019). These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence (Radford, 2018). Then, transfer learning (Pan and Yang, 2010; Ruder et al., 2019; Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006; Aggarwal and Zhai, 2012; Reimers et al., 2019; Sun et al., 2019b). Devlin et al. (2019) introduced the “Bidirectional Encoder Representations from Transformers” (BERT), a pre-trained language model based on the Transformer architecture (Vaswani et al., 2017). BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context (Taylor, 1953). The fact is, BERT has achieved state of the art results on the “General Language Understanding Evaluation” (GLUE) benchmark (Wang et al., 2018) by only training a single, task-specific layer a"
2020.coling-main.100,D08-1058,0,0.0592978,"ell as its adaptability to languages other than English (Martin et al., 2020; Antoun et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1158 Proceedings of the 28th International Conference on Computational Linguistics, pages 1158–1171 Barcelona, Spain (Online), December 8-13, 2020 2020; Agerri et al., 2020). However, the fine-tuning data set often contains thousands of labeled data points. This plethora of training data is often not available in real world scenarios (Tan and Zhang, 2008; Wan, 2008; Salameh et al., 2015; Fang and Cohn, 2017). In this paper, we focus on the low-resource setting with less than 1,000 training data points. Our research attempts to answer the question if pool-based active learning can be used to increase the performance of a text classifier based on a Transformer architecture such as BERT. That leads to the next question: How can layer freezing techniques (Yosinski et al., 2014; Howard and Ruder, 2018; Peters et al., 2019), i.e. reducing the parameter space, impact model training convergence with fewer data points? To answer these questions, we explore the u"
2020.coling-main.100,W18-5446,0,0.215212,"06; Aggarwal and Zhai, 2012; Reimers et al., 2019; Sun et al., 2019b). Devlin et al. (2019) introduced the “Bidirectional Encoder Representations from Transformers” (BERT), a pre-trained language model based on the Transformer architecture (Vaswani et al., 2017). BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context (Taylor, 1953). The fact is, BERT has achieved state of the art results on the “General Language Understanding Evaluation” (GLUE) benchmark (Wang et al., 2018) by only training a single, task-specific layer at the output and fine-tuning the base model for each task. Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis (Sun et al., 2019a; Xu et al., 2019; Li et al., 2019), relation extraction (Baldini Soares et al., 2019; Huang et al., 2019b) and word sense disambiguation (Huang et al., 2019a; Hadiwinoto et al., 2019; Huang et al., 2019a), as well as its adaptability to languages other than English (Martin et al., 2020; Antoun et al., This work is licensed und"
2020.coling-main.100,N19-1242,0,0.0223847,"nal model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context (Taylor, 1953). The fact is, BERT has achieved state of the art results on the “General Language Understanding Evaluation” (GLUE) benchmark (Wang et al., 2018) by only training a single, task-specific layer at the output and fine-tuning the base model for each task. Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis (Sun et al., 2019a; Xu et al., 2019; Li et al., 2019), relation extraction (Baldini Soares et al., 2019; Huang et al., 2019b) and word sense disambiguation (Huang et al., 2019a; Hadiwinoto et al., 2019; Huang et al., 2019a), as well as its adaptability to languages other than English (Martin et al., 2020; Antoun et al., This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1158 Proceedings of the 28th International Conference on Computational Linguistics, pages 1158–1171 Barcelona, Spain (Online), December 8-13, 2020 2020; Agerri et a"
2020.coling-main.100,D16-1163,0,0.0287108,"ng get a much higher BALD score and thus are more likely to be sampled during the acquisition phase. The model at this point in training thus seems to already have learned to perform its task confidently on the simpler examples and can thus concentrate more on the non-trivial data-points. 6 Related Work Low-resource NLP Previous work in low-resource NLP tasks includes feature-engineering (Tan and Zhang, 2008) which requires a recurring effort when adapting to a new data set. Another approach is to transfer knowledge across domains to increase the amount of data that is available for training (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018; Yang et al., 2017; Gupta et al., 2018). One of these approaches relied on adversarial training (Goodfellow et al., 2014) to learn a domain adaptive classifier (Ganin et al., 2016) in another domain or language where training data was plentiful while ensuring that the model generalizes to the low-resource domain (Chen et al., 2018). However, these approaches have not used a pre-trained generic language model, but perform pre-training for each task individually. Adapting pre-trained models The effectiveness of transfer learning in low-resource se"
2020.conll-1.2,D19-1300,0,0.0249883,"isms can significantly improve performance for different NLP tasks, performance degrades when models are exposed to inherent properties of natural language, such as semantic ambiguity, inferring information, or out of domain data (Blohm et al., 2018; Niven and Kao, 2019). These findings encourage work towards enhancing network’s generalizability, deterring reliance on the closed-world assumption (Reiter, 1981). In machine reading comprehension (MRC), it has been proposed that the more similar systems are to human behavior, the more suitable they become for such a task (Trischler et al., 2017; Luo et al., 2019; Zheng et al., 2019). As a result, much recent work aims to build machines which read and understand text, mimicking specific aspects of human behavior (Hermann et al., 2015; Nguyen et al., 2016; Rajpurkar et al., 2016; While neural networks with attention mechanisms have achieved superior performance on many natural language processing tasks, it remains unclear to which extent learned attention resembles human visual attention. In this paper, we propose a new method that leverages eye-tracking data to investigate the relationship between human visual attention and neural attention in machine"
2020.conll-1.2,D15-1166,0,0.0468152,"r text have not been explored so far. When the goal is to purely improve performance, several papers proposed integrating gaze data into neural attention as an additional variable in the equation or as a regularization method (Sugano and Bulling, 2016; Barrett et al., 2018; Qiao et al., 2018; Sood et al., 2020). Attention Mechanisms In the attention-based encoder-decoder architecture, rather than ignoring the internal encoder states, the attention mechanism takes advantage of these weights to generate a context vector, which is used by the decoder at various time steps (Bahdanau et al., 2014; Luong et al., 2015; Chorowski et al., 2015; Wang and Jiang; Yang et al., 2016; Dzendzik et al., 2017). In Transformer networks, the main differences to previous attentive models are that these networks are purely based on attention where LSTM or GRU units are not used, and attention is applied via selfattention and multi-headed attention (Vaswani et al., 2017) without any order constraint. Since the introduction of pre-trained Transformer networks, we have observed, on the one hand, a rise in state of the art performance across a multitude of tasks in NLP (Devlin et al., 2019; Radford et al., 2018; Yang et al.,"
2020.conll-1.2,D12-1071,0,0.0254752,"del versus human visual attention in comprehension tasks. To the best of our knowledge there are no available eye tracking datasets which use machine learning corpora as stimuli. Therefore, we build and use our reading comprehension gaze dataset as the gold standard. In addition, we provide coreference chains labeled by two human annotators2 . Based on the lower fixation durations observed in the eye tracking data, we find that humans can easily resolve pronouns in the MQA-RC dataset (cf. Figure 6), where fixation durations are used to measure information processing load (Arnold et al., 2000; Rahman and Ng, 2012; Cinkara and Cabaro˘glu, 2015). The figure also shows saliency over the proper nouns compared to their mentions in the chains. Study 2 We conducted a follow up study in which we took only the plots for which the majority of CNN and LSTM models predicted correctly. We hypothesized that such items that are, on average, easier for the models are also easier for the humans (higher correlation score). In this study, we only collected data for the regular QA task (condition 1). The experiment was performed by five new participants. Each participant saw all the 16 plots in a randomized order. 3 Data"
2020.conll-1.2,P19-1459,0,0.0267872,"ion in an image or a word in a sentence (Frintrop et al., 2010). Attention mechanisms have recently gained significant popularity and have boosted performance in natural language processing tasks and computer vision (Ma and Zhang, 2003; Sun and Fisher, 2003; Seo et al., 2016; Veliˇckovi´c et al., 2017; Sood et al., 2020). Although attention mechanisms can significantly improve performance for different NLP tasks, performance degrades when models are exposed to inherent properties of natural language, such as semantic ambiguity, inferring information, or out of domain data (Blohm et al., 2018; Niven and Kao, 2019). These findings encourage work towards enhancing network’s generalizability, deterring reliance on the closed-world assumption (Reiter, 1981). In machine reading comprehension (MRC), it has been proposed that the more similar systems are to human behavior, the more suitable they become for such a task (Trischler et al., 2017; Luo et al., 2019; Zheng et al., 2019). As a result, much recent work aims to build machines which read and understand text, mimicking specific aspects of human behavior (Hermann et al., 2015; Nguyen et al., 2016; Rajpurkar et al., 2016; While neural networks with attenti"
2020.conll-1.2,N16-3020,0,0.0331272,"stems, indicating that models might process text in a different manner than humans: they rely on pattern matching in lieu of human-like decision making processes which are required in comprehension tasks (Just and Carpenter, 1980; Posner et al., 1980; Blohm et al., 2018). 2.3.1 Neural Interpretability To shed light on the decisions taken by these networks, multiple interpretability studies have investigated their outputs and predictions (AlvarezMelis and Jaakkola, 2018; Blohm et al., 2018; Gilpin et al., 2018), and analyzed their behavior through loss visualization from various architectures (Ribeiro et al., 2016). Nevertheless, a real understanding of the internal processes of these black boxes is still rather limited (Gilpin et al., 2018). Although these interpretations might explain predictions, there is still a lack of explanation regarding the mechanisms by which models work as well as limited insight regarding the relationship between machine and human visual attention (Lipton, 2018). Eye Tracking and Neural Networks In the past years, researchers have started leveraging human gaze data for attentive neural modeling 14 3 3.1 Resources questions; in Study 2 we selected a different set of 16 docume"
2020.conll-1.2,W12-4501,0,0.0463778,"Missing"
2020.conll-1.2,2020.acl-main.419,0,0.121445,"t al., 2018; Yang et al., 2019). On the other hand, much effort is needed to interpret these highly complex models (e.g. in Vig and Belinkov (2019)). 2.3 2.4 In order to further understand the behavior of neural networks, research in neural interpretability has grown dramatically in the recent years (Lipton, 2018; Gilpin et al., 2018; Hooker et al., 2019). Such methods include: introducing adversarial examples, error class analysis, modeling techniques (e.g. self-explaining networks), and post-hoc analysis of attention distributions (Lipton, 2018; AlvarezMelis and Jaakkola, 2018; Rudin, 2019; Sen et al., 2020). Question Answering and Machine Comprehension We use question answering (QA) tasks to compare human and machine attention. Although such tasks have been widely explored with neural attention models, creating systems to comprehend semantically diverse text documents and answer related questions remains challenging (Qiu et al., 2019). These models tend to fail when faced with adversarial attacks: the type of noise humans can easily resolve (Jia and Liang, 2017; Blohm et al., 2018; Yuan et al., 2019). These studies uncovered the limitations of QA systems, indicating that models might process tex"
2020.conll-1.2,W19-4808,0,0.0173307,"networks, the main differences to previous attentive models are that these networks are purely based on attention where LSTM or GRU units are not used, and attention is applied via selfattention and multi-headed attention (Vaswani et al., 2017) without any order constraint. Since the introduction of pre-trained Transformer networks, we have observed, on the one hand, a rise in state of the art performance across a multitude of tasks in NLP (Devlin et al., 2019; Radford et al., 2018; Yang et al., 2019). On the other hand, much effort is needed to interpret these highly complex models (e.g. in Vig and Belinkov (2019)). 2.3 2.4 In order to further understand the behavior of neural networks, research in neural interpretability has grown dramatically in the recent years (Lipton, 2018; Gilpin et al., 2018; Hooker et al., 2019). Such methods include: introducing adversarial examples, error class analysis, modeling techniques (e.g. self-explaining networks), and post-hoc analysis of attention distributions (Lipton, 2018; AlvarezMelis and Jaakkola, 2018; Rudin, 2019; Sen et al., 2020). Question Answering and Machine Comprehension We use question answering (QA) tasks to compare human and machine attention. Althou"
2020.conll-1.2,D19-1002,0,0.0320712,"istics https://doi.org/10.18653/v1/P17 cnn When the band looks to Rick, he nods his Laszlo orders the house band to defiantly play &quot;La Marseillaise&quot;. 0.05 0.04 sualization tool to qualitatively compare the differences in attentive behaviors between neural models and humans by showing their patterns over time in a split screen mode. Second, as widely suggested in the cognitive science literature, we quantify human attention in terms of the word-level gaze duration recorded in our eye tracking dataset (Rouse and Morris, 1986; Milosavljevic and Cerf, 2008; Van Hooft and Born, 2012; Lipton, 2018; Wiegreffe and Pinter, 2019). Third, we interpret the relationship between human attention and three state of the art systems based on CNN, LSTM, and XLNet (Hochreiter and Schmidhuber, 1997; Yang et al., 2019) using Kullback-Leibler divergence (Kullback and Leibler, 1951). By doing so, we are able to compare, evaluate and better understand neural attention distributions on text across these attention models. To the best of our knowledge, we are the first to propose a systematic approach for comparing neural attention to human gaze data in machine reading comprehension. The main findings of our work are two-fold: First, w"
2020.conll-1.2,N16-1174,0,0.058957,"rely improve performance, several papers proposed integrating gaze data into neural attention as an additional variable in the equation or as a regularization method (Sugano and Bulling, 2016; Barrett et al., 2018; Qiao et al., 2018; Sood et al., 2020). Attention Mechanisms In the attention-based encoder-decoder architecture, rather than ignoring the internal encoder states, the attention mechanism takes advantage of these weights to generate a context vector, which is used by the decoder at various time steps (Bahdanau et al., 2014; Luong et al., 2015; Chorowski et al., 2015; Wang and Jiang; Yang et al., 2016; Dzendzik et al., 2017). In Transformer networks, the main differences to previous attentive models are that these networks are purely based on attention where LSTM or GRU units are not used, and attention is applied via selfattention and multi-headed attention (Vaswani et al., 2017) without any order constraint. Since the introduction of pre-trained Transformer networks, we have observed, on the one hand, a rise in state of the art performance across a multitude of tasks in NLP (Devlin et al., 2019; Radford et al., 2018; Yang et al., 2019). On the other hand, much effort is needed to interpr"
2020.conll-1.2,W17-2623,0,0.0316627,"lthough attention mechanisms can significantly improve performance for different NLP tasks, performance degrades when models are exposed to inherent properties of natural language, such as semantic ambiguity, inferring information, or out of domain data (Blohm et al., 2018; Niven and Kao, 2019). These findings encourage work towards enhancing network’s generalizability, deterring reliance on the closed-world assumption (Reiter, 1981). In machine reading comprehension (MRC), it has been proposed that the more similar systems are to human behavior, the more suitable they become for such a task (Trischler et al., 2017; Luo et al., 2019; Zheng et al., 2019). As a result, much recent work aims to build machines which read and understand text, mimicking specific aspects of human behavior (Hermann et al., 2015; Nguyen et al., 2016; Rajpurkar et al., 2016; While neural networks with attention mechanisms have achieved superior performance on many natural language processing tasks, it remains unclear to which extent learned attention resembles human visual attention. In this paper, we propose a new method that leverages eye-tracking data to investigate the relationship between human visual attention and neural at"
2020.lrec-1.489,E17-2052,0,0.0180393,"ack of corpora, whose collection can be challenging (C¸etino˘glu et al., 2016). It is unsurprising that the available corpora are scarce. In this section, we present an overview of corpora similar to ours, i.e. annotated CS corpora, then give a brief overview of other ones that are related to ours only in that they involve the Arabic language. 2.1. Morpho-Syntactically Annotated CS Corpora A number of researchers have worked to provide morphosyntactically annotated CS corpora, including treebanks. In all the cases we are aware of, the raw data source is either social media (C¸etino˘glu, 2016; Bhat et al., 2017) or transcribed conversations (C¸etino˘glu, 2017). The corpus 3973 we present here is no exception, since it is also based on transcribed speech data. 2 An early effort was the annotation of an English-Spanish corpus with part-of-speech tags (Solorio and Liu, 2008). This was before the popularity of the Universal Dependencies (UD) scheme (Nivre et al., 2016). Hence, for each of the languages, a different tagset was used. More recent efforts have made use of the unified UD tagset and unified guidelines for morpho-syntactic annotation, in order to make available CS corpora in which both language"
2020.lrec-1.489,N18-1090,0,0.0168606,"vre et al., 2016). Hence, for each of the languages, a different tagset was used. More recent efforts have made use of the unified UD tagset and unified guidelines for morpho-syntactic annotation, in order to make available CS corpora in which both languages use the same annotation scheme, making it easier for subsequent research (e.g. in linguistics) to extract insights from the data, but also for NLP researchers to make use of monolingual UD resources. This is evidenced in the Turkish-German CS corpus (C ¸ etino˘glu and C ¸ o¨ ltekin, 2016) and the Hindi-English treebank (Bhat et al., 2017; Bhat et al., 2018). Although this is not an extensive overview, the number of morpho-syntactically annotated CS corpora is still very limited, and so is the number of language pairs that are represented in them. Our corpus is therefore a valuable addition, especially as it provides data for a previously unrepresented language pair. 2.2. Arabic CS Corpora Recently, more attention has been given by researchers towards the CS phenomenon. Text corpora have been collected (Mustafa and Suleman, 2011; Hamed et al., 2017) by crawling documents and web pages related to computers and computer science. Some corpora have a"
2020.lrec-1.489,L16-1667,1,0.902845,"Missing"
2020.lrec-1.489,W16-1714,1,0.905776,"Missing"
2020.lrec-1.489,W19-7809,1,0.845275,"Missing"
2020.lrec-1.489,W16-5801,1,0.901413,"Missing"
2020.lrec-1.489,W17-0804,1,0.87119,"Missing"
2020.lrec-1.489,C12-2029,0,0.14823,"really nice. Q J» IÒÊª I learnt a lot.) CS is a phenomenon commonly observed in the Arabicspeaking world. Arabic itself encompasses many varieties, most prominently modern standard Arabic (MSA) and dialectal Arabic (DA). The former is used in formal contexts such as formal writings and speeches, while the latter is the language used in everyday conversations as well as informal writings such as in social media. DA itself encompasses a number of dialects, of which the most widely used (by number of speakers) is Egyptian Arabic. Arabic speakers typically mix MSA and DA in the same utterances (Elfardy and Diab, 2012), sometimes in addition to English, French or both. Colonization and international business and education have played a major role in introducing the English and French languages into everyday conversations. Despite Arabic being one of the most widely-used languages, there is still a huge gap in the available resources and Natural Language Processing (NLP) applications. Most of the work has focused on MSA, with comparatively less work on DA, and even less on code-switched DA. In this paper, we aim at providing a multi-lingual corpus1 for code-switched Egyptian Arabic-English that can 1 The cor"
2020.lrec-1.489,habash-etal-2012-conventional,0,0.0363566,"to lack of resources, no external annotators are recruited for this phase. All the annotations are done in the CONLL-U format6 . 4. 4.1. Guidelines Orthography Unlike MSA, DA orthography is not standardized, which poses a challenge to NLP tasks, as the same word is represented differently across various texts. Our first goal is to tackle this problem by presenting guidelines for Egyptian Arabic writing. We abide by these writings when obtaining corpus transcriptions to insure standardization throughout the text. There have been attempts to standardize DA orthography, most notably the work of Habash et al. (2012), 2 https://arz.wikipedia.org/wiki/ AK YJ .J ºK ð:Introduction in English which attempts to remain as close to MSA orthography as #Rules of writing 3 Arabic written in the Latin script 4 Algerian, Moroccan and Tunisian 5 6 3974 Not using the UD tagset. https://universaldependencies.org/format.html possible, and diverges only when needed. However, we decided to base our orthography guidelines on the ones developed and used by the Egyptian Arabic Wikipedia community 7 . There are two main reasons for preferring the Wikipedia guidelines over guidelines used in previous academic studies. First, w"
2020.lrec-1.489,L18-1601,1,0.723886,"Missing"
2020.lrec-1.489,L16-1262,0,0.0777607,"Missing"
2020.lrec-1.489,E14-1078,0,0.0197147,"endent from each other. They mix them in sometimes rather unexpected ways. This brings us to the third and last operation, namely disambiguation. How can we deal with a code-switched word or phrase which can be interpreted differently according to which grammar we use to explain it. For instance should we consider È@text as one token (an English noun in Arabic definite state) or as two tokens, a definite article followed by a noun? Without further evidence, it appears to the authors that in fact both explanations are plausible. Although there exist frameworks that can handle such uncertainty (Plank et al., 2014), it is conventional for treebanks that are encoded in CONLL-U format to list only one annotation for each sentence. In this case, as we observe that it is more often the case that Arabic grammar is thrust upon English words than vice versa, we decide to consider the Arabic interpretation the ”correct” one. 5. English 62 2752 51 Arabic 303 8097 1109 CS 788 437 62 total 1153 11286 1222 Table 1: Overview of the number of sentences and tokens. POS NOUN VERB PROPN ADJ English 1073 197 111 231 Arabic 853 1251 79 232 CS 335 74 16 12 Table 2: Breakdown of POS counts by language. Only POS tags for whi"
2020.lrec-1.489,L16-1658,0,0.014984,"dition, especially as it provides data for a previously unrepresented language pair. 2.2. Arabic CS Corpora Recently, more attention has been given by researchers towards the CS phenomenon. Text corpora have been collected (Mustafa and Suleman, 2011; Hamed et al., 2017) by crawling documents and web pages related to computers and computer science. Some corpora have also covered the Algerian Arabic-French language. Cotterell et al. (2014) present a text corpus of 339,504 comments with romanized Arabic3 collected from news story of an Algerian newspaper and annotated for word-level language ID. Samih and Maier (2016) also provided a text corpus for the Arabic-Moroccan Darija language. The corpus contains 233,000 token crawled from internet discussion forums and blogs. Sabty et al. (2019) have collected a text corpus of 1,331 sentences from speech transcriptions (Hamed et al., 2018) and Twitter and annotated them with named-entities. Abainia (2019) has provided an Algerian Arabic-French parallel corpus containing 2, 400 Facebook comments annotated for gender, regions and cities, named entities, emotion and level of abuse. Amazouz et al. (2018) collected the FACST Algerian Arabic-French speech corpus having"
2020.lrec-1.489,D08-1110,0,0.021664,"f other ones that are related to ours only in that they involve the Arabic language. 2.1. Morpho-Syntactically Annotated CS Corpora A number of researchers have worked to provide morphosyntactically annotated CS corpora, including treebanks. In all the cases we are aware of, the raw data source is either social media (C¸etino˘glu, 2016; Bhat et al., 2017) or transcribed conversations (C¸etino˘glu, 2017). The corpus 3973 we present here is no exception, since it is also based on transcribed speech data. 2 An early effort was the annotation of an English-Spanish corpus with part-of-speech tags (Solorio and Liu, 2008). This was before the popularity of the Universal Dependencies (UD) scheme (Nivre et al., 2016). Hence, for each of the languages, a different tagset was used. More recent efforts have made use of the unified UD tagset and unified guidelines for morpho-syntactic annotation, in order to make available CS corpora in which both languages use the same annotation scheme, making it easier for subsequent research (e.g. in linguistics) to extract insights from the data, but also for NLP researchers to make use of monolingual UD resources. This is evidenced in the Turkish-German CS corpus (C ¸ etino˘gl"
2020.lrec-1.489,W17-1320,0,0.024333,"To clarify this, consider the following examples that are the same in MSA and DA: For the different annotation layers, we base our guidelines on the Universal Dependencies (UD) scheme. We are confronted with two challenges, which we have to resolve in order to develop coherent guidelines in the spirit of the UD community. The first challenge is the fact that, unlike similar codeswitch corpora for which UD annotated corpora existed for the individual languages, there are no UD-annotated treebanks for Egyptian Arabic at the time of this writing. There exist, however, multiple treebanks for MSA (Taji et al., 2017; Hajic et al., 2004). We have therefore to adapt the annotation principles used for MSA to our concrete use case, namely Egyptian Arabic. Just as for orthography, we diverge a lot from MSA in order to reflect the true morpho-syntactic properties of Egyptian Arabic. For instance, consider the annotation of the first participle (called in Arabic É«A®Ë@ Õæ @) in both varieties. In MSA corpora, this has always to be considered a noun or an adjective. However, we observe it also used as a verb in Egyptian Arabic, that can take direct, indirect or prepositional objects: • •  K. (a big house, liter"
2020.lrec-1.523,W16-5801,1,0.90354,"Missing"
2020.lrec-1.523,W17-0804,0,0.0344117,"Missing"
2020.lrec-1.523,O09-5003,0,0.0458517,"mazouz et al., 2018): 7.5 hours of read speech from books and movie transcripts, as well as informal conversations, gathered from 20 speakers. The corpus contains transcriptions, sentence segmentation, language boundary and phone-level time codes information. • Maghrebian Arabic-French (MOHDEB-AMAZOUZ et al., 2016): 53 hours of spontaneous speech gathered from TV entertainment and talk shows, involving speakers from Algeria, Morocco and Tunisia. The corpus contains sentence segmentation and language annotation. The vast majority of the available CS speech corpora have covered Chinese-English (Chan et al., 2009; Shen et al., 2011; Li et al., 2012; Lyu et al., 2015; Ahmed and Tan, 2012), Hindi-English (Dey and Fung, 2014; Ramanarayanan and Suendermann-Oeft, 2017; Sivasankaran et al., 2018; Sreeram et al., 2018; Pandey et al., 2017) and Spanish-English (Solorio and Liu, 2008; Deuchar et al., 2014; Ramanarayanan and Suendermann-Oeft, 2017) language pairs. Less work has covered Arabic-English (Hamed et al., 2018; Ismail, 2015), Arabic-French (Amazouz et al., 2018; MOHDEB-AMAZOUZ et al., 2016), Frisian-Dutch (Yilmaz et al., 2016), Mandarin-Taiwanese (Lyu et al., 2006; Lyu and Lyu, 2008), Turkish-German ("
2020.lrec-1.523,choukri-etal-2004-network,0,0.0869374,"he link to the modern world and a tool for better job opportunities. 4238 3. Related Work According to Ethnologue (Eberhard et al., 2019), the Arabic language has 273.9 million total number of speakers (ranking 6th among all languages). Despite Arabic being one of the most widely used languages, there is huge scarcity in the available resources. Most of the collected speech corpora are for MSA, mainly covering news data, politics and economics, such as GALE data (Walker, 2017; Walker, 2018), GlobalPhone (Schultz, 2002), United Nations Proceedings Speech (Chay et al., 2014), NetDC Arabic BNSC (Choukri et al., 2004), and Arabic Broadcast News Speech and Transcripts (Maamouri et al., 2001; Maamouri et al., 2006). Fewer resources are available for dialectal Arabic, such as CALLHOME Egyptian Arabic corpus (Gadalla et al., 1997) and JANA: A Human-Human Dialogues Corpus for Egyptian Dialect (Elmadany et al., 2016). Very few speech corpora have been collected for mixed Arabic. Up to our knowledge, Arabic CS speech corpora have only been collected for the following dialects: significant progress in the available CS speech corpora, the available corpora are yet limited; as they are mostly relatively-small and co"
2020.lrec-1.523,W14-5152,0,0.0359494,"Missing"
2020.lrec-1.523,dey-fung-2014-hindi,0,0.0113303,"ions, gathered from 20 speakers. The corpus contains transcriptions, sentence segmentation, language boundary and phone-level time codes information. • Maghrebian Arabic-French (MOHDEB-AMAZOUZ et al., 2016): 53 hours of spontaneous speech gathered from TV entertainment and talk shows, involving speakers from Algeria, Morocco and Tunisia. The corpus contains sentence segmentation and language annotation. The vast majority of the available CS speech corpora have covered Chinese-English (Chan et al., 2009; Shen et al., 2011; Li et al., 2012; Lyu et al., 2015; Ahmed and Tan, 2012), Hindi-English (Dey and Fung, 2014; Ramanarayanan and Suendermann-Oeft, 2017; Sivasankaran et al., 2018; Sreeram et al., 2018; Pandey et al., 2017) and Spanish-English (Solorio and Liu, 2008; Deuchar et al., 2014; Ramanarayanan and Suendermann-Oeft, 2017) language pairs. Less work has covered Arabic-English (Hamed et al., 2018; Ismail, 2015), Arabic-French (Amazouz et al., 2018; MOHDEB-AMAZOUZ et al., 2016), Frisian-Dutch (Yilmaz et al., 2016), Mandarin-Taiwanese (Lyu et al., 2006; Lyu and Lyu, 2008), Turkish-German (C¸etino˘glu, 2017), English-Malay (Ahmed and Tan, 2012), English-isiZulu (van der Westhuizen and Niesler, 2016)"
2020.lrec-1.523,C12-2029,0,0.0628441,"Missing"
2020.lrec-1.523,L18-1601,1,0.855957,"Missing"
2020.lrec-1.523,li-etal-2012-mandarin,0,0.0247779,"d speech from books and movie transcripts, as well as informal conversations, gathered from 20 speakers. The corpus contains transcriptions, sentence segmentation, language boundary and phone-level time codes information. • Maghrebian Arabic-French (MOHDEB-AMAZOUZ et al., 2016): 53 hours of spontaneous speech gathered from TV entertainment and talk shows, involving speakers from Algeria, Morocco and Tunisia. The corpus contains sentence segmentation and language annotation. The vast majority of the available CS speech corpora have covered Chinese-English (Chan et al., 2009; Shen et al., 2011; Li et al., 2012; Lyu et al., 2015; Ahmed and Tan, 2012), Hindi-English (Dey and Fung, 2014; Ramanarayanan and Suendermann-Oeft, 2017; Sivasankaran et al., 2018; Sreeram et al., 2018; Pandey et al., 2017) and Spanish-English (Solorio and Liu, 2008; Deuchar et al., 2014; Ramanarayanan and Suendermann-Oeft, 2017) language pairs. Less work has covered Arabic-English (Hamed et al., 2018; Ismail, 2015), Arabic-French (Amazouz et al., 2018; MOHDEB-AMAZOUZ et al., 2016), Frisian-Dutch (Yilmaz et al., 2016), Mandarin-Taiwanese (Lyu et al., 2006; Lyu and Lyu, 2008), Turkish-German (C¸etino˘glu, 2017), English-Malay (A"
2020.lrec-1.523,W18-3202,0,0.0131724,"tions, sentence segmentation, language boundary and phone-level time codes information. • Maghrebian Arabic-French (MOHDEB-AMAZOUZ et al., 2016): 53 hours of spontaneous speech gathered from TV entertainment and talk shows, involving speakers from Algeria, Morocco and Tunisia. The corpus contains sentence segmentation and language annotation. The vast majority of the available CS speech corpora have covered Chinese-English (Chan et al., 2009; Shen et al., 2011; Li et al., 2012; Lyu et al., 2015; Ahmed and Tan, 2012), Hindi-English (Dey and Fung, 2014; Ramanarayanan and Suendermann-Oeft, 2017; Sivasankaran et al., 2018; Sreeram et al., 2018; Pandey et al., 2017) and Spanish-English (Solorio and Liu, 2008; Deuchar et al., 2014; Ramanarayanan and Suendermann-Oeft, 2017) language pairs. Less work has covered Arabic-English (Hamed et al., 2018; Ismail, 2015), Arabic-French (Amazouz et al., 2018; MOHDEB-AMAZOUZ et al., 2016), Frisian-Dutch (Yilmaz et al., 2016), Mandarin-Taiwanese (Lyu et al., 2006; Lyu and Lyu, 2008), Turkish-German (C¸etino˘glu, 2017), English-Malay (Ahmed and Tan, 2012), English-isiZulu (van der Westhuizen and Niesler, 2016) and Sepedi-English (Modipa et al., 2013). Although CS has received m"
2020.lrec-1.523,D08-1102,0,0.0180922,"aghrebian Arabic-French (MOHDEB-AMAZOUZ et al., 2016): 53 hours of spontaneous speech gathered from TV entertainment and talk shows, involving speakers from Algeria, Morocco and Tunisia. The corpus contains sentence segmentation and language annotation. The vast majority of the available CS speech corpora have covered Chinese-English (Chan et al., 2009; Shen et al., 2011; Li et al., 2012; Lyu et al., 2015; Ahmed and Tan, 2012), Hindi-English (Dey and Fung, 2014; Ramanarayanan and Suendermann-Oeft, 2017; Sivasankaran et al., 2018; Sreeram et al., 2018; Pandey et al., 2017) and Spanish-English (Solorio and Liu, 2008; Deuchar et al., 2014; Ramanarayanan and Suendermann-Oeft, 2017) language pairs. Less work has covered Arabic-English (Hamed et al., 2018; Ismail, 2015), Arabic-French (Amazouz et al., 2018; MOHDEB-AMAZOUZ et al., 2016), Frisian-Dutch (Yilmaz et al., 2016), Mandarin-Taiwanese (Lyu et al., 2006; Lyu and Lyu, 2008), Turkish-German (C¸etino˘glu, 2017), English-Malay (Ahmed and Tan, 2012), English-isiZulu (van der Westhuizen and Niesler, 2016) and Sepedi-English (Modipa et al., 2013). Although CS has received more attention from the linguistic and NLP communities and researchers have made The Arz"
2020.lrec-1.523,L16-1739,0,0.0462131,"Missing"
2020.msr-1.4,P17-1183,0,0.0271995,"tation of the stack and the buffer will be updated. 2.4 Subsequent Modules The subsequent modules are exactly the same as in previous year, we only very briefly describe them, and refer the reader to Yu et al. (2019) for the details. The function word completion module takes the ordered deep dependency tree as input, and generates function words to the left and right for each content word. Only when a special symbol signifying stopping generation is predicted, we move to generate function words to the next content word. The inflection module is similar to the hard-monotonic attention model by Aharoni and Goldberg (2017), which uses a pointer to indicate the current input character in the lemma and predicts edit operations to generate the inflected word. To increase the robustness on irregular inflection, we combine the model with a rule-based system, which takes precedence if the lemma and morphological tag combination is frequent and unique in the training data. The contraction module has two steps. We first assign BIO tags to each word, which group the words that need to be contracted together. The grouped words are then concatenated as a sequence, and passed to a standard Seq2Seq model to generate the con"
2020.msr-1.4,C10-1012,0,0.0351028,"nally use a bidirectional LSTM to model the sequence, and the output vector is also added into the final representation. (seq) vi x0i = 2.2 (token) ...vn(token) )i (6) (tree) vi (seq) vi (7) = BiLSTM(v0 (token) vi + + Graph-based Linearization Similar to our previous system, we take a divide-and-conquer approach by breaking each dependency tree into subtrees, consisting of a head and several dependents, and order the subtrees individually. Finally, we combine the ordered subtrees into a full sentence. In our previous system, we used beam search to build up the sequence incrementally following Bohnet et al. (2010). It achieves good performance, however, the nature of beam search restricts the training and decoding speed, since the model has to make a calculation at every step. Instead, we use a graph-based decoding method proposed in Yu et al. (2020), in which we model the linearization task as a Traveling Salesman Problem (TSP): each word is treated as a city, the score of a bigram in the output sequence is the traveling cost from a city to another, and the optimal sequence of words is the shortest route that visits each city exactly once. In this formulation, the model only needs to calculate the cos"
2020.msr-1.4,2020.acl-main.665,0,0.0308116,"Missing"
2020.msr-1.4,D19-6301,0,0.101047,"token representation and a better linearizer, as well as a simple ensembling approach. We also experiment with data augmentation, which brings some additional performance gain. The system is available at https://github.com/EggplantElf/IMSurReal. 1 Introduction This paper presents our submission to the Surface Realization Shared Task 2020 (Mille et al., 2020). As in the previous year, we participate in both the shallow and the deep track. Additionally, we also participate in the new setting, where additional unlabeled data is permitted to train the models. Since last year’s SR’19 shared task (Mille et al., 2019), we have made some improvements over the winning system, mostly described in Yu et al. (2020). In particular, we designed a new linearization module, where we cast the task of word ordering as a Traveling Salesman Problem (TSP), and use the biaffine attention model (Dozat and Manning, 2016) to calculate the score. The new linearizer greatly reduces the decoding time and improves the performance. We also addressed a problem in the previous system which restricted the output to projective trees by employing a transition-based reordering system, which is inspired by transition-based non-projecti"
2020.msr-1.4,2020.msr-1.1,0,0.656591,"ermany firstname.lastname@ims.uni-stuttgart.de Abstract We introduce the IMS contribution to the Surface Realization Shared Task 2020. Our current system achieves substantial improvement over the state-of-the-art system from last year, mainly due to a better token representation and a better linearizer, as well as a simple ensembling approach. We also experiment with data augmentation, which brings some additional performance gain. The system is available at https://github.com/EggplantElf/IMSurReal. 1 Introduction This paper presents our submission to the Surface Realization Shared Task 2020 (Mille et al., 2020). As in the previous year, we participate in both the shallow and the deep track. Additionally, we also participate in the new setting, where additional unlabeled data is permitted to train the models. Since last year’s SR’19 shared task (Mille et al., 2019), we have made some improvements over the winning system, mostly described in Yu et al. (2020). In particular, we designed a new linearization module, where we cast the task of word ordering as a Traveling Salesman Problem (TSP), and use the biaffine attention model (Dozat and Manning, 2016) to calculate the score. The new linearizer greatl"
2020.msr-1.4,P09-1040,0,0.237158,"de some improvements over the winning system, mostly described in Yu et al. (2020). In particular, we designed a new linearization module, where we cast the task of word ordering as a Traveling Salesman Problem (TSP), and use the biaffine attention model (Dozat and Manning, 2016) to calculate the score. The new linearizer greatly reduces the decoding time and improves the performance. We also addressed a problem in the previous system which restricted the output to projective trees by employing a transition-based reordering system, which is inspired by transition-based non-projective parsing (Nivre, 2009). Inspired by the large improvement from data augmentation in Elder et al. (2020), we utilize extra training data for all languages. We parse unlabeled sentences from news and Wikipedia text, then convert the parse trees into the shared task format, and train our system on the augmented data. Finally, we apply simple ensembling by majority voting using ten models to improve the stability of the system and further push towards the performance upper bound. 2 Surface Realization System Our system largely follows our submission from the previous year (Yu et al., 2019), with some major change in th"
2020.msr-1.4,D19-6306,1,0.930467,"ion-based non-projective parsing (Nivre, 2009). Inspired by the large improvement from data augmentation in Elder et al. (2020), we utilize extra training data for all languages. We parse unlabeled sentences from news and Wikipedia text, then convert the parse trees into the shared task format, and train our system on the augmented data. Finally, we apply simple ensembling by majority voting using ten models to improve the stability of the system and further push towards the performance upper bound. 2 Surface Realization System Our system largely follows our submission from the previous year (Yu et al., 2019), with some major change in the linearization module. It consists of up to five modules working in a pipeline fashion. The first module is linearization, which orders the input dependency tree (shallow or deep alike) into a sequence of lemmata. The second module is reordering, which adjusts the sequence from the previous step such that the new sequence could be non-projective. The third module is function word completion (for the deep track only), which generates function words as the dependents of the content words from the previous step. The fourth module is morphological inflection, which t"
2020.msr-1.4,2020.acl-main.134,1,0.859074,"xperiment with data augmentation, which brings some additional performance gain. The system is available at https://github.com/EggplantElf/IMSurReal. 1 Introduction This paper presents our submission to the Surface Realization Shared Task 2020 (Mille et al., 2020). As in the previous year, we participate in both the shallow and the deep track. Additionally, we also participate in the new setting, where additional unlabeled data is permitted to train the models. Since last year’s SR’19 shared task (Mille et al., 2019), we have made some improvements over the winning system, mostly described in Yu et al. (2020). In particular, we designed a new linearization module, where we cast the task of word ordering as a Traveling Salesman Problem (TSP), and use the biaffine attention model (Dozat and Manning, 2016) to calculate the score. The new linearizer greatly reduces the decoding time and improves the performance. We also addressed a problem in the previous system which restricted the output to projective trees by employing a transition-based reordering system, which is inspired by transition-based non-projective parsing (Nivre, 2009). Inspired by the large improvement from data augmentation in Elder et"
2020.sigmorphon-1.5,Q16-1022,0,0.0616144,"Missing"
2020.sigmorphon-1.5,P17-1183,0,0.021931,"models with different inductive biases. The first type is the Finite-State-Transducer (FST) baseline by Lee et al. (2020), based on the pair n-gram model (Novak et al., 2016). The other three types are all variants of Seq2Seq models, where we use the same BiLSTM encoder to encode the input grapheme sequence. The first one is a vanilla Seq2Seq model with attention (attn), similar to Luong et al. (2015), where the decoder applies attention on the encoded input and use the attended input vector to predict the output phonemes. The second one is a hard monotonic attention model (mono), similar to Aharoni and Goldberg (2017), where the decoder uses a pointer to select the input vector to make a prediction: either producGrapheme-to-Phoneme Conversion Task and Data We first apply our framework on the grapheme-tophoneme conversion task (Gorman et al., 2020), which includes 15 languages from the WikiPron project (Lee et al., 2020) with a diverse typological spectrum: Armenian (arm), Bulgarian (bul), French (fre), Georgian (geo), Hindi (hin), Hungarian (hun), Icelandic (ice), Korean (kor), Lithuanian (lit), Modern Greek (gre), Adyghe (ady), Dutch (dut), Japanese hiragana (jpn), Romanian (rum), and Vietnamese (vie). As"
2020.sigmorphon-1.5,D19-1091,0,0.06609,"Missing"
2020.sigmorphon-1.5,K17-2002,0,0.0345938,"Missing"
2020.sigmorphon-1.5,K17-2010,0,0.0349667,"em is available at https://www.ims.uni-stuttgart.de/ en/institute/team/Yu-00010/. 1 Introduction The vast majority of languages in the world have very few annotated dataset available for training natural language processing models, if at all. Dealing with the low-resource languages has sparked much interest in the NLP community (Garrette et al., 2013; Agi´c et al., 2016; Zoph et al., 2016). When annotation is difficult to obtain, data augmentation is a common practice to increase training data size with reasonable quality to feed to powerful models (Ragni et al., 2014; Bergmanis et al., 2017; Silfverberg et al., 2017). For example, the data hallucination method by Anastasopoulos and Neubig (2019) automatically creates non-existing “words” to augment morphological inflection data, which alleviates the label bias problem in the generation model. However, the data created by such method can only help regularize the model, but cannot be viewed as valid words of a language. Orthogonal to the data augmentation approach, another commonly used method to boost model performance without changing the architecture is ensembling, i.e., by training several models of the same kind and selecting the output by majority vot"
2020.sigmorphon-1.5,W03-0407,0,0.18492,"ramework to search for the optimal ensemble and simultaneously annotate unlabeled data. The proposed method is an iterative process, which uses an ensemble of heterogeneous models to select and annotate unlabeled data based on the agreement of the ensemble, and use the annotated data to train new models, which are in turn potential members of the new ensemble. The ensemble is a subset of all trained models that maximizes the accuracy on the development set, and we use a genetic algorithm to find such combination of models. This approach can be viewed as a type of selftraining (Yarowsky, 1995; Clark et al., 2003), but instead of using the confidence of one model, we use the agreement of many models to annotate new data. The key difference is that the model diversity in the ensemble can alleviate the confirmation bias of typical self-training approaches. We apply the framework on two of the SIGMORPHON 2020 Shared Tasks: grapheme-to-phoneme conversion (Gorman et al., 2020) and morphological inflection (Vylomova et al., 2020). Our system rank the first in the former and the fourth in the latter. While analyzing the contribution of each component of our framework, we found that the data augmentation metho"
2020.sigmorphon-1.5,D13-1021,0,0.0299244,", Romanian (rum), and Vietnamese (vie). As preprocessing, we romanize the scripts of 1 https://pypi.org/project/pykakasi/ https://pypi.org/project/ hangul-romanize/ 3 https://github.com/hermitdave/ FrequencyWords/ 4 https://github.com/timarkh/ uniparser-grammar-adyghe 5 https://github.com/akalongman/ geo-words 6 Georgian is actually in OpenSubtitles, but we accidentally missed it because of a confusion with the language code. 2 72 ing a phoneme, or moving the pointer to the next position. The monotonic alignment of the input and output is obtained with the Chinese Restaurant Process following Sudoh et al. (2013), which is provided in the baseline model of the SIGMORPHON 2016 Shared Task (Cotterell et al., 2016). The third one is essentially a hybrid of hard monotonic attention model and tagging model (tag), i.e., for each grapheme we predict a short sequence of phonemes that is aligned to it. It relies on the same monotonic alignment for training. This model is different from the previous one in that it can potentially alleviate the error propagation problem, since the short sequences are nonautoregressive and independent of each other, much like tagging. For each of the three models, we further crea"
2020.sigmorphon-1.5,P13-1057,0,0.0301993,"shared tasks: graphemeto-phoneme conversion and morphological inflection. With very simple base models in the ensemble, we rank the first and the fourth in these two tasks. We show in the analysis that our system works especially well on lowresource languages. The system is available at https://www.ims.uni-stuttgart.de/ en/institute/team/Yu-00010/. 1 Introduction The vast majority of languages in the world have very few annotated dataset available for training natural language processing models, if at all. Dealing with the low-resource languages has sparked much interest in the NLP community (Garrette et al., 2013; Agi´c et al., 2016; Zoph et al., 2016). When annotation is difficult to obtain, data augmentation is a common practice to increase training data size with reasonable quality to feed to powerful models (Ragni et al., 2014; Bergmanis et al., 2017; Silfverberg et al., 2017). For example, the data hallucination method by Anastasopoulos and Neubig (2019) automatically creates non-existing “words” to augment morphological inflection data, which alleviates the label bias problem in the generation model. However, the data created by such method can only help regularize the model, but cannot be viewe"
2020.sigmorphon-1.5,2020.sigmorphon-1.2,0,0.578796,"he ensemble is a subset of all trained models that maximizes the accuracy on the development set, and we use a genetic algorithm to find such combination of models. This approach can be viewed as a type of selftraining (Yarowsky, 1995; Clark et al., 2003), but instead of using the confidence of one model, we use the agreement of many models to annotate new data. The key difference is that the model diversity in the ensemble can alleviate the confirmation bias of typical self-training approaches. We apply the framework on two of the SIGMORPHON 2020 Shared Tasks: grapheme-to-phoneme conversion (Gorman et al., 2020) and morphological inflection (Vylomova et al., 2020). Our system rank the first in the former and the fourth in the latter. While analyzing the contribution of each component of our framework, we found that the data augmentation method does not significantly improve the results for languages with medium or large training data in the shared tasks, i.e., the advantage of our system mainly comes from the massive ensemble of a variety of base models. However, when we simulate the low-resource scenario or consider only the low-resource languages, the benefit of data augmentation becomes prominent."
2020.sigmorphon-1.5,P15-2111,0,0.0646665,"Missing"
2020.sigmorphon-1.5,2020.lrec-1.521,0,0.324908,"the middle path, in which we keep half of all additional data from the previous iteration together with the selected data in the current iteration. For example, there are 3600 additional instances produced in iteration 0, 3600/2 + 3600 = 5400 in iteration 1, 5400/2 + 3600 = 6300 in iteration 2, and the size eventually converges to 3600 × 2 = 7200. 3 3.1 3.2 Models As the framework desires the models to be as diverse as possible to maximize its benefit, we employ four different types of base models with different inductive biases. The first type is the Finite-State-Transducer (FST) baseline by Lee et al. (2020), based on the pair n-gram model (Novak et al., 2016). The other three types are all variants of Seq2Seq models, where we use the same BiLSTM encoder to encode the input grapheme sequence. The first one is a vanilla Seq2Seq model with attention (attn), similar to Luong et al. (2015), where the decoder applies attention on the encoded input and use the attended input vector to predict the output phonemes. The second one is a hard monotonic attention model (mono), similar to Aharoni and Goldberg (2017), where the decoder uses a pointer to select the input vector to make a prediction: either prod"
2020.sigmorphon-1.5,L16-1147,0,0.0706111,"Missing"
2020.sigmorphon-1.5,P19-1148,0,0.0347615,"n the previous task, and each model has about 0.5M parameters. AVG 54.2 35.5 35.6 25.2 54.7 53.4 35.9 29.2 Table 3: WER on the development set for the simulated low-resource experiment in the scenarios with and without data augmentation. In each scenario, we show the average model performance and the ensemble performance in the first iteration and the best iteration. 4.3 Experiments Table 4 compares the average test accuracy between our system (IMS-00-0) and the systems of the winning teams as well as the baselines. The baselines include a hard monotonic attention model with latent alignment (Wu and Cotterell, 2019) and a carefully tuned transformer (Vaswani et al., 2017; Wu et al., 2020), noted as mono and trm. They are additionally trained with augmented data by Anastasopoulos and Neubig (2019), noted as mono-aug and trm-aug. On average, our system ranks the fourth among the participating teams and the third in the restricted setting (without external data source or cross-lingual methods). It outperforms the hard monotonic attention baseline, but not the transformer baseline. More details on the systems and their comparisons are described in Vylomova et al. (2020). Compared to the previous task, we use"
2020.sigmorphon-1.5,P95-1026,0,0.741894,"y developing a framework to search for the optimal ensemble and simultaneously annotate unlabeled data. The proposed method is an iterative process, which uses an ensemble of heterogeneous models to select and annotate unlabeled data based on the agreement of the ensemble, and use the annotated data to train new models, which are in turn potential members of the new ensemble. The ensemble is a subset of all trained models that maximizes the accuracy on the development set, and we use a genetic algorithm to find such combination of models. This approach can be viewed as a type of selftraining (Yarowsky, 1995; Clark et al., 2003), but instead of using the confidence of one model, we use the agreement of many models to annotate new data. The key difference is that the model diversity in the ensemble can alleviate the confirmation bias of typical self-training approaches. We apply the framework on two of the SIGMORPHON 2020 Shared Tasks: grapheme-to-phoneme conversion (Gorman et al., 2020) and morphological inflection (Vylomova et al., 2020). Our system rank the first in the former and the fourth in the latter. While analyzing the contribution of each component of our framework, we found that the da"
2020.sigmorphon-1.5,D16-1163,0,0.0302611,"and morphological inflection. With very simple base models in the ensemble, we rank the first and the fourth in these two tasks. We show in the analysis that our system works especially well on lowresource languages. The system is available at https://www.ims.uni-stuttgart.de/ en/institute/team/Yu-00010/. 1 Introduction The vast majority of languages in the world have very few annotated dataset available for training natural language processing models, if at all. Dealing with the low-resource languages has sparked much interest in the NLP community (Garrette et al., 2013; Agi´c et al., 2016; Zoph et al., 2016). When annotation is difficult to obtain, data augmentation is a common practice to increase training data size with reasonable quality to feed to powerful models (Ragni et al., 2014; Bergmanis et al., 2017; Silfverberg et al., 2017). For example, the data hallucination method by Anastasopoulos and Neubig (2019) automatically creates non-existing “words” to augment morphological inflection data, which alleviates the label bias problem in the generation model. However, the data created by such method can only help regularize the model, but cannot be viewed as valid words of a language. Orthogon"
2020.sustainlp-1.10,W17-5514,0,0.0540808,"Missing"
2020.sustainlp-1.10,E17-2026,0,0.0317028,"Missing"
2020.sustainlp-1.10,D19-1433,0,0.0149293,"The wrong predictions of Name Labeling, e.g., false-positive rates of names (e.g., object select, 78 The domain adaptation approach is used to transfer the domain-general feature space from source tasks as “prior knowledge” to the target task in order to overcome the hand-labeled data scarcity (Blitzer et al., 2006; Daume III and Marcu, 2006; Ramponi and Plank, 2020). For POS tagging, Jiang and Zhai (2007) propose a supervised instance weighting technique with or without labeled instances in target domain, whereas Kann et al. (2018) use character-level and subword-level supervision. However, Han and Eisenstein (2019) demonstrate unsupervised multi-task learning with the domain-adaptive fine-tuning method by utilizing contextualized word embeddings for the new domains. Similarly, NER is a sequence labeling task that is often addressed by domain adaptation and multi-task learning because of the low-resource domain. But, most of the NER tasks consist of different label spaces. Jia et al. (2019) use crossdomain language modeling for performing crosstask knowledge transfer by extracting knowledge of domain differences from raw text, while Peng and Dredze (2017b) utilize multi-task learning approach for shared"
2020.sustainlp-1.10,W06-1615,0,0.200799,"and proposed models. We observed that BERT shows consistent lower results for the tokens like city, state from BookResteurant, and location name, object location type from SearchScreeningE whereas it outperforms ELMo for proper name detection like object name from RateBook and SearchCreativeW. domains. The wrong predictions of Name Labeling, e.g., false-positive rates of names (e.g., object select, 78 The domain adaptation approach is used to transfer the domain-general feature space from source tasks as “prior knowledge” to the target task in order to overcome the hand-labeled data scarcity (Blitzer et al., 2006; Daume III and Marcu, 2006; Ramponi and Plank, 2020). For POS tagging, Jiang and Zhai (2007) propose a supervised instance weighting technique with or without labeled instances in target domain, whereas Kann et al. (2018) use character-level and subword-level supervision. However, Han and Eisenstein (2019) demonstrate unsupervised multi-task learning with the domain-adaptive fine-tuning method by utilizing contextualized word embeddings for the new domains. Similarly, NER is a sequence labeling task that is often addressed by domain adaptation and multi-task learning because of the low-resour"
2020.sustainlp-1.10,P18-1031,0,0.0208192,"oduce unique embeddings based-on the current context, instead of using a single, fixed vector per word like in Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). The pre-trained models, usually an LSTM (Hochreiter and Schmidhuber, 1997) or a Transformer (Vaswani et al., 2017) can be trained for token-level classification tasks, e.g., named entitity recognition, part-of-speech, or sentence-level classifications, e.g., text classification, sentiment analysis. At the same time, they can leverage the the language modeling (Peters et al., 2018; Devlin et al., 2019) by fine-tuning (Howard and Ruder, 2018) the trained objectives on domain-specific dataset as well as they can be used as feature-based models (Peters et al., 2018; Tenney et al., 2019; Brunner et al., 2020) for the down-stream tasks. In this study, we employ feature-based BERT and ELMo for the slot-filling task in low-resource domain. BERT uses a bidirectional transformer model which is trained on a masked language modeling task. It uses WordPiece embeddings (Wu et al., 2016) which means each word of an input represented with its sub-tokens. Thus, we use the first sub-token for representing the word as it turns out in (Devlin et al"
2020.sustainlp-1.10,W19-4828,0,0.0373097,"we employ feature-based BERT and ELMo for the slot-filling task in low-resource domain. BERT uses a bidirectional transformer model which is trained on a masked language modeling task. It uses WordPiece embeddings (Wu et al., 2016) which means each word of an input represented with its sub-tokens. Thus, we use the first sub-token for representing the word as it turns out in (Devlin et al., 2019). Additionally, BERT consists of multiple successive layers, i.e., 24 layers because of preferred BERT-large-cased model, and each layer represents different linguistic notions of syntax or semantics (Clark et al., 2019). In order to find the focused layers on local context (Tenney et al., 2019) in these linguistic notions, the attention visualization tool (Vig, 2019) is used on randomly Problem Statement Problem Definition We partition the slot filling task in two consecutive sub-tasks which are called Slot Labeling and Name Labeling. The Slot Labeling task requires to predict for each token in a sentence one of classes S = {O, SLOT } where SLOT corresponds to slots whereas O represents non-slot tokens. The Name Labeling task requires to predict one label from a predefined name label set N = {...} for a set"
2020.sustainlp-1.10,T75-2026,0,0.43741,"Missing"
2020.sustainlp-1.10,P19-1236,0,0.0173091,"iang and Zhai (2007) propose a supervised instance weighting technique with or without labeled instances in target domain, whereas Kann et al. (2018) use character-level and subword-level supervision. However, Han and Eisenstein (2019) demonstrate unsupervised multi-task learning with the domain-adaptive fine-tuning method by utilizing contextualized word embeddings for the new domains. Similarly, NER is a sequence labeling task that is often addressed by domain adaptation and multi-task learning because of the low-resource domain. But, most of the NER tasks consist of different label spaces. Jia et al. (2019) use crossdomain language modeling for performing crosstask knowledge transfer by extracting knowledge of domain differences from raw text, while Peng and Dredze (2017b) utilize multi-task learning approach for shared representations in multiple tasks simultaneously to have better generalize for domain adaptation. As examined here, most existing work in NLP considers the low-resource issue as a problem of shared feature spaces. The main consideration is always augmenting the most similar feature intersection of source and target domains and use this feature space to improve the low-resource ta"
2020.sustainlp-1.10,P07-1034,0,0.132296,"e city, state from BookResteurant, and location name, object location type from SearchScreeningE whereas it outperforms ELMo for proper name detection like object name from RateBook and SearchCreativeW. domains. The wrong predictions of Name Labeling, e.g., false-positive rates of names (e.g., object select, 78 The domain adaptation approach is used to transfer the domain-general feature space from source tasks as “prior knowledge” to the target task in order to overcome the hand-labeled data scarcity (Blitzer et al., 2006; Daume III and Marcu, 2006; Ramponi and Plank, 2020). For POS tagging, Jiang and Zhai (2007) propose a supervised instance weighting technique with or without labeled instances in target domain, whereas Kann et al. (2018) use character-level and subword-level supervision. However, Han and Eisenstein (2019) demonstrate unsupervised multi-task learning with the domain-adaptive fine-tuning method by utilizing contextualized word embeddings for the new domains. Similarly, NER is a sequence labeling task that is often addressed by domain adaptation and multi-task learning because of the low-resource domain. But, most of the NER tasks consist of different label spaces. Jia et al. (2019) us"
2020.sustainlp-1.10,W18-3401,0,0.0249626,"per name detection like object name from RateBook and SearchCreativeW. domains. The wrong predictions of Name Labeling, e.g., false-positive rates of names (e.g., object select, 78 The domain adaptation approach is used to transfer the domain-general feature space from source tasks as “prior knowledge” to the target task in order to overcome the hand-labeled data scarcity (Blitzer et al., 2006; Daume III and Marcu, 2006; Ramponi and Plank, 2020). For POS tagging, Jiang and Zhai (2007) propose a supervised instance weighting technique with or without labeled instances in target domain, whereas Kann et al. (2018) use character-level and subword-level supervision. However, Han and Eisenstein (2019) demonstrate unsupervised multi-task learning with the domain-adaptive fine-tuning method by utilizing contextualized word embeddings for the new domains. Similarly, NER is a sequence labeling task that is often addressed by domain adaptation and multi-task learning because of the low-resource domain. But, most of the NER tasks consist of different label spaces. Jia et al. (2019) use crossdomain language modeling for performing crosstask knowledge transfer by extracting knowledge of domain differences from ra"
2020.sustainlp-1.10,N19-1423,0,0.503888,"ectors in the class, i.e., builds a prototype vector for each class. Decision process is simply made based on distance metrics. Because of the availability of only a small amount of data in the current domain and the semantically rich and robust presentation in contextual pretrained embeddings, we argue that Rocchio classifier is sufficient for our task. Furthermore by using this simple classification method, we show the effectiveness of the non-slot noise reduction step from the input. 2 2.1 3.1 Inputs The contextualized word representation methods, e.g., ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), use a pre-trained network over the sentence in order to produce unique embeddings based-on the current context, instead of using a single, fixed vector per word like in Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). The pre-trained models, usually an LSTM (Hochreiter and Schmidhuber, 1997) or a Transformer (Vaswani et al., 2017) can be trained for token-level classification tasks, e.g., named entitity recognition, part-of-speech, or sentence-level classifications, e.g., text classification, sentiment analysis. At the same time, they can leverage the the language modeling"
2020.sustainlp-1.10,P17-1119,0,0.340216,"Missing"
2020.sustainlp-1.10,2020.coling-main.603,0,0.274441,"comparison of the results from both models indicates that the wrong predictions of the ’O’ label drastically reduced with NeuralSlot+Name model. Similar proper nouns, e.g., album and track, in the same domain denote the weakness of the proposed systems. NeuralSlot+Name model is not able to distinguish similar proper nouns. For example, the highest false-negative rate for album is track while it is album for track. 7 7.1 Related Work Low-resource Domain in NLP Typically in NLP, the domain is meant to refer to some coherent type of dataset that related to the underlying linguistic distribution (Ramponi and Plank, 2020). When the linguistic distribution between target and source domain differ, the performance drops on the target domain. Therefore, hand-labeled samples are needed for many NLP applications even though they are expensive to create and often not available for low-resource languages or domains. Many studies have recently been proposed to tackle the low-resource issue by using different approaches such as transfer learning for domain adaptation (Daume III and Marcu, 2006; Pan and Yang, 2009), and multi-task learning (Peng and Dredze, 2017a). Here, we review the slot filling like sequence labeling"
2020.sustainlp-1.10,D16-1223,0,0.241422,"Missing"
2020.sustainlp-1.10,D17-1038,0,0.0141173,"ing for performing crosstask knowledge transfer by extracting knowledge of domain differences from raw text, while Peng and Dredze (2017b) utilize multi-task learning approach for shared representations in multiple tasks simultaneously to have better generalize for domain adaptation. As examined here, most existing work in NLP considers the low-resource issue as a problem of shared feature spaces. The main consideration is always augmenting the most similar feature intersection of source and target domains and use this feature space to improve the low-resource target domain (Daum´e III, 2009; Ruder and Plank, 2017; Ramponi and Plank, 2020). 7.2 with NER as an auxiliary task through a multi-task learning setup and show improvement in slot filling with low-resource scenarios. Another direction relies on zero-shot learning approaches, i.e., learning method with label descriptions or label names, which have recently been popular in slot filling task. Zero-shot learning (Socher et al., 2013) is a classification setup in learning systems, where the model predict samples from classes that were not seen during training at test time. Zero-shot slot filling, i,e., either relies on slot names or slot descriptions"
2020.sustainlp-1.10,W18-5711,0,0.0411921,"Missing"
2020.sustainlp-1.10,P19-3007,0,0.0263038,"ed language modeling task. It uses WordPiece embeddings (Wu et al., 2016) which means each word of an input represented with its sub-tokens. Thus, we use the first sub-token for representing the word as it turns out in (Devlin et al., 2019). Additionally, BERT consists of multiple successive layers, i.e., 24 layers because of preferred BERT-large-cased model, and each layer represents different linguistic notions of syntax or semantics (Clark et al., 2019). In order to find the focused layers on local context (Tenney et al., 2019) in these linguistic notions, the attention visualization tool (Vig, 2019) is used on randomly Problem Statement Problem Definition We partition the slot filling task in two consecutive sub-tasks which are called Slot Labeling and Name Labeling. The Slot Labeling task requires to predict for each token in a sentence one of classes S = {O, SLOT } where SLOT corresponds to slots whereas O represents non-slot tokens. The Name Labeling task requires to predict one label from a predefined name label set N = {...} for a set of candidate slots. This implies that candidate slots have already been identified as SLOT by Slot Labeling task as shown in Figure 1. While S is shar"
2020.sustainlp-1.10,W17-2612,0,0.411972,"that related to the underlying linguistic distribution (Ramponi and Plank, 2020). When the linguistic distribution between target and source domain differ, the performance drops on the target domain. Therefore, hand-labeled samples are needed for many NLP applications even though they are expensive to create and often not available for low-resource languages or domains. Many studies have recently been proposed to tackle the low-resource issue by using different approaches such as transfer learning for domain adaptation (Daume III and Marcu, 2006; Pan and Yang, 2009), and multi-task learning (Peng and Dredze, 2017a). Here, we review the slot filling like sequence labeling studies such as part-of-speech tagging (POS) and named entity recognition (NER) within domain adaptation and multi-task learning. Qualitative Analysis We analyzed the results on individual slots by comparing them according to contextualized embeddings and proposed models. We observed that BERT shows consistent lower results for the tokens like city, state from BookResteurant, and location name, object location type from SearchScreeningE whereas it outperforms ELMo for proper name detection like object name from RateBook and SearchCrea"
2020.sustainlp-1.10,D14-1162,0,0.0855289,"h and robust presentation in contextual pretrained embeddings, we argue that Rocchio classifier is sufficient for our task. Furthermore by using this simple classification method, we show the effectiveness of the non-slot noise reduction step from the input. 2 2.1 3.1 Inputs The contextualized word representation methods, e.g., ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), use a pre-trained network over the sentence in order to produce unique embeddings based-on the current context, instead of using a single, fixed vector per word like in Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). The pre-trained models, usually an LSTM (Hochreiter and Schmidhuber, 1997) or a Transformer (Vaswani et al., 2017) can be trained for token-level classification tasks, e.g., named entitity recognition, part-of-speech, or sentence-level classifications, e.g., text classification, sentiment analysis. At the same time, they can leverage the the language modeling (Peters et al., 2018; Devlin et al., 2019) by fine-tuning (Howard and Ruder, 2018) the trained objectives on domain-specific dataset as well as they can be used as feature-based models (Peters et al., 2018; Tenney et al., 2019; Brunner"
2020.sustainlp-1.10,N18-1202,0,0.331725,"as the center of mass of all vectors in the class, i.e., builds a prototype vector for each class. Decision process is simply made based on distance metrics. Because of the availability of only a small amount of data in the current domain and the semantically rich and robust presentation in contextual pretrained embeddings, we argue that Rocchio classifier is sufficient for our task. Furthermore by using this simple classification method, we show the effectiveness of the non-slot noise reduction step from the input. 2 2.1 3.1 Inputs The contextualized word representation methods, e.g., ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), use a pre-trained network over the sentence in order to produce unique embeddings based-on the current context, instead of using a single, fixed vector per word like in Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). The pre-trained models, usually an LSTM (Hochreiter and Schmidhuber, 1997) or a Transformer (Vaswani et al., 2017) can be trained for token-level classification tasks, e.g., named entitity recognition, part-of-speech, or sentence-level classifications, e.g., text classification, sentiment analysis. At the same time, they can lev"
2020.sustainlp-1.10,W18-5047,0,0.215169,"ostic and therefore, can be trained by exploiting out-of-domain data. The second step identifies slot names only for slot tokens by using state-of-the-art pretrained contextual embeddings such as ELMO and BERT. We show that our approach outperforms other state-of-art systems on the SNIPS benchmark dataset. 1 Introduction Slot filling models, which predict task-specific names (e.g. artist, time) for these slots from user utterances, are a key component of spoken language understanding (SLU) systems. Deep learning approaches (Mesnil et al., 2013; Hakkani-T¨ur et al., 2016; Zhang and Wang, 2016; Zhu and Yu, 2018; Chen et al., 2013; Gupta et al., 2018; Bapna et al., 2017a) for SLU involve training on a large amount of annotated training data. Likewise, multi-domain studies (Hakkani-T¨ur et al., 2016; Liu and Lane, 2017) that rely on deep learning methods require a large amount of data for each domain. However, slot filling is a very challenging task if only a few labeled samples are available. Therefore, this paper proposes methods to address the low-resource domain issue of slot filling. We aim at improving performance of the slot filling task in different low-resource scenarios by exploring the effe"
2021.acl-tutorials.3,D19-1112,0,0.0649945,"ml19metalearning 2 http://cs330.stanford.edu/ 15 meta-learning approaches below. Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts, pages 15–20, August 1st, 2021. ©2021 Association for Computational Linguistics Table 1: Referrence of NLP tasks using different meta-learning methods. (A) Learning to initialize Text Classification Sequence Labelng Machine Translation Speech Recognition Relation Classification Dialogue Parsing Word Embedding (Dou et al., 2019) (Bansal et al., 2019) (Wu et al., 2020) (Gu et al., 2018) (Indurthi et al., 2020) (Obamuyide and Vlachos, 2019) (Bose et al., 2019) (Lv et al., 2019) (Wang et al., 2019) (Qian and Yu, 2019) (Madotto et al., 2019) (Mi et al., 2019) (Guo et al., 2019) (Huang et al., 2018) (Hu et al., 2019) Learning to optimize: (Chien and Lieow, 2019) (Sun et al., 2018) (Chen et al., 2020a) Learning the learning algorithm: (Sur´ıs et al., 2019) Network architecture search: (Mazzawi et al., 2019) (Shimada et al., 2020) (Chou et al., 2019) Learning the learning algorithm: (Chen et al., 2019b) (Serr`a et al., 2019"
2021.acl-tutorials.3,D19-1334,0,0.0418615,"Missing"
2021.acl-tutorials.3,P19-1542,0,0.10289,"ernational Joint Conference on Natural Language Processing: Tutorial Abstracts, pages 15–20, August 1st, 2021. ©2021 Association for Computational Linguistics Table 1: Referrence of NLP tasks using different meta-learning methods. (A) Learning to initialize Text Classification Sequence Labelng Machine Translation Speech Recognition Relation Classification Dialogue Parsing Word Embedding (Dou et al., 2019) (Bansal et al., 2019) (Wu et al., 2020) (Gu et al., 2018) (Indurthi et al., 2020) (Obamuyide and Vlachos, 2019) (Bose et al., 2019) (Lv et al., 2019) (Wang et al., 2019) (Qian and Yu, 2019) (Madotto et al., 2019) (Mi et al., 2019) (Guo et al., 2019) (Huang et al., 2018) (Hu et al., 2019) Learning to optimize: (Chien and Lieow, 2019) (Sun et al., 2018) (Chen et al., 2020a) Learning the learning algorithm: (Sur´ıs et al., 2019) Network architecture search: (Mazzawi et al., 2019) (Shimada et al., 2020) (Chou et al., 2019) Learning the learning algorithm: (Chen et al., 2019b) (Serr`a et al., 2019) Voice Cloning 3.1.1 Learning the learning algorithm: (Wu et al., 2019) (Ye and Ling, 2019) (Chen et al., 2019a) (Xiong et al., 2018) (Gao et al., 2019) (Eloff et al., 2019) Sound Event Detection (C) Other Learni"
2021.acl-tutorials.3,P19-1589,0,0.0634424,"Missing"
2021.acl-tutorials.3,P19-1253,0,0.12813,"ics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts, pages 15–20, August 1st, 2021. ©2021 Association for Computational Linguistics Table 1: Referrence of NLP tasks using different meta-learning methods. (A) Learning to initialize Text Classification Sequence Labelng Machine Translation Speech Recognition Relation Classification Dialogue Parsing Word Embedding (Dou et al., 2019) (Bansal et al., 2019) (Wu et al., 2020) (Gu et al., 2018) (Indurthi et al., 2020) (Obamuyide and Vlachos, 2019) (Bose et al., 2019) (Lv et al., 2019) (Wang et al., 2019) (Qian and Yu, 2019) (Madotto et al., 2019) (Mi et al., 2019) (Guo et al., 2019) (Huang et al., 2018) (Hu et al., 2019) Learning to optimize: (Chien and Lieow, 2019) (Sun et al., 2018) (Chen et al., 2020a) Learning the learning algorithm: (Sur´ıs et al., 2019) Network architecture search: (Mazzawi et al., 2019) (Shimada et al., 2020) (Chou et al., 2019) Learning the learning algorithm: (Chen et al., 2019b) (Serr`a et al., 2019) Voice Cloning 3.1.1 Learning the learning algorithm: (Wu et al., 2019) (Ye and Ling, 2019) (Chen et al., 2019a) (Xiong et al., 2018) (Gao et al., 2019) (Eloff et al., 2019) Sound Event Det"
2021.acl-tutorials.3,D19-1403,0,0.110977,"020) (Chou et al., 2019) Learning the learning algorithm: (Chen et al., 2019b) (Serr`a et al., 2019) Voice Cloning 3.1.1 Learning the learning algorithm: (Wu et al., 2019) (Ye and Ling, 2019) (Chen et al., 2019a) (Xiong et al., 2018) (Gao et al., 2019) (Eloff et al., 2019) Sound Event Detection (C) Other Learning to optimize: (Klejch et al., 2018) Network architecture search: (Chen et al., 2020b) (Baruwa et al., 2019) (Hsu et al., 2020) (Klejch et al., 2019) (Winata et al., 2020a) (Winata et al., 2020b) Multi-model Keyword Spotting (B) Learning to compare (Yu et al., 2018) (Tan et al., 2019) (Geng et al., 2019) (Sun et al., 2019) (Hou et al., 2020) Learning to Initialize Gradient descent is the core learning algorithm for deep learning. Most of the components in gradient descent are handcrafted. First, we have to determine how to initialize network parameters. Then the gradient is computed to update the parameters, and the learning rates are determined heuristically. Determining these components usually need experience, intuition, and trial and error. With meta-learning, those hyperparameters can be learned from data automatically. Among these series of approaches, learning a set of parameters to in"
2021.acl-tutorials.3,D18-1398,0,0.0280624,"ing approaches below. Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts, pages 15–20, August 1st, 2021. ©2021 Association for Computational Linguistics Table 1: Referrence of NLP tasks using different meta-learning methods. (A) Learning to initialize Text Classification Sequence Labelng Machine Translation Speech Recognition Relation Classification Dialogue Parsing Word Embedding (Dou et al., 2019) (Bansal et al., 2019) (Wu et al., 2020) (Gu et al., 2018) (Indurthi et al., 2020) (Obamuyide and Vlachos, 2019) (Bose et al., 2019) (Lv et al., 2019) (Wang et al., 2019) (Qian and Yu, 2019) (Madotto et al., 2019) (Mi et al., 2019) (Guo et al., 2019) (Huang et al., 2018) (Hu et al., 2019) Learning to optimize: (Chien and Lieow, 2019) (Sun et al., 2018) (Chen et al., 2020a) Learning the learning algorithm: (Sur´ıs et al., 2019) Network architecture search: (Mazzawi et al., 2019) (Shimada et al., 2020) (Chou et al., 2019) Learning the learning algorithm: (Chen et al., 2019b) (Serr`a et al., 2019) Voice Cloning 3.1.1 Learning the learning algorithm: (Wu"
2021.acl-tutorials.3,P19-1082,0,0.0484913,"Missing"
2021.acl-tutorials.3,2020.acl-main.128,0,0.24576,"earning algorithm: (Chen et al., 2019b) (Serr`a et al., 2019) Voice Cloning 3.1.1 Learning the learning algorithm: (Wu et al., 2019) (Ye and Ling, 2019) (Chen et al., 2019a) (Xiong et al., 2018) (Gao et al., 2019) (Eloff et al., 2019) Sound Event Detection (C) Other Learning to optimize: (Klejch et al., 2018) Network architecture search: (Chen et al., 2020b) (Baruwa et al., 2019) (Hsu et al., 2020) (Klejch et al., 2019) (Winata et al., 2020a) (Winata et al., 2020b) Multi-model Keyword Spotting (B) Learning to compare (Yu et al., 2018) (Tan et al., 2019) (Geng et al., 2019) (Sun et al., 2019) (Hou et al., 2020) Learning to Initialize Gradient descent is the core learning algorithm for deep learning. Most of the components in gradient descent are handcrafted. First, we have to determine how to initialize network parameters. Then the gradient is computed to update the parameters, and the learning rates are determined heuristically. Determining these components usually need experience, intuition, and trial and error. With meta-learning, those hyperparameters can be learned from data automatically. Among these series of approaches, learning a set of parameters to initialize gradient descent, or learning"
2021.acl-tutorials.3,P19-1402,0,0.0385432,"Missing"
2021.acl-tutorials.3,2020.acl-main.636,0,0.0241378,"uilding because modern personal assistants, such as Alexa and Siri, are composed of thousands of single-domain task-oriented dialog systems. However, training a learnable model for a task requires a large amount of labeled in-domain data, and collecting and annotating training data for the tasks is costly since it involves real user interactions. Therefore, researchers apply meta-learning to learn from multiple rich-resource tasks and adapt the meta-learned models to new domains with minimal training samples for dialog response generation (Qian and Yu, 2019) and dialogue state tracking (DST) (Huang et al., 2020). Also, training personalized chatbot that can mimic speakers with different personas is useful but challenging. Collecting many dialogs involving a specific persona is expensive, while it is challenging to capture a persona using only a few conversations. Thus, meta-learning comes into play for learning persona with few-shot example conversations (Madotto et al., 2019). Automatic Speech Recognition and 4 Diversity Neural Machine Translation Automatic speech recognition (ASR), Neural ma- As the main applications of the meta-learning apchine translation (NMT), and speech translation17 proaches"
2021.acl-tutorials.3,D18-1173,0,0.0511713,"Missing"
2021.acl-tutorials.3,D19-1045,0,0.119802,"019) Learning the learning algorithm: (Chen et al., 2019b) (Serr`a et al., 2019) Voice Cloning 3.1.1 Learning the learning algorithm: (Wu et al., 2019) (Ye and Ling, 2019) (Chen et al., 2019a) (Xiong et al., 2018) (Gao et al., 2019) (Eloff et al., 2019) Sound Event Detection (C) Other Learning to optimize: (Klejch et al., 2018) Network architecture search: (Chen et al., 2020b) (Baruwa et al., 2019) (Hsu et al., 2020) (Klejch et al., 2019) (Winata et al., 2020a) (Winata et al., 2020b) Multi-model Keyword Spotting (B) Learning to compare (Yu et al., 2018) (Tan et al., 2019) (Geng et al., 2019) (Sun et al., 2019) (Hou et al., 2020) Learning to Initialize Gradient descent is the core learning algorithm for deep learning. Most of the components in gradient descent are handcrafted. First, we have to determine how to initialize network parameters. Then the gradient is computed to update the parameters, and the learning rates are determined heuristically. Determining these components usually need experience, intuition, and trial and error. With meta-learning, those hyperparameters can be learned from data automatically. Among these series of approaches, learning a set of parameters to initialize gradient d"
2021.acl-tutorials.3,D19-1364,0,0.0909181,"(Shimada et al., 2020) (Chou et al., 2019) Learning the learning algorithm: (Chen et al., 2019b) (Serr`a et al., 2019) Voice Cloning 3.1.1 Learning the learning algorithm: (Wu et al., 2019) (Ye and Ling, 2019) (Chen et al., 2019a) (Xiong et al., 2018) (Gao et al., 2019) (Eloff et al., 2019) Sound Event Detection (C) Other Learning to optimize: (Klejch et al., 2018) Network architecture search: (Chen et al., 2020b) (Baruwa et al., 2019) (Hsu et al., 2020) (Klejch et al., 2019) (Winata et al., 2020a) (Winata et al., 2020b) Multi-model Keyword Spotting (B) Learning to compare (Yu et al., 2018) (Tan et al., 2019) (Geng et al., 2019) (Sun et al., 2019) (Hou et al., 2020) Learning to Initialize Gradient descent is the core learning algorithm for deep learning. Most of the components in gradient descent are handcrafted. First, we have to determine how to initialize network parameters. Then the gradient is computed to update the parameters, and the learning rates are determined heuristically. Determining these components usually need experience, intuition, and trial and error. With meta-learning, those hyperparameters can be learned from data automatically. Among these series of approaches, learning a set"
2021.acl-tutorials.3,D19-1024,0,0.0485146,"Missing"
2021.acl-tutorials.3,D19-1444,0,0.0399863,"Missing"
2021.acl-tutorials.3,D18-1223,0,0.0558469,"Missing"
2021.acl-tutorials.3,P19-1277,0,0.0323047,"Missing"
2021.acl-tutorials.3,N18-1109,0,0.155506,"zawi et al., 2019) (Shimada et al., 2020) (Chou et al., 2019) Learning the learning algorithm: (Chen et al., 2019b) (Serr`a et al., 2019) Voice Cloning 3.1.1 Learning the learning algorithm: (Wu et al., 2019) (Ye and Ling, 2019) (Chen et al., 2019a) (Xiong et al., 2018) (Gao et al., 2019) (Eloff et al., 2019) Sound Event Detection (C) Other Learning to optimize: (Klejch et al., 2018) Network architecture search: (Chen et al., 2020b) (Baruwa et al., 2019) (Hsu et al., 2020) (Klejch et al., 2019) (Winata et al., 2020a) (Winata et al., 2020b) Multi-model Keyword Spotting (B) Learning to compare (Yu et al., 2018) (Tan et al., 2019) (Geng et al., 2019) (Sun et al., 2019) (Hou et al., 2020) Learning to Initialize Gradient descent is the core learning algorithm for deep learning. Most of the components in gradient descent are handcrafted. First, we have to determine how to initialize network parameters. Then the gradient is computed to update the parameters, and the learning rates are determined heuristically. Determining these components usually need experience, intuition, and trial and error. With meta-learning, those hyperparameters can be learned from data automatically. Among these series of approac"
2021.americasnlp-1.23,2021.americasnlp-1.30,0,0.035558,"ill now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a transformer-based sequence to sequence model (Vaswani et al., 2017). We employed the hyperparameters proposed by Guzmán et al. (2019) for a low-resource scenario. We implemented the model using Fairseq (Ott et al., 2019). The implementation of the baseline can be found in the official shared task repository.4 4.2 University of British Columbia The team of the University of British Columbia (UBC-NLP; Billah-Nagoudi et al., 2021) participated for all ten language pairs and in both tracks. They used an encoder-decoder transformer model based on T5 (Raffel et al., 2020). This model was pretrained on a dataset consisting of 10 indigenous languages and Spanish, that was collected by the team from different sources such as the Bible and Wikipedia, totaling 1.17 GB of text. However, given that some of the languages have more available data than others, this dataset is unbalanced in favor of languages like Nahuatl, Guarani, and Quechua. The team also proposed a two-stage fine-tuning method: first fine-tuning on the entire da"
2021.americasnlp-1.23,2021.americasnlp-1.28,0,0.0389782,"Missing"
2021.americasnlp-1.23,2020.lrec-1.320,1,0.692133,"test datasets were translated to modern Nahuatl. In particular, the translations belong to Nahuatl Central/Nahuatl de la Huasteca (Hidalgo y San Luis Potosí) dialects. In order to be closer to the training corpus, an orthographic normalization was applied. A simple rule based approach was used, which was based on the most predictable orthographic changes between modern varieties and Classical Nahuatl. Spanish—Guarani Guarani is mostly spoken in Paraguay, Bolivia, Argentina and Brazil. It belongs to the Tupian language family (ISO gnw, gun, gug, gui, grn, nhd). The training corpus for Guarani (Chiruzzo et al., 2020) was collected from web sources (blogs and news articles) that contained a mix of dialects, from pure Guarani to more mixed Jopara which combines Guarani with Spanish neologisms. The development and test corpora, on the other hand, are in standard Paraguayan Guarani. Spanish—Bribri Bribri is a Chibchan language spoken in southern Costa Rica (ISO code bzd). The training set for Bribri was extracted from six sources (Feldman and Coto-Solano, 2020; Margery, 2005; Jara Murillo, 2018a; Constenla et al., 2004; Jara Murillo and García Segura, 2013; Jara Murillo, 2018b; Flores Solórzano, 2017), includ"
2021.americasnlp-1.23,D18-1269,0,0.0884352,"uage families: Aymaran, Arawak, Chibchan, Tupi-Guarani, UtoAztecan, Oto-Manguean, Quechuan, and Panoan. The ten language pairs included in the shared task are: Quechua–Spanish, Wixarika–Spanish, Shipibo-Konibo–Spanish, Asháninka–Spanish, Raramuri–Spanish, Nahuatl–Spanish, Otomí– Spanish, Aymara–Spanish, Guarani–Spanish, and Bribri–Spanish. For development and testing, we used parallel sentences belonging to a new natural language inference dataset for the 10 indigenous languages featured in our shared task, which is a manual translation of the Spanish version of the multilingual XNLI dataset (Conneau et al., 2018). For a complete description of this dataset we refer the reader to Ebrahimi et al. (2021). Together with the data, we also provided: a simple baseline based on the small transformer architecture (Vaswani et al., 2017) proposed together with the FLORES dataset (Guzmán et al., 2019); and a description of challenges and particular characteristics for all provided resources1 . We established two tracks: one where training models on the development set after hyperparameter tuning is 1 https://github.com/AmericasNLP/americasnlp2021/ blob/main/data/information_datasets.pdf 202 Proceedings of the Fir"
2021.americasnlp-1.23,P18-1128,0,0.020486,"8.4 8.3 ChrF 39.4 38.3 35.8 33.2 32.8 31.8 26.9 10.3 9.8 9.0 6.6 ChrF 39.9 38.0 29.7 28.6 16.3 15.5 12.4 ChrF 25.8 24.8 24.7 23.9 21.6 16.5 15.9 12.2 10.5 10.5 8.4 Table 3: Results of Track 1 (development set used for training) for all systems and language pairs. The results are ranked by the official metric of the shared task: ChrF. One team decided to send a anonymous submission (Anonym). Best results are shown in bold, and they are significantly better than the second place team (in each language-pair) according to the Wilcoxon signed-ranked test and Pitman’s permutation test with p&lt;0.05 (Dror et al., 2018). 208 ious high-resource languages, and then finetuned for each target language using the official provided data. 4.7 NRC-CNRC The team of the National Research Council Canada (NRC-CNRC; Knowles et al., 2021) submitted systems for the Spanish to Wixárika, Nahuatl, Rarámuri and Guarani language pairs for both tracks. Due to ethical considerations, the team decided not to use external data, and restricted themselves to the data provided for the shared task. All data was preprocessed with standard Moses tools (Koehn et al., 2007). The submitted systems were based on a Transformer model, and used"
2021.americasnlp-1.23,2020.coling-main.351,1,0.754898,"oken in Paraguay, Bolivia, Argentina and Brazil. It belongs to the Tupian language family (ISO gnw, gun, gug, gui, grn, nhd). The training corpus for Guarani (Chiruzzo et al., 2020) was collected from web sources (blogs and news articles) that contained a mix of dialects, from pure Guarani to more mixed Jopara which combines Guarani with Spanish neologisms. The development and test corpora, on the other hand, are in standard Paraguayan Guarani. Spanish—Bribri Bribri is a Chibchan language spoken in southern Costa Rica (ISO code bzd). The training set for Bribri was extracted from six sources (Feldman and Coto-Solano, 2020; Margery, 2005; Jara Murillo, 2018a; Constenla et al., 2004; Jara Murillo and García Segura, 2013; Jara Murillo, 2018b; Flores Solórzano, 2017), including a dictionary, a grammar, two language learning textbooks, one storybook and the transcribed sentences from Spanish–Wixarika Wixarika (also known as 2 Huichol) with ISO code hch is spoken in Mexico ISO 639-3 for the Nahutal languages: and belongs to the Yuto-Aztecan linguistic family. nch, ncx, naz, nln, nhe, ngu, nhk, nhx, nhp, ncl, nhm, nhy, The training, development and test sets all belong nlv, ppl, nhz, npl, nhc, nhv, to the same dialec"
2021.americasnlp-1.23,galarreta-etal-2017-corpus,1,0.842998,"training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al., 1993). Translated text was created by a bilingual teacher, and follows the most recent guidelines of the Minister of Education in Peru, however, the third source is an extraction of parallel sentences from an old dictionary. The development and test sets were created following the official convention as in the translated training sets. Spanish—Quechua Quechua is a family of languages spoken in Argentina, Bolivia, Colombia, Spanish—Asháninka Asháninka is an Ecuador, Peru, and Chile with many ISO codes for Arawakan language (ISO: cni) spoken"
2021.americasnlp-1.23,W19-6804,1,0.768784,"az jilata, the largest variant. This is similar to the variant of the available training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al., 1993). Translated text was created by a bilingual teacher, and follows the most recent guidelines of the Minister of Education in Peru, however, the third source is an extraction of parallel sentences from an old dictionary. The development and test sets were created following the official convention as in the translated training sets. Spanish—Quechua Quechua is a family of languages spoken in Argentina, Bolivia, Colombia, Spanish—Asháninka Asháninka is an Ecu"
2021.americasnlp-1.23,N15-2021,1,0.735842,"eceive worryingly little attention from NLP researchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; Barrault et al., 2020). ∗ *The first three authors contributed equally. The AmericasNLP 2021 Shared Task on Open Machine Translation (OMT) aimed at moving research on indigenous and endangered languages more into the focus of the NLP community. As the official shared task training sets, we provided a collection of publicly available parallel corpora (§3). Additionally, all participants were allowed to use other existing datasets or create their own resources for training in order to improve their systems. E"
2021.americasnlp-1.23,L16-1666,1,0.700987,"ng similarities and differences between training sets on the one hand and development and test sets on the other. The training data (Mager et al., 2018a) is a translation of the fairy tales of Hans Christian Andersen and contains word acquisitions and code-switching. Spanish–Nahuatl Nahuatl is a Yuto-Aztecan language spoken in Mexico and El Salvador, with a wide dialectal variation (around 30 variants). For each main dialect a specific ISO 639-3 code is available.2 There is a lack of consensus regarding the orthographic standard. This is very noticeable in the training data: the train corpus (Gutierrez-Vasques et al., 2016) has dialectal, domain, orthographic and diachronic variation (Nahuatl side). However, the majority of entries are closer to a Classical Nahuatl orthographic “standard”. The development and test datasets were translated to modern Nahuatl. In particular, the translations belong to Nahuatl Central/Nahuatl de la Huasteca (Hidalgo y San Luis Potosí) dialects. In order to be closer to the training corpus, an orthographic normalization was applied. A simple rule based approach was used, which was based on the most predictable orthographic changes between modern varieties and Classical Nahuatl. Spani"
2021.americasnlp-1.23,D19-1632,1,0.923833,"nish, Aymara–Spanish, Guarani–Spanish, and Bribri–Spanish. For development and testing, we used parallel sentences belonging to a new natural language inference dataset for the 10 indigenous languages featured in our shared task, which is a manual translation of the Spanish version of the multilingual XNLI dataset (Conneau et al., 2018). For a complete description of this dataset we refer the reader to Ebrahimi et al. (2021). Together with the data, we also provided: a simple baseline based on the small transformer architecture (Vaswani et al., 2017) proposed together with the FLORES dataset (Guzmán et al., 2019); and a description of challenges and particular characteristics for all provided resources1 . We established two tracks: one where training models on the development set after hyperparameter tuning is 1 https://github.com/AmericasNLP/americasnlp2021/ blob/main/data/information_datasets.pdf 202 Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas, pages 202–217 June 11, 2021. ©2021 Association for Computational Linguistics allowed (Track 1), and one where models cannot be trained directly on the development set (Track 2). Machine translation"
2021.americasnlp-1.23,2021.americasnlp-1.25,0,0.125338,"dded to the existing dataset. Then, a normalization process was done using existing tools, and the aligned data was further filtered. The quality of the data was also considered, and each dataset was assigned a weight depending on a noisiness estimation. The team used a transformer sequenceto-sequence model trained via two steps. For their main submission they first trained on data which 4 Otomí online corpus: https://tsunkua.elotl.mx/about/ 206 https://github.com/AmericasNLP/americasnlp2021 Team Langs. Sub. CoAStaL (Bollmann et al., 2021) 10 20 Helsinki (Vázquez et al., 2021) 10 50 NRC-CNRC (Knowles et al., 2021) 4 17 REPUcs (Moreno, 2021) Tamalli (Parida et al., 2021) 1 2 10 UBC-NLP (BillahNagoudi et al., 2021) UTokyo (Zheng et al., 2021) Anonymous Data Models Multilingual Pretrained PB-SMT, Constrained Random Strings Transformer NMT No No 42 Bible, JW300, OPUS, Wikipedia, New collected data Bible, OPUS, Constitutions, Normalization, Filtering, BackTranslation No external data, preoricessing, BPE Dropout. JW300, New dataset, Europarl - 8 29 Bible, Wikipedia 10 40 8 14 Monolingual from other languages. Data - Yes, all ST No languages + Spanish-English Transofrmer NMT Yes, languages 4- No Transformer N"
2021.americasnlp-1.23,P07-2045,0,0.0229964,"lcoxon signed-ranked test and Pitman’s permutation test with p&lt;0.05 (Dror et al., 2018). 208 ious high-resource languages, and then finetuned for each target language using the official provided data. 4.7 NRC-CNRC The team of the National Research Council Canada (NRC-CNRC; Knowles et al., 2021) submitted systems for the Spanish to Wixárika, Nahuatl, Rarámuri and Guarani language pairs for both tracks. Due to ethical considerations, the team decided not to use external data, and restricted themselves to the data provided for the shared task. All data was preprocessed with standard Moses tools (Koehn et al., 2007). The submitted systems were based on a Transformer model, and used BPE for tokenization. The team experimented with multilingual models pretrained on either 3 or 4 languages, finding that the 4 language model achieved higher performance. Additionally the team trained a Translation Memory (Simard and Fujita, 2012) using half of the examples of the development set. Surprisingly, even given its small amount of training data, this system outperformed the team’s Track 2 submission for Rarámuri. 4.8 Tamalli The team Tamalli7 (Parida et al., 2021) participated in Track 1 for all 10 language pairs. T"
2021.americasnlp-1.23,D18-2012,0,0.0268314,"encoders using UD annotations could not get any meaningful results. 4.5 REPUcs the the Spanish–Quechua language pair in both tracks. The team collected external data from 3 different sources and analyzed the domain disparity between this training data and the development set. To solve the problem of domain mismatch, they decided to collect additional data that could be a better match for the target domain. The used data from a handbook (Iter and Ortiz-Cárdenas, 2019), a lexicon,5 and poems on the web (Duran, 2010).6 Their model is a transformer encoder-decoder architecture with SentencePiece (Kudo and Richardson, 2018) tokenization. Together with the existing parallel corpora, the new paired data was used for finetuning on top of a pretrained Spanish–English translation model. The team submitted two versions of their system: the first was only finetuned on JW300+ data, while the second one additionally leveraged the newly collected dataset. 4.6 UTokyo The team of the University of Tokyo (UTokyo; Zheng et al., 2021) submitted systems for all languages and both tracks. A multilingual pretrained encoder-decoder model (mBART; Liu et al., 2020) was used, implemented with the Fairseq toolkit (Ott et al., 2019). T"
2021.americasnlp-1.23,C18-1006,1,0.909443,"ms achieved 12.97 ChrF higher than baseline, when averaged across languages. 1 Introduction Many of the world’s languages, including languages native to the Americas, receive worryingly little attention from NLP researchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; Barrault et al., 2020). ∗ *The first three authors contributed equally. The AmericasNLP 2021 Shared Task on Open Machine Translation (OMT) aimed at moving research on indigenous and endangered languages more into the focus of the NLP community. As the official shared task training sets, we provided a collection of publicly available para"
2021.americasnlp-1.23,mayer-cysouw-2014-creating,0,0.0200209,"to the Valle del Mezquital dialect (ote). This was specially challenging for the translation task, since the development and test sets are from the Ñûhmû de Ixtenco, Tlaxcala, variant (otz), which also has its own orthographic system. This variant is especially endangered as less than 100 elders still speak it. 3.3 External Data Used by Participants In addition to the provided datasets, participants also used additional publicly available parallel data, monolingual corpora or newly collected data sets. The most common datasets were JW300 (Agi´c and Vuli´c, 2019) and the Bible’s New Testament (Mayer and Cysouw, 2014; Christodouloupoulos and Steedman, 2015; McCarthy et al., 2020). Besides those, GlobalVoices (Prokopidis et al., 2016) and datasets available at OPUS (Tiedemann, 2012) were added. New datasets were extracted from constitutions, dictionaries, and educational books. For monolingual text, Wikipedia was most commonly used, assuming one was available in a language. 3 4 Baseline and Submitted Systems We will now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a"
2021.americasnlp-1.23,2020.lrec-1.352,0,0.0111319,"llenging for the translation task, since the development and test sets are from the Ñûhmû de Ixtenco, Tlaxcala, variant (otz), which also has its own orthographic system. This variant is especially endangered as less than 100 elders still speak it. 3.3 External Data Used by Participants In addition to the provided datasets, participants also used additional publicly available parallel data, monolingual corpora or newly collected data sets. The most common datasets were JW300 (Agi´c and Vuli´c, 2019) and the Bible’s New Testament (Mayer and Cysouw, 2014; Christodouloupoulos and Steedman, 2015; McCarthy et al., 2020). Besides those, GlobalVoices (Prokopidis et al., 2016) and datasets available at OPUS (Tiedemann, 2012) were added. New datasets were extracted from constitutions, dictionaries, and educational books. For monolingual text, Wikipedia was most commonly used, assuming one was available in a language. 3 4 Baseline and Submitted Systems We will now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a transformer-based sequence to sequence model (Vaswani et al., 20"
2021.americasnlp-1.23,nordhoff-hammarstrom-2012-glottolog,0,0.0139496,"manually translated sentences for the development and test sets. An official baseline trained on this data was also provided. Team submissions featured a variety of architectures, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the baseline. The best performing systems achieved 12.97 ChrF higher than baseline, when averaged across languages. 1 Introduction Many of the world’s languages, including languages native to the Americas, receive worryingly little attention from NLP researchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; B"
2021.americasnlp-1.23,2020.loresmt-1.1,1,0.864016,"earchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; Barrault et al., 2020). ∗ *The first three authors contributed equally. The AmericasNLP 2021 Shared Task on Open Machine Translation (OMT) aimed at moving research on indigenous and endangered languages more into the focus of the NLP community. As the official shared task training sets, we provided a collection of publicly available parallel corpora (§3). Additionally, all participants were allowed to use other existing datasets or create their own resources for training in order to improve their systems. Each language pair used in the shared task c"
2021.americasnlp-1.23,N19-4009,1,0.919181,"acted from constitutions, dictionaries, and educational books. For monolingual text, Wikipedia was most commonly used, assuming one was available in a language. 3 4 Baseline and Submitted Systems We will now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a transformer-based sequence to sequence model (Vaswani et al., 2017). We employed the hyperparameters proposed by Guzmán et al. (2019) for a low-resource scenario. We implemented the model using Fairseq (Ott et al., 2019). The implementation of the baseline can be found in the official shared task repository.4 4.2 University of British Columbia The team of the University of British Columbia (UBC-NLP; Billah-Nagoudi et al., 2021) participated for all ten language pairs and in both tracks. They used an encoder-decoder transformer model based on T5 (Raffel et al., 2020). This model was pretrained on a dataset consisting of 10 indigenous languages and Spanish, that was collected by the team from different sources such as the Bible and Wikipedia, totaling 1.17 GB of text. However, given that some of the languages h"
2021.americasnlp-1.23,P02-1040,0,0.110098,"ee to use any resources they could find. Possible resources could, for instance, include existing or newly created parallel data, dictionaries, tools, or pretrained models. We invited submissions to two different tracks: Systems in Track 1 were allowed to use the development set as part of the training data, since this is a common practice in the machine translation community. Systems in Track 2 were not allowed to be trained directly on the development set, mimicking a more realistic low-resource setting. 2.2 resulting in a small number of words per sentence. We further reported BLEU scores (Papineni et al., 2002) for all systems and languages. Adequacy The output sentence expresses the meaning of the reference. 1. Extremely bad: The original meaning is not contained at all. 2. Bad: Some words or phrases allow to guess the content. 3. Neutral. 4. Sufficiently good: The original meaning is understandable, but some parts are unclear or incorrect. 5. Excellent: The meaning of the output is the same as that of the reference. Fluency The output sentence is easily readable and looks like a human-produced text. Primary Evaluation In order to be able to evaluate a large number of systems on all 10 languages, w"
2021.americasnlp-1.23,2021.americasnlp-1.24,0,0.0516404,"Missing"
2021.americasnlp-1.23,W15-3049,0,0.0738763,"Missing"
2021.americasnlp-1.23,L16-1144,0,0.161871,"2019), which consists of Jehovah’s Witness texts, sentences extracted from the official dictionary of the Minister of Education (MINEDU), and miscellaneous dictionary entries and samples which have been collected and reviewed by Huarcaya Taquiri (2020). Spanish–Aymara Aymara is a Aymaran language spoken in Bolivia, Peru, and Chile (ISO codes aym, ayr, ayc). The development and test sets are translated into the Central Aymara variant (ayr), specifically Aymara La Paz jilata, the largest variant. This is similar to the variant of the available training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al.,"
2021.americasnlp-1.23,2012.amta-papers.26,0,0.0148085,"ems for the Spanish to Wixárika, Nahuatl, Rarámuri and Guarani language pairs for both tracks. Due to ethical considerations, the team decided not to use external data, and restricted themselves to the data provided for the shared task. All data was preprocessed with standard Moses tools (Koehn et al., 2007). The submitted systems were based on a Transformer model, and used BPE for tokenization. The team experimented with multilingual models pretrained on either 3 or 4 languages, finding that the 4 language model achieved higher performance. Additionally the team trained a Translation Memory (Simard and Fujita, 2012) using half of the examples of the development set. Surprisingly, even given its small amount of training data, this system outperformed the team’s Track 2 submission for Rarámuri. 4.8 Tamalli The team Tamalli7 (Parida et al., 2021) participated in Track 1 for all 10 language pairs. The team used an IBM Model 2 for SMT, and a transformer model for NMT. The team’s NMT models were trained in two settings: one-to-one, with one model being trained per target language, and one-to-many, where decoder weights were shared across languages and a language embedding layer was added to the decoder. They s"
2021.americasnlp-1.23,tiedemann-2012-parallel,0,0.274715,"sentences extracted from the official dictionary of the Minister of Education (MINEDU), and miscellaneous dictionary entries and samples which have been collected and reviewed by Huarcaya Taquiri (2020). Spanish–Aymara Aymara is a Aymaran language spoken in Bolivia, Peru, and Chile (ISO codes aym, ayr, ayc). The development and test sets are translated into the Central Aymara variant (ayr), specifically Aymara La Paz jilata, the largest variant. This is similar to the variant of the available training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al., 1993). Translated text was created by a"
2021.americasnlp-1.23,2021.americasnlp-1.29,0,0.0275308,"rmer model based on T5 (Raffel et al., 2020). This model was pretrained on a dataset consisting of 10 indigenous languages and Spanish, that was collected by the team from different sources such as the Bible and Wikipedia, totaling 1.17 GB of text. However, given that some of the languages have more available data than others, this dataset is unbalanced in favor of languages like Nahuatl, Guarani, and Quechua. The team also proposed a two-stage fine-tuning method: first fine-tuning on the entire dataset, and then only on the target languages. 4.3 Helsinki The University of Helsinki (Helsinki; Vázquez et al., 2021) participated for all ten language pairs in both tracks. This team did an extensive exploration of the existing datasets, and collected additional resources both from commonly used sources such as the Bible and Wikipedia, as well as other minor sources such as constitutions. Monolingual data was used to generate paired sentences through back-translation, and these parallel examples were added to the existing dataset. Then, a normalization process was done using existing tools, and the aligned data was further filtered. The quality of the data was also considered, and each dataset was assigned"
2021.americasnlp-1.23,2020.emnlp-main.43,0,0.0550595,"Missing"
2021.americasnlp-1.23,2021.americasnlp-1.26,0,0.0686019,"d. The quality of the data was also considered, and each dataset was assigned a weight depending on a noisiness estimation. The team used a transformer sequenceto-sequence model trained via two steps. For their main submission they first trained on data which 4 Otomí online corpus: https://tsunkua.elotl.mx/about/ 206 https://github.com/AmericasNLP/americasnlp2021 Team Langs. Sub. CoAStaL (Bollmann et al., 2021) 10 20 Helsinki (Vázquez et al., 2021) 10 50 NRC-CNRC (Knowles et al., 2021) 4 17 REPUcs (Moreno, 2021) Tamalli (Parida et al., 2021) 1 2 10 UBC-NLP (BillahNagoudi et al., 2021) UTokyo (Zheng et al., 2021) Anonymous Data Models Multilingual Pretrained PB-SMT, Constrained Random Strings Transformer NMT No No 42 Bible, JW300, OPUS, Wikipedia, New collected data Bible, OPUS, Constitutions, Normalization, Filtering, BackTranslation No external data, preoricessing, BPE Dropout. JW300, New dataset, Europarl - 8 29 Bible, Wikipedia 10 40 8 14 Monolingual from other languages. Data - Yes, all ST No languages + Spanish-English Transofrmer NMT Yes, languages 4- No Transformer NMT. WB-SMT. Transformer NMT, Transformer T5 Yes, with Spanish-English 10-languages Spanish-English pretraining No 10-Languages Ne"
2021.blackboxnlp-1.3,2020.acl-main.656,0,0.0241345,"adiction neutral A dog is a form of animal, for there to be snow it must be cold weather outside, and &quot;jumping for a frisbee'' is a rephrasing of &quot;playing with a plastic toy.'' Figure 1: NLI example instance from e-SNLI (Camburu et al., 2018). The system needs to include commonsense knowledge, such as “snow → cold weather”. end-users. The former can use the insights to improve models and the latter can base their decision on them whether to trust the system or not. One approach to gain insight into a system is to train it to generate explanations as an additional output (Camburu et al., 2018; Atanasova et al., 2020). Such selfexplaining models are particularly interesting for NLI because the explanation can indicate the commonsense knowledge which was utilized during prediction. The integration of external knowledge was shown to improve NLI systems (Jijkoun and de Rijke, 2005; Chen et al., 2018; Li et al., 2019; Faldu et al., 2021). However, the following question remains: Does the positive effect of external knowledge on the inference ability transfer to the generation of explanations? (R1) Figure 1 shows an NLI example for which external knowledge potentially helps to infer the correct label and explan"
2021.blackboxnlp-1.3,2021.eacl-main.202,0,0.145899,"ions and the ground truth explanations (Camburu et al., 2018; Kumar and Talukdar, 2020; Narang et al., 2020). BLEU scores can only quantify explanation quality loosely (Narang et al., 2020). Therefore, previous work evaluates explanation quality either by manual annotation (Camburu et al., 2018; Kumar and Talukdar, 2020) or crowdsourcing (Narang et al., 2020). However, previous human evaluations regarding explainable NLI are limited to assess label and/or explanation correctness. In contrast, we additionally evaluate commonsense inclusion as well as grammatical correctness of explanations. As Clinciu et al. (2021) find automatic BLEURT scores to have distinctly stronger correlations to human ratings of generated textual explanations than BLEU, we investigate whether BLEURT is a viable replacement for a user study. 2 We pass inputs of the form [CLS] premise [SEP] hypothesis to BERT and use a softmax layer on top of the CLS token’s embedding to predict the entailment label and fine-tune the model for up to 2 epochs. 28 antecedent Kpi ,hj ∧ Api ,hj in R2 is a conjunctive condition that becomes true if a word pair is both in a relation and aligned by a model’s original attention. If such a conjunctive cond"
2021.blackboxnlp-1.3,2021.deelio-1.7,0,0.0308212,"ng et al. (2019) show the potential of knowledge from ConceptNet for NLI systems. Li et al. (2019) find that external knowledge from pretraining helps NLI and suggest to combine it with external knowledge from human-curated resources. Li and Sethy (2019) propose knowledge-enhanced attention modifications for Transformers and decomposable methods and show that their methods improve model robustness. Faldu et al. (2021) extend BERT by extracting entities from the input text and adding their projected KG embeddings derived from ConceptNet and WordNet as sequential input to a modified BERT layer. Bauer et al. (2021) present ERNIE-NLI, a modified ERNIE (Zhang et al., 2019) model using NLI-specific knowledge embeddings and find that it improves performance over a non-adapted ERNIE model using generaldomain TransE embeddings. To compare different possibilities of integrating external knowledge, we propose various models in this paper. Further, we address the question whether external knowledge also improves explanation generation. To evaluate NLI models, mainly automatic measures, such as accuracy, are used. However, model weaknesses can stay unnoticed using automatic scores alone. Moreover, Schuff et al. ("
2021.blackboxnlp-1.3,N19-1423,0,0.155615,"s the positive effect of external knowledge on the inference ability transfer to the generation of explanations? (R1) Figure 1 shows an NLI example for which external knowledge potentially helps to infer the correct label and explanation. In the example, the system needs to link dog to animal, jumping for a Frisbee to playing, Frisbee to plastic toy and snow to outside as well as to cold weather. The predicted explanation needs to explicitly state this reasoning chain and thus would be expected to benefit from external knowledge. Especially recently, pre-trained language models, such as BERT (Devlin et al., 2019) or GPT (RadIntroduction Natural language inference (NLI) is closely related to real-world applications, such as fact checking. Given two sentences (premise and hypothesis), the task is to decide whether (a) the first sentence entails the second sentence, (b) the two sentences contradict each other or (c) they have a neutral relation. Figure 1 shows an example for an entailment relation. Solving the task requires models to not only reason over the provided information but also to link it with commonsense knowledge. As for other natural language tasks, state-of-theart NLI systems rely on deep n"
2021.blackboxnlp-1.3,P19-1470,0,0.15759,"nowledge of language models compared to symbolic sources of knowledge, such as knowledge base triplets? (RQ2) able AI. To facilitate future work, we release our model’s predictions as well as the crowdsourced human ratings.1 2 2.1 Related Work External Knowledge for NLI External knowledge was shown to help across a wide variety of NLP tasks (Shi et al., 2016; Seyler et al., 2018; Pan et al., 2019; Lin et al., 2019). While early sources for external knowledge are WordNet and NomBank (Jijkoun and de Rijke, 2005; MacCartney et al., 2008), today a large variety of possibilities exist: From COMET (Bosselut et al., 2019) over ConceptNet (Speer et al., 2017) to language models. Chen et al. (2018) show that enriching an NLI system with external lexical-level semantic knowledge increases accuracy scores on SNLI and enhances transfer to MultiNLI. Wang et al. (2019) show the potential of knowledge from ConceptNet for NLI systems. Li et al. (2019) find that external knowledge from pretraining helps NLI and suggest to combine it with external knowledge from human-curated resources. Li and Sethy (2019) propose knowledge-enhanced attention modifications for Transformers and decomposable methods and show that their met"
2021.blackboxnlp-1.3,D15-1075,0,0.0469134,"plainable-nli 27 3 text spans or snippets from the input or external text (Zaidan and Eisner, 2008; Lei et al., 2016; Yang et al., 2018). Beyond that, there exists various resources and approaches designed to generate textual explanations. Rajani et al. (2019) present a dataset that contains free-text explanations for multiple-choice commonsense reasoning and Bhagavatula et al. (2020) provide a dataset for abductive multiple choice answering as well as abductive NLG. Camburu et al. (2018) provide the e-SNLI dataset, which adds free-text explanations as an additional layer on the SNLI dataset (Bowman et al., 2015). As numerous models with and without external knowledge have been developed on the SNLI dataset, we use its explainable extension e-SNLI to conduct our analysis and train our models on. Various models have been proposed on e-SNLI including systems based on alignment (Swanson et al., 2020), label-specific explanation generators (Kumar and Talukdar, 2020) and fine-tuned textto-text models (Narang et al., 2020). In contrast to those, our focus is not on proposing a new architecture or paradigm to develop a high-scoring system. Much more, we seek to conduct a broad comparison across knowledge sou"
2021.blackboxnlp-1.3,2020.acl-main.771,0,0.378314,"soning and Bhagavatula et al. (2020) provide a dataset for abductive multiple choice answering as well as abductive NLG. Camburu et al. (2018) provide the e-SNLI dataset, which adds free-text explanations as an additional layer on the SNLI dataset (Bowman et al., 2015). As numerous models with and without external knowledge have been developed on the SNLI dataset, we use its explainable extension e-SNLI to conduct our analysis and train our models on. Various models have been proposed on e-SNLI including systems based on alignment (Swanson et al., 2020), label-specific explanation generators (Kumar and Talukdar, 2020) and fine-tuned textto-text models (Narang et al., 2020). In contrast to those, our focus is not on proposing a new architecture or paradigm to develop a high-scoring system. Much more, we seek to conduct a broad comparison across knowledge sources and isolate their effect on automatic scores as well as human perception. 2.3 Method In the following, we describe our base model and then present the models we analyze. 3.1 Base Model We combine a state-of-the-art attention-based inference model with an explainable NLI model that predicts entailment labels and generates explanations. In particular,"
2021.blackboxnlp-1.3,D16-1011,0,0.0684705,"Missing"
2021.blackboxnlp-1.3,D08-1084,0,0.0751037,"Missing"
2021.blackboxnlp-1.3,D19-1410,0,0.0143047,"R2 is a conjunctive condition that becomes true if a word pair is both in a relation and aligned by a model’s original attention. If such a conjunctive condition is true, the word pair must be aligned which results in a new alignment as the consequent A0pi ,hj indicates. find that chunking them into noun and verb subphrases based on POS tags patterns yields better object phrase generations.3 Thus, for each sentence (premise/hypothesis) we generate #chunks × #relations object phrases.4 Afterwards, we embed each object phrase (with the respective relation string prependend) with Sentence-BERT (Reimers and Gurevych, 2019) and quantify its similarity to the embedding of the source sentence using cosine similarity. For each relation, we keep the object phrase with the highest similarity score. Given the relation HasA and the chunked sentence The dog |is walking in the snow, for example, COMET will generate bone and effect of freeze for the two sub-phrases, respectively. We only preserve the object phrase effect of freeze as it has a higher similarity to the source sentence. To condense the object phrases into a fixedlength vector representation, we average the respective Sentence-BERT embeddings. This procedure"
2021.blackboxnlp-1.3,C18-1198,0,0.0253604,"orse than all other models. 5 Human Evaluation While automatic scores, such as BLEU, provide a valuable starting point for evaluating explanations, they fall short in capturing the model’s real explanation capabilities. We, therefore, conduct a large-scale crowdsourcing study to complement our automatic evaluations on e-SNLI and the stress tests. Following related work (Narang et al., 2020), we assess explanation quality based on ratings from crowdworkers on Mechanical Turk. While previStress Test Evaluation Table 3 shows the results of our models on the NLI stress test evaluation proposed by Naik et al. (2018). The dataset contains multiple subsets of which each subset is used to evaluate the robustness of the system against a specific type of perturbation, 31 Competence Test non-LM Type Noise Test Model Total Antonymy Numerical Word Overlap Length Mismatch Negation Spelling PRED - EXPL 48.69 56.94 57.05 57.09 56.26 36.36 37.94 34.54 32.50 44.43 36.55 32.24 35.48 40.28 34.16 47.17 55.46 57.31 52.10 51.34 53.44 65.21 64.15 64.35 64.39 45.31 52.03 52.85 53.38 49.36 52.42 62.90 62.33 62.77 63.03 52.74 59.28 59.19 58.99 51.81 54.84 37.97 52.53 31.33 28.80 34.03 28.54 55.91 64.06 58.13 63.70 60.97 68.72"
2021.blackboxnlp-1.3,D19-5804,0,0.0176961,"et al., 2019), became popular. It was shown that they are able to learn and store commonsense knowledge implicitly (Petroni et al., 2019). As a result, an open question is: How effective is the implicit commonsense knowledge of language models compared to symbolic sources of knowledge, such as knowledge base triplets? (RQ2) able AI. To facilitate future work, we release our model’s predictions as well as the crowdsourced human ratings.1 2 2.1 Related Work External Knowledge for NLI External knowledge was shown to help across a wide variety of NLP tasks (Shi et al., 2016; Seyler et al., 2018; Pan et al., 2019; Lin et al., 2019). While early sources for external knowledge are WordNet and NomBank (Jijkoun and de Rijke, 2005; MacCartney et al., 2008), today a large variety of possibilities exist: From COMET (Bosselut et al., 2019) over ConceptNet (Speer et al., 2017) to language models. Chen et al. (2018) show that enriching an NLI system with external lexical-level semantic knowledge increases accuracy scores on SNLI and enhances transfer to MultiNLI. Wang et al. (2019) show the potential of knowledge from ConceptNet for NLI systems. Li et al. (2019) find that external knowledge from pretraining hel"
2021.blackboxnlp-1.3,2020.emnlp-main.575,1,0.719359,"r et al. (2021) present ERNIE-NLI, a modified ERNIE (Zhang et al., 2019) model using NLI-specific knowledge embeddings and find that it improves performance over a non-adapted ERNIE model using generaldomain TransE embeddings. To compare different possibilities of integrating external knowledge, we propose various models in this paper. Further, we address the question whether external knowledge also improves explanation generation. To evaluate NLI models, mainly automatic measures, such as accuracy, are used. However, model weaknesses can stay unnoticed using automatic scores alone. Moreover, Schuff et al. (2020) showed that automatic scores are not necessarily correlated to human-perceived model quality. Thus, human evaluation is a crucial step in the development of user-centered explainable AI systems. Therefore, we ask the question: How do humans perceive explanation quality of state-of-the-art natural language inference models? (R3) In this paper, we investigate the three previously mentioned research questions R1–R3. To answer them, we analyze the impact of external knowledge from multiple sources, such as knowledge graphs, embeddings and language models and propose novel architectures to include"
2021.blackboxnlp-1.3,P02-1040,0,0.11154,"a natural language knowledge base to query background knowledge for premise and hypothesis. COMET is based on a transformer language model that is fine-tuned on a knowledge base completion task on ConceptNet. Given an input sentence and a ConceptNet relation, it generates a phrase to complete the object in a knowledge statement expressed in the (subject, relation, object) format. Instead of feeding in the whole premise and hypothesis, we Explainable NLI system performance is typically scored using (i) accuracy with respect to annotated gold labels on a reference dataset and (ii) BLEU scores (Papineni et al., 2002) between the generated explanations and the ground truth explanations (Camburu et al., 2018; Kumar and Talukdar, 2020; Narang et al., 2020). BLEU scores can only quantify explanation quality loosely (Narang et al., 2020). Therefore, previous work evaluates explanation quality either by manual annotation (Camburu et al., 2018; Kumar and Talukdar, 2020) or crowdsourcing (Narang et al., 2020). However, previous human evaluations regarding explainable NLI are limited to assess label and/or explanation correctness. In contrast, we additionally evaluate commonsense inclusion as well as grammatical c"
2021.blackboxnlp-1.3,2020.acl-main.704,0,0.0142249,"between predicted and ground truth explanation texts. Higher values are better. of the T5 language model (Raffel et al., 2020). We train all non-LM models with five random seeds and report scores of the median model based on label accuracy. Table 2 shows predicted explanations for the subset of models that we investigate within the human evaluation in Section 5. Further examples are provided in the appendix. 4.1 Performance on e-SNLI Following prior work on e-SNLI, we report label accuracy as well as BLEU scores (Papineni et al., 2002) for explanations. We additionally evaluate BLEURT scores (Sellam et al., 2020), which is a reference-based learned evaluation metric to model human judgements of text generation. BLEURT is of particular interest for explanation evaluation as Clinciu et al. (2021) compare how various automatic scores such as BLEU, ROUGE and METEOR correlate to human ratings of generated explanations and find that embedding-based methods and particularly BLEURT scores show distinctly higher correlations than, e.g., BLEU. Table 1 shows the respective scores for all considered models.5 The upper block lists models that share or extend the PRED - EXPL architecture. Compared to PRED - EXPL, t"
2021.blackboxnlp-1.3,D19-1250,0,0.0292757,"to link it with commonsense knowledge. As for other natural language tasks, state-of-theart NLI systems rely on deep neural architectures which do not easily expose their inner workings. However, following a model’s reasoning process is valuable to machine learning engineers as well as 26 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 26–41 Online, November 11, 2021. ©2021 Association for Computational Linguistics ford et al., 2019), became popular. It was shown that they are able to learn and store commonsense knowledge implicitly (Petroni et al., 2019). As a result, an open question is: How effective is the implicit commonsense knowledge of language models compared to symbolic sources of knowledge, such as knowledge base triplets? (RQ2) able AI. To facilitate future work, we release our model’s predictions as well as the crowdsourced human ratings.1 2 2.1 Related Work External Knowledge for NLI External knowledge was shown to help across a wide variety of NLP tasks (Shi et al., 2016; Seyler et al., 2018; Pan et al., 2019; Lin et al., 2019). While early sources for external knowledge are WordNet and NomBank (Jijkoun and de Rijke, 2005; MacCa"
2021.blackboxnlp-1.3,P18-2039,0,0.0198156,"onal Linguistics ford et al., 2019), became popular. It was shown that they are able to learn and store commonsense knowledge implicitly (Petroni et al., 2019). As a result, an open question is: How effective is the implicit commonsense knowledge of language models compared to symbolic sources of knowledge, such as knowledge base triplets? (RQ2) able AI. To facilitate future work, we release our model’s predictions as well as the crowdsourced human ratings.1 2 2.1 Related Work External Knowledge for NLI External knowledge was shown to help across a wide variety of NLP tasks (Shi et al., 2016; Seyler et al., 2018; Pan et al., 2019; Lin et al., 2019). While early sources for external knowledge are WordNet and NomBank (Jijkoun and de Rijke, 2005; MacCartney et al., 2008), today a large variety of possibilities exist: From COMET (Bosselut et al., 2019) over ConceptNet (Speer et al., 2017) to language models. Chen et al. (2018) show that enriching an NLI system with external lexical-level semantic knowledge increases accuracy scores on SNLI and enhances transfer to MultiNLI. Wang et al. (2019) show the potential of knowledge from ConceptNet for NLI systems. Li et al. (2019) find that external knowledge fr"
2021.blackboxnlp-1.3,W18-6319,0,0.0123086,"ther, we include two recent models proposed for e-SNLI: NILE:post-hoc, which is the highest performing model from Kumar and Talukdar (2020), and WT5-11B from Narang et al. (2020), which holds the current state-of-theart performance. While NILE:post-hoc is based on GPT-2 as well, WT5-11B is a fine-tuned version 5 For NILE:post-hoc (Kumar and Talukdar, 2020) and WT5-11B (Narang et al., 2020) we report the label accuracy from their paper and calculate BLEU/BLEURT scores based on the explanation predictions provided by the authors. Narang et al. (2020) calculate BLEU scores using SacreBLEU v1.3. (Post, 2018) leading to a higher reported score of 33.7. 30 Model Predicted Explanation GROUND - a man is either playing the accordion or performs a mime act while happy people pass by or angry people glare at him. a man can not be playing and a mime at the same time the man is either playing the accordion or a mime people can not be playing and angry at the same time the man can not be playing the accordion and the mime at the same time Happy people are not angry people. The man cannot be playing the accordion and performing a mime act at the same time. TRUTH VANILLA COMET CONT COMET + CONT GPT- LF WT 5-"
2021.blackboxnlp-1.3,2020.acl-main.496,0,0.013224,"ins free-text explanations for multiple-choice commonsense reasoning and Bhagavatula et al. (2020) provide a dataset for abductive multiple choice answering as well as abductive NLG. Camburu et al. (2018) provide the e-SNLI dataset, which adds free-text explanations as an additional layer on the SNLI dataset (Bowman et al., 2015). As numerous models with and without external knowledge have been developed on the SNLI dataset, we use its explainable extension e-SNLI to conduct our analysis and train our models on. Various models have been proposed on e-SNLI including systems based on alignment (Swanson et al., 2020), label-specific explanation generators (Kumar and Talukdar, 2020) and fine-tuned textto-text models (Narang et al., 2020). In contrast to those, our focus is not on proposing a new architecture or paradigm to develop a high-scoring system. Much more, we seek to conduct a broad comparison across knowledge sources and isolate their effect on automatic scores as well as human perception. 2.3 Method In the following, we describe our base model and then present the models we analyze. 3.1 Base Model We combine a state-of-the-art attention-based inference model with an explainable NLI model that pre"
2021.blackboxnlp-1.3,D18-1259,0,0.0267232,"easoning. In general, such explanation can take various forms, such as weights and gradients over the input (Simonyan et al., 2014; Ribeiro et al., 2016; Lundberg and Lee, 2017) and Our results urge caution to solely rely on automatic scores for explainability. Therefore, we expect our paper to motivate the development of dedicated evaluation tasks and scores and further emphasize the importance of the user within explain1 https://github.com/boschresearch/ external-knowledge-explainable-nli 27 3 text spans or snippets from the input or external text (Zaidan and Eisner, 2008; Lei et al., 2016; Yang et al., 2018). Beyond that, there exists various resources and approaches designed to generate textual explanations. Rajani et al. (2019) present a dataset that contains free-text explanations for multiple-choice commonsense reasoning and Bhagavatula et al. (2020) provide a dataset for abductive multiple choice answering as well as abductive NLG. Camburu et al. (2018) provide the e-SNLI dataset, which adds free-text explanations as an additional layer on the SNLI dataset (Bowman et al., 2015). As numerous models with and without external knowledge have been developed on the SNLI dataset, we use its explain"
2021.blackboxnlp-1.3,D08-1004,0,0.0289863,"hat allows the user to assess the model’s reasoning. In general, such explanation can take various forms, such as weights and gradients over the input (Simonyan et al., 2014; Ribeiro et al., 2016; Lundberg and Lee, 2017) and Our results urge caution to solely rely on automatic scores for explainability. Therefore, we expect our paper to motivate the development of dedicated evaluation tasks and scores and further emphasize the importance of the user within explain1 https://github.com/boschresearch/ external-knowledge-explainable-nli 27 3 text spans or snippets from the input or external text (Zaidan and Eisner, 2008; Lei et al., 2016; Yang et al., 2018). Beyond that, there exists various resources and approaches designed to generate textual explanations. Rajani et al. (2019) present a dataset that contains free-text explanations for multiple-choice commonsense reasoning and Bhagavatula et al. (2020) provide a dataset for abductive multiple choice answering as well as abductive NLG. Camburu et al. (2018) provide the e-SNLI dataset, which adds free-text explanations as an additional layer on the SNLI dataset (Bowman et al., 2015). As numerous models with and without external knowledge have been developed o"
2021.blackboxnlp-1.3,P19-1139,0,0.0169319,"ceptNet for NLI systems. Li et al. (2019) find that external knowledge from pretraining helps NLI and suggest to combine it with external knowledge from human-curated resources. Li and Sethy (2019) propose knowledge-enhanced attention modifications for Transformers and decomposable methods and show that their methods improve model robustness. Faldu et al. (2021) extend BERT by extracting entities from the input text and adding their projected KG embeddings derived from ConceptNet and WordNet as sequential input to a modified BERT layer. Bauer et al. (2021) present ERNIE-NLI, a modified ERNIE (Zhang et al., 2019) model using NLI-specific knowledge embeddings and find that it improves performance over a non-adapted ERNIE model using generaldomain TransE embeddings. To compare different possibilities of integrating external knowledge, we propose various models in this paper. Further, we address the question whether external knowledge also improves explanation generation. To evaluate NLI models, mainly automatic measures, such as accuracy, are used. However, model weaknesses can stay unnoticed using automatic scores alone. Moreover, Schuff et al. (2020) showed that automatic scores are not necessarily co"
2021.conll-1.1,P19-1651,0,0.0259803,"tal Models: (1a) How do participants perceive an AI dialog partner in a cooperative setting (e.g., trust, game engagement)? (1b) What types of mental models do participants form about an interactive AI dialog partner in a cooperative game? During interactions with a system, users’ mental models can be formed or changed (Rutjes et al., 2019). Here, XAI can support users in creating correct mental models about a system. In addition, explanations have the potential to increase trust and acceptance (Cramer et al., 2008). Although there has been increasing interest in collaborative dialog systems (Kim et al., 2019; Polyak et al., 2017; Danise and Striegnitz, 2012; van Waveren et al., 2019), to our knowledge there has only been one experiment which sought to understand how users form mental models in this setting. In their work, Gero et al. (2020) develop a word guessing game, where the human player must guess which word their partner is thinking of based off of one word clues over multiple games. In our study, we focus on mental models formed during a single longer interaction, rather than multiple short ones, and how a more complex dialog interaction affects the models formed. 2 2.1 • RQ2. Miscommunic"
2021.conll-1.1,2020.acl-demos.31,1,0.881591,"interactions with a system, users’ mental models can be formed or changed (Rutjes et al., 2019). Here, XAI can support users in creating correct mental models about a system. In addition, explanations have the potential to increase trust and acceptance (Cramer et al., 2008). Although there has been increasing interest in collaborative dialog systems (Kim et al., 2019; Polyak et al., 2017; Danise and Striegnitz, 2012; van Waveren et al., 2019), to our knowledge there has only been one experiment which sought to understand how users form mental models in this setting. In their work, Gero et al. (2020) develop a word guessing game, where the human player must guess which word their partner is thinking of based off of one word clues over multiple games. In our study, we focus on mental models formed during a single longer interaction, rather than multiple short ones, and how a more complex dialog interaction affects the models formed. 2 2.1 • RQ2. Miscommunication: (2a) Do participants’ attributes (e.g., age, gender, previous game experience) have an impact on their ability to successfully collaborate with an AI dialog partner? (2b) Which strategies do participants use to resolve miscommunic"
2021.conll-1.4,P19-3016,1,0.89205,"Missing"
2021.eacl-main.134,D18-1398,0,0.0213368,"and-labeled data whereas they suffer a persistent challenge of low-resource. The approach of learning with few samples, known as few-shot learning - a branch of meta-learning (learn to learn) - has recently been popularized (Fei-Fei et al., 2006; Ravi and Larochelle, 2016; Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018) in computer vision. Recently, few-shot learning has also been applied to NLP tasks, e.g. natural language understanding (Dou et al., 2019), text classification (Jiang et al., 2018; Rios and Kavuluru, 2018; Gao et al., 2019; Geng et al., 2019), machine translation (Gu et al., 2018) and relation classification (Obamuyide and Vlachos, 2019). In the slot tagging task, we aim at predicting taskspecific values (e.g. artist, time) for slots (placeholders) in user utterances. Oguz and Vu (2020) propose a two-stage modeling approach to exploit domain-agnostic features to tackle low-resource domain challenges. Besides, the other state of the art techniques e.g. based on external memory (Peng and Yao, 2015), ranking loss (Vu et al., 2016), encoder (Kurata et al., 2016), and attention (Zhu and Yu, 2017) have achieved promising results with a wide range of neural networks methods."
2021.eacl-main.134,W19-4828,0,0.014385,"rectional language modeling task and return the hidden states for the given input sequence. BERT (Devlin et al., 2019) uses a bidirectional transformer model that is trained on a masked language modeling task. Because of WordPiece embeddings (Wu et al., 2016), there are different choices of presenting words. We use the first subtoken for representing the word as proposed in (Devlin et al., 2019). Additionally, due to the structure of multiple successive layers, i.e., 24 layers and as suggested in (Oguz and Vu, 2020), we select 10th, 11th, 12th, and 13th as the focused layers on local context (Clark et al., 2019; Tenney et al., 2018) for slot tagging. 2.2 Meta-learning strategy Despite the fact that the proposed methods differ in their learning strategies, episode-based training is the same in meta-training and meta-testing phases for proposed meta models as mentioned in (Chen et al., 2018). For the purpose of applying episodic training in a robust way, we follow the proposed procedures in Vinyals et al. (2016); Snell Embedding Module Relation Module Turkey :: country Stuttgart :: city Matrix :: movie book :: object fφ gφ = album :: m_item Germany :: country Vector Concatenation Figure 1: One-shot tr"
2021.eacl-main.134,2020.acl-main.128,0,0.0269867,"posed to overcome this low-resource challenge using different techniques, e.g. multitask modeling (Jaech et al., 2016), adversarial training (Kim et al., 2017), and pointer networks (Zhai et al., 2017). In addition, studies like zero-shot learning has influenced the studies of the domain scaling problem for slots prediction (Bapna et al., 2017), eliminating the need of labeled examples for transferring reusable concepts (Zhu and Yu, 2018; Lee and Jha, 2019), and conveying the domainagnostic concepts between the intents (Shah et al., 2019) by exploiting label names and descriptions. Likewise, (Hou et al., 2020) use label semantics within a few-shot classification method TapNet (Yoon et al., 2019). We suggest using a small amount of annotated samples from different domains as training input instead of slot descriptions and slot names as in previos zero-shot (Bapna et al., 2017; Lee and Jha, 2019; Shah et al., 2019) and few-shot (Hou et al., 2020) slot tagging studies for two reasons: (1) The creation of slot descriptions needs qualified linguistic expertise and is thus expensive. (2) The relationship between slot names and the corresponding tokens is not constant. To give an example, the relationship"
2021.eacl-main.134,N19-1423,0,0.00695394,"relation networks (Sung et al., 2018) by learning to attend local and global features (Jetley et al., 2018). Experimental results on SNIPS data show that the proposed model outperforms other few-shot learning networks. 2 2.1 Methods Input FastText (Mikolov et al., 2018) is an approach to enrich the word vectors with a bag of character n-gram vectors. ELMo (Peters et al., 2018) is a contextualized word representation methods. It concatenates the output of two LSTM independently trained on the bidirectional language modeling task and return the hidden states for the given input sequence. BERT (Devlin et al., 2019) uses a bidirectional transformer model that is trained on a masked language modeling task. Because of WordPiece embeddings (Wu et al., 2016), there are different choices of presenting words. We use the first subtoken for representing the word as proposed in (Devlin et al., 2019). Additionally, due to the structure of multiple successive layers, i.e., 24 layers and as suggested in (Oguz and Vu, 2020), we select 10th, 11th, 12th, and 13th as the focused layers on local context (Clark et al., 2019; Tenney et al., 2018) for slot tagging. 2.2 Meta-learning strategy Despite the fact that the propos"
2021.eacl-main.134,D19-1403,0,0.0206217,"LP) applications with a large amount of hand-labeled data whereas they suffer a persistent challenge of low-resource. The approach of learning with few samples, known as few-shot learning - a branch of meta-learning (learn to learn) - has recently been popularized (Fei-Fei et al., 2006; Ravi and Larochelle, 2016; Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018) in computer vision. Recently, few-shot learning has also been applied to NLP tasks, e.g. natural language understanding (Dou et al., 2019), text classification (Jiang et al., 2018; Rios and Kavuluru, 2018; Gao et al., 2019; Geng et al., 2019), machine translation (Gu et al., 2018) and relation classification (Obamuyide and Vlachos, 2019). In the slot tagging task, we aim at predicting taskspecific values (e.g. artist, time) for slots (placeholders) in user utterances. Oguz and Vu (2020) propose a two-stage modeling approach to exploit domain-agnostic features to tackle low-resource domain challenges. Besides, the other state of the art techniques e.g. based on external memory (Peng and Yao, 2015), ranking loss (Vu et al., 2016), encoder (Kurata et al., 2016), and attention (Zhu and Yu, 2017) have achieved promising results with a"
2021.eacl-main.134,P17-1119,0,0.0164861,"state of the art techniques e.g. based on external memory (Peng and Yao, 2015), ranking loss (Vu et al., 2016), encoder (Kurata et al., 2016), and attention (Zhu and Yu, 2017) have achieved promising results with a wide range of neural networks methods. However as many other NLP applications, the low-resource issue is a tremendous challenge for slot tagging in new domains, although labeled samples exist in related domains. Many studies have recently proposed to overcome this low-resource challenge using different techniques, e.g. multitask modeling (Jaech et al., 2016), adversarial training (Kim et al., 2017), and pointer networks (Zhai et al., 2017). In addition, studies like zero-shot learning has influenced the studies of the domain scaling problem for slots prediction (Bapna et al., 2017), eliminating the need of labeled examples for transferring reusable concepts (Zhu and Yu, 2018; Lee and Jha, 2019), and conveying the domainagnostic concepts between the intents (Shah et al., 2019) by exploiting label names and descriptions. Likewise, (Hou et al., 2020) use label semantics within a few-shot classification method TapNet (Yoon et al., 2019). We suggest using a small amount of annotated samples"
2021.eacl-main.134,D16-1223,0,0.0291827,"ssification (Jiang et al., 2018; Rios and Kavuluru, 2018; Gao et al., 2019; Geng et al., 2019), machine translation (Gu et al., 2018) and relation classification (Obamuyide and Vlachos, 2019). In the slot tagging task, we aim at predicting taskspecific values (e.g. artist, time) for slots (placeholders) in user utterances. Oguz and Vu (2020) propose a two-stage modeling approach to exploit domain-agnostic features to tackle low-resource domain challenges. Besides, the other state of the art techniques e.g. based on external memory (Peng and Yao, 2015), ranking loss (Vu et al., 2016), encoder (Kurata et al., 2016), and attention (Zhu and Yu, 2017) have achieved promising results with a wide range of neural networks methods. However as many other NLP applications, the low-resource issue is a tremendous challenge for slot tagging in new domains, although labeled samples exist in related domains. Many studies have recently proposed to overcome this low-resource challenge using different techniques, e.g. multitask modeling (Jaech et al., 2016), adversarial training (Kim et al., 2017), and pointer networks (Zhai et al., 2017). In addition, studies like zero-shot learning has influenced the studies of the do"
2021.eacl-main.134,L18-1008,0,0.0224122,"used to learn meta-knowledge, whereas unseen labels from low-resource domains are used to evaluate the models. Furthermore, we propose a novel modeling approach - Attentive Relational Network, inspired by (Sung et al., 2018; Jiang et al., 2018; Jetley et al., 2018), that leverages contextual embeddings such as ELMO and BERT and extends the previous relation networks (Sung et al., 2018) by learning to attend local and global features (Jetley et al., 2018). Experimental results on SNIPS data show that the proposed model outperforms other few-shot learning networks. 2 2.1 Methods Input FastText (Mikolov et al., 2018) is an approach to enrich the word vectors with a bag of character n-gram vectors. ELMo (Peters et al., 2018) is a contextualized word representation methods. It concatenates the output of two LSTM independently trained on the bidirectional language modeling task and return the hidden states for the given input sequence. BERT (Devlin et al., 2019) uses a bidirectional transformer model that is trained on a masked language modeling task. Because of WordPiece embeddings (Wu et al., 2016), there are different choices of presenting words. We use the first subtoken for representing the word as prop"
2021.eacl-main.134,P19-1589,0,0.0262245,"ent challenge of low-resource. The approach of learning with few samples, known as few-shot learning - a branch of meta-learning (learn to learn) - has recently been popularized (Fei-Fei et al., 2006; Ravi and Larochelle, 2016; Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018) in computer vision. Recently, few-shot learning has also been applied to NLP tasks, e.g. natural language understanding (Dou et al., 2019), text classification (Jiang et al., 2018; Rios and Kavuluru, 2018; Gao et al., 2019; Geng et al., 2019), machine translation (Gu et al., 2018) and relation classification (Obamuyide and Vlachos, 2019). In the slot tagging task, we aim at predicting taskspecific values (e.g. artist, time) for slots (placeholders) in user utterances. Oguz and Vu (2020) propose a two-stage modeling approach to exploit domain-agnostic features to tackle low-resource domain challenges. Besides, the other state of the art techniques e.g. based on external memory (Peng and Yao, 2015), ranking loss (Vu et al., 2016), encoder (Kurata et al., 2016), and attention (Zhu and Yu, 2017) have achieved promising results with a wide range of neural networks methods. However as many other NLP applications, the low-resource i"
2021.eacl-main.134,2020.sustainlp-1.10,1,0.85537,"een popularized (Fei-Fei et al., 2006; Ravi and Larochelle, 2016; Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018) in computer vision. Recently, few-shot learning has also been applied to NLP tasks, e.g. natural language understanding (Dou et al., 2019), text classification (Jiang et al., 2018; Rios and Kavuluru, 2018; Gao et al., 2019; Geng et al., 2019), machine translation (Gu et al., 2018) and relation classification (Obamuyide and Vlachos, 2019). In the slot tagging task, we aim at predicting taskspecific values (e.g. artist, time) for slots (placeholders) in user utterances. Oguz and Vu (2020) propose a two-stage modeling approach to exploit domain-agnostic features to tackle low-resource domain challenges. Besides, the other state of the art techniques e.g. based on external memory (Peng and Yao, 2015), ranking loss (Vu et al., 2016), encoder (Kurata et al., 2016), and attention (Zhu and Yu, 2017) have achieved promising results with a wide range of neural networks methods. However as many other NLP applications, the low-resource issue is a tremendous challenge for slot tagging in new domains, although labeled samples exist in related domains. Many studies have recently proposed t"
2021.eacl-main.134,N18-1202,0,0.0344981,"Furthermore, we propose a novel modeling approach - Attentive Relational Network, inspired by (Sung et al., 2018; Jiang et al., 2018; Jetley et al., 2018), that leverages contextual embeddings such as ELMO and BERT and extends the previous relation networks (Sung et al., 2018) by learning to attend local and global features (Jetley et al., 2018). Experimental results on SNIPS data show that the proposed model outperforms other few-shot learning networks. 2 2.1 Methods Input FastText (Mikolov et al., 2018) is an approach to enrich the word vectors with a bag of character n-gram vectors. ELMo (Peters et al., 2018) is a contextualized word representation methods. It concatenates the output of two LSTM independently trained on the bidirectional language modeling task and return the hidden states for the given input sequence. BERT (Devlin et al., 2019) uses a bidirectional transformer model that is trained on a masked language modeling task. Because of WordPiece embeddings (Wu et al., 2016), there are different choices of presenting words. We use the first subtoken for representing the word as proposed in (Devlin et al., 2019). Additionally, due to the structure of multiple successive layers, i.e., 24 lay"
2021.eacl-main.134,D18-1352,0,0.0262284,"utilized in natural language processing (NLP) applications with a large amount of hand-labeled data whereas they suffer a persistent challenge of low-resource. The approach of learning with few samples, known as few-shot learning - a branch of meta-learning (learn to learn) - has recently been popularized (Fei-Fei et al., 2006; Ravi and Larochelle, 2016; Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018) in computer vision. Recently, few-shot learning has also been applied to NLP tasks, e.g. natural language understanding (Dou et al., 2019), text classification (Jiang et al., 2018; Rios and Kavuluru, 2018; Gao et al., 2019; Geng et al., 2019), machine translation (Gu et al., 2018) and relation classification (Obamuyide and Vlachos, 2019). In the slot tagging task, we aim at predicting taskspecific values (e.g. artist, time) for slots (placeholders) in user utterances. Oguz and Vu (2020) propose a two-stage modeling approach to exploit domain-agnostic features to tackle low-resource domain challenges. Besides, the other state of the art techniques e.g. based on external memory (Peng and Yao, 2015), ranking loss (Vu et al., 2016), encoder (Kurata et al., 2016), and attention (Zhu and Yu, 2017) h"
2021.eacl-main.134,P19-1547,0,0.0186995,"h labeled samples exist in related domains. Many studies have recently proposed to overcome this low-resource challenge using different techniques, e.g. multitask modeling (Jaech et al., 2016), adversarial training (Kim et al., 2017), and pointer networks (Zhai et al., 2017). In addition, studies like zero-shot learning has influenced the studies of the domain scaling problem for slots prediction (Bapna et al., 2017), eliminating the need of labeled examples for transferring reusable concepts (Zhu and Yu, 2018; Lee and Jha, 2019), and conveying the domainagnostic concepts between the intents (Shah et al., 2019) by exploiting label names and descriptions. Likewise, (Hou et al., 2020) use label semantics within a few-shot classification method TapNet (Yoon et al., 2019). We suggest using a small amount of annotated samples from different domains as training input instead of slot descriptions and slot names as in previos zero-shot (Bapna et al., 2017; Lee and Jha, 2019; Shah et al., 2019) and few-shot (Hou et al., 2020) slot tagging studies for two reasons: (1) The creation of slot descriptions needs qualified linguistic expertise and is thus expensive. (2) The relationship between slot names and the c"
2021.eacl-main.134,J80-3005,0,0.656781,"Missing"
2021.eacl-main.134,W18-5047,0,0.0188484,"plications, the low-resource issue is a tremendous challenge for slot tagging in new domains, although labeled samples exist in related domains. Many studies have recently proposed to overcome this low-resource challenge using different techniques, e.g. multitask modeling (Jaech et al., 2016), adversarial training (Kim et al., 2017), and pointer networks (Zhai et al., 2017). In addition, studies like zero-shot learning has influenced the studies of the domain scaling problem for slots prediction (Bapna et al., 2017), eliminating the need of labeled examples for transferring reusable concepts (Zhu and Yu, 2018; Lee and Jha, 2019), and conveying the domainagnostic concepts between the intents (Shah et al., 2019) by exploiting label names and descriptions. Likewise, (Hou et al., 2020) use label semantics within a few-shot classification method TapNet (Yoon et al., 2019). We suggest using a small amount of annotated samples from different domains as training input instead of slot descriptions and slot names as in previos zero-shot (Bapna et al., 2017; Lee and Jha, 2019; Shah et al., 2019) and few-shot (Hou et al., 2020) slot tagging studies for two reasons: (1) The creation of slot descriptions needs"
2021.emnlp-demo.14,D16-1203,0,0.146498,"t skills, such as commonsense or external knowledge reasoning, visual reasoning, or reading text in images. Traditionally, evaluation relies 1 * Authors contributed equally https://github.com/patilli/vqa_benchmarking solely on answering accuracy. However, it is misleading to believe that a single number, like high accuracy on a given benchmark, corresponds to a system’s ability to answer arbitrary questions with high quality. Each dataset contains biases which state-of-the-art deep neural networks are prone to exploit, resulting in higher accuracy scores (Goyal et al., 2017; Das et al., 2017; Agrawal et al., 2016; Jabri et al., 2016). Thus, most VQA models, if evaluated on multiple benchmarks at all, are re-trained per dataset to achieve higher numbers in specialized leaderboards. Further shortcomings of current leaderboards include ignoring prediction cost and robustness, as discussed in (Ethayarajh and Jurafsky, 2020) for NLP. In VQA, we need even more specialized evaluation due to the challenges inherent to open-ended, multi-modal reasoning. In order to successfully develop VQA systems that are able to answer arbitrary questions with human-like performance, we should overcome the previously mention"
2021.emnlp-demo.14,2020.emnlp-main.393,0,0.0724564,"r, like high accuracy on a given benchmark, corresponds to a system’s ability to answer arbitrary questions with high quality. Each dataset contains biases which state-of-the-art deep neural networks are prone to exploit, resulting in higher accuracy scores (Goyal et al., 2017; Das et al., 2017; Agrawal et al., 2016; Jabri et al., 2016). Thus, most VQA models, if evaluated on multiple benchmarks at all, are re-trained per dataset to achieve higher numbers in specialized leaderboards. Further shortcomings of current leaderboards include ignoring prediction cost and robustness, as discussed in (Ethayarajh and Jurafsky, 2020) for NLP. In VQA, we need even more specialized evaluation due to the challenges inherent to open-ended, multi-modal reasoning. In order to successfully develop VQA systems that are able to answer arbitrary questions with human-like performance, we should overcome the previously mentioned shortcomings of current leaderboards as one of the first essential steps. To this end, we propose a benchmarking tool, to our knowledge the first of its kind in the VQA domain, that goes beyond current leaderboard evaluations. It follows the four following principles: 1. Realism To better simulate real-world"
2021.emnlp-demo.14,2020.challengehml-1.9,0,0.0301555,"the VQA domain. As proof of concept, we integrate several popular and state-of-the-art models from public code repositories and investigate their abilities and weaknesses. Through our case study, we demonstrate that all of these models fail to generalize, even to other in-domain test sets. Our metrics quantify the influence of model architecture decisions, which accuracy cannot capture, such as the effect of image and question embeddings on model robustness. 2 Related Work Metrics For more automated, dataset-level insight, many methods try to analyze single aspects of VQA models. For example, Halbe (2020) use feature attribution to assess the influence of individual question words on model predictions. Das et al. (2017) compare human attention to machine attention to explore whether they focus on the same image regions. To measure robustness w.r.t. question input, Huang et al. (2019) collect a dataset of semantically relevant questions, and rank them by similarity, feeding the top-3 into the network to observe changes in prediction. Identifying Biases Agrawal et al. (2016) measured multiple properties: generalization to novel instances as selected by dissimilarity, question understanding based"
2021.emnlp-demo.14,D17-1215,0,0.0277459,"ning phase (Cadene et al., 2019). However, this requires training one model per modality and cannot be applied easily to all architectures, e.g. to attention mechanisms computed on joint feature spaces. Robustness and Adversarial Examples Adversarial examples originate from image classification, where perturbations barely visible to a human fool the classifier and cause sudden prediction changes (Szegedy et al., 2014). The same idea was later applied to NLP, where, e.g., appending distracting text to the context in a question answering scenario resulted in F1-score dropping by more than half (Jia and Liang, 2017). VQA Benchmarks Benchmarks often emphasize certain sub-tasks of the general VQA problem. For example, CLEVR (Johnson et al., 2017) tests visual reasoning abilities such as shape recognition and Benchmarking Tools Liu et al. (2021) propose spatial relationships between objects, rather than a leaderboard for NLP tasks to compare model perreal-world scenarios. Other approaches change the formance. They differentiate among several NLP answer distributions of existing datasets, such as tasks and datasets. All methods are applied postVQA-CP (Agrawal et al., 2018) originating from hoc to analyze the"
2021.emnlp-demo.14,P18-1079,0,0.0181944,"ne by adding Gaussian noise in embedding space, a reasonable approach under the assumption that similar vectors in embedding space have similar meaning. Again, multiple trials are performed. High values in robustness correspond to models unaffected by noise in one modality for many samples, e.g. a question robustness of 100 indicates a model that never changed its predictions due to noise added in question embedding space. Robustness to Adversarial Questions Semantically Equivalent Adversarial Ruless (SEARs) alter textual input according to a set of rules, while preserving original semantics (Ribeiro et al., 2018). For the questions in the VQA dataset, the authors come up with the four rules that most affect the predictions in their tests, using a combination of Part-of-Speech (POS)-Tags and vocabulary entries: • Rule 1 WP VBZ → WP’s • Rule 2 What NOUN → Which NOUN • Rule 3 color → colour • Rule 4 ADV VBZ → ADV’s Uncertainty To measure model certainty, we leverage the dropout-based Monte-Carlo method (Gal and Ghahramani, 2016). Forwarding a sample multiple times with active dropout, the averaged 1 PN output vector N n=1 f (x). 3.3 Views We support inspection of the included metrics at different levels"
2021.emnlp-demo.14,2021.acl-demo.34,0,0.0382007,"ples Adversarial examples originate from image classification, where perturbations barely visible to a human fool the classifier and cause sudden prediction changes (Szegedy et al., 2014). The same idea was later applied to NLP, where, e.g., appending distracting text to the context in a question answering scenario resulted in F1-score dropping by more than half (Jia and Liang, 2017). VQA Benchmarks Benchmarks often emphasize certain sub-tasks of the general VQA problem. For example, CLEVR (Johnson et al., 2017) tests visual reasoning abilities such as shape recognition and Benchmarking Tools Liu et al. (2021) propose spatial relationships between objects, rather than a leaderboard for NLP tasks to compare model perreal-world scenarios. Other approaches change the formance. They differentiate among several NLP answer distributions of existing datasets, such as tasks and datasets. All methods are applied postVQA-CP (Agrawal et al., 2018) originating from hoc to analyze the predictions a model outputs. VQA2 (Goyal et al., 2017) or GQA-OOD from Other benchmarking tools, for example, focus on GQA (Kervadec et al., 2021). These changes are intended to mitigate learnable bias. Another ap- runtime compari"
2021.iwslt-1.21,P19-1310,0,0.0492018,"Missing"
2021.iwslt-1.21,2020.iwslt-1.3,0,0.0579677,"Missing"
2021.iwslt-1.21,2020.emnlp-main.480,0,0.0187365,"Missing"
2021.iwslt-1.21,D18-2012,0,0.091333,"ata for each target language comprises of 434 randomly sampled utterances from the shared task training data. The testing data is the shared task validation data, that also has 434 sentences per target language. Model For the text-to-text neural machine translation (NMT) system we use a Transformer big model (Vaswani et al., 2017) using the fairseq implementation (Ott et al., 2019). We train three versions of the translation model. First we train a vanilla NMT (vanillaNMT) system using only the data from the parallel training dataset. For preprocessing we use the SentencePiece implementation (Kudo and Richardson, 2018) of BPEs (Sennrich et al., 2016). For our second experiment for the NMT system (preprocNMT), we apply the same written to spoken language conversion as used for the ASR transcriptions (section §2.1) to the source text S and obtain ASR-like text St . St is then segmented using a BPE model and used as input for our NMT model. The last approach was using a multi-task framework to train the system (multiNMT), where all parameters of the translation model were shared. The main task of this model is to translate ASR output St to the target language T (task asrS), while our auxiliary task is to trans"
2021.iwslt-1.21,2020.findings-emnlp.195,0,0.0253984,"Missing"
2021.iwslt-1.21,2020.acl-main.156,0,0.0347471,"Missing"
2021.iwslt-1.21,N19-4009,0,0.0134963,"l system (the external LM weights are language-specific). 3 Tanzil (Tiedemann, 2012), TED2020 (Reimers and Gurevych, 2020), Ubuntu (Tiedemann, 2012), WikiMatrix (Schwenk et al., 2019) and wikimedia (Tiedemann, 2012). The validation data for each target language comprises of 434 randomly sampled utterances from the shared task training data. The testing data is the shared task validation data, that also has 434 sentences per target language. Model For the text-to-text neural machine translation (NMT) system we use a Transformer big model (Vaswani et al., 2017) using the fairseq implementation (Ott et al., 2019). We train three versions of the translation model. First we train a vanilla NMT (vanillaNMT) system using only the data from the parallel training dataset. For preprocessing we use the SentencePiece implementation (Kudo and Richardson, 2018) of BPEs (Sennrich et al., 2016). For our second experiment for the NMT system (preprocNMT), we apply the same written to spoken language conversion as used for the ASR transcriptions (section §2.1) to the source text S and obtain ASR-like text St . St is then segmented using a BPE model and used as input for our NMT model. The last approach was using a mu"
2021.iwslt-1.21,2020.emnlp-main.365,0,0.0216548,"Missing"
2021.iwslt-1.21,P16-1162,0,0.117665,"well as wav2vec 2.0 (Baevski et al., 2020) based models XLSR-53 (Conneau et al., 2020) and VoxPopuli (Wang et al., 2021). Table 1: Datasets used for the ASR system. 2.2 2.4 Model The ASR system is based on end-to-end Conformer ASR (Gulati et al., 2020) and its ESPnet implementation (Guo et al., 2020). Following the latest LibriSpeech recipe (Kamo, 2021), our model has 12 Conformer blocks in Encoder and 6 Transformer blocks in Decoder with 8 heads and attention dimension of 512. The input features are 80 dimensional log Mel filterbanks. The output units are 100 byte-pair-encoding (BPE) tokens (Sennrich et al., 2016). The warm-up learning rate strategy (Vaswani et al., 2017) is used, while the learning rate coefficient is set to 0.005 and the number of warm-up steps is set to 10000. The model is optimized to jointly minimize cross-entropy and connectionist temporal classification (CTC) (Graves et al., 2006) loss functions, both with the coefficient of 0.5. The training is performed for 35 epochs on 2 GPUs with the total batch size of 20M bins and gradient accumulation over each 2 steps. After that, Pre-trained models Results Table 2 summarizes the explored ASR settings and the results on the shared task v"
2021.iwslt-1.21,tiedemann-2012-parallel,0,0.0358589,".7 17.9 15.1 14.5 14.0 13.6 13.5 13.8 swc 26.5 24.4 25.0 25.0 24.9 25.4 22.9 20.8 21.1 21.4 19.7 19.7 19.5 19.6 19.5 19.6 18.4 18.3 18.5 18.7 19.1 19.9 IWSLT’21 Gamayun ELRC 2922 GNOME CCAligned MultiCCAligned EUbookshop GlobalVoices JW300 sw JW300 swc ParaCrawl MultiParaCrawl Tanzil TED2020 Ubuntu WikiMatrix wikimedia Total 3.2 Data Table 3 summarizes the datasets used to train our MT systems. The training data comprises of the shared task training data, Gamayun kit4 (English – Swahili and Congolese Swahili – French parallel text corpora) as well as multiple corpora from the OPUS collection (Tiedemann, 2012), namely: ELRC 2922 (Tiedemann, 2012), GNOME (Tiedemann, 2012), CCAligned and MultiCCAligned (El-Kishky et al., 2020), EUbookshop (Tiedemann, 2012), GlobalVoices (Tiedemann, 2012), JW300 for sw and swc source languages (Agi´c and Vuli´c, 2019), ParaCrawl and MultiParaCrawl5 , 4 https://gamayun.translatorswb.org/ data/ 5 https://www.paracrawl.eu/ Words →eng →fra 31,594 51,111 39,608 216,408 12,691 170 170 18,038,994 18,039,148 10,713,654 228 223 576,222 347,671 15,811,865 15,763,811 9,108,342 9,094,008 3,207,700 996,664 1,734,247 117,975 136,162 134,601 2,655 189 923,898 271,673 66,704 1,431 41"
2021.iwslt-1.21,2021.acl-long.80,0,0.0300614,"Missing"
C16-1254,D14-1151,0,0.0604566,"Missing"
C16-1254,P15-1144,0,0.258715,"create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings. The resulting word embeddings were used in similarity tasks, and to distinguish between antonyms and synonyms. Faruqui et al. (2015) improved word embeddings without relying on lexical resources, by applying ideas from sparse coding to transform dense word embeddings into sparse word embeddings. The dense vectors in their models can be transformed into sparse overcomplete vectors or sparse binary overcomplete vectors. They showed that the resulting vector representations were more similar to interpretable features in NLP and outperformed the original vector representations on several benchmark tasks. In this paper, we aim to improve word embeddings by reducing their noise. The hypothesis behind our approaches is that word"
C16-1254,J15-4004,0,0.0362333,"Z strongly differ from the word embeddings X; hence, the denoising is affected. However, the performance of the vectors Z∗ still outperforms the original vectors X, A and B after the denoising process. 3.3.1 Relatedness and Similarity Tasks For the relatedness task, we use two kinds of datasets: MEN (Bruni et al., 2014) consists of 3000 word pairs comprising 656 nouns, 57 adjectives and 38 verbs. The WordSim-353 relatedness dataset (Finkelstein et al., 2001) contains 252 word pairs. Concerning the similarity tasks, we evaluate the denoising vectors again on two kinds of datasets: SimLex-999 (Hill et al., 2015) contains 999 word pairs including 666 noun, 222 verb and 111 adjective pairs. The WordSim-353 similarity dataset consists of 203 word pairs. In addition, we evaluate our denoising vectors on the WordSim-353 dataset which contains 353 pairs for both similarity and relatedness relations. We calculate cosine similarity between the vectors of two words forming a test pair, and report the Spearman rank-order correlation coefficient ρ (Siegel and Castellan, 1988) against the respective gold standards of human ratings. 3.3.2 Synonymy We evaluate on 80 TOEFL (Test of English as a Foreign Language) sy"
C16-1254,D14-1181,0,0.00993301,"embeddings. The word denoising embeddings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings. 1 Introduction Word embeddings aim to represent words as low-dimensional dense vectors. In comparison to distributional count vectors, word embeddings address the problematic sparsity of word vectors and achieved impressive results in many NLP tasks such as sentiment analysis (e.g., Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.g., Lazaridou et al. (2013)). Moreover, word embeddings are attractive because they can be learned in an unsupervised fashion from unlabeled raw corpora. There are two main approaches to create word embeddings. The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013). The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices. In recent years, a number of a"
C16-1254,D13-1196,0,0.157921,"formation and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings. 1 Introduction Word embeddings aim to represent words as low-dimensional dense vectors. In comparison to distributional count vectors, word embeddings address the problematic sparsity of word vectors and achieved impressive results in many NLP tasks such as sentiment analysis (e.g., Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.g., Lazaridou et al. (2013)). Moreover, word embeddings are attractive because they can be learned in an unsupervised fashion from unlabeled raw corpora. There are two main approaches to create word embeddings. The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013). The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices. In recent years, a number of approaches have focused on improving word embeddings, often by integrating lexical resources. Fo"
C16-1254,N13-1090,0,0.485452,"rs. In comparison to distributional count vectors, word embeddings address the problematic sparsity of word vectors and achieved impressive results in many NLP tasks such as sentiment analysis (e.g., Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.g., Lazaridou et al. (2013)). Moreover, word embeddings are attractive because they can be learned in an unsupervised fashion from unlabeled raw corpora. There are two main approaches to create word embeddings. The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013). The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices. In recent years, a number of approaches have focused on improving word embeddings, often by integrating lexical resources. For example, Adel and Sch¨utze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Sk"
C16-1254,P16-2074,1,0.794306,"factorizing word-context co-occurrence matrices. In recent years, a number of approaches have focused on improving word embeddings, often by integrating lexical resources. For example, Adel and Sch¨utze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings. The resulting word embeddings were used in similarity tasks, and to distinguish between antonyms and synonyms. Faruqui et al. (2015) improved word embeddings without relying on lexical resources, by applying ideas from sparse coding to transform dense word embeddings into sparse word embeddings. The dense vectors in their models can be transformed into sparse overcomplete vectors or sparse binary overcomplete vectors. They showed that the resulting vector re"
C16-1254,D14-1162,0,0.0854031,"dings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings. 1 Introduction Word embeddings aim to represent words as low-dimensional dense vectors. In comparison to distributional count vectors, word embeddings address the problematic sparsity of word vectors and achieved impressive results in many NLP tasks such as sentiment analysis (e.g., Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.g., Lazaridou et al. (2013)). Moreover, word embeddings are attractive because they can be learned in an unsupervised fashion from unlabeled raw corpora. There are two main approaches to create word embeddings. The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013). The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices. In recent years, a number of approaches have focused on improving word embedding"
C16-1254,P15-2004,0,0.0137759,"approaches to create word embeddings. The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013). The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices. In recent years, a number of approaches have focused on improving word embeddings, often by integrating lexical resources. For example, Adel and Sch¨utze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings. The resulting word embeddings were used in similarity tasks, and to distinguish between antonyms and synonyms. Faruqui et al. (2015) improved word embeddings without relying on lexi"
C16-1254,schafer-bildhauer-2012-building,0,0.0608687,"Missing"
D17-1022,E12-1004,0,0.526533,"ional inclusion hypothesis (Geffet and Dagan, 2005; ZhitomirskyGeffet and Dagan, 2009), or the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). These measures assign scores to semantic relation pairs, and hypernymy scores are expected to be higher than those of other relation pairs. Typically, Average Precision (AP) (Kotlerman et al., 2010) is applied to rank and distinguish between the predicted relations. Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (LR), to predict hypernymy. Across approaches, Shwartz et al. (2017) demonstrated that there is no single unsupervised measure which consistently deals well with discriminating hypernymy from other semantic relations. Furthermore, Levy et al. (2015) showed that supervised methods memorize prototypical hypernyms instead of learning a relation between two words. Approaches of hypernymy-specific embeddings utilize neural models to learn vector representations for"
D17-1022,W11-2501,0,0.135005,"BLESS Lenci&Benotto Weeds Hypernymy vs. other relations meronymy attribute antonymy synonymy other relations meronymy coordination attribute event other relations antonymy synonymy coordination Baseline 0.353 0.675 0.651 0.55 0.657 0.051 0.76 0.537 0.74 0.779 0.382 0.624 0.725 0.441 HyperScore 0.538 0.811 0.800 0.743 0.793 0.454 0.913 0.888 0.918 0.620 0.574 0.696 0.751 0.850 Table 2: AP results of HyperScore in comparison to state-of-the-art measures. Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2016). The evaluation was performed on four semantic relation datasets: BLESS (Baroni and Lenci, 2011), W EEDS (Weeds et al., 2004), EVAL UTION (Santus et al., 2015), and L ENCI &B ENOTTO (Benotto, 2015). Table 1 describes the detail of these datasets in terms of the semantic relations and the number of instances. The Average Precision (AP) ranking measure is used to evaluate the performance of the measures. Unsupervised Hypernymy Detection and Directionality In this section, we assess our model on two experimental setups: i) a ranking retrieval setup that expects hypernymy pairs to have a higher similarity score than instances from other semantic relations; ii) a classification setup that req"
D17-1022,W09-0215,0,0.543267,"e-extracted. The resulting term embeddings are fed to an SVM classifier to predict hypernymy. However, this model only learns term pairs without considering their contexts, leading to a lack of generalization for term embeddings. Tuan et al. (2016) introduced a dynamic weighting neural network to learn term embeddings that encode information about hypernymy and also about their contexts, considering all words between a hypernym and its Related Work Unsupervised hypernymy measures: A variety of directional measures for unsupervised hypernymy detection (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; 1 www.ims.uni-stuttgart.de/data/hypervec 234 hyponym in a sentence. The proposed model is trained on a set of hypernym relations extracted from WordNet (Miller, 1995). The embeddings are applied as features to detect hypernymy, using an SVM classifier. Tuan et al. (2016) handles the drawback of the approach by Yu et al. (2015), considering the contextual information between two terms; however the method still is not able to determine the directionality of a hypernym pair. Vendrov et al. (2016) proposed a method to encode order into learned distributed representations,"
D17-1022,N13-1073,0,0.0279829,"the representations, a mapping function between a source language (German, Italian) and our English HyperVec space is learned, by relying on the least-squares error method from previous work using cross-lingual data (Mikolov et al., 2013a) and different modalities (Lazaridou et al., 2015). To learn a mapping function between two languages, a one-to-one correspondence (word translations) between two sets of vectors is required. We obtained these translations by using the parallel Europarl3 V7 corpus for German–English and Italian–English. Word alignment counts were extracted using fast align (Dyer et al., 2013). We then assigned each source word to the English word with the maximum number of alignments in the parallel corpus. We could match 25,547 pairs for DE→EN and 47,475 pairs for IT→EN. Taking the aligned subset of both spaces, we assume that X is the matrix obtained by concatenating all source vectors, and likewise Y is the matrix obtained by concatenating all corresponding English elements. Applying the `2-regularized leastsquares error objective can be described using the following equation: Table 5: Results (ρ) of HyperScore and state-ofthe-art measures and word embedding models on graded le"
D17-1022,P05-1014,0,0.378666,"Missing"
D17-1022,N16-1018,0,0.0686701,"Missing"
D17-1022,P15-2020,0,0.675742,"Missing"
D17-1022,P15-1027,0,0.0164543,"he languagespecific representations are obtained using the same hyper-parameter settings as for our English SGNS model (cf. Section 4.1). As corpus resource we relied on Wikipedia dumps2 . Note that we do not use any additional resource, such as the German or Italian WordNet, to tune the embeddings for hypernymy detection. Based on the representations, a mapping function between a source language (German, Italian) and our English HyperVec space is learned, by relying on the least-squares error method from previous work using cross-lingual data (Mikolov et al., 2013a) and different modalities (Lazaridou et al., 2015). To learn a mapping function between two languages, a one-to-one correspondence (word translations) between two sets of vectors is required. We obtained these translations by using the parallel Europarl3 V7 corpus for German–English and Italian–English. Word alignment counts were extracted using fast align (Dyer et al., 2013). We then assigned each source word to the English word with the maximum number of alignments in the parallel corpus. We could match 25,547 pairs for DE→EN and 47,475 pairs for IT→EN. Taking the aligned subset of both spaces, we assume that X is the matrix obtained by con"
D17-1022,P16-2074,1,0.775424,"r relations. For example, the hypernym pair animal–frog will be assigned a higher cosine score than the co-hyponymy pair eagle–frog. Secondly, the embeddings are learned to capture the distributional hierarchy between hyponym and hypernym, as an indicator to differentiate between hypernym and hyponym. For example, given a hyponym–hypernym pair (p, q), we can exploit the Euclidean norms of ~q and p~ to differentiate between the two words, such that the Euclidean norm of the hypernym ~q is larger than the Euclidean norm of the hyponym p~. Inspired by the distributional lexical contrast model in Nguyen et al. (2016) for distinguishing antonymy from synonymy, this paper proposes two objective functions to learn hierarchical embeddings for hypernymy. Before moving Learning Hierarchical Embeddings Our approach makes use of a set of hypernyms which could be obtained from either exploiting the transitivity of the hypernymy relation (Fallucchi and Zanzotto, 2011) or lexical databases, to learn hierarchical embeddings. We rely on WordNet, a large lexical database of English (Fellbaum, 1998), and extract all hypernym–hyponym pairs for nouns and for verbs, including both direct and indirect hypernymy, e.g., anima"
D17-1022,S12-1012,0,0.570854,"d be closer to the vector of bird than to the vector of animal. We evaluate our HyperVec model on both unsupervised and supervised hypernymy detection and directionality tasks. In addition, we apply the model to the task of graded lexical entailment (Vuli´c et al., 2016), and we assess the capability of HyperVec on generalizing hypernymy by mapping to German and Italian. Results on benchmark datasets of hypernymy show that the hierarchical embeddings outperform state-of-the-art measures and previous embedding models. Furthermore, the implementation of our models is made publicly available.1 2 Lenci and Benotto, 2012) all rely on some variation of the distributional inclusion hypothesis: If u is a semantically narrower term than v, then a significant number of salient distributional features of u is expected to be included in the feature vector of v as well. In addition, Santus et al. (2014) proposed the distributional informativeness hypothesis, that hypernyms tend to be less informative than hyponyms, and that they occur in more general contexts than their hyponyms. All of these approaches represent words as vectors in distributional semantic models (Turney and Pantel, 2010), relying on the distributiona"
D17-1022,D14-1162,0,0.0901717,"utional semantic models (Turney and Pantel, 2010), relying on the distributional hypothesis (Harris, 1954; Firth, 1957). For evaluation, these directional models use the AP measure to assess the proportion of hypernyms at the top of a score-sorted list. In a different vein, Kiela et al. (2015) introduced three unsupervised methods drawn from visual properties of images to determine a concept’s generality in hypernymy tasks. Supervised hypernymy methods: The studies in this area are based on word embeddings which represent words as low-dimensional and realvalued vectors (Mikolov et al., 2013b; Pennington et al., 2014). Each hypernymy pair is encoded by some combination of the two word vectors, such as concatenation (Baroni et al., 2012) or difference (Roller et al., 2014; Weeds et al., 2014). Hypernymy is distinguished from other relations by using a classification approach, such as SVM or LR. Because word embeddings are trained for similar and symmetric vectors, it is however unclear whether the supervised methods do actually learn the asymmetry in hypernymy (Levy et al., 2015). Hypernymy-specific embeddings: These approaches are closest to our work. Yu et al. (2015) proposed a dynamic distance-margin mod"
D17-1022,N15-1098,0,0.270569,"rank and distinguish between the predicted relations. Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (LR), to predict hypernymy. Across approaches, Shwartz et al. (2017) demonstrated that there is no single unsupervised measure which consistently deals well with discriminating hypernymy from other semantic relations. Furthermore, Levy et al. (2015) showed that supervised methods memorize prototypical hypernyms instead of learning a relation between two words. Approaches of hypernymy-specific embeddings utilize neural models to learn vector representations for hypernymy. Yu et al. (2015) proposed a supervised method to learn term embeddings for hypernymy identification, based on pre-extracted hypernymy pairs. Recently, Tuan et al. (2016) proposed a dynamic weighting neural model to learn term embeddings in which the model encodes not only the information of hypernyms vs. hyponyms, but also their contextual information. The performance of"
D17-1022,E14-1054,0,0.105106,"tionality Kim Anh Nguyen, Maximilian K¨oper, Sabine Schulte im Walde, Ngoc Thang Vu Institut f¨ur Maschinelle Sprachverarbeitung Universit¨at Stuttgart Pfaffenwaldring 5B, 70569 Stuttgart, Germany {nguyenkh,koepermn,schulte,thangvu}@ims.uni-stuttgart.de Abstract tions. Distributional count approaches make use of either directionally unsupervised measures or of supervised classification methods. Unsupervised measures exploit the distributional inclusion hypothesis (Geffet and Dagan, 2005; ZhitomirskyGeffet and Dagan, 2009), or the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). These measures assign scores to semantic relation pairs, and hypernymy scores are expected to be higher than those of other relation pairs. Typically, Average Precision (AP) (Kotlerman et al., 2010) is applied to rank and distinguish between the predicted relations. Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (LR), to predict"
D17-1022,C14-1097,0,0.379172,"Missing"
D17-1022,E14-4008,1,0.931991,"y Detection and Directionality Kim Anh Nguyen, Maximilian K¨oper, Sabine Schulte im Walde, Ngoc Thang Vu Institut f¨ur Maschinelle Sprachverarbeitung Universit¨at Stuttgart Pfaffenwaldring 5B, 70569 Stuttgart, Germany {nguyenkh,koepermn,schulte,thangvu}@ims.uni-stuttgart.de Abstract tions. Distributional count approaches make use of either directionally unsupervised measures or of supervised classification methods. Unsupervised measures exploit the distributional inclusion hypothesis (Geffet and Dagan, 2005; ZhitomirskyGeffet and Dagan, 2009), or the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). These measures assign scores to semantic relation pairs, and hypernymy scores are expected to be higher than those of other relation pairs. Typically, Average Precision (AP) (Kotlerman et al., 2010) is applied to rank and distinguish between the predicted relations. Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (L"
D17-1022,W15-4208,0,0.125309,"ttribute antonymy synonymy other relations meronymy coordination attribute event other relations antonymy synonymy coordination Baseline 0.353 0.675 0.651 0.55 0.657 0.051 0.76 0.537 0.74 0.779 0.382 0.624 0.725 0.441 HyperScore 0.538 0.811 0.800 0.743 0.793 0.454 0.913 0.888 0.918 0.620 0.574 0.696 0.751 0.850 Table 2: AP results of HyperScore in comparison to state-of-the-art measures. Kotlerman et al., 2010; Lenci and Benotto, 2012; Santus et al., 2016). The evaluation was performed on four semantic relation datasets: BLESS (Baroni and Lenci, 2011), W EEDS (Weeds et al., 2004), EVAL UTION (Santus et al., 2015), and L ENCI &B ENOTTO (Benotto, 2015). Table 1 describes the detail of these datasets in terms of the semantic relations and the number of instances. The Average Precision (AP) ranking measure is used to evaluate the performance of the measures. Unsupervised Hypernymy Detection and Directionality In this section, we assess our model on two experimental setups: i) a ranking retrieval setup that expects hypernymy pairs to have a higher similarity score than instances from other semantic relations; ii) a classification setup that requires both hypernymy detection and directionality. 4.2.1 Rankin"
D17-1022,C14-1212,0,0.773944,"an, 2005; ZhitomirskyGeffet and Dagan, 2009), or the distributional informativeness hypothesis (Santus et al., 2014; Rimell, 2014). These measures assign scores to semantic relation pairs, and hypernymy scores are expected to be higher than those of other relation pairs. Typically, Average Precision (AP) (Kotlerman et al., 2010) is applied to rank and distinguish between the predicted relations. Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (LR), to predict hypernymy. Across approaches, Shwartz et al. (2017) demonstrated that there is no single unsupervised measure which consistently deals well with discriminating hypernymy from other semantic relations. Furthermore, Levy et al. (2015) showed that supervised methods memorize prototypical hypernyms instead of learning a relation between two words. Approaches of hypernymy-specific embeddings utilize neural models to learn vector representations for hypernymy. Yu et al. (2015) proposed a su"
D17-1022,schafer-bildhauer-2012-building,0,0.0253683,"Missing"
D17-1022,W03-1011,0,0.657699,"on the taxonomic relation data which is pre-extracted. The resulting term embeddings are fed to an SVM classifier to predict hypernymy. However, this model only learns term pairs without considering their contexts, leading to a lack of generalization for term embeddings. Tuan et al. (2016) introduced a dynamic weighting neural network to learn term embeddings that encode information about hypernymy and also about their contexts, considering all words between a hypernym and its Related Work Unsupervised hypernymy measures: A variety of directional measures for unsupervised hypernymy detection (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; 1 www.ims.uni-stuttgart.de/data/hypervec 234 hyponym in a sentence. The proposed model is trained on a set of hypernym relations extracted from WordNet (Miller, 1995). The embeddings are applied as features to detect hypernymy, using an SVM classifier. Tuan et al. (2016) handles the drawback of the approach by Yu et al. (2015), considering the contextual information between two terms; however the method still is not able to determine the directionality of a hypernym pair. Vendrov et al. (2016) proposed a method to encode order into le"
D17-1022,W14-5814,1,0.879094,"Missing"
D17-1022,C04-1146,0,0.643677,"Missing"
D17-1022,E17-1007,0,0.814922,"to semantic relation pairs, and hypernymy scores are expected to be higher than those of other relation pairs. Typically, Average Precision (AP) (Kotlerman et al., 2010) is applied to rank and distinguish between the predicted relations. Supervised classification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014). The resulting vector is fed into a Support Vector Machine (SVM) or into Logistic Regression (LR), to predict hypernymy. Across approaches, Shwartz et al. (2017) demonstrated that there is no single unsupervised measure which consistently deals well with discriminating hypernymy from other semantic relations. Furthermore, Levy et al. (2015) showed that supervised methods memorize prototypical hypernyms instead of learning a relation between two words. Approaches of hypernymy-specific embeddings utilize neural models to learn vector representations for hypernymy. Yu et al. (2015) proposed a supervised method to learn term embeddings for hypernymy identification, based on pre-extracted hypernymy pairs. Recently, Tuan et al. (2016) proposed a dynamic wei"
D17-1022,P06-1101,0,0.0488304,"asures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment. 1 Introduction Hypernymy represents a major semantic relation and a key organization principle of semantic memory (Miller and Fellbaum, 1991; Murphy, 2002). It is an asymmetric relation between two terms, a hypernym (superordinate) and a hyponym (subordiate), as in animal–bird and flower–rose, where the hyponym necessarily implies the hypernym, but not vice versa. From a computational point of view, automatic hypernymy detection is useful for NLP tasks such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011), recognizing textual entailment (Dagan et al., 2013), and text generation (Biran and McKeown, 2013), among many others. Two families of approaches to identify and discriminate hypernyms are predominent in NLP, both of them relying on word vector representa233 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 233–243 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics In this paper, we propose a novel neural model HyperVec to learn hierarchical embeddings that (i) discriminate hypernymy f"
D17-1022,J09-3004,0,0.0317116,"Missing"
D17-1022,D16-1039,0,0.42042,"Across approaches, Shwartz et al. (2017) demonstrated that there is no single unsupervised measure which consistently deals well with discriminating hypernymy from other semantic relations. Furthermore, Levy et al. (2015) showed that supervised methods memorize prototypical hypernyms instead of learning a relation between two words. Approaches of hypernymy-specific embeddings utilize neural models to learn vector representations for hypernymy. Yu et al. (2015) proposed a supervised method to learn term embeddings for hypernymy identification, based on pre-extracted hypernymy pairs. Recently, Tuan et al. (2016) proposed a dynamic weighting neural model to learn term embeddings in which the model encodes not only the information of hypernyms vs. hyponyms, but also their contextual information. The performance of this family of models is typically evaluated by using an SVM to discriminate hypernymy from other relations. We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order"
D17-1022,I13-1095,0,\N,Missing
D19-6306,W19-7911,1,0.408633,"tgart.de Abstract 2 Our system takes a pipeline approach, which consists of up to five steps to produce the final detokenized text. The steps are: linearization (§2.2), completion (§2.3), inflection (§2.4), contraction (§2.5), and detokenization (§2.6), among which completion is used only in the deep track. All the steps except for the rule-based detokenization use the same Tree-LSTM encoder architecture (§2.1). As the multi-task style training hurt performance in the preliminary experiments, all the steps are trained separately. Since the submission is mostly based on our system described in Yu et al. (2019b), here we mainly focus on the changes introduced for this shared task, and we refer the reader to Yu et al. (2019b) for more details, especially on the explanation and ablation experiments of the TreeLSTM encoder and the linearization decoder. We introduce the IMS contribution to the Surface Realization Shared Task 2019. Our submission achieves the state-of-the-art performance without using any external resources. The system takes a pipeline approach consisting of five steps: linearization, completion, inflection, contraction, and detokenization. We compare the performance of our linearizati"
D19-6306,P17-1183,0,0.0415336,"e input vectors and predicts an output, which could be a symbol 3 to copy the current input character, a symbol 7 to ignore the current input character, or a character from the alphabet to generate a new one. When 3 or 7 is predicted, the input pointer will move one step forward, while if a character is generated, the input pointer does not move. The ground truth of such sequence is calculated from the Levenshtein edit operations between the lemma and the word form, where only insertion and deletion is allowed (no substitution). Our model is in a way similar to the hard monotonic attention in Aharoni and Goldberg (2017), but we use a much simpler source-target alignment (Levenshtein edit operations), and we use copy as an edit operation to avoid completion errors while they do not. Furthermore, our edit operations are associated with the moving of the pointer, while they treat moving the pointer as an atomic operation, which lead to longer prediction sequences. Generally, our model performs on a par with theirs, see the comparison in Yu et al. (2019b). (1) (move pointer) h d(+1) d(+2) $ (2) (new token f1) f1 h d(+1) d(+2) f1 $ (3) f2 h d(+1) f1 (new token f2) d(+2) f2 $ (4) (move pointer) h d(+1) f1 f2 d(+2)"
D19-6306,W19-8636,1,0.201402,"tgart.de Abstract 2 Our system takes a pipeline approach, which consists of up to five steps to produce the final detokenized text. The steps are: linearization (§2.2), completion (§2.3), inflection (§2.4), contraction (§2.5), and detokenization (§2.6), among which completion is used only in the deep track. All the steps except for the rule-based detokenization use the same Tree-LSTM encoder architecture (§2.1). As the multi-task style training hurt performance in the preliminary experiments, all the steps are trained separately. Since the submission is mostly based on our system described in Yu et al. (2019b), here we mainly focus on the changes introduced for this shared task, and we refer the reader to Yu et al. (2019b) for more details, especially on the explanation and ablation experiments of the TreeLSTM encoder and the linearization decoder. We introduce the IMS contribution to the Surface Realization Shared Task 2019. Our submission achieves the state-of-the-art performance without using any external resources. The system takes a pipeline approach consisting of five steps: linearization, completion, inflection, contraction, and detokenization. We compare the performance of our linearizati"
D19-6306,C16-1274,0,0.0667815,"based on its lemma, UPOS, morphological features, and dependency label. We use embeddings for the lemma, UPOS and dependency label, and employ an LSTM to process the list of morphological features.1 We then concatenate all of the obtained vectors as the representation of each token (v◦ ). The representation is further processed by a bidirectional Tree-LSTM to encode the tree structure information. The encoder is generally the same as described in Yu et al. (2019b), consisting of two passes of information: a bottom-up pass followed by a top-down pass. In the bottom-up pass, we use a Tree-LSTM (Zhou et al., 2016) to compose the bottom-up vector of the head from the vectors of the dependents, attended by the 1 There could be better treatment of the morphological features, since they are not sequences in nature. 50 Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019), pages 50–58 c Hong Kong, China, November 3rd, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 token-level vector of the head, denoted as v↑ . The bottom-up vectors are then fed into a sequential LSTM for the top-down pass from the root to each leaf token, so that every token ha"
D19-6306,D12-1085,1,0.838767,"Missing"
D19-6306,C10-1012,0,0.705967,"tep (3) another token f2 is created and attached to f1 ; in step (4) the pointer is moved to d(+2) ; in step (5) the pointer is moved again to $, which terminates the process and outputs the sequence [h, d(+1) , f1 , f2 , d(+2) ]. The left and right completion processes are independent of each other, since both forward LSTMs are only aware of the initial linearized tokens on both sides but not the newly generated tokens. We tried several variations in the prelimiLinearization The linearization algorithm is the same as in Yu et al. (2019b), which is in turn based on the linearizer described by Bohnet et al. (2010). The algorithm takes an divide-and-conquer strategy, which orders each subtree (a head and its dependents) individually, and then combines them into a fully linearized tree.2 The main improvement of our algorithm to Bohnet et al. (2010) is that instead of ordering the subtrees from left to right, we start from the head (thus called the head-first decoder), and add the dependents on both sides of the head incrementally. We also train a left-to-right and a right-toleft decoders to form an ensemble with a shared encoder, which is shown in Yu et al. (2019b) to achieve the best performance. We use"
D19-6306,D19-6301,0,0.0978772,"state-of-the-art performance without using any external resources. The system takes a pipeline approach consisting of five steps: linearization, completion, inflection, contraction, and detokenization. We compare the performance of our linearization algorithm with two external baselines and report results for each step in the pipeline. Furthermore, we perform detailed error analysis revealing correlation between word order freedom and difficulty of the linearization task. 1 Surface Realization System Introduction This paper presents our submission to the Surface Realization Shared Task 2019 (Mille et al., 2019). We participate in both shallow and deep track of the shared task, where the shallow track requires the recovery of the linear order and inflection of a dependency tree, and the deep track additionally requires the completion of function words. We approach both tasks with very similar pipelines, consisting of linearizing the unordered dependency trees, completing function words (for the deep track only), inflecting lemmata to word forms, and contracting several words as one token, and finally detokenizing to obtain the natural written text. We use machine learning models for the first four st"
D19-6306,L16-1262,0,0.0429906,"Missing"
D19-6306,N16-1058,0,0.0805804,"Missing"
E17-1008,N13-1090,0,0.289181,"els and pattern-based models. These distributional semantic models (DSMs) offer a means to represent meaning vectors of words or word pairs, and to determine their semantic relatedness (Turney and Pantel, 2010). In co-occurrence models, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in particular contexts. When using word embeddings, these models rely on neural methods to represent words as low-dimensional vectors. To create the word embeddings, the models either make use of neural-based techniques, such as the skipgram model (Mikolov et al., 2013), or use matrix factorization (Pennington et al., 2014) that builds word embeddings by factorizing word-context cooccurrence matrices. In comparison to standard co-occurrence vector representations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.g., Vu et al. (2016)), and antonym-synonym distinction (e.g., Nguyen et al. (2016)). In pattern-based models, vector representations make use of lexico-syntactic surface patterns to distinguish be"
E17-1008,P16-2074,1,0.942688,"o create the word embeddings, the models either make use of neural-based techniques, such as the skipgram model (Mikolov et al., 2013), or use matrix factorization (Pennington et al., 2014) that builds word embeddings by factorizing word-context cooccurrence matrices. In comparison to standard co-occurrence vector representations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.g., Vu et al. (2016)), and antonym-synonym distinction (e.g., Nguyen et al. (2016)). In pattern-based models, vector representations make use of lexico-syntactic surface patterns to distinguish between the relations of word pairs. For example, Justeson and Katz (1991) suggested that adjectival opposites co-occur with each other in specific linear sequences, such as between X and Y. Hearst (1992) determined surface patterns, e.g., X such as Y, to identify nominal hypernyms. Lin et al. (2003) proposed two textual patterns indicating semantic incompatibility, from X to Y and either X or Y, to distinguish opposites from semantically similar Distinguishing between antonyms and s"
E17-1008,E12-1004,0,0.0310582,"s then fed into a logistic regression layer whose target is the class label associated with the pair (x, y). Finally, the pair (x, y) is predicted as positive (i.e., antonymous) word pair if the probability of the prediction for ~vxy is larger than 0.5. 4 Baseline Models To compare AntSynNET with baseline models for pattern-based classification of antonyms and synonyms, we introduce two pattern-based baseline methods: the distributional method (Section 4.1), and the distributed method (Section 4.2). 4.1 3.3.2 Combined AntSynNET Inspired by the supervised distributional concatenation method in Baroni et al. (2012) and the integrated path-based and distributional method for hypernymy detection in Shwartz et al. (2016), we Distributional Baseline As a first baseline, we apply the approach by Roth and Schulte im Walde (2014), henceforth R&SiW. They used a vector space model to represent pairs of words by a combination of standard lexico80 5 syntactic patterns and discourse markers. In addition to the patterns, the discourse markers added information to express discourse relations, which in turn may indicate the specific semantic relation between the two words in a word pair. For example, contrast relation"
E17-1008,N15-1100,0,0.059795,"t the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Instead of taking into account all words in a window of a certain size for feature extraction, the authors experimented with only words of a certain part-of-speech, and restricted distributions. Santus et al. (2014) proposed a different method to distinguish antonyms from synonyms by identifying the most salient dimensions of meaning in vector representations and reporting a new average-precision-based distributional measure and an entropy-based measure. Ono et al. (2015) trained supervised word embeddings for the task of identifying antonymy. They proposed two models to learn word embeddings: the first model relied on thesaurus information; the second model made use of distributional information and thesaurus information. More recently, Nguyen et al. (2016) proposed two methods to distinguish antonyms from synonyms: in the first method, the authors improved the quality of weighted feature vectors by strengthening those features that are most salient in the vectors, and by putting less emphasis on those that are of minor importance when distinguishing degrees"
E17-1008,D14-1162,0,0.0912449,"emantic models (DSMs) offer a means to represent meaning vectors of words or word pairs, and to determine their semantic relatedness (Turney and Pantel, 2010). In co-occurrence models, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in particular contexts. When using word embeddings, these models rely on neural methods to represent words as low-dimensional vectors. To create the word embeddings, the models either make use of neural-based techniques, such as the skipgram model (Mikolov et al., 2013), or use matrix factorization (Pennington et al., 2014) that builds word embeddings by factorizing word-context cooccurrence matrices. In comparison to standard co-occurrence vector representations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.g., Vu et al. (2016)), and antonym-synonym distinction (e.g., Nguyen et al. (2016)). In pattern-based models, vector representations make use of lexico-syntactic surface patterns to distinguish between the relations of word pairs. For example, Justeso"
E17-1008,P14-2086,1,0.701568,"Missing"
E17-1008,C92-2082,0,0.564987,"ntations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.g., Vu et al. (2016)), and antonym-synonym distinction (e.g., Nguyen et al. (2016)). In pattern-based models, vector representations make use of lexico-syntactic surface patterns to distinguish between the relations of word pairs. For example, Justeson and Katz (1991) suggested that adjectival opposites co-occur with each other in specific linear sequences, such as between X and Y. Hearst (1992) determined surface patterns, e.g., X such as Y, to identify nominal hypernyms. Lin et al. (2003) proposed two textual patterns indicating semantic incompatibility, from X to Y and either X or Y, to distinguish opposites from semantically similar Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations. In this paper, we present a novel neural network model AntSynNET that expl"
E17-1008,E14-4008,1,0.844498,"ssigning signs to the entries in the cooccurrence matrix on which latent semantic analysis operates, such that synonyms would tend to have positive cosine similarities, and antonyms would tend to have negative cosine similarities. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Instead of taking into account all words in a window of a certain size for feature extraction, the authors experimented with only words of a certain part-of-speech, and restricted distributions. Santus et al. (2014) proposed a different method to distinguish antonyms from synonyms by identifying the most salient dimensions of meaning in vector representations and reporting a new average-precision-based distributional measure and an entropy-based measure. Ono et al. (2015) trained supervised word embeddings for the task of identifying antonymy. They proposed two models to learn word embeddings: the first model relied on thesaurus information; the second model made use of distributional information and thesaurus information. More recently, Nguyen et al. (2016) proposed two methods to distinguish antonyms f"
E17-1008,P82-1020,0,0.778913,"Missing"
E17-1008,I13-1056,1,0.844685,"istribution of patterns, Schwartz et al. used patterns to learn word embeddings. Vector representation methods: Yih et al. (2012) introduced a new vector representation where antonyms lie on opposite sides of a sphere. They derived this representation with the incorporation of a thesaurus and latent semantic analhttps://github.com/nguyenkh/AntSynNET 77 3.1 ysis, by assigning signs to the entries in the cooccurrence matrix on which latent semantic analysis operates, such that synonyms would tend to have positive cosine similarities, and antonyms would tend to have negative cosine similarities. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Instead of taking into account all words in a window of a certain size for feature extraction, the authors experimented with only words of a certain part-of-speech, and restricted distributions. Santus et al. (2014) proposed a different method to distinguish antonyms from synonyms by identifying the most salient dimensions of meaning in vector representations and reporting a new average-precision-based distributional measure and an entropy-base"
E17-1008,J91-1001,0,0.38196,", 2014) that builds word embeddings by factorizing word-context cooccurrence matrices. In comparison to standard co-occurrence vector representations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.g., Vu et al. (2016)), and antonym-synonym distinction (e.g., Nguyen et al. (2016)). In pattern-based models, vector representations make use of lexico-syntactic surface patterns to distinguish between the relations of word pairs. For example, Justeson and Katz (1991) suggested that adjectival opposites co-occur with each other in specific linear sequences, such as between X and Y. Hearst (1992) determined surface patterns, e.g., X such as Y, to identify nominal hypernyms. Lin et al. (2003) proposed two textual patterns indicating semantic incompatibility, from X to Y and either X or Y, to distinguish opposites from semantically similar Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have p"
E17-1008,K15-1026,0,0.22757,"contexts, which makes it challenging to automatically distinguish between them. Two families of approaches to differentiate between antonyms and synonyms are predominent 76 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 76–85, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics words. Roth and Schulte im Walde (2014) proposed a method that combined patterns with discourse markers for classifying paradigmatic relations including antonymy, synonymy, and hypernymy. Recently, Schwartz et al. (2015) used two prominent patterns from Lin et al. (2003) to learn word embeddings that distinguished antonyms from similar words in determining degrees of similarity and word analogy. In this paper, we present a novel patternbased neural method AntSynNET to distinguish antonyms from synonyms. We hypothesize that antonymous word pairs co-occur with each other in lexico-syntactic patterns within a sentence more often than would be expected by synonymous pairs. This hypothesis is inspired by corpus-based studies on antonymy and synonymy. Among others, Charles and Miller (1989) suggested that adjectiva"
E17-1008,P16-1226,0,0.10629,"Missing"
E17-1008,N16-1065,1,0.89008,"Missing"
E17-1008,D12-1111,0,0.184336,"Missing"
K18-1011,P16-1223,0,0.0467243,"Missing"
K18-1011,N18-1170,0,0.0572327,"Missing"
K18-1011,D17-1215,0,0.271609,"Luong et al., 2015) to compare the question and candidate answers, and a CNN to aggregate information. However, there is still an ongoing debate whether CNNs or RNNs are more suitable to NLP (Yin et al., 2017), and the behavioral differences between them are still under research. Many papers reported remarkable gains when combining these two models in ensembles (Deng and Platt, 2014; Zhou et al., 2015; Vu et al., 2016), since they process information in different ways and thus are complimentary to each other. Despite the seemingly high accuracies of many models on machine comprehension tasks, Jia and Liang (2017) argued that many questions in such datasets are easily solvable by superficial cues. They showed with adversarial examples that most models can be easily tricked by modifications on the data which do not confuse humans. Similarly, Sanchez et al. (2018) performed controlled experiments on the robustness of several Natural Language Inference models by altering hypernym, hyponym, and antonym relations in the data. Both We propose a machine reading comprehension model based on the compare-aggregate framework with two-staged attention that achieves state-of-the-art results on the MovieQA question"
K18-1011,D14-1181,0,0.0168886,"tasks including computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012) and more recently natural language processing (NLP) (Collobert et al., 2011). Neural-based NLP systems often use word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013) which are then fed into a convolutional neural network (CNN) (LeCun et al., 1990; Waibel et al., 1990) or a recurrent neural network (RNN) (Elman, 1990; Hochreiter and Schmidhuber, 1997) for further classification. These approaches proved to be successful for many NLP tasks (Mikolov et al., 2010; Kim, 2014; Hu et al., 2014; Bahdanau et al., 2014). Along with the success of DL in a wide range of applications, adversarial examples (Goodfellow et al., 2014) - that aim to confuse the system - have gained popularity in a wide range of research communities such as computer vision and NLP, since they can reveal the limitations in the generalizability of the models. As opposed to adversarial examples in computer vision, which are computed 108 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 108–118 c Brussels, Belgium, October 31 - November 1, 2018. 2018"
K18-1011,D15-1166,0,0.412613,"ion such as the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014) cannot be directly applied to NLP. Machine comprehension has recently received increased interest in the NLP community (Yang et al., 2015; Tapaswi et al., 2016; Rajpurkar et al., 2016; Chen et al., 2016). Neural network models perform reasonably well on many data sets with different question answering setups, e.g. multiple choice or answer generation (Wang and Jiang, 2016; Liu et al., 2017; Yu et al., 2018). Among others, Wang and Jiang (2016) proposed the compare-aggregate framework, which uses an attention mechanism (Luong et al., 2015) to compare the question and candidate answers, and a CNN to aggregate information. However, there is still an ongoing debate whether CNNs or RNNs are more suitable to NLP (Yin et al., 2017), and the behavioral differences between them are still under research. Many papers reported remarkable gains when combining these two models in ensembles (Deng and Platt, 2014; Zhou et al., 2015; Vu et al., 2016), since they process information in different ways and thus are complimentary to each other. Despite the seemingly high accuracies of many models on machine comprehension tasks, Jia and Liang (2017"
K18-1011,N16-1174,0,0.053429,"odel in two aspects that lead to significant improvements. Given a preprocessed matrix-representation of the question Q, a text (movie plot) P , and k answer candidates A1 . . . Ak , the main idea of Wang and Jiang (2016)’s compare-aggregate model is to compare P to Q and each Aj and then aggregate this information into a vector to derive a confidence score cj for each answer candidate. Wang and Jiang (2016) concatenate all plot sentences and do not leverage the inherent structuring of the text into sentences. Inspired by the recent success of hierarchical models in NLP (Sordoni et al., 2015; Yang et al., 2016; Liu et al., 2017) we extend the model to perform comparison and aggregation on the word and sentence G = softmax X T P  (2) H = XG, (3) where X on the word level represents Q or an answer candidate Aj and on the sentence level rq or raj .3 Comparison The comparison operation performs an element-wise comparison of each hl in H with its counterparts ql /ajl on the word level and rq /raj on the sentence level, respectively. Wang and Jiang (2016) compared many comparison functions. Here we use only the SUBMULT function since it performed best for MovieQA:  tl = ReLU(W (xl − hl ) (xl − hl ) xl"
K18-1011,D14-1162,0,0.0799085,"Missing"
K18-1011,D16-1264,0,0.0341155,"Institute for Natural Language Processing (IMS) Universit¨at Stuttgart, Germany {blohmms,jagfelga,soodea,xiangyu,thangvu} @ims.uni-stuttgart.de Abstract on continuous data and can thus easily be imperceptible if desired, adversarial attacks in NLP entail the necessity to perform discrete and perceptible changes to the data. Thus, attack methods for computer vision such as the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014) cannot be directly applied to NLP. Machine comprehension has recently received increased interest in the NLP community (Yang et al., 2015; Tapaswi et al., 2016; Rajpurkar et al., 2016; Chen et al., 2016). Neural network models perform reasonably well on many data sets with different question answering setups, e.g. multiple choice or answer generation (Wang and Jiang, 2016; Liu et al., 2017; Yu et al., 2018). Among others, Wang and Jiang (2016) proposed the compare-aggregate framework, which uses an attention mechanism (Luong et al., 2015) to compare the question and candidate answers, and a CNN to aggregate information. However, there is still an ongoing debate whether CNNs or RNNs are more suitable to NLP (Yin et al., 2017), and the behavioral differences between them are"
K18-1011,N18-1179,0,0.0246193,"em are still under research. Many papers reported remarkable gains when combining these two models in ensembles (Deng and Platt, 2014; Zhou et al., 2015; Vu et al., 2016), since they process information in different ways and thus are complimentary to each other. Despite the seemingly high accuracies of many models on machine comprehension tasks, Jia and Liang (2017) argued that many questions in such datasets are easily solvable by superficial cues. They showed with adversarial examples that most models can be easily tricked by modifications on the data which do not confuse humans. Similarly, Sanchez et al. (2018) performed controlled experiments on the robustness of several Natural Language Inference models by altering hypernym, hyponym, and antonym relations in the data. Both We propose a machine reading comprehension model based on the compare-aggregate framework with two-staged attention that achieves state-of-the-art results on the MovieQA question answering dataset. To investigate the limitations of our model as well as the behavioral difference between convolutional and recurrent neural networks, we generate adversarial examples to confuse the model and compare to human performance. Furthermore,"
K18-1011,N16-1065,1,0.904456,"Missing"
K18-1011,D15-1237,0,0.0805056,"Missing"
N16-1065,P15-1061,0,0.36422,"on task. 1 Introduction Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the &lt;e1&gt;milk&lt;/e1&gt; into the &lt;e2&gt;pumpkin mixture&lt;/e2&gt;.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) . This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions: (1) We propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to th"
N16-1065,S10-1006,0,0.0250728,"Missing"
N16-1065,D14-1181,0,0.016276,"ed task participants used, i.a., support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010). Recently, their results on this data set were outperformed by applying NNs (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015). 534 Proceedings of NAACL-HLT 2016, pages 534–539, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Zeng et al. (2014) built a CNN based only on the context between the relation arguments and extended it with several lexical features. Kim (2014) and others used convolutional filters of different sizes for CNNs. Nguyen and Grishman (2015) applied this to relation classification and obtained improvements over single filter sizes. Dos Santos et al. (2015) replaced the softmax layer of the CNN with a ranking layer. They showed improvements and published the best result so far on the SemEval dataset, to our knowledge. Socher et al. (2012) used another NN architecture for relation classification: recursive neural networks that built recursive sentence representations based on syntactic parsing. In contrast, Zhang and Wang (2015) investigat"
N16-1065,W15-1506,0,0.342383,"Eval 2010 relation classification task. 1 Introduction Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the &lt;e1&gt;milk&lt;/e1&gt; into the &lt;e2&gt;pumpkin mixture&lt;/e2&gt;.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) . This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions: (1) We propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pay"
N16-1065,S10-1057,0,0.0908329,"w that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task. 1 Introduction Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the &lt;e1&gt;milk&lt;/e1&gt; into the &lt;e2&gt;pumpkin mixture&lt;/e2&gt;.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) . This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions: (1) We propose extended middle context, a new context representation for CNNs for rela"
N16-1065,D12-1110,0,0.101558,"neural models achieve state-of-the-art results on the SemEval 2010 relation classification task. 1 Introduction Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the &lt;e1&gt;milk&lt;/e1&gt; into the &lt;e2&gt;pumpkin mixture&lt;/e2&gt;.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) . This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions: (1) We propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation ar"
N16-1065,S10-1049,0,0.342279,"nal and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task. 1 Introduction Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the &lt;e1&gt;milk&lt;/e1&gt; into the &lt;e2&gt;pumpkin mixture&lt;/e2&gt;.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) . This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions: (1) We propose extended middle context, a new context representation for CNNs for relation classification. The"
N16-1065,C14-1220,0,0.254999,"state-of-the-art results on the SemEval 2010 relation classification task. 1 Introduction Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the &lt;e1&gt;milk&lt;/e1&gt; into the &lt;e2&gt;pumpkin mixture&lt;/e2&gt;.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers (Rink and Harabagiu, 2010a; Tratz and Hovy, 2010), recent research showed performance improvements by applying neural networks (NNs) (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014; Nguyen and Grishman, 2015; Dos Santos et al., 2015; Zhang and Wang, 2015) on the benchmark data from SemEval 2010 shared task 8 (Hendrickx et al., 2010) . This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions: (1) We propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between th"
N16-1065,W09-2415,0,\N,Missing
N18-2032,J15-4004,0,0.453705,", gender, number, and tense, thus differing strongly from Western European languages. We introduce two novel datasets for Vietnamese: a dataset of lexical contrast pairs ViCon to distinguish between similarity (synonymy) and dissimilarity (antonymy), and a dataset of semantic relation pairs ViSim-400 to reflect the continuum between similarity and relatedness. The two datasets are publicly available.1 Moreover, we verify our novel datasets through standard and neural co-occurrence models, in order to show that we obtain a similar behaviour as for the corresponding English datasets SimLex-999 (Hill et al., 2015), and the lexical contrast dataset (henceforth LexCon), cf. Nguyen et al. (2016a). We present two novel datasets for the lowresource language Vietnamese to assess models of semantic similarity: ViCon comprises pairs of synonyms and antonyms across word classes, thus offering data to distinguish between similarity and dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets. 1 Introduction C"
N18-2032,P12-1015,0,0.0300536,"al., 2006), and machine translation (He et al., 2008; Marton et al., 2009). In order to evaluate these models, gold standard resources with word pairs have to be collected (typically across semantic relations such as synonymy, hypernymy, antonymy, co-hyponymy, meronomy, etc.) and annotated for their degree of similarity via human judgements. The most prominent examples of gold standard similarity resources for English are the Rubenstein & Goodenough (RG) dataset (Rubenstein and Goodenough, 1965), the TOEFL test questions (Landauer and Dumais, 1997), WordSim353 (Finkelstein et al., 2001), MEN (Bruni et al., 2012), SimLex-999 (Hill et al., 2015), and the lexical contrast datasets by (Nguyen et al., 2016a, 2017). For other languages, resource examples are the translation of the RG dataset to German (Gurevych, 2005), the German dataset of paradigmatic relations (Scheible and Schulte im Walde, 2 Related Work Over the years a number of datasets have been collected for studying and evaluating semantic similarity and semantic relatedness. For English, Rubenstein and Goodenough (1965) presented a small dataset (RG) of 65 noun pairs. For each pair, the degree of similarity in meaning was provided by 15 raters."
N18-2032,J06-1003,0,0.187743,"nt two novel datasets for the lowresource language Vietnamese to assess models of semantic similarity: ViCon comprises pairs of synonyms and antonyms across word classes, thus offering data to distinguish between similarity and dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets. 1 Introduction Computational models that distinguish between semantic similarity and semantic relatedness (Budanitsky and Hirst, 2006) are important for many NLP applications, such as the automatic generation of dictionaries, thesauri, and ontologies (Biemann, 2005; Cimiano et al., 2005; Li et al., 2006), and machine translation (He et al., 2008; Marton et al., 2009). In order to evaluate these models, gold standard resources with word pairs have to be collected (typically across semantic relations such as synonymy, hypernymy, antonymy, co-hyponymy, meronomy, etc.) and annotated for their degree of similarity via human judgements. The most prominent examples of gold standard similarity resources for English are the Rubenstei"
N18-2032,P06-1129,0,0.126557,"Missing"
N18-2032,D09-1040,0,0.0267108,"ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets. 1 Introduction Computational models that distinguish between semantic similarity and semantic relatedness (Budanitsky and Hirst, 2006) are important for many NLP applications, such as the automatic generation of dictionaries, thesauri, and ontologies (Biemann, 2005; Cimiano et al., 2005; Li et al., 2006), and machine translation (He et al., 2008; Marton et al., 2009). In order to evaluate these models, gold standard resources with word pairs have to be collected (typically across semantic relations such as synonymy, hypernymy, antonymy, co-hyponymy, meronomy, etc.) and annotated for their degree of similarity via human judgements. The most prominent examples of gold standard similarity resources for English are the Rubenstein & Goodenough (RG) dataset (Rubenstein and Goodenough, 1965), the TOEFL test questions (Landauer and Dumais, 1997), WordSim353 (Finkelstein et al., 2001), MEN (Bruni et al., 2012), SimLex-999 (Hill et al., 2015), and the lexical contr"
N18-2032,goldhahn-etal-2012-building,0,0.0182433,"E model (Nguyen et al., 2016a), and the mLCM model (Pham et al., 2015). Both the dLCE and the mLCM models integrated lexical contrast information into the basic Skipgram model to train word embeddings for distinguishing antonyms from synonyms, and for reflecting degrees of similarity. The three models were trained with 300 dimensions, a window size of 5 words, and 10 negative samples. Regarding the corpora, we relied on Vietnamese corpora with a total of ≈145 million tokens, including the Vietnamese Wikipedia,9 VNESEcorpus and VNTQcorpus,10 and the Leipzig Corpora Collection for Vietnamese11 (Goldhahn et al., 2012). For word segmentation and POS tagging, we used the opensource toolkit UETnlp12 (Nguyen and Le, 2016). The antonym and synonym pairs to train the 9 https://dumps.wikimedia.org/viwiki/latest/ http://viet.jnlp.org/ download-du-lieu-tu-vung-corpus 11 http://wortschatz.uni-leipzig.de/en/download 12 https://github.com/phongnt570/UETnlp 10 202 dLCE and mLCM models were extracted from VWN consisting of 49,458 antonymous pairs and 338,714 synonymous pairs. All pairs which appeared in ViSim-400 were excluded from this set. Table 2 shows Spearman’s correlations ρ, comparing the scores of the three mode"
N18-2032,W04-2607,0,0.065819,"of synonymy and co-hyponymy. For example, in Vietnamese “đội” (team) and “nhóm” (group) represents a synonym pair; “ô_tô” (car) and “xe_đạp” (bike) is a co-hyponymy pair. More specifically, words in the pair “ô_tô” (car) and “xe_đạp” (bike) share several features such as physical (e.g. bánh_xe / wheels) and functional (e.g. vận_tải / transport), so that the two Vietnamese words are interchangeable regarding the kinds of transportation. The concept of semantic relatedness is broader and holds for relations such as meronymy, antonymy, functional association, and other “nonclassical relations” (Morris and Hirst, 2004). For example, “ô_tô” (car) and “xăng_dầu” (petrol) represent a meronym pair. In contrast to similarity, this meronym pair expresses a clearly functional relationship; the words are strongly associated with each other but not similar. Empirical studies have shown that the predictions of distributional models as well as humans are strongly related to the part-of-speech (POS) category of the learned concepts. Among others, Gentner (2006) showed that verb concepts are harder to learn by children than noun concepts. Distinguishing antonymy from synonymy is one of the most difficult challenges. Whi"
N18-2032,I05-1067,0,0.306203,"such as synonymy, hypernymy, antonymy, co-hyponymy, meronomy, etc.) and annotated for their degree of similarity via human judgements. The most prominent examples of gold standard similarity resources for English are the Rubenstein & Goodenough (RG) dataset (Rubenstein and Goodenough, 1965), the TOEFL test questions (Landauer and Dumais, 1997), WordSim353 (Finkelstein et al., 2001), MEN (Bruni et al., 2012), SimLex-999 (Hill et al., 2015), and the lexical contrast datasets by (Nguyen et al., 2016a, 2017). For other languages, resource examples are the translation of the RG dataset to German (Gurevych, 2005), the German dataset of paradigmatic relations (Scheible and Schulte im Walde, 2 Related Work Over the years a number of datasets have been collected for studying and evaluating semantic similarity and semantic relatedness. For English, Rubenstein and Goodenough (1965) presented a small dataset (RG) of 65 noun pairs. For each pair, the degree of similarity in meaning was provided by 15 raters. The RG dataset is assumed to reflect similarity rather than relatedness. Finkelstein et al. (2001) created a set of 353 English nounnoun pairs (WordSim-353)2 , where each pair was rated by 16 subjects ac"
N18-2032,E17-1008,1,0.799014,"Missing"
N18-2032,P16-2074,1,0.652555,"guages. We introduce two novel datasets for Vietnamese: a dataset of lexical contrast pairs ViCon to distinguish between similarity (synonymy) and dissimilarity (antonymy), and a dataset of semantic relation pairs ViSim-400 to reflect the continuum between similarity and relatedness. The two datasets are publicly available.1 Moreover, we verify our novel datasets through standard and neural co-occurrence models, in order to show that we obtain a similar behaviour as for the corresponding English datasets SimLex-999 (Hill et al., 2015), and the lexical contrast dataset (henceforth LexCon), cf. Nguyen et al. (2016a). We present two novel datasets for the lowresource language Vietnamese to assess models of semantic similarity: ViCon comprises pairs of synonyms and antonyms across word classes, thus offering data to distinguish between similarity and dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets. 1 Introduction Computational models that distinguish between semantic similarity and semantic r"
N18-2032,D08-1011,0,0.0334893,"nd dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets. 1 Introduction Computational models that distinguish between semantic similarity and semantic relatedness (Budanitsky and Hirst, 2006) are important for many NLP applications, such as the automatic generation of dictionaries, thesauri, and ontologies (Biemann, 2005; Cimiano et al., 2005; Li et al., 2006), and machine translation (He et al., 2008; Marton et al., 2009). In order to evaluate these models, gold standard resources with word pairs have to be collected (typically across semantic relations such as synonymy, hypernymy, antonymy, co-hyponymy, meronomy, etc.) and annotated for their degree of similarity via human judgements. The most prominent examples of gold standard similarity resources for English are the Rubenstein & Goodenough (RG) dataset (Rubenstein and Goodenough, 1965), the TOEFL test questions (Landauer and Dumais, 1997), WordSim353 (Finkelstein et al., 2001), MEN (Bruni et al., 2012), SimLex-999 (Hill et al., 2015),"
N18-2032,2016.gwc-1.38,0,0.125361,"guages. We introduce two novel datasets for Vietnamese: a dataset of lexical contrast pairs ViCon to distinguish between similarity (synonymy) and dissimilarity (antonymy), and a dataset of semantic relation pairs ViSim-400 to reflect the continuum between similarity and relatedness. The two datasets are publicly available.1 Moreover, we verify our novel datasets through standard and neural co-occurrence models, in order to show that we obtain a similar behaviour as for the corresponding English datasets SimLex-999 (Hill et al., 2015), and the lexical contrast dataset (henceforth LexCon), cf. Nguyen et al. (2016a). We present two novel datasets for the lowresource language Vietnamese to assess models of semantic similarity: ViCon comprises pairs of synonyms and antonyms across word classes, thus offering data to distinguish between similarity and dissimilarity. ViSim-400 provides degrees of similarity across five semantic relations, as rated by human judges. The two datasets are verified through standard co-occurrence and neural network models, showing results comparable to the respective English datasets. 1 Introduction Computational models that distinguish between semantic similarity and semantic r"
N18-2032,D07-1042,0,0.0971335,"Missing"
N18-2032,P15-2004,0,0.0246537,"ility of the annotated dataset. Furthermore, the box plots in Figure 1 present the distributions of all rated pairs in terms of the fine-grained semantic relations across word classes. They reveal that –across word classes– synonym pairs are clearly rated as the most similar words, and antonym as well as unrelated pairs 4.1 Verification of ViSim-400 We adopt a comparison of neural models on SimLex-999 as suggested by Nguyen et al. (2016a). They applied three models, a Skip-gram model with negative sampling SGNS (Mikolov et al., 2013), the dLCE model (Nguyen et al., 2016a), and the mLCM model (Pham et al., 2015). Both the dLCE and the mLCM models integrated lexical contrast information into the basic Skipgram model to train word embeddings for distinguishing antonyms from synonyms, and for reflecting degrees of similarity. The three models were trained with 300 dimensions, a window size of 5 words, and 10 negative samples. Regarding the corpora, we relied on Vietnamese corpora with a total of ≈145 million tokens, including the Vietnamese Wikipedia,9 VNESEcorpus and VNTQcorpus,10 and the Leipzig Corpora Collection for Vietnamese11 (Goldhahn et al., 2012). For word segmentation and POS tagging, we used"
N18-2032,D10-1114,0,0.0294189,"ms of six relations: the five extracted relations from VCL and VWN, 8 Annotation of ViSim-400 3.5 Agreement in ViSim-400 We analysed the ratings of the ViSim-400 annotators with two different inter-annotator agreement (IAA) measures, Krippendorff’s alpha coefficient (Krippendorff, 2004), and the average standard deviation (STD) of all pairs across word classes. The first IAA measure, IAA-pairwise, computes the average pairwise Spearman’s ρ correlation between any two raters. This IAA measure has been a common choice in previous data collections in distributional semantics (Pad´o et al., 2007; Reisinger and Mooney, 2010; Hill et al., 2015). http://viet.wordnet.vn/wnms/ 201 ANT COHYPO HOLO HYPE SYN 6 6 6 4 4 4 2 2 2 0 0 UNREL 0 Adj Noun Verb Figure 1: Distribution of scored pairs in ViSim-400 across parts-of-speech and semantic relations. IAA-Mean ρ IAA-Pairwise ρ Krippendorff’s α STD All Noun Verb Adjective 0.86 0.79 0.78 0.87 0.86 0.76 0.76 0.87 0.86 0.78 0.78 0.90 0.78 0.75 0.86 0.82 are clearly rated as the most dissimilar words. Hypernymy, co-hyponymy and holonymy are in between, but rather similar than dissimilar. 4 Verification of Datasets In this section, we verify our novel datasets ViCon and ViSim-4"
N18-2032,W14-5814,1,0.906451,"Missing"
N18-2032,J17-4004,0,0.0591779,"Missing"
P13-2037,N03-2002,0,0.0646711,"t recurrent neural network language models (RNNLMs) can improve perplexity and error rates in speech recognition systems in comparison to traditional n-gram approaches (Mikolov et al., 2010; Mikolov et al., 2011). One reason for that is their ability to handle longer contexts. Furthermore, the integration of additional features as input is rather straightforward due to their structure. On the other hand, factored language models (FLMs) have been used successfully for languages with rich morphology due to their ability to process syntactical features, such as word stems or part-of-speech tags (Bilmes and Kirchhoff, 2003; El-Desoky et al., 2010). The main contribution of this paper is the application of RNNLMs and FLMs to the challenging task of Code-Switching. Furthermore, the two different models are combined using linear interpolation. In addition, a comparison between them is provided including a detailed analysis to explain their results. In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate partof-speech tags (POS) and language informati"
P13-2037,D08-1102,0,0.508751,"Missing"
P13-2037,C04-1022,0,0.38513,"es, POS tags or word stems. Hence, it provides another possibility to integrate syntactical features into the language modeling process. In (Bilmes and Kirchhoff, 2003), it is shown that factored language models are able to outperform standard n-gram techniques in terms of perplexity. In the same paper, generalized parallel backoff is introduced. This technique can be used to generalize traditional backoff methods and to improve the performance of factored language models. Due to the integration of various features, it is possible to handle rich morphology in languages like Arabic or Turkish (Duh and Kirchhoff, 2004; El-Desoky et al., 2010). Code-Switching Language Modeling 3.1 Motivation Since there is a lack of Code-Switching data, language modeling is a challenging task. Traditional n-gram approaches may not provide reliable estimates. Hence, more general features than words should be integrated into the language models. Therefore, we apply recurrent neural networks and factored language models. As features, we use part-of-speech tags and language identifiers. 3.2 Using Recurrent Neural Networks As Language Model This section describes the structure of the recurrent neural network (RNNLM) that we use"
P13-2037,W00-1308,0,0.137886,"Missing"
P13-2037,N10-1104,0,0.040109,"Missing"
P13-2037,N03-1033,0,0.0531177,"aining text Evaluation For evaluation, we compute the perplexity of each language model on the SEAME development and evaluation set und perform an analysis of the different back-off levels to understand in detail the behavior of each language model. A traditional 3gram LM trained with the SEAME transcriptions serves as baseline. POS Tagger for Code-Switching Speech To be able to assign part-of-speech tags to our bilingual text corpus, we apply the POS tagger described in (Schultz et al., 2010) and (Adel, Vu et al., 2013). It consists of two different monolingual (Stanford log-linear) taggers (Toutanova et al., 2003; Toutanova et al., 2000) and a combination of their results. While (Solorio et al., 2008b) passes the whole Code-Switching text to both monolingual taggers and combines their results using different heuristics, in this work, the text is splitted into different languages first. The tagging process is illustrated in figure 3. Mandarin is determined as matrix language (the main language of an utterance) and English as embedded language. If three or more words of the embedded language are detected, they are passed to the English tagger. The rest of the text is passed to the Mandarin tagger, even"
P13-2037,J93-2004,0,0.0431218,"Missing"
P13-2037,D08-1110,0,0.393588,"Missing"
P16-2074,D14-1151,0,0.0570947,"Missing"
P16-2074,J06-1003,0,0.16654,"Lyons, 1977). From a computational point of view, distinguishing between antonymy and synonymy is important for NLP applications such as Machine Translation and Textual Entailment, which go beyond a general notion of semantic relatedness and require to identify specific semantic relations. However, due to interchangeable substitution, antonyms and synonyms often occur in similar contexts, which makes it challenging to automatically distinguish between them. Distributional semantic models (DSMs) offer a means to represent meaning vectors of words and to determine their semantic “relatedness” (Budanitsky and Hirst, 2006; Turney and Pantel, 2010). 454 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 454–459, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics information (LMI) (Evert, 2005) to determine the original strengths of the word features. Our score weightSA (w, f ) subsequently defines the weights of a target word w and a feature f : posed thesaurus-based word embeddings to capture antonyms. They proposed two models: the WE-T model that trains word embeddings on thesaurus information; and the WE-TD model that incorporated"
P16-2074,P15-2004,0,0.459613,"the 54th Annual Meeting of the Association for Computational Linguistics, pages 454–459, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics information (LMI) (Evert, 2005) to determine the original strengths of the word features. Our score weightSA (w, f ) subsequently defines the weights of a target word w and a feature f : posed thesaurus-based word embeddings to capture antonyms. They proposed two models: the WE-T model that trains word embeddings on thesaurus information; and the WE-TD model that incorporated distributional information into the WET model. Pham et al. (2015) introduced the multitask lexical contrast model (mLCM) by incorporating WordNet into a skip-gram model to optimize semantic vectors to predict contexts. Their model outperformed standard skip-gram models with negative sampling on both general semantic tasks and distinguishing antonyms from synonyms. In this paper, we propose two approaches that make use of lexical contrast information in distributional semantic space and word embeddings for antonym–synonym distinction. Firstly, we incorporate lexical contrast into distributional vectors and strengthen those word features that are most salient"
P16-2074,P14-2086,1,0.917612,"Missing"
P16-2074,J15-4004,0,0.192303,"information, we used WordNet (Miller, 1995) and Wordnik1 to collect antonyms and synonyms, obtaining a total of 363,309 synonym and 38,423 antonym pairs. 3.2 3.3 The second experiment evaluates the performance of our dLCE model on both antonym–synonym distinction and a word similarity task. The similarity task requires to predict the degree of similarity for word pairs, and the ranked list of predictions is evaluated against a gold standard of human ratings, relying on the Spearman rank-order correlation coefficient ρ (Siegel and Castellan, 1988). In this paper, we use the SimLex-999 dataset (Hill et al., 2015) to evaluate word embedding models on predicting similarities. The resource contains 999 word pairs (666 noun, 222 verb and 111 adjective pairs) and was explicitly built to test models on capturing similarity rather than relatedness or association. Table 2 shows that our dLCE model outperforms both SGNS and mLCM, proving that the lexical contrast information has a positive effect on predicting similarity. Distinguishing antonyms from synonyms The first experiment evaluates our lexical contrast vectors by applying the vector representations with the improved weightSA scores to the task of disti"
P16-2074,E14-4008,1,0.905876,"riples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns ‘from X to Y’ or ‘either X or Y’ significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations). Lately, antonym–synonym distinction has also been a focus of word embedding models. For example, Adel and Sch¨utze (2014) integrated coreference chains extracted from large corpora into a skip-gram model to create word embeddings that identified antonyms. Ono et al. (2015) proWe propose a novel vector repre"
P16-2074,Y14-1018,0,0.235039,"riples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns ‘from X to Y’ or ‘either X or Y’ significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations). Lately, antonym–synonym distinction has also been a focus of word embedding models. For example, Adel and Sch¨utze (2014) integrated coreference chains extracted from large corpora into a skip-gram model to create word embeddings that identified antonyms. Ono et al. (2015) proWe propose a novel vector repre"
P16-2074,schafer-bildhauer-2012-building,0,0.0865315,"Missing"
P16-2074,N13-1090,0,0.462808,"h negative sampling on both general semantic tasks and distinguishing antonyms from synonyms. In this paper, we propose two approaches that make use of lexical contrast information in distributional semantic space and word embeddings for antonym–synonym distinction. Firstly, we incorporate lexical contrast into distributional vectors and strengthen those word features that are most salient for determining word similarities, assuming that feature overlap in synonyms is stronger than feature overlap in antonyms. Secondly, we propose a novel extension of a skip-gram model with negative sampling (Mikolov et al., 2013b) that integrates the lexical contrast information into the objective function. The proposed model optimizes the semantic vectors to predict degrees of word similarity and also to distinguish antonyms from synonyms. The improved word embeddings outperform state-of-the-art models on antonym– synonym distinction and a word similarity task. 2 P 1 weightSA (w, f ) = #(w,u) sim(w, u) P P u∈W (f )∩S(w) 1 0 − #(w0 ,v) v∈W (f )∩S(w0 ) sim(w , v) w0 ∈A(w) The new weightSA scores of a target word w and a feature f exploit the differences between the average similarities of synonyms to the target word ("
P16-2074,I13-1056,1,0.890467,"allenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies. For example, Lin et al. (2003) used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns ‘from X to Y’ or ‘either X or Y’ significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations). Lately, antonym–synonym distinction has also been a focus of word embedding models. For example, Adel and Sch¨utze (2014) integrated core"
P16-2074,J13-3004,0,0.0815755,"th synonyms (such as formal–conventional) and antonyms (such as formal–informal) as related words and cannot sufficiently distinguish between the two relations. In recent years, a number of distributional approaches have accepted the challenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies. For example, Lin et al. (2003) used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns ‘from X to Y’ or ‘either X or Y’ significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-b"
P16-2074,N15-1100,0,0.186051,"l by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations). Lately, antonym–synonym distinction has also been a focus of word embedding models. For example, Adel and Sch¨utze (2014) integrated coreference chains extracted from large corpora into a skip-gram model to create word embeddings that identified antonyms. Ono et al. (2015) proWe propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66–0.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex999, and on distingui"
P17-2106,P16-1231,0,0.0211169,"?1 , ?2 , ?3 , ?? , ?1 , ?2 , ?3 are all the parameters that will be learned through back propagation with averaged stochastic gradient descent in mini-batches. Note that Weiss et al. (2015) directly concatenate the embeddings of the words, tags, and labels of all the tokens together as input to the hidden layer. Instead, we first group the embeddings of the word, tag, and label of each token and compute 2 We only experiment with the greedy parser, since this paper focuses on the character-level input features and is independent of the global training and inference as in Weiss et al. (2015); Andor et al. (2016). 3 The tokens in the stack and buffer do not have labels yet, we use a special label &lt;NOLABEL&gt; instead. We do not use the highway networks since it did not improve the performance in preliminary experiments. 5 The details of the padding is in Appendix B. 4 673 Model Int Ext WORD W2V LSTM CNN LSTM+WORD CNN+WORD LSTM+W2V CNN+W2V B15-WORD B15-LSTM BestPub Ara 84.50 85.11 83.42 84.65 84.75 84.58 85.35 85.67 83.46 83.40 86.21 Baq 77.87 79.07 82.97 83.91 83.43 84.22 83.94 84.37 73.56 78.61 85.70 Fre 82.20 82.73 81.35 82.41 82.25 81.79 83.04 83.09 82.03 81.08 85.66 Ger 85.35 86.60 85.34 85.61 85.56"
P17-2106,W06-2922,0,0.0396418,"Missing"
P17-2106,D15-1041,0,0.216656,"chverarbeitung Universität Stuttgart {xiangyu,thangvu}@ims.uni-stuttgart.de Pre-trained word embeddings can alleviate the OOV problem by expanding the vocabulary, but it does not model the morphological information. Instead of looking up word embeddings, many researchers propose to compose the word representation from characters for various tasks, e.g., part-of-speech tagging (dos Santos and Zadrozny, 2014; Plank et al., 2016), named entity recognition (dos Santos and Guimarães, 2015), language modeling (Ling et al., 2015), machine translation (Costa-jussà and Fonollosa, 2016). In particular, Ballesteros et al. (2015) use a bidirectional long short-term memory (LSTM) character model for dependency parsing. Kim et al. (2016) present a convolutional neural network (CNN) character model for language modeling, but make no comparison among the character models, and state that “it remains open as to which character composition model (i.e., LSTM or CNN) performs better”. We propose to apply the CNN model by Kim et al. (2016) in a greedy transition-based dependency parser with feed-forward neural networks (Chen and Manning, 2014; Weiss et al., 2015). This model requires no extra unlabeled data but performs better"
P17-2106,W13-4916,0,0.0642326,"Missing"
P17-2106,P16-2058,0,0.0315912,"u and Ngoc Thang Vu Institut für Maschinelle Sprachverarbeitung Universität Stuttgart {xiangyu,thangvu}@ims.uni-stuttgart.de Pre-trained word embeddings can alleviate the OOV problem by expanding the vocabulary, but it does not model the morphological information. Instead of looking up word embeddings, many researchers propose to compose the word representation from characters for various tasks, e.g., part-of-speech tagging (dos Santos and Zadrozny, 2014; Plank et al., 2016), named entity recognition (dos Santos and Guimarães, 2015), language modeling (Ling et al., 2015), machine translation (Costa-jussà and Fonollosa, 2016). In particular, Ballesteros et al. (2015) use a bidirectional long short-term memory (LSTM) character model for dependency parsing. Kim et al. (2016) present a convolutional neural network (CNN) character model for language modeling, but make no comparison among the character models, and state that “it remains open as to which character composition model (i.e., LSTM or CNN) performs better”. We propose to apply the CNN model by Kim et al. (2016) in a greedy transition-based dependency parser with feed-forward neural networks (Chen and Manning, 2014; Weiss et al., 2015). This model requires no"
P17-2106,P15-1032,0,0.153546,"anslation (Costa-jussà and Fonollosa, 2016). In particular, Ballesteros et al. (2015) use a bidirectional long short-term memory (LSTM) character model for dependency parsing. Kim et al. (2016) present a convolutional neural network (CNN) character model for language modeling, but make no comparison among the character models, and state that “it remains open as to which character composition model (i.e., LSTM or CNN) performs better”. We propose to apply the CNN model by Kim et al. (2016) in a greedy transition-based dependency parser with feed-forward neural networks (Chen and Manning, 2014; Weiss et al., 2015). This model requires no extra unlabeled data but performs better than using pre-trained word embeddings. Furthermore, it can be combined with word embeddings from the lookup table since they capture different aspects of word similarities. Experimental results show that the CNN model works especially well on agglutinative languages, where the OOV rates are high. On other morphologically rich languages, the CNN model also performs at least as good as the word-lookup model. Furthermore, our CNN model outperforms both the original and our re-implementation of the bidirectional LSTM model by Balle"
P17-2106,D15-1176,0,0.156876,"on Morphologically Rich Languages Xiang Yu and Ngoc Thang Vu Institut für Maschinelle Sprachverarbeitung Universität Stuttgart {xiangyu,thangvu}@ims.uni-stuttgart.de Pre-trained word embeddings can alleviate the OOV problem by expanding the vocabulary, but it does not model the morphological information. Instead of looking up word embeddings, many researchers propose to compose the word representation from characters for various tasks, e.g., part-of-speech tagging (dos Santos and Zadrozny, 2014; Plank et al., 2016), named entity recognition (dos Santos and Guimarães, 2015), language modeling (Ling et al., 2015), machine translation (Costa-jussà and Fonollosa, 2016). In particular, Ballesteros et al. (2015) use a bidirectional long short-term memory (LSTM) character model for dependency parsing. Kim et al. (2016) present a convolutional neural network (CNN) character model for language modeling, but make no comparison among the character models, and state that “it remains open as to which character composition model (i.e., LSTM or CNN) performs better”. We propose to apply the CNN model by Kim et al. (2016) in a greedy transition-based dependency parser with feed-forward neural networks (Chen and Man"
P17-2106,W04-0308,0,0.0926344,"Missing"
P17-2106,P16-2067,0,0.0554183,"Missing"
P17-2106,W14-6111,0,0.0385994,"Missing"
P19-3016,P15-2130,0,0.0693005,"Missing"
P19-3016,P17-1163,0,0.0546112,"Missing"
P19-3016,N07-2038,0,0.205724,"n outputting something for that turn or asking for user input, the system steps through an additional turn, using the rewritten utterance as the new user input. In this way the meta policy is able to intelligently coordinate switching or combining domains, preserving as much information as possible to make as informed of a decision as possible. This architecture can be seen in figure 1. Yes NLU T Polic IN User Simulator To support automatic evaluation and enable RL, we implemented a user simulator to provide user actions at the intention level. For this purpose, we integrated the Agendabased (Schatzmann et al., 2007) user simulator into our framework. The task of the system is to fulfil the user’s goal within the course of the dialog. For this purpose, we populated our agendas with actions requesting information and informing about the constraints based on the specified goal. For the design of a user simulator, we additionally considered that its objective was not only to fulfil the user’s goal but also to support the RL policy in the learning process. Therefore, it was not sufficient to answer the system’s request and fulfil the user’s goal, but also to force the system to answer with suitable actions. I"
P19-3016,P17-4013,0,0.131854,"Missing"
P19-3016,W12-1814,0,0.596935,"ain task-oriented conversations in two languages. It additionally provides a flexible architecture in which modules can be arbitrarily combined or exchanged - allowing for easy switching between rules-based and neural network based implementations. Furthermore, ADVISER offers a transparent, user-friendly framework designed for interdisciplinary collaboration: from a flexible back end, allowing easy integration of new features, to an intuitive graphical user interface supporting nontechnical users. 1 During the last years, several toolkits (Bohus and Rudnicky, 2009; Skantze and Moubayed, 2012; Baumann and Schlangen, 2012; Lison and Kennington, 2016; Ultes et al., 2017; Miller et al., 2017) have been introduced to accelerate the development and testing process of goal-oriented dialog systems, for both single-domain and multidomain systems. Almost all of them have been developed for fast prototyping, where new domains can be developed by swapping in new implementations for each module following the toolkit’s architecture. However, generating natural and effective dialogs requires linguistic knowledge to be integrated throughout the system design, and most of these toolkits are primarily designed for technical u"
P19-3016,W17-4602,1,0.828988,"be evaluated. After a UAR is created, it is analysed by the specified module and the user is informed via a tick (X) or a cross (×) whether the user acts were detected correctly. The user can save and load files in JSON format. 4.2 Our machine learning based belief state tracker is trained to predict the belief state directly from text without the need for an NLU. To track the constraints and requests issued by the user, we feed system actions and user input turn-wise into a recurrent network and concatenate the resulting hidden states of both inputs before predicting the final belief state (Jagfeld and Vu, 2017). Policy The rules-based policy aims to provide users with a single entity matching the constraints they have specified. After each turn, the policy verifies that the user has not ended the dialog. It then reads the current belief state and generates a suitable query for the database. If there are multiple results, the next system act will request more information from the user to disambiguate. Otherwise, the system is able to make an offer – directly informing the user about a specific entity – or to give more details about a current offer. Our machine learning policy is trained using deep RL"
P19-3016,E17-1042,0,0.0744257,"Missing"
P19-3016,P16-4012,0,0.443356,"ns in two languages. It additionally provides a flexible architecture in which modules can be arbitrarily combined or exchanged - allowing for easy switching between rules-based and neural network based implementations. Furthermore, ADVISER offers a transparent, user-friendly framework designed for interdisciplinary collaboration: from a flexible back end, allowing easy integration of new features, to an intuitive graphical user interface supporting nontechnical users. 1 During the last years, several toolkits (Bohus and Rudnicky, 2009; Skantze and Moubayed, 2012; Baumann and Schlangen, 2012; Lison and Kennington, 2016; Ultes et al., 2017; Miller et al., 2017) have been introduced to accelerate the development and testing process of goal-oriented dialog systems, for both single-domain and multidomain systems. Almost all of them have been developed for fast prototyping, where new domains can be developed by swapping in new implementations for each module following the toolkit’s architecture. However, generating natural and effective dialogs requires linguistic knowledge to be integrated throughout the system design, and most of these toolkits are primarily designed for technical users, which can limit the ea"
P19-3016,D17-2014,0,0.0301933,"flexible architecture in which modules can be arbitrarily combined or exchanged - allowing for easy switching between rules-based and neural network based implementations. Furthermore, ADVISER offers a transparent, user-friendly framework designed for interdisciplinary collaboration: from a flexible back end, allowing easy integration of new features, to an intuitive graphical user interface supporting nontechnical users. 1 During the last years, several toolkits (Bohus and Rudnicky, 2009; Skantze and Moubayed, 2012; Baumann and Schlangen, 2012; Lison and Kennington, 2016; Ultes et al., 2017; Miller et al., 2017) have been introduced to accelerate the development and testing process of goal-oriented dialog systems, for both single-domain and multidomain systems. Almost all of them have been developed for fast prototyping, where new domains can be developed by swapping in new implementations for each module following the toolkit’s architecture. However, generating natural and effective dialogs requires linguistic knowledge to be integrated throughout the system design, and most of these toolkits are primarily designed for technical users, which can limit the ease of collaboration with linguists and may"
S15-2088,D11-1052,0,0.0226662,"sheer amount of possible sequences increases the OOV rate dramatically. Therefore, all sequences of punctuations are replaced by a list of distinct punctuations in this sequence (e.g., “!?!?” is replaced by “[!?]”). That reduces the OOV rate and still keeps most of the information. Mohammad et al. (2013) showed that sentiment lexicons are crucial for achieving good polarity classification. Unfortunately, miss-spellings and elongated surface forms of sentiment-bearing tokens, such as “cooooolllll”, lead to lower coverage of all sentiment lexicons. Since elongated words often convey sentiment (Brody and Diakopoulos, 2011), we carefully normalize them in the following way. First, all elongated words are identified by searching for tokens that contain a sequence of at least three equal characters. Afterwards, for each elongated word a candidate set is created by removing the repeated character one by one until only one occurrence is left. If a word contains several repeated character sequences, all combinations are taken as candidates. For instance, the candidate set of the word “cooolll” will be {coolll, colll, cooll, coll, cool, col}. We then search every candidate in a sentiment lexicon to find the correct ca"
S15-2088,P14-1062,0,0.0294958,"om 2013 and 2014, but worse on 2015. Even worse results are achieved on the sarcasm data. However, the results should be taken with care, because this sub set is very small. 5 F1,neutral Related Work One early work that used CNNs to model sentences was published by Collobert et al. (2011). They used one convolution layer followed by a max pooling layer to create a sentence representation. We extend their method by incorporating additional features focused on the polarity classification task. In contrast 531 to their approach, we do not embed our external features, but make direct use of them. Kalchbrenner et al. (2014) show that a CNN for modeling sentences can achieve competitive results in polarity classification. Among others, they introduce dynamic k-max pooling, a method that adapts max pooling to the length of an input sentence. Compared to their work we use a simpler architecture of the CNN without max-pooling, because this technique did not show any improvements in our experiments. Furthermore, we use the same filter for each dimension to reduce the number of parameters, whereas their model uses a different filter per dimension. Finally, our CNN model is combined with another classifier to produce t"
S15-2088,S13-2053,0,0.284029,"entions are replaced by “<user&gt;” and all urls by “<web&gt;”, because they do not provide any cues of polarity. We do not replace hashtags, because they often contain valuable information such as topics or even sentiment. Punctuation sequences like “!?!?” can act as exaggeration or other polarity modifier. However, the sheer amount of possible sequences increases the OOV rate dramatically. Therefore, all sequences of punctuations are replaced by a list of distinct punctuations in this sequence (e.g., “!?!?” is replaced by “[!?]”). That reduces the OOV rate and still keeps most of the information. Mohammad et al. (2013) showed that sentiment lexicons are crucial for achieving good polarity classification. Unfortunately, miss-spellings and elongated surface forms of sentiment-bearing tokens, such as “cooooolllll”, lead to lower coverage of all sentiment lexicons. Since elongated words often convey sentiment (Brody and Diakopoulos, 2011), we carefully normalize them in the following way. First, all elongated words are identified by searching for tokens that contain a sequence of at least three equal characters. Afterwards, for each elongated word a candidate set is created by removing the repeated character on"
S15-2088,N13-1039,0,0.0627824,"Missing"
S15-2088,W02-1011,0,0.0186085,"e. Compared to their work we use a simpler architecture of the CNN without max-pooling, because this technique did not show any improvements in our experiments. Furthermore, we use the same filter for each dimension to reduce the number of parameters, whereas their model uses a different filter per dimension. Finally, our CNN model is combined with another classifier to produce the final polarity label. Using an SVM for polarity classification is a common approach. One of the first polarity classification systems used bag-of-words features and an SVM to classify the polarity of movie reviews (Pang et al., 2002). The winning system of SemEval 2013 and SemEval 2014 also used an SVM with many different features (Mohammad et al., 2013). We implemented their most helpful features, which is bagof-words and lexicon features and added the CNN output as an additional feature to improve the final performance. 6 Conclusion This paper summarizes the features of our automatic sentiment analysis system – CIS-positive – for the SemEval 2015 shared task - Task 10, subtask B. We carefully normalize the Twitter data and integrate the output of convolutional neural networks into support vector machines for the polarit"
S15-2088,S15-2078,0,0.0230011,"g due to many different factors, such as ambiguous word senses, context dependency, sarcasm, etc. Specific properties of Twitter text make this task even more challenging. The limit of 140 character per message leads to countless acronyms and abbreviations. Moreover, the vast majority of tweets is of informal character and contains intentional miss-spellings and wrong use of grammar. Hence, the out-of-vocabulary (OOV) rate of Twitter text is rather high, which leads to information loss. One of the SemEval 2015 shared tasks – Task 10: Sentiment Analysis in Twitter – addresses these challenges (Rosenthal et al., 2015). We participated in Subtask B the “Message Polarity Classification” task. The goal is to predict the polarity of a given tweet into positive, negative, or neutral. The task organizers provided tweet IDs and corresponding labels to have a common ground for training polarity classification systems. More information about the task, its other subtasks as well as information about how the data was selected can be found in (Rosenthal et al., 2015). In this paper, we present our sentiment analysis system for SemEval 2015 - Task 10. Our system addresses the above mentioned challenges in two ways. Fir"
S15-2088,D13-1140,0,0.0227879,"ow many elements 529 the filter spans. Applying this concept to a two dimensional input leads to a convolution matrix where the elements are computed by Ci,j = mT Si,j:j+m−1 , where i is the ith row in S and j is the start index of the convolution. A, the output of the convolution layer is computed by an element-wise addition of a bias term (one bias per row) and an element-wise non-linearity: A = f (C + b). As non-linear function we use a rectified linear unit: f (x) = max(0, x). This non-linearity proved to be a crucial part in object recognition (Jarrett et al., 2009), machine translation (Vaswani et al., 2013), and speech recognition (Zeiler et al., 2013). Our model uses two layers of convolution. The concatenation of all rows of the second convolution layer output is the input to a sequence of three fully connected hidden layers. A hidden layer transforms the input vector x into z = f (W x + b), where W is a weight matrix that is learned during training and b is a bias. In order to convert the final hidden layer output z into a probability distribution over polarity labels o ∈ R3 , the softmax function is used: oi = Pexp(zi ) . exp(z ) j j Pretraining of Word Embeddings The standard way of initial"
S15-2088,H05-1044,0,0.23017,"olution layers SVM hidden layers confidence softmax probabilities SVM final label Figure 1: System architecture the shortest match is taken. Since several sentiment lexicons with different qualities exist, we apply a sequential approach. We search the canonical form of the elongated word in one lexicon. If it does not exist, the next lexicon in the sequence is searched. The sequence of sentiment lexicons is sorted based on the reliability of the lexicon. Manually created lexicons precede automatically created lexicons. In this paper, the ordering is as follows: MPQA subjectivity cues lexicon (Wilson et al., 2005), Opinion lexicon (Hu and Liu, 2004), NRCC Emotion lexicon (Mohammad and Turney, 2013), sentiment 140, and Hashtag lexicon (both in (Mohammad et al., 2013)). As a result, a mapping from elongated words to their canonical form is found and used to normalize the corpus. Lowercasing finalizes the preprocessing step. 3 Model The system architecture consists of three main components and is depicted in Figure 1. The first component is a CNN (left part in the figure), which makes use of the sequence of all words in a tweet. The second component is an SVM classifier which uses several linguistic featu"
W14-3904,C12-1102,0,0.311158,"Missing"
W14-3904,J93-2004,0,0.0457452,"Missing"
W14-3904,D08-1110,0,0.0831504,"Missing"
W14-3904,D08-1102,0,0.606674,"Missing"
W15-2915,S15-2097,0,0.0338013,"Nakov et al., 2013; Rosenthal et al., 2015). The final evaluation is done on the SemEval 2015 test set. Table 1 lists all data set sizes in detail. To test the generality of our findings we additionally report results on the manually labeled test set of the Sentiment140 corpus (Sent140) (Go et 2 http://sentiment.christopherpotts.net/lingstruc.html The list of emoticons was taken from SentiStrengh: http://sentistrength.wlv.ac.uk/ 3 4 111 objective instances were considered to be neutral For reference we add the first and second best systems of the SemEval 2015 tweet level polarity task: Webis (Hagen et al., 2015) and UNITN (Severyn and Moschitti, 2015). Webis is an ensemble based on four systems, which participated in the same task of SemEval 2014. The UNITN system trains a CNN similar to ours. They rely on pretraining the entire model on a large distant supervised training corpus (10M labeled tweets). This approach is orthogonal to ours and can easily be combined with our idea if linguistic feature integration. This combination is likely to increase the performance further. features SemEval Sent140 SVM bow ling. bow + ling. Webis UNITN 50.51 57.28 59.28 67.34 66.90 70.21 64.84 64.59 - 57.83 59.24 62."
W15-2915,P14-1062,0,0.36019,"d of information into the model. We present two intuitive and simple methods: The first one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 conv. pooling Introduction softmax This paper explores the use of Convolutional Neural Networks (CNN) for sentiment analysis. CNNs reach state-of-the-art results in several polarity classification tasks (Mohammad et al., 2013; Tang et al., 2014a; Kim, 2014; Severyn and Moschitti, 2015; Kalchbrenner et al., 2014). Reasons are their ability to deal with arbitrary input sentence lengths and to preserve word order. Moreover, they learn to find the most important polarity indicators and ignore the rest of the sentence. That is beneficial, since most of the words in a text do not convey sentiment information. Finally, CNNs can make use of powerful pretrained word representations (e.g., Mikolov et al. (2013)). However, training such a model requires a large amount of labeled training data. One approach to address this issue is to enlarge training data in a semi-supervised fashion (Severyn and Moschitti, 201"
W15-2915,D14-1181,0,0.196423,"N), which incorporates this valuable kind of information into the model. We present two intuitive and simple methods: The first one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 conv. pooling Introduction softmax This paper explores the use of Convolutional Neural Networks (CNN) for sentiment analysis. CNNs reach state-of-the-art results in several polarity classification tasks (Mohammad et al., 2013; Tang et al., 2014a; Kim, 2014; Severyn and Moschitti, 2015; Kalchbrenner et al., 2014). Reasons are their ability to deal with arbitrary input sentence lengths and to preserve word order. Moreover, they learn to find the most important polarity indicators and ignore the rest of the sentence. That is beneficial, since most of the words in a text do not convey sentiment information. Finally, CNNs can make use of powerful pretrained word representations (e.g., Mikolov et al. (2013)). However, training such a model requires a large amount of labeled training data. One approach to address this issue is to enlarge training data"
W15-2915,S13-2053,0,0.239287,"formed Convolutional Neural Network (lingCNN), which incorporates this valuable kind of information into the model. We present two intuitive and simple methods: The first one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 conv. pooling Introduction softmax This paper explores the use of Convolutional Neural Networks (CNN) for sentiment analysis. CNNs reach state-of-the-art results in several polarity classification tasks (Mohammad et al., 2013; Tang et al., 2014a; Kim, 2014; Severyn and Moschitti, 2015; Kalchbrenner et al., 2014). Reasons are their ability to deal with arbitrary input sentence lengths and to preserve word order. Moreover, they learn to find the most important polarity indicators and ignore the rest of the sentence. That is beneficial, since most of the words in a text do not convey sentiment information. Finally, CNNs can make use of powerful pretrained word representations (e.g., Mikolov et al. (2013)). However, training such a model requires a large amount of labeled training data. One approach to address this is"
W15-2915,N13-1039,0,0.0966121,"Missing"
W15-2915,S15-2078,0,0.0329058,"(SVM) for classification. According to their analysis, bag-of-word features ({1, 2, 3}-grams for words and {3, 4, 5}-grams for characters) and linguistic features are the most important ones. Therefore, we implement both of them. There are three feature settings: (i) only bag-of-words features (for both, word and characters), (ii) only linguistic features, and (iii) the combination of bag-of-words and linguistic features. We use LIBLINEAR (Fan et al., 2008) to train the model and optimized the C parameter on the development set. Data To evaluate the lingCNN, we use the SemEval 2015 data set (Rosenthal et al., 2015). We train the model on the SemEval 2013 training and development set and use the SemEval 2013 test set as development set (Nakov et al., 2013; Rosenthal et al., 2015). The final evaluation is done on the SemEval 2015 test set. Table 1 lists all data set sizes in detail. To test the generality of our findings we additionally report results on the manually labeled test set of the Sentiment140 corpus (Sent140) (Go et 2 http://sentiment.christopherpotts.net/lingstruc.html The list of emoticons was taken from SentiStrengh: http://sentistrength.wlv.ac.uk/ 3 4 111 objective instances were considered"
W15-2915,S15-2079,0,0.192968,"ncorporates this valuable kind of information into the model. We present two intuitive and simple methods: The first one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 conv. pooling Introduction softmax This paper explores the use of Convolutional Neural Networks (CNN) for sentiment analysis. CNNs reach state-of-the-art results in several polarity classification tasks (Mohammad et al., 2013; Tang et al., 2014a; Kim, 2014; Severyn and Moschitti, 2015; Kalchbrenner et al., 2014). Reasons are their ability to deal with arbitrary input sentence lengths and to preserve word order. Moreover, they learn to find the most important polarity indicators and ignore the rest of the sentence. That is beneficial, since most of the words in a text do not convey sentiment information. Finally, CNNs can make use of powerful pretrained word representations (e.g., Mikolov et al. (2013)). However, training such a model requires a large amount of labeled training data. One approach to address this issue is to enlarge training data in a semi-supervised fashion"
W15-2915,D13-1170,0,0.00342617,"es. Especially, where only limited training data is available, the performance difference is large. Even with only 1000 training samples, the CNN with linguistic features yields a reasonable result of 60.89. The CNN that does not have access to this source of information reaches only 49.89. Although, the performance 112 1000 3000 powerful for unbounded dependencies, but tweets are short; the sentiment of a tweet is usually determined by one part of it and unlike RNN/LSTM, convolution plus max pooling can learn to focus on that. Recursive architectures like the Recursive Neural Tensor Network (Socher et al., 2013). assume some kind of hierarchical sentence structure. This structure does not exist or is hard to recognize for many noisy tweets. As mentioned before, we use the SemEval 2013 and SemEval 2014 winning system (Mohammad et al., 2013) as baseline. Moreover, we include several features of their system to improve the CNN. all emb. 49.89 58.10 62.72 emb. + word + sent. 60.89 62.51 64.46 Table 3: Different training set sizes. of the CNN without linguistic features increases much for 3000 training examples, this model is still more than 4 points behind the linguistically informed model. 6 Related Wor"
W15-2915,S14-2033,0,0.145095,"ural Network (lingCNN), which incorporates this valuable kind of information into the model. We present two intuitive and simple methods: The first one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 conv. pooling Introduction softmax This paper explores the use of Convolutional Neural Networks (CNN) for sentiment analysis. CNNs reach state-of-the-art results in several polarity classification tasks (Mohammad et al., 2013; Tang et al., 2014a; Kim, 2014; Severyn and Moschitti, 2015; Kalchbrenner et al., 2014). Reasons are their ability to deal with arbitrary input sentence lengths and to preserve word order. Moreover, they learn to find the most important polarity indicators and ignore the rest of the sentence. That is beneficial, since most of the words in a text do not convey sentiment information. Finally, CNNs can make use of powerful pretrained word representations (e.g., Mikolov et al. (2013)). However, training such a model requires a large amount of labeled training data. One approach to address this issue is to enlarge t"
W15-2915,P14-1146,0,0.269857,"ural Network (lingCNN), which incorporates this valuable kind of information into the model. We present two intuitive and simple methods: The first one integrates word-level features, the second sentence-level features. By combining both types of features our model achieves results that are comparable to state-of-theart systems. 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 conv. pooling Introduction softmax This paper explores the use of Convolutional Neural Networks (CNN) for sentiment analysis. CNNs reach state-of-the-art results in several polarity classification tasks (Mohammad et al., 2013; Tang et al., 2014a; Kim, 2014; Severyn and Moschitti, 2015; Kalchbrenner et al., 2014). Reasons are their ability to deal with arbitrary input sentence lengths and to preserve word order. Moreover, they learn to find the most important polarity indicators and ignore the rest of the sentence. That is beneficial, since most of the words in a text do not convey sentiment information. Finally, CNNs can make use of powerful pretrained word representations (e.g., Mikolov et al. (2013)). However, training such a model requires a large amount of labeled training data. One approach to address this issue is to enlarge t"
W15-2915,H05-1044,0,0.0278602,"ft and right side (i.e., the sentence length becomes n + 2 ∗ (m − 1)). This makes sure that every column of the filter reaches every column of the input. The model uses 2d convolution, because a filter that span all d dimensions in height has the advantage that it can find features that interact with multiple dimensions. Binary Sentiment Indicators Binary features that indicate a word’s prior polarity. We create two such features per word per lexicon. The first feature indicates positive and the second negative polarity of that word in the lexicon. The lexicons for this feature type are MPQA (Wilson et al., 2005), Opinion lexicon (Hu and Liu, 2004), and NRCC Emotion lexicon (Mohammad and Turney, 2013). Max Pooling To keep only the most salient features, the lingCNN selects the largest value of each convolution output. This way we hope to find the most important polarity indicators, independently of their position. To the remaining values we add a bias b(2) and apply a rectified linear unit nonlinearity: a(2) = max(0, a(1) + b(2) ) (Nair and Hinton, 2010). Sentiment Scores The Sentiment140 lexicon and the Hashtag lexicon (Mohammad et al., 2013) both provide a score for each word instead of just a label"
W15-2915,C14-1220,0,0.0405243,"ve the CNN. all emb. 49.89 58.10 62.72 emb. + word + sent. 60.89 62.51 64.46 Table 3: Different training set sizes. of the CNN without linguistic features increases much for 3000 training examples, this model is still more than 4 points behind the linguistically informed model. 6 Related Work Collobert et al. (2011) published the first CNN architecture for a range of natural language processing tasks. We adopt their idea of using look-up tables to incorporate linguistic features at the wordlevel into the CNN. Since then CNNs have been used for a variety of sentence classification tasks (e.g., Zeng et al. (2014)), including polarity classification (e.g., Kim (2014)). Kalchbrenner et al. (2014) showed that their DCNN for modeling sentences can achieve competitive results in this field. Our CNN architecture is simpler than theirs. There are alternative approaches of integrating linguistic features into model training. By adding more labeled data, implicit knowledge is given to the model. This approach usually requires manual labeling effort. A different approach is to incorporate linguistic knowledge into the objective function to guide the model training. For instance Tang et al. (2014b) incorporate t"
W15-2915,S13-2052,0,\N,Missing
W16-2117,P05-1045,0,0.0253383,"Missing"
W16-2117,P14-1062,0,0.0314561,"and unlabeled data word approach may not be sufficient to distinguish them to suggest appropriate claims for the annotators. Following, we present our claim classification method using deep learning to automatically detect important features for finding claims. 5.2 Table 1: F1 score for claim classification Claim classification 5.2.1 Systems using random initialized word embs + replace NEs using pretrained word embs + replace NEs Settings Claim classification can be considered as a sentence classification task. Hence, we applied convolutional neural networks (CNNs) - a state-ofthe-art method (Kalchbrenner et al., 2014; Kim, 2014) for this task. CNNs perform a discrete convolution on an input matrix with a set of different filters. The input matrix represents a sentence, i.e. each column of the matrix stores the word embedding of the corresponding word. Word embedding can be randomly initialised or pre-trained with unsupervised training method. In both cases, we fine-tuned the embeddings during the network training. By applying a filter with a width of e.g. three columns, three neighbouring words (trigram) are convolved. Afterwards, the convolution results are pooled. In this work, our model used filters of"
W16-2117,D15-1008,0,0.0224666,"ts in the energy and climate funds, finding repositories for nuclear waste. Important events related to the setup of security and ethic commissions to examine the safety of nuclear reactors can be spotted from Figure 4. Related work Textual content analysis in social science is still a handcrafted discipline which requires manual annotations (Baumgartner et al., 2008; Bruycker and Beyers, 2015; Koopmans and Statham, 1999). The main drawback besides the expensive manual work is that for each research questions the whole process has to be repeated. In contrast to other content analysis systems (Bamman and Smith, 2015; Qiu et al., 2015; Levy et al., 2014; Slonim et al., 2014) our approach can be seen as a bottom-up task-driven approach instead of a topdown approach based on the theory of argumentation (Moens, 2013). Finally, we grouped claims based on actors and do topic inference for these claims over time. Figure 5 shows an example of a topic timeline for the CDU party and Angela Merkel. Some events related to the election results and nuclear company reactions to the government can be spotted from the timeline (e.g., election in Baden-W¨urttemberg (BW) - the first time CDU lost the presidential mandate,"
W16-2117,C14-1141,0,0.0119467,"g repositories for nuclear waste. Important events related to the setup of security and ethic commissions to examine the safety of nuclear reactors can be spotted from Figure 4. Related work Textual content analysis in social science is still a handcrafted discipline which requires manual annotations (Baumgartner et al., 2008; Bruycker and Beyers, 2015; Koopmans and Statham, 1999). The main drawback besides the expensive manual work is that for each research questions the whole process has to be repeated. In contrast to other content analysis systems (Bamman and Smith, 2015; Qiu et al., 2015; Levy et al., 2014; Slonim et al., 2014) our approach can be seen as a bottom-up task-driven approach instead of a topdown approach based on the theory of argumentation (Moens, 2013). Finally, we grouped claims based on actors and do topic inference for these claims over time. Figure 5 shows an example of a topic timeline for the CDU party and Angela Merkel. Some events related to the election results and nuclear company reactions to the government can be spotted from the timeline (e.g., election in Baden-W¨urttemberg (BW) - the first time CDU lost the presidential mandate, final decision of the federal state r"
W16-2117,C14-2004,0,0.0389758,"Missing"
W16-5801,P13-2037,1,0.90289,"Missing"
W16-5801,P06-2005,0,0.0203934,"their data will be used for analysis, which therefore makes it a more naturalistic setting. They give their consent after, once the content is created. Among social media sites, Twitter has its disadvan2 tages like license issues, and limited characters per tweet. Other media that does not have these advantages remain popular sources. 3 Normalisation Text normalisation is the task of standardising text that deviates from some agreed-upon (or canonical) form. This can e.g. refer to normalising social media language to standard language (“newspaper language”) (cf. e.g. Schulz et al. (2016) and Aw et al. (2006)) or historical language to its modern form (Bollmann et al., 2011). Since mixed text often occurs in spoken language or text close in nature to spoken language like social media, normalisation is a highly relevant task for the processing of such text. In the case of mixed text there are two languages embedded into each other. Defining a canonical form is a challenge because each of the languages should be standardised to its own normal form. Normalisation of text has started out as a task mainly solved on token-level. Most of the recent approaches are based on context e.g. in characterbased m"
W16-5801,W14-3902,0,0.369085,"edescribe NLP tasks under the assumption that the data contains more than one language. For tasks that are studied more compared to others we compile approaches taken by previous work. Examples from different language pairs highlight the challenges, supporting the need for awareness about the nature of mixed data for successful automatic processing. 2 Data Nature of the data Annotated CS corpora, that are designed for computational purposes, center around three sources so far: spoken data (Solorio and Liu, 2008b; Lyu et al., 2015; Yılmaz et al., 2016), social media (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Vyas et al., 2014; Solorio et al., 2014; Choudhury et al., 2014; Jamatia et al., 2015; Çetino˘glu, 2016; Samih and Maier, 2016), and historical text (Schulz and Keller, 2016). All these data sources are challenging on their own even if they do not exhibit CS. They are non-canonical in their orthography, lexicon, and syntax, thus the existing resources and tools should be adapted, or new ones should be created to handle domain-specific issues in addition to the challenges of CS itself. Accessing the data Although CS is prominent in every day life, especially in countries with a high percentag"
W16-5801,N03-2002,0,0.0138888,"oman script, in order to utilise resources from other writing systems, the text has to be mapped back to the system of the respective language. Due to this problem Barman et al. (2014) do not use existing Bengali and Hindi resources in their dictionary-based approach. Das and Gambäck (2014) Romanised the resources whereas Vyas et al. (2014) go in the opposite direction and develop a back-transliteration component. 4 Language Modelling A statistical language model assigns a probability to a given sentence or utterance. Models such as ngram models (Brown et al., 1992), factored language models (Bilmes and Kirchhoff, 2003) and neural language models (Bengio et al., 2003) are used in many NLP applications such as machine translation and automatic speech recognition. One can consider mixed text as an individual language and use existing techniques to train the model. Tokenisation and normalisation are the first steps to prepare the training data. Hence, one will face the problems presented in Section 3 first. Another serious problem is the lack of CS training data which makes statistical language models unreliable. We consider three kinds of CS to identify challenges of code-switching language modelling (LM). Int"
W16-5801,W11-4106,0,0.0302435,"it a more naturalistic setting. They give their consent after, once the content is created. Among social media sites, Twitter has its disadvan2 tages like license issues, and limited characters per tweet. Other media that does not have these advantages remain popular sources. 3 Normalisation Text normalisation is the task of standardising text that deviates from some agreed-upon (or canonical) form. This can e.g. refer to normalising social media language to standard language (“newspaper language”) (cf. e.g. Schulz et al. (2016) and Aw et al. (2006)) or historical language to its modern form (Bollmann et al., 2011). Since mixed text often occurs in spoken language or text close in nature to spoken language like social media, normalisation is a highly relevant task for the processing of such text. In the case of mixed text there are two languages embedded into each other. Defining a canonical form is a challenge because each of the languages should be standardised to its own normal form. Normalisation of text has started out as a task mainly solved on token-level. Most of the recent approaches are based on context e.g. in characterbased machine translation (Liu et al., 2012; Schulz et al., 2016). This re"
W16-5801,J92-4003,0,0.358438,"Missing"
W16-5801,W16-1714,1,0.880135,"Missing"
W16-5801,L16-1667,1,0.886858,"Missing"
W16-5801,elfardy-diab-2012-simplified,0,0.0689677,"Missing"
W16-5801,R15-1033,0,0.0450532,". For tasks that are studied more compared to others we compile approaches taken by previous work. Examples from different language pairs highlight the challenges, supporting the need for awareness about the nature of mixed data for successful automatic processing. 2 Data Nature of the data Annotated CS corpora, that are designed for computational purposes, center around three sources so far: spoken data (Solorio and Liu, 2008b; Lyu et al., 2015; Yılmaz et al., 2016), social media (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Vyas et al., 2014; Solorio et al., 2014; Choudhury et al., 2014; Jamatia et al., 2015; Çetino˘glu, 2016; Samih and Maier, 2016), and historical text (Schulz and Keller, 2016). All these data sources are challenging on their own even if they do not exhibit CS. They are non-canonical in their orthography, lexicon, and syntax, thus the existing resources and tools should be adapted, or new ones should be created to handle domain-specific issues in addition to the challenges of CS itself. Accessing the data Although CS is prominent in every day life, especially in countries with a high percentage of multilingual communities, accessing it is still problematic. Speech as one of the"
W16-5801,C82-1023,0,0.173698,"Myers-Scotton, 1993; Muysken, 2000; Auer and Wei, 2007; Bullock and Toribio, 2012). This has also brought different perspectives on the definition and types of mixed language. Switching between sentences (inter-sentential) is distinguished from switching inside of one sentence (intra-sentential). Poplack (1980) defines code-switching as ‘the alternation of two languages within a single discourse, Computational approaches in the analysis of CS data are quite recent as compared to linguistic studies. The first theoretical framework to parse codeswitched sentences dates back to the early 1980s (Joshi, 1982), yet few studies are done in the 2000s (Goyal et al., 2003; Sinha and Thakur, 2005; Solorio and Liu, 2008a; Solorio and Liu, 2008b). With the beginning of the last decade, this picture has changed due to increasingly multi-cultural societies and the rise of social media. Supported with the introduction of annotated data sets on several language pairs, different tasks are applied on CS data. The characteristics of mixed data affect tasks in different ways, sometimes changing the definition (e.g. in language identification, the shift from document-level to word-level), sometimes by creating new"
W16-5801,Q16-1023,0,0.016741,"Missing"
W16-5801,C12-1102,0,0.0206062,"Missing"
W16-5801,D14-1098,0,0.0526511,"challenges, each pair requires a dedicated corpus. To alleviate the data sparsity problem, some approaches work by generating artificial CS text based on a CS-aware recurrent neural network decoder (Vu and Schultz, 2014) or a machine translation system to create CS data from monolingual data (Vu et al., 2012). Such techniques would benefit from better understanding of the characteristics of codeswitching data. This is why we enriched our paper with examples from data sets covering different language pairs. So far, very little NLP research makes use of linguistic insights into CS patterns (cf. Li and Fung (2014)). Such cues might improve results in the discussed tasks herein. Another recurring and not yet addressed issue8 , is the inter-relatedness of all these tasks. Features required for one task are the output of the other. Pipeline approaches cannot take advantage of these features when task dependencies are cyclic (e.g., normalisation and language identification). Moreover pipelines cause error propagation. This fact asks for attention on joint modelling approaches. Acknowledgments We thank our anonymous reviewers for their helpful comments. This work was funded by the Deutsche Forschungsgemeins"
W16-5801,P12-1109,0,0.0155648,"uage to its modern form (Bollmann et al., 2011). Since mixed text often occurs in spoken language or text close in nature to spoken language like social media, normalisation is a highly relevant task for the processing of such text. In the case of mixed text there are two languages embedded into each other. Defining a canonical form is a challenge because each of the languages should be standardised to its own normal form. Normalisation of text has started out as a task mainly solved on token-level. Most of the recent approaches are based on context e.g. in characterbased machine translation (Liu et al., 2012; Schulz et al., 2016). This results from the fact that normalisation requires a certain degree of semantic disambiguation of words in context to determine if there is a normalisation problem. These problems can appear on two levels: a) The word is an out-ofvocabulary (OOV) word (which are the easy cases), thus it does not exist in the lexicon. b) The word is the wrong word in context (often just the wrong graphematic realisation e.g. tree instead of three). This context dependency results in issues for mixed text. The presence of CS increases the number of possible actions regarding an errone"
W16-5801,P12-3005,0,0.0305283,"Missing"
W16-5801,W15-1608,0,0.124503,"Missing"
W16-5801,D13-1084,0,0.0251565,"Missing"
W16-5801,L16-1262,0,0.00653824,"Missing"
W16-5801,petrov-etal-2012-universal,0,0.0195327,"Missing"
W16-5801,L16-1658,0,0.0488565,"ed to others we compile approaches taken by previous work. Examples from different language pairs highlight the challenges, supporting the need for awareness about the nature of mixed data for successful automatic processing. 2 Data Nature of the data Annotated CS corpora, that are designed for computational purposes, center around three sources so far: spoken data (Solorio and Liu, 2008b; Lyu et al., 2015; Yılmaz et al., 2016), social media (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Vyas et al., 2014; Solorio et al., 2014; Choudhury et al., 2014; Jamatia et al., 2015; Çetino˘glu, 2016; Samih and Maier, 2016), and historical text (Schulz and Keller, 2016). All these data sources are challenging on their own even if they do not exhibit CS. They are non-canonical in their orthography, lexicon, and syntax, thus the existing resources and tools should be adapted, or new ones should be created to handle domain-specific issues in addition to the challenges of CS itself. Accessing the data Although CS is prominent in every day life, especially in countries with a high percentage of multilingual communities, accessing it is still problematic. Speech as one of the main sources requires consent prior to rec"
W16-5801,W16-5811,0,0.0239177,"ell as user historybased context. Since training material for such systems might be sparse for some language pairs, methods for mixed text tend to return to smaller context windows as done by Dutta et al. (2015). They suggest to use two monolingual language models with context windows depending on the neighbouring words using language identification information. In case of a high density of switch points between languages, the context window might be small. 3 As another normalisation challenge, Kaur and Singh (2015) describe issues emerging from mixing different scripts in Punjabi-English and Sarkar (2016) for Hindi-English and Bengali-English social media text. Since text is often realised in Roman script, in order to utilise resources from other writing systems, the text has to be mapped back to the system of the respective language. Due to this problem Barman et al. (2014) do not use existing Bengali and Hindi resources in their dictionary-based approach. Das and Gambäck (2014) Romanised the resources whereas Vyas et al. (2014) go in the opposite direction and develop a back-transliteration component. 4 Language Modelling A statistical language model assigns a probability to a given sentence"
W16-5801,W16-2105,1,0.83308,"revious work. Examples from different language pairs highlight the challenges, supporting the need for awareness about the nature of mixed data for successful automatic processing. 2 Data Nature of the data Annotated CS corpora, that are designed for computational purposes, center around three sources so far: spoken data (Solorio and Liu, 2008b; Lyu et al., 2015; Yılmaz et al., 2016), social media (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Vyas et al., 2014; Solorio et al., 2014; Choudhury et al., 2014; Jamatia et al., 2015; Çetino˘glu, 2016; Samih and Maier, 2016), and historical text (Schulz and Keller, 2016). All these data sources are challenging on their own even if they do not exhibit CS. They are non-canonical in their orthography, lexicon, and syntax, thus the existing resources and tools should be adapted, or new ones should be created to handle domain-specific issues in addition to the challenges of CS itself. Accessing the data Although CS is prominent in every day life, especially in countries with a high percentage of multilingual communities, accessing it is still problematic. Speech as one of the main sources requires consent prior to recording. One way to keep recordings as natural a"
W16-5801,N16-1159,0,0.0185284,"Missing"
W16-5801,2005.mtsummit-papers.20,0,0.0853776,"bio, 2012). This has also brought different perspectives on the definition and types of mixed language. Switching between sentences (inter-sentential) is distinguished from switching inside of one sentence (intra-sentential). Poplack (1980) defines code-switching as ‘the alternation of two languages within a single discourse, Computational approaches in the analysis of CS data are quite recent as compared to linguistic studies. The first theoretical framework to parse codeswitched sentences dates back to the early 1980s (Joshi, 1982), yet few studies are done in the 2000s (Goyal et al., 2003; Sinha and Thakur, 2005; Solorio and Liu, 2008a; Solorio and Liu, 2008b). With the beginning of the last decade, this picture has changed due to increasingly multi-cultural societies and the rise of social media. Supported with the introduction of annotated data sets on several language pairs, different tasks are applied on CS data. The characteristics of mixed data affect tasks in different ways, sometimes changing the definition (e.g. in language identification, the shift from document-level to word-level), sometimes by creating new lexical and syntactic structures (e.g. mixed words that consist of morphemes from"
W16-5801,D08-1102,0,0.791069,"o brought different perspectives on the definition and types of mixed language. Switching between sentences (inter-sentential) is distinguished from switching inside of one sentence (intra-sentential). Poplack (1980) defines code-switching as ‘the alternation of two languages within a single discourse, Computational approaches in the analysis of CS data are quite recent as compared to linguistic studies. The first theoretical framework to parse codeswitched sentences dates back to the early 1980s (Joshi, 1982), yet few studies are done in the 2000s (Goyal et al., 2003; Sinha and Thakur, 2005; Solorio and Liu, 2008a; Solorio and Liu, 2008b). With the beginning of the last decade, this picture has changed due to increasingly multi-cultural societies and the rise of social media. Supported with the introduction of annotated data sets on several language pairs, different tasks are applied on CS data. The characteristics of mixed data affect tasks in different ways, sometimes changing the definition (e.g. in language identification, the shift from document-level to word-level), sometimes by creating new lexical and syntactic structures (e.g. mixed words that consist of morphemes from two different languages"
W16-5801,D08-1110,0,0.0702481,"o brought different perspectives on the definition and types of mixed language. Switching between sentences (inter-sentential) is distinguished from switching inside of one sentence (intra-sentential). Poplack (1980) defines code-switching as ‘the alternation of two languages within a single discourse, Computational approaches in the analysis of CS data are quite recent as compared to linguistic studies. The first theoretical framework to parse codeswitched sentences dates back to the early 1980s (Joshi, 1982), yet few studies are done in the 2000s (Goyal et al., 2003; Sinha and Thakur, 2005; Solorio and Liu, 2008a; Solorio and Liu, 2008b). With the beginning of the last decade, this picture has changed due to increasingly multi-cultural societies and the rise of social media. Supported with the introduction of annotated data sets on several language pairs, different tasks are applied on CS data. The characteristics of mixed data affect tasks in different ways, sometimes changing the definition (e.g. in language identification, the shift from document-level to word-level), sometimes by creating new lexical and syntactic structures (e.g. mixed words that consist of morphemes from two different languages"
W16-5801,W14-3907,0,0.0203575,"that the data contains more than one language. For tasks that are studied more compared to others we compile approaches taken by previous work. Examples from different language pairs highlight the challenges, supporting the need for awareness about the nature of mixed data for successful automatic processing. 2 Data Nature of the data Annotated CS corpora, that are designed for computational purposes, center around three sources so far: spoken data (Solorio and Liu, 2008b; Lyu et al., 2015; Yılmaz et al., 2016), social media (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Vyas et al., 2014; Solorio et al., 2014; Choudhury et al., 2014; Jamatia et al., 2015; Çetino˘glu, 2016; Samih and Maier, 2016), and historical text (Schulz and Keller, 2016). All these data sources are challenging on their own even if they do not exhibit CS. They are non-canonical in their orthography, lexicon, and syntax, thus the existing resources and tools should be adapted, or new ones should be created to handle domain-specific issues in addition to the challenges of CS itself. Accessing the data Although CS is prominent in every day life, especially in countries with a high percentage of multilingual communities, accessing"
W16-5801,P16-2069,0,0.0281438,"Missing"
W16-5801,W14-3903,0,0.0472373,"Missing"
W16-5801,voss-etal-2014-finding,0,0.0302455,"Missing"
W16-5801,W14-3904,1,0.841557,"poses from non-canonicity to incomplete syntactic structures to OOV-words. Although this would already suggest that a higher number of training instances are needed, there are just small amounts of annotated data available. So far there are annotated bilingual training resources for just three of the tasks (LID, POS and ASR) for specific language pairs. Since each mixed language comes with its own challenges, each pair requires a dedicated corpus. To alleviate the data sparsity problem, some approaches work by generating artificial CS text based on a CS-aware recurrent neural network decoder (Vu and Schultz, 2014) or a machine translation system to create CS data from monolingual data (Vu et al., 2012). Such techniques would benefit from better understanding of the characteristics of codeswitching data. This is why we enriched our paper with examples from data sets covering different language pairs. So far, very little NLP research makes use of linguistic insights into CS patterns (cf. Li and Fung (2014)). Such cues might improve results in the discussed tasks herein. Another recurring and not yet addressed issue8 , is the inter-relatedness of all these tasks. Features required for one task are the out"
W16-5801,D14-1105,0,0.0640905,"nder the assumption that the data contains more than one language. For tasks that are studied more compared to others we compile approaches taken by previous work. Examples from different language pairs highlight the challenges, supporting the need for awareness about the nature of mixed data for successful automatic processing. 2 Data Nature of the data Annotated CS corpora, that are designed for computational purposes, center around three sources so far: spoken data (Solorio and Liu, 2008b; Lyu et al., 2015; Yılmaz et al., 2016), social media (Nguyen and Do˘gruöz, 2013; Barman et al., 2014; Vyas et al., 2014; Solorio et al., 2014; Choudhury et al., 2014; Jamatia et al., 2015; Çetino˘glu, 2016; Samih and Maier, 2016), and historical text (Schulz and Keller, 2016). All these data sources are challenging on their own even if they do not exhibit CS. They are non-canonical in their orthography, lexicon, and syntax, thus the existing resources and tools should be adapted, or new ones should be created to handle domain-specific issues in addition to the challenges of CS itself. Accessing the data Although CS is prominent in every day life, especially in countries with a high percentage of multilingual c"
W16-5801,L16-1739,0,0.125148,"Missing"
W17-4118,W15-3904,0,0.0753053,"Missing"
W17-4118,W15-2215,1,0.903954,"Missing"
W17-4118,P06-1037,0,0.0375992,"Missing"
W17-4118,P16-2067,0,0.118666,"ons, and more stable for unseen or unnormalized words, which is the main benefit of character composition models. Yu and Vu (2017) compared the performance of CNN and LSTM as character composition model for dependency parsing, and concluded that CNN performs better than LSTM. In this paper, we show that this is also the case for POS tagging. Furthermore, we extend the scope to morphological tagging and supertagging, in which the tag set is much larger or long-distance dependencies between words are more important. In these three tagging tasks, we compare our tagger with the bilstm-aux tagger (Plank et al., 2016) and the CRF-based morphological tagger MarMot (M¨uller et al., 2013) as baselines. The CNN tagger shows robust performance across the three tasks, and achieves the highest average accuracies in all tasks. It considerably outperforms the LSTM tagger in morphological tagging and both baselines in supertagging. To test the robustness of the taggers against the OOV problem, we also conduct experiments on unnormalized text by artificially corrupting words in the normal dev sets. With the increasing degree of unnormalization, the performance of the CNN tagger degrades much slower than the other two"
W17-4118,N16-1065,1,0.89123,"Missing"
W17-4118,C94-1024,0,0.574706,"a fixed input size of 32 characters for each word, with padding on both sides or cutting from the middle if needed. We apply four convolution filters with sizes of 3, 5, 7, and 9. Each filter has an output channel of 25 dimensions, thus the composed vector is 100-dimensional. We apply Gaussian noise with standard deviation of 0.1 on the composed vector during training. 2 http://universaldependencies.org We use all training data for Czech, while Plank et al. (2016) only use a subset. 3 125 UD treebanks as one string which contains several key-value pairs of morphological features.4 Supertags (Joshi and Bangalore, 1994) are tags that encode more syntactic information than standard POS tags, e.g. the head direction or the subcategorization frame. We use dependency-based supertags (Foth et al., 2006) which are extracted from the dependency treebanks. Adding such tags into feature models of statistical dependency parsers significantly improves their performance (Ouchi et al., 2014; Fale´nska et al., 2015). Supertags can be designed with different levels of granularity. We use the standard Model 1 from Ouchi et al. (2014), where each tag consists of head direction, dependency label and dependent directions. The"
W17-4118,P17-2106,1,0.904249,"NLP tasks, mainly because of their robustness in dealing with outof-vocabulary (OOV) words by capturing subword informations. Among the character composition models, bidirectional long short-term memory (LSTM) models and convolutional neural networks (CNN) are widely applied in many tasks, e.g. part-of-speech (POS) tagging (dos Santos and Zadrozny, 2014; Plank et al., 2016), named entity recognition (dos Santos and Guimar˜aes, 2015), language modeling (Ling et al., 2015; Kim et al., 2016), machine translation (Costa-juss`a and Fonollosa, 2016) and dependency parsing (Ballesteros et al., 2015; Yu and Vu, 2017). In this paper, we present a state-of-the-art general-purpose tagger that uses CNNs both to compose word representations from characters and to encode context information for tagging.1 We show that the CNN model is more capable than 2 Model Our proposed CNN tagger has two main components: the character composition model and the context encoding model. Both components are essentially very similar CNN models, capturing dif1 The tagger is available at http://www.ims. uni-stuttgart.de/institut/mitarbeiter/ xiangyu/index.en.html 124 Proceedings of the First Workshop on Subword and Character Level"
W17-4118,D15-1176,0,0.159428,"m; it performs well on artificially unnormalized texts. 1 Introduction Recently, character composition models have shown great success in many NLP tasks, mainly because of their robustness in dealing with outof-vocabulary (OOV) words by capturing subword informations. Among the character composition models, bidirectional long short-term memory (LSTM) models and convolutional neural networks (CNN) are widely applied in many tasks, e.g. part-of-speech (POS) tagging (dos Santos and Zadrozny, 2014; Plank et al., 2016), named entity recognition (dos Santos and Guimar˜aes, 2015), language modeling (Ling et al., 2015; Kim et al., 2016), machine translation (Costa-juss`a and Fonollosa, 2016) and dependency parsing (Ballesteros et al., 2015; Yu and Vu, 2017). In this paper, we present a state-of-the-art general-purpose tagger that uses CNNs both to compose word representations from characters and to encode context information for tagging.1 We show that the CNN model is more capable than 2 Model Our proposed CNN tagger has two main components: the character composition model and the context encoding model. Both components are essentially very similar CNN models, capturing dif1 The tagger is available at http"
W17-4118,J93-2004,0,0.0594589,"filter are fed through a max pooling layer, and the pooling outputs are concatenated to represent the word. 2.2 3.1 We use treebanks from version 1.2 of Universal Dependencies2 (UD), and in the case of several treebanks for one language, we only use the canonical one. There are in total 22 treebanks, as in Plank et al. (2016).3 Each treebank splits into train, dev, and test sets, we use the dev sets for early stop training. In order to compare to more previous works on POS tagging, we additionally experiment POS tagging on the more established Penn Treebank Wall Street Journal (WSJ) data set (Marcus et al., 1993). We use the standard splitting, where sections 0-18 are used for training, 19-21 for tuning, and 22-24 for testing (Collins, 2002). Context Encoding Model The context encoding model captures the context information of the target word by scanning through the word representations of its context window. The word representation could be only word embeddings (w), ~ only composed vectors (~c), or the concatenation of both (w ~ + ~c). A context window consists of N words to both sides of the target word and the target word itself. To indicate the target word, we concatenate a binary feature to each"
W17-4118,D13-1032,0,0.176562,"Missing"
W17-4118,E14-4030,0,0.0172128,"2 http://universaldependencies.org We use all training data for Czech, while Plank et al. (2016) only use a subset. 3 125 UD treebanks as one string which contains several key-value pairs of morphological features.4 Supertags (Joshi and Bangalore, 1994) are tags that encode more syntactic information than standard POS tags, e.g. the head direction or the subcategorization frame. We use dependency-based supertags (Foth et al., 2006) which are extracted from the dependency treebanks. Adding such tags into feature models of statistical dependency parsers significantly improves their performance (Ouchi et al., 2014; Fale´nska et al., 2015). Supertags can be designed with different levels of granularity. We use the standard Model 1 from Ouchi et al. (2014), where each tag consists of head direction, dependency label and dependent directions. The S UPER task is more difficult than P OS and M ORPH because it generally requires taking long-distance dependencies between words into consideration. These three tagging tasks differ strongly in tag set sizes. Generally, the P OS set sizes for all the languages are no more than 17 and S UPER set sizes are around 200. When treating morphological features as a strin"
W17-4118,petrov-etal-2012-universal,0,0.0289411,"des of the target word and the target word itself. To indicate the target word, we concatenate a binary feature to each of the word representations with 1 indicating the target and 0 otherwise, similar to Vu et al. (2016). Additional to the binary feature, we also concatenate a position embedding to encode the relative position of each context word, similar to Gehring et al. (2017). 2.3 Data 3.2 Tasks We evaluate the taggers on three tagging tasks: POS tagging (P OS), morphological tagging (M ORPH) and supertagging (S UPER). For POS tagging we use Universal POS tags, which are an extension of Petrov et al. (2012). The universal tag set tries to capture the “universal” properties of words and facilitate cross-lingual learning. Therefore the tag set is very coarse and leaves out most of the language-specific properties to morphological features. Morphological tags encode the languagespecific morphological features of the words, e.g., number, gender, case. They are represented in the Hyper-parameters For the character composition model, we take a fixed input size of 32 characters for each word, with padding on both sides or cutting from the middle if needed. We apply four convolution filters with sizes o"
W17-4118,P16-2058,0,\N,Missing
W17-4602,W14-4337,0,0.119764,"Missing"
W17-4602,W14-4340,0,0.0490744,"Missing"
W17-4602,W14-4339,0,0.0900938,"and Zhao and Esk´enazi (2016), yet these approaches do not integrate ASR into the joint reasoning process. We take a first step towards integrating ASR in an end-to-end SDS by passing on a richer hypothesis space to subsequent components. Specifically, we investigate how the richer ASR hypothesis space can improve DST. We focus on these two components because they are at the beginning of the processing pipeline and provide vital information for the subsequent SDS components. Typically, ASR systems output the best hypothesis or an n-best list, which the majority of DST approaches so far uses (Williams, 2014; Henderson et al., 2014c; Mrksic et al., 2017; Zilka and Jurc´ıcek, 2015). However, n-best lists can only This paper presents our novel method to encode word confusion networks, which can represent a rich hypothesis space of automatic speech recognition systems, via recurrent neural networks. We demonstrate the utility of our approach for the task of dialog state tracking in spoken dialog systems that relies on automatic speech recognition output. Encoding confusion networks outperforms encoding the best hypothesis of the automatic speech recognition in a neural system for dialog state tracki"
W17-4602,E17-1029,0,0.0111347,"ral language generation (NLG) module forms the system reply that is converted into an audio signal via a text-to-speech synthesizer. Error propagation poses a major problem in modular architectures as later components depend on the output of the previous steps. We show in this paper that DST suffers from ASR errors, which was also noted by Mrksic et al. (2017). One solution is to avoid modularity and instead perform joint reasoning over several subtasks, e.g. many DST systems directly operate on ASR output and do not rely on a separate SLU module (Henderson et al., 2014c; Mrksic et al., 2017; Perez, 2017). End-to-end systems that can be directly trained on dialogs without intermediate annotations have been proposed for open-domain dialog systems (Vinyals and Le, 2015). This is more difficult to realize for task-oriented systems as they often require domain knowledge and external databases. First steps into this direction were taken by Wen et al. (2016) and Zhao and Esk´enazi (2016), yet these approaches do not integrate ASR into the joint reasoning process. We take a first step towards integrating ASR in an end-to-end SDS by passing on a richer hypothesis space to subsequent components. Specif"
W17-4605,bazillon-etal-2012-syntactic,0,0.0153466,"d rescored with a syntactic parser for a more global view on the source sentence. The noisy channel is then formalized as TAG that maps source sentences to target sentences, where repairs are treated as the cleaned target side of the reparanda on the source side. Besides the words themselves, Johnson and Charniak (2004) use POS tags for the alignment of reparandum and repair, which indicates their usefulness in detecting disfluencies. Approaching spontaneous speech issues from another angle, B´echet et al. (2014) adapt a parser trained on written text by means of an interactive web interface (Bazillon et al., 2012) in which users can modify POS and dependency tags writing regular expressions. dobj nsubj music NN you PRP nn like VB rap NN music NN Figure 6: Dependency graph comparison #3. Correct JointPOS tree on the left, incorrect tri4 tree on the right. while the joint model is entirely correct, the recognition error in the pipeline causes two POS tagging errors resulting in an incorrect parse. root root ccomp nsubj nsubjpass dep advmod advmod auxpass let VB ’s PRP just RB get VB started VBN auxpass it PRP ’s BES just RB get VB started VBN Figure 7: Dependency graph comparison #4. Correct JointPOS tre"
W17-4605,P04-1030,0,0.0583105,"mise in data-driven methods that more data is better data, we choose the Switchboard-1 Release 21 (Godfrey et al., 1992) corpus with about 2400 dialogs. The Switchboard (SWBD) corpus has more recently been furnished with the NXT Switchboard annotations2 (Calhoun et al., 2010). NXT provides a plethora of annotations and most importantly for our work, an alignment of Treebank-33 (Marcus et al., 1999) text and SWBD transcriptions4 . While the Treebank-3 corpus proIntroduction Parsing speech is an essential part (Chow and Roukos, 1989; Moore et al., 1989; Su et al., 1992; Chappelier et al., 1999; Collins et al., 2004) of spoken language understanding (SLU) and difficult because spontaneous speech and syntax clash (Ehrlich and Hanrieder, 1996; Charniak and Johnson, 2001; B´echet et al., 2014). Pipeline approaches concatenating a speech recognizer, a POS tagger and a parser often rely on n-best hypotheses decoded from lattices. While n-best hypotheses cover more of the hypothesis space than the 1-best hypothesis, they are redundant and incomplete. Lattices on the other hand are efficiently representing all hypotheses under consideration and therefore allow recovery from more ASR errors. Recent work on recurr"
W17-4605,N01-1016,0,0.31792,"gs. The Switchboard (SWBD) corpus has more recently been furnished with the NXT Switchboard annotations2 (Calhoun et al., 2010). NXT provides a plethora of annotations and most importantly for our work, an alignment of Treebank-33 (Marcus et al., 1999) text and SWBD transcriptions4 . While the Treebank-3 corpus proIntroduction Parsing speech is an essential part (Chow and Roukos, 1989; Moore et al., 1989; Su et al., 1992; Chappelier et al., 1999; Collins et al., 2004) of spoken language understanding (SLU) and difficult because spontaneous speech and syntax clash (Ehrlich and Hanrieder, 1996; Charniak and Johnson, 2001; B´echet et al., 2014). Pipeline approaches concatenating a speech recognizer, a POS tagger and a parser often rely on n-best hypotheses decoded from lattices. While n-best hypotheses cover more of the hypothesis space than the 1-best hypothesis, they are redundant and incomplete. Lattices on the other hand are efficiently representing all hypotheses under consideration and therefore allow recovery from more ASR errors. Recent work on recurrent neural network architectures with lattices as input (Ladhak et al., 2016; Su et al., 2017) promises the use of enriched lattices in SLU. 1 LDC: https:"
W17-4605,Q14-1011,0,0.0644365,"ecause of the token mismatch nonetheless. If we had not allowed the imprecise evaluation, we would not have observed this kind of error. The example in Figure 7 also has an ASR error in the pipeline approach at its core. In this case, 42 root root aux dobj nsubj do VBP you PRP nn like VB rap NN tion and DP. Rasooli and Tetreault (2013) extend the arc-eager transition system (Nivre, 2008) with actions that handle reparanda, discourse markers and interjections, thereby also explicitly using marked disfluencies on SWBD for joint DP and disfluency detection. Where Rasooli and Tetreault (2013) and Honnibal and Johnson (2014) work with SWBD text data, Yoshikawa et al. (2016) are close to our setting and assume ASR output text as parser input. Yoshikawa et al. (2016) create an alignment that enables the transfer of gold treebank data to ASR output texts and add three actions to manage disfluencies and ASR errors to the arc-eager shift-reduce transition system of Zhang and Nivre (2011). While they do not parse lattices or confusion networks (lattices can be converted to confusion networks, see Mangu et al. (2000)) directly, Yoshikawa et al. (2016) use information from word confusion networks to discover erroneous re"
W17-4605,P04-1005,0,0.0567615,"1). While they do not parse lattices or confusion networks (lattices can be converted to confusion networks, see Mangu et al. (2000)) directly, Yoshikawa et al. (2016) use information from word confusion networks to discover erroneous regions in the ASR output. Charniak and Johnson (2001) parse SWBD after removing edited speech that they identify with a linear classifier. Additionally, Charniak and Johnson (2001) introduce a relaxed edited parsing metric that considers a simplified gold standard constituent parse (removed edited words are added back into the constituent parse for evaluation). Johnson and Charniak (2004) model speech repairs in a noisy channel model utilizing tree adjoining grammars (TAGs). Source sentence probabilities in the noisy channel are computed with a bigram LM and rescored with a syntactic parser for a more global view on the source sentence. The noisy channel is then formalized as TAG that maps source sentences to target sentences, where repairs are treated as the cleaned target side of the reparanda on the source side. Besides the words themselves, Johnson and Charniak (2004) use POS tags for the alignment of reparandum and repair, which indicates their usefulness in detecting dis"
W17-4605,W07-2435,0,0.0223845,"es four word errors in sequence and “families” does not appear in its hypothesis. 6 Related work Spoken language poses a variety of problems for NLP. The recognition of spoken language can suffer from poor recording equipment, noisy environments, unclear speech or speech pathologies. It also exhibits spontaneity, ungrammaticality and disfluencies, e.g. repairs and restarts (cf. Shriberg (1994)). Hence, in addition to ASR errors, downstream tasks such as parsing have to deal with these difficulties of conversational speech, whether the ASR output is in the form of n-best sequences or lattices. Jørgensen (2007) remove disfluencies prior to parsing and find their removal improves the performance of both a dependency and a head-driven lexicalized statistical parser on SWBD. In a more general joint approach of disfluency detection and DP, Honnibal and Johnson (2014) in contrast to Jørgensen (2007) make use of the disfluency annotations and report strong results for both, disfluency annotaNatural speech poses specific problems, but also comes with acoustic information that can improve parsing speech through its incorporation (Tran et al., 2017) or reranking (Kahn et al., 2005). Handling disfluencies fol"
W17-4605,H05-1030,0,0.0439998,"-best sequences or lattices. Jørgensen (2007) remove disfluencies prior to parsing and find their removal improves the performance of both a dependency and a head-driven lexicalized statistical parser on SWBD. In a more general joint approach of disfluency detection and DP, Honnibal and Johnson (2014) in contrast to Jørgensen (2007) make use of the disfluency annotations and report strong results for both, disfluency annotaNatural speech poses specific problems, but also comes with acoustic information that can improve parsing speech through its incorporation (Tran et al., 2017) or reranking (Kahn et al., 2005). Handling disfluencies following Charniak and Johnson 43 Treebank-3 Joint-POS ID Word POS Head 1 2 3 4 5 6 7 8 9 well how many uh uh families own a refrigerator UH WRB JJ UH UH NNS VBP DT NN 7 3 6 6 6 7 0 9 7 Dep. Word POS Head discourse advmod amod discourse discourse nsubj root det dobj well how many of of families own a refrigerator UH WRB JJ IN IN NNS VB DT NN 7 3 7 3 3 5 0 9 7 tri4 Dep. Word POS Head discourse advmod nsubj dep prep pobj root det dobj well how many of of own on a refrigerator UH WRB JJ IN IN NNS IN DT NN 0 3 1 3 3 5 3 9 7 Dep. root advmod dep dep prep pobj prep det pobj T"
W17-4605,W96-0213,0,0.135042,"likelihood linear regression (fMLLR, Gales, 1998). We train this tri4 AM on the training split in Table 1 with duplicate utterances removed. 3.2 Baseline POS tagging We perform POS tagging with three out-of-thebox taggers, two of them with pretrained models, and choose the best one for our baseline pipeline model. NLTK’s (Bird et al., 2009) former default maximum entropy-based (ME) POS tagger with the pretrained model trained on WSJ data from the PTB (for an overview, see Taylor et al., 2003) is the first tagger and we term it ME.pre. We also train a ME POS tagger10 that is implemented after Ratnaparkhi (1996) on the first 70,000 sentences11 of our SWBD training split, described in Section 2, and denote our self-trained model by ME.70k. We configure the ME classifier to use the optimized version of MEGAM (Daum´e III, 2004) for speed. The second tagger is NLTK’s current default tagger, based on a greedy averaged perceptron (AP) tagger developed by Matthew Honnibal12 . We name the AP tagger with the pretrained NLTK model AP.pre, and the same tagger trained on the full training split AP. To have an NLTK-external industry-standard POS tagger in our comparison, we also run spaCy’s POS tagger (see https:"
W17-4605,roark-etal-2006-sparseval,0,0.0345371,"in the reranking framework of Collins (2000). Kahn et al. (2005) find that combining prosodic features with non-local syntactic features increase F -scores in the relaxed edited metric of Charniak and Johnson (2001). Kahn and Ostendorf (2012) present an approach that automatically recognizes speech, segments a stream of words (e.g. a conversation side/speaker turn) into sentences and parses these. A reranker that can take into account ASR posteriors for n-best ASR hypotheses as well as parse-specific features for m-best parses can then jointly optimize towards WER (n hypotheses) or SParseval (Roark et al., 2006) (n × m hypotheses) metrics (Kahn and Ostendorf, 2012). Ehrlich and Hanrieder (1996) describe an agenda-driven chart parser that considers an acoustic word-level score from a word lattice and can combine a sentencespanning analysis from partial hypotheses if a full parse is unobtainable. Tran et al. (2017) use speech and text domain cues for constituent parsing in an attention-based encoder-decoder approach based on Vinyals et al. (2015). They show that word-level acoustic-prosodic features learned with convolutional neural networks improve performance. and a smoothing approach that backs off"
W17-4605,H89-1043,0,0.709289,"all of these need to be available. Considering the general premise in data-driven methods that more data is better data, we choose the Switchboard-1 Release 21 (Godfrey et al., 1992) corpus with about 2400 dialogs. The Switchboard (SWBD) corpus has more recently been furnished with the NXT Switchboard annotations2 (Calhoun et al., 2010). NXT provides a plethora of annotations and most importantly for our work, an alignment of Treebank-33 (Marcus et al., 1999) text and SWBD transcriptions4 . While the Treebank-3 corpus proIntroduction Parsing speech is an essential part (Chow and Roukos, 1989; Moore et al., 1989; Su et al., 1992; Chappelier et al., 1999; Collins et al., 2004) of spoken language understanding (SLU) and difficult because spontaneous speech and syntax clash (Ehrlich and Hanrieder, 1996; Charniak and Johnson, 2001; B´echet et al., 2014). Pipeline approaches concatenating a speech recognizer, a POS tagger and a parser often rely on n-best hypotheses decoded from lattices. While n-best hypotheses cover more of the hypothesis space than the 1-best hypothesis, they are redundant and incomplete. Lattices on the other hand are efficiently representing all hypotheses under consideration and the"
W17-4605,J08-4003,0,0.0177104,"short sentences for presentability. The third graph visualization in Figure 6 illustrates an ASR deletion error on the first word. The pipeline tri4 model handles the error gracefully, but receives lower US and LS scores because of the token mismatch nonetheless. If we had not allowed the imprecise evaluation, we would not have observed this kind of error. The example in Figure 7 also has an ASR error in the pipeline approach at its core. In this case, 42 root root aux dobj nsubj do VBP you PRP nn like VB rap NN tion and DP. Rasooli and Tetreault (2013) extend the arc-eager transition system (Nivre, 2008) with actions that handle reparanda, discourse markers and interjections, thereby also explicitly using marked disfluencies on SWBD for joint DP and disfluency detection. Where Rasooli and Tetreault (2013) and Honnibal and Johnson (2014) work with SWBD text data, Yoshikawa et al. (2016) are close to our setting and assume ASR output text as parser input. Yoshikawa et al. (2016) create an alignment that enables the transfer of gold treebank data to ASR output texts and add three actions to manage disfluencies and ASR errors to the arc-eager shift-reduce transition system of Zhang and Nivre (201"
W17-4605,D13-1013,0,0.0160555,"from the instances counted in the All column of Table 7 and focus on short sentences for presentability. The third graph visualization in Figure 6 illustrates an ASR deletion error on the first word. The pipeline tri4 model handles the error gracefully, but receives lower US and LS scores because of the token mismatch nonetheless. If we had not allowed the imprecise evaluation, we would not have observed this kind of error. The example in Figure 7 also has an ASR error in the pipeline approach at its core. In this case, 42 root root aux dobj nsubj do VBP you PRP nn like VB rap NN tion and DP. Rasooli and Tetreault (2013) extend the arc-eager transition system (Nivre, 2008) with actions that handle reparanda, discourse markers and interjections, thereby also explicitly using marked disfluencies on SWBD for joint DP and disfluency detection. Where Rasooli and Tetreault (2013) and Honnibal and Johnson (2014) work with SWBD text data, Yoshikawa et al. (2016) are close to our setting and assume ASR output text as parser input. Yoshikawa et al. (2016) create an alignment that enables the transfer of gold treebank data to ASR output texts and add three actions to manage disfluencies and ASR errors to the arc-eager s"
W17-4605,P15-1032,0,0.0380764,"Missing"
W17-4605,D16-1109,0,0.0126859,"t allowed the imprecise evaluation, we would not have observed this kind of error. The example in Figure 7 also has an ASR error in the pipeline approach at its core. In this case, 42 root root aux dobj nsubj do VBP you PRP nn like VB rap NN tion and DP. Rasooli and Tetreault (2013) extend the arc-eager transition system (Nivre, 2008) with actions that handle reparanda, discourse markers and interjections, thereby also explicitly using marked disfluencies on SWBD for joint DP and disfluency detection. Where Rasooli and Tetreault (2013) and Honnibal and Johnson (2014) work with SWBD text data, Yoshikawa et al. (2016) are close to our setting and assume ASR output text as parser input. Yoshikawa et al. (2016) create an alignment that enables the transfer of gold treebank data to ASR output texts and add three actions to manage disfluencies and ASR errors to the arc-eager shift-reduce transition system of Zhang and Nivre (2011). While they do not parse lattices or confusion networks (lattices can be converted to confusion networks, see Mangu et al. (2000)) directly, Yoshikawa et al. (2016) use information from word confusion networks to discover erroneous regions in the ASR output. Charniak and Johnson (200"
W17-4605,P11-2033,0,0.0416819,"system (Nivre, 2008) with actions that handle reparanda, discourse markers and interjections, thereby also explicitly using marked disfluencies on SWBD for joint DP and disfluency detection. Where Rasooli and Tetreault (2013) and Honnibal and Johnson (2014) work with SWBD text data, Yoshikawa et al. (2016) are close to our setting and assume ASR output text as parser input. Yoshikawa et al. (2016) create an alignment that enables the transfer of gold treebank data to ASR output texts and add three actions to manage disfluencies and ASR errors to the arc-eager shift-reduce transition system of Zhang and Nivre (2011). While they do not parse lattices or confusion networks (lattices can be converted to confusion networks, see Mangu et al. (2000)) directly, Yoshikawa et al. (2016) use information from word confusion networks to discover erroneous regions in the ASR output. Charniak and Johnson (2001) parse SWBD after removing edited speech that they identify with a linear classifier. Additionally, Charniak and Johnson (2001) introduce a relaxed edited parsing metric that considers a simplified gold standard constituent parse (removed edited words are added back into the constituent parse for evaluation). Jo"
W17-4605,P11-2000,0,0.100929,"008) with actions that handle reparanda, discourse markers and interjections, thereby also explicitly using marked disfluencies on SWBD for joint DP and disfluency detection. Where Rasooli and Tetreault (2013) and Honnibal and Johnson (2014) work with SWBD text data, Yoshikawa et al. (2016) are close to our setting and assume ASR output text as parser input. Yoshikawa et al. (2016) create an alignment that enables the transfer of gold treebank data to ASR output texts and add three actions to manage disfluencies and ASR errors to the arc-eager shift-reduce transition system of Zhang and Nivre (2011). While they do not parse lattices or confusion networks (lattices can be converted to confusion networks, see Mangu et al. (2000)) directly, Yoshikawa et al. (2016) use information from word confusion networks to discover erroneous regions in the ASR output. Charniak and Johnson (2001) parse SWBD after removing edited speech that they identify with a linear classifier. Additionally, Charniak and Johnson (2001) introduce a relaxed edited parsing metric that considers a simplified gold standard constituent parse (removed edited words are added back into the constituent parse for evaluation). Jo"
W17-4610,amoia-etal-2012-coreference,0,0.0212771,"solution with automatically predicted prosodic information Ina R¨osiger∗ , Sabrina Stehwien∗ , Arndt Riester, Ngoc Thang Vu Institute for Natural Language Processing University of Stuttgart, Germany {roesigia,stehwisa,arndt,thangvu}@ims.uni-stuttgart.de Abstract there exist a few systems for pronoun resolution in transcripts of spoken text (Strube and M¨uller, 2003; Tetreault and Allen, 2004). It has been shown that there are differences between written and spoken text that lead to a drop in performance when coreference resolution systems developed for written text are applied on spoken text (Amoia et al., 2012). For this reason, it may help to use additional information available from the speech signal, for example prosody. In West-Germanic languages, such as English and German, there is a tendency for coreferent items, i.e. entities that have already been introduced into the discourse (their information status is given), to be deaccented, as the speaker assumes the entity to be salient in the listener’s discourse model (cf. Terken and Hirschberg (1994); Baumann and Riester (2013); Baumann and Roth (2014)). We can make use of this fact by providing prosodic information to the coreference resolver. E"
W17-4610,P15-2014,1,0.731146,"Missing"
W17-4610,P03-1022,0,0.173247,"Missing"
W17-4610,bjorkelund-etal-2014-extended,1,0.904624,"Missing"
W17-4610,W12-4501,0,0.0298643,"tity. (1) (2) {John}1 has {an old cottage}2 . Last year {he}1 reconstructed {the shed}? . The pitch accent on shed in (2a) leads to the interpretation that the shed and the cottage refer to different entities, where the shed is a part of the cottage (they are in a bridging relation). In contrast, in (2b), the shed is deaccented, which suggests that the shed and the cottage corefer. A pilot study by R¨osiger and Riester (2015) has Coreference resolution is an active NLP research area, with its own track at most NLP conferences and several shared tasks such as the CoNLL or SemEval shared tasks (Pradhan et al., 2012; Recasens et al., 2010) or the CORBON shared task 20171 . Almost all work is based on text, although 1 {John}1 has {an old cottage}2 . a. Last year {he}1 reconstructed {the SHED}3 . b. Last year {he}1 reconSTRUCted the shed}2 . 2 The anaphor under consideration is typed in boldface, its antecedent is underlined. Accented syllables are capitalised. *The two first authors contributed equally to this work. http://corbon.nlp.ipipan.waw.pl/ 78 Proceedings of the First Workshop on Speech-Centric Natural Language Processing, pages 78–83 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association f"
W17-4610,W09-2411,0,0.0811576,"Missing"
W17-5530,E17-1003,0,0.0292284,"Missing"
W17-5530,N06-1036,0,0.64774,"Missing"
W17-5530,N16-1037,0,0.151537,"atural language understanding tasks and spoken dialog systems. This classification task has been approached using traditional statistical methods such as hidden Markov models (HMMs) (Stolcke et al., 2000), conditional random fields (CRF) (Zimmermann, 2009) and support vector machines (SVMs) (Henderson et al., 2012). However, recent works with deep learning (DL) techniques have brought state-ofthe-art models in DA classification, such as convolutional neural networks (CNNs) (Kalchbrenner and Blunsom, 2013; Lee and Dernoncourt, 2016), recurrent neural networks (RNNs) (Lee and Dernoncourt, 2016; Ji et al., 2016) and long short-term memory (LSTM) models (Shen and Lee, 2016). Attention mechanisms (AMs) introduced by Bahdanau et al. (2014) have contributed to significant improvements in many natural language processing tasks, for instance machine translation (Bahdanau et al., 2014), sentence classification (Shen and Lee, 2016) and summarization (Rush et al., 2015), uncertainty detection (Adel and Sch¨utze, 2017), speech recognition (Chorowski et al., 2015), sentence pair modeling (Yin et al., 2015), question-answering (Golub and He, 2016), 247 Proceedings of the SIGDIAL 2017 Conference, pages 247–252, c"
W17-5530,W14-4012,0,0.11893,"Missing"
W17-5530,W13-3214,0,0.244893,"m the Switchboard (SwDA) dataset with DA annotation. Automatic DA classification is an important pre-processing step in natural language understanding tasks and spoken dialog systems. This classification task has been approached using traditional statistical methods such as hidden Markov models (HMMs) (Stolcke et al., 2000), conditional random fields (CRF) (Zimmermann, 2009) and support vector machines (SVMs) (Henderson et al., 2012). However, recent works with deep learning (DL) techniques have brought state-ofthe-art models in DA classification, such as convolutional neural networks (CNNs) (Kalchbrenner and Blunsom, 2013; Lee and Dernoncourt, 2016), recurrent neural networks (RNNs) (Lee and Dernoncourt, 2016; Ji et al., 2016) and long short-term memory (LSTM) models (Shen and Lee, 2016). Attention mechanisms (AMs) introduced by Bahdanau et al. (2014) have contributed to significant improvements in many natural language processing tasks, for instance machine translation (Bahdanau et al., 2014), sentence classification (Shen and Lee, 2016) and summarization (Rush et al., 2015), uncertainty detection (Adel and Sch¨utze, 2017), speech recognition (Chorowski et al., 2015), sentence pair modeling (Yin et al., 2015)"
W17-5530,D14-1181,0,0.0124714,"Missing"
W17-5530,N16-1062,0,0.732366,"it is not always obvious even for human beings to find the corresponding dialog act. In many cases, the utterances are too short so that is hard to classify them, for example the utterance ’Right’ can be either an Agreement or a Backchannel indicating the interlocutor to go on talking, in this case the context plays a key role at disambiguating. Therefore, using context information from the previous utterances in a dialog flow is a crucial step for improving DA classification. Few papers in the literature have suggested to utilize context as a potential knowledge source for DA classification (Lee and Dernoncourt, 2016; Shen and Lee, 2016). Recently, Ribeiro et al. (2015) presented an extensive analysis of the influence of context on DA recognition concluding that contextual information from preceding utterances helps to improve the classification performance. Nonetheless, such information should be differentiable from the current utterance information, otherwise, the contextual information could have a negative impact. Introduction The study of spoken dialogs between two or more speakers can be approached by analyzing the dialog acts (DAs), which is the intention of the speaker at every utterance during a"
W17-5530,D15-1044,0,0.051811,"ning (DL) techniques have brought state-ofthe-art models in DA classification, such as convolutional neural networks (CNNs) (Kalchbrenner and Blunsom, 2013; Lee and Dernoncourt, 2016), recurrent neural networks (RNNs) (Lee and Dernoncourt, 2016; Ji et al., 2016) and long short-term memory (LSTM) models (Shen and Lee, 2016). Attention mechanisms (AMs) introduced by Bahdanau et al. (2014) have contributed to significant improvements in many natural language processing tasks, for instance machine translation (Bahdanau et al., 2014), sentence classification (Shen and Lee, 2016) and summarization (Rush et al., 2015), uncertainty detection (Adel and Sch¨utze, 2017), speech recognition (Chorowski et al., 2015), sentence pair modeling (Yin et al., 2015), question-answering (Golub and He, 2016), 247 Proceedings of the SIGDIAL 2017 Conference, pages 247–252, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics 2.2 document classification (Yang et al., 2016) and entailment (Rockt¨aschel et al., 2015) . AMs let the model decide what parts of the input to pay attention to according to the relevance for the task. In this paper, we explore the use of AMs to learn the context r"
W17-5530,W04-2319,0,0.839654,"Missing"
W17-5530,J00-3003,0,0.603788,"Missing"
W17-5530,N16-1174,0,0.0850796,". (2014) have contributed to significant improvements in many natural language processing tasks, for instance machine translation (Bahdanau et al., 2014), sentence classification (Shen and Lee, 2016) and summarization (Rush et al., 2015), uncertainty detection (Adel and Sch¨utze, 2017), speech recognition (Chorowski et al., 2015), sentence pair modeling (Yin et al., 2015), question-answering (Golub and He, 2016), 247 Proceedings of the SIGDIAL 2017 Conference, pages 247–252, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics 2.2 document classification (Yang et al., 2016) and entailment (Rockt¨aschel et al., 2015) . AMs let the model decide what parts of the input to pay attention to according to the relevance for the task. In this paper, we explore the use of AMs to learn the context representation, as a manner to differentiate the current utterance from its context as well as a mechanism to highlight the most relevant information, while ignoring unimportant parts for DA classification. We propose and compare extensively different neural-based methods for context representation learning by leveraging a recurrent neural network architecture with LSTM (Hochreit"
W18-1204,W13-3520,0,0.0472256,"Missing"
W18-1204,W07-0607,0,0.0481769,"performance for the model based on individual character embeddings. Its forte is to deal with low-data situations, predicting meanings for unfamiliar words by utilizing familiar morphemes and other subword structures, in line with Landauer et al.’s (1997) claim of “vast numbers of weak interrelations that, if properly exploited, can greatly amplify learning by a process of inference”. In the future, we will evaluate our character-based model for other languages, and assess other aspects of its psycholinguistic plausibility, such as matching human behavior in performance and acquisition speed (Baroni et al., 2007). 10M corpora results. Finally, Table 6 shows the results for the 10M corpora. Here, we see a relatively heterogeneous picture regarding the individual models across benchmarks: WL does best on WS353-en; FT does best on RW and WS353-de; CL does best on GUR350. This behavior is consistent with the patterns we found for the 100M corpora, but more marked. Due to the inhomogeneity among models, the fusion model CAT does particularly well, outperforming FT on 3 of 4 benchmarks, often substantially so. As with the 100M corpora, the character aware models perform much better than WL for OOV pairs. Fo"
W18-1204,Q14-1020,1,0.803186,"Missing"
W18-1204,N07-2052,0,0.0676834,"Missing"
W18-1204,W13-3512,0,0.635245,"al ability of the human language faculty to generalize from little data. By seventh grade, students have only heard about 50 million spoken words, and read about 3.8 million tokens of text, acquiring a vocabulary of 40,000– 100,000 words (Landauer and Dumais, 1997). This also means that any new text likely contains outof-vocabulary words which students interpret by generalizing from existing knowledge – an ability that plain word embedding models lack. There are some studies that have focused on modeling infrequent and unseen words by capturing information at the subword and character levels. Luong et al. (2013) break words into morphemes, and use recursive neural networks to compose word meanings from morpheme meanings. Similarly, Bojanowski et al. (2017) represent words as bags of character n-grams, allowing morphology to inform word embeddings without requiring morphological analysis. However, both models are still typically applied to large corpora of training data, with the smallest English corpora used comprising about 1 billion tokens. Our study investigates how embedding models fare when applied to much smaller corpora, containing only millions of words. Few studies, except Sahlgren and Lenci"
W18-1204,D14-1162,0,0.097665,"two types of character-based embeddings on word relatedness prediction. On large corpora, performance of both model types is equal for frequent words, but character awareness already helps for infrequent words. Consistently, on small corpora, the characterbased models perform overall better than skipgrams. The concatenation of different embeddings performs best on small corpora and robustly on large corpora. 1 Introduction State-of-the-art word embedding models are routinely trained on very large corpora. For example, Mikolov et al. (2013a) train word2vec on a corpus of 6 billion tokens, and Pennington et al. (2014) report the best GloVe results on 42 billion tokens. From a language technology perspective, it is perfectly reasonable to use large corpora where available. However, even with large corpora, embeddings struggle to accurately model the meaning of infrequent words (Luong et al., 2013). Moreover, for the vast majority of languages, substantially less data is available. For example, there are only 4 languages with Wikipedias larger than 1 billion words,1 and 25 languages with more than 100 mil1 https://en.wikipedia.org/wiki/List_ of_Wikipedias#Detailed_list (as of 9 Jan 2018) 32 Proceedings of th"
W18-1204,D16-1099,0,0.100995,"uong et al. (2013) break words into morphemes, and use recursive neural networks to compose word meanings from morpheme meanings. Similarly, Bojanowski et al. (2017) represent words as bags of character n-grams, allowing morphology to inform word embeddings without requiring morphological analysis. However, both models are still typically applied to large corpora of training data, with the smallest English corpora used comprising about 1 billion tokens. Our study investigates how embedding models fare when applied to much smaller corpora, containing only millions of words. Few studies, except Sahlgren and Lenci (2016), have considered this setup in detail. We evaluate one word-based and two character-based embedding models on word relatedness tasks for English and German. We find that that the character-based models mimics human learning more closely, with both better results on small datasets and better performance on rare words. At the same time, a fused representation that takes both word and character level into account yields the best results for small corpora. Most modern approaches to computing word embeddings assume the availability of text corpora with billions of words. In this paper, we explore"
W18-6529,P16-1154,0,0.0928934,"Missing"
W18-6529,P16-1160,0,0.0272043,"ard neural network (Zhou et al., 2017; Wiseman et al., 2017) or (3) by the means of an encoder RNN, which processes variable-sized inputs sequentially, giving rise to the Seq2Seq architecture. generate novel structures resulting from template combinations. In sum, we make the following contribution: • We compare word- and character-based Seq2Seq models for NLG on two datasets. • We conduct an extensive automatic and manual analysis of the generated texts and compare them to human performance. Character-based Seq2Seq models were first proposed for neural machine translation (Ling et al., 2015; Chung et al., 2016; Lee et al., 2017). Their main advantage over word-based models is that they can represent an unlimited word inventory with a small vocabulary. They can learn to copy any string from the input to the output, which is especially useful for data-to-text NLG, as information from the input such as the name of a restaurant or a database entity is often expected to appear verbatim in the generated text. Word-based models, in contrast, have to make use of delexicalization during pre- and postprocessing (Wen et al., 2015b; Duˇsek and Jurc´ıcek, 2016) or have to apply dedicated copy mechanisms (Gu et"
W18-6529,W17-3541,0,0.0327962,"Missing"
W18-6529,P15-2017,0,0.0285844,"observe any nonwords generated by the character-based models. Automatic Evaluation of Output Diversity While correctness is a necessity in NLG, in many settings it is not sufficient. Often, variation of the generated texts is crucial to avoid repetitive and unnatural outputs. Table 6 shows automatically computed statistics on the diversity of the generated texts of both models and human texts and on the overlap of the (generated) texts with the training set. We measure diversity by the number of unique sentences and words in all development set references and generated texts, as done e.g. by Devlin et al. (2015). Additionally, we report the Shannon text entropy as measure of the amount of variation in the texts following (Oraby et al., 2018b). We compute the text entropy E for words (unigrams) and uni-, bi-, and trigrams as follows: The most frequent content error in both datasets concerns omission of information. For the E2E dataset, the family friendly attribute is most frequently dropped by both model types, indicating that the verbalization of this boolean attribute is more difficult to learn than other attributes, whose values mostly appear verbatim in the text. Information modification of the w"
W18-6529,D16-1032,0,0.0332287,"tuttgart, Germany {jagfelga,beersa,thangvu}@ims.uni-stuttgart.de Abstract text datasets of two recent shared tasks for end-toend NLG, namely the E2E challenge (Novikova et al., 2017b) and WebNLG challenge (Gardent et al., 2017b). Example input-text pairs for both datasets are shown in Figure 1. Neural sequence to sequence (Seq2Seq) models (Graves, 2013; Sutskever et al., 2014) have shown promising results for this task, especially in combination with an attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). Several recent NLG approaches (Duˇsek and Jurc´ıcek, 2016; Mei et al., 2016; Kiddon et al., 2016; Agarwal and Dymetman, 2017), as well as most systems in the E2E and WebNLG challenge are based on this architecture. While most NLG models generate text word by word, promising results were also obtained by encoding the input and generating the output text character-by-character (Lipton et al., 2015; Goyal et al., 2016; Agarwal and Dymetman, 2017). Five out of 62 E2E challenge submissions operate on the character-level. However, it is difficult to draw conclusions from the challenge results with respect to this difference, since the submitted systems also differ in other aspects and were eva"
W18-6529,P16-2008,0,0.0502837,"Missing"
W18-6529,P17-4012,0,0.0349414,"odels, and found a smaller beam of five to yield better results for the character-based models. This is probably due to the much smaller vocabulary size of the character-based models. For automatic evaluation, we report BLEU (Papineni et al., 2002), which measures the precision of the generated n-grams compared to the references, and recall-oriented ROUGE-L (Lin, 2004), which measures the longest common subsequence between the generated texts and the references. We compute these scores with the E2E challenge evaluation script2 . Experiments We conduct our experiments with the OpenNMT toolkit (Klein et al., 2017), which we extend to also perform character-based processing. We 2 224 https://github.com/tuetschek/e2e-metrics 6 Results and Analysis formation (mainly concerning the attribute family friendly). We conclude that the large performance difference might be caused by automatic evaluation measures punishing additions more severely than omissions. Table 2 and 3 display the results on the E2E and WebNLG test sets for models of the respective challenges and our own models3 . Since the performance of neural models can vary considerably due to random parameter initialization and randomized training pro"
W18-6529,W17-4912,0,0.0561838,"Missing"
W18-6529,W17-4123,0,0.0214798,"nt detailed statistical and human analyses shed light on the differences between the two input representations and the diversity of the generated texts. In a controlled experiment with synthetic training data generated from templates, we demonstrate the ability of neural models to learn novel combinations of the templates and thereby generalize beyond the linguistic structures they were trained on. 1 Introduction Natural language generation (NLG) is an actively researched task, which according to Gatt and Krahmer (2018) can be divided into text-to-text generation, such as machine translation (Koehn, 2017), text summarization (See et al., 2017), or open-domain conversation response generation (Vinyals and Le, 2015) on the one hand, and data-to-text generation on the other hand. Here, we focus on the latter, the task of generating textual descriptions for structured data. Data-to-text generation comprises the generation of system responses based on dialog acts in task-oriented dialog systems (Wen et al., 2015b), sport games reports and weather forecasts (Angeli et al., 2010), and database entry descriptions (Gardent et al., 2017a). In this paper, we focus on sentence planning and surface realiza"
W18-6529,P17-1017,0,0.413751,") can be divided into text-to-text generation, such as machine translation (Koehn, 2017), text summarization (See et al., 2017), or open-domain conversation response generation (Vinyals and Le, 2015) on the one hand, and data-to-text generation on the other hand. Here, we focus on the latter, the task of generating textual descriptions for structured data. Data-to-text generation comprises the generation of system responses based on dialog acts in task-oriented dialog systems (Wen et al., 2015b), sport games reports and weather forecasts (Angeli et al., 2010), and database entry descriptions (Gardent et al., 2017a). In this paper, we focus on sentence planning and surface realization. We build on data-to221 Proceedings of The 11th International Natural Language Generation Conference, pages 221–232, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics E2E input: name[Midsummer House], customer rating [average], near [The Bakers] reference 1: Customers gave Midsummer House, near The Bakers, a 3 out of 5 rating. reference 2: Midsummer house has an average customer rating and is near The Bakers. delexicalized input: name[NAME], customer rating [average], near [NEA"
W18-6529,Q17-1026,0,0.0199938,"Zhou et al., 2017; Wiseman et al., 2017) or (3) by the means of an encoder RNN, which processes variable-sized inputs sequentially, giving rise to the Seq2Seq architecture. generate novel structures resulting from template combinations. In sum, we make the following contribution: • We compare word- and character-based Seq2Seq models for NLG on two datasets. • We conduct an extensive automatic and manual analysis of the generated texts and compare them to human performance. Character-based Seq2Seq models were first proposed for neural machine translation (Ling et al., 2015; Chung et al., 2016; Lee et al., 2017). Their main advantage over word-based models is that they can represent an unlimited word inventory with a small vocabulary. They can learn to copy any string from the input to the output, which is especially useful for data-to-text NLG, as information from the input such as the name of a restaurant or a database entity is often expected to appear verbatim in the generated text. Word-based models, in contrast, have to make use of delexicalization during pre- and postprocessing (Wen et al., 2015b; Duˇsek and Jurc´ıcek, 2016) or have to apply dedicated copy mechanisms (Gu et al., 2016; See et a"
W18-6529,W17-3518,0,0.346647,") can be divided into text-to-text generation, such as machine translation (Koehn, 2017), text summarization (See et al., 2017), or open-domain conversation response generation (Vinyals and Le, 2015) on the one hand, and data-to-text generation on the other hand. Here, we focus on the latter, the task of generating textual descriptions for structured data. Data-to-text generation comprises the generation of system responses based on dialog acts in task-oriented dialog systems (Wen et al., 2015b), sport games reports and weather forecasts (Angeli et al., 2010), and database entry descriptions (Gardent et al., 2017a). In this paper, we focus on sentence planning and surface realization. We build on data-to221 Proceedings of The 11th International Natural Language Generation Conference, pages 221–232, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics E2E input: name[Midsummer House], customer rating [average], near [The Bakers] reference 1: Customers gave Midsummer House, near The Bakers, a 3 out of 5 rating. reference 2: Midsummer house has an average customer rating and is near The Bakers. delexicalized input: name[NAME], customer rating [average], near [NEA"
W18-6529,P16-1094,0,0.0600302,"Missing"
W18-6529,W04-1013,0,0.0241824,"0 dimensions each. While a unidirectional encoder was sufficient for the word-based models, bidirectional encoders were beneficial for the character-based models on both datasets. We use a beam size of 15 for decoding with the word-based models, and found a smaller beam of five to yield better results for the character-based models. This is probably due to the much smaller vocabulary size of the character-based models. For automatic evaluation, we report BLEU (Papineni et al., 2002), which measures the precision of the generated n-grams compared to the references, and recall-oriented ROUGE-L (Lin, 2004), which measures the longest common subsequence between the generated texts and the references. We compute these scores with the E2E challenge evaluation script2 . Experiments We conduct our experiments with the OpenNMT toolkit (Klein et al., 2017), which we extend to also perform character-based processing. We 2 224 https://github.com/tuetschek/e2e-metrics 6 Results and Analysis formation (mainly concerning the attribute family friendly). We conclude that the large performance difference might be caused by automatic evaluation measures punishing additions more severely than omissions. Table 2"
W18-6529,C16-1103,0,0.150538,"Missing"
W18-6529,D15-1166,0,0.0823011,"gfeld, Sabrina Jenne, Ngoc Thang Vu Institute for Natural Language Processing (IMS) Universit¨at Stuttgart, Germany {jagfelga,beersa,thangvu}@ims.uni-stuttgart.de Abstract text datasets of two recent shared tasks for end-toend NLG, namely the E2E challenge (Novikova et al., 2017b) and WebNLG challenge (Gardent et al., 2017b). Example input-text pairs for both datasets are shown in Figure 1. Neural sequence to sequence (Seq2Seq) models (Graves, 2013; Sutskever et al., 2014) have shown promising results for this task, especially in combination with an attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). Several recent NLG approaches (Duˇsek and Jurc´ıcek, 2016; Mei et al., 2016; Kiddon et al., 2016; Agarwal and Dymetman, 2017), as well as most systems in the E2E and WebNLG challenge are based on this architecture. While most NLG models generate text word by word, promising results were also obtained by encoding the input and generating the output text character-by-character (Lipton et al., 2015; Goyal et al., 2016; Agarwal and Dymetman, 2017). Five out of 62 E2E challenge submissions operate on the character-level. However, it is difficult to draw conclusions from the challenge results with"
W18-6529,W18-5019,0,0.162049,"tandard Seq2Seq model (Cho et al., 2014) with Long Short-Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and attention. Given a training dataset of input-text pairs D = {(x1 , y¯1 ), (x2 , y¯2 ) . . . }, the model encodes an input sequence x = {x1 . . . xn } of symbols xi into a sequence of hidden states {h1 . . . hn } by applying a recurrent neural network (RNN) with LSTM cells that can store and forget sequence information: Output Diversity Evaluation of data-to-text NLG has traditionally centered around semantic fidelity, grammaticality, and naturalness (Gatt and Krahmer, 2018; Oraby et al., 2018b). More recently, the controllability of the style of the outputs and their variation has moved into focus as well (Ficler and Goldberg, 2017; Herzig et al., 2017; Oraby et al., 2018b,a). Oraby et al. (2018b) showed that the n-gram entropy of the outputs of a neural NLG system is significantly lower compared to its training data. This can be seen as evidence that the NLG system extracts only a few dominant patterns from the training data that it will generate over and over. Without explicit supervision signals, neural NLG models cannot distinguish linguistic or stylistic variation from noise."
W18-6529,J14-4003,0,0.0211189,"reviews relevant related work according to the two main aspects of this paper: different input and output representations for data-totext NLG as well as measuring and controlling the variation in the generated outputs. 2.1 Input and Output Representations While the first NLG systems relied on handwritten rules or templates that were filled with the input information (Cheyer and Guzzoni, 2006; Mirkovic et al., 2006), the availability of larger datasets has accelerated the progress in statistical methods to train NLG systems from data-text pairs in the last twenty years (Oh and Rudnicky, 2000; Mairesse and Young, 2014). Generating output via language models based on recurrent neural networks (RNNs) conditioned on the input (Sutskever et al., 2011) proved to be an effective method for end-to-end NLG (Wen et al., 2015a,b, 2016). Subword-based representations (Sennrich et al., 2016; Wu et al., 2016) can offer a trade-off between word- and character-based processing and are a popular choice in NMT and summarization (See et al., 2017). Here, the vocabulary consists of subword units of different lengths, which are assigned by minimizing the entropy on the training set. We also experimented with such representatio"
W18-6529,P02-1040,0,0.101409,"ith 64 units each. All other models use 500-dimensional word- or character embeddings and two layers in the encoder and decoder with 500 dimensions each. While a unidirectional encoder was sufficient for the word-based models, bidirectional encoders were beneficial for the character-based models on both datasets. We use a beam size of 15 for decoding with the word-based models, and found a smaller beam of five to yield better results for the character-based models. This is probably due to the much smaller vocabulary size of the character-based models. For automatic evaluation, we report BLEU (Papineni et al., 2002), which measures the precision of the generated n-grams compared to the references, and recall-oriented ROUGE-L (Lin, 2004), which measures the longest common subsequence between the generated texts and the references. We compute these scores with the E2E challenge evaluation script2 . Experiments We conduct our experiments with the OpenNMT toolkit (Klein et al., 2017), which we extend to also perform character-based processing. We 2 224 https://github.com/tuetschek/e2e-metrics 6 Results and Analysis formation (mainly concerning the attribute family friendly). We conclude that the large perfor"
W18-6529,P08-1020,0,0.0847722,"Missing"
W18-6529,D17-1035,0,0.0284911,"e extend to also perform character-based processing. We 2 224 https://github.com/tuetschek/e2e-metrics 6 Results and Analysis formation (mainly concerning the attribute family friendly). We conclude that the large performance difference might be caused by automatic evaluation measures punishing additions more severely than omissions. Table 2 and 3 display the results on the E2E and WebNLG test sets for models of the respective challenges and our own models3 . Since the performance of neural models can vary considerably due to random parameter initialization and randomized training procedures (Reimers and Gurevych, 2017), we train ten models with different random seeds for each setting and report the average (avg) and standard deviation (SD). On the E2E test set, our single best word- and character-based models reach comparable results to the best challenge submissions. The wordbased models achieve significantly higher BLEU and ROUGE-L scores than the character-based models4 . On the WebNLG test set, the BLEU score of our best word-based model outperforms the best challenge submission by a small margin. The character-based model achieves a significantly higher ROUGE-L score than the wordbased model, whereas t"
W18-6529,N16-1086,0,0.0274606,"MS) Universit¨at Stuttgart, Germany {jagfelga,beersa,thangvu}@ims.uni-stuttgart.de Abstract text datasets of two recent shared tasks for end-toend NLG, namely the E2E challenge (Novikova et al., 2017b) and WebNLG challenge (Gardent et al., 2017b). Example input-text pairs for both datasets are shown in Figure 1. Neural sequence to sequence (Seq2Seq) models (Graves, 2013; Sutskever et al., 2014) have shown promising results for this task, especially in combination with an attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). Several recent NLG approaches (Duˇsek and Jurc´ıcek, 2016; Mei et al., 2016; Kiddon et al., 2016; Agarwal and Dymetman, 2017), as well as most systems in the E2E and WebNLG challenge are based on this architecture. While most NLG models generate text word by word, promising results were also obtained by encoding the input and generating the output text character-by-character (Lipton et al., 2015; Goyal et al., 2016; Agarwal and Dymetman, 2017). Five out of 62 E2E challenge submissions operate on the character-level. However, it is difficult to draw conclusions from the challenge results with respect to this difference, since the submitted systems also differ in other"
W18-6529,J09-4008,0,0.0362354,"fo. info. info. info. character-based models5 . Strikingly, on the E2E development set, both model variants significantly outperform human texts by far with respect to both automatic evaluation measures. While the human BLEU score is significantly higher than those of both systems on the WebNLG development set, there is no statistical difference between human and system ROUGE-L scores. This further demonstrates the limited utility of BLEU and ROUGLE-L scores to evaluate NLG outputs, which was previously suggested by weak correlations of such scores with human judgments (Scott and Moore, 2006; Reiter and Belz, 2009; Novikova et al., 2017a). Furthermore, the high scores on the E2E dataset imply that the models succeed in picking up patterns from the training data that transfer well to the similar development set, whereas human variation and creativity are punished by lexical overlap-based automatic evaluation scores. 6.3 dropped added modified repeated 40.0 0.0 4.4 0.0 30.0 0.0 0.0 0.0 42.9 6.7 19.0 15.2 66.7 1.9 1.9 28.6 8.6 15.2 9.5 3.8 12.4 5.7 46.7 69.5 33.3 31.4 79.0 26.7 linguistic errors punctuation errors grammatical errors spelling mistakes 5.6 13.3 0.0 5.6 14.4 0.0 overall correctness content c"
W18-6529,D17-1299,0,0.0513963,"Missing"
W18-6529,P17-1099,0,0.442615,"analyses shed light on the differences between the two input representations and the diversity of the generated texts. In a controlled experiment with synthetic training data generated from templates, we demonstrate the ability of neural models to learn novel combinations of the templates and thereby generalize beyond the linguistic structures they were trained on. 1 Introduction Natural language generation (NLG) is an actively researched task, which according to Gatt and Krahmer (2018) can be divided into text-to-text generation, such as machine translation (Koehn, 2017), text summarization (See et al., 2017), or open-domain conversation response generation (Vinyals and Le, 2015) on the one hand, and data-to-text generation on the other hand. Here, we focus on the latter, the task of generating textual descriptions for structured data. Data-to-text generation comprises the generation of system responses based on dialog acts in task-oriented dialog systems (Wen et al., 2015b), sport games reports and weather forecasts (Angeli et al., 2010), and database entry descriptions (Gardent et al., 2017a). In this paper, we focus on sentence planning and surface realization. We build on data-to221 Proceeding"
W18-6529,D17-1238,0,0.0426717,"Missing"
W18-6529,P16-1162,0,0.0397913,"rst NLG systems relied on handwritten rules or templates that were filled with the input information (Cheyer and Guzzoni, 2006; Mirkovic et al., 2006), the availability of larger datasets has accelerated the progress in statistical methods to train NLG systems from data-text pairs in the last twenty years (Oh and Rudnicky, 2000; Mairesse and Young, 2014). Generating output via language models based on recurrent neural networks (RNNs) conditioned on the input (Sutskever et al., 2011) proved to be an effective method for end-to-end NLG (Wen et al., 2015a,b, 2016). Subword-based representations (Sennrich et al., 2016; Wu et al., 2016) can offer a trade-off between word- and character-based processing and are a popular choice in NMT and summarization (See et al., 2017). Here, the vocabulary consists of subword units of different lengths, which are assigned by minimizing the entropy on the training set. We also experimented with such representations in preliminary experiments, but 222 3 found them to perform much worse than wordor character-based representations. Our impression is that recurring entity names in the training data coming from multiple reference texts for the same input lead to overfitting on"
W18-6529,W17-5525,0,0.0531749,"Missing"
W18-6529,W00-0306,0,0.214317,"lated Work This section reviews relevant related work according to the two main aspects of this paper: different input and output representations for data-totext NLG as well as measuring and controlling the variation in the generated outputs. 2.1 Input and Output Representations While the first NLG systems relied on handwritten rules or templates that were filled with the input information (Cheyer and Guzzoni, 2006; Mirkovic et al., 2006), the availability of larger datasets has accelerated the progress in statistical methods to train NLG systems from data-text pairs in the last twenty years (Oh and Rudnicky, 2000; Mairesse and Young, 2014). Generating output via language models based on recurrent neural networks (RNNs) conditioned on the input (Sutskever et al., 2011) proved to be an effective method for end-to-end NLG (Wen et al., 2015a,b, 2016). Subword-based representations (Sennrich et al., 2016; Wu et al., 2016) can offer a trade-off between word- and character-based processing and are a popular choice in NMT and summarization (See et al., 2017). Here, the vocabulary consists of subword units of different lengths, which are assigned by minimizing the entropy on the training set. We also experimen"
W18-6529,E17-1059,0,0.0190779,"ved by the Abilene regional airport. reference 2: Abilene, part of Texas, is served by the Abilene regional airport. delexicalized input: city served(AGENT-1[ BRIDGE -1]), is part of(BRIDGE -1[ PATIENT-1]) delexicalized reference 1: BRIDGE -1 is in PATIENT-1 and is served by the AGENT-1. Figure 1: Example input-reference pairs from the E2E and WebNLG development set. The input can be represented in several ways: (1) In a discrete vector space via one-hotvectors (Wen et al., 2015a,b), or in a continuous space either (2) by encoding fixed-size input information in a feed-forward neural network (Zhou et al., 2017; Wiseman et al., 2017) or (3) by the means of an encoder RNN, which processes variable-sized inputs sequentially, giving rise to the Seq2Seq architecture. generate novel structures resulting from template combinations. In sum, we make the following contribution: • We compare word- and character-based Seq2Seq models for NLG on two datasets. • We conduct an extensive automatic and manual analysis of the generated texts and compare them to human performance. Character-based Seq2Seq models were first proposed for neural machine translation (Ling et al., 2015; Chung et al., 2016; Lee et al., 2017)"
W18-6529,W15-4639,0,0.243925,"troduction Natural language generation (NLG) is an actively researched task, which according to Gatt and Krahmer (2018) can be divided into text-to-text generation, such as machine translation (Koehn, 2017), text summarization (See et al., 2017), or open-domain conversation response generation (Vinyals and Le, 2015) on the one hand, and data-to-text generation on the other hand. Here, we focus on the latter, the task of generating textual descriptions for structured data. Data-to-text generation comprises the generation of system responses based on dialog acts in task-oriented dialog systems (Wen et al., 2015b), sport games reports and weather forecasts (Angeli et al., 2010), and database entry descriptions (Gardent et al., 2017a). In this paper, we focus on sentence planning and surface realization. We build on data-to221 Proceedings of The 11th International Natural Language Generation Conference, pages 221–232, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics E2E input: name[Midsummer House], customer rating [average], near [The Bakers] reference 1: Customers gave Midsummer House, near The Bakers, a 3 out of 5 rating. reference 2: Midsummer house ha"
W18-6529,N16-1015,0,0.0357033,"Missing"
W18-6529,D15-1199,0,0.400371,"troduction Natural language generation (NLG) is an actively researched task, which according to Gatt and Krahmer (2018) can be divided into text-to-text generation, such as machine translation (Koehn, 2017), text summarization (See et al., 2017), or open-domain conversation response generation (Vinyals and Le, 2015) on the one hand, and data-to-text generation on the other hand. Here, we focus on the latter, the task of generating textual descriptions for structured data. Data-to-text generation comprises the generation of system responses based on dialog acts in task-oriented dialog systems (Wen et al., 2015b), sport games reports and weather forecasts (Angeli et al., 2010), and database entry descriptions (Gardent et al., 2017a). In this paper, we focus on sentence planning and surface realization. We build on data-to221 Proceedings of The 11th International Natural Language Generation Conference, pages 221–232, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics E2E input: name[Midsummer House], customer rating [average], near [The Bakers] reference 1: Customers gave Midsummer House, near The Bakers, a 3 out of 5 rating. reference 2: Midsummer house ha"
W18-6529,W17-5519,0,\N,Missing
W19-4815,2018.lilt-16.1,0,0.26119,"or balanced Dyck words. We compare four models with different architectures and different training objectives, but the main target is the same. All models have the same encoder architecture, a one-layer bidirectional LSTM with 50 hidden units, and only differ in the decoder. The first model tagger-last predicts the target with a simple linear transformation from the LSTM state of the last token. The second model tagger-all has exactly the same architecture, but it is trained to predict a closing bracket after every token in the sequence. The last predicted bracket is the main target for evaluaBernardy (2018) and Sennhauser and Berwick (2018) both frame the task as predicting the next valid closing bracket at any position in a Dyck word, which is arguably more difficult to bypass. Both works also put much emphasis on the generalization over the depth. Comparing to Sennhauser and Berwick (2018), our tagger models, while not perfectly generalizable, perform well above chance level, and the discrepancy between training and testing performance is much smaller. But the general conclusion holds, the RNN-based taggers do not generalize well for the task. Bernardy (2018) reports better results, but they u"
W19-4815,J76-4004,0,0.396313,"Missing"
W19-4815,W18-5414,0,0.483081,"has been a testbed for several research on the ability of Recurrent Neural Networks (RNNs), in particular the Long Short-term Memory model (LSTM) (Hochreiter and Schmidhuber, 1997), to learn contextfree grammars. It consists of strings with balanced pairs of brackets of different types, e.g., “[ < > ] [ ] < [ ] >”. Recognizing the generalized Dyck language is considered to be more difficult than an bn as tested in Gers and Schmidhuber (2001), since it cannot be simply solved by counting. Rather, the model has to remember the sequence of different (unclosed) brackets. Among the recent studies, Sennhauser and Berwick (2018) analyze the generalizability of LSTMs to learn the generalized Dyck language 2 Related Work Modeling context-free grammars with RNNs is of great interest for natural language processing, since recursion is considered an essential characteristic of natural languages if not universal (Hauser et al., 2002). Several recent studies focus on the RNNs’ ability to model deeper structural information against surface-level attractors (Linzen et al., 2016; Gulordava et al., 2018; Wilcox et al., 2018). 138 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NL"
W19-4815,W18-5425,0,0.508136,"nd Sch¨utzenberger, 1963). 3 Task and Models The Dyck language consists of strings (a Dyck word) of equal number of opening and closing brackets, and the number of closing brackets is never more than the opening brackets in any prefix of the string (a Dyck prefix). The generalized Dyck language has more than one type of bracket pairs, where all pairs have to be balanced and no crossing of different pairs is allowed. Formally, the generalized Dyck language is defined as DP with (oi , ci ) ∈ P, where (oi , ci ) are different bracket pairs. The language can be described by the following grammar: Skachkova et al. (2018) probes the recognition of the generalized Dyck language in two tasks. The first one is a language modeling task which predicts the next bracket in the actual generated data and measures the perplexity, which complicates the evaluation by introducing unnecessary non-determinism. The second task predicts the last closing bracket of a balanced Dyck word, which could be solved with a short-cut. One simply needs to keep a counter for the depth of the sequence, and record the most recent opening bracket each time the counter hits zero. The task is over-simplified by the fact that all the instances"
W19-4815,N18-1108,0,0.0312651,"by counting. Rather, the model has to remember the sequence of different (unclosed) brackets. Among the recent studies, Sennhauser and Berwick (2018) analyze the generalizability of LSTMs to learn the generalized Dyck language 2 Related Work Modeling context-free grammars with RNNs is of great interest for natural language processing, since recursion is considered an essential characteristic of natural languages if not universal (Hauser et al., 2002). Several recent studies focus on the RNNs’ ability to model deeper structural information against surface-level attractors (Linzen et al., 2016; Gulordava et al., 2018; Wilcox et al., 2018). 138 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 138–146 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics is much simpler both as a task and as a language. To solve this task, the model only needs to approximate a counter, and increment or decrement upon an opening or closing bracket. The LSTM’s ability to approximate a counter machine is discussed in Weiss et al. (2018b). While all these studies test the RNNs as acceptor (classifier) or transducer (tagger), we also test thei"
W19-4815,P18-2117,0,0.0856143,"’ ability to model deeper structural information against surface-level attractors (Linzen et al., 2016; Gulordava et al., 2018; Wilcox et al., 2018). 138 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 138–146 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics is much simpler both as a task and as a language. To solve this task, the model only needs to approximate a counter, and increment or decrement upon an opening or closing bracket. The LSTM’s ability to approximate a counter machine is discussed in Weiss et al. (2018b). While all these studies test the RNNs as acceptor (classifier) or transducer (tagger), we also test their ability to decode sequences, which is arguably a harder task. Instead of natural language data, where the correlation between structural and contextual/semantic information is difficult to avoid, many analyses with synthetic data have been conducted on RNNs since their invention, e.g., handling the XOR problem (Elman, 1990), the context-free language an bn and context-sensitive language an bn cn (Gers and Schmidhuber, 2001; Weiss et al., 2018b), and extracting finite-state automata (We"
W19-4815,W18-5423,0,0.0190741,"model has to remember the sequence of different (unclosed) brackets. Among the recent studies, Sennhauser and Berwick (2018) analyze the generalizability of LSTMs to learn the generalized Dyck language 2 Related Work Modeling context-free grammars with RNNs is of great interest for natural language processing, since recursion is considered an essential characteristic of natural languages if not universal (Hauser et al., 2002). Several recent studies focus on the RNNs’ ability to model deeper structural information against surface-level attractors (Linzen et al., 2016; Gulordava et al., 2018; Wilcox et al., 2018). 138 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 138–146 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics is much simpler both as a task and as a language. To solve this task, the model only needs to approximate a counter, and increment or decrement upon an opening or closing bracket. The LSTM’s ability to approximate a counter machine is discussed in Weiss et al. (2018b). While all these studies test the RNNs as acceptor (classifier) or transducer (tagger), we also test their ability to decode se"
W19-4815,Q16-1037,0,0.0260566,"not be simply solved by counting. Rather, the model has to remember the sequence of different (unclosed) brackets. Among the recent studies, Sennhauser and Berwick (2018) analyze the generalizability of LSTMs to learn the generalized Dyck language 2 Related Work Modeling context-free grammars with RNNs is of great interest for natural language processing, since recursion is considered an essential characteristic of natural languages if not universal (Hauser et al., 2002). Several recent studies focus on the RNNs’ ability to model deeper structural information against surface-level attractors (Linzen et al., 2016; Gulordava et al., 2018; Wilcox et al., 2018). 138 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 138–146 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics is much simpler both as a task and as a language. To solve this task, the model only needs to approximate a counter, and increment or decrement upon an opening or closing bracket. The LSTM’s ability to approximate a counter machine is discussed in Weiss et al. (2018b). While all these studies test the RNNs as acceptor (classifier) or transducer (ta"
W19-4815,D15-1166,0,0.049821,"which 139 tion and comparison, while the rest can be viewed as an auxiliary task. The third model generator-simple follows the common seq2seq architecture (Sutskever et al., 2014). It generates a sequence of closing brackets to eagerly complete the Dyck prefix into a balanced Dyck word, and the generation stops when the ‘$’ symbol is predicted. For this model, the first generated bracket is the main target and the rest is the auxiliary task. The fourth model generator-attention augments the decoder with the attention mechanism (Bahdanau et al., 2014). It uses the general variant of attention (Luong et al., 2015): [ < < [ depth = 2 > [ < < > [ ] distance = 8 [ ] < > [ ] [ embedded depth = 3 ] Figure 1: A visual representation of a Dyck prefix and the property values of the target token (the last ‘]’). probability 1 − p. If the current sequence is balanced (no unclosed brackets), then generating a random opening bracket is the only option. This generation procedure is similar to Skachkova et al. (2018), except that we do not stop generation once the sequence is balanced, instead we generate a new opening bracket and continue until the desired length is reached. Also, the generated sequence is not neces"
W19-5908,P17-1163,0,0.0287229,"Missing"
W19-5908,P17-4013,0,0.176653,"Missing"
W19-5908,N07-2038,0,0.731318,"Missing"
W19-5908,P17-1045,0,0.0209994,"s.uni-stuttgart.de Abstract to learn such policies automatically (Williams and Young, 2007) with a user simulator such as proposed in Schatzmann et al. (2007) within a task (Dhingra et al., 2017; Peng et al., 2018), between task and non-task (Yu et al., 2017) and also in multimodal dialog systems (Manuvinakurike et al., 2017; Zhang et al., 2018). Deep RL has been proven to be successful with Deep Q-Learning (DQN) (Mnih et al., 2013) introducing the idea of using neural networks as a Q-function approximator. It has been widely used in the context of dialog policy learning (Fatemi et al., 2016; Dhingra et al., 2017; Casanueva et al., 2017). However according to a recent comparison (Casanueva et al., 2017) in the context of dialog policy learning, it performed worse than other RL methods such as Gaussian Process in many testing conditions. Recently, several advances in deep RL such as distributional RL (Bellemare et al., 2017) , dueling network architectures (Wang et al., 2016) and their combination (Hessel et al., 2018) a Rainbow agent - have been shown to be promising for further improvements of deep RL agents in benchmark environments, e.g. Atari 2600. However, it is still unclear whether these method"
W19-5908,W16-3613,0,0.0339464,"Missing"
W19-5908,E17-1042,0,0.0763142,"Missing"
W19-5908,W17-5539,0,0.0690637,"Missing"
W19-5908,W18-5015,0,0.0346205,"Missing"
W19-8636,W18-3601,0,0.326729,"es to capture the structural information from the input and score the (partial) output sequence, and use the perceptron to learn the parameters. Both linearizers achieve state-of-the-art performance on the Surface Realization Shared Task 2011 data (Belz et al., 2011) as part of a pipeline or joint system for the full task including deep semantic generation and word inflection (Bohnet et al., 2011; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Also, they have not been tested on the data from the recent shared task (Belz et al., 2018), where they could have served as very strong baselines to put recent developments into context. Song et al. (2018) are the first to use a neural model for syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning (2014) to predict transitions for linearization, which essentially replaces the perceptron with an MLP for the transition system in Liu et al. (2015). However, their adoption of neural models only takes advantage of the token-level representation such as word embeddings, while the structural information is still not well modeled. Recently, many neura"
W19-8636,P14-1005,1,0.900164,"Missing"
W19-8636,C10-1012,0,0.71832,"the red dotted arrows indicate the bottom-up pass, and the blue dashed arrows indicate the top-down pass. We highlight how node 8 influences node 4. Its representation v8◦ is first propagated up to the lowest common ancestor v2↑ , then goes down to v4↓ . 2 3 5 4 6 7 8 Figure 2: An illustration of the information flow in the encoder, where the red dotted arrows represent the bottom-up pass and the blue dashed arrows represent the top-down pass. The solid arrows illustrate the information flow from node 8 to node 4. 2.1.2 Head-First Decoder We adopt the general divide-and-conquer strategy as in Bohnet et al. (2010), by first linearizing each subtree and then combining the ordered subtrees into a full sentence. Instead of generating the sequences from left to right as in Bohnet et al. (2010), we generate the sequence from inside out, i.e., we initialize the sequence with the head, and expand outwards by appending the dependents to the left or the right end of the sequence. This new generation order is motivated by the linguistic research on word order constraints, which largely focuses on the relative direction and distance of the dependent to the head (Gibson, 1998; Liu, 2010; Gulordava, 2018). Followin"
W19-8636,D14-1082,0,0.0423061,"Belz et al., 2011) as part of a pipeline or joint system for the full task including deep semantic generation and word inflection (Bohnet et al., 2011; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Also, they have not been tested on the data from the recent shared task (Belz et al., 2018), where they could have served as very strong baselines to put recent developments into context. Song et al. (2018) are the first to use a neural model for syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning (2014) to predict transitions for linearization, which essentially replaces the perceptron with an MLP for the transition system in Liu et al. (2015). However, their adoption of neural models only takes advantage of the token-level representation such as word embeddings, while the structural information is still not well modeled. Recently, many neural models are proposed to represent graph structures, cf. Zhou et al. (2018) for an overview. Among them, Tree-LSTM, in particular the Child-Sum variation (Tai et al., 2015), has been proposed to model (unordered) We present a dependency tree linearizatio"
W19-8636,P04-1015,0,0.10905,"in Bj¨orkelund and Kuhn (2014): at every step after pruning the beam, we check if there is still at least one gold partial sequence in the beam. If not, then we calculate the hinge loss between the highest scoring gold sequence and all incorrect sequences in the beam2 . We also follow the delayed LaSO strategy in Bj¨orkelund and Kuhn (2014): after all gold partial sequences fall out of the beam and a loss is incurred, we continue training by putting the gold sequence back into the beam, until reaching the full sequence. This is shown to be more sample efficient than the early-update strategy (Collins and Roark, 2004), since it allows the model to train on the full sequence, even if the gold path falls out of the beam early. The standard hinge loss updates the gold sequence against the incorrect ones by enforcing a margin (typically 1), which punishes all incorrect sequences equally. However, not all incorrect sequences are equally bad in terms of the BLEU score, therefore, maintaining a larger margin for worse sequences could improve the performance. We cannot directly use BLEU score as the margin, since it is calculated on the sentence level, while we are training on the subtree level, and the sequences"
W19-8636,P17-1183,0,0.0484982,"H2LR and Voting) with Bohnet et al. (2010) and Puduppully et al. (2016). 3.4 Inflection Table 2 shows the inflection performance with different models: the first model predicts edit script as a tag (EditTag); the second model predicts the character sequence of the inflected word (CharSeq); the third model predicts the edit scripts as sequences of actions (EditSeq); and the last one uses the same model as the third, but first applies the extracted rules if available (+rule). The results are compared to the reported inflection accuracy in Puzikov and Gurevych (2018) (P18), which is adapted from Aharoni and Goldberg (2017). Among our first three models, EditTag performs the lowest, mainly because of the very large tag sets in many languages (the sizes vary from around 300 for English to over 10000 for Finnish and Russian), which prevents effective learning and generalization. The CharSeq model performs much 6 https://www.ims.uni-stuttgart.de/ institut/mitarbeiter/xiangyu/ 7 They use beam size of 1000, which can cover all possible permutations of up to 6 tokens (6! = 720). 284 P18 EditTag CharSeq EditSeq +rule ar cs en es fi fr it nl pt ru 93.07 99.53 98.11 99.59 95.46 95.56 97.44 95.68 99.30 98.22 88.02 97.52 9"
W19-8636,W18-3606,0,0.0855203,"e we compare with the results from other participants in the shared task, as well as the linearizers of Bohnet et al. (2010) and Puduppully et al. (2016) combined with our inflection and detokenization models as additional baselines for the shared task. Table 4 shows the performance of the full pipeline on the test sets. B10 and P16 are the linearizers by Bohnet et al. (2010) and Puduppully et al. (2016) combined with our inflection and detokenization model, ST18 are the best results for each language in the shared task (King and White, 2018; Puzikov and Gurevych, 2018; Ferreira et al., 2018; Elder and Hokamp, 2018). The last column contains the results of our system. Detokenization As the final step, we evaluate the performance of the detokenization, which includes contracting words and attaching punctuation. We use gold linearization and inflection as the input. We separate the evaluation into two parts: for contraction, we evaluate the token-based BLEU score against the gold contraction on the UD development set; for the punctuation attachment, we 285 It is apparent that both B10 and P16 have higher performance than the other systems by a large margin. The advantage of our linearizer also carries over"
W19-8636,C00-1007,0,0.148962,"n, as in two previous surface realization shared tasks (Belz et al., 2011, 2018). As morphological inflection prediction is in itself a separate task (Cotterell et al., 2016), we mainly focus on the linearization in this paper. Syntactic linearization has been extensively studied in the literature. Earlier work mostly focuses on grammar-based approaches using different syntactic formalisms (Elhadad and Robin, 1992; Lavoie and Rainbow, 1997; Carroll et al., 1999). Recently, with the increasing availability of annotated treebanks, statistical methods gain popularity (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Filippova and Strube, 2009). 279 Proceedings of The 12th International Conference on Natural Language Generation, pages 279–289, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics cheio dependency trees. It differs from the sequential LSTM (Hochreiter and Schmidhuber, 1997) in that it aggregates the hidden states of multiple dependents by summation. It is in turn improved by adding the attention mechanism to the hidden states (Zhou et al., 2016), so that each dependent influences the head representation to different degrees. Miwa and Bansal (2016) propose a"
W19-8636,W11-2832,0,0.690464,"ategy and use beam search to incrementally find the best linearization for each subtree; Liu et al. (2015) propose a transition system akin to dependency parsing that produces a sentence that respects the given tree constraints, which is later improved by Puduppully et al. (2016) with look-ahead features. Both approaches rely on rich feature templates to capture the structural information from the input and score the (partial) output sequence, and use the perceptron to learn the parameters. Both linearizers achieve state-of-the-art performance on the Surface Realization Shared Task 2011 data (Belz et al., 2011) as part of a pipeline or joint system for the full task including deep semantic generation and word inflection (Bohnet et al., 2011; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Also, they have not been tested on the data from the recent shared task (Belz et al., 2018), where they could have served as very strong baselines to put recent developments into context. Song et al. (2018) are the first to use a neural model for syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning ("
W19-8636,W18-3604,0,0.0141388,"e full experiment, where we compare with the results from other participants in the shared task, as well as the linearizers of Bohnet et al. (2010) and Puduppully et al. (2016) combined with our inflection and detokenization models as additional baselines for the shared task. Table 4 shows the performance of the full pipeline on the test sets. B10 and P16 are the linearizers by Bohnet et al. (2010) and Puduppully et al. (2016) combined with our inflection and detokenization model, ST18 are the best results for each language in the shared task (King and White, 2018; Puzikov and Gurevych, 2018; Ferreira et al., 2018; Elder and Hokamp, 2018). The last column contains the results of our system. Detokenization As the final step, we evaluate the performance of the detokenization, which includes contracting words and attaching punctuation. We use gold linearization and inflection as the input. We separate the evaluation into two parts: for contraction, we evaluate the token-based BLEU score against the gold contraction on the UD development set; for the punctuation attachment, we 285 It is apparent that both B10 and P16 have higher performance than the other systems by a large margin. The advantage of our lin"
W19-8636,N09-2057,0,0.0421763,"e realization shared tasks (Belz et al., 2011, 2018). As morphological inflection prediction is in itself a separate task (Cotterell et al., 2016), we mainly focus on the linearization in this paper. Syntactic linearization has been extensively studied in the literature. Earlier work mostly focuses on grammar-based approaches using different syntactic formalisms (Elhadad and Robin, 1992; Lavoie and Rainbow, 1997; Carroll et al., 1999). Recently, with the increasing availability of annotated treebanks, statistical methods gain popularity (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Filippova and Strube, 2009). 279 Proceedings of The 12th International Conference on Natural Language Generation, pages 279–289, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics cheio dependency trees. It differs from the sequential LSTM (Hochreiter and Schmidhuber, 1997) in that it aggregates the hidden states of multiple dependents by summation. It is in turn improved by adding the attention mechanism to the hidden states (Zhou et al., 2016), so that each dependent influences the head representation to different degrees. Miwa and Bansal (2016) propose a bidirectional extension that"
W19-8636,A97-1039,0,0.115898,"is a natural language generation task that searches for the natural linear order of words given an unordered syntax tree. Often, the task is accompanied by predicting word inflection, as in two previous surface realization shared tasks (Belz et al., 2011, 2018). As morphological inflection prediction is in itself a separate task (Cotterell et al., 2016), we mainly focus on the linearization in this paper. Syntactic linearization has been extensively studied in the literature. Earlier work mostly focuses on grammar-based approaches using different syntactic formalisms (Elhadad and Robin, 1992; Lavoie and Rainbow, 1997; Carroll et al., 1999). Recently, with the increasing availability of annotated treebanks, statistical methods gain popularity (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Filippova and Strube, 2009). 279 Proceedings of The 12th International Conference on Natural Language Generation, pages 279–289, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics cheio dependency trees. It differs from the sequential LSTM (Hochreiter and Schmidhuber, 1997) in that it aggregates the hidden states of multiple dependents by summation. It is in turn improved by add"
W19-8636,W15-2112,0,0.0206996,". (2) inflection est´ a (3) detokenization est´ a Figure 1: Overview of the pipeline and an example of the process from an unordered dependency tree to the final sentence. hensive comparison with several strong baselines on the recent multilingual linearization shared task data, and achieve state-of-the-art performance. In most linearization models, the incremental generation algorithm follows the left-to-right sequential order. However, in the linguistic study, the head position often plays a central role in describing the constraints and optimization of word orders (Gibson, 1998; Liu, 2010; Futrell et al., 2015). In the linearization models that employ left-to-right generation, such word order properties are only implicitly reflected in the features, if at all. Inspired by the above-mentioned study on head-oriented word order constraints, we adopt an improved linearization algorithm, in which we generate the sequence starting from the head and expanding to both directions. The head-first generation order can easily capture the constraints, since it naturally separates the decision into two aspects: (1) which side of the head to append the dependent and (2) which dependent to attach closer to the head"
W19-8636,N15-1012,0,0.0184646,"1; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Also, they have not been tested on the data from the recent shared task (Belz et al., 2018), where they could have served as very strong baselines to put recent developments into context. Song et al. (2018) are the first to use a neural model for syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning (2014) to predict transitions for linearization, which essentially replaces the perceptron with an MLP for the transition system in Liu et al. (2015). However, their adoption of neural models only takes advantage of the token-level representation such as word embeddings, while the structural information is still not well modeled. Recently, many neural models are proposed to represent graph structures, cf. Zhou et al. (2018) for an overview. Among them, Tree-LSTM, in particular the Child-Sum variation (Tai et al., 2015), has been proposed to model (unordered) We present a dependency tree linearization model with two novel components: (1) a tree-structured encoder based on bidirectional Tree-LSTM that propagates information first bottom-up t"
W19-8636,N10-1115,0,0.107977,"ation, inflection, and detokenization and evaluate with the official evaluation script. We also apply our inflection and detokenization steps on the predicted linearization of B10 and P16, so that they can also be compared to other systems. 3.2 The H2LR order performs better than L2R and R2L, which could be explained in multiple aspects. One explanation is our motivation that generating from the head could better reflect word order constraints. The other explanation is that training with latent generation order allows the model to make easier decision first, similar to the easyfirst parser by Goldberg and Elhadad (2010). Finally, when combining the three decoders together by voting, it achieves 2 BLEU points higher than B10. There are two main reasons for this improvement: (1) multitask-style training helps regularize the parameters, and (2) different generation directions tend to prune the correct sequences at different locations, and the mistake in one direction might be saved by the other two. Implementation Details Our model is implemented with the DyNet Library (Neubig et al., 2017), and is available at the first author’s website6 . We use the embedding sizes of 64, 32 and 32 for lemma, UPOS and depende"
W19-8636,P16-1105,0,0.0843817,"1998; Bangalore and Rambow, 2000; Filippova and Strube, 2009). 279 Proceedings of The 12th International Conference on Natural Language Generation, pages 279–289, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics cheio dependency trees. It differs from the sequential LSTM (Hochreiter and Schmidhuber, 1997) in that it aggregates the hidden states of multiple dependents by summation. It is in turn improved by adding the attention mechanism to the hidden states (Zhou et al., 2016), so that each dependent influences the head representation to different degrees. Miwa and Bansal (2016) propose a bidirectional extension that traverses the tree both bottom-up and top-down to allow the tokens access information from their descendants as well as ancestors. We adopt and combine their proposed models to represent the tree structure in our task, while improving the bidirectional extension by using the output of the bottom-up pass as the input for the top-down pass, so that each token can access information from all other tokens. tesouro este estar . de (1) linearization estar cheio de este tesouro . cheia de estes tesouros . cheia destes tesouros. (2) inflection est´ a (3) detoken"
W19-8636,L16-1262,0,0.0749806,"Missing"
W19-8636,P09-1091,0,0.020622,"nted word order constraints, we adopt an improved linearization algorithm, in which we generate the sequence starting from the head and expanding to both directions. The head-first generation order can easily capture the constraints, since it naturally separates the decision into two aspects: (1) which side of the head to append the dependent and (2) which dependent to attach closer to the head, which exactly correspond to the two aspects of the word order constraints, namely (1) the direction of the dependent and (2) the distance of dependent to the head. The algorithm is somewhat similar to He et al. (2009), which also emphasizes the central role of the head by first predicting for each dependent which side of the head it is placed. However, they exhaustively score all permutations, which could be intractable for subtrees with too many dependents, while we use incremental beam-search to guarantee the efficiency. 2 Model We use a pipeline system for the surface realization task, consisting of three steps: linearization (§2.1), inflection (§2.2), and detokenization (§2.3). Figure 1 gives an overview of the pipeline along with an example from the input tree to output text. The input is an unordered"
W19-8636,N16-1058,0,0.278197,"earization with Tree-Structured Representation Xiang Yu, Agnieszka Falenska, Ngoc Thang Vu, Jonas Kuhn Institut f¨ur Maschinelle Sprachverarbeitung Universit¨at Stuttgart, Germany firstname.lastname@ims.uni-stuttgart.de Abstract Among the most successful statistical linearization systems, Bohnet et al. (2010) employ the divide-and-conquer strategy and use beam search to incrementally find the best linearization for each subtree; Liu et al. (2015) propose a transition system akin to dependency parsing that produces a sentence that respects the given tree constraints, which is later improved by Puduppully et al. (2016) with look-ahead features. Both approaches rely on rich feature templates to capture the structural information from the input and score the (partial) output sequence, and use the perceptron to learn the parameters. Both linearizers achieve state-of-the-art performance on the Surface Realization Shared Task 2011 data (Belz et al., 2011) as part of a pipeline or joint system for the full task including deep semantic generation and word inflection (Bohnet et al., 2011; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Al"
W19-8636,W16-2010,0,0.0707323,"Missing"
W19-8636,E17-1061,0,0.0135066,"y parsing that produces a sentence that respects the given tree constraints, which is later improved by Puduppully et al. (2016) with look-ahead features. Both approaches rely on rich feature templates to capture the structural information from the input and score the (partial) output sequence, and use the perceptron to learn the parameters. Both linearizers achieve state-of-the-art performance on the Surface Realization Shared Task 2011 data (Belz et al., 2011) as part of a pipeline or joint system for the full task including deep semantic generation and word inflection (Bohnet et al., 2011; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Also, they have not been tested on the data from the recent shared task (Belz et al., 2018), where they could have served as very strong baselines to put recent developments into context. Song et al. (2018) are the first to use a neural model for syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning (2014) to predict transitions for linearization, which essentially replaces the perceptron with an MLP for the transition system in Liu et al. (2015). However,"
W19-8636,W18-3605,0,0.270518,"best variant for each step in the pipeline for the full experiment, where we compare with the results from other participants in the shared task, as well as the linearizers of Bohnet et al. (2010) and Puduppully et al. (2016) combined with our inflection and detokenization models as additional baselines for the shared task. Table 4 shows the performance of the full pipeline on the test sets. B10 and P16 are the linearizers by Bohnet et al. (2010) and Puduppully et al. (2016) combined with our inflection and detokenization model, ST18 are the best results for each language in the shared task (King and White, 2018; Puzikov and Gurevych, 2018; Ferreira et al., 2018; Elder and Hokamp, 2018). The last column contains the results of our system. Detokenization As the final step, we evaluate the performance of the detokenization, which includes contracting words and attaching punctuation. We use gold linearization and inflection as the input. We separate the evaluation into two parts: for contraction, we evaluate the token-based BLEU score against the gold contraction on the UD development set; for the punctuation attachment, we 285 It is apparent that both B10 and P16 have higher performance than the other"
W19-8636,W18-3602,0,0.154943,"where we compare different generation orders (L2R, R2L, H2LR and Voting) with Bohnet et al. (2010) and Puduppully et al. (2016). 3.4 Inflection Table 2 shows the inflection performance with different models: the first model predicts edit script as a tag (EditTag); the second model predicts the character sequence of the inflected word (CharSeq); the third model predicts the edit scripts as sequences of actions (EditSeq); and the last one uses the same model as the third, but first applies the extracted rules if available (+rule). The results are compared to the reported inflection accuracy in Puzikov and Gurevych (2018) (P18), which is adapted from Aharoni and Goldberg (2017). Among our first three models, EditTag performs the lowest, mainly because of the very large tag sets in many languages (the sizes vary from around 300 for English to over 10000 for Finnish and Russian), which prevents effective learning and generalization. The CharSeq model performs much 6 https://www.ims.uni-stuttgart.de/ institut/mitarbeiter/xiangyu/ 7 They use beam size of 1000, which can cover all possible permutations of up to 6 tokens (6! = 720). 284 P18 EditTag CharSeq EditSeq +rule ar cs en es fi fr it nl pt ru 93.07 99.53 98.1"
W19-8636,W18-6553,0,0.0157674,"tron to learn the parameters. Both linearizers achieve state-of-the-art performance on the Surface Realization Shared Task 2011 data (Belz et al., 2011) as part of a pipeline or joint system for the full task including deep semantic generation and word inflection (Bohnet et al., 2011; Puduppully et al., 2017). However, to the best of our knowledge, the two linearizers alone have never been directly compared. Also, they have not been tested on the data from the recent shared task (Belz et al., 2018), where they could have served as very strong baselines to put recent developments into context. Song et al. (2018) are the first to use a neural model for syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning (2014) to predict transitions for linearization, which essentially replaces the perceptron with an MLP for the transition system in Liu et al. (2015). However, their adoption of neural models only takes advantage of the token-level representation such as word embeddings, while the structural information is still not well modeled. Recently, many neural models are proposed to represent graph structures, cf. Zhou et al. (2018) for an overview. Among them, Tree-LSTM,"
W19-8636,P15-1150,0,0.474288,"syntactic linearization; they adapt the neural dependency parsing model by Chen and Manning (2014) to predict transitions for linearization, which essentially replaces the perceptron with an MLP for the transition system in Liu et al. (2015). However, their adoption of neural models only takes advantage of the token-level representation such as word embeddings, while the structural information is still not well modeled. Recently, many neural models are proposed to represent graph structures, cf. Zhou et al. (2018) for an overview. Among them, Tree-LSTM, in particular the Child-Sum variation (Tai et al., 2015), has been proposed to model (unordered) We present a dependency tree linearization model with two novel components: (1) a tree-structured encoder based on bidirectional Tree-LSTM that propagates information first bottom-up then top-down, which allows each token to access information from the entire tree; and (2) a linguistically motivated headfirst decoder that emphasizes the central role of the head and linearizes the subtree by incrementally attaching the dependents on both sides of the head. With the new encoder and decoder, we reach state-of-the-art performance on the Surface Realization"
W19-8636,C16-1274,0,0.271415,"sing availability of annotated treebanks, statistical methods gain popularity (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Filippova and Strube, 2009). 279 Proceedings of The 12th International Conference on Natural Language Generation, pages 279–289, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics cheio dependency trees. It differs from the sequential LSTM (Hochreiter and Schmidhuber, 1997) in that it aggregates the hidden states of multiple dependents by summation. It is in turn improved by adding the attention mechanism to the hidden states (Zhou et al., 2016), so that each dependent influences the head representation to different degrees. Miwa and Bansal (2016) propose a bidirectional extension that traverses the tree both bottom-up and top-down to allow the tokens access information from their descendants as well as ancestors. We adopt and combine their proposed models to represent the tree structure in our task, while improving the bidirectional extension by using the output of the bottom-up pass as the input for the top-down pass, so that each token can access information from all other tokens. tesouro este estar . de (1) linearization estar ch"
